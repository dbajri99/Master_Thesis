[
    {
        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
        "authors": [
            "Hangfeng He",
            "Hongming Zhang",
            "Dan Roth"
        ],
        "published": "2022-12-31T22:35:34Z",
        "summary": "Despite the success of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, the stored knowledge in these models may\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\nincorporating external knowledge often require additional training or\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\nthis issue, we propose a novel post-processing approach, rethinking with\nretrieval (RR), which retrieves relevant external knowledge based on the\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\nThis lightweight approach does not require additional training or fine-tuning\nand is not limited by the input length of LLMs. We evaluate the effectiveness\nof RR through extensive experiments with GPT-3 on three complex reasoning\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\nresults show that RR can produce more faithful explanations and improve the\nperformance of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.00303v1.pdf"
    },
    {
        "title": "A Survey on In-context Learning",
        "authors": [
            "Qingxiu Dong",
            "Lei Li",
            "Damai Dai",
            "Ce Zheng",
            "Zhiyong Wu",
            "Baobao Chang",
            "Xu Sun",
            "Jingjing Xu",
            "Lei Li",
            "Zhifang Sui"
        ],
        "published": "2022-12-31T15:57:09Z",
        "summary": "With the increasing ability of large language models (LLMs), in-context\nlearning (ICL) has become a new paradigm for natural language processing (NLP),\nwhere LLMs make predictions only based on contexts augmented with a few\nexamples. It has been a new trend to explore ICL to evaluate and extrapolate\nthe ability of LLMs. In this paper, we aim to survey and summarize the progress\nand challenges of ICL. We first present a formal definition of ICL and clarify\nits correlation to related studies. Then, we organize and discuss advanced\ntechniques, including training strategies, demonstration designing strategies,\nas well as related analysis. Finally, we discuss the challenges of ICL and\nprovide potential directions for further research. We hope that our work can\nencourage more research on uncovering how ICL works and improving ICL.",
        "pdf_link": "https://arxiv.org/pdf/2301.00234v3.pdf"
    },
    {
        "title": "Logic Mill -- A Knowledge Navigation System",
        "authors": [
            "Sebastian Erhardt",
            "Mainak Ghosh",
            "Erik Buunk",
            "Michael E. Rose",
            "Dietmar Harhoff"
        ],
        "published": "2022-12-31T13:46:50Z",
        "summary": "Logic Mill is a scalable and openly accessible software system that\nidentifies semantically similar documents within either one domain-specific\ncorpus or multi-domain corpora. It uses advanced Natural Language Processing\n(NLP) techniques to generate numerical representations of documents. Currently\nit leverages a large pre-trained language model to generate these document\nrepresentations. The system focuses on scientific publications and patent\ndocuments and contains more than 200 million documents. It is easily accessible\nvia a simple Application Programming Interface (API) or via a web interface.\nMoreover, it is continuously being updated and can be extended to text corpora\nfrom other domains. We see this system as a general-purpose tool for future\nresearch applications in the social sciences and other domains.",
        "pdf_link": "https://arxiv.org/pdf/2301.00200v2.pdf"
    },
    {
        "title": "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?",
        "authors": [
            "Wenhao Wu",
            "Haipeng Luo",
            "Bo Fang",
            "Jingdong Wang",
            "Wanli Ouyang"
        ],
        "published": "2022-12-31T11:50:32Z",
        "summary": "Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .",
        "pdf_link": "https://arxiv.org/pdf/2301.00184v3.pdf"
    },
    {
        "title": "Towards Proactively Forecasting Sentence-Specific Information Popularity within Online News Documents",
        "authors": [
            "Sayar Ghosh Roy",
            "Anshul Padhi",
            "Risubh Jain",
            "Manish Gupta",
            "Vasudeva Varma"
        ],
        "published": "2022-12-31T08:40:08Z",
        "summary": "Multiple studies have focused on predicting the prospective popularity of an\nonline document as a whole, without paying attention to the contributions of\nits individual parts. We introduce the task of proactively forecasting\npopularities of sentences within online news documents solely utilizing their\nnatural language content. We model sentence-specific popularity forecasting as\na sequence regression task. For training our models, we curate InfoPop, the\nfirst dataset containing popularity labels for over 1.7 million sentences from\nover 50,000 online news documents. To the best of our knowledge, this is the\nfirst dataset automatically created using streams of incoming search engine\nqueries to generate sentence-level popularity annotations. We propose a novel\ntransfer learning approach involving sentence salience prediction as an\nauxiliary task. Our proposed technique coupled with a BERT-based neural model\nexceeds nDCG values of 0.8 for proactive sentence-specific popularity\nforecasting. Notably, our study presents a non-trivial takeaway: though\npopularity and salience are different concepts, transfer learning from salience\nprediction enhances popularity forecasting. We release InfoPop and make our\ncode publicly available: https://github.com/sayarghoshroy/InfoPopularity",
        "pdf_link": "https://arxiv.org/pdf/2301.00152v1.pdf"
    },
    {
        "title": "Broad Learning System with Takagi-Sugeno Fuzzy Subsystem for Tobacco Origin Identification based on Near Infrared Spectroscopy",
        "authors": [
            "Di Wang",
            "Simon X. Yang"
        ],
        "published": "2022-12-31T05:38:37Z",
        "summary": "Tobacco origin identification is significantly important in tobacco industry.\nModeling analysis for sensor data with near infrared spectroscopy has become a\npopular method for rapid detection of internal features. However, for sensor\ndata analysis using traditional artificial neural network or deep network\nmodels, the training process is extremely time-consuming. In this paper, a\nnovel broad learning system with Takagi-Sugeno (TS) fuzzy subsystem is proposed\nfor rapid identification of tobacco origin. Incremental learning is employed in\nthe proposed method, which obtains the weight matrix of the network after a\nvery small amount of computation, resulting in much shorter training time for\nthe model, with only about 3 seconds for the extra step training. The\nexperimental results show that the TS fuzzy subsystem can extract features from\nthe near infrared data and effectively improve the recognition performance. The\nproposed method can achieve the highest prediction accuracy (95.59 %) in\ncomparison to the traditional classification algorithms, artificial neural\nnetwork, and deep convolutional neural network, and has a great advantage in\nthe training time with only about 128 seconds.",
        "pdf_link": "https://arxiv.org/pdf/2301.00126v1.pdf"
    },
    {
        "title": "Inconsistencies in Masked Language Models",
        "authors": [
            "Tom Young",
            "Yunan Chen",
            "Yang You"
        ],
        "published": "2022-12-30T22:53:25Z",
        "summary": "Learning to predict masked tokens in a sequence has been shown to be a\nhelpful pretraining objective for powerful language models such as PaLM2. After\ntraining, such masked language models (MLMs) can provide distributions of\ntokens in the masked positions in a sequence. However, this paper shows that\ndistributions corresponding to different masking patterns can demonstrate\nconsiderable inconsistencies, i.e., they cannot be derived from a coherent\njoint distribution when considered together.\n  This fundamental flaw in MLMs can lead to self-contradictory behaviors during\ninference. On various benchmark datasets including MMLU, MLMs can give\ndifferent predictions to the same input question. From BERT-base to UL2-20B, we\nshow that such inconsistencies exist ubiquitously in MLMs of diverse sizes and\nconfigurations. In light of our observations, we further propose an\ninference-time strategy for MLMs called Ensemble of Conditionals. It jointly\nconsiders a selected range of inconsistent conditionals directly produced by\nthe MLM for the final prediction, which often leads to considerable accuracy\nimprovement.",
        "pdf_link": "https://arxiv.org/pdf/2301.00068v3.pdf"
    },
    {
        "title": "Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition",
        "authors": [
            "Yukun Feng",
            "Ming Tu",
            "Rui Xia",
            "Chuanzeng Huang",
            "Yuxuan Wang"
        ],
        "published": "2022-12-30T22:26:57Z",
        "summary": "Recent studies have shown that using an external Language Model (LM) benefits\nthe end-to-end Automatic Speech Recognition (ASR). However, predicting tokens\nthat appear less frequently in the training set is still quite challenging. The\nlong-tail prediction problems have been widely studied in many applications,\nbut only been addressed by a few studies for ASR and LMs. In this paper, we\npropose a new memory augmented lookup dictionary based Transformer architecture\nfor LM. The newly introduced lookup dictionary incorporates rich contextual\ninformation in training set, which is vital to correctly predict long-tail\ntokens. With intensive experiments on Chinese and English data sets, our\nproposed method is proved to outperform the baseline Transformer LM by a great\nmargin on both word/character error rate and tail tokens error rate. This is\nachieved without impact on the decoding efficiency. Overall, we demonstrate the\neffectiveness of our proposed method in boosting the ASR decoding performance,\nespecially for long-tail tokens.",
        "pdf_link": "https://arxiv.org/pdf/2301.00066v1.pdf"
    },
    {
        "title": "ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports",
        "authors": [
            "Katharina Jeblick",
            "Balthasar Schachtner",
            "Jakob Dexl",
            "Andreas Mittermeier",
            "Anna Theresa St\u00fcber",
            "Johanna Topalis",
            "Tobias Weber",
            "Philipp Wesp",
            "Bastian Sabel",
            "Jens Ricke",
            "Michael Ingrisch"
        ],
        "published": "2022-12-30T18:55:16Z",
        "summary": "The release of ChatGPT, a language model capable of generating text that\nappears human-like and authentic, has gained significant attention beyond the\nresearch community. We expect that the convincing performance of ChatGPT\nincentivizes users to apply it to a variety of downstream tasks, including\nprompting the model to simplify their own medical reports. To investigate this\nphenomenon, we conducted an exploratory case study. In a questionnaire, we\nasked 15 radiologists to assess the quality of radiology reports simplified by\nChatGPT. Most radiologists agreed that the simplified reports were factually\ncorrect, complete, and not potentially harmful to the patient. Nevertheless,\ninstances of incorrect statements, missed key medical findings, and potentially\nharmful passages were reported. While further studies are needed, the initial\ninsights of this study indicate a great potential in using large language\nmodels like ChatGPT to improve patient-centered care in radiology and other\nmedical domains.",
        "pdf_link": "https://arxiv.org/pdf/2212.14882v1.pdf"
    },
    {
        "title": "An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models",
        "authors": [
            "Yufeng Zhang",
            "Boyi Liu",
            "Qi Cai",
            "Lingxiao Wang",
            "Zhaoran Wang"
        ],
        "published": "2022-12-30T17:59:01Z",
        "summary": "With the attention mechanism, transformers achieve significant empirical\nsuccesses. Despite the intuitive understanding that transformers perform\nrelational inference over long sequences to produce desirable representations,\nwe lack a rigorous theory on how the attention mechanism achieves it. In\nparticular, several intriguing questions remain open: (a) What makes a\ndesirable representation? (b) How does the attention mechanism infer the\ndesirable representation within the forward pass? (c) How does a pretraining\nprocedure learn to infer the desirable representation through the backward\npass?\n  We observe that, as is the case in BERT and ViT, input tokens are often\nexchangeable since they already include positional encodings. The notion of\nexchangeability induces a latent variable model that is invariant to input\nsizes, which enables our theoretical analysis.\n  - To answer (a) on representation, we establish the existence of a sufficient\nand minimal representation of input tokens. In particular, such a\nrepresentation instantiates the posterior distribution of the latent variable\ngiven input tokens, which plays a central role in predicting output labels and\nsolving downstream tasks.\n  - To answer (b) on inference, we prove that attention with the desired\nparameter infers the latent posterior up to an approximation error, which is\ndecreasing in input sizes. In detail, we quantify how attention approximates\nthe conditional mean of the value given the key, which characterizes how it\nperforms relational inference over long sequences.\n  - To answer (c) on learning, we prove that both supervised and\nself-supervised objectives allow empirical risk minimization to learn the\ndesired parameter up to a generalization error, which is independent of input\nsizes. Particularly, in the self-supervised setting, we identify a condition\nnumber that is pivotal to solving downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.14852v3.pdf"
    },
    {
        "title": "Black-box language model explanation by context length probing",
        "authors": [
            "Ond\u0159ej C\u00edfka",
            "Antoine Liutkus"
        ],
        "published": "2022-12-30T16:24:10Z",
        "summary": "The increasingly widespread adoption of large language models has highlighted\nthe need for improving their explainability. We present context length probing,\na novel explanation technique for causal language models, based on tracking the\npredictions of a model as a function of the length of available context, and\nallowing to assign differential importance scores to different contexts. The\ntechnique is model-agnostic and does not rely on access to model internals\nbeyond computing token-level probabilities. We apply context length probing to\nlarge pre-trained language models and offer some initial analyses and insights,\nincluding the potential for studying long-range dependencies. The source code\nand an interactive demo of the method are available.",
        "pdf_link": "https://arxiv.org/pdf/2212.14815v3.pdf"
    },
    {
        "title": "Distant Reading of the German Coalition Deal: Recognizing Policy Positions with BERT-based Text Classification",
        "authors": [
            "Michael Zylla",
            "Thomas Haider"
        ],
        "published": "2022-12-30T12:20:39Z",
        "summary": "Automated text analysis has become a widely used tool in political science.\nIn this research, we use a BERT model trained on German party manifestos to\nidentify the individual parties' contribution to the coalition agreement of\n2021.",
        "pdf_link": "https://arxiv.org/pdf/2212.14648v1.pdf"
    },
    {
        "title": "How would Stance Detection Techniques Evolve after the Launch of ChatGPT?",
        "authors": [
            "Bowen Zhang",
            "Daijun Ding",
            "Liwen Jing"
        ],
        "published": "2022-12-30T05:03:15Z",
        "summary": "Stance detection refers to the task of extracting the standpoint (Favor,\nAgainst or Neither) towards a target in given texts. Such research gains\nincreasing attention with the proliferation of social media contents. The\nconventional framework of handling stance detection is converting it into text\nclassification tasks. Deep learning models have already replaced rule-based\nmodels and traditional machine learning models in solving such problems.\nCurrent deep neural networks are facing two main challenges which are\ninsufficient labeled data and information in social media posts and the\nunexplainable nature of deep learning models. A new pre-trained language model\nchatGPT was launched on Nov 30, 2022. For the stance detection tasks, our\nexperiments show that ChatGPT can achieve SOTA or similar performance for\ncommonly used datasets including SemEval-2016 and P-Stance. At the same time,\nChatGPT can provide explanation for its own prediction, which is beyond the\ncapability of any existing model. The explanations for the cases it cannot\nprovide classification results are especially useful. ChatGPT has the potential\nto be the best AI model for stance detection tasks in NLP, or at least change\nthe research paradigm of this field. ChatGPT also opens up the possibility of\nbuilding explanatory AI for stance detection.",
        "pdf_link": "https://arxiv.org/pdf/2212.14548v3.pdf"
    },
    {
        "title": "Targeted Phishing Campaigns using Large Scale Language Models",
        "authors": [
            "Rabimba Karanjai"
        ],
        "published": "2022-12-30T03:18:05Z",
        "summary": "In this research, we aim to explore the potential of natural language models\n(NLMs) such as GPT-3 and GPT-2 to generate effective phishing emails. Phishing\nemails are fraudulent messages that aim to trick individuals into revealing\nsensitive information or taking actions that benefit the attackers. We propose\na framework for evaluating the performance of NLMs in generating these types of\nemails based on various criteria, including the quality of the generated text,\nthe ability to bypass spam filters, and the success rate of tricking\nindividuals. Our evaluations show that NLMs are capable of generating phishing\nemails that are difficult to detect and that have a high success rate in\ntricking individuals, but their effectiveness varies based on the specific NLM\nand training data used. Our research indicates that NLMs could have a\nsignificant impact on the prevalence of phishing attacks and emphasizes the\nneed for further study on the ethical and security implications of using NLMs\nfor malicious purposes.",
        "pdf_link": "https://arxiv.org/pdf/2301.00665v1.pdf"
    },
    {
        "title": "GPT Takes the Bar Exam",
        "authors": [
            "Michael Bommarito II",
            "Daniel Martin Katz"
        ],
        "published": "2022-12-29T18:19:43Z",
        "summary": "Nearly all jurisdictions in the United States require a professional license\nexam, commonly referred to as \"the Bar Exam,\" as a precondition for law\npractice. To even sit for the exam, most jurisdictions require that an\napplicant completes at least seven years of post-secondary education, including\nthree years at an accredited law school. In addition, most test-takers also\nundergo weeks to months of further, exam-specific preparation. Despite this\nsignificant investment of time and capital, approximately one in five\ntest-takers still score under the rate required to pass the exam on their first\ntry. In the face of a complex task that requires such depth of knowledge, what,\nthen, should we expect of the state of the art in \"AI?\" In this research, we\ndocument our experimental evaluation of the performance of OpenAI's\n`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate\nmultiple choice (MBE) section of the exam. While we find no benefit in\nfine-tuning over GPT-3.5's zero-shot performance at the scale of our training\ndata, we do find that hyperparameter optimization and prompt engineering\npositively impacted GPT-3.5's zero-shot performance. For best prompt and\nparameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete\nNCBE MBE practice exam, significantly in excess of the 25% baseline guessing\nrate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's\nranking of responses is also highly-correlated with correctness; its top two\nand top three choices are correct 71% and 88% of the time, respectively,\nindicating very strong non-entailment performance. While our ability to\ninterpret these results is limited by nascent scientific understanding of LLMs\nand the proprietary nature of GPT, we believe that these results strongly\nsuggest that an LLM will pass the MBE component of the Bar Exam in the near\nfuture.",
        "pdf_link": "https://arxiv.org/pdf/2212.14402v1.pdf"
    },
    {
        "title": "Maximizing Use-Case Specificity through Precision Model Tuning",
        "authors": [
            "Pranjali Awasthi",
            "David Recio-Mitter",
            "Yosuke Kyle Sugi"
        ],
        "published": "2022-12-29T07:50:14Z",
        "summary": "Language models have become increasingly popular in recent years for tasks\nlike information retrieval. As use-cases become oriented toward specific\ndomains, fine-tuning becomes default for standard performance. To fine-tune\nthese models for specific tasks and datasets, it is necessary to carefully tune\nthe model's hyperparameters and training techniques. In this paper, we present\nan in-depth analysis of the performance of four transformer-based language\nmodels on the task of biomedical information retrieval. The models we consider\nare DeepMind's RETRO (7B parameters), GPT-J (6B parameters), GPT-3 (175B\nparameters), and BLOOM (176B parameters). We compare their performance on the\nbasis of relevance, accuracy, and interpretability, using a large corpus of\n480000 research papers on protein structure/function prediction as our dataset.\nOur findings suggest that smaller models, with <10B parameters and fine-tuned\non domain-specific datasets, tend to outperform larger language models on\nhighly specific questions in terms of accuracy, relevancy, and interpretability\nby a significant margin (+50% on average). However, larger models do provide\ngenerally better results on broader prompts.",
        "pdf_link": "https://arxiv.org/pdf/2212.14206v1.pdf"
    },
    {
        "title": "Cramming: Training a Language Model on a Single GPU in One Day",
        "authors": [
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2022-12-28T18:59:28Z",
        "summary": "Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n  We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.",
        "pdf_link": "https://arxiv.org/pdf/2212.14034v1.pdf"
    },
    {
        "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
        "authors": [
            "Omar Khattab",
            "Keshav Santhanam",
            "Xiang Lisa Li",
            "David Hall",
            "Percy Liang",
            "Christopher Potts",
            "Matei Zaharia"
        ],
        "published": "2022-12-28T18:52:44Z",
        "summary": "Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-intensive tasks using frozen language models (LM) and\nretrieval models (RM). Existing work has combined these in simple\n\"retrieve-then-read\" pipelines in which the RM retrieves passages that are\ninserted into the LM prompt. To begin to fully realize the potential of frozen\nLMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that\nrelies on passing natural language texts in sophisticated pipelines between an\nLM and an RM. DSP can express high-level programs that bootstrap pipeline-aware\ndemonstrations, search for relevant passages, and generate grounded\npredictions, systematically breaking down problems into small transformations\nthat the LM and RM can handle more reliably. We have written novel DSP programs\nfor answering questions in open-domain, multi-hop, and conversational settings,\nestablishing in early evaluations new state-of-the-art in-context learning\nresults and delivering 37-120%, 8-39%, and 80-290% relative gains against the\nvanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a\ncontemporaneous self-ask pipeline, respectively. We release DSP at\nhttps://github.com/stanfordnlp/dsp",
        "pdf_link": "https://arxiv.org/pdf/2212.14024v2.pdf"
    },
    {
        "title": "Using Large Language Models to Generate Engaging Captions for Data Visualizations",
        "authors": [
            "Ashley Liew",
            "Klaus Mueller"
        ],
        "published": "2022-12-27T23:56:57Z",
        "summary": "Creating compelling captions for data visualizations has been a longstanding\nchallenge. Visualization researchers are typically untrained in journalistic\nreporting and hence the captions that are placed below data visualizations tend\nto be not overly engaging and rather just stick to basic observations about the\ndata. In this work we explore the opportunities offered by the newly emerging\ncrop of large language models (LLM) which use sophisticated deep learning\ntechnology to produce human-like prose. We ask, can these powerful software\ndevices be purposed to produce engaging captions for generic data\nvisualizations like a scatterplot. It turns out that the key challenge lies in\ndesigning the most effective prompt for the LLM, a task called prompt\nengineering. We report on first experiments using the popular LLM GPT-3 and\ndeliver some promising results.",
        "pdf_link": "https://arxiv.org/pdf/2212.14047v1.pdf"
    },
    {
        "title": "TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence",
        "authors": [
            "Wang Qi",
            "Rui Liu",
            "Yuan Zuo",
            "Yong Chen",
            "Dell Zhang"
        ],
        "published": "2022-12-27T11:50:14Z",
        "summary": "Creating an essay based on a few given topics is a challenging NLP task.\nAlthough several effective methods for this problem, topic-to-essay generation,\nhave appeared recently, there is still much room for improvement, especially in\nterms of the coverage of the given topics and the coherence of the generated\ntext. In this paper, we propose a novel approach called TegFormer which\nutilizes the Transformer architecture where the encoder is enriched with\ndomain-specific contexts while the decoder is enhanced by a large-scale\npre-trained language model. Specifically, a \\emph{Topic-Extension} layer\ncapturing the interaction between the given topics and their domain-specific\ncontexts is plugged into the encoder. Since the given topics are usually\nconcise and sparse, such an additional layer can bring more topic-related\nsemantics in to facilitate the subsequent natural language generation.\nMoreover, an \\emph{Embedding-Fusion} module that combines the domain-specific\nword embeddings learnt from the given corpus and the general-purpose word\nembeddings provided by a GPT-2 model pre-trained on massive text data is\nintegrated into the decoder. Since GPT-2 is at a much larger scale, it contains\na lot more implicit linguistic knowledge which would help the decoder to\nproduce more grammatical and readable text. Extensive experiments have shown\nthat the pieces of text generated by TegFormer have better topic coverage and\nhigher text coherence than those from SOTA topic-to-essay techniques, according\nto automatic and human evaluations. As revealed by ablation studies, both the\nTopic-Extension layer and the Embedding-Fusion module contribute substantially\nto TegFormer's performance advantage.",
        "pdf_link": "https://arxiv.org/pdf/2212.13456v1.pdf"
    },
    {
        "title": "A Survey on Knowledge-Enhanced Pre-trained Language Models",
        "authors": [
            "Chaoqi Zhen",
            "Yanlei Shang",
            "Xiangyu Liu",
            "Yifei Li",
            "Yong Chen",
            "Dell Zhang"
        ],
        "published": "2022-12-27T09:54:14Z",
        "summary": "Natural Language Processing (NLP) has been revolutionized by the use of\nPre-trained Language Models (PLMs) such as BERT. Despite setting new records in\nnearly every NLP task, PLMs still face a number of challenges including poor\ninterpretability, weak reasoning capability, and the need for a lot of\nexpensive annotated data when applied to downstream tasks. By integrating\nexternal knowledge into PLMs,\n\\textit{\\underline{K}nowledge-\\underline{E}nhanced \\underline{P}re-trained\n\\underline{L}anguage \\underline{M}odels} (KEPLMs) have the potential to\novercome the above-mentioned limitations. In this paper, we examine KEPLMs\nsystematically through a series of studies. Specifically, we outline the common\ntypes and different formats of knowledge to be integrated into KEPLMs, detail\nthe existing methods for building and evaluating KEPLMS, present the\napplications of KEPLMs in downstream tasks, and discuss the future research\ndirections. Researchers will benefit from this survey by gaining a quick and\ncomprehensive overview of the latest developments in this field.",
        "pdf_link": "https://arxiv.org/pdf/2212.13428v1.pdf"
    },
    {
        "title": "DeepCuts: Single-Shot Interpretability based Pruning for BERT",
        "authors": [
            "Jasdeep Singh Grover",
            "Bhavesh Gawri",
            "Ruskin Raj Manku"
        ],
        "published": "2022-12-27T07:21:41Z",
        "summary": "As language models have grown in parameters and layers, it has become much\nharder to train and infer with them on single GPUs. This is severely\nrestricting the availability of large language models such as GPT-3,\nBERT-Large, and many others. A common technique to solve this problem is\npruning the network architecture by removing transformer heads, fully-connected\nweights, and other modules. The main challenge is to discern the important\nparameters from the less important ones. Our goal is to find strong metrics for\nidentifying such parameters. We thus propose two strategies: Cam-Cut based on\nthe GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for\ncalculating the importance scores. Through this work, we show that our scoring\nfunctions are able to assign more relevant task-based scores to the network\nparameters, and thus both our pruning approaches significantly outperform the\nstandard weight and gradient-based strategies, especially at higher compression\nratios in BERT-based models. We also analyze our pruning masks and find them to\nbe significantly different from the ones obtained using standard metrics.",
        "pdf_link": "https://arxiv.org/pdf/2212.13392v1.pdf"
    },
    {
        "title": "Measuring an artificial intelligence agent's trust in humans using machine incentives",
        "authors": [
            "Tim Johnson",
            "Nick Obradovich"
        ],
        "published": "2022-12-27T06:05:49Z",
        "summary": "Scientists and philosophers have debated whether humans can trust advanced\nartificial intelligence (AI) agents to respect humanity's best interests. Yet\nwhat about the reverse? Will advanced AI agents trust humans? Gauging an AI\nagent's trust in humans is challenging because--absent costs for\ndishonesty--such agents might respond falsely about their trust in humans. Here\nwe present a method for incentivizing machine decisions without altering an AI\nagent's underlying algorithms or goal orientation. In two separate experiments,\nwe then employ this method in hundreds of trust games between an AI agent (a\nLarge Language Model (LLM) from OpenAI) and a human experimenter (author TJ).\nIn our first experiment, we find that the AI agent decides to trust humans at\nhigher rates when facing actual incentives than when making hypothetical\ndecisions. Our second experiment replicates and extends these findings by\nautomating game play and by homogenizing question wording. We again observe\nhigher rates of trust when the AI agent faces real incentives. Across both\nexperiments, the AI agent's trust decisions appear unrelated to the magnitude\nof stakes. Furthermore, to address the possibility that the AI agent's trust\ndecisions reflect a preference for uncertainty, the experiments include two\nconditions that present the AI agent with a non-social decision task that\nprovides the opportunity to choose a certain or uncertain option; in those\nconditions, the AI agent consistently chooses the certain option. Our\nexperiments suggest that one of the most advanced AI language models to date\nalters its social behavior in response to incentives and displays behavior\nconsistent with trust toward a human interlocutor when incentivized.",
        "pdf_link": "https://arxiv.org/pdf/2212.13371v1.pdf"
    },
    {
        "title": "Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers",
        "authors": [
            "Qihao Zhu",
            "Xinyu Zhang",
            "Jianxi Luo"
        ],
        "published": "2022-12-26T16:06:04Z",
        "summary": "Biological systems in nature have evolved for millions of years to adapt and\nsurvive the environment. Many features they developed can be inspirational and\nbeneficial for solving technical problems in modern industries. This leads to a\nspecific form of design-by-analogy called bio-inspired design (BID). Although\nBID as a design method has been proven beneficial, the gap between biology and\nengineering continuously hinders designers from effectively applying the\nmethod. Therefore, we explore the recent advance of artificial intelligence\n(AI) for a data-driven approach to bridge the gap. This paper proposes a\ngenerative design approach based on the generative pre-trained language model\n(PLM) to automatically retrieve and map biological analogy and generate BID in\nthe form of natural language. The latest generative pre-trained transformer,\nnamely GPT-3, is used as the base PLM. Three types of design concept generators\nare identified and fine-tuned from the PLM according to the looseness of the\nproblem space representation. Machine evaluators are also fine-tuned to assess\nthe mapping relevancy between the domains within the generated BID concepts.\nThe approach is evaluated and then employed in a real-world project of\ndesigning light-weighted flying cars during its conceptual design phase The\nresults show our approach can generate BID concepts with good performance.",
        "pdf_link": "https://arxiv.org/pdf/2212.13196v1.pdf"
    },
    {
        "title": "Off-Policy Reinforcement Learning with Loss Function Weighted by Temporal Difference Error",
        "authors": [
            "Bumgeun Park",
            "Taeyoung Kim",
            "Woohyeon Moon",
            "Luiz Felipe Vecchietti",
            "Dongsoo Har"
        ],
        "published": "2022-12-26T14:32:16Z",
        "summary": "Training agents via off-policy deep reinforcement learning (RL) requires a\nlarge memory, named replay memory, that stores past experiences used for\nlearning. These experiences are sampled, uniformly or non-uniformly, to create\nthe batches used for training. When calculating the loss function, off-policy\nalgorithms assume that all samples are of the same importance. In this paper,\nwe hypothesize that training can be enhanced by assigning different importance\nfor each experience based on their temporal-difference (TD) error directly in\nthe training objective. We propose a novel method that introduces a weighting\nfactor for each experience when calculating the loss function at the learning\nstage. In addition to improving convergence speed when used with uniform\nsampling, the method can be combined with prioritization methods for\nnon-uniform sampling. Combining the proposed method with prioritization methods\nimproves sampling efficiency while increasing the performance of TD-based\noff-policy RL algorithms. The effectiveness of the proposed method is\ndemonstrated by experiments in six environments of the OpenAI Gym suite. The\nexperimental results demonstrate that the proposed method achieves a 33%~76%\nreduction of convergence speed in three environments and an 11% increase in\nreturns and a 3%~10% increase in success rate for other three environments.",
        "pdf_link": "https://arxiv.org/pdf/2212.13175v1.pdf"
    },
    {
        "title": "Large Language Models Encode Clinical Knowledge",
        "authors": [
            "Karan Singhal",
            "Shekoofeh Azizi",
            "Tao Tu",
            "S. Sara Mahdavi",
            "Jason Wei",
            "Hyung Won Chung",
            "Nathan Scales",
            "Ajay Tanwani",
            "Heather Cole-Lewis",
            "Stephen Pfohl",
            "Perry Payne",
            "Martin Seneviratne",
            "Paul Gamble",
            "Chris Kelly",
            "Nathaneal Scharli",
            "Aakanksha Chowdhery",
            "Philip Mansfield",
            "Blaise Aguera y Arcas",
            "Dale Webster",
            "Greg S. Corrado",
            "Yossi Matias",
            "Katherine Chou",
            "Juraj Gottweis",
            "Nenad Tomasev",
            "Yun Liu",
            "Alvin Rajkomar",
            "Joelle Barral",
            "Christopher Semturs",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2022-12-26T14:28:24Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2212.13138v1.pdf"
    },
    {
        "title": "Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment",
        "authors": [
            "Yechun Tang",
            "Xiaoxia Cheng",
            "Weiming Lu"
        ],
        "published": "2022-12-26T08:12:41Z",
        "summary": "Complex knowledge base question answering can be achieved by converting\nquestions into sequences of predefined actions. However, there is a significant\nsemantic and structural gap between natural language and action sequences,\nwhich makes this conversion difficult. In this paper, we introduce an\nalignment-enhanced complex question answering framework, called ALCQA, which\nmitigates this gap through question-to-action alignment and\nquestion-to-question alignment. We train a question rewriting model to align\nthe question and each action, and utilize a pretrained language model to\nimplicitly align the question and KG artifacts. Moreover, considering that\nsimilar questions correspond to similar action sequences, we retrieve top-k\nsimilar question-answer pairs at the inference stage through\nquestion-to-question alignment and propose a novel reward-guided action\nsequence selection strategy to select from candidate action sequences. We\nconduct experiments on CQA and WQSP datasets, and the results show that our\napproach outperforms state-of-the-art methods and obtains a 9.88\\% improvements\nin the F1 metric on CQA dataset. Our source code is available at\nhttps://github.com/TTTTTTTTy/ALCQA.",
        "pdf_link": "https://arxiv.org/pdf/2212.13036v1.pdf"
    },
    {
        "title": "Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text",
        "authors": [
            "Liam Dugan",
            "Daphne Ippolito",
            "Arun Kirubarajan",
            "Sherry Shi",
            "Chris Callison-Burch"
        ],
        "published": "2022-12-24T06:40:25Z",
        "summary": "As text generated by large language models proliferates, it becomes vital to\nunderstand how humans engage with such text, and whether or not they are able\nto detect when the text they are reading did not originate with a human writer.\nPrior work on human detection of generated text focuses on the case where an\nentire passage is either human-written or machine-generated. In this paper, we\nstudy a more realistic setting where text begins as human-written and\ntransitions to being generated by state-of-the-art neural language models. We\nshow that, while annotators often struggle at this task, there is substantial\nvariance in annotator skill and that given proper incentives, annotators can\nimprove at this task over time. Furthermore, we conduct a detailed comparison\nstudy and analyze how a variety of variables (model size, decoding strategy,\nfine-tuning, prompt genre, etc.) affect human detection performance. Finally,\nwe collect error annotations from our participants and use them to show that\ncertain textual genres influence models to make different types of errors and\nthat certain sentence-level features correlate highly with annotator selection.\nWe release the RoFT dataset: a collection of over 21,000 human annotations\npaired with error classifications to encourage future work in human detection\nand evaluation of generated text.",
        "pdf_link": "https://arxiv.org/pdf/2212.12672v1.pdf"
    },
    {
        "title": "Benchmark for Uncertainty & Robustness in Self-Supervised Learning",
        "authors": [
            "Ha Manh Bui",
            "Iliana Maifeld-Carucci"
        ],
        "published": "2022-12-23T15:46:23Z",
        "summary": "Self-Supervised Learning (SSL) is crucial for real-world applications,\nespecially in data-hungry domains such as healthcare and self-driving cars. In\naddition to a lack of labeled data, these applications also suffer from\ndistributional shifts. Therefore, an SSL method should provide robust\ngeneralization and uncertainty estimation in the test dataset to be considered\na reliable model in such high-stakes domains. However, existing approaches\noften focus on generalization, without evaluating the model's uncertainty. The\nability to compare SSL techniques for improving these estimates is therefore\ncritical for research on the reliability of self-supervision models. In this\npaper, we explore variants of SSL methods, including Jigsaw Puzzles, Context,\nRotation, Geometric Transformations Prediction for vision, as well as BERT and\nGPT for language tasks. We train SSL in auxiliary learning for vision and\npre-training for language model, then evaluate the generalization (in-out\nclassification accuracy) and uncertainty (expected calibration error) across\ndifferent distribution covariate shift datasets, including MNIST-C, CIFAR-10-C,\nCIFAR-10.1, and MNLI. Our goal is to create a benchmark with outputs from\nexperiments, providing a starting point for new SSL methods in Reliable Machine\nLearning. All source code to reproduce results is available at\nhttps://github.com/hamanhbui/reliable_ssl_baselines.",
        "pdf_link": "https://arxiv.org/pdf/2212.12411v1.pdf"
    },
    {
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2022-12-23T03:57:54Z",
        "summary": "This work presents a detailed linguistic analysis into why larger\nTransformer-based pre-trained language models with more parameters and lower\nperplexity nonetheless yield surprisal estimates that are less predictive of\nhuman reading times. First, regression analyses show a strictly monotonic,\npositive log-linear relationship between perplexity and fit to reading times\nfor the more recently released five GPT-Neo variants and eight OPT variants on\ntwo separate datasets, replicating earlier results limited to just GPT-2 (Oh et\nal., 2022). Subsequently, analysis of residual errors reveals a systematic\ndeviation of the larger variants, such as underpredicting reading times of\nnamed entities and making compensatory overpredictions for reading times of\nfunction words such as modals and conjunctions. These results suggest that the\npropensity of larger Transformer-based models to 'memorize' sequences during\ntraining makes their surprisal estimates diverge from humanlike expectations,\nwhich warrants caution in using pre-trained language models to study human\nlanguage processing.",
        "pdf_link": "https://arxiv.org/pdf/2212.12131v1.pdf"
    },
    {
        "title": "Enhancing the prediction of disease outcomes using electronic health records and pretrained deep learning models",
        "authors": [
            "Zhichao Yang",
            "Weisong Liu",
            "Dan Berlowitz",
            "Hong Yu"
        ],
        "published": "2022-12-22T22:53:32Z",
        "summary": "Question: Can an encoder-decoder architecture pretrained on a large dataset\nof longitudinal electronic health records improves patient outcome predictions?\nFindings: In this prognostic study of 6.8 million patients, our denoising\nsequence-to-sequence prediction model of multiple outcomes outperformed\nstate-of-the-art models scuh pretrained BERT on a broad range of patient\noutcomes, including intentional self-harm and pancreatic cancer. Meaning: Deep\nbidirectional and autoregressive representation improves patient outcome\nprediction.",
        "pdf_link": "https://arxiv.org/pdf/2212.12067v1.pdf"
    },
    {
        "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
        "authors": [
            "Srinivasan Iyer",
            "Xi Victoria Lin",
            "Ramakanth Pasunuru",
            "Todor Mihaylov",
            "Daniel Simig",
            "Ping Yu",
            "Kurt Shuster",
            "Tianlu Wang",
            "Qing Liu",
            "Punit Singh Koura",
            "Xian Li",
            "Brian O'Horo",
            "Gabriel Pereyra",
            "Jeff Wang",
            "Christopher Dewan",
            "Asli Celikyilmaz",
            "Luke Zettlemoyer",
            "Ves Stoyanov"
        ],
        "published": "2022-12-22T19:56:09Z",
        "summary": "Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats -- PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.",
        "pdf_link": "https://arxiv.org/pdf/2212.12017v3.pdf"
    },
    {
        "title": "Methodological reflections for AI alignment research using human feedback",
        "authors": [
            "Thilo Hagendorff",
            "Sarah Fabi"
        ],
        "published": "2022-12-22T14:27:33Z",
        "summary": "The field of artificial intelligence (AI) alignment aims to investigate\nwhether AI technologies align with human interests and values and function in a\nsafe and ethical manner. AI alignment is particularly relevant for large\nlanguage models (LLMs), which have the potential to exhibit unintended behavior\ndue to their ability to learn and adapt in ways that are difficult to predict.\nIn this paper, we discuss methodological challenges for the alignment problem\nspecifically in the context of LLMs trained to summarize texts. In particular,\nwe focus on methods for collecting reliable human feedback on summaries to\ntrain a reward model which in turn improves the summarization model. We\nconclude by suggesting specific improvements in the experimental design of\nalignment studies for LLMs' summarization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2301.06859v1.pdf"
    },
    {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "authors": [
            "Zhenghao Lin",
            "Yeyun Gong",
            "Yelong Shen",
            "Tong Wu",
            "Zhihao Fan",
            "Chen Lin",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2022-12-22T13:17:11Z",
        "summary": "In this paper, we introduce a novel dIffusion language modEl pre-training\nframework for text generation, which we call GENIE. GENIE is a large-scale\npretrained diffusion language model that consists of an encoder and a\ndiffusion-based decoder, which can generate text by gradually transforming a\nrandom noise sequence into a coherent text sequence. To pre-train GENIE on a\nlarge-scale language corpus, we design a new continuous paragraph denoise\nobjective, which encourages the diffusion-decoder to reconstruct a clean text\nparagraph from a corrupted version, while preserving the semantic and syntactic\ncoherence. We evaluate GENIE on four downstream text generation benchmarks,\nnamely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results\nshow that GENIE achieves comparable performance with the state-of-the-art\nautoregressive models on these benchmarks, and generates more diverse text\nsamples. The code and models of GENIE are available at\nhttps://github.com/microsoft/ProphetNet/tree/master/GENIE.",
        "pdf_link": "https://arxiv.org/pdf/2212.11685v2.pdf"
    },
    {
        "title": "Multi-Lingual DALL-E Storytime",
        "authors": [
            "Noga Mudrik",
            "Adam S. Charles"
        ],
        "published": "2022-12-22T07:06:35Z",
        "summary": "While recent advancements in artificial intelligence (AI) language models\ndemonstrate cutting-edge performance when working with English texts,\nequivalent models do not exist in other languages or do not reach the same\nperformance level. This undesired effect of AI advancements increases the gap\nbetween access to new technology from different populations across the world.\nThis unsought bias mainly discriminates against individuals whose English\nskills are less developed, e.g., non-English speakers children. Following\nsignificant advancements in AI research in recent years, OpenAI has recently\npresented DALL-E: a powerful tool for creating images based on English text\nprompts. While DALL-E is a promising tool for many applications, its decreased\nperformance when given input in a different language, limits its audience and\ndeepens the gap between populations. An additional limitation of the current\nDALL-E model is that it only allows for the creation of a few images in\nresponse to a given input prompt, rather than a series of consecutive coherent\nframes that tell a story or describe a process that changes over time. Here, we\npresent an easy-to-use automatic DALL-E storytelling framework that leverages\nthe existing DALL-E model to enable fast and coherent visualizations of\nnon-English songs and stories, pushing the limit of the one-step-at-a-time\noption DALL-E currently offers. We show that our framework is able to\neffectively visualize stories from non-English texts and portray the changes in\nthe plot over time. It is also able to create a narrative and maintain\ninterpretable changes in the description across frames. Additionally, our\nframework offers users the ability to specify constraints on the story\nelements, such as a specific location or context, and to maintain a consistent\nstyle throughout the visualization.",
        "pdf_link": "https://arxiv.org/pdf/2212.11985v1.pdf"
    },
    {
        "title": "CAMeMBERT: Cascading Assistant-Mediated Multilingual BERT",
        "authors": [
            "Dan DeGenaro",
            "Jugal Kalita"
        ],
        "published": "2022-12-22T02:19:25Z",
        "summary": "Large language models having hundreds of millions, and even billions, of\nparameters have performed extremely well on a variety of natural language\nprocessing (NLP) tasks. Their widespread use and adoption, however, is hindered\nby the lack of availability and portability of sufficiently large computational\nresources. This paper proposes a knowledge distillation (KD) technique building\non the work of LightMBERT, a student model of multilingual BERT (mBERT). By\nrepeatedly distilling mBERT through increasingly compressed toplayer distilled\nteacher assistant networks, CAMeMBERT aims to improve upon the time and space\ncomplexities of mBERT while keeping loss of accuracy beneath an acceptable\nthreshold. At present, CAMeMBERT has an average accuracy of around 60.1%, which\nis subject to change after future improvements to the hyperparameters used in\nfine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2212.11456v1.pdf"
    },
    {
        "title": "Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning",
        "authors": [
            "Chris Lengerich",
            "Gabriel Synnaeve",
            "Amy Zhang",
            "Hugh Leather",
            "Kurt Shuster",
            "Fran\u00e7ois Charton",
            "Charysse Redwood"
        ],
        "published": "2022-12-21T20:43:46Z",
        "summary": "Traditional approaches to RL have focused on learning decision policies\ndirectly from episodic decisions, while slowly and implicitly learning the\nsemantics of compositional representations needed for generalization. While\nsome approaches have been adopted to refine representations via auxiliary\nself-supervised losses while simultaneously learning decision policies,\nlearning compositional representations from hand-designed and\ncontext-independent self-supervised losses (multi-view) still adapts relatively\nslowly to the real world, which contains many non-IID subspaces requiring rapid\ndistribution shift in both time and spatial attention patterns at varying\nlevels of abstraction. In contrast, supervised language model cascades have\nshown the flexibility to adapt to many diverse manifolds, and hints of\nself-learning needed for autonomous task transfer. However, to date, transfer\nmethods for language models like few-shot learning and fine-tuning still\nrequire human supervision and transfer learning using self-learning methods has\nbeen underexplored. We propose a self-supervised loss policy called contrastive\ndistillation which manifests latent variables with high mutual information with\nboth source and target tasks from weights to tokens. We show how this\noutperforms common methods of transfer learning and suggests a useful design\naxis of trading off compute for generalizability for online transfer.\nContrastive distillation is improved through sampling from memory and suggests\na simple algorithm for more efficiently sampling negative examples for\ncontrastive losses than random sampling.",
        "pdf_link": "https://arxiv.org/pdf/2212.11353v1.pdf"
    },
    {
        "title": "What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis",
        "authors": [
            "Xiang Deng",
            "Vasilisa Bashlovkina",
            "Feng Han",
            "Simon Baumgartner",
            "Michael Bendersky"
        ],
        "published": "2022-12-21T19:11:19Z",
        "summary": "Market sentiment analysis on social media content requires knowledge of both\nfinancial markets and social media jargon, which makes it a challenging task\nfor human raters. The resulting lack of high-quality labeled data stands in the\nway of conventional supervised learning methods. Instead, we approach this\nproblem using semi-supervised learning with a large language model (LLM). Our\npipeline generates weak financial sentiment labels for Reddit posts with an LLM\nand then uses that data to train a small model that can be served in\nproduction. We find that prompting the LLM to produce Chain-of-Thought\nsummaries and forcing it through several reasoning paths helps generate more\nstable and accurate labels, while using a regression loss further improves\ndistillation quality. With only a handful of prompts, the final model performs\non par with existing supervised models. Though production applications of our\nmodel are limited by ethical considerations, the model's competitive\nperformance points to the great potential of using LLMs for tasks that\notherwise require skill-intensive annotation.",
        "pdf_link": "https://arxiv.org/pdf/2212.11311v1.pdf"
    },
    {
        "title": "Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges",
        "authors": [
            "Fabricio Goes",
            "Zisen Zhou",
            "Piotr Sawicki",
            "Marek Grzes",
            "Daniel G. Brown"
        ],
        "published": "2022-12-21T17:41:16Z",
        "summary": "This paper presents the Crowd Score, a novel method to assess the funniness\nof jokes using large language models (LLMs) as AI judges. Our method relies on\ninducing different personalities into the LLM and aggregating the votes of the\nAI judges into a single score to rate jokes. We validate the votes using an\nauditing technique that checks if the explanation for a particular vote is\nreasonable using the LLM. We tested our methodology on 52 jokes in a crowd of\nfour AI voters with different humour types: affiliative, self-enhancing,\naggressive and self-defeating. Our results show that few-shot prompting leads\nto better results than zero-shot for the voting question. Personality induction\nshowed that aggressive and self-defeating voters are significantly more\ninclined to find more jokes funny of a set of aggressive/self-defeating jokes\nthan the affiliative and self-enhancing voters. The Crowd Score follows the\nsame trend as human judges by assigning higher scores to jokes that are also\nconsidered funnier by human judges. We believe that our methodology could be\napplied to other creative domains such as story, poetry, slogans, etc. It could\nboth help the adoption of a flexible and accurate standard approach to compare\ndifferent work in the CC community under a common metric and by minimizing\nhuman participation in assessing creative artefacts, it could accelerate the\nprototyping of creative artefacts and reduce the cost of hiring human\nparticipants to rate creative artefacts.",
        "pdf_link": "https://arxiv.org/pdf/2212.11214v1.pdf"
    },
    {
        "title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2022-12-21T16:56:07Z",
        "summary": "Transformer-based large language models are trained to make predictions about\nthe next word by aggregating representations of previous tokens through their\nself-attention mechanism. In the field of cognitive modeling, such attention\npatterns have recently been interpreted as embodying the process of cue-based\nretrieval, in which attention over multiple targets is taken to generate\ninterference and latency during retrieval. Under this framework, this work\nfirst defines an entropy-based predictor that quantifies the diffuseness of\nself-attention, as well as distance-based predictors that capture the\nincremental change in attention patterns across timesteps. Moreover, following\nrecent studies that question the informativeness of attention weights, we also\nexperiment with alternative methods for incorporating vector norms into\nattention weights. Regression experiments using predictors calculated from the\nGPT-2 language model show that these predictors deliver a substantially better\nfit to held-out self-paced reading and eye-tracking data over a rigorous\nbaseline including GPT-2 surprisal. Additionally, the distance-based predictors\ngenerally demonstrated higher predictive power, with effect sizes of up to 6.59\nms per standard deviation on self-paced reading times (compared to 2.82 ms for\nsurprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared\nto 3.81 ms for surprisal).",
        "pdf_link": "https://arxiv.org/pdf/2212.11185v1.pdf"
    },
    {
        "title": "Parallel Context Windows for Large Language Models",
        "authors": [
            "Nir Ratner",
            "Yoav Levine",
            "Yonatan Belinkov",
            "Ori Ram",
            "Inbal Magar",
            "Omri Abend",
            "Ehud Karpas",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2022-12-21T11:38:51Z",
        "summary": "When applied to processing long text, Large Language Models (LLMs) are\nlimited by their context window. Existing efforts to address this limitation\ninvolve training specialized architectures, and cannot be easily applied to\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\nalleviates the context window restriction for any off-the-shelf LLM without\nfurther training. The key to the approach is to carve a long context into\nchunks (``windows''), restrict the attention mechanism to apply only within\neach window, and re-use the positional embeddings across the windows. Our main\nresults test the PCW approach on in-context learning with models that range in\nsize between 750 million and 178 billion parameters, and show substantial\nimprovements for tasks with diverse input and output spaces. We show additional\nbenefits in other settings where long context windows may be beneficial:\nmulti-hop questions and retrieval-augmented question answering with multiple\nretrieved documents. Our results highlight Parallel Context Windows as a\npromising method for applying off-the-shelf LLMs in a range of settings that\nrequire long text sequences. We make our code publicly available at\nhttps://github.com/ai21labs/parallel-context-windows.",
        "pdf_link": "https://arxiv.org/pdf/2212.10947v3.pdf"
    },
    {
        "title": "Critic-Guided Decoding for Controlled Text Generation",
        "authors": [
            "Minbeom Kim",
            "Hwanhee Lee",
            "Kang Min Yoo",
            "Joonsuk Park",
            "Hwaran Lee",
            "Kyomin Jung"
        ],
        "published": "2022-12-21T11:25:41Z",
        "summary": "Steering language generation towards objectives or away from undesired\ncontent has been a long-standing goal in utilizing language models (LM). Recent\nwork has demonstrated reinforcement learning and weighted decoding as effective\napproaches to achieve a higher level of language control and quality with pros\nand cons. In this work, we propose a novel critic decoding method for\ncontrolled language generation (CriticControl) that combines the strengths of\nreinforcement learning and weighted decoding. Specifically, we adopt the\nactor-critic framework to train an LM-steering critic from non-differentiable\nreward models. And similar to weighted decoding, our method freezes the\nlanguage model and manipulates the output token distribution using called\ncritic, improving training efficiency and stability. Evaluation of our method\non three controlled generation tasks, namely topic control, sentiment control,\nand detoxification, shows that our approach generates more coherent and\nwell-controlled texts than previous methods. In addition, CriticControl\ndemonstrates superior generalization ability in zero-shot settings. Human\nevaluation studies also corroborate our findings.",
        "pdf_link": "https://arxiv.org/pdf/2212.10938v1.pdf"
    },
    {
        "title": "SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning",
        "authors": [
            "M Saiful Bari",
            "Aston Zhang",
            "Shuai Zheng",
            "Xingjian Shi",
            "Yi Zhu",
            "Shafiq Joty",
            "Mu Li"
        ],
        "published": "2022-12-21T11:18:09Z",
        "summary": "Pre-trained large language models can efficiently interpolate human-written\nprompts in a natural way. Multitask prompted learning can help generalization\nthrough a diverse set of tasks at once, thus enhancing the potential for more\neffective downstream fine-tuning. To perform efficient multitask-inference in\nthe same batch, parameter-efficient fine-tuning methods such as prompt tuning\nhave been proposed. However, the existing prompt tuning methods may lack\ngeneralization. We propose SPT, a semi-parametric prompt tuning method for\nmultitask prompted learning. The novel component of SPT is a memory bank from\nwhere memory prompts are retrieved based on discrete prompts. Extensive\nexperiments, such as (i) fine-tuning a full language model with SPT on 31\ndifferent tasks from 8 different domains and evaluating zero-shot\ngeneralization on 9 heldout datasets under 5 NLP task categories and (ii)\npretraining SPT on the GLUE datasets and evaluating fine-tuning on the\nSuperGLUE datasets, demonstrate effectiveness of SPT.",
        "pdf_link": "https://arxiv.org/pdf/2212.10929v1.pdf"
    },
    {
        "title": "Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?",
        "authors": [
            "Ningyu Xu",
            "Tao Gui",
            "Ruotian Ma",
            "Qi Zhang",
            "Jingting Ye",
            "Menghan Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022-12-21T09:44:08Z",
        "summary": "Multilingual BERT (mBERT) has demonstrated considerable cross-lingual\nsyntactic ability, whereby it enables effective zero-shot cross-lingual\ntransfer of syntactic knowledge. The transfer is more successful between some\nlanguages, but it is not well understood what leads to this variation and\nwhether it fairly reflects difference between languages. In this work, we\ninvestigate the distributions of grammatical relations induced from mBERT in\nthe context of 24 typologically different languages. We demonstrate that the\ndistance between the distributions of different languages is highly consistent\nwith the syntactic difference in terms of linguistic formalisms. Such\ndifference learnt via self-supervision plays a crucial role in the zero-shot\ntransfer performance and can be predicted by variation in morphosyntactic\nproperties between languages. These results suggest that mBERT properly encodes\nlanguages in a way consistent with linguistic diversity and provide insights\ninto the mechanism of cross-lingual transfer.",
        "pdf_link": "https://arxiv.org/pdf/2212.10879v1.pdf"
    },
    {
        "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners",
        "authors": [
            "Hyunsoo Cho",
            "Hyuhng Joon Kim",
            "Junyeob Kim",
            "Sang-Woo Lee",
            "Sang-goo Lee",
            "Kang Min Yoo",
            "Taeuk Kim"
        ],
        "published": "2022-12-21T09:37:05Z",
        "summary": "Through in-context learning (ICL), large-scale language models are effective\nfew-shot learners without additional model fine-tuning. However, the ICL\nperformance does not scale well with the number of available training samples\nas it is limited by the inherent input length constraint of the underlying\nlanguage model. Meanwhile, many studies have revealed that language models are\nalso powerful feature extractors, allowing them to be utilized in a black-box\nmanner and enabling the linear probing paradigm, where lightweight\ndiscriminators are trained on top of the pre-extracted input representations.\nThis paper proposes prompt-augmented linear probing (PALP), a hybrid of linear\nprobing and ICL, which leverages the best of both worlds. PALP inherits the\nscalability of linear probing and the capability of enforcing language models\nto derive more meaningful representations via tailoring input into a more\nconceivable form. Throughout in-depth investigations on various datasets, we\nverified that PALP significantly enhances the input representations closing the\ngap between ICL in the data-hungry scenario and fine-tuning in the\ndata-abundant scenario with little training overhead, potentially making PALP a\nstrong alternative in a black-box scenario.",
        "pdf_link": "https://arxiv.org/pdf/2212.10873v3.pdf"
    },
    {
        "title": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",
        "authors": [
            "Jiaxian Guo",
            "Junnan Li",
            "Dongxu Li",
            "Anthony Meng Huat Tiong",
            "Boyang Li",
            "Dacheng Tao",
            "Steven C. H. Hoi"
        ],
        "published": "2022-12-21T08:39:36Z",
        "summary": "Large language models (LLMs) have demonstrated excellent zero-shot\ngeneralization to new language tasks. However, effective utilization of LLMs\nfor zero-shot visual question-answering (VQA) remains challenging, primarily\ndue to the modality disconnection and task disconnection between LLM and VQA\ntask. End-to-end training on vision and language data may bridge the\ndisconnections, but is inflexible and computationally expensive. To address\nthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that provides\nthe prompts that can bridge the aforementioned modality and task\ndisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end\ntraining. In order to provide such prompts, we further employ LLM-agnostic\nmodels to provide prompts that can describe image content and self-constructed\nquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQA\ntasks. Img2Prompt offers the following benefits: 1) It can flexibly work with\nvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, it\nsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It\nachieves comparable or better performance than methods relying on end-to-end\ntraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by\n5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms\nfew-shot methods by as much as 20\\%.",
        "pdf_link": "https://arxiv.org/pdf/2212.10846v3.pdf"
    },
    {
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Jason Wolfe",
            "Subhro Roy"
        ],
        "published": "2022-12-21T07:06:55Z",
        "summary": "We explore the use of large language models (LLMs) for zero-shot semantic\nparsing. Semantic parsing involves mapping natural language utterances to\ntask-specific meaning representations. Language models are generally trained on\nthe publicly available text and code and cannot be expected to directly\ngeneralize to domain-specific parsing tasks in a zero-shot setting. In this\nwork, we propose ZEROTOP, a zero-shot task-oriented parsing method that\ndecomposes a semantic parsing problem into a set of abstractive and extractive\nquestion-answering (QA) problems, enabling us to leverage the ability of LLMs\nto zero-shot answer reading comprehension questions. For each utterance, we\nprompt the LLM with questions corresponding to its top-level intent and a set\nof slots and use the LLM generations to construct the target meaning\nrepresentation. We observe that current LLMs fail to detect unanswerable\nquestions; and as a result, cannot handle questions corresponding to missing\nslots. To address this problem, we fine-tune a language model on public QA\ndatasets using synthetic negative samples. Experimental results show that our\nQA-based decomposition paired with the fine-tuned LLM can correctly parse ~16%\nof utterances in the MTOP dataset without requiring any annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2212.10815v1.pdf"
    },
    {
        "title": "KL Regularized Normalization Framework for Low Resource Tasks",
        "authors": [
            "Neeraj Kumar",
            "Ankur Narang",
            "Brejesh Lall"
        ],
        "published": "2022-12-21T05:59:25Z",
        "summary": "Large pre-trained models, such as Bert, GPT, and Wav2Vec, have demonstrated\ngreat potential for learning representations that are transferable to a wide\nvariety of downstream tasks . It is difficult to obtain a large quantity of\nsupervised data due to the limited availability of resources and time. In light\nof this, a significant amount of research has been conducted in the area of\nadopting large pre-trained datasets for diverse downstream tasks via fine\ntuning, linear probing, or prompt tuning in low resource settings.\nNormalization techniques are essential for accelerating training and improving\nthe generalization of deep neural networks and have been successfully used in a\nwide variety of applications. A lot of normalization techniques have been\nproposed but the success of normalization in low resource downstream NLP and\nspeech tasks is limited. One of the reasons is the inability to capture\nexpressiveness by rescaling parameters of normalization. We propose\nKullbackLeibler(KL) Regularized normalization (KL-Norm) which make the\nnormalized data well behaved and helps in better generalization as it reduces\nover-fitting, generalises well on out of domain distributions and removes\nirrelevant biases and features with negligible increase in model parameters and\nmemory overheads. Detailed experimental evaluation on multiple low resource NLP\nand speech tasks, demonstrates the superior performance of KL-Norm as compared\nto other popular normalization and regularization techniques.",
        "pdf_link": "https://arxiv.org/pdf/2212.11275v1.pdf"
    },
    {
        "title": "SERENGETI: Massively Multilingual Language Models for Africa",
        "authors": [
            "Ife Adebara",
            "AbdelRahim Elmadany",
            "Muhammad Abdul-Mageed",
            "Alcides Alcoba Inciarte"
        ],
        "published": "2022-12-21T05:54:14Z",
        "summary": "Multilingual pretrained language models (mPLMs) acquire valuable,\ngeneralizable linguistic information during pretraining and have advanced the\nstate of the art on task-specific finetuning. To date, only ~31 out of ~2,000\nAfrican languages are covered in existing language models. We ameliorate this\nlimitation by developing SERENGETI, a massively multilingual language model\nthat covers 517 African languages and language varieties. We evaluate our novel\nmodels on eight natural language understanding tasks across 20 datasets,\ncomparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms\nother models on 11 datasets across the eights tasks, achieving 82.27 average\nF_1. We also perform analyses of errors from our models, which allows us to\ninvestigate the influence of language genealogy and linguistic similarity when\nthe models are applied under zero-shot settings. We will publicly release our\nmodels for\nresearch.\\footnote{\\href{https://github.com/UBC-NLP/serengeti}{https://github.com/UBC-NLP/serengeti}}",
        "pdf_link": "https://arxiv.org/pdf/2212.10785v2.pdf"
    },
    {
        "title": "Towards Efficient Visual Simplification of Computational Graphs in Deep Neural Networks",
        "authors": [
            "Rusheng Pan",
            "Zhiyong Wang",
            "Yating Wei",
            "Han Gao",
            "Gongchang Ou",
            "Caleb Chen Cao",
            "Jingli Xu",
            "Tong Xu",
            "Wei Chen"
        ],
        "published": "2022-12-21T05:17:13Z",
        "summary": "A computational graph in a deep neural network (DNN) denotes a specific data\nflow diagram (DFD) composed of many tensors and operators. Existing toolkits\nfor visualizing computational graphs are not applicable when the structure is\nhighly complicated and large-scale (e.g., BERT [1]). To address this problem,\nwe propose leveraging a suite of visual simplification techniques, including a\ncycle-removing method, a module-based edge-pruning algorithm, and an isomorphic\nsubgraph stacking strategy. We design and implement an interactive\nvisualization system that is suitable for computational graphs with up to 10\nthousand elements. Experimental results and usage scenarios demonstrate that\nour tool reduces 60% elements on average and hence enhances the performance for\nrecognizing and diagnosing DNN models. Our contributions are integrated into an\nopen-source DNN visualization toolkit, namely, MindInsight [2].",
        "pdf_link": "https://arxiv.org/pdf/2212.10774v1.pdf"
    },
    {
        "title": "ImPaKT: A Dataset for Open-Schema Knowledge Base Construction",
        "authors": [
            "Luke Vilnis",
            "Zach Fisher",
            "Bhargav Kanagal",
            "Patrick Murray",
            "Sumit Sanghai"
        ],
        "published": "2022-12-21T05:02:49Z",
        "summary": "Large language models have ushered in a golden age of semantic parsing. The\nseq2seq paradigm allows for open-schema and abstractive attribute and relation\nextraction given only small amounts of finetuning data. Language model\npretraining has simultaneously enabled great strides in natural language\ninference, reasoning about entailment and implication in free text. These\nadvances motivate us to construct ImPaKT, a dataset for open-schema information\nextraction, consisting of around 2500 text snippets from the C4 corpus, in the\nshopping domain (product buying guides), professionally annotated with\nextracted attributes, types, attribute summaries (attribute schema discovery\nfrom idiosyncratic text), many-to-one relations between compound and atomic\nattributes, and implication relations. We release this data in hope that it\nwill be useful in fine tuning semantic parsers for information extraction and\nknowledge base construction across a variety of domains. We evaluate the power\nof this approach by fine-tuning the open source UL2 language model on a subset\nof the dataset, extracting a set of implication relations from a corpus of\nproduct buying guides, and conducting human evaluations of the resulting\npredictions.",
        "pdf_link": "https://arxiv.org/pdf/2212.10770v1.pdf"
    },
    {
        "title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",
        "authors": [
            "Lingjun Zhao",
            "Khanh Nguyen",
            "Hal Daum\u00e9 III"
        ],
        "published": "2022-12-21T04:43:19Z",
        "summary": "Recent work studies the cognitive capabilities of language models through\npsychological tests designed for humans. While these studies are helpful for\nunderstanding the general capabilities of these models, there is no guarantee\nthat a model possessing sufficient capabilities to pass those tests would\nactually use those capabilities in performing real-life tasks. In this work, we\nformulate task-oriented cognitive capabilities, which are human-like cognitive\ncapabilities that language models leverage to perform tasks. These capabilities\nare (i) the ability to quickly generate good candidate utterances (the search\ncapability) (ii) the ability to predict how a listener interprets those\nutterances and choose the most appropriate one (the pragmatic capability). We\ndesign an evaluation scheme for comparing these capabilities of a language\nmodel with those of a human. Applying this scheme to examine various models in\na navigation instruction generation problem, we find that their pragmatic\ncapability is severely lacking. This insight leads us to augment them with\nbetter models of the listener and obtain a significant boost of 11% in success\nrate in guiding real humans. Our work advocates for having a principled\nprocedure for aligning language models with humans that involves (i)\nformulating task-oriented capabilities, (ii) devising a method to quantify\ntheir deficiency, and (iii) iteratively improving them.",
        "pdf_link": "https://arxiv.org/pdf/2301.05149v2.pdf"
    },
    {
        "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
        "authors": [
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed",
            "AbdelRahim Elmadany",
            "Alcides Alcoba Inciarte",
            "Md Tawkat Islam Khondaker"
        ],
        "published": "2022-12-21T04:21:46Z",
        "summary": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric,\nleaving serious gaps in our understanding of the whole class of autoregressive\nmodels. For example, we have little knowledge about the potential of these\nmodels and their societal impacts in diverse linguistic and cultural settings.\nWe alleviate this issue for Arabic, a wide collection of languages and\ndialectal varieties with more than 400 million population, by introducing\nJASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer\nlanguage models ranging in size between 300 million-6.7 billion parameters\npretrained on a large and diverse dataset (~ 235 GB of text). We also carefully\ndesign and release a comprehensive benchmark for both automated and human\nevaluation of Arabic autoregressive models, with coverage of potential social\nbiases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE\nextensively showing powerful performance intrinsically as well as in few-shot\nlearning on a wide range of NLP tasks. We aim to responsibly release our models\nand evaluation benchmark with interested researchers, along with code for\nexperimenting with them.",
        "pdf_link": "https://arxiv.org/pdf/2212.10755v2.pdf"
    },
    {
        "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
        "authors": [
            "Yijiang River Dong",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2022-12-21T04:21:35Z",
        "summary": "Story generation and understanding -- as with all NLG/NLU tasks -- has seen a\nsurge in neurosymbolic work. Researchers have recognized that, while large\nlanguage models (LLMs) have tremendous utility, they can be augmented with\nsymbolic means to be even better and to make up for any flaws that the neural\nnetworks might have. However, symbolic methods are extremely costly in terms of\nthe amount of time and expertise needed to create them. In this work, we\ncapitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use\nof symbolic methods for tracking the state of stories and aiding in story\nunderstanding. We show that our CoRRPUS system and abstracted prompting\nprocedures can beat current state-of-the-art structured LLM techniques on\npre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand\nengineering. We hope that this work can help highlight the importance of\nsymbolic representations and specialized prompting for LLMs as these models\nrequire some guidance for performing reasoning tasks properly.",
        "pdf_link": "https://arxiv.org/pdf/2212.10754v3.pdf"
    },
    {
        "title": "Spoken Language Understanding for Conversational AI: Recent Advances and Future Direction",
        "authors": [
            "Soyeon Caren Han",
            "Siqu Long",
            "Henry Weld",
            "Josiah Poon"
        ],
        "published": "2022-12-21T02:47:52Z",
        "summary": "When a human communicates with a machine using natural language on the web\nand online, how can it understand the human's intention and semantic context of\ntheir talk? This is an important AI task as it enables the machine to construct\na sensible answer or perform a useful action for the human. Meaning is\nrepresented at the sentence level, identification of which is known as intent\ndetection, and at the word level, a labelling task called slot filling. This\ndual-level joint task requires innovative thinking about natural language and\ndeep learning network design, and as a result, many approaches and models have\nbeen proposed and applied.\n  This tutorial will discuss how the joint task is set up and introduce Spoken\nLanguage Understanding/Natural Language Understanding (SLU/NLU) with Deep\nLearning techniques. We will cover the datasets, experiments and metrics used\nin the field. We will describe how the machine uses the latest NLP and Deep\nLearning techniques to address the joint task, including recurrent and\nattention-based Transformer networks and pre-trained models (e.g. BERT). We\nwill then look in detail at a network that allows the two levels of the task,\nintent classification and slot filling, to interact to boost performance\nexplicitly. We will do a code demonstration of a Python notebook for this model\nand attendees will have an opportunity to watch coding demo tasks on this joint\nNLU to further their understanding.",
        "pdf_link": "https://arxiv.org/pdf/2212.10728v1.pdf"
    },
    {
        "title": "Zero-shot Triplet Extraction by Template Infilling",
        "authors": [
            "Bosung Kim",
            "Hayate Iso",
            "Nikita Bhutani",
            "Estevam Hruschka",
            "Ndapa Nakashole",
            "Tom Mitchell"
        ],
        "published": "2022-12-21T00:57:24Z",
        "summary": "The task of triplet extraction aims to extract pairs of entities and their\ncorresponding relations from unstructured text. Most existing methods train an\nextraction model on training data involving specific target relations, and are\nincapable of extracting new relations that were not observed at training time.\nGeneralizing the model to unseen relations typically requires fine-tuning on\nsynthetic training data which is often noisy and unreliable. We show that by\nreducing triplet extraction to a template infilling task over a pre-trained\nlanguage model (LM), we can equip the extraction model with zero-shot learning\ncapabilities and eliminate the need for additional training data. We propose a\nnovel framework, ZETT (ZEro-shot Triplet extraction by Template infilling),\nthat aligns the task objective to the pre-training objective of generative\ntransformers to generalize to unseen relations. Experiments on FewRel and\nWiki-ZSL datasets demonstrate that ZETT shows consistent and stable\nperformance, outperforming previous state-of-the-art methods, even when using\nautomatically generated templates. https://github.com/megagonlabs/zett/",
        "pdf_link": "https://arxiv.org/pdf/2212.10708v2.pdf"
    },
    {
        "title": "Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering",
        "authors": [
            "Akshay Chaturvedi",
            "Swarnadeep Bhar",
            "Soumadeep Saha",
            "Utpal Garain",
            "Nicholas Asher"
        ],
        "published": "2022-12-21T00:00:01Z",
        "summary": "Transformer-based language models have been shown to be highly effective for\nseveral NLP tasks. In this paper, we consider three transformer models, BERT,\nRoBERTa, and XLNet, in both small and large versions, and investigate how\nfaithful their representations are with respect to the semantic content of\ntexts. We formalize a notion of semantic faithfulness, in which the semantic\ncontent of a text should causally figure in a model's inferences in question\nanswering. We then test this notion by observing a model's behavior on\nanswering questions about a story after performing two novel semantic\ninterventions: deletion intervention and negation intervention. While\ntransformer models achieve high performance on standard question answering\ntasks, we show that they fail to be semantically faithful once we perform these\ninterventions for a significant number of cases (~50% for deletion\nintervention, and ~20% drop in accuracy for negation intervention). We then\npropose an intervention-based training regime that can mitigate the undesirable\neffects for deletion intervention by a significant margin (from ~ 50% to ~6%).\nWe analyze the inner-workings of the models to better understand the\neffectiveness of intervention-based training for deletion intervention. But we\nshow that this training does not attenuate other aspects of semantic\nunfaithfulness such as the models' inability to deal with negation intervention\nor to capture the predicate-argument structure of texts. We also test\nInstructGPT, via prompting, for its ability to handle the two interventions and\nto capture predicate-argument structure. While InstructGPT models do achieve\nvery high performance on predicate-argument structure task, they fail to\nrespond adequately to our deletion and negation interventions.",
        "pdf_link": "https://arxiv.org/pdf/2212.10696v2.pdf"
    },
    {
        "title": "Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing",
        "authors": [
            "Justus Mattern",
            "Zhijing Jin",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Bernhard Sch\u00f6lkopf"
        ],
        "published": "2022-12-20T22:41:24Z",
        "summary": "Generated texts from large pretrained language models have been shown to\nexhibit a variety of harmful, human-like biases about various demographics.\nThese findings prompted large efforts aiming to understand and measure such\neffects, with the goal of providing benchmarks that can guide the development\nof techniques mitigating these stereotypical associations. However, as recent\nresearch has pointed out, the current benchmarks lack a robust experimental\nsetup, consequently hindering the inference of meaningful conclusions from\ntheir evaluation metrics. In this paper, we extend these arguments and\ndemonstrate that existing techniques and benchmarks aiming to measure\nstereotypes tend to be inaccurate and consist of a high degree of experimental\nnoise that severely limits the knowledge we can gain from benchmarking language\nmodels based on them. Accordingly, we propose a new framework for robustly\nmeasuring and quantifying biases exhibited by generative language models.\nFinally, we use this framework to investigate GPT-3's occupational gender bias\nand propose prompting techniques for mitigating these biases without the need\nfor fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2212.10678v1.pdf"
    },
    {
        "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter",
        "authors": [
            "Ali Edalati",
            "Marzieh Tahaei",
            "Ivan Kobyzev",
            "Vahid Partovi Nia",
            "James J. Clark",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2022-12-20T20:56:52Z",
        "summary": "Fine-tuning a Pre-trained Language Model (PLM) on a specific downstream task\nhas been a well-known paradigm in Natural Language Processing. However, with\nthe ever-growing size of PLMs, training the entire model on several downstream\ntasks becomes very expensive and resource-hungry. Recently, different Parameter\nEfficient Tuning (PET) techniques are proposed to improve the efficiency of\nfine-tuning PLMs. One popular category of PET methods is the low-rank\nadaptation methods which insert learnable truncated SVD modules into the\noriginal model either sequentially or in parallel. However, low-rank\ndecomposition suffers from limited representation power. In this work, we\naddress this problem using the Kronecker product instead of the low-rank\nrepresentation. We introduce KronA, a Kronecker product-based adapter module\nfor efficient fine-tuning of Transformer-based PLMs. We apply the proposed\nmethods for fine-tuning T5 on the GLUE benchmark to show that incorporating the\nKronecker-based modules can outperform state-of-the-art PET methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.10650v1.pdf"
    },
    {
        "title": "A Vision-free Baseline for Multimodal Grammar Induction",
        "authors": [
            "Boyi Li",
            "Rodolfo Corona",
            "Karttikeya Mangalam",
            "Catherine Chen",
            "Daniel Flaherty",
            "Serge Belongie",
            "Kilian Q. Weinberger",
            "Jitendra Malik",
            "Trevor Darrell",
            "Dan Klein"
        ],
        "published": "2022-12-20T18:59:50Z",
        "summary": "Past work has shown that paired vision-language signals substantially improve\ngrammar induction in multimodal datasets such as MSCOCO. We investigate whether\nadvancements in large language models (LLMs) that are only trained with text\ncould provide strong assistance for grammar induction in multimodal settings.\nWe find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms\nprevious multi-modal methods, and achieves state-of-the-art grammar induction\nperformance for various multimodal datasets. Compared to image-aided grammar\ninduction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1\npoints, with an 85% reduction in parameter count and 1.7x faster training\nspeed. Across three video-assisted grammar induction benchmarks, LC-PCFG\noutperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster\ntraining. These results shed light on the notion that text-only language models\nmight include visually grounded cues that aid in grammar induction in\nmultimodal contexts. Moreover, our results emphasize the importance of\nestablishing a robust vision-free baseline when evaluating the benefit of\nmultimodal approaches.",
        "pdf_link": "https://arxiv.org/pdf/2212.10564v2.pdf"
    },
    {
        "title": "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
        "authors": [
            "Eric Zelikman",
            "Qian Huang",
            "Gabriel Poesia",
            "Noah D. Goodman",
            "Nick Haber"
        ],
        "published": "2022-12-20T18:59:23Z",
        "summary": "Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs.\nWith Parsel, we automatically decompose algorithmic tasks into hierarchical\nnatural language function descriptions and then search over combinations of\npossible function implementations using tests. We show that Parsel can be used\nacross domains requiring hierarchical reasoning, including program synthesis\nand robotic planning. We find that, using Parsel, LLMs solve more\ncompetition-level problems in the APPS dataset, resulting in pass rates over\n75\\% higher than prior results from directly sampling AlphaCode and Codex,\nwhile often using a smaller sample budget. Moreover, with automatically\ngenerated tests, we find that Parsel can improve the state-of-the-art pass@1\nperformance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated\nrobotic plans using Parsel are more than twice as likely to be considered\naccurate than directly generated plans. Lastly, we explore how Parsel addresses\nLLM limitations and discuss how Parsel may be useful for human programmers. We\nrelease our code at https://github.com/ezelikman/parsel",
        "pdf_link": "https://arxiv.org/pdf/2212.10561v3.pdf"
    },
    {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "authors": [
            "Yizhong Wang",
            "Yeganeh Kordi",
            "Swaroop Mishra",
            "Alisa Liu",
            "Noah A. Smith",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-20T18:59:19Z",
        "summary": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.",
        "pdf_link": "https://arxiv.org/pdf/2212.10560v2.pdf"
    },
    {
        "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
        "authors": [
            "Damai Dai",
            "Yutao Sun",
            "Li Dong",
            "Yaru Hao",
            "Shuming Ma",
            "Zhifang Sui",
            "Furu Wei"
        ],
        "published": "2022-12-20T18:58:48Z",
        "summary": "Large pretrained language models have shown surprising in-context learning\n(ICL) ability. With a few demonstration input-label pairs, they can predict the\nlabel for an unseen input without parameter updates. Despite the great success\nin performance, its working mechanism still remains an open question. In this\npaper, we explain language models as meta-optimizers and understand in-context\nlearning as implicit finetuning. Theoretically, we figure out that Transformer\nattention has a dual form of gradient descent. On top of it, we understand ICL\nas follows: GPT first produces meta-gradients according to the demonstration\nexamples, and then these meta-gradients are applied to the original GPT to\nbuild an ICL model. We comprehensively compare the behaviors of in-context\nlearning and explicit finetuning on real tasks to provide empirical evidence\nthat supports our understanding. Experimental results show that in-context\nlearning behaves similarly to explicit finetuning from multiple perspectives.\nInspired by the dual form between Transformer attention and gradient descent,\nwe design a momentum-based attention by analogy with gradient descent with\nmomentum. The improved performance over vanilla attention further supports our\nunderstanding from another perspective, and more importantly, shows the\npotential to utilize our understanding for future model design. The code is\navailable at \\url{https://aka.ms/icl}.",
        "pdf_link": "https://arxiv.org/pdf/2212.10559v3.pdf"
    },
    {
        "title": "PairReranker: Pairwise Reranking for Natural Language Generation",
        "authors": [
            "Dongfu Jiang",
            "Bill Yuchen Lin",
            "Xiang Ren"
        ],
        "published": "2022-12-20T18:56:57Z",
        "summary": "Pre-trained language models have been successful in natural language\ngeneration (NLG) tasks. While various decoding methods have been employed, they\noften produce suboptimal results. We first present an empirical analysis of\nthree NLG tasks: summarization, machine translation, and constrained text\ngeneration. We found that selecting the best output from the results of\nmultiple decoding methods can significantly improve performance. To further\nimprove reranking for NLG tasks, we proposed a novel method,\n\\textsc{PairReranker}, which uses a single encoder and a pairwise loss function\nto jointly encode a source input and a pair of candidates and compare them.\nExperiments on three NLG tasks demonstrated the effectiveness and flexibility\nof \\textsc{PairReranker}, showing strong results, compared with previous\nbaselines. In addition, our \\textsc{PairReranker} can generalize to\nsignificantly improve GPT-3 (text-davinci-003) results (e.g., 24.55\\% on\nCommonGen and 11.35\\% on WMT18 zh-en), even though our rerankers are not\ntrained with any GPT-3 candidates.",
        "pdf_link": "https://arxiv.org/pdf/2212.10555v1.pdf"
    },
    {
        "title": "Pretraining Without Attention",
        "authors": [
            "Junxiong Wang",
            "Jing Nathan Yan",
            "Albert Gu",
            "Alexander M. Rush"
        ],
        "published": "2022-12-20T18:50:08Z",
        "summary": "Transformers have been essential to pretraining success in NLP. While other\narchitectures have been used, downstream accuracy is either significantly\nworse, or requires attention layers to match standard benchmarks such as GLUE.\nThis work explores pretraining without attention by using recent advances in\nsequence routing based on state-space models (SSMs). Our proposed model,\nBidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative\ngating architecture that has been effective in simplified sequence modeling\narchitectures. The model learns static layers that do not consider pair-wise\ninteractions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE\nand can be extended to long-form pretraining of 4096 tokens without\napproximation. Analysis shows that while the models have similar average\naccuracy, the approach has different inductive biases than BERT in terms of\ninteractions and syntactic representations. All models from this work are\navailable at https://github.com/jxiw/BiGS.",
        "pdf_link": "https://arxiv.org/pdf/2212.10544v2.pdf"
    },
    {
        "title": "Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?",
        "authors": [
            "Weijia Shi",
            "Xiaochuang Han",
            "Hila Gonen",
            "Ari Holtzman",
            "Yulia Tsvetkov",
            "Luke Zettlemoyer"
        ],
        "published": "2022-12-20T18:47:13Z",
        "summary": "Large language models can perform new tasks in a zero-shot fashion, given\nnatural language prompts that specify the desired behavior. Such prompts are\ntypically hand engineered, but can also be learned with gradient-based methods\nfrom labeled data. However, it is underexplored what factors make the prompts\neffective, especially when the prompts are natural language. In this paper, we\ninvestigate common attributes shared by effective prompts. We first propose a\nhuman readable prompt tuning method (F LUENT P ROMPT) based on Langevin\ndynamics that incorporates a fluency constraint to find a diverse distribution\nof effective and fluent prompts. Our analysis reveals that effective prompts\nare topically related to the task domain and calibrate the prior probability of\nlabel words. Based on these findings, we also propose a method for generating\nprompts using only unlabeled data, outperforming strong baselines by an average\nof 7.0% accuracy across three tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.10539v1.pdf"
    },
    {
        "title": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models",
        "authors": [
            "Martha Lewis",
            "Nihal V. Nayak",
            "Peilin Yu",
            "Qinan Yu",
            "Jack Merullo",
            "Stephen H. Bach",
            "Ellie Pavlick"
        ],
        "published": "2022-12-20T18:46:28Z",
        "summary": "Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying ''red cube'' by\nreasoning over the constituents ''red'' and ''cube''. In this work, we focus on\nthe ability of a large pretrained vision and language model (CLIP) to encode\ncompositional concepts and to bind variables in a structure-sensitive way\n(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In\norder to inspect the performance of CLIP, we compare several architectures from\nresearch on compositional distributional semantics models (CDSMs), a line of\nresearch that attempts to implement traditional compositional linguistic\nstructures within embedding spaces. We find that CLIP can compose concepts in a\nsingle-object setting, but in situations where concept binding is needed,\nperformance drops dramatically. At the same time, CDSMs also perform poorly,\nwith best performance at chance level.",
        "pdf_link": "https://arxiv.org/pdf/2212.10537v2.pdf"
    },
    {
        "title": "DISCO: Distilling Counterfactuals with Large Language Models",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao",
            "Antoine Bosselut",
            "Ashish Sabharwal",
            "Kyle Richardson"
        ],
        "published": "2022-12-20T18:46:08Z",
        "summary": "Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco",
        "pdf_link": "https://arxiv.org/pdf/2212.10534v3.pdf"
    },
    {
        "title": "Evaluating Psychological Safety of Large Language Models",
        "authors": [
            "Xingxuan Li",
            "Yutong Li",
            "Lin Qiu",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2022-12-20T18:45:07Z",
        "summary": "In this work, we designed unbiased prompts to systematically evaluate the\npsychological safety of large language models (LLMs). First, we tested five\ndifferent LLMs by using two personality tests: Short Dark Triad (SD-3) and Big\nFive Inventory (BFI). All models scored higher than the human average on SD-3,\nsuggesting a relatively darker personality pattern. Despite being instruction\nfine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and\nGPT-4 still showed dark personality patterns; these models scored higher than\nself-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.\nThen, we evaluated the LLMs in the GPT series by using well-being tests to\nstudy the impact of fine-tuning with more training data. We observed a\ncontinuous increase in the well-being scores of GPT models. Following these\nobservations, we showed that fine-tuning Llama-2-chat-7B with responses from\nBFI using direct preference optimization could effectively reduce the\npsychological toxicity of the model. Based on the findings, we recommended the\napplication of systematic and comprehensive psychological metrics to further\nevaluate and improve the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2212.10529v3.pdf"
    },
    {
        "title": "Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End",
        "authors": [
            "Yanran Chen",
            "Steffen Eger"
        ],
        "published": "2022-12-20T18:37:11Z",
        "summary": "We consider the end-to-end abstract-to-title generation problem, exploring\nseven recent transformer based models (including ChatGPT) fine-tuned on more\nthan 30k abstract-title pairs from NLP and machine learning (ML) venues. As an\nextension, we also consider the harder problem of generating humorous paper\ntitles. For the latter, we compile the first large-scale humor annotated\ndataset for scientific papers in the NLP/ML domains, comprising almost ~2.6k\ntitles. We evaluate all models using human and automatic metrics. Our human\nevaluation suggests that our best end-to-end system performs similarly to human\nauthors (but arguably slightly worse). Generating funny titles is more\ndifficult, however, and our automatic systems clearly underperform relative to\nhumans and often learn dataset artefacts of humor. Finally, ChatGPT, without\nany fine-tuning, performs on the level of our best fine-tuned system.",
        "pdf_link": "https://arxiv.org/pdf/2212.10522v2.pdf"
    },
    {
        "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
        "authors": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-20T18:30:15Z",
        "summary": "Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\nlimitations of relying solely on their parameters to encode a wealth of world\nknowledge. This paper aims to understand LMs' strengths and limitations in\nmemorizing factual knowledge, by conducting large-scale knowledge probing\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\npopular factual knowledge, and that scaling fails to appreciably improve\nmemorization of factual knowledge in the long tail. We then show that\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\nwhile unassisted LMs remain competitive in questions about high-popularity\nentities. Based on those findings, we devise a simple, yet effective, method\nfor powerful and efficient retrieval-augmented LMs, which retrieves\nnon-parametric memories only when necessary. Experimental results show that\nthis significantly improves models' performance while reducing the inference\ncosts.",
        "pdf_link": "https://arxiv.org/pdf/2212.10511v4.pdf"
    },
    {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "Harsh Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published": "2022-12-20T18:26:34Z",
        "summary": "Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural language reasoning steps or Chains-of-Thoughts (CoT) for\nmulti-step question answering (QA). They struggle, however, when the necessary\nknowledge is either unavailable to the LLM or not up-to-date within its\nparameters. While using the question to retrieve relevant text from an external\nknowledge source helps LLMs, we observe that this one-step retrieve-and-read\napproach is insufficient for multi-step QA. Here, \\textit{what to retrieve}\ndepends on \\textit{what has already been derived}, which in turn may depend on\n\\textit{what was previously retrieved}. To address this, we propose IRCoT, a\nnew approach for multi-step QA that interleaves retrieval with steps\n(sentences) in a CoT, guiding the retrieval with CoT and in turn using\nretrieved results to improve CoT. Using IRCoT with GPT3 substantially improves\nretrieval (up to 21 points) as well as downstream QA (up to 15 points) on four\ndatasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar\nsubstantial gains in out-of-distribution (OOD) settings as well as with much\nsmaller models such as Flan-T5-large without additional training. IRCoT reduces\nmodel hallucination, resulting in factually more accurate CoT reasoning. Code,\ndata, and prompts are available at \\url{https://github.com/stonybrooknlp/ircot}",
        "pdf_link": "https://arxiv.org/pdf/2212.10509v2.pdf"
    },
    {
        "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
        "authors": [
            "Fangyu Liu",
            "Julian Martin Eisenschlos",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Wenhu Chen",
            "Nigel Collier",
            "Yasemin Altun"
        ],
        "published": "2022-12-20T18:20:50Z",
        "summary": "Visual language such as charts and plots is ubiquitous in the human world.\nComprehending plots and charts requires strong reasoning skills. Prior\nstate-of-the-art (SOTA) models require at least tens of thousands of training\nexamples and their reasoning capabilities are still much limited, especially on\ncomplex human-written queries. This paper presents the first one-shot solution\nto visual language reasoning. We decompose the challenge of visual language\nreasoning into two steps: (1) plot-to-text translation, and (2) reasoning over\nthe translated text. The key in this method is a modality conversion module,\nnamed as DePlot, which translates the image of a plot or chart to a linearized\ntable. The output of DePlot can then be directly used to prompt a pretrained\nlarge language model (LLM), exploiting the few-shot reasoning capabilities of\nLLMs. To obtain DePlot, we standardize the plot-to-table task by establishing\nunified task formats and metrics, and train DePlot end-to-end on this task.\nDePlot can then be used off-the-shelf together with LLMs in a plug-and-play\nfashion. Compared with a SOTA model finetuned on more than >28k data points,\nDePlot+LLM with just one-shot prompting achieves a 24.0% improvement over\nfinetuned SOTA on human-written queries from the task of chart QA.",
        "pdf_link": "https://arxiv.org/pdf/2212.10505v2.pdf"
    },
    {
        "title": "Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?",
        "authors": [
            "Sang-Woo Lee",
            "Sungdong Kim",
            "Donghyeon Ko",
            "Donghoon Ham",
            "Youngki Hong",
            "Shin Ah Oh",
            "Hyunhoon Jung",
            "Wangkyo Jung",
            "Kyunghyun Cho",
            "Donghyun Kwak",
            "Hyungsuk Noh",
            "Woomyoung Park"
        ],
        "published": "2022-12-20T18:18:41Z",
        "summary": "Task-oriented dialogue (TOD) systems are mainly based on the\nslot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down\ninto smaller, controllable units (i.e., slots) to fulfill a specific task. A\nseries of approaches based on this framework achieved remarkable success on\nvarious TOD benchmarks. However, we argue that the current TOD benchmarks are\nlimited to surrogate real-world scenarios and that the current TOD models are\nstill a long way to cover the scenarios. In this position paper, we first\nidentify current status and limitations of SF-TOD systems. After that, we\nexplore the WebTOD framework, the alternative direction for building a scalable\nTOD system when a web/mobile interface is available. In WebTOD, the dialogue\nsystem learns how to understand the web/mobile interface that the human agent\ninteracts with, powered by a large-scale language model.",
        "pdf_link": "https://arxiv.org/pdf/2212.10504v2.pdf"
    },
    {
        "title": "A Measure-Theoretic Characterization of Tight Language Models",
        "authors": [
            "Li Du",
            "Lucas Torroba Hennigen",
            "Tiago Pimentel",
            "Clara Meister",
            "Jason Eisner",
            "Ryan Cotterell"
        ],
        "published": "2022-12-20T18:17:11Z",
        "summary": "Language modeling, a central task in natural language processing, involves\nestimating a probability distribution over strings. In most cases, the\nestimated distribution sums to 1 over all finite strings. However, in some\npathological cases, probability mass can ``leak'' onto the set of infinite\nsequences. In order to characterize the notion of leakage more precisely, this\npaper offers a measure-theoretic treatment of language modeling. We prove that\nmany popular language model families are in fact tight, meaning that they will\nnot leak in this sense. We also generalize characterizations of tightness\nproposed in previous works.",
        "pdf_link": "https://arxiv.org/pdf/2212.10502v2.pdf"
    },
    {
        "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels",
        "authors": [
            "Luyu Gao",
            "Xueguang Ma",
            "Jimmy Lin",
            "Jamie Callan"
        ],
        "published": "2022-12-20T18:09:52Z",
        "summary": "While dense retrieval has been shown effective and efficient across tasks and\nlanguages, it remains difficult to create effective fully zero-shot dense\nretrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\nquery, HyDE first zero-shot instructs an instruction-following language model\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\nrelevance patterns but is unreal and may contain false details. Then, an\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\ndocument into an embedding vector. This vector identifies a neighborhood in the\ncorpus embedding space, where similar real documents are retrieved based on\nvector similarity. This second step ground the generated document to the actual\ncorpus, with the encoder's dense bottleneck filtering out the incorrect\ndetails. Our experiments show that HyDE significantly outperforms the\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).",
        "pdf_link": "https://arxiv.org/pdf/2212.10496v1.pdf"
    },
    {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "authors": [
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022-12-20T17:49:49Z",
        "summary": "State-of-the-art poetry generation systems are often complex. They either\nconsist of task-specific model pipelines, incorporate prior knowledge in the\nform of manually created constraints, or both. In contrast, end-to-end models\nwould not suffer from the overhead of having to model prior knowledge and could\nlearn the nuances of poetry from data alone, reducing the degree of human\nsupervision required. In this work, we investigate end-to-end poetry generation\nconditioned on styles such as rhyme, meter, and alliteration. We identify and\naddress lack of training data and mismatching tokenization algorithms as\npossible limitations of past attempts. In particular, we successfully pre-train\nByGPT5, a new token-free decoder-only language model, and fine-tune it on a\nlarge custom corpus of English and German quatrains annotated with our styles.\nWe show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and\nChatGPT, while also being more parameter efficient and performing favorably\ncompared to humans. In addition, we analyze its runtime performance and\ndemonstrate that it is not prone to memorization. We make our code, models, and\ndatasets publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2212.10474v2.pdf"
    },
    {
        "title": "Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models",
        "authors": [
            "Evgeniia Razumovskaia",
            "Joshua Maynez",
            "Annie Louis",
            "Mirella Lapata",
            "Shashi Narayan"
        ],
        "published": "2022-12-20T17:42:16Z",
        "summary": "Previous work has demonstrated the effectiveness of planning for story\ngeneration exclusively in a monolingual setting focusing primarily on English.\nWe consider whether planning brings advantages to automatic story generation\nacross languages. We propose a new task of cross-lingual story generation with\nplanning and present a new dataset for this task. We conduct a comprehensive\nstudy of different plans and generate stories in several languages, by\nleveraging the creative and reasoning capabilities of large pre-trained\nlanguage models. Our results demonstrate that plans which structure stories\ninto three acts lead to more coherent and interesting narratives, while\nallowing to explicitly control their content and structure.",
        "pdf_link": "https://arxiv.org/pdf/2212.10471v3.pdf"
    },
    {
        "title": "Generic Temporal Reasoning with Differential Analysis and Explanation",
        "authors": [
            "Yu Feng",
            "Ben Zhou",
            "Haoyu Wang",
            "Helen Jin",
            "Dan Roth"
        ],
        "published": "2022-12-20T17:40:03Z",
        "summary": "Temporal reasoning is the task of predicting temporal relations of event\npairs. While temporal reasoning models can perform reasonably well on in-domain\nbenchmarks, we have little idea of these systems' generalizability due to\nexisting datasets' limitations. In this work, we introduce a novel task named\nTODAY that bridges this gap with temporal differential analysis, which as the\nname suggests, evaluates whether systems can correctly understand the effect of\nincremental changes. Specifically, TODAY introduces slight contextual changes\nfor given event pairs, and systems are asked to tell how this subtle contextual\nchange would affect relevant temporal relation distributions. To facilitate\nlearning, TODAY also annotates human explanations. We show that existing\nmodels, including GPT-3.5, drop to random guessing on TODAY, suggesting that\nthey heavily rely on spurious information rather than proper reasoning for\ntemporal predictions. On the other hand, we show that TODAY's supervision style\nand explanation annotations can be used in joint learning, encouraging models\nto use more appropriate signals during training and thus outperform across\nseveral benchmarks. TODAY can also be used to train models to solicit\nincidental supervision from noisy sources such as GPT-3.5, thus moving us more\ntoward the goal of generic temporal reasoning systems.",
        "pdf_link": "https://arxiv.org/pdf/2212.10467v2.pdf"
    },
    {
        "title": "Controllable Text Generation with Language Constraints",
        "authors": [
            "Howard Chen",
            "Huihan Li",
            "Danqi Chen",
            "Karthik Narasimhan"
        ],
        "published": "2022-12-20T17:39:21Z",
        "summary": "We consider the task of text generation in language models with constraints\nspecified in natural language. To this end, we first create a challenging\nbenchmark Cognac that provides as input to the model a topic with example text,\nalong with a constraint on text to be avoided. Unlike prior work, our benchmark\ncontains knowledge-intensive constraints sourced from databases like Wordnet\nand Wikidata, which allows for straightforward evaluation while striking a\nbalance between broad attribute-level and narrow lexical-level controls. We\nfind that even state-of-the-art language models like GPT-3 fail often on this\ntask, and propose a solution to leverage a language model's own internal\nknowledge to guide generation. Our method, called CognacGen, first queries the\nlanguage model to generate guidance terms for a specified topic or constraint,\nand uses the guidance to modify the model's token generation probabilities. We\npropose three forms of guidance (binary verifier, top-k tokens, textual\nexample), and employ prefix-tuning approaches to distill the guidance to tackle\ndiverse natural language constraints. Through extensive empirical evaluations,\nwe demonstrate that CognacGen can successfully generalize to unseen\ninstructions and outperform competitive baselines in generating constraint\nconforming text.",
        "pdf_link": "https://arxiv.org/pdf/2212.10466v1.pdf"
    },
    {
        "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
        "authors": [
            "Hyunwoo Kim",
            "Jack Hessel",
            "Liwei Jiang",
            "Peter West",
            "Ximing Lu",
            "Youngjae Yu",
            "Pei Zhou",
            "Ronan Le Bras",
            "Malihe Alikhani",
            "Gunhee Kim",
            "Maarten Sap",
            "Yejin Choi"
        ],
        "published": "2022-12-20T17:38:47Z",
        "summary": "Data scarcity has been a long standing issue in the field of open-domain\nsocial dialogue. To quench this thirst, we present SODA: the first publicly\navailable, million-scale high-quality social dialogue dataset. By\ncontextualizing social commonsense knowledge from a knowledge graph, we are\nable to distill an exceptionally broad spectrum of social interactions from a\nlarge language model. Human evaluation shows that conversations in SODA are\nmore consistent, specific, and (surprisingly) natural than those in prior\nhuman-authored datasets.\n  Using SODA, we train COSMO: a generalizable conversation model that is\nsignificantly more natural and consistent on unseen datasets than\nbest-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).\nExperiments reveal COSMO is sometimes even preferred to the original\nhuman-written gold responses. Additionally, our results shed light on the\ndistinction between knowledge-enriched conversations and natural social\nchitchats. We plan to make our data, model, and code public.",
        "pdf_link": "https://arxiv.org/pdf/2212.10465v3.pdf"
    },
    {
        "title": "Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models",
        "authors": [
            "Jingjing Xu",
            "Qingxiu Dong",
            "Hongyi Liu",
            "Lei Li"
        ],
        "published": "2022-12-20T17:36:49Z",
        "summary": "With increasing scale, large language models demonstrate both quantitative\nimprovement and new qualitative capabilities, especially as zero-shot learners,\nlike GPT-3. However, these results rely heavily on delicate prompt design and\nlarge computation. In this work, we explore whether the strong zero-shot\nability could be achieved at a smaller model scale without any external\nsupervised data. To achieve this goal, we revisit masked language modeling and\npresent a geometry-guided self-supervised learning method (Go-tuningfor short)\nby taking a small number of task-aware self-supervised data to update language\nmodels further. Experiments show that Go-tuning can enable T5-small (80M)\ncompetitive zero-shot results compared with large language models, such as\nT5-XL (3B). We also apply Go-tuning on multi-task settings and develop a\nmulti-task model, mgo-T5 (250M). It can reach the average performance of OPT\n(175B) on 9 datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.10461v1.pdf"
    },
    {
        "title": "Is GPT-3 a Good Data Annotator?",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Linlin Liu",
            "Yew Ken Chia",
            "Shafiq Joty",
            "Boyang Li",
            "Lidong Bing"
        ],
        "published": "2022-12-20T17:28:41Z",
        "summary": "Data annotation is the process of labeling data that could be used to train\nmachine learning models. Having high-quality annotation is crucial, as it\nallows the model to learn the relationship between the input data and the\ndesired output. GPT-3, a large-scale language model developed by OpenAI, has\ndemonstrated impressive zero- and few-shot performance on a wide range of NLP\ntasks. It is therefore natural to wonder whether it can be used to effectively\nannotate data for NLP tasks. In this paper, we evaluate the performance of\nGPT-3 as a data annotator by comparing it with traditional data annotation\nmethods and analyzing its output on a range of tasks. Through this analysis, we\naim to provide insight into the potential of GPT-3 as a general-purpose data\nannotator in NLP.",
        "pdf_link": "https://arxiv.org/pdf/2212.10450v2.pdf"
    },
    {
        "title": "Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters",
        "authors": [
            "Eugene Yang",
            "Suraj Nair",
            "Dawn Lawrie",
            "James Mayfield",
            "Douglas W. Oard"
        ],
        "published": "2022-12-20T17:25:04Z",
        "summary": "A popular approach to creating a zero-shot cross-language retrieval model is\nto substitute a monolingual pretrained language model in the retrieval model\nwith a multilingual pretrained language model such as Multilingual BERT. This\nmultilingual model is fined-tuned to the retrieval task with monolingual data\nsuch as English MS MARCO using the same training recipe as the monolingual\nretrieval model used. However, such transferred models suffer from mismatches\nin the languages of the input text during training and inference. In this work,\nwe propose transferring monolingual retrieval models using adapters, a\nparameter-efficient component for a transformer network. By adding adapters\npretrained on language tasks for a specific language with task-specific\nadapters, prior work has shown that the adapter-enhanced models perform better\nthan fine-tuning the entire model when transferring across languages in various\nNLP tasks. By constructing dense retrieval models with adapters, we show that\nmodels trained with monolingual data are more effective than fine-tuning the\nentire model when transferring to a Cross Language Information Retrieval (CLIR)\nsetting. However, we found that the prior suggestion of replacing the language\nadapters to match the target language at inference time is suboptimal for dense\nretrieval models. We provide an in-depth analysis of this discrepancy between\nother cross-language NLP tasks and CLIR.",
        "pdf_link": "https://arxiv.org/pdf/2212.10448v1.pdf"
    },
    {
        "title": "Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data",
        "authors": [
            "Tim Jansen",
            "Yangling Tong",
            "Victoria Zevallos",
            "Pedro Ortiz Suarez"
        ],
        "published": "2022-12-20T17:14:45Z",
        "summary": "As demand for large corpora increases with the size of current\nstate-of-the-art language models, using web data as the main part of the\npre-training corpus for these models has become a ubiquitous practice. This, in\nturn, has introduced an important challenge for NLP practitioners, as they are\nnow confronted with the task of developing highly optimized models and\npipelines for pre-processing large quantities of textual data, which implies,\neffectively classifying and filtering multilingual, heterogeneous and noisy\ndata, at web scale. One of the main components of this pre-processing step for\nthe pre-training corpora of large language models, is the removal of adult and\nharmful content. In this paper we explore different methods for detecting adult\nand harmful of content in multilingual heterogeneous web data. We first show\nhow traditional methods in harmful content detection, that seemingly perform\nquite well in small and specialized datasets quickly break down when confronted\nwith heterogeneous noisy web data. We then resort to using a perplexity based\napproach but with a twist: Instead of using a so-called \"clean\" corpus to train\na small language model and then use perplexity so select the documents with low\nperplexity, i.e., the documents that resemble this so-called \"clean\" corpus the\nmost. We train solely with adult and harmful textual data, and then select the\ndocuments having a perplexity value above a given threshold. This approach will\nvirtually cluster our documents into two distinct groups, which will greatly\nfacilitate the choice of the threshold for the perplexity and will also allow\nus to obtain higher precision than with the traditional classification methods\nfor detecting adult and harmful content.",
        "pdf_link": "https://arxiv.org/pdf/2212.10440v1.pdf"
    },
    {
        "title": "Towards Reasoning in Large Language Models: A Survey",
        "authors": [
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2022-12-20T16:29:03Z",
        "summary": "Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.",
        "pdf_link": "https://arxiv.org/pdf/2212.10403v2.pdf"
    },
    {
        "title": "Data Curation Alone Can Stabilize In-context Learning",
        "authors": [
            "Ting-Yun Chang",
            "Robin Jia"
        ],
        "published": "2022-12-20T15:58:54Z",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by prompting them with a sequence of training examples. However, it is\nknown that ICL is very sensitive to the choice of training examples: randomly\nsampling examples from a training set leads to high variance in performance. In\nthis paper, we show that carefully curating a subset of training data greatly\nstabilizes ICL performance without any other changes to the ICL algorithm\n(e.g., prompt retrieval or calibration). We introduce two methods to choose\ntraining subsets -- both score training examples individually, then select the\nhighest-scoring ones. CondAcc scores a training example by its average dev-set\nICL accuracy when combined with random training examples, while Datamodels\nlearns linear regressors that estimate how the presence of each training\nexample influences LLM outputs. Across five tasks and two LLMs, sampling from\nstable subsets selected by CondAcc and Datamodels improves average accuracy\nover sampling from the entire training set by 7.7% and 6.3%, respectively.\nSurprisingly, the stable subset examples are not especially diverse in content\nor low in perplexity, in contrast with other work suggesting that diversity and\nperplexity are important when prompting LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2212.10378v2.pdf"
    },
    {
        "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Alexander I. Rudnicky",
            "Peter J. Ramadge"
        ],
        "published": "2022-12-20T15:40:17Z",
        "summary": "Length extrapolation permits training a transformer language model on short\nsequences that preserves perplexities when tested on substantially longer\nsequences. A relative positional embedding design, ALiBi, has had the widest\nusage to date. We dissect ALiBi via the lens of receptive field analysis\nempowered by a novel cumulative normalized gradient tool. The concept of\nreceptive field further allows us to modify the vanilla Sinusoidal positional\nembedding to create ~\\textbf{Sandwich}, the first parameter-free relative\npositional embedding design that truly length information uses longer than the\ntraining sequence. Sandwich shares with KERPLE and T5 the same logarithmic\ndecaying temporal bias pattern with learnable relative positional embeddings;\nthese elucidate future extrapolatable positional embedding design.",
        "pdf_link": "https://arxiv.org/pdf/2212.10356v2.pdf"
    },
    {
        "title": "Identifying and Manipulating the Personality Traits of Language Models",
        "authors": [
            "Graham Caron",
            "Shashank Srivastava"
        ],
        "published": "2022-12-20T14:24:11Z",
        "summary": "Psychology research has long explored aspects of human personality such as\nextroversion, agreeableness and emotional stability. Categorizations like the\n`Big Five' personality traits are commonly used to assess and diagnose\npersonality types. In this work, we explore the question of whether the\nperceived personality in language models is exhibited consistently in their\nlanguage generation. For example, is a language model such as GPT2 likely to\nrespond in a consistent way if asked to go out to a party? We also investigate\nwhether such personality traits can be controlled. We show that when provided\ndifferent types of contexts (such as personality descriptions, or answers to\ndiagnostic questions about personality traits), language models such as BERT\nand GPT2 can consistently identify and reflect personality markers in those\ncontexts. This behavior illustrates an ability to be manipulated in a highly\npredictable way, and frames them as tools for identifying personality traits\nand controlling personas in applications such as dialog systems. We also\ncontribute a crowd-sourced data-set of personality descriptions of human\nsubjects paired with their `Big Five' personality assessment data, and a\ndata-set of personality descriptions collated from Reddit.",
        "pdf_link": "https://arxiv.org/pdf/2212.10276v1.pdf"
    },
    {
        "title": "ReCode: Robustness Evaluation of Code Generation Models",
        "authors": [
            "Shiqi Wang",
            "Zheng Li",
            "Haifeng Qian",
            "Chenghao Yang",
            "Zijian Wang",
            "Mingyue Shang",
            "Varun Kumar",
            "Samson Tan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Ramesh Nallapati",
            "Murali Krishna Ramanathan",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2022-12-20T14:11:31Z",
        "summary": "Code generation models have achieved impressive performance. However, they\ntend to be brittle as slight edits to a prompt could lead to very different\ngenerations; these robustness properties, critical for user experience when\ndeployed in real-life applications, are not well understood. Most existing\nworks on robustness in text or code tasks have focused on classification, while\nrobustness in generation tasks is an uncharted area and to date there is no\ncomprehensive benchmark for robustness in code generation. In this paper, we\npropose ReCode, a comprehensive robustness evaluation benchmark for code\ngeneration models. We customize over 30 transformations specifically for code\non docstrings, function and variable names, code syntax, and code format. They\nare carefully designed to be natural in real-life coding practice, preserve the\noriginal semantic meaning, and thus provide multifaceted assessments of a\nmodel's robustness performance. With human annotators, we verified that over\n90% of the perturbed prompts do not alter the semantic meaning of the original\nprompt. In addition, we define robustness metrics for code generation models\nconsidering the worst-case behavior under each type of perturbation, taking\nadvantage of the fact that executing the generated code can serve as objective\nevaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well\nas function completion tasks derived from them. Interesting observations\ninclude: better robustness for CodeGen over InCoder and GPT-J; models are most\nsensitive to syntax perturbations; more challenging robustness evaluation on\nMBPP over HumanEval.",
        "pdf_link": "https://arxiv.org/pdf/2212.10264v1.pdf"
    },
    {
        "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
        "authors": [
            "Yahan Yang",
            "Soham Dan",
            "Dan Roth",
            "Insup Lee"
        ],
        "published": "2022-12-20T14:06:50Z",
        "summary": "Recently it has been shown that state-of-the-art NLP models are vulnerable to\nadversarial attacks, where the predictions of a model can be drastically\naltered by slight modifications to the input (such as synonym substitutions).\nWhile several defense techniques have been proposed, and adapted, to the\ndiscrete nature of text adversarial attacks, the benefits of general-purpose\nregularization methods such as label smoothing for language models, have not\nbeen studied. In this paper, we study the adversarial robustness provided by\nvarious label smoothing strategies in foundational models for diverse NLP tasks\nin both in-domain and out-of-domain settings. Our experiments show that label\nsmoothing significantly improves adversarial robustness in pre-trained models\nlike BERT, against various popular attacks. We also analyze the relationship\nbetween prediction confidence and robustness, showing that label smoothing\nreduces over-confident errors on adversarial examples.",
        "pdf_link": "https://arxiv.org/pdf/2212.10258v2.pdf"
    },
    {
        "title": "Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study",
        "authors": [
            "Di Wu",
            "Wasi Uddin Ahmad",
            "Kai-Wei Chang"
        ],
        "published": "2022-12-20T13:20:21Z",
        "summary": "Neural models that do not rely on pre-training have excelled in the keyphrase\ngeneration task with large annotated datasets. Meanwhile, new approaches have\nincorporated pre-trained language models (PLMs) for their data efficiency.\nHowever, there lacks a systematic study of how the two types of approaches\ncompare and how different design choices can affect the performance of\nPLM-based models. To fill in this knowledge gap and facilitate a more informed\nuse of PLMs for keyphrase extraction and keyphrase generation, we present an\nin-depth empirical study. Formulating keyphrase extraction as sequence labeling\nand keyphrase generation as sequence-to-sequence generation, we perform\nextensive experiments in three domains. After showing that PLMs have\ncompetitive high-resource performance and state-of-the-art low-resource\nperformance, we investigate important design choices including in-domain PLMs,\nPLMs with different pre-training objectives, using PLMs with a parameter\nbudget, and different formulations for present keyphrases. Further results show\nthat (1) in-domain BERT-like PLMs can be used to build strong and\ndata-efficient keyphrase generation models; (2) with a fixed parameter budget,\nprioritizing model depth over width and allocating more layers in the encoder\nleads to better encoder-decoder models; and (3) introducing four in-domain\nPLMs, we achieve a competitive performance in the news domain and the\nstate-of-the-art performance in the scientific domain.",
        "pdf_link": "https://arxiv.org/pdf/2212.10233v2.pdf"
    },
    {
        "title": "Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite",
        "authors": [
            "Xun Wang",
            "Tao Ge",
            "Allen Mao",
            "Yuki Li",
            "Furu Wei",
            "Si-Qing Chen"
        ],
        "published": "2022-12-20T12:02:34Z",
        "summary": "We introduce \\textsc{PoliteRewrite} -- a dataset for polite language rewrite\nwhich is a novel sentence rewrite task. Compared with previous text style\ntransfer tasks that can be mostly addressed by slight token- or phrase-level\nedits, polite language rewrite requires deep understanding and extensive\nsentence-level edits over an offensive and impolite sentence to deliver the\nsame message euphemistically and politely, which is more challenging -- not\nonly for NLP models but also for human annotators to rewrite with effort. To\nalleviate the human effort for efficient annotation, we first propose a novel\nannotation paradigm by a collaboration of human annotators and GPT-3.5 to\nannotate \\textsc{PoliteRewrite}. The released dataset has 10K polite sentence\nrewrites annotated collaboratively by GPT-3.5 and human, which can be used as\ngold standard for training, validation and test; and 100K high-quality polite\nsentence rewrites by GPT-3.5 without human review. We wish this work (The\ndataset (10K+100K) will be released soon) could contribute to the research on\nmore challenging sentence rewrite, and provoke more thought in future on\nresource annotation paradigm with the help of the large-scaled pretrained\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2212.10190v1.pdf"
    },
    {
        "title": "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
        "authors": [
            "Qingyu Lu",
            "Liang Ding",
            "Liping Xie",
            "Kanjian Zhang",
            "Derek F. Wong",
            "Dacheng Tao"
        ],
        "published": "2022-12-20T11:36:22Z",
        "summary": "The state-of-the-art language model-based automatic metrics, e.g. BARTScore,\nbenefiting from large-scale contextualized pre-training, have been successfully\nused in a wide range of natural language generation (NLG) tasks, including\nmachine translation, text summarization, and data-to-text. Recent studies show\nthat considering both major errors (e.g. mistranslated tokens) and minor errors\n(e.g. imperfections in fluency) can produce high-quality human judgments. This\ninspires us to approach the final goal of the evaluation metrics (human-like\nevaluations) by automatic error analysis. To this end, we augment BARTScore by\nincorporating the human-like error analysis strategies, namely BARTScore++,\nwhere the final score consists of both the evaluations of major errors and\nminor errors. Experimental results show that BARTScore++ can consistently\nimprove the performance of vanilla BARTScore and outperform existing\ntop-scoring metrics in 20 out of 25 test settings. We hope our technique can\nalso be extended to other pre-trained model-based metrics. We will release our\ncode and scripts to facilitate the community.",
        "pdf_link": "https://arxiv.org/pdf/2212.10179v1.pdf"
    },
    {
        "title": "Human-Guided Fair Classification for Natural Language Processing",
        "authors": [
            "Florian E. Dorner",
            "Momchil Peychev",
            "Nikola Konstantinov",
            "Naman Goel",
            "Elliott Ash",
            "Martin Vechev"
        ],
        "published": "2022-12-20T10:46:40Z",
        "summary": "Text classifiers have promising applications in high-stake tasks such as\nresume screening and content moderation. These classifiers must be fair and\navoid discriminatory decisions by being invariant to perturbations of sensitive\nattributes such as gender or ethnicity. However, there is a gap between human\nintuition about these perturbations and the formal similarity specifications\ncapturing them. While existing research has started to address this gap,\ncurrent methods are based on hardcoded word replacements, resulting in\nspecifications with limited expressivity or ones that fail to fully align with\nhuman intuition (e.g., in cases of asymmetric counterfactuals). This work\nproposes novel methods for bridging this gap by discovering expressive and\nintuitive individual fairness specifications. We show how to leverage\nunsupervised style transfer and GPT-3's zero-shot capabilities to automatically\ngenerate expressive candidate pairs of semantically similar sentences that\ndiffer along sensitive attributes. We then validate the generated pairs via an\nextensive crowdsourcing study, which confirms that a lot of these pairs align\nwith human intuition about fairness in the context of toxicity classification.\nFinally, we show how limited amounts of human feedback can be leveraged to\nlearn a similarity specification that can be used to train downstream\nfairness-aware models.",
        "pdf_link": "https://arxiv.org/pdf/2212.10154v2.pdf"
    },
    {
        "title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
        "authors": [
            "Maksym Del",
            "Mark Fishel"
        ],
        "published": "2022-12-20T09:34:43Z",
        "summary": "Large language models (LLMs) have demonstrated solid zero-shot reasoning\ncapabilities, which is reflected in their performance on the current test\ntasks. This calls for a more challenging benchmark requiring highly advanced\nreasoning ability to be solved. In this paper, we introduce such a benchmark,\nconsisting of 191 long-form (1200 words on average) mystery narratives\nconstructed as detective puzzles. Puzzles are sourced from the \"5 Minute\nMystery\" platform and include a multiple-choice question for evaluation. Only\n47% of humans solve a puzzle successfully on average, while the best human\nsolvers achieve over 80% success rate. We show that GPT-3 models barely\noutperform random on this benchmark (with 28% accuracy) while state-of-the-art\nGPT-4 solves only 38% of puzzles. This indicates that there is still a\nsignificant gap in the deep reasoning abilities of LLMs and humans and\nhighlights the need for further research in this area. Our work introduces a\nchallenging benchmark for future studies on reasoning in language models and\ncontributes to a better understanding of the limits of LLMs' abilities.",
        "pdf_link": "https://arxiv.org/pdf/2212.10114v2.pdf"
    },
    {
        "title": "Hybrid Rule-Neural Coreference Resolution System based on Actor-Critic Learning",
        "authors": [
            "Yu Wang",
            "Hongxia Jin"
        ],
        "published": "2022-12-20T08:55:47Z",
        "summary": "A coreference resolution system is to cluster all mentions that refer to the\nsame entity in a given context. All coreference resolution systems need to\ntackle two main tasks: one task is to detect all of the potential mentions, and\nthe other is to learn the linking of an antecedent for each possible mention.\nIn this paper, we propose a hybrid rule-neural coreference resolution system\nbased on actor-critic learning, such that it can achieve better coreference\nperformance by leveraging the advantages from both the heuristic rules and a\nneural conference model. This end-to-end system can also perform both mention\ndetection and resolution by leveraging a joint training algorithm. We\nexperiment on the BERT model to generate input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.",
        "pdf_link": "https://arxiv.org/pdf/2212.10087v1.pdf"
    },
    {
        "title": "Large Language Models Are Reasoning Teachers",
        "authors": [
            "Namgyu Ho",
            "Laura Schmid",
            "Se-Young Yun"
        ],
        "published": "2022-12-20T08:24:45Z",
        "summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.",
        "pdf_link": "https://arxiv.org/pdf/2212.10071v2.pdf"
    },
    {
        "title": "A Twitter BERT Approach for Offensive Language Detection in Marathi",
        "authors": [
            "Tanmay Chavan",
            "Shantanu Patankar",
            "Aditya Kane",
            "Omkar Gokhale",
            "Raviraj Joshi"
        ],
        "published": "2022-12-20T07:22:45Z",
        "summary": "Automated offensive language detection is essential in combating the spread\nof hate speech, particularly in social media. This paper describes our work on\nOffensive Language Identification in low resource Indic language Marathi. The\nproblem is formulated as a text classification task to identify a tweet as\noffensive or non-offensive. We evaluate different mono-lingual and\nmulti-lingual BERT models on this classification task, focusing on BERT models\npre-trained with social media datasets. We compare the performance of MuRIL,\nMahaTweetBERT, MahaTweetBERT-Hateful, and MahaBERT on the HASOC 2022 test set.\nWe also explore external data augmentation from other existing Marathi hate\nspeech corpus HASOC 2021 and L3Cube-MahaHate. The MahaTweetBERT, a BERT model,\npre-trained on Marathi tweets when fine-tuned on the combined dataset (HASOC\n2021 + HASOC 2022 + MahaHate), outperforms all models with an F1 score of 98.43\non the HASOC 2022 test set. With this, we also provide a new state-of-the-art\nresult on HASOC 2022 / MOLD v2 test set.",
        "pdf_link": "https://arxiv.org/pdf/2212.10039v1.pdf"
    },
    {
        "title": "Do language models have coherent mental models of everyday things?",
        "authors": [
            "Yuling Gu",
            "Bhavana Dalvi Mishra",
            "Peter Clark"
        ],
        "published": "2022-12-20T06:54:04Z",
        "summary": "When people think of everyday things like an egg, they typically have a\nmental image associated with it. This allows them to correctly judge, for\nexample, that \"the yolk surrounds the shell\" is a false statement. Do language\nmodels similarly have a coherent picture of such everyday things? To\ninvestigate this, we propose a benchmark dataset consisting of 100 everyday\nthings, their parts, and the relationships between these parts, expressed as\n11,720 \"X relation Y?\" true/false questions. Using these questions as probes,\nwe observe that state-of-the-art pre-trained language models (LMs) like GPT-3\nand Macaw have fragments of knowledge about these everyday things, but do not\nhave fully coherent \"parts mental models\" (54-59% accurate, 19-43% conditional\nconstraint violation). We propose an extension where we add a constraint\nsatisfaction layer on top of the LM's raw predictions to apply commonsense\nconstraints. As well as removing inconsistencies, we find that this also\nsignificantly improves accuracy (by 16-20%), suggesting how the incoherence of\nthe LM's pictures of everyday things can be significantly reduced.",
        "pdf_link": "https://arxiv.org/pdf/2212.10029v3.pdf"
    },
    {
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "authors": [
            "Tianxing He",
            "Jingyu Zhang",
            "Tianle Wang",
            "Sachin Kumar",
            "Kyunghyun Cho",
            "James Glass",
            "Yulia Tsvetkov"
        ],
        "published": "2022-12-20T06:24:25Z",
        "summary": "In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps://github.com/cloudygoose/blindspot_nlg.",
        "pdf_link": "https://arxiv.org/pdf/2212.10020v3.pdf"
    },
    {
        "title": "DocAsRef: An Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely",
        "authors": [
            "Forrest Sheng Bao",
            "Ruixuan Tu",
            "Ge Luo",
            "Yinfei Yang",
            "Hebi Li",
            "Minghui Qiu",
            "Youbiao He",
            "Cen Chen"
        ],
        "published": "2022-12-20T06:01:13Z",
        "summary": "Automated summary quality assessment falls into two categories:\nreference-based and reference-free. Reference-based metrics, historically\ndeemed more accurate due to the additional information provided by\nhuman-written references, are limited by their reliance on human input. In this\npaper, we hypothesize that the comparison methodologies used by some\nreference-based metrics to evaluate a system summary against its corresponding\nreference can be effectively adapted to assess it against its source document,\nthereby transforming these metrics into reference-free ones. Experimental\nresults support this hypothesis. After being repurposed reference-freely, the\nzero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of <0.5B\nparameters consistently outperforms its original reference-based version across\nvarious aspects on the SummEval and Newsroom datasets. It also excels in\ncomparison to most existing reference-free metrics and closely competes with\nzero-shot summary evaluators based on GPT-3.5.",
        "pdf_link": "https://arxiv.org/pdf/2212.10013v2.pdf"
    },
    {
        "title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English",
        "authors": [
            "Jianfeng Chi",
            "Wasi Uddin Ahmad",
            "Yuan Tian",
            "Kai-Wei Chang"
        ],
        "published": "2022-12-20T05:58:32Z",
        "summary": "Privacy policies provide individuals with information about their rights and\nhow their personal information is handled. Natural language understanding (NLU)\ntechnologies can support individuals and practitioners to understand better\nprivacy practices described in lengthy and complex documents. However, existing\nefforts that use NLU technologies are limited by processing the language in a\nway exclusive to a single task focusing on certain privacy practices. To this\nend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)\nbenchmark, a multi-task benchmark for evaluating the privacy policy language\nunderstanding across various tasks. We also collect a large corpus of privacy\npolicies to enable privacy policy domain-specific language model pre-training.\nWe evaluate several generic pre-trained language models and continue\npre-training them on the collected corpus. We demonstrate that domain-specific\ncontinual pre-training offers performance improvements across all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.10011v2.pdf"
    },
    {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "authors": [
            "Boshi Wang",
            "Sewon Min",
            "Xiang Deng",
            "Jiaming Shen",
            "You Wu",
            "Luke Zettlemoyer",
            "Huan Sun"
        ],
        "published": "2022-12-20T05:20:54Z",
        "summary": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.",
        "pdf_link": "https://arxiv.org/pdf/2212.10001v2.pdf"
    },
    {
        "title": "Are Deep Neural Networks SMARTer than Second Graders?",
        "authors": [
            "Anoop Cherian",
            "Kuan-Chuan Peng",
            "Suhas Lohit",
            "Kevin A. Smith",
            "Joshua B. Tenenbaum"
        ],
        "published": "2022-12-20T04:33:32Z",
        "summary": "Recent times have witnessed an increasing number of applications of deep\nneural networks towards solving tasks that require superior cognitive\nabilities, e.g., playing Go, generating art, ChatGPT, etc. Such a dramatic\nprogress raises the question: how generalizable are neural networks in solving\nproblems that demand broad skills? To answer this question, we propose SMART: a\nSimple Multimodal Algorithmic Reasoning Task and the associated SMART-101\ndataset, for evaluating the abstraction, deduction, and generalization\nabilities of neural networks in solving visuo-linguistic puzzles designed\nspecifically for children in the 6--8 age group. Our dataset consists of 101\nunique puzzles; each puzzle comprises a picture and a question, and their\nsolution needs a mix of several elementary skills, including arithmetic,\nalgebra, and spatial reasoning, among others. To scale our dataset towards\ntraining deep neural networks, we programmatically generate entirely new\ninstances for each puzzle, while retaining their solution algorithm. To\nbenchmark performances on SMART-101, we propose a vision and language\nmeta-learning model using varied state-of-the-art backbones. Our experiments\nreveal that while powerful deep models offer reasonable performances on puzzles\nin a supervised setting, they are not better than random accuracy when analyzed\nfor generalization. We also evaluate the recent ChatGPT and other large\nlanguage models on a subset of SMART-101 and find that while these models show\nconvincing reasoning abilities, the answers are often incorrect.",
        "pdf_link": "https://arxiv.org/pdf/2212.09993v6.pdf"
    },
    {
        "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
        "authors": [
            "Yixin Liu",
            "Budhaditya Deb",
            "Milagro Teruel",
            "Aaron Halfaker",
            "Dragomir Radev",
            "Ahmed H. Awadallah"
        ],
        "published": "2022-12-20T02:47:37Z",
        "summary": "Despite the recent progress in language generation models, their outputs may\nnot always meet user expectations. In this work, we study whether informational\nfeedback in natural language can be leveraged to improve generation quality and\nuser preference alignment. To this end, we consider factual consistency in\nsummarization, the quality that the summary should only contain information\nsupported by the input documents, as the user-expected preference. We collect a\nhigh-quality dataset, DeFacto, containing human demonstrations and\ninformational natural language feedback consisting of corrective instructions,\nedited summaries, and explanations with respect to the factual consistency of\nthe summary. Using our dataset, we study three natural language generation\ntasks: (1) editing a summary by following the human feedback, (2) generating\nhuman feedback for editing the original summary, and (3) revising the initial\nsummary to correct factual errors by generating both the human feedback and\nedited summary. We show that DeFacto can provide factually consistent\nhuman-edited summaries and further insights into summarization factual\nconsistency thanks to its informational natural language feedback. We further\ndemonstrate that fine-tuned language models can leverage our dataset to improve\nthe summary factual consistency, while large language models lack the zero-shot\nlearning ability in our proposed tasks that require controllable text\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2212.09968v2.pdf"
    },
    {
        "title": "AnyTOD: A Programmable Task-Oriented Dialog System",
        "authors": [
            "Jeffrey Zhao",
            "Yuan Cao",
            "Raghav Gupta",
            "Harrison Lee",
            "Abhinav Rastogi",
            "Mingqiu Wang",
            "Hagen Soltau",
            "Izhak Shafran",
            "Yonghui Wu"
        ],
        "published": "2022-12-20T01:23:01Z",
        "summary": "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system\ncapable of handling unseen tasks without task-specific training. We view TOD as\na program executed by a language model (LM), where program logic and ontology\nis provided by a designer as a schema. To enable generalization to unseen\nschemas and programs without prior training, AnyTOD adopts a neuro-symbolic\napproach. A neural LM keeps track of events occurring during a conversation and\na symbolic program implementing the dialog policy is executed to recommend next\nactions AnyTOD should take. This approach drastically reduces data annotation\nand model training requirements, addressing the enduring challenge of rapidly\nadapting a TOD system to unseen tasks and domains. We demonstrate\nstate-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate\nstrong zero-shot transfer ability in low-resource settings, such as zero-shot\non MultiWOZ. In addition, we release STARv2, an updated version of the STAR\ndataset with richer annotations, for benchmarking zero-shot end-to-end TOD\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2212.09939v2.pdf"
    },
    {
        "title": "Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC",
        "authors": [
            "Patrick Emami",
            "Aidan Perreault",
            "Jeffrey Law",
            "David Biagioni",
            "Peter C. St. John"
        ],
        "published": "2022-12-20T00:26:23Z",
        "summary": "A long-standing goal of machine-learning-based protein engineering is to\naccelerate the discovery of novel mutations that improve the function of a\nknown protein. We introduce a sampling framework for evolving proteins in\nsilico that supports mixing and matching a variety of unsupervised models, such\nas protein language models, and supervised models that predict protein function\nfrom sequence. By composing these models, we aim to improve our ability to\nevaluate unseen mutations and constrain search to regions of sequence space\nlikely to contain functional proteins. Our framework achieves this without any\nmodel fine-tuning or re-training by constructing a product of experts\ndistribution directly in discrete protein space. Instead of resorting to brute\nforce search or random sampling, which is typical of classic directed\nevolution, we introduce a fast MCMC sampler that uses gradients to propose\npromising mutations. We conduct in silico directed evolution experiments on\nwide fitness landscapes and across a range of different pre-trained\nunsupervised models, including a 650M parameter protein language model. Our\nresults demonstrate an ability to efficiently discover variants with high\nevolutionary likelihood as well as estimated activity multiple mutations away\nfrom a wild type protein, suggesting our sampler provides a practical and\neffective new paradigm for machine-learning-based protein engineering.",
        "pdf_link": "https://arxiv.org/pdf/2212.09925v2.pdf"
    },
    {
        "title": "Improved Long-Form Spoken Language Translation with Large Language Models",
        "authors": [
            "Arya D. McCarthy",
            "Hao Zhang",
            "Shankar Kumar",
            "Felix Stahlberg",
            "Axel H. Ng"
        ],
        "published": "2022-12-19T22:36:53Z",
        "summary": "A challenge in spoken language translation is that plenty of spoken content\nis long-form, but short units are necessary for obtaining high-quality\ntranslations. To address this mismatch, we fine-tune a general-purpose, large\nlanguage model to split long ASR transcripts into segments that can be\nindependently translated so as to maximize the overall translation quality. We\ncompare to several segmentation strategies and find that our approach improves\nBLEU score on three languages by an average of 2.7 BLEU overall compared to an\nautomatic punctuation baseline. Further, we demonstrate the effectiveness of\ntwo constrained decoding strategies to improve well-formedness of the model\noutput from above 99% to 100%.",
        "pdf_link": "https://arxiv.org/pdf/2212.09895v1.pdf"
    },
    {
        "title": "Python Code Generation by Asking Clarification Questions",
        "authors": [
            "Haau-Sing Li",
            "Mohsen Mesgar",
            "Andr\u00e9 F. T. Martins",
            "Iryna Gurevych"
        ],
        "published": "2022-12-19T22:08:36Z",
        "summary": "Code generation from text requires understanding the user's intent from a\nnatural language description and generating an executable code snippet that\nsatisfies this intent. While recent pretrained language models demonstrate\nremarkable performance for this task, these models fail when the given natural\nlanguage description is under-specified. In this work, we introduce a novel and\nmore realistic setup for this task. We hypothesize that the under-specification\nof a natural language description can be resolved by asking clarification\nquestions. Therefore, we collect and introduce a new dataset named CodeClarQA\ncontaining pairs of natural language descriptions and code with created\nsynthetic clarification questions and answers. The empirical results of our\nevaluation of pretrained language model performance on code generation show\nthat clarifications result in more precisely generated code, as shown by the\nsubstantial improvement of model performance in all evaluation metrics.\nAlongside this, our task and dataset introduce new challenges to the community,\nincluding when and what clarification questions should be asked. Our code and\ndataset are available on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2212.09885v2.pdf"
    },
    {
        "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "authors": [
            "Xinxi Lyu",
            "Sewon Min",
            "Iz Beltagy",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-19T21:34:26Z",
        "summary": "Although large language models can be prompted for both zero- and few-shot\nlearning, performance drops significantly when no demonstrations are available.\nIn this paper, we introduce Z-ICL, a new zero-shot method that closes the gap\nby constructing pseudo-demonstrations for a given test input using a raw text\ncorpus. Concretely, pseudo-demonstrations are constructed by (1) finding the\nnearest neighbors to the test input from the corpus and pairing them with\nrandom task labels, and (2) applying a set of techniques to reduce the amount\nof direct copying the model does from the resulting demonstrations. Evaluation\non nine classification datasets shows that Z-ICL outperforms previous zero-shot\nmethods by a significant margin, and is on par with in-context learning with\nlabeled training data in the few-shot setting. Overall, Z-ICL provides a\nsignificantly higher estimate of the zero-shot performance levels of a model,\nand supports future efforts to develop better pseudo-demonstrations that\nfurther improve zero-shot results.",
        "pdf_link": "https://arxiv.org/pdf/2212.09865v2.pdf"
    },
    {
        "title": "MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders",
        "authors": [
            "Xiaofei Li",
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz"
        ],
        "published": "2022-12-19T20:57:45Z",
        "summary": "In this paper we present our contribution to the TSAR-2022 Shared Task on\nLexical Simplification of the EMNLP 2022 Workshop on Text Simplification,\nAccessibility, and Readability. Our approach builds on and extends the\nunsupervised lexical simplification system with pretrained encoders (LSBert)\nsystem in the following ways: For the subtask of simplification candidate\nselection, it utilizes a RoBERTa transformer language model and expands the\nsize of the generated candidate list. For subsequent substitution ranking, it\nintroduces a new feature weighting scheme and adopts a candidate filtering\nmethod based on textual entailment to maximize semantic similarity between the\ntarget word and its simplification. Our best-performing system improves LSBert\nby 5.9% accuracy and achieves second place out of 33 ranked solutions.",
        "pdf_link": "https://arxiv.org/pdf/2212.09855v1.pdf"
    },
    {
        "title": "(Psycho-)Linguistic Features Meet Transformer Models for Improved Explainable and Controllable Text Simplification",
        "authors": [
            "Yu Qiao",
            "Xiaofei Li",
            "Daniel Wiechmann",
            "Elma Kerz"
        ],
        "published": "2022-12-19T20:46:21Z",
        "summary": "State-of-the-art text simplification (TS) systems adopt end-to-end neural\nnetwork models to directly generate the simplified version of the input text,\nand usually function as a blackbox. Moreover, TS is usually treated as an\nall-purpose generic task under the assumption of homogeneity, where the same\nsimplification is suitable for all. In recent years, however, there has been\nincreasing recognition of the need to adapt the simplification techniques to\nthe specific needs of different target groups. In this work, we aim to advance\ncurrent research on explainable and controllable TS in two ways: First,\nbuilding on recently proposed work to increase the transparency of TS systems,\nwe use a large set of (psycho-)linguistic features in combination with\npre-trained language models to improve explainable complexity prediction.\nSecond, based on the results of this preliminary task, we extend a\nstate-of-the-art Seq2Seq TS model, ACCESS, to enable explicit control of ten\nattributes. The results of experiments show (1) that our approach improves the\nperformance of state-of-the-art models for predicting explainable complexity\nand (2) that explicitly conditioning the Seq2Seq model on ten attributes leads\nto a significant improvement in performance in both within-domain and\nout-of-domain settings.",
        "pdf_link": "https://arxiv.org/pdf/2212.09848v1.pdf"
    },
    {
        "title": "Exploring Hybrid and Ensemble Models for Multiclass Prediction of Mental Health Status on Social Media",
        "authors": [
            "Sourabh Zanwar",
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz"
        ],
        "published": "2022-12-19T20:31:47Z",
        "summary": "In recent years, there has been a surge of interest in research on automatic\nmental health detection (MHD) from social media data leveraging advances in\nnatural language processing and machine learning techniques. While significant\nprogress has been achieved in this interdisciplinary research area, the vast\nmajority of work has treated MHD as a binary classification task. The\nmulticlass classification setup is, however, essential if we are to uncover the\nsubtle differences among the statistical patterns of language use associated\nwith particular mental health conditions. Here, we report on experiments aimed\nat predicting six conditions (anxiety, attention deficit hyperactivity\ndisorder, bipolar disorder, post-traumatic stress disorder, depression, and\npsychological stress) from Reddit social media posts. We explore and compare\nthe performance of hybrid and ensemble models leveraging transformer-based\narchitectures (BERT and RoBERTa) and BiLSTM neural networks trained on\nwithin-text distributions of a diverse set of linguistic features. This set\nencompasses measures of syntactic complexity, lexical sophistication and\ndiversity, readability, and register-specific ngram frequencies, as well as\nsentiment and emotion lexicons. In addition, we conduct feature ablation\nexperiments to investigate which types of features are most indicative of\nparticular mental health conditions.",
        "pdf_link": "https://arxiv.org/pdf/2212.09839v1.pdf"
    },
    {
        "title": "Evaluating Human-Language Model Interaction",
        "authors": [
            "Mina Lee",
            "Megha Srivastava",
            "Amelia Hardy",
            "John Thickstun",
            "Esin Durmus",
            "Ashwin Paranjape",
            "Ines Gerard-Ursin",
            "Xiang Lisa Li",
            "Faisal Ladhak",
            "Frieda Rong",
            "Rose E. Wang",
            "Minae Kwon",
            "Joon Sung Park",
            "Hancheng Cao",
            "Tony Lee",
            "Rishi Bommasani",
            "Michael Bernstein",
            "Percy Liang"
        ],
        "published": "2022-12-19T18:59:45Z",
        "summary": "Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2212.09746v5.pdf"
    },
    {
        "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
        "authors": [
            "Mounica Maddela",
            "Yao Dou",
            "David Heineman",
            "Wei Xu"
        ],
        "published": "2022-12-19T18:56:52Z",
        "summary": "Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.09739v4.pdf"
    },
    {
        "title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
        "authors": [
            "Yu Gu",
            "Xiang Deng",
            "Yu Su"
        ],
        "published": "2022-12-19T18:55:21Z",
        "summary": "A key missing capacity of current language models (LMs) is grounding to\nreal-world environments. Most existing work for grounded language understanding\nuses LMs to directly generate plans that can be executed in the environment to\nachieve the desired effects. It thereby casts the burden of ensuring\ngrammaticality, faithfulness, and controllability all on the LMs. We propose\nPangu, a generic framework for grounded language understanding that capitalizes\non the discriminative ability of LMs instead of their generative ability. Pangu\nconsists of a symbolic agent and a neural LM working in a concerted fashion:\nThe agent explores the environment to incrementally construct valid plans, and\nthe LM evaluates the plausibility of the candidate plans to guide the search\nprocess. A case study on the challenging problem of knowledge base question\nanswering (KBQA), which features a massive environment, demonstrates the\nremarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient\nfor setting a new record on standard KBQA datasets, and larger LMs further\nbring substantial gains. Pangu also enables, for the first time, effective\nfew-shot in-context learning for KBQA with large LMs such as Codex.",
        "pdf_link": "https://arxiv.org/pdf/2212.09736v2.pdf"
    },
    {
        "title": "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages",
        "authors": [
            "Shashank Sonkar",
            "Zichao Wang",
            "Richard G. Baraniuk"
        ],
        "published": "2022-12-19T18:49:50Z",
        "summary": "This paper investigates the problem of Named Entity Recognition (NER) for\nextreme low-resource languages with only a few hundred tagged data samples. NER\nis a fundamental task in Natural Language Processing (NLP). A critical driver\naccelerating NER systems' progress is the existence of large-scale language\ncorpora that enable NER systems to achieve outstanding performance in languages\nsuch as English and French with abundant training data. However, NER for\nlow-resource languages remains relatively unexplored. In this paper, we\nintroduce Mask Augmented Named Entity Recognition (MANER), a new methodology\nthat leverages the distributional hypothesis of pre-trained masked language\nmodels (MLMs) for NER. The <mask> token in pre-trained MLMs encodes valuable\nsemantic contextual information. MANER re-purposes the <mask> token for NER\nprediction. Specifically, we prepend the <mask> token to every word in a\nsentence for which we would like to predict the named entity tag. During\ntraining, we jointly fine-tune the MLM and a new NER prediction head attached\nto each <mask> token. We demonstrate that MANER is well-suited for NER in\nlow-resource languages; our experiments show that for 100 languages with as few\nas 100 training examples, it improves on state-of-the-art methods by up to 48%\nand by 12% on average on F1 score. We also perform detailed analyses and\nablation studies to understand the scenarios that are best-suited to MANER.",
        "pdf_link": "https://arxiv.org/pdf/2212.09723v1.pdf"
    },
    {
        "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
        "authors": [
            "Tim Dettmers",
            "Luke Zettlemoyer"
        ],
        "published": "2022-12-19T18:48:33Z",
        "summary": "Quantization methods reduce the number of bits required to represent each\nparameter in a model, trading accuracy for smaller memory footprints and\ninference latencies. However, the final model size depends on both the number\nof parameters of the original model and the rate of compression. For example, a\n30B 8-bit model and a 60B 4-bit model have the same number of bits but may have\nvery different zero-shot accuracies. In this work, we study this trade-off by\ndeveloping inference scaling laws of zero-shot performance in Large Language\nModels (LLMs) to determine the bit-precision and model size that maximizes\nzero-shot performance. We run more than 35,000 experiments with 16-bit inputs\nand k-bit parameters to examine which zero-shot quantization methods improve\nscaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the\nLLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging\nto improve the bit-level scaling trade-off, with the only improvements being\nthe use of a small block size -- splitting the parameters into small\nindependently quantized blocks -- and the quantization data type being used\n(e.g., Int vs Float). Overall, our findings show that {4-bit} precision is\nalmost universally optimal for total model bits and zero-shot accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2212.09720v2.pdf"
    },
    {
        "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
        "authors": [
            "Or Honovich",
            "Thomas Scialom",
            "Omer Levy",
            "Timo Schick"
        ],
        "published": "2022-12-19T18:21:00Z",
        "summary": "Instruction tuning enables pretrained language models to perform new tasks\nfrom inference-time natural language descriptions. These approaches rely on\nvast amounts of human supervision in the form of crowdsourced datasets or user\ninteractions. In this work, we introduce Unnatural Instructions: a large\ndataset of creative and diverse instructions, collected with virtually no human\nlabor. We collect 64,000 examples by prompting a language model with three seed\nexamples of instructions and eliciting a fourth. This set is then expanded by\nprompting the model to rephrase each instruction, creating a total of\napproximately 240,000 examples of instructions, inputs, and outputs.\nExperiments show that despite containing a fair amount of noise, training on\nUnnatural Instructions rivals the effectiveness of training on open-source\nmanually-curated datasets, surpassing the performance of models such as T0++\nand Tk-Instruct across various benchmarks. These results demonstrate the\npotential of model-generated data as a cost-effective alternative to\ncrowdsourcing for dataset expansion and diversification.",
        "pdf_link": "https://arxiv.org/pdf/2212.09689v1.pdf"
    },
    {
        "title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP",
        "authors": [
            "Matan Eyal",
            "Hila Noga",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty"
        ],
        "published": "2022-12-19T18:10:23Z",
        "summary": "Recent work attributes progress in NLP to large language models (LMs) with\nincreased model size and large quantities of pretraining data. Despite this,\ncurrent state-of-the-art LMs for Hebrew are both under-parameterized and\nunder-trained compared to LMs in other languages. Additionally, previous work\non pretrained Hebrew LMs focused on encoder-only models. While the encoder-only\narchitecture is beneficial for classification tasks, it does not cater well for\nsub-word prediction tasks, such as Named Entity Recognition, when considering\nthe morphologically rich nature of Hebrew. In this paper we argue that\nsequence-to-sequence generative architectures are more suitable for LLMs in the\ncase of morphologically rich languages (MRLs) such as Hebrew. We demonstrate\nthat by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, we can\nleverage powerful multilingual, pretrained sequence-to-sequence models as mT5,\neliminating the need for a specialized, morpheme-based, separately fine-tuned\ndecoder. Using this approach, our experiments show substantial improvements\nover previously published results on existing Hebrew NLP benchmarks. These\nresults suggest that multilingual sequence-to-sequence models present a\npromising building block for NLP for MRLs.",
        "pdf_link": "https://arxiv.org/pdf/2212.09682v1.pdf"
    },
    {
        "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
        "authors": [
            "Fangyu Liu",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Yasemin Altun",
            "Nigel Collier",
            "Julian Martin Eisenschlos"
        ],
        "published": "2022-12-19T17:44:54Z",
        "summary": "Visual language data such as plots, charts, and infographics are ubiquitous\nin the human world. However, state-of-the-art vision-language models do not\nperform well on these data. We propose MatCha (Math reasoning and Chart\nderendering pretraining) to enhance visual language models' capabilities in\njointly modeling charts/plots and language data. Specifically, we propose\nseveral pretraining tasks that cover plot deconstruction and numerical\nreasoning which are the key capabilities in visual language modeling.\n  We perform the MatCha pretraining starting from Pix2Struct, a recently\nproposed image-to-text visual language model. On standard benchmarks such as\nPlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as\nmuch as nearly 20%. We also examine how well MatCha pretraining transfers to\ndomains such as screenshots, textbook diagrams, and document figures and\nobserve overall improvement, verifying the usefulness of MatCha pretraining on\nbroader visual language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.09662v2.pdf"
    },
    {
        "title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
        "authors": [
            "Jayr Pereira",
            "Robson Fidalgo",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-12-19T17:39:07Z",
        "summary": "This paper proposes a question-answering system that can answer questions\nwhose supporting evidence is spread over multiple (potentially long) documents.\nThe system, called Visconde, uses a three-step pipeline to perform the task:\ndecompose, retrieve, and aggregate. The first step decomposes the question into\nsimpler questions using a few-shot large language model (LLM). Then, a\nstate-of-the-art search engine is used to retrieve candidate passages from a\nlarge collection for each decomposed question. In the final step, we use the\nLLM in a few-shot setting to aggregate the contents of the passages into the\nfinal answer. The system is evaluated on three datasets: IIRC, Qasper, and\nStrategyQA. Results suggest that current retrievers are the main bottleneck and\nthat readers are already performing at the human level as long as relevant\npassages are provided. The system is also shown to be more effective when the\nmodel is induced to give explanations before answering a question. Code is\navailable at \\url{https://github.com/neuralmind-ai/visconde}.",
        "pdf_link": "https://arxiv.org/pdf/2212.09656v1.pdf"
    },
    {
        "title": "Optimizing Prompts for Text-to-Image Generation",
        "authors": [
            "Yaru Hao",
            "Zewen Chi",
            "Li Dong",
            "Furu Wei"
        ],
        "published": "2022-12-19T16:50:41Z",
        "summary": "Well-designed prompts can guide text-to-image models to generate amazing\nimages. However, the performant prompts are often model-specific and misaligned\nwith user input. Instead of laborious human engineering, we propose prompt\nadaptation, a general framework that automatically adapts original user input\nto model-preferred prompts. Specifically, we first perform supervised\nfine-tuning with a pretrained language model on a small collection of manually\nengineered prompts. Then we use reinforcement learning to explore better\nprompts. We define a reward function that encourages the policy to generate\nmore aesthetically pleasing images while preserving the original user\nintentions. Experimental results on Stable Diffusion show that our method\noutperforms manual prompt engineering in terms of both automatic metrics and\nhuman preference ratings. Moreover, reinforcement learning further boosts\nperformance, especially on out-of-domain prompts. The pretrained checkpoints\nare available at https://aka.ms/promptist. The demo can be found at\nhttps://aka.ms/promptist-demo.",
        "pdf_link": "https://arxiv.org/pdf/2212.09611v2.pdf"
    },
    {
        "title": "Explanation Regeneration via Information Bottleneck",
        "authors": [
            "Qintong Li",
            "Zhiyong Wu",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2022-12-19T16:41:19Z",
        "summary": "Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2212.09603v2.pdf"
    },
    {
        "title": "Reasoning with Language Model Prompting: A Survey",
        "authors": [
            "Shuofei Qiao",
            "Yixin Ou",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yunzhi Yao",
            "Shumin Deng",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "published": "2022-12-19T16:32:42Z",
        "summary": "Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).",
        "pdf_link": "https://arxiv.org/pdf/2212.09597v8.pdf"
    },
    {
        "title": "Unsupervised Summarization Re-ranking",
        "authors": [
            "Mathieu Ravaut",
            "Shafiq Joty",
            "Nancy Chen"
        ],
        "published": "2022-12-19T16:29:26Z",
        "summary": "With the rise of task-specific pre-training objectives, abstractive\nsummarization models like PEGASUS offer appealing zero-shot performance on\ndownstream summarization tasks. However, the performance of such unsupervised\nmodels still lags significantly behind their supervised counterparts. Similarly\nto the supervised setup, we notice a very high variance in quality among\nsummary candidates from these models while only one candidate is kept as the\nsummary output. In this paper, we propose to re-rank summary candidates in an\nunsupervised manner, aiming to close the performance gap between unsupervised\nand supervised models. Our approach improves the unsupervised PEGASUS by up to\n7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted\nsummarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%\nfrom XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on\na dataset, evaluating on another).",
        "pdf_link": "https://arxiv.org/pdf/2212.09593v3.pdf"
    },
    {
        "title": "Large Language Models are Better Reasoners with Self-Verification",
        "authors": [
            "Yixuan Weng",
            "Minjun Zhu",
            "Fei Xia",
            "Bin Li",
            "Shizhu He",
            "Shengping Liu",
            "Bin Sun",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2022-12-19T15:51:52Z",
        "summary": "Recently, with the chain of thought (CoT) prompting, large language models\n(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural\nlanguage processing tasks such as arithmetic, commonsense, and logical\nreasoning. However, LLMs with CoT require multi-step prompting and multi-token\nprediction, which is highly sensitive to individual mistakes and vulnerable to\nerror accumulation. The above issues make the LLMs need the ability to verify\nthe answers. In fact, after inferring conclusions in some thinking decision\ntasks, people often check them by re-verifying steps to avoid some mistakes. In\nthis paper, we propose and prove that LLMs also have similar self-verification\nabilities. We take the conclusion obtained by CoT as one of the conditions for\nsolving the original problem. By performing a backward verification of the\nanswers that LLM deduced for itself, we can obtain interpretable answer\nvalidation scores to select the candidate answer with the highest score.\nExperimental results demonstrate that the proposed method can improve the\nreasoning performance on various arithmetic, commonsense, and logical reasoning\ndatasets. Our code is publicly available at:\nhttps://github.com/WENGSYX/Self-Verification.",
        "pdf_link": "https://arxiv.org/pdf/2212.09561v5.pdf"
    },
    {
        "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
        "authors": [
            "Zheng-Xin Yong",
            "Hailey Schoelkopf",
            "Niklas Muennighoff",
            "Alham Fikri Aji",
            "David Ifeoluwa Adelani",
            "Khalid Almubarak",
            "M Saiful Bari",
            "Lintang Sutawika",
            "Jungo Kasai",
            "Ahmed Baruwa",
            "Genta Indra Winata",
            "Stella Biderman",
            "Edward Raff",
            "Dragomir Radev",
            "Vassilina Nikoulina"
        ],
        "published": "2022-12-19T15:24:45Z",
        "summary": "The BLOOM model is a large publicly available multilingual language model,\nbut its pretraining was limited to 46 languages. To extend the benefits of\nBLOOM to other languages without incurring prohibitively large costs, it is\ndesirable to adapt BLOOM to new languages not seen during pretraining. In this\nwork, we apply existing language adaptation strategies to BLOOM and benchmark\nits zero-shot prompting performance on eight new languages in a\nresource-constrained setting. We find language adaptation to be effective at\nimproving zero-shot performance in new languages. Surprisingly, we find that\nadapter-based finetuning is more effective than continued pretraining for large\nmodels. In addition, we discover that prompting performance is not\nsignificantly affected by language specifics, such as the writing system. It is\nprimarily determined by the size of the language adaptation data. We also add\nnew languages to BLOOMZ, which is a multitask finetuned version of BLOOM\ncapable of following task instructions zero-shot. We find including a new\nlanguage in the multitask fine-tuning mixture to be the most effective method\nto teach BLOOMZ a new language. We conclude that with sufficient training data\nlanguage adaptation can generalize well to diverse languages. Our code is\navailable at https://github.com/bigscience-workshop/multilingual-modeling.",
        "pdf_link": "https://arxiv.org/pdf/2212.09535v3.pdf"
    },
    {
        "title": "Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features",
        "authors": [
            "Sourabh Zanwar",
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz"
        ],
        "published": "2022-12-19T13:58:48Z",
        "summary": "In recent years, there has been increased interest in building predictive\nmodels that harness natural language processing and machine learning techniques\nto detect emotions from various text sources, including social media posts,\nmicro-blogs or news articles. Yet, deployment of such models in real-world\nsentiment and emotion applications faces challenges, in particular poor\nout-of-domain generalizability. This is likely due to domain-specific\ndifferences (e.g., topics, communicative goals, and annotation schemes) that\nmake transfer between different models of emotion recognition difficult. In\nthis work we propose approaches for text-based emotion detection that leverage\ntransformer models (BERT and RoBERTa) in combination with Bidirectional Long\nShort-Term Memory (BiLSTM) networks trained on a comprehensive set of\npsycholinguistic features. First, we evaluate the performance of our models\nwithin-domain on two benchmark datasets: GoEmotion and ISEAR. Second, we\nconduct transfer learning experiments on six datasets from the Unified Emotion\nDataset to evaluate their out-of-domain robustness. We find that the proposed\nhybrid models improve the ability to generalize to out-of-distribution data\ncompared to a standard transformer-based approach. Moreover, we observe that\nthese models perform competitively on in-domain data.",
        "pdf_link": "https://arxiv.org/pdf/2212.09465v1.pdf"
    },
    {
        "title": "Large Language Models Meet NL2Code: A Survey",
        "authors": [
            "Daoguang Zan",
            "Bei Chen",
            "Fengji Zhang",
            "Dianjie Lu",
            "Bingchao Wu",
            "Bei Guan",
            "Yongji Wang",
            "Jian-Guang Lou"
        ],
        "published": "2022-12-19T12:55:32Z",
        "summary": "The task of generating code from a natural language description, or NL2Code,\nis considered a pressing and significant challenge in code intelligence. Thanks\nto the rapid development of pre-training techniques, surging large language\nmodels are being proposed for code, sparking the advances in NL2Code. To\nfacilitate further research and applications in this field, in this paper, we\npresent a comprehensive survey of 27 existing large language models for\nNL2Code, and also review benchmarks and metrics. We provide an intuitive\ncomparison of all existing models on the HumanEval benchmark. Through in-depth\nobservation and analysis, we provide some insights and conclude that the key\nfactors contributing to the success of large language models for NL2Code are\n\"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges\nand opportunities regarding the gap between models and humans. We also create a\nwebsite https://nl2code.github.io to track the latest progress through\ncrowd-sourcing. To the best of our knowledge, this is the first survey of large\nlanguage models for NL2Code, and we believe it will contribute to the ongoing\ndevelopment of the field.",
        "pdf_link": "https://arxiv.org/pdf/2212.09420v2.pdf"
    },
    {
        "title": "Less is More: Parameter-Free Text Classification with Gzip",
        "authors": [
            "Zhiying Jiang",
            "Matthew Y. R. Yang",
            "Mikhail Tsirlin",
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "published": "2022-12-19T12:40:18Z",
        "summary": "Deep neural networks (DNNs) are often used for text classification tasks as\nthey usually achieve high levels of accuracy. However, DNNs can be\ncomputationally intensive with billions of parameters and large amounts of\nlabeled data, which can make them expensive to use, to optimize and to transfer\nto out-of-distribution (OOD) cases in practice. In this paper, we propose a\nnon-parametric alternative to DNNs that's easy, light-weight and universal in\ntext classification: a combination of a simple compressor like gzip with a\n$k$-nearest-neighbor classifier. Without any training, pre-training or\nfine-tuning, our method achieves results that are competitive with\nnon-pretrained deep learning methods on six in-distributed datasets. It even\noutperforms BERT on all five OOD datasets, including four low-resource\nlanguages. Our method also performs particularly well in few-shot settings\nwhere labeled data are too scarce for DNNs to achieve a satisfying accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2212.09410v1.pdf"
    },
    {
        "title": "Enriching Relation Extraction with OpenIE",
        "authors": [
            "Alessandro Temperoni",
            "Maria Biryukov",
            "Martin Theobald"
        ],
        "published": "2022-12-19T11:26:23Z",
        "summary": "Relation extraction (RE) is a sub-discipline of information extraction (IE)\nwhich focuses on the prediction of a relational predicate from a\nnatural-language input unit (such as a sentence, a clause, or even a short\nparagraph consisting of multiple sentences and/or clauses). Together with\nnamed-entity recognition (NER) and disambiguation (NED), RE forms the basis for\nmany advanced IE tasks such as knowledge-base (KB) population and verification.\nIn this work, we explore how recent approaches for open information extraction\n(OpenIE) may help to improve the task of RE by encoding structured information\nabout the sentences' principal units, such as subjects, objects, verbal\nphrases, and adverbials, into various forms of vectorized (and hence\nunstructured) representations of the sentences. Our main conjecture is that the\ndecomposition of long and possibly convoluted sentences into multiple smaller\nclauses via OpenIE even helps to fine-tune context-sensitive language models\nsuch as BERT (and its plethora of variants) for RE. Our experiments over two\nannotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy\nof our enriched models compared to existing RE approaches. Our best results\nreach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,\nproving the effectiveness of our approach on competitive benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2212.09376v1.pdf"
    },
    {
        "title": "ChatGPT: The End of Online Exam Integrity?",
        "authors": [
            "Teo Susnjak"
        ],
        "published": "2022-12-19T08:15:16Z",
        "summary": "This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.",
        "pdf_link": "https://arxiv.org/pdf/2212.09292v1.pdf"
    },
    {
        "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
        "authors": [
            "Soumya Sanyal",
            "Yichong Xu",
            "Shuohang Wang",
            "Ziyi Yang",
            "Reid Pryzant",
            "Wenhao Yu",
            "Chenguang Zhu",
            "Xiang Ren"
        ],
        "published": "2022-12-19T07:40:02Z",
        "summary": "Logical reasoning of text is an important ability that requires understanding\nthe information present in the text, their interconnections, and then reasoning\nthrough them to infer new conclusions. Prior works on improving the logical\nreasoning ability of language models require complex processing of training\ndata (e.g., aligning symbolic knowledge to text), yielding task-specific data\naugmentation solutions that restrict the learning of general logical reasoning\nskills. In this work, we propose APOLLO, an adaptively pretrained language\nmodel that has improved logical reasoning abilities. We select a subset of\nWikipedia, based on a set of logical inference keywords, for continued\npretraining of a language model. We use two self-supervised loss functions: a\nmodified masked language modeling loss where only specific parts-of-speech\nwords, that would likely require more reasoning than basic language\nunderstanding, are masked, and a sentence-level classification loss that\nteaches the model to distinguish between entailment and contradiction types of\nsentences. The proposed training paradigm is both simple and independent of\ntask formats. We demonstrate the effectiveness of APOLLO by comparing it with\nprior baselines on two logical reasoning datasets. APOLLO performs comparably\non ReClor and outperforms baselines on LogiQA. The code base has been made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2212.09282v2.pdf"
    },
    {
        "title": "Very Large Language Model as a Unified Methodology of Text Mining",
        "authors": [
            "Meng Jiang"
        ],
        "published": "2022-12-19T06:52:13Z",
        "summary": "Text data mining is the process of deriving essential information from\nlanguage text. Typical text mining tasks include text categorization, text\nclustering, topic modeling, information extraction, and text summarization.\nVarious data sets are collected and various algorithms are designed for the\ndifferent types of tasks. In this paper, I present a blue sky idea that very\nlarge language model (VLLM) will become an effective unified methodology of\ntext mining. I discuss at least three advantages of this new methodology\nagainst conventional methods. Finally I discuss the challenges in the design\nand development of VLLM techniques for text mining.",
        "pdf_link": "https://arxiv.org/pdf/2212.09271v2.pdf"
    },
    {
        "title": "PromptBoosting: Black-Box Text Classification with Ten Forward Passes",
        "authors": [
            "Bairu Hou",
            "Joe O'Connor",
            "Jacob Andreas",
            "Shiyu Chang",
            "Yang Zhang"
        ],
        "published": "2022-12-19T06:04:54Z",
        "summary": "We describe PromptBoosting, a query-efficient procedure for building a text\nclassifier from a neural language model (LM) without access to the LM's\nparameters, gradients, or hidden representations. This form of \"black-box\"\nclassifier training has become increasingly important as the cost of training\nand inference in large-scale LMs grows. But existing black-box LM classifier\nlearning approaches are themselves computationally inefficient, typically\nspecializing LMs to the target task by searching in a large space of (discrete\nor continuous) prompts using zeroth-order optimization methods. Instead of\ndirectly optimizing in prompt space, PromptBoosting obtains a small pool of\nprompts via a gradient-free approach and then constructs a large pool of weak\nlearners by pairing these prompts with different elements of the LM's output\ndistribution. These weak learners are then ensembled using the AdaBoost\nalgorithm. The entire learning process requires only a small number of forward\npasses and no backward pass. Experiments show that PromptBoosting achieves\nstate-of-the-art performance in multiple black-box few-shot classification\ntasks, and matches or outperforms full fine-tuning in both few-shot and\nstandard learning paradigms, while training 10x faster than existing black-box\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2212.09257v2.pdf"
    },
    {
        "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization",
        "authors": [
            "Bairu Hou",
            "Jinghan Jia",
            "Yihua Zhang",
            "Guanhua Zhang",
            "Yang Zhang",
            "Sijia Liu",
            "Shiyu Chang"
        ],
        "published": "2022-12-19T05:55:58Z",
        "summary": "Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.",
        "pdf_link": "https://arxiv.org/pdf/2212.09254v1.pdf"
    },
    {
        "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
        "authors": [
            "Ethan Perez",
            "Sam Ringer",
            "Kamil\u0117 Luko\u0161i\u016bt\u0117",
            "Karina Nguyen",
            "Edwin Chen",
            "Scott Heiner",
            "Craig Pettit",
            "Catherine Olsson",
            "Sandipan Kundu",
            "Saurav Kadavath",
            "Andy Jones",
            "Anna Chen",
            "Ben Mann",
            "Brian Israel",
            "Bryan Seethor",
            "Cameron McKinnon",
            "Christopher Olah",
            "Da Yan",
            "Daniela Amodei",
            "Dario Amodei",
            "Dawn Drain",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Guro Khundadze",
            "Jackson Kernion",
            "James Landis",
            "Jamie Kerr",
            "Jared Mueller",
            "Jeeyoon Hyun",
            "Joshua Landau",
            "Kamal Ndousse",
            "Landon Goldberg",
            "Liane Lovitt",
            "Martin Lucas",
            "Michael Sellitto",
            "Miranda Zhang",
            "Neerav Kingsland",
            "Nelson Elhage",
            "Nicholas Joseph",
            "Noem\u00ed Mercado",
            "Nova DasSarma",
            "Oliver Rausch",
            "Robin Larson",
            "Sam McCandlish",
            "Scott Johnston",
            "Shauna Kravec",
            "Sheer El Showk",
            "Tamera Lanham",
            "Timothy Telleen-Lawton",
            "Tom Brown",
            "Tom Henighan",
            "Tristan Hume",
            "Yuntao Bai",
            "Zac Hatfield-Dodds",
            "Jack Clark",
            "Samuel R. Bowman",
            "Amanda Askell",
            "Roger Grosse",
            "Danny Hernandez",
            "Deep Ganguli",
            "Evan Hubinger",
            "Nicholas Schiefer",
            "Jared Kaplan"
        ],
        "published": "2022-12-19T05:13:52Z",
        "summary": "As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user's preferred answer\n(\"sycophancy\") and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2212.09251v1.pdf"
    },
    {
        "title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
        "authors": [
            "Pengcheng Yin",
            "Wen-Ding Li",
            "Kefan Xiao",
            "Abhishek Rao",
            "Yeming Wen",
            "Kensen Shi",
            "Joshua Howland",
            "Paige Bailey",
            "Michele Catasta",
            "Henryk Michalewski",
            "Alex Polozov",
            "Charles Sutton"
        ],
        "published": "2022-12-19T05:06:00Z",
        "summary": "Computational notebooks, such as Jupyter notebooks, are interactive computing\nenvironments that are ubiquitous among data scientists to perform data\nwrangling and analytic tasks. To measure the performance of AI pair programmers\nthat automatically synthesize programs for those tasks given natural language\n(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation\nproblems using the pandas data analysis framework in data science notebooks.\nARCADE features multiple rounds of NL-to-code problems from the same notebook.\nIt requires a model to understand rich multi-modal contexts, such as existing\nnotebook cells and their execution states as well as previous turns of\ninteraction. To establish a strong baseline on this challenging task, we\ndevelop PaChiNCo, a 62B code language model (LM) for Python computational\nnotebooks, which significantly outperforms public code LMs. Finally, we explore\nfew-shot prompting strategies to elicit better code with step-by-step\ndecomposition and NL explanation, showing the potential to improve the\ndiversity and explainability of model predictions.",
        "pdf_link": "https://arxiv.org/pdf/2212.09248v1.pdf"
    },
    {
        "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation",
        "authors": [
            "Chandra Bhagavatula",
            "Jena D. Hwang",
            "Doug Downey",
            "Ronan Le Bras",
            "Ximing Lu",
            "Lianhui Qin",
            "Keisuke Sakaguchi",
            "Swabha Swayamdipta",
            "Peter West",
            "Yejin Choi"
        ],
        "published": "2022-12-19T04:47:49Z",
        "summary": "Commonsense capabilities of pre-trained language models dramatically improve\nwith scale, leading many to believe that scale is the only winning recipe. But\nis it? Here, we investigate an alternative that a priori seems impossible: can\nsmaller language models (e.g., GPT-2) win over models that are orders of\nmagnitude larger and better (e.g., GPT-3), if powered with novel commonsense\ndistillation algorithms? The key intellectual challenge is to design a learning\nalgorithm that achieve a competitive level of commonsense acquisition, without\nrelying on the benefits of scale. In particular, we study generative models of\ncommonsense knowledge, focusing on the task of generating generics, statements\nof commonsense facts about everyday concepts, e.g., birds can fly.\n  We introduce I2D2, a novel commonsense distillation framework that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale teacher model with two innovations: (1) the\nnovel adaptation of NeuroLogic Decoding to enhance the generation quality of\nthe weak, off-the-shelf language models, and (2) self-imitation learning to\niteratively learn from the model's own enhanced commonsense acquisition\ncapabilities. Empirical results suggest that scale is not the only way, as\nnovel algorithms can be a promising alternative. Moreover, our study leads to a\nnew corpus of generics, Gen-A-tomic, that is the largest and highest quality\navailable to date.",
        "pdf_link": "https://arxiv.org/pdf/2212.09246v3.pdf"
    },
    {
        "title": "Emergent Analogical Reasoning in Large Language Models",
        "authors": [
            "Taylor Webb",
            "Keith J. Holyoak",
            "Hongjing Lu"
        ],
        "published": "2022-12-19T00:04:56Z",
        "summary": "The recent advent of large language models has reinvigorated debate over\nwhether human cognitive capacities might emerge in such generic models given\nsufficient training data. Of particular interest is the ability of these models\nto reason about novel problems zero-shot, without any direct training. In human\ncognition, this capacity is closely tied to an ability to reason by analogy.\nHere, we performed a direct comparison between human reasoners and a large\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\ntasks, including a non-visual matrix reasoning task based on the rule structure\nof Raven's Standard Progressive Matrices. We found that GPT-3 displayed a\nsurprisingly strong capacity for abstract pattern induction, matching or even\nsurpassing human capabilities in most settings; preliminary tests of GPT-4\nindicated even better performance. Our results indicate that large language\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\nsolutions to a broad range of analogy problems.",
        "pdf_link": "https://arxiv.org/pdf/2212.09196v3.pdf"
    },
    {
        "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
        "authors": [
            "Parishad BehnamGhader",
            "Santiago Miret",
            "Siva Reddy"
        ],
        "published": "2022-12-18T19:27:41Z",
        "summary": "Augmenting pretrained language models with retrievers has shown promise in\neffectively solving common NLP problems, such as language modeling and question\nanswering. In this paper, we evaluate the strengths and weaknesses of popular\nretriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,\nContriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved\nstatements across different tasks. Our findings indicate that the simple\nsimilarity metric employed by retrievers is insufficient for retrieving all the\nnecessary statements for reasoning. Additionally, the language models do not\nexhibit strong reasoning even when provided with only the required statements.\nFurthermore, when combined with imperfect retrievers, the performance of the\nlanguage models becomes even worse, e.g., Flan-T5's performance drops by 28.6%\nwhen retrieving 5 statements using Contriever. While larger language models\nimprove performance, there is still a substantial room for enhancement. Our\nfurther analysis indicates that multihop retrieve-and-read is promising for\nlarge language models like GPT-3.5, but does not generalize to other language\nmodels like Flan-T5-xxl.",
        "pdf_link": "https://arxiv.org/pdf/2212.09146v3.pdf"
    },
    {
        "title": "Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
        "authors": [
            "Chengyue Jiang",
            "Wenyang Hui",
            "Yong Jiang",
            "Xiaobin Wang",
            "Pengjun Xie",
            "Kewei Tu"
        ],
        "published": "2022-12-18T16:42:52Z",
        "summary": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,\npresident, politician) of a given entity mention (e.g., Joe Biden) in context.\nState-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.\nCE concatenates the mention (and its context) with each type and feeds the\npairs into a pretrained language model (PLM) to score their relevance. It\nbrings deeper interaction between mention and types to reach better performance\nbut has to perform N (type set size) forward passes to infer types of a single\nmention. CE is therefore very slow in inference when the type set is large\n(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a\nrecall-expand-filter manner. The recall and expand stages prune the large type\nset and generate K (K is typically less than 256) most relevant type candidates\nfor each mention. At the filter stage, we use a novel model called MCCE to\nconcurrently encode and score these K candidates in only one forward pass to\nobtain the final type prediction. We investigate different variants of MCCE and\nextensive experiments show that MCCE under our paradigm reaches SOTA\nperformance on ultra-fine entity typing and is thousands of times faster than\nthe cross-encoder. We also found MCCE is very effective in fine-grained (130\ntypes) and coarse-grained (9 types) entity typing. Our code is available at\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.",
        "pdf_link": "https://arxiv.org/pdf/2212.09125v1.pdf"
    },
    {
        "title": "Chatbots in a Botnet World",
        "authors": [
            "Forrest McKee",
            "David Noever"
        ],
        "published": "2022-12-18T16:08:40Z",
        "summary": "Question-and-answer formats provide a novel experimental platform for\ninvestigating cybersecurity questions. Unlike previous chatbots, the latest\nChatGPT model from OpenAI supports an advanced understanding of complex coding\nquestions. The research demonstrates thirteen coding tasks that generally\nqualify as stages in the MITRE ATT&CK framework, ranging from credential access\nto defense evasion. With varying success, the experimental prompts generate\nexamples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled\nransomware. The empirical results illustrate cases that support the broad gain\nof functionality, including self-replication and self-modification, evasion,\nand strategic understanding of complex cybersecurity goals. One surprising\nfeature of ChatGPT as a language-only model centers on its ability to spawn\ncoding approaches that yield images that obfuscate or embed executable\nprogramming steps or links.",
        "pdf_link": "https://arxiv.org/pdf/2212.11126v2.pdf"
    },
    {
        "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
        "authors": [
            "Hritik Bansal",
            "Karthik Gopalakrishnan",
            "Saket Dingliwal",
            "Sravan Bodapati",
            "Katrin Kirchhoff",
            "Dan Roth"
        ],
        "published": "2022-12-18T14:36:07Z",
        "summary": "Language models have been shown to perform better with an increase in scale\non a wide variety of tasks via the in-context learning paradigm. In this paper,\nwe investigate the hypothesis that the ability of a large language model to\nin-context learn-perform a task is not uniformly spread across all of its\nunderlying components. Using a 66 billion parameter language model (OPT-66B)\nacross a diverse set of 14 downstream tasks, we find this is indeed the case:\n$\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be\nremoved with minimal decline in task performance. We find substantial overlap\nin the set of attention heads (un)important for in-context learning across\ntasks and number of in-context examples. We also address our hypothesis through\na task-agnostic lens, finding that a small set of attention heads in OPT-66B\nscore highly on their ability to perform primitive induction operations\nassociated with in-context learning, namely, prefix matching and copying. These\ninduction heads overlap with task-specific important heads, reinforcing\narguments by Olsson et al. (arXiv:2209.11895) regarding induction head\ngenerality to more sophisticated behaviors associated with in-context learning.\nOverall, our study provides several insights that indicate large language\nmodels may be under-trained for in-context learning and opens up questions on\nhow to pre-train language models to more effectively perform in-context\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2212.09095v2.pdf"
    },
    {
        "title": "Neural Coreference Resolution based on Reinforcement Learning",
        "authors": [
            "Yu Wang",
            "Hongxia Jin"
        ],
        "published": "2022-12-18T07:36:35Z",
        "summary": "The target of a coreference resolution system is to cluster all mentions that\nrefer to the same entity in a given context. All coreference resolution systems\nneed to solve two subtasks; one task is to detect all of the potential\nmentions, and the other is to learn the linking of an antecedent for each\npossible mention. In this paper, we propose a reinforcement learning\nactor-critic-based neural coreference resolution system, which can achieve both\nmention detection and mention clustering by leveraging an actor-critic deep\nreinforcement learning technique and a joint training algorithm. We experiment\non the BERT model to generate different input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.",
        "pdf_link": "https://arxiv.org/pdf/2212.09028v1.pdf"
    },
    {
        "title": "Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search",
        "authors": [
            "Shuai Wang",
            "Harrisen Scells",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2022-12-18T05:26:40Z",
        "summary": "Medical systematic reviews typically require assessing all the documents\nretrieved by a search. The reason is two-fold: the task aims for ``total\nrecall''; and documents retrieved using Boolean search are an unordered set,\nand thus it is unclear how an assessor could examine only a subset. Screening\nprioritisation is the process of ranking the (unordered) set of retrieved\ndocuments, allowing assessors to begin the downstream processes of the\nsystematic review creation earlier, leading to earlier completion of the\nreview, or even avoiding screening documents ranked least relevant.\n  Screening prioritisation requires highly effective ranking methods.\nPre-trained language models are state-of-the-art on many IR tasks but have yet\nto be applied to systematic review screening prioritisation. In this paper, we\napply several pre-trained language models to the systematic review document\nranking task, both directly and fine-tuned. An empirical analysis compares how\neffective neural methods compare to traditional methods for this task. We also\ninvestigate different types of document representations for neural methods and\ntheir impact on ranking performance.\n  Our results show that BERT-based rankers outperform the current\nstate-of-the-art screening prioritisation methods. However, BERT rankers and\nexisting methods can actually be complementary, and thus, further improvements\nmay be achieved if used in conjunction.",
        "pdf_link": "https://arxiv.org/pdf/2212.09017v1.pdf"
    },
    {
        "title": "Sentence-level Feedback Generation for English Language Learners: Does Data Augmentation Help?",
        "authors": [
            "Shabnam Behzad",
            "Amir Zeldes",
            "Nathan Schneider"
        ],
        "published": "2022-12-18T03:53:44Z",
        "summary": "In this paper, we present strong baselines for the task of Feedback Comment\nGeneration for Writing Learning. Given a sentence and an error span, the task\nis to generate a feedback comment explaining the error. Sentences and feedback\ncomments are both in English. We experiment with LLMs and also create multiple\npseudo datasets for the task, investigating how it affects the performance of\nour system. We present our results for the task along with extensive analysis\nof the generated comments with the aim of aiding future studies in feedback\ncomment generation for English language learners.",
        "pdf_link": "https://arxiv.org/pdf/2212.08999v1.pdf"
    },
    {
        "title": "Language model acceptability judgements are not always robust to context",
        "authors": [
            "Koustuv Sinha",
            "Jon Gauthier",
            "Aaron Mueller",
            "Kanishka Misra",
            "Keren Fuentes",
            "Roger Levy",
            "Adina Williams"
        ],
        "published": "2022-12-18T00:11:06Z",
        "summary": "Targeted syntactic evaluations of language models ask whether models show\nstable preferences for syntactically acceptable content over minimal-pair\nunacceptable inputs. Most targeted syntactic evaluation datasets ask models to\nmake these judgements with just a single context-free sentence as input. This\ndoes not match language models' training regime, in which input sentences are\nalways highly contextualized by the surrounding corpus. This mismatch raises an\nimportant question: how robust are models' syntactic judgements in different\ncontexts? In this paper, we investigate the stability of language models'\nperformance on targeted syntactic evaluations as we vary properties of the\ninput context: the length of the context, the types of syntactic phenomena it\ncontains, and whether or not there are violations of grammaticality. We find\nthat model judgements are generally robust when placed in randomly sampled\nlinguistic contexts. However, they are substantially unstable for contexts\ncontaining syntactic structures matching those in the critical test content.\nAmong all tested models (GPT-2 and five variants of OPT), we significantly\nimprove models' judgements by providing contexts with matching syntactic\nstructures, and conversely significantly worsen them using unacceptable\ncontexts with matching but violated syntactic structures. This effect is\namplified by the length of the context, except for unrelated inputs. We show\nthat these changes in model performance are not explainable by simple features\nmatching the context and the test inputs, such as lexical overlap and\ndependency overlap. This sensitivity to highly specific syntactic features of\nthe context can only be explained by the models' implicit in-context learning\nabilities.",
        "pdf_link": "https://arxiv.org/pdf/2212.08979v1.pdf"
    },
    {
        "title": "Enhancing Cyber Resilience of Networked Microgrids using Vertical Federated Reinforcement Learning",
        "authors": [
            "Sayak Mukherjee",
            "Ramij R. Hossain",
            "Yuan Liu",
            "Wei Du",
            "Veronica Adetola",
            "Sheik M. Mohiuddin",
            "Qiuhua Huang",
            "Tianzhixi Yin",
            "Ankit Singhal"
        ],
        "published": "2022-12-17T22:56:02Z",
        "summary": "This paper presents a novel federated reinforcement learning (Fed-RL)\nmethodology to enhance the cyber resiliency of networked microgrids. We\nformulate a resilient reinforcement learning (RL) training setup which (a)\ngenerates episodic trajectories injecting adversarial actions at primary\ncontrol reference signals of the grid forming (GFM) inverters and (b) trains\nthe RL agents (or controllers) to alleviate the impact of the injected\nadversaries. To circumvent data-sharing issues and concerns for proprietary\nprivacy in multi-party-owned networked grids, we bring in the aspects of\nfederated machine learning and propose a novel Fed-RL algorithm to train the RL\nagents. To this end, the conventional horizontal Fed-RL approaches using\ndecoupled independent environments fail to capture the coupled dynamics in a\nnetworked microgrid, which leads us to propose a multi-agent vertically\nfederated variation of actor-critic algorithms, namely federated soft\nactor-critic (FedSAC) algorithm. We created a customized simulation setup\nencapsulating microgrid dynamics in the GridLAB-D/HELICS co-simulation platform\ncompatible with the OpenAI Gym interface for training RL agents. Finally, the\nproposed methodology is validated with numerical examples of modified IEEE\n123-bus benchmark test systems consisting of three coupled microgrids.",
        "pdf_link": "https://arxiv.org/pdf/2212.08973v1.pdf"
    },
    {
        "title": "Graph Learning and Its Advancements on Large Language Models: A Holistic Survey",
        "authors": [
            "Shaopeng Wei",
            "Yu Zhao",
            "Xingyan Chen",
            "Qing Li",
            "Fuzhen Zhuang",
            "Ji Liu",
            "Fuji Ren",
            "Gang Kou"
        ],
        "published": "2022-12-17T22:05:07Z",
        "summary": "Graph learning is a prevalent domain that endeavors to learn the intricate\nrelationships among nodes and the topological structure of graphs. Over the\nyears, graph learning has transcended from graph theory to graph data mining.\nWith the advent of representation learning, it has attained remarkable\nperformance in diverse scenarios. Owing to its extensive application prospects,\ngraph learning attracts copious attention. While some researchers have\naccomplished impressive surveys on graph learning, they failed to connect\nrelated objectives, methods, and applications in a more coherent way. As a\nresult, they did not encompass current ample scenarios and challenging problems\ndue to the rapid expansion of graph learning. Particularly, large language\nmodels have recently had a disruptive effect on human life, but they also show\nrelative weakness in structured scenarios. The question of how to make these\nmodels more powerful with graph learning remains open. Our survey focuses on\nthe most recent advancements in integrating graph learning with pre-trained\nlanguage models, specifically emphasizing their application within the domain\nof large language models. Different from previous surveys on graph learning, we\nprovide a holistic review that analyzes current works from the perspective of\ngraph structure, and discusses the latest applications, trends, and challenges\nin graph learning. Specifically, we commence by proposing a taxonomy and then\nsummarize the methods employed in graph learning. We then provide a detailed\nelucidation of mainstream applications. Finally, we propose future directions.",
        "pdf_link": "https://arxiv.org/pdf/2212.08966v4.pdf"
    },
    {
        "title": "Claim Optimization in Computational Argumentation",
        "authors": [
            "Gabriella Skitalinskaya",
            "Maximilian Splieth\u00f6ver",
            "Henning Wachsmuth"
        ],
        "published": "2022-12-17T16:30:27Z",
        "summary": "An optimal delivery of arguments is key to persuasion in any debate, both for\nhumans and for AI systems. This requires the use of clear and fluent claims\nrelevant to the given debate. Prior work has studied the automatic assessment\nof argument quality extensively. Yet, no approach actually improves the quality\nso far. To fill this gap, this paper proposes the task of claim optimization:\nto rewrite argumentative claims in order to optimize their delivery. As\nmultiple types of optimization are possible, we approach this task by first\ngenerating a diverse set of candidate claims using a large language model, such\nas BART, taking into account contextual information. Then, the best candidate\nis selected using various quality metrics. In automatic and human evaluation on\nan English-language corpus, our quality-based candidate selection outperforms\nseveral baselines, improving 60% of all claims (worsening 16% only). Follow-up\nanalyses reveal that, beyond copy editing, our approach often specifies claims\nwith details, whereas it adds less evidence than humans do. Moreover, its\ncapabilities generalize well to other domains, such as instructional texts.",
        "pdf_link": "https://arxiv.org/pdf/2212.08913v2.pdf"
    },
    {
        "title": "Exploiting Rich Textual User-Product Context for Improving Sentiment Analysis",
        "authors": [
            "Chenyang Lyu",
            "Linyi Yang",
            "Yue Zhang",
            "Yvette Graham",
            "Jennifer Foster"
        ],
        "published": "2022-12-17T14:57:52Z",
        "summary": "User and product information associated with a review is useful for sentiment\npolarity prediction. Typical approaches incorporating such information focus on\nmodeling users and products as implicitly learned representation vectors. Most\ndo not exploit the potential of historical reviews, or those that currently do\nrequire unnecessary modifications to model architecture or do not make full use\nof user/product associations. The contribution of this work is twofold: i) a\nmethod to explicitly employ historical reviews belonging to the same\nuser/product to initialize representations, and ii) efficient incorporation of\ntextual associations between users and products via a user-product\ncross-context module. Experiments on IMDb, Yelp-2013 and Yelp-2014 benchmarks\nshow that our approach substantially outperforms previous state-of-the-art.\nSince we employ BERT-base as the encoder, we additionally provide experiments\nin which our approach performs well with Span-BERT and Longformer. Furthermore,\nexperiments where the reviews of each user/product in the training data are\ndownsampled demonstrate the effectiveness of our approach under a low-resource\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2212.08888v1.pdf"
    },
    {
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Chuanqi Tan",
            "Fei Huang",
            "Songfang Huang"
        ],
        "published": "2022-12-17T11:56:21Z",
        "summary": "Language models with the Transformers structure have shown great performance\nin natural language processing. However, there still poses problems when\nfine-tuning pre-trained language models on downstream tasks, such as\nover-fitting or representation collapse. In this work, we propose HyPe, a\nsimple yet effective fine-tuning technique to alleviate such problems by\nperturbing hidden representations of Transformers layers. Unlike previous works\nthat only add noise to inputs or parameters, we argue that the hidden\nrepresentations of Transformers layers convey more diverse and meaningful\nlanguage information. Therefore, making the Transformers layers more robust to\nhidden representation perturbations can further benefit the fine-tuning of PLMs\nen bloc. We conduct extensive experiments and analyses on GLUE and other\nnatural language inference datasets. Results demonstrate that HyPe outperforms\nvanilla fine-tuning and enhances generalization of hidden representations from\ndifferent layers. In addition, HyPe acquires negligible computational\noverheads, and is better than and compatible with previous state-of-the-art\nfine-tuning techniques.",
        "pdf_link": "https://arxiv.org/pdf/2212.08853v2.pdf"
    },
    {
        "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
        "authors": [
            "Alex Nichol",
            "Heewoo Jun",
            "Prafulla Dhariwal",
            "Pamela Mishkin",
            "Mark Chen"
        ],
        "published": "2022-12-16T23:22:59Z",
        "summary": "While recent work on text-conditional 3D object generation has shown\npromising results, the state-of-the-art methods typically require multiple\nGPU-hours to produce a single sample. This is in stark contrast to\nstate-of-the-art generative image models, which produce samples in a number of\nseconds or minutes. In this paper, we explore an alternative method for 3D\nobject generation which produces 3D models in only 1-2 minutes on a single GPU.\nOur method first generates a single synthetic view using a text-to-image\ndiffusion model, and then produces a 3D point cloud using a second diffusion\nmodel which conditions on the generated image. While our method still falls\nshort of the state-of-the-art in terms of sample quality, it is one to two\norders of magnitude faster to sample from, offering a practical trade-off for\nsome use cases. We release our pre-trained point cloud diffusion models, as\nwell as evaluation code and models, at https://github.com/openai/point-e.",
        "pdf_link": "https://arxiv.org/pdf/2212.08751v1.pdf"
    },
    {
        "title": "Neural Story Planning",
        "authors": [
            "Anbang Ye",
            "Christopher Cui",
            "Taiwei Shi",
            "Mark O. Riedl"
        ],
        "published": "2022-12-16T21:29:41Z",
        "summary": "Automated plot generation is the challenge of generating a sequence of events\nthat will be perceived by readers as the plot of a coherent story. Traditional\nsymbolic planners plan a story from a goal state and guarantee logical causal\nplot coherence but rely on a library of hand-crafted actions with their\npreconditions and effects. This closed world setting limits the length and\ndiversity of what symbolic planners can generate. On the other hand,\npre-trained neural language models can generate stories with great diversity,\nwhile being generally incapable of ending a story in a specified manner and can\nhave trouble maintaining coherence. In this paper, we present an approach to\nstory plot generation that unifies causal planning with neural language models.\nWe propose to use commonsense knowledge extracted from large language models to\nrecursively expand a story plot in a backward chaining fashion. Specifically,\nour system infers the preconditions for events in the story and then events\nthat will cause those conditions to become true. We performed automatic\nevaluation to measure narrative coherence as indicated by the ability to answer\nquestions about whether different events in the story are causally related to\nother events. Results indicate that our proposed method produces more coherent\nplotlines than several strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2212.08718v1.pdf"
    },
    {
        "title": "Plansformer: Generating Symbolic Plans using Transformers",
        "authors": [
            "Vishal Pallagani",
            "Bharath Muppasani",
            "Keerthiram Murugesan",
            "Francesca Rossi",
            "Lior Horesh",
            "Biplav Srivastava",
            "Francesco Fabiano",
            "Andrea Loreggia"
        ],
        "published": "2022-12-16T19:06:49Z",
        "summary": "Large Language Models (LLMs) have been the subject of active research,\nsignificantly advancing the field of Natural Language Processing (NLP). From\nBERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural\nlanguage tasks such as question answering, summarization, and text generation.\nMany ongoing efforts focus on understanding LLMs' capabilities, including their\nknowledge of the world, syntax, and semantics. However, extending the textual\nprowess of LLMs to symbolic reasoning has been slow and predominantly focused\non tackling problems related to the mathematical field. In this paper, we\nexplore the use of LLMs for automated planning - a branch of AI concerned with\nthe realization of action sequences (plans) to achieve a goal, typically\nexecuted by intelligent agents, autonomous robots, and unmanned vehicles. We\nintroduce Plansformer; an LLM fine-tuned on planning problems and capable of\ngenerating plans with favorable behavior in terms of correctness and length\nwith reduced knowledge-engineering efforts. We also demonstrate the\nadaptability of Plansformer in solving different planning domains with varying\ncomplexities, owing to the transfer learning abilities of LLMs. For one\nconfiguration of Plansformer, we achieve ~97% valid plans, out of which ~95%\nare optimal for Towers of Hanoi - a puzzle-solving domain.",
        "pdf_link": "https://arxiv.org/pdf/2212.08681v1.pdf"
    },
    {
        "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
        "authors": [
            "Junlong Li",
            "Jinyuan Wang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022-12-16T18:23:43Z",
        "summary": "Open-Domain Question Answering (ODQA) aims to answer questions without\nexplicitly providing specific background documents. This task becomes notably\nchallenging in a zero-shot setting where no data is available to train tailored\nretrieval-reader models. While recent Large Language Models (LLMs) like GPT-3\nhave demonstrated their effectiveness in zero-shot ODQA using direct prompting\nmethods, these methods still fall short of fully harnessing the potential of\nLLMs when implicitly invoked. In this paper, we propose a Self-Prompting\nframework to explicitly utilize the massive knowledge encoded in the parameters\nof LLMs and their strong instruction understanding abilities. Concretely, we\nprompt LLMs step by step to generate multiple pseudo QA pairs with background\npassages and explanations entirely from scratch. These generated elements are\nthen utilized for in-context learning. Experimental results show that our\nmethod significantly surpasses previous state-of-the-art zero-shot methods on\nthree widely-used ODQA datasets and even achieves comparable performance with\nvarious customized fine-tuned models on full training data. Our code is\navailable at https://github.com/lockon-n/self-prompting.",
        "pdf_link": "https://arxiv.org/pdf/2212.08635v3.pdf"
    },
    {
        "title": "Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation",
        "authors": [
            "Qian Yang",
            "Qian Chen",
            "Wen Wang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2022-12-16T18:12:04Z",
        "summary": "Multi-modal multi-hop question answering involves answering a question by\nreasoning over multiple input sources from different modalities. Existing\nmethods often retrieve evidences separately and then use a language model to\ngenerate an answer based on the retrieved evidences, and thus do not adequately\nconnect candidates and are unable to model the interdependent relations during\nretrieval. Moreover, the pipelined approaches of retrieval and generation might\nresult in poor generation performance when retrieval performance is low. To\naddress these issues, we propose a Structured Knowledge and Unified\nRetrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion\nEncoder to align sources from different modalities using shared entities. It\nthen uses a unified Retrieval-Generation Decoder to integrate intermediate\nretrieval results for answer generation and also adaptively determine the\nnumber of retrieval steps. Extensive experiments on two representative\nmulti-modal multi-hop QA datasets MultimodalQA and WebQA demonstrate that SKURG\noutperforms the state-of-the-art models in both source retrieval and answer\ngeneration performance with fewer parameters. Our code is available at\nhttps://github.com/HITsz-TMG/SKURG.",
        "pdf_link": "https://arxiv.org/pdf/2212.08632v2.pdf"
    },
    {
        "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
        "authors": [
            "Swarnadeep Saha",
            "Xinyan Velocity Yu",
            "Mohit Bansal",
            "Ramakanth Pasunuru",
            "Asli Celikyilmaz"
        ],
        "published": "2022-12-16T17:36:23Z",
        "summary": "Prompting large language models has enabled significant recent progress in\nmulti-step reasoning over text. However, when applied to text generation from\nsemi-structured data (e.g., graphs or tables), these methods typically suffer\nfrom low semantic coverage, hallucination, and logical inconsistency. We\npropose MURMUR, a neuro-symbolic modular approach to text generation from\nsemi-structured data with multi-step reasoning. MURMUR is a best-first search\nmethod that generates reasoning paths using: (1) neural and symbolic modules\nwith specific linguistic and logical skills, (2) a grammar whose production\nrules define valid compositions of modules, and (3) value functions that assess\nthe quality of each reasoning step. We conduct experiments on two diverse\ndata-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in\ntheir data representations (graphs and tables) and span multiple linguistic and\nlogical skills. MURMUR obtains significant improvements over recent few-shot\nbaselines like direct prompting and chain-of-thought prompting, while also\nachieving comparable performance to fine-tuned GPT-2 on out-of-domain data.\nMoreover, human evaluation shows that MURMUR generates highly faithful and\ncorrect reasoning paths that lead to 26% more logically consistent summaries on\nLogicNLG, compared to direct prompting.",
        "pdf_link": "https://arxiv.org/pdf/2212.08607v1.pdf"
    },
    {
        "title": "POIBERT: A Transformer-based Model for the Tour Recommendation Problem",
        "authors": [
            "Ngai Lam Ho",
            "Kwan Hui Lim"
        ],
        "published": "2022-12-16T12:32:15Z",
        "summary": "Tour itinerary planning and recommendation are challenging problems for\ntourists visiting unfamiliar cities. Many tour recommendation algorithms only\nconsider factors such as the location and popularity of Points of Interest\n(POIs) but their solutions may not align well with the user's own preferences\nand other location constraints. Additionally, these solutions do not take into\nconsideration of the users' preference based on their past POIs selection. In\nthis paper, we propose POIBERT, an algorithm for recommending personalized\nitineraries using the BERT language model on POIs. POIBERT builds upon the\nhighly successful BERT language model with the novel adaptation of a language\nmodel to our itinerary recommendation task, alongside an iterative approach to\ngenerate consecutive POIs.\n  Our recommendation algorithm is able to generate a sequence of POIs that\noptimizes time and users' preference in POI categories based on past\ntrajectories from similar tourists. Our tour recommendation algorithm is\nmodeled by adapting the itinerary recommendation problem to the sentence\ncompletion problem in natural language processing (NLP). We also innovate an\niterative algorithm to generate travel itineraries that satisfies the time\nconstraints which is most likely from past trajectories. Using a Flickr dataset\nof seven cities, experimental results show that our algorithm out-performs many\nsequence prediction algorithms based on measures in recall, precision and\nF1-scores.",
        "pdf_link": "https://arxiv.org/pdf/2212.13900v1.pdf"
    },
    {
        "title": "Teaching Small Language Models to Reason",
        "authors": [
            "Lucie Charlotte Magister",
            "Jonathan Mallinson",
            "Jakub Adamek",
            "Eric Malmi",
            "Aliaksei Severyn"
        ],
        "published": "2022-12-16T11:24:42Z",
        "summary": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.",
        "pdf_link": "https://arxiv.org/pdf/2212.08410v3.pdf"
    },
    {
        "title": "FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks",
        "authors": [
            "Weilong Dong",
            "Xinwei Wu",
            "Junzhuo Li",
            "Shuangzhi Wu",
            "Chao Bian",
            "Deyi Xiong"
        ],
        "published": "2022-12-16T09:01:56Z",
        "summary": "Massively multi-task learning with large language models has recently made\nsubstantial progress on few-shot generalization. However, this is usually\nperformed in a centralized learning fashion, ignoring the privacy sensitivity\nissue of (annotated) data used in multiple tasks. To mitigate this issue, we\npropose FewFedWeight, a few-shot federated learning framework across multiple\ntasks, to achieve the best of both worlds: privacy preservation and cross-task\ngeneralization. FewFedWeight trains client models in isolated devices without\nsharing data. It broadcasts the global model in the server to each client and\nproduces pseudo data for clients so that knowledge from the global model can be\nexplored to enhance few-shot learning of each client model. An energy-based\nalgorithm is further proposed to weight pseudo samples in order to reduce the\nnegative impact of noise from the generated pseudo data. Adaptive model weights\nof client models are also tuned according to their performance. We use these\nmodel weights to dynamically aggregate client models to update the global\nmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantly\nimprove the performance of client models on 61% tasks with an average\nperformance improvement rate of 30.5% over the baseline and substantially\noutperform FedAvg and other decentralized learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.08354v1.pdf"
    },
    {
        "title": "ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Zhongyang Li",
            "Li Du",
            "Bing Qin",
            "Yi Zheng",
            "Baoxing Huai"
        ],
        "published": "2022-12-16T07:48:02Z",
        "summary": "Causal chain reasoning (CCR) is an essential ability for many decision-making\nAI systems, which requires the model to build reliable causal chains by\nconnecting causal pairs. However, CCR suffers from two main transitive\nproblems: threshold effect and scene drift. In other words, the causal pairs to\nbe spliced may have a conflicting threshold boundary or scenario. To address\nthese issues, we propose a novel Reliable Causal chain reasoning\nframework~(ReCo), which introduces exogenous variables to represent the\nthreshold and scene factors of each causal pair within the causal chain, and\nestimates the threshold and scene contradictions across exogenous variables via\nstructural causal recurrent neural networks~(SRNN). Experiments show that ReCo\noutperforms a series of strong baselines on both Chinese and English CCR\ndatasets. Moreover, by injecting reliable causal chain knowledge distilled by\nReCo, BERT can achieve better performances on four downstream causal-related\ntasks than BERT models enhanced by other kinds of knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2212.08322v1.pdf"
    },
    {
        "title": "Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language",
        "authors": [
            "Yusuke Yasuda",
            "Tomoki Toda"
        ],
        "published": "2022-12-16T07:47:03Z",
        "summary": "End-to-end text-to-speech synthesis (TTS) can generate highly natural\nsynthetic speech from raw text. However, rendering the correct pitch accents is\nstill a challenging problem for end-to-end TTS. To tackle the challenge of\nrendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a\nself-supervised pretrained model in the character and phoneme domain for TTS.\nWe investigate the effects of features captured by PnG~BERT on Japanese TTS by\nmodifying the fine-tuning condition to determine the conditions helpful\ninferring pitch accents. We manipulate content of PnG~BERT features from being\ntext-oriented to speech-oriented by changing the number of fine-tuned layers\nduring TTS. In addition, we teach PnG~BERT pitch accent information by\nfine-tuning with tone prediction as an additional downstream task. Our\nexperimental results show that the features of PnG~BERT captured by pretraining\ncontain information helpful inferring pitch accent, and PnG~BERT outperforms\nbaseline Tacotron on accent correctness in a listening test.",
        "pdf_link": "https://arxiv.org/pdf/2212.08321v1.pdf"
    },
    {
        "title": "ALERT: Adapting Language Models to Reasoning Tasks",
        "authors": [
            "Ping Yu",
            "Tianlu Wang",
            "Olga Golovneva",
            "Badr AlKhamissi",
            "Siddharth Verma",
            "Zhijing Jin",
            "Gargi Ghosh",
            "Mona Diab",
            "Asli Celikyilmaz"
        ],
        "published": "2022-12-16T05:15:41Z",
        "summary": "Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems.",
        "pdf_link": "https://arxiv.org/pdf/2212.08286v2.pdf"
    },
    {
        "title": "LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension",
        "authors": [
            "Wenyue Hua",
            "Yuchen Zhang",
            "Zhe Chen",
            "Josie Li",
            "Melanie Weber"
        ],
        "published": "2022-12-16T00:15:14Z",
        "summary": "The application of Natural Language Processing (NLP) to specialized domains,\nsuch as the law, has recently received a surge of interest. As many legal\nservices rely on processing and analyzing large collections of documents,\nautomating such tasks with NLP tools emerges as a key challenge. Many popular\nlanguage models, such as BERT or RoBERTa, are general-purpose models, which\nhave limitations on processing specialized legal terminology and syntax. In\naddition, legal documents may contain specialized vocabulary from other\ndomains, such as medical terminology in personal injury text. Here, we propose\nLegalRelectra, a legal-domain language model that is trained on mixed-domain\nlegal and medical corpora. We show that our model improves over general-domain\nand single-domain medical and legal language models when processing\nmixed-domain (personal injury) text. Our training architecture implements the\nElectra framework, but utilizes Reformer instead of BERT for its generator and\ndiscriminator. We show that this improves the model's performance on processing\nlong passages and results in better long-range text comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2212.08204v1.pdf"
    },
    {
        "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
        "authors": [
            "Michiel de Jong",
            "Yury Zemlyanskiy",
            "Joshua Ainslie",
            "Nicholas FitzGerald",
            "Sumit Sanghai",
            "Fei Sha",
            "William Cohen"
        ],
        "published": "2022-12-15T21:35:46Z",
        "summary": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, the\narchitecture used for FiD was chosen by making minimal modifications to a\nstandard T5 model, which our analysis shows to be highly suboptimal for a\nretrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to\nthe encoder, while the majority of inference time results from memory bandwidth\nconstraints in the decoder. We propose two simple changes to the FiD\narchitecture to alleviate memory bandwidth constraints, and speed up inference\nby 7x. This allows us to use a much larger decoder at modest cost. We denote\nFiD with the above modifications as FiDO, and show that it strongly improves\nperformance over existing FiD models for a wide range of inference budgets. For\nexample, FiDO-Large-XXL performs faster inference than FiD-Base and achieves\nbetter performance than FiD-Large.",
        "pdf_link": "https://arxiv.org/pdf/2212.08153v2.pdf"
    },
    {
        "title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection",
        "authors": [
            "Benjamin Steenhoek",
            "Hongyang Gao",
            "Wei Le"
        ],
        "published": "2022-12-15T19:49:27Z",
        "summary": "Deep learning-based vulnerability detection has shown great performance and,\nin some studies, outperformed static analysis tools. However, the\nhighest-performing approaches use token-based transformer models, which are not\nthe most efficient to capture code semantics required for vulnerability\ndetection. Classical program analysis techniques such as dataflow analysis can\ndetect many types of bugs based on their root causes. In this paper, we propose\nto combine such causal-based vulnerability detection algorithms with deep\nlearning, aiming to achieve more efficient and effective vulnerability\ndetection. Specifically, we designed DeepDFA, a dataflow analysis-inspired\ngraph learning framework and an embedding technique that enables graph learning\nto simulate dataflow computation. We show that DeepDFA is both performant and\nefficient. DeepDFA outperformed all non-transformer baselines. It was trained\nin 9 minutes, 75x faster than the highest-performing baseline model. When using\nonly 50+ vulnerable and several hundreds of total examples as training data,\nthe model retained the same performance as 100% of the dataset. DeepDFA also\ngeneralized to real-world vulnerabilities in DbgBench; it detected 8.7 out of\n17 vulnerabilities on average across folds and was able to distinguish between\npatched and buggy versions, while the highest-performing baseline models did\nnot detect any vulnerabilities. By combining DeepDFA with a large language\nmodel, we surpassed the state-of-the-art vulnerability detection performance on\nthe Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our\nreplication package is located at https://doi.org/10.6084/m9.figshare.21225413 .",
        "pdf_link": "https://arxiv.org/pdf/2212.08108v3.pdf"
    },
    {
        "title": "Joint processing of linguistic properties in brains and language models",
        "authors": [
            "Subba Reddy Oota",
            "Manish Gupta",
            "Mariya Toneva"
        ],
        "published": "2022-12-15T19:13:42Z",
        "summary": "Language models have been shown to be very effective in predicting brain\nrecordings of subjects experiencing complex language stimuli. For a deeper\nunderstanding of this alignment, it is important to understand the\ncorrespondence between the detailed processing of linguistic information by the\nhuman brain versus language models. We investigate this correspondence via a\ndirect approach, in which we eliminate information related to specific\nlinguistic properties in the language model representations and observe how\nthis intervention affects the alignment with fMRI brain recordings obtained\nwhile participants listened to a story. We investigate a range of linguistic\nproperties (surface, syntactic, and semantic) and find that the elimination of\neach one results in a significant decrease in brain alignment. Specifically, we\nfind that syntactic properties (i.e. Top Constituents and Tree Depth) have the\nlargest effect on the trend of brain alignment across model layers. These\nfindings provide clear evidence for the role of specific linguistic information\nin the alignment between brain and language models, and open new avenues for\nmapping the joint information processing in both systems. We make the code\npublicly available\n[https://github.com/subbareddy248/linguistic-properties-brain-alignment].",
        "pdf_link": "https://arxiv.org/pdf/2212.08094v2.pdf"
    },
    {
        "title": "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
        "authors": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "published": "2022-12-15T18:59:32Z",
        "summary": "Generating a Chain of Thought (CoT) has been shown to consistently improve\nlarge language model (LLM) performance on a wide range of NLP tasks. However,\nprior work has mainly focused on logical reasoning tasks (e.g. arithmetic,\ncommonsense QA); it remains unclear whether improvements hold for more diverse\ntypes of reasoning, especially in socially situated contexts. Concretely, we\nperform a controlled evaluation of zero-shot CoT across two socially sensitive\ndomains: harmful questions and stereotype benchmarks. We find that zero-shot\nCoT reasoning in sensitive domains significantly increases a model's likelihood\nto produce harmful or undesirable output, with trends holding across different\nprompt formats and model variants. Furthermore, we show that harmful CoTs\nincrease with model size, but decrease with improved instruction following. Our\nwork suggests that zero-shot CoT should be used with caution on socially\nimportant tasks, especially when marginalized groups or sensitive topics are\ninvolved.",
        "pdf_link": "https://arxiv.org/pdf/2212.08061v2.pdf"
    },
    {
        "title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
        "authors": [
            "Bernd Bohnet",
            "Vinh Q. Tran",
            "Pat Verga",
            "Roee Aharoni",
            "Daniel Andor",
            "Livio Baldini Soares",
            "Massimiliano Ciaramita",
            "Jacob Eisenstein",
            "Kuzman Ganchev",
            "Jonathan Herzig",
            "Kai Hui",
            "Tom Kwiatkowski",
            "Ji Ma",
            "Jianmo Ni",
            "Lierni Sestorain Saralegui",
            "Tal Schuster",
            "William W. Cohen",
            "Michael Collins",
            "Dipanjan Das",
            "Donald Metzler",
            "Slav Petrov",
            "Kellie Webster"
        ],
        "published": "2022-12-15T18:45:29Z",
        "summary": "Large language models (LLMs) have shown impressive results while requiring\nlittle or no direct supervision. Further, there is mounting evidence that LLMs\nmay have potential in information-seeking scenarios. We believe the ability of\nan LLM to attribute the text that it generates is likely to be crucial in this\nsetting. We formulate and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We propose a reproducible evaluation framework\nfor the task and benchmark a broad set of architectures. We take human\nannotations as a gold standard and show that a correlated automatic metric is\nsuitable for development. Our experimental work gives concrete answers to two\nkey questions (How to measure attribution?, and How well do current\nstate-of-the-art methods perform on attribution?), and give some hints as to\nhow to address a third (How to build LLMs with attribution?).",
        "pdf_link": "https://arxiv.org/pdf/2212.08037v2.pdf"
    },
    {
        "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
        "authors": [
            "Yixin Liu",
            "Alexander R. Fabbri",
            "Pengfei Liu",
            "Yilun Zhao",
            "Linyong Nan",
            "Ruilin Han",
            "Simeng Han",
            "Shafiq Joty",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Dragomir Radev"
        ],
        "published": "2022-12-15T17:26:05Z",
        "summary": "Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation studies for summarization either exhibit a low inter-annotator\nagreement or have insufficient scale, and an in-depth analysis of human\nevaluation is lacking. Therefore, we address the shortcomings of existing\nsummarization evaluation along the following axes: (1) We propose a modified\nsummarization salience protocol, Atomic Content Units (ACUs), which is based on\nfine-grained semantic units and allows for a high inter-annotator agreement.\n(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large\nhuman evaluation dataset consisting of 22,000 summary-level annotations over 28\ntop-performing systems on three datasets. (3) We conduct a comparative study of\nfour human evaluation protocols, underscoring potential confounding factors in\nevaluation setups. (4) We evaluate 50 automatic metrics and their variants\nusing the collected human annotations across evaluation protocols and\ndemonstrate how our benchmark leads to more statistically stable and\nsignificant results. The metrics we benchmarked include recent methods based on\nlarge language models (LLMs), GPTScore and G-Eval. Furthermore, our findings\nhave important implications for evaluating LLMs, as we show that LLMs adjusted\nby human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,\nwhich is affected by the annotators' prior, input-agnostic preferences, calling\nfor more robust, targeted evaluation methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.07981v2.pdf"
    },
    {
        "title": "Visually-augmented pretrained language models for NLP tasks without images",
        "authors": [
            "Hangyu Guo",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Qinyu Zhang",
            "Ji-Rong Wen"
        ],
        "published": "2022-12-15T16:13:25Z",
        "summary": "Although pre-trained language models~(PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense. Existing solutions often rely on explicit images for visual\nknowledge augmentation (requiring time-consuming retrieval or generation), and\nthey also conduct the augmentation for the whole input text, without\nconsidering whether it is actually needed in specific inputs or tasks. To\naddress these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented\nfine-tuning approach that can be generally applied to various PLMs or NLP\ntasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages,\nnamely \\textbf{VAWI}. Experimental results show that our approach can\nconsistently improve the performance of BERT, RoBERTa, BART, and T5 at\ndifferent scales, and outperform several competitive baselines on ten tasks.\nOur codes and data are publicly available\nat~\\url{https://github.com/RUCAIBox/VAWI}.",
        "pdf_link": "https://arxiv.org/pdf/2212.07937v2.pdf"
    },
    {
        "title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
        "authors": [
            "Olga Golovneva",
            "Moya Chen",
            "Spencer Poff",
            "Martin Corredor",
            "Luke Zettlemoyer",
            "Maryam Fazel-Zarandi",
            "Asli Celikyilmaz"
        ],
        "published": "2022-12-15T15:52:39Z",
        "summary": "Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.",
        "pdf_link": "https://arxiv.org/pdf/2212.07919v2.pdf"
    },
    {
        "title": "The Effects of In-domain Corpus Size on pre-training BERT",
        "authors": [
            "Chris Sanchez",
            "Zheyuan Zhang"
        ],
        "published": "2022-12-15T15:49:27Z",
        "summary": "Many prior language modeling efforts have shown that pre-training on an\nin-domain corpus can significantly improve performance on downstream\ndomain-specific NLP tasks. However, the difficulties associated with collecting\nenough in-domain data might discourage researchers from approaching this\npre-training task. In this paper, we conducted a series of experiments by\npre-training Bidirectional Encoder Representations from Transformers (BERT)\nwith different sizes of biomedical corpora. The results demonstrate that\npre-training on a relatively small amount of in-domain data (4GB) with limited\ntraining steps, can lead to better performance on downstream domain-specific\nNLP tasks compared with fine-tuning models pre-trained on general corpora.",
        "pdf_link": "https://arxiv.org/pdf/2212.07914v1.pdf"
    },
    {
        "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
        "authors": [
            "Kun Zhou",
            "Xiao Liu",
            "Yeyun Gong",
            "Wayne Xin Zhao",
            "Daxin Jiang",
            "Nan Duan",
            "Ji-Rong Wen"
        ],
        "published": "2022-12-15T13:57:07Z",
        "summary": "Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense\nretrieval methods for parameter initialization, and recent studies are\nexploring more effective pre-training tasks for further improving the quality\nof dense vectors. Although various novel and effective tasks have been\nproposed, their different input formats and learning objectives make them hard\nto be integrated for jointly improving the model performance. In this work, we\naim to unify a variety of pre-training tasks into the bottlenecked masked\nautoencoder manner, and integrate them into a multi-task pre-trained model,\nnamely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder\narchitecture that can construct a representation bottleneck to compress the\nabundant semantic information across tasks into dense vectors. Based on it, we\nintegrate three types of representative pre-training tasks: corrupted passages\nrecovering, related passages recovering and PLMs outputs recovering, to\ncharacterize the inner-passage information, inter-passage relations and PLMs\nknowledge. Extensive experiments have shown that our approach outperforms\ncompetitive dense retrieval methods. Our code and data are publicly released in\n\\url{https://github.com/microsoft/SimXNS}.",
        "pdf_link": "https://arxiv.org/pdf/2212.07841v2.pdf"
    },
    {
        "title": "Enhancing Indic Handwritten Text Recognition Using Global Semantic Information",
        "authors": [
            "Ajoy Mondal",
            "C. V. Jawahar"
        ],
        "published": "2022-12-15T12:53:26Z",
        "summary": "Handwritten Text Recognition (HTR) is more interesting and challenging than\nprinted text due to uneven variations in the handwriting style of the writers,\ncontent, and time. HTR becomes more challenging for the Indic languages because\nof (i) multiple characters combined to form conjuncts which increase the number\nof characters of respective languages, and (ii) near to 100 unique basic\nUnicode characters in each Indic script. Recently, many recognition methods\nbased on the encoder-decoder framework have been proposed to handle such\nproblems. They still face many challenges, such as image blur and incomplete\ncharacters due to varying writing styles and ink density. We argue that most\nencoder-decoder methods are based on local visual features without explicit\nglobal semantic information.\n  In this work, we enhance the performance of Indic handwritten text\nrecognizers using global semantic information. We use a semantic module in an\nencoder-decoder framework for extracting global semantic information to\nrecognize the Indic handwritten texts. The semantic information is used in both\nthe encoder for supervision and the decoder for initialization. The semantic\ninformation is predicted from the word embedding of a pre-trained language\nmodel. Extensive experiments demonstrate that the proposed framework achieves\nstate-of-the-art results on handwritten texts of ten Indic languages.",
        "pdf_link": "https://arxiv.org/pdf/2212.07776v1.pdf"
    },
    {
        "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
        "authors": [
            "Mingyu Lee",
            "Jun-Hyung Park",
            "Junho Kim",
            "Kang-Min Kim",
            "SangKeun Lee"
        ],
        "published": "2022-12-15T05:01:59Z",
        "summary": "Masked language modeling (MLM) has been widely used for pre-training\neffective bidirectional representations, but incurs substantial training costs.\nIn this paper, we propose a novel concept-based curriculum masking (CCM) method\nto efficiently pre-train a language model. CCM has two key differences from\nexisting curriculum learning approaches to effectively reflect the nature of\nMLM. First, we introduce a carefully-designed linguistic difficulty criterion\nthat evaluates the MLM difficulty of each token. Second, we construct a\ncurriculum that gradually masks words related to the previously masked words by\nretrieving a knowledge graph. Experimental results show that CCM significantly\nimproves pre-training efficiency. Specifically, the model trained with CCM\nshows comparative performance with the original BERT on the General Language\nUnderstanding Evaluation benchmark at half of the training cost.",
        "pdf_link": "https://arxiv.org/pdf/2212.07617v1.pdf"
    },
    {
        "title": "DeepJoin: Joinable Table Discovery with Pre-trained Language Models",
        "authors": [
            "Yuyang Dong",
            "Chuan Xiao",
            "Takuma Nozawa",
            "Masafumi Enomoto",
            "Masafumi Oyamada"
        ],
        "published": "2022-12-15T02:40:57Z",
        "summary": "Due to the usefulness in data enrichment for data analysis tasks, joinable\ntable discovery has become an important operation in data lake management.\nExisting approaches target equi-joins, the most common way of combining tables\nfor creating a unified view, or semantic joins, which tolerate misspellings and\ndifferent formats to deliver more join results. They are either exact solutions\nwhose running time is linear in the sizes of query column and target table\nrepository or approximate solutions lacking precision. In this paper, we\npropose Deepjoin, a deep learning model for accurate and efficient joinable\ntable discovery. Our solution is an embedding-based retrieval, which employs a\npre-trained language model (PLM) and is designed as one framework serving both\nequi- and semantic joins. We propose a set of contextualization options to\ntransform column contents to a text sequence. The PLM reads the sequence and is\nfine-tuned to embed columns to vectors such that columns are expected to be\njoinable if they are close to each other in the vector space. Since the output\nof the PLM is fixed in length, the subsequent search procedure becomes\nindependent of the column size. With a state-of-the-art approximate nearest\nneighbor search algorithm, the search time is logarithmic in the repository\nsize. To train the model, we devise the techniques for preparing training data\nas well as data augmentation. The experiments on real datasets demonstrate that\nby training on a small subset of a corpus, Deepjoin generalizes to large\ndatasets and its precision consistently outperforms other approximate\nsolutions'. Deepjoin is even more accurate than an exact solution to semantic\njoins when evaluated with labels from experts. Moreover, when equipped with a\nGPU, Deepjoin is up to two orders of magnitude faster than existing solutions.",
        "pdf_link": "https://arxiv.org/pdf/2212.07588v2.pdf"
    },
    {
        "title": "Robust Policy Optimization in Deep Reinforcement Learning",
        "authors": [
            "Md Masudur Rahman",
            "Yexiang Xue"
        ],
        "published": "2022-12-14T22:43:56Z",
        "summary": "The policy gradient method enjoys the simplicity of the objective where the\nagent optimizes the cumulative reward directly. Moreover, in the continuous\naction domain, parameterized distribution of action distribution allows easy\ncontrol of exploration, resulting from the variance of the representing\ndistribution. Entropy can play an essential role in policy optimization by\nselecting the stochastic policy, which eventually helps better explore the\nenvironment in reinforcement learning (RL). However, the stochasticity often\nreduces as the training progresses; thus, the policy becomes less exploratory.\nAdditionally, certain parametric distributions might only work for some\nenvironments and require extensive hyperparameter tuning. This paper aims to\nmitigate these issues. In particular, we propose an algorithm called Robust\nPolicy Optimization (RPO), which leverages a perturbed distribution. We\nhypothesize that our method encourages high-entropy actions and provides a way\nto represent the action space better. We further provide empirical evidence to\nverify our hypothesis. We evaluated our methods on various continuous control\ntasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed\nthat in many settings, RPO increases the policy entropy early in training and\nthen maintains a certain level of entropy throughout the training period.\nEventually, our agent RPO shows consistently improved performance compared to\nPPO and other techniques: entropy regularization, different distributions, and\ndata augmentation. Furthermore, in several settings, our method stays robust in\nperformance, while other baseline mechanisms fail to improve and even worsen\nthe performance.",
        "pdf_link": "https://arxiv.org/pdf/2212.07536v1.pdf"
    },
    {
        "title": "MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling",
        "authors": [
            "Nathan Godey",
            "Roman Castagn\u00e9",
            "\u00c9ric de la Clergerie",
            "Beno\u00eet Sagot"
        ],
        "published": "2022-12-14T15:33:44Z",
        "summary": "Static subword tokenization algorithms have been an essential component of\nrecent works on language modeling. However, their static nature results in\nimportant flaws that degrade the models' downstream performance and robustness.\nIn this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.\nMANTa is a differentiable tokenizer trained end-to-end with the language model.\nThe resulting system offers a trade-off between the expressiveness of\nbyte-level models and the speed of models trained using subword tokenization.\nIn addition, our tokenizer is highly explainable since it produces an explicit\nsegmentation of sequences into blocks. We evaluate our pre-trained model on\nseveral English datasets from different domains as well as on synthetic noise.\nWe find that MANTa improves robustness to character perturbations and\nout-of-domain data. We then show that MANTa performs comparably to other models\non the general-domain GLUE benchmark. Finally, we show that it is considerably\nfaster than strictly byte-level models.",
        "pdf_link": "https://arxiv.org/pdf/2212.07284v1.pdf"
    },
    {
        "title": "Multi-task Learning for Cross-Lingual Sentiment Analysis",
        "authors": [
            "Gaurish Thakkar",
            "Nives Mikelic Preradovic",
            "Marko Tadic"
        ],
        "published": "2022-12-14T11:29:03Z",
        "summary": "This paper presents a cross-lingual sentiment analysis of news articles using\nzero-shot and few-shot learning. The study aims to classify the Croatian news\narticles with positive, negative, and neutral sentiments using the Slovene\ndataset. The system is based on a trilingual BERT-based model trained in three\nlanguages: English, Slovene, Croatian. The paper analyses different setups\nusing datasets in two languages and proposes a simple multi-task model to\nperform sentiment classification. The evaluation is performed using the\nfew-shot and zero-shot scenarios in single-task and multi-task experiments for\nCroatian and Slovene.",
        "pdf_link": "https://arxiv.org/pdf/2212.07160v1.pdf"
    },
    {
        "title": "Reproducible scaling laws for contrastive language-image learning",
        "authors": [
            "Mehdi Cherti",
            "Romain Beaumont",
            "Ross Wightman",
            "Mitchell Wortsman",
            "Gabriel Ilharco",
            "Cade Gordon",
            "Christoph Schuhmann",
            "Ludwig Schmidt",
            "Jenia Jitsev"
        ],
        "published": "2022-12-14T10:24:50Z",
        "summary": "Scaling up neural networks has led to remarkable performance across a wide\nrange of tasks. Moreover, performance often follows reliable scaling laws as a\nfunction of training set size, model size, and compute, which offers valuable\nguidance as large-scale experiments are becoming increasingly expensive.\nHowever, previous work on scaling laws has primarily used private data \\&\nmodels or focused on uni-modal language or vision learning. To address these\nlimitations, we investigate scaling laws for contrastive language-image\npre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP\nrepository. Our large-scale experiments involve models trained on up to two\nbillion image-text pairs and identify power law scaling for multiple downstream\ntasks including zero-shot classification, retrieval, linear probing, and\nend-to-end fine-tuning. We find that the training distribution plays a key role\nin scaling laws as the OpenAI and OpenCLIP models exhibit different scaling\nbehavior despite identical model architectures and similar training recipes. We\nopen-source our evaluation workflow and all models, including the largest\npublic CLIP models, to ensure reproducibility and make scaling laws research\nmore accessible. Source code and instructions to reproduce this study will be\navailable at https://github.com/LAION-AI/scaling-laws-openclip",
        "pdf_link": "https://arxiv.org/pdf/2212.07143v1.pdf"
    },
    {
        "title": "Explainability of Text Processing and Retrieval Methods: A Critical Survey",
        "authors": [
            "Sourav Saha",
            "Debapriyo Majumdar",
            "Mandar Mitra"
        ],
        "published": "2022-12-14T09:25:49Z",
        "summary": "Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.",
        "pdf_link": "https://arxiv.org/pdf/2212.07126v1.pdf"
    },
    {
        "title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning",
        "authors": [
            "Hongkuan Zhang",
            "Saku Sugawara",
            "Akiko Aizawa",
            "Lei Zhou",
            "Ryohei Sasano",
            "Koichi Takeda"
        ],
        "published": "2022-12-14T07:52:36Z",
        "summary": "Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.",
        "pdf_link": "https://arxiv.org/pdf/2212.07075v1.pdf"
    },
    {
        "title": "Paraphrase Identification with Deep Learning: A Review of Datasets and Methods",
        "authors": [
            "Chao Zhou",
            "Cheng Qiu",
            "Daniel E. Acuna"
        ],
        "published": "2022-12-13T23:06:20Z",
        "summary": "The rapid advancement of AI technology has made text generation tools like\nGPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can\npose serious threat to the credibility of various forms of media if these\ntechnologies are used for plagiarism, including scientific literature and news\nsources. Despite the development of automated methods for paraphrase\nidentification, detecting this type of plagiarism remains a challenge due to\nthe disparate nature of the datasets on which these methods are trained. In\nthis study, we review traditional and current approaches to paraphrase\nidentification and propose a refined typology of paraphrases. We also\ninvestigate how this typology is represented in popular datasets and how\nunder-representation of certain types of paraphrases impacts detection\ncapabilities. Finally, we outline new directions for future research and\ndatasets in the pursuit of more effective paraphrase detection using AI.",
        "pdf_link": "https://arxiv.org/pdf/2212.06933v1.pdf"
    },
    {
        "title": "Deep Image Style Transfer from Freeform Text",
        "authors": [
            "Tejas Santanam",
            "Mengyang Liu",
            "Jiangyue Yu",
            "Zhaodong Yang"
        ],
        "published": "2022-12-13T19:24:08Z",
        "summary": "This paper creates a novel method of deep neural style transfer by generating\nstyle images from freeform user text input. The language model and style\ntransfer model form a seamless pipeline that can create output images with\nsimilar losses and improved quality when compared to baseline style transfer\nmethods. The language model returns a closely matching image given a style text\nand description input, which is then passed to the style transfer model with an\ninput content image to create a final output. A proof-of-concept tool is also\ndeveloped to integrate the models and demonstrate the effectiveness of deep\nimage style transfer from freeform text.",
        "pdf_link": "https://arxiv.org/pdf/2212.06868v1.pdf"
    },
    {
        "title": "CREPE: Can Vision-Language Foundation Models Reason Compositionally?",
        "authors": [
            "Zixian Ma",
            "Jerry Hong",
            "Mustafa Omer Gul",
            "Mona Gandhi",
            "Irena Gao",
            "Ranjay Krishna"
        ],
        "published": "2022-12-13T19:17:36Z",
        "summary": "A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, we find that: across 7 architectures\ntrained with 4 algorithms on massive datasets, they struggle at\ncompositionality. To arrive at this conclusion, we introduce a new\ncompositionality evaluation benchmark, CREPE, which measures two important\naspects of compositionality identified by cognitive science literature:\nsystematicity and productivity. To measure systematicity, CREPE consists of a\ntest dataset containing over $370K$ image-text pairs and three different\nseen-unseen splits. The three splits are designed to test models trained on\nthree popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also\ngenerate $325K$, $316K$, and $309K$ hard negative captions for a subset of the\npairs. To test productivity, CREPE contains $17K$ image-text pairs with nine\ndifferent complexities plus $183K$ hard negative captions with atomic, swapping\nand negation foils. The datasets are generated by repurposing the Visual Genome\nscene graphs and region descriptions and applying handcrafted templates and\nGPT-3. For systematicity, we find that model performance decreases consistently\nwhen novel compositions dominate the retrieval set, with Recall@1 dropping by\nup to $12\\%$. For productivity, models' retrieval success decays as complexity\nincreases, frequently nearing random chance at high complexity. These results\nhold regardless of model and training dataset size.",
        "pdf_link": "https://arxiv.org/pdf/2212.07796v3.pdf"
    },
    {
        "title": "Foresight -- Generative Pretrained Transformer (GPT) for Modelling of Patient Timelines using EHRs",
        "authors": [
            "Zeljko Kraljevic",
            "Dan Bean",
            "Anthony Shek",
            "Rebecca Bendayan",
            "Harry Hemingway",
            "Joshua Au Yeung",
            "Alexander Deng",
            "Alfie Baston",
            "Jack Ross",
            "Esther Idowu",
            "James T Teo",
            "Richard J Dobson"
        ],
        "published": "2022-12-13T19:06:00Z",
        "summary": "Background: Electronic Health Records hold detailed longitudinal information\nabout each patient's health status and general clinical history, a large\nportion of which is stored within the unstructured text. Existing approaches\nfocus mostly on structured data and a subset of single-domain outcomes. We\nexplore how temporal modelling of patients from free text and structured data,\nusing deep generative transformers can be used to forecast a wide range of\nfuture disorders, substances, procedures or findings. Methods: We present\nForesight, a novel transformer-based pipeline that uses named entity\nrecognition and linking tools to convert document text into structured, coded\nconcepts, followed by providing probabilistic forecasts for future medical\nevents such as disorders, substances, procedures and findings. We processed the\nentire free-text portion from three different hospital datasets totalling\n811336 patients covering both physical and mental health. Findings: On tests in\ntwo UK hospitals (King's College Hospital, South London and Maudsley) and the\nUS MIMIC-III dataset precision@10 0.68, 0.76 and 0.88 was achieved for\nforecasting the next disorder in a patient timeline, while precision@10 of\n0.80, 0.81 and 0.91 was achieved for forecasting the next biomedical concept.\nForesight was also validated on 34 synthetic patient timelines by five\nclinicians and achieved relevancy of 97% for the top forecasted candidate\ndisorder. As a generative model, it can forecast follow-on biomedical concepts\nfor as many steps as required. Interpretation: Foresight is a general-purpose\nmodel for biomedical concept modelling that can be used for real-world risk\nforecasting, virtual trials and clinical research to study the progression of\ndisorders, simulate interventions and counterfactuals, and educational\npurposes.",
        "pdf_link": "https://arxiv.org/pdf/2212.08072v2.pdf"
    },
    {
        "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages",
        "authors": [
            "Yekun Chai",
            "Shuohuan Wang",
            "Chao Pang",
            "Yu Sun",
            "Hao Tian",
            "Hua Wu"
        ],
        "published": "2022-12-13T17:21:44Z",
        "summary": "Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nrelease our code and pre-trained checkpoints.",
        "pdf_link": "https://arxiv.org/pdf/2212.06742v2.pdf"
    },
    {
        "title": "What do Vision Transformers Learn? A Visual Exploration",
        "authors": [
            "Amin Ghiasi",
            "Hamid Kazemi",
            "Eitan Borgnia",
            "Steven Reich",
            "Manli Shu",
            "Micah Goldblum",
            "Andrew Gordon Wilson",
            "Tom Goldstein"
        ],
        "published": "2022-12-13T16:55:12Z",
        "summary": "Vision transformers (ViTs) are quickly becoming the de-facto architecture for\ncomputer vision, yet we understand very little about why they work and what\nthey learn. While existing studies visually analyze the mechanisms of\nconvolutional neural networks, an analogous exploration of ViTs remains\nchallenging. In this paper, we first address the obstacles to performing\nvisualizations on ViTs. Assisted by these solutions, we observe that neurons in\nViTs trained with language model supervision (e.g., CLIP) are activated by\nsemantic concepts rather than visual features. We also explore the underlying\ndifferences between ViTs and CNNs, and we find that transformers detect image\nbackground features, just like their convolutional counterparts, but their\npredictions depend far less on high-frequency information. On the other hand,\nboth architecture types behave similarly in the way features progress from\nabstract patterns in early layers to concrete objects in late layers. In\naddition, we show that ViTs maintain spatial information in all layers except\nthe final layer. In contrast to previous works, we show that the last layer\nmost likely discards the spatial information and behaves as a learned global\npooling operation. Finally, we conduct large-scale visualizations on a wide\nrange of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to\nvalidate the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2212.06727v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation",
        "authors": [
            "Shailja Thakur",
            "Baleegh Ahmad",
            "Zhenxing Fan",
            "Hammond Pearce",
            "Benjamin Tan",
            "Ramesh Karri",
            "Brendan Dolan-Gavitt",
            "Siddharth Garg"
        ],
        "published": "2022-12-13T16:34:39Z",
        "summary": "Automating hardware design could obviate a significant amount of human error\nfrom the engineering process and lead to fewer errors. Verilog is a popular\nhardware description language to model and design digital systems, thus\ngenerating Verilog code is a critical first step. Emerging large language\nmodels (LLMs) are able to write high-quality code in other programming\nlanguages. In this paper, we characterize the ability of LLMs to generate\nuseful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets\ncollected from GitHub and Verilog textbooks. We construct an evaluation\nframework comprising test-benches for functional analysis and a flow to test\nthe syntax of Verilog code generated in response to problems of varying\ndifficulty. Our findings show that across our problem scenarios, the\nfine-tuning results in LLMs more capable of producing syntactically correct\ncode (25.9% overall). Further, when analyzing functional correctness, a\nfine-tuned open-source CodeGen LLM can outperform the state-of-the-art\ncommercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM\ncheckpoints are available: https://github.com/shailja-thakur/VGen.",
        "pdf_link": "https://arxiv.org/pdf/2212.11140v1.pdf"
    },
    {
        "title": "Structured Prompting: Scaling In-Context Learning to 1,000 Examples",
        "authors": [
            "Yaru Hao",
            "Yutao Sun",
            "Li Dong",
            "Zhixiong Han",
            "Yuxian Gu",
            "Furu Wei"
        ],
        "published": "2022-12-13T16:31:21Z",
        "summary": "Large language models have exhibited intriguing in-context learning\ncapability, achieving promising zero- and few-shot performance without updating\nthe parameters. However, conventional in-context learning is usually restricted\nby length constraints, rendering it ineffective to absorb supervision from a\nlarge number of examples. In order to go beyond few shots, we introduce\nstructured prompting that breaks the length limit and scales in-context\nlearning to thousands of examples. Specifically, demonstration examples are\nseparately encoded with well-designed position embeddings, and then they are\njointly attended by the test example using a rescaled attention mechanism. So\nwe can scale the number of exemplars with linear complexity instead of\nquadratic complexity with respect to length. Experimental results on a diverse\nset of tasks show that our approach improves end-task performance and reduces\nevaluation variance over conventional in-context learning as the number of\ndemonstration examples increases. Code has been released at\nhttps://aka.ms/structured-prompting.",
        "pdf_link": "https://arxiv.org/pdf/2212.06713v1.pdf"
    },
    {
        "title": "Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?",
        "authors": [
            "David Mueller",
            "Nicholas Andrews",
            "Mark Dredze"
        ],
        "published": "2022-12-13T15:28:57Z",
        "summary": "Traditional multi-task learning architectures train a single model across\nmultiple tasks through a shared encoder followed by task-specific decoders.\nLearning these models often requires specialized training algorithms that\naddress task-conflict in the shared parameter updates, which otherwise can lead\nto negative transfer. A new type of multi-task learning within NLP homogenizes\nmulti-task architectures as a shared encoder and language model decoder, which\ndoes surprisingly well across a range of diverse tasks. Does this new\narchitecture suffer from task-conflicts that require specialized training\nalgorithms? We study how certain factors in the shift towards text-to-text\nmodels affects multi-task conflict and negative transfer, finding that both\ndirectional conflict and transfer are surprisingly constant across\narchitectures.",
        "pdf_link": "https://arxiv.org/pdf/2212.06645v1.pdf"
    },
    {
        "title": "On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning",
        "authors": [
            "Yiting Qu",
            "Xinlei He",
            "Shannon Pierson",
            "Michael Backes",
            "Yang Zhang",
            "Savvas Zannettou"
        ],
        "published": "2022-12-13T13:38:04Z",
        "summary": "The dissemination of hateful memes online has adverse effects on social media\nplatforms and the real world. Detecting hateful memes is challenging, one of\nthe reasons being the evolutionary nature of memes; new hateful memes can\nemerge by fusing hateful connotations with other cultural ideas or symbols. In\nthis paper, we propose a framework that leverages multimodal contrastive\nlearning models, in particular OpenAI's CLIP, to identify targets of hateful\ncontent and systematically investigate the evolution of hateful memes. We find\nthat semantic regularities exist in CLIP-generated embeddings that describe\nsemantic relationships within the same modality (images) or across modalities\n(images and text). Leveraging this property, we study how hateful memes are\ncreated by combining visual elements from multiple images or fusing textual\ninformation with a hateful image. We demonstrate the capabilities of our\nframework for analyzing the evolution of hateful memes by focusing on\nantisemitic memes, particularly the Happy Merchant meme. Using our framework on\na dataset extracted from 4chan, we find 3.3K variants of the Happy Merchant\nmeme, with some linked to specific countries, persons, or organizations. We\nenvision that our framework can be used to aid human moderators by flagging new\nvariants of hateful memes so that moderators can manually verify them and\nmitigate the problem of hateful content online.",
        "pdf_link": "https://arxiv.org/pdf/2212.06573v2.pdf"
    },
    {
        "title": "The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique",
        "authors": [
            "Beatrice Couture",
            "Farah Verret",
            "Maxime Gohier",
            "Dominique Deslandres"
        ],
        "published": "2022-12-13T12:42:12Z",
        "summary": "The arrival of handwriting recognition technologies offers new possibilities\nfor research in heritage studies. However, it is now necessary to reflect on\nthe experiences and the practices developed by research teams. Our use of the\nTranskribus platform since 2018 has led us to search for the most significant\nways to improve the performance of our handwritten text recognition (HTR)\nmodels which are made to transcribe French handwriting dating from the 17th\ncentury. This article therefore reports on the impacts of creating transcribing\nprotocols, using the language model at full scale and determining the best way\nto use base models in order to help increase the performance of HTR models.\nCombining all of these elements can indeed increase the performance of a single\nmodel by more than 20% (reaching a Character Error Rate below 5%). This article\nalso discusses some challenges regarding the collaborative nature of HTR\nplatforms such as Transkribus and the way researchers can share their data\ngenerated in the process of creating or training handwritten text recognition\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2212.11146v4.pdf"
    },
    {
        "title": "Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model",
        "authors": [
            "Jiang-Long Song",
            "Wu-He Zou",
            "Feng Li",
            "Xiao-Lei Qin",
            "Wei-Dong Zhang"
        ],
        "published": "2022-12-13T04:57:04Z",
        "summary": "Prompt tuning recently becomes a hot-spot in the applications of large\npretrained language models on specific downstream tasks. Regarding the Language\nModel as a Service (LMaaS), black-box tuning using derivative-free optimization\n(DFO) provides a novel approach to expand the practical scenarios of pretrained\nmodels and enrich the researches of few-shot learning. In this report, we\npresent our solution in this competition that is based on the LMaaS scenario.\nOur solution consists of several modifications to BBTv2, including multiple\nlabel words, selection of P0, rolling update strategy, multi-task loss from MLP\nclassifier, and finally using the ensemble method to further improve\ngeneralization ability. We also shared some strategies that we tried but didn't\nuse in the final submission for further discussion. In the end we raised a\nquestion about the SNLI dataset and the impact on the results, as well as our\nconcerns about the competition.",
        "pdf_link": "https://arxiv.org/pdf/2212.06369v3.pdf"
    },
    {
        "title": "Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety",
        "authors": [
            "Joshua Albrecht",
            "Ellie Kitanidis",
            "Abraham J. Fetterman"
        ],
        "published": "2022-12-13T00:29:45Z",
        "summary": "Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2212.06295v1.pdf"
    },
    {
        "title": "Evaluation of Synthetic Datasets for Conversational Recommender Systems",
        "authors": [
            "Harsh Lara",
            "Manoj Tiwari"
        ],
        "published": "2022-12-12T18:53:10Z",
        "summary": "For researchers leveraging Large-Language Models (LLMs) in the generation of\ntraining datasets, especially for conversational recommender systems - the\nabsence of robust evaluation frameworks has been a long-standing problem. The\nefficiency brought about by LLMs in the data generation phase is impeded during\nthe process of evaluation of the generated data, since it generally requires\nhuman-raters to ensure that the data generated is of high quality and has\nsufficient diversity. Since the quality of training data is critical for\ndownstream applications, it is important to develop metrics that evaluate the\nquality holistically and identify biases. In this paper, we present a framework\nthat takes a multi-faceted approach towards evaluating datasets produced by\ngenerative models and discuss the advantages and limitations of various\nevaluation methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.08167v1.pdf"
    },
    {
        "title": "Prompting Is Programming: A Query Language for Large Language Models",
        "authors": [
            "Luca Beurer-Kellner",
            "Marc Fischer",
            "Martin Vechev"
        ],
        "published": "2022-12-12T18:09:09Z",
        "summary": "Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n  Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks while abstracting language model internals and providing\nhigh-level semantics.\n  To enable LMP, we implement LMQL(short for Language Model Query Language),\nwhich leverages the constraints and control flow from an LMP prompt to generate\nan efficient inference procedure that minimizes the number of expensive calls\nto the underlying language model.\n  We show that LMQL can capture a wide range of state-of-the-art prompting\nmethods in an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (26-85% cost savings).",
        "pdf_link": "https://arxiv.org/pdf/2212.06094v3.pdf"
    },
    {
        "title": "Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts",
        "authors": [
            "Yu Zhang",
            "Yunyi Zhang",
            "Martin Michalski",
            "Yucheng Jiang",
            "Yu Meng",
            "Jiawei Han"
        ],
        "published": "2022-12-12T16:03:38Z",
        "summary": "Instead of mining coherent topics from a given text corpus in a completely\nunsupervised manner, seed-guided topic discovery methods leverage user-provided\nseed words to extract distinctive and coherent topics so that the mined topics\ncan better cater to the user's interest. To model the semantic correlation\nbetween words and seeds for discovering topic-indicative terms, existing\nseed-guided approaches utilize different types of context signals, such as\ndocument-level word co-occurrences, sliding window-based local contexts, and\ngeneric linguistic knowledge brought by pre-trained language models. In this\nwork, we analyze and show empirically that each type of context information has\nits value and limitation in modeling word semantics under seed guidance, but\ncombining three types of contexts (i.e., word embeddings learned from local\ncontexts, pre-trained language model representations obtained from\ngeneral-domain training, and topic-indicative sentences retrieved based on seed\ninformation) allows them to complement each other for discovering quality\ntopics. We propose an iterative framework, SeedTopicMine, which jointly learns\nfrom the three types of contexts and gradually fuses their context signals via\nan ensemble ranking process. Under various sets of seeds and on multiple\ndatasets, SeedTopicMine consistently yields more coherent and accurate topics\nthan existing seed-guided topic discovery approaches.",
        "pdf_link": "https://arxiv.org/pdf/2212.06002v2.pdf"
    },
    {
        "title": "DexBERT: Effective, Task-Agnostic and Fine-grained Representation Learning of Android Bytecode",
        "authors": [
            "Tiezhu Sun",
            "Kevin Allix",
            "Kisub Kim",
            "Xin Zhou",
            "Dongsun Kim",
            "David Lo",
            "Tegawend\u00e9 F. Bissyand\u00e9",
            "Jacques Klein"
        ],
        "published": "2022-12-12T15:32:31Z",
        "summary": "The automation of a large number of software engineering tasks is becoming\npossible thanks to Machine Learning (ML). Central to applying ML to software\nartifacts (like source or executable code) is converting them into forms\nsuitable for learning. Traditionally, researchers have relied on manually\nselected features, based on expert knowledge which is sometimes imprecise and\ngenerally incomplete. Representation learning has allowed ML to automatically\nchoose suitable representations and relevant features. Yet, for Android-related\ntasks, existing models like apk2vec focus on whole-app levels, or target\nspecific tasks like smali2vec, which limits their applicability. Our work is\npart of a new line of research that investigates effective, task-agnostic, and\nfine-grained universal representations of bytecode to mitigate both of these\ntwo limitations. Such representations aim to capture information relevant to\nvarious low-level downstream tasks (e.g., at the class-level). We are inspired\nby the field of Natural Language Processing, where the problem of universal\nrepresentation was addressed by building Universal Language Models, such as\nBERT, whose goal is to capture abstract semantic information about sentences,\nin a way that is reusable for a variety of tasks. We propose DexBERT, a\nBERT-like Language Model dedicated to representing chunks of DEX bytecode, the\nmain binary format used in Android applications. We empirically assess whether\nDexBERT is able to model the DEX language and evaluate the suitability of our\nmodel in three distinct class-level software engineering tasks: Malicious Code\nLocalization, Defect Prediction, and Component Type Classification. We also\nexperiment with strategies to deal with the problem of catering to apps having\nvastly different sizes, and we demonstrate one example of using our technique\nto investigate what information is relevant to a given task.",
        "pdf_link": "https://arxiv.org/pdf/2212.05976v2.pdf"
    },
    {
        "title": "MaNLP@SMM4H22: BERT for Classification of Twitter Posts",
        "authors": [
            "Keshav Kapur",
            "Rajitha Harikrishnan"
        ],
        "published": "2022-12-12T14:43:46Z",
        "summary": "The reported work is our straightforward approach for the shared task\nClassification of tweets self-reporting age organized by the Social Media\nMining for Health Applications (SMM4H) workshop. This literature describes the\napproach that was used to build a binary classification system, that classifies\nthe tweets related to birthday posts into two classes namely, exact\nage(positive class) and non-exact age(negative class). We made two submissions\nwith variations in the preprocessing of text which yielded F1 scores of 0.80\nand 0.81 when evaluated by the organizers.",
        "pdf_link": "https://arxiv.org/pdf/2301.05395v1.pdf"
    },
    {
        "title": "\"I think this is the most disruptive technology\": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",
        "authors": [
            "Mubin Ul Haque",
            "Isuru Dharmadasa",
            "Zarrin Tasnim Sworna",
            "Roshan Namal Rajapakse",
            "Hussain Ahmad"
        ],
        "published": "2022-12-12T12:41:24Z",
        "summary": "Large language models have recently attracted significant attention due to\ntheir impressive performance on a variety of tasks. ChatGPT developed by OpenAI\nis one such implementation of a large, pre-trained language model that has\ngained immense popularity among early adopters, where certain users go to the\nextent of characterizing it as a disruptive technology in many domains.\nUnderstanding such early adopters' sentiments is important because it can\nprovide insights into the potential success or failure of the technology, as\nwell as its strengths and weaknesses. In this paper, we conduct a mixed-method\nstudy using 10,732 tweets from early ChatGPT users. We first use topic\nmodelling to identify the main topics and then perform an in-depth qualitative\nsentiment analysis of each topic. Our results show that the majority of the\nearly adopters have expressed overwhelmingly positive sentiments related to\ntopics such as Disruptions to software development, Entertainment and\nexercising creativity. Only a limited percentage of users expressed concerns\nabout issues such as the potential for misuse of ChatGPT, especially regarding\ntopics such as Impact on educational aspects. We discuss these findings by\nproviding specific examples for each topic and then detail implications related\nto addressing these concerns for both researchers and users.",
        "pdf_link": "https://arxiv.org/pdf/2212.05856v1.pdf"
    },
    {
        "title": "A Study of Slang Representation Methods",
        "authors": [
            "Aravinda Kolla",
            "Filip Ilievski",
            "H\u00f4ng-\u00c2n Sandlin",
            "Alain Mermoud"
        ],
        "published": "2022-12-11T21:56:44Z",
        "summary": "Considering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.",
        "pdf_link": "https://arxiv.org/pdf/2212.05613v3.pdf"
    },
    {
        "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
        "authors": [
            "Sumanth Doddapaneni",
            "Rahul Aralikatte",
            "Gowtham Ramesh",
            "Shreya Goyal",
            "Mitesh M. Khapra",
            "Anoop Kunchukuttan",
            "Pratyush Kumar"
        ],
        "published": "2022-12-11T04:45:50Z",
        "summary": "Building Natural Language Understanding (NLU) capabilities for Indic\nlanguages, which have a collective speaker base of more than one billion\nspeakers is absolutely crucial. In this work, we aim to improve the NLU\ncapabilities of Indic languages by making contributions along 3 important axes\n(i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on\nIndic languages. Specifically, we curate the largest monolingual corpora,\nIndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a\n2.3x increase over prior work, while supporting 12 additional languages. Next,\nwe create a human-supervised benchmark, IndicXTREME, consisting of nine diverse\nNLU tasks covering 20 languages. Across languages and tasks, IndicXTREME\ncontains a total of 105 evaluation sets, of which 52 are new contributions to\nthe literature. To the best of our knowledge, this is the first effort towards\ncreating a standard benchmark for Indic languages that aims to test the\nmultilingual zero-shot capabilities of pretrained language models. Finally, we\ntrain IndicBERT v2, a state-of-the-art model supporting all the languages.\nAveraged across languages and tasks, the model achieves an absolute improvement\nof 2 points over a strong baseline. The data and models are available at\nhttps://github.com/AI4Bharat/IndicBERT.",
        "pdf_link": "https://arxiv.org/pdf/2212.05409v3.pdf"
    },
    {
        "title": "Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin",
        "authors": [
            "Abhinav Rao",
            "Ho Thi-Nga",
            "Chng Eng-Siong"
        ],
        "published": "2022-12-10T19:54:53Z",
        "summary": "This paper presents the work of restoring punctuation for ASR transcripts\ngenerated by multilingual ASR systems. The focus languages are English,\nMandarin, and Malay which are three of the most popular languages in Singapore.\nTo the best of our knowledge, this is the first system that can tackle\npunctuation restoration for these three languages simultaneously. Traditional\napproaches usually treat the task as a sequential labeling task, however, this\nwork adopts a slot-filling approach that predicts the presence and type of\npunctuation marks at each word boundary. The approach is similar to the\nMasked-Language Model approach employed during the pre-training stages of BERT,\nbut instead of predicting the masked word, our model predicts masked\npunctuation. Additionally, we find that using Jieba1 instead of only using the\nbuilt-in SentencePiece tokenizer of XLM-R can significantly improve the\nperformance of punctuating Mandarin transcripts. Experimental results on\nEnglish and Mandarin IWSLT2022 datasets and Malay News show that the proposed\napproach achieved state-of-the-art results for Mandarin with 73.8% F1-score\nwhile maintaining a reasonable F1-score for English and Malay, i.e. 74.7% and\n78% respectively. Our source code that allows reproducing the results and\nbuilding a simple web-based application for demonstration purposes is available\non Github.",
        "pdf_link": "https://arxiv.org/pdf/2212.05356v1.pdf"
    },
    {
        "title": "Elixir: Train a Large Language Model on a Small GPU Cluster",
        "authors": [
            "Haichen Huang",
            "Jiarui Fang",
            "Hongxin Liu",
            "Shenggui Li",
            "Yang You"
        ],
        "published": "2022-12-10T17:26:05Z",
        "summary": "In recent years, large language models have achieved great success due to\ntheir unprecedented size. However, training these models poses a challenge for\nmost researchers as it requires a substantial number of GPUs. To reduce GPU\nmemory usage, memory partitioning, and memory offloading have been proposed.\nThese approaches eliminate memory redundancies and offload memory usage to the\nCPU and NVMe memory, respectively, enabling training on small GPU clusters.\nHowever, directly deploying these solutions often leads to suboptimal\nefficiency. Only experienced experts can unleash the full potential of hardware\nby carefully tuning the distributed configuration. Thus, we present a novel\nsolution, Elixir, which automates efficient large-model training based on\npre-runtime model profiling. Elixir aims to identify the optimal combination of\npartitioning and offloading techniques to maximize training throughput. In our\nexperiments, Elixir significantly outperforms the current state-of-the-art\nbaseline. Our optimal configuration achieves up to a 3.4$\\times$ speedup on\nGPT-2 models compared with SOTA solutions. We hope that our work will benefit\nindividuals who lack computing resources and expertise, granting them access to\nlarge models. The beta version of Elixir is now available at\nhttps://github.com/hpcaitech/ColossalAI/tree/feature/elixir.",
        "pdf_link": "https://arxiv.org/pdf/2212.05339v3.pdf"
    },
    {
        "title": "A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks",
        "authors": [
            "Ruiqing Ding",
            "Xiao Han",
            "Leye Wang"
        ],
        "published": "2022-12-10T09:18:43Z",
        "summary": "By focusing the pre-training process on domain-specific corpora, some\ndomain-specific pre-trained language models (PLMs) have achieved\nstate-of-the-art results. However, it is under-investigated to design a unified\nparadigm to inject domain knowledge in the PLM fine-tuning stage. We propose\nKnowledgeDA, a unified domain language model development service to enhance the\ntask-specific training procedure with domain knowledge graphs. Given\ndomain-specific task texts input, KnowledgeDA can automatically generate a\ndomain-specific language model following three steps: (i) localize domain\nknowledge entities in texts via an embedding-similarity approach; (ii) generate\naugmented samples by retrieving replaceable domain entity pairs from two views\nof both knowledge graph and training data; (iii) select high-quality augmented\nsamples for fine-tuning via confidence-based assessment. We implement a\nprototype of KnowledgeDA to learn language models for two domains, healthcare\nand software development. Experiments on domain-specific text classification\nand QA tasks verify the effectiveness and generalizability of KnowledgeDA.",
        "pdf_link": "https://arxiv.org/pdf/2212.05251v2.pdf"
    },
    {
        "title": "Structured information extraction from complex scientific text with fine-tuned large language models",
        "authors": [
            "Alexander Dunn",
            "John Dagdelen",
            "Nicholas Walker",
            "Sanghoon Lee",
            "Andrew S. Rosen",
            "Gerbrand Ceder",
            "Kristin Persson",
            "Anubhav Jain"
        ],
        "published": "2022-12-10T07:51:52Z",
        "summary": "Intelligently extracting and linking complex scientific information from\nunstructured text is a challenging endeavor particularly for those\ninexperienced with natural language processing. Here, we present a simple\nsequence-to-sequence approach to joint named entity recognition and relation\nextraction for complex hierarchical information in scientific text. The\napproach leverages a pre-trained large language model (LLM), GPT-3, that is\nfine-tuned on approximately 500 pairs of prompts (inputs) and completions\n(outputs). Information is extracted either from single sentences or across\nsentences in abstracts/passages, and the output can be returned as simple\nEnglish sentences or a more structured format, such as a list of JSON objects.\nWe demonstrate that LLMs trained in this way are capable of accurately\nextracting useful records of complex scientific knowledge for three\nrepresentative tasks in materials chemistry: linking dopants with their host\nmaterials, cataloging metal-organic frameworks, and general\nchemistry/phase/morphology/application information extraction. This approach\nrepresents a simple, accessible, and highly-flexible route to obtaining large\ndatabases of structured knowledge extracted from unstructured text. An online\ndemo is available at http://www.matscholar.com/info-extraction.",
        "pdf_link": "https://arxiv.org/pdf/2212.05238v1.pdf"
    },
    {
        "title": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Chen Sun",
            "Zirui Wang",
            "Kai-Wei Chang",
            "Yizhou Sun",
            "Cordelia Schmid",
            "David A. Ross",
            "Alireza Fathi"
        ],
        "published": "2022-12-10T06:17:56Z",
        "summary": "In this paper, we propose an end-to-end Retrieval-Augmented Visual Language\nModel (REVEAL) that learns to encode world knowledge into a large-scale memory,\nand to retrieve from it to answer knowledge-intensive queries. REVEAL consists\nof four key components: the memory, the encoder, the retriever and the\ngenerator. The large-scale memory encodes various sources of multimodal world\nknowledge (e.g. image-text pairs, question answering pairs, knowledge graph\ntriplets, etc) via a unified encoder. The retriever finds the most relevant\nknowledge entries in the memory, and the generator fuses the retrieved\nknowledge with the input query to produce the output. A key novelty in our\napproach is that the memory, encoder, retriever and generator are all\npre-trained end-to-end on a massive amount of data. Furthermore, our approach\ncan use a diverse set of multimodal knowledge sources, which is shown to result\nin significant gains. We show that REVEAL achieves state-of-the-art results on\nvisual question answering and image captioning.",
        "pdf_link": "https://arxiv.org/pdf/2212.05221v2.pdf"
    },
    {
        "title": "Thinking Fast and Slow in Large Language Models",
        "authors": [
            "Thilo Hagendorff",
            "Sarah Fabi",
            "Michal Kosinski"
        ],
        "published": "2022-12-10T05:07:30Z",
        "summary": "Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Therefore, it is of\ngreat importance to evaluate their emerging abilities. In this study, we show\nthat LLMs like GPT-3 exhibit behavior that strikingly resembles human-like\nintuition - and the cognitive errors that come with it. However, LLMs with\nhigher cognitive capabilities, in particular ChatGPT and GPT-4, learned to\navoid succumbing to these errors and perform in a hyperrational manner. For our\nexperiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as\nsemantic illusions that were originally designed to investigate intuitive\ndecision-making in humans. Our study demonstrates that investigating LLMs with\nmethods from psychology has the potential to reveal otherwise unknown emergent\ntraits.",
        "pdf_link": "https://arxiv.org/pdf/2212.05206v2.pdf"
    },
    {
        "title": "Artificial Text Detection with Multiple Training Strategies",
        "authors": [
            "Bin Li",
            "Yixuan Weng",
            "Qiya Song",
            "Hanjun Deng"
        ],
        "published": "2022-12-10T03:57:28Z",
        "summary": "As the deep learning rapidly promote, the artificial texts created by\ngenerative models are commonly used in news and social media. However, such\nmodels can be abused to generate product reviews, fake news, and even fake\npolitical content. The paper proposes a solution for the Russian Artificial\nText Detection in the Dialogue shared task 2022 (RuATD 2022) to distinguish\nwhich model within the list is used to generate this text. We introduce the\nDeBERTa pre-trained language model with multiple training strategies for this\nshared task. Extensive experiments conducted on the RuATD dataset validate the\neffectiveness of our proposed method. Moreover, our submission ranked second\nplace in the evaluation phase for RuATD 2022 (Multi-Class).",
        "pdf_link": "https://arxiv.org/pdf/2212.05194v1.pdf"
    },
    {
        "title": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding",
        "authors": [
            "Le Xue",
            "Mingfei Gao",
            "Chen Xing",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Jiajun Wu",
            "Caiming Xiong",
            "Ran Xu",
            "Juan Carlos Niebles",
            "Silvio Savarese"
        ],
        "published": "2022-12-10T01:34:47Z",
        "summary": "The recognition capabilities of current state-of-the-art 3D models are\nlimited by datasets with a small number of annotated data and a pre-defined set\nof categories. In its 2D counterpart, recent advances have shown that similar\nproblems can be significantly alleviated by employing knowledge from other\nmodalities, such as language. Inspired by this, leveraging multimodal\ninformation for 3D modality could be promising to improve 3D understanding\nunder the restricted data regime, but this line of research is not well\nstudied. Therefore, we introduce ULIP to learn a unified representation of\nimages, texts, and 3D point clouds by pre-training with object triplets from\nthe three modalities. To overcome the shortage of training triplets, ULIP\nleverages a pre-trained vision-language model that has already learned a common\nvisual and textual space by training with massive image-text pairs. Then, ULIP\nlearns a 3D representation space aligned with the common image-text space,\nusing a small number of automatically synthesized triplets. ULIP is agnostic to\n3D backbone networks and can easily be integrated into any 3D architecture.\nExperiments show that ULIP effectively improves the performance of multiple\nrecent 3D backbones by simply pre-training them on ShapeNet55 using our\nframework, achieving state-of-the-art performance in both standard 3D\nclassification and zero-shot 3D classification on ModelNet40 and ScanObjectNN.\nULIP also improves the performance of PointMLP by around 3% in 3D\nclassification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1\naccuracy for zero-shot 3D classification on ModelNet40. Our code and\npre-trained models are released at https://github.com/salesforce/ULIP.",
        "pdf_link": "https://arxiv.org/pdf/2212.05171v4.pdf"
    },
    {
        "title": "Incorporating Emotions into Health Mention Classification Task on Social Media",
        "authors": [
            "Olanrewaju Tahir Aduragba",
            "Jialin Yu",
            "Alexandra I. Cristea"
        ],
        "published": "2022-12-09T18:38:41Z",
        "summary": "The health mention classification (HMC) task is the process of identifying\nand classifying mentions of health-related concepts in text. This can be useful\nfor identifying and tracking the spread of diseases through social media posts.\nHowever, this is a non-trivial task. Here we build on recent studies suggesting\nthat using emotional information may improve upon this task. Our study results\nin a framework for health mention classification that incorporates affective\nfeatures. We present two methods, an intermediate task fine-tuning approach\n(implicit) and a multi-feature fusion approach (explicit) to incorporate\nemotions into our target task of HMC. We evaluated our approach on 5\nHMC-related datasets from different social media platforms including three from\nTwitter, one from Reddit and another from a combination of social media\nsources. Extensive experiments demonstrate that our approach results in\nstatistically significant performance gains on HMC tasks. By using the\nmulti-feature fusion approach, we achieve at least a 3% improvement in F1 score\nover BERT baselines across all datasets. We also show that considering only\nnegative emotions does not significantly affect performance on the HMC task.\nAdditionally, our results indicate that HMC models infused with emotional\nknowledge are an effective alternative, especially when other HMC datasets are\nunavailable for domain-specific fine-tuning. The source code for our models is\nfreely available at https://github.com/tahirlanre/Emotion_PHM.",
        "pdf_link": "https://arxiv.org/pdf/2212.05039v1.pdf"
    },
    {
        "title": "The Turing Deception",
        "authors": [
            "David Noever",
            "Matt Ciolino"
        ],
        "published": "2022-12-09T16:32:11Z",
        "summary": "This research revisits the classic Turing test and compares recent large\nlanguage models such as ChatGPT for their abilities to reproduce human-level\ncomprehension and compelling text generation. Two task challenges --\nsummarization, and question answering -- prompt ChatGPT to produce original\ncontent (98-99%) from a single text entry and also sequential questions\noriginally posed by Turing in 1950. We score the original and generated content\nagainst the OpenAI GPT-2 Output Detector from 2019, and establish multiple\ncases where the generated content proves original and undetectable (98%). The\nquestion of a machine fooling a human judge recedes in this work relative to\nthe question of \"how would one prove it?\" The original contribution of the work\npresents a metric and simple grammatical set for understanding the writing\nmechanics of chatbots in evaluating their readability and statistical clarity,\nengagement, delivery, and overall quality. While Turing's original prose scores\nat least 14% below the machine-generated output, the question of whether an\nalgorithm displays hints of Turing's truly original thoughts (the \"Lovelace\n2.0\" test) remains unanswered and potentially unanswerable for now.",
        "pdf_link": "https://arxiv.org/pdf/2212.06721v2.pdf"
    },
    {
        "title": "TRBLLmaker -- Transformer Reads Between Lyrics Lines maker",
        "authors": [
            "Mor Ventura",
            "Michael Toker"
        ],
        "published": "2022-12-09T15:27:36Z",
        "summary": "Even for us, it can be challenging to comprehend the meaning of songs. As\npart of this project, we explore the process of generating the meaning of\nsongs. Despite the widespread use of text-to-text models, few attempts have\nbeen made to achieve a similar objective. Songs are primarily studied in the\ncontext of sentiment analysis. This involves identifying opinions and emotions\nin texts, evaluating them as positive or negative, and utilizing these\nevaluations to make music recommendations. In this paper, we present a\ngenerative model that offers implicit meanings for several lines of a song. Our\nmodel uses a decoder Transformer architecture GPT-2, where the input is the\nlyrics of a song. Furthermore, we compared the performance of this architecture\nwith that of the encoder-decoder Transformer architecture of the T5 model. We\nalso examined the effect of different prompt types with the option of appending\nadditional information, such as the name of the artist and the title of the\nsong. Moreover, we tested different decoding methods with different training\nparameters and evaluated our results using ROUGE. In order to build our\ndataset, we utilized the 'Genious' API, which allowed us to acquire the lyrics\nof songs and their explanations, as well as their rich metadata.",
        "pdf_link": "https://arxiv.org/pdf/2212.04917v1.pdf"
    },
    {
        "title": "CKG: Dynamic Representation Based on Context and Knowledge Graph",
        "authors": [
            "Xunzhu Tang",
            "Tiezhu Sun",
            "Rujie Zhu",
            "Shi Wang"
        ],
        "published": "2022-12-09T15:17:35Z",
        "summary": "Recently, neural language representation models pre-trained on large corpus\ncan capture rich co-occurrence information and be fine-tuned in downstream\ntasks to improve the performance. As a result, they have achieved\nstate-of-the-art results in a large range of language tasks. However, there\nexists other valuable semantic information such as similar, opposite, or other\npossible meanings in external knowledge graphs (KGs). We argue that entities in\nKGs could be used to enhance the correct semantic meaning of language\nsentences. In this paper, we propose a new method CKG: Dynamic Representation\nBased on \\textbf{C}ontext and \\textbf{K}nowledge \\textbf{G}raph. On the one\nside, CKG can extract rich semantic information of large corpus. On the other\nside, it can make full use of inside information such as co-occurrence in large\ncorpus and outside information such as similar entities in KGs. We conduct\nextensive experiments on a wide range of tasks, including QQP, MRPC, SST-5,\nSQuAD, CoNLL 2003, and SNLI. The experiment results show that CKG achieves SOTA\n89.2 on SQuAD compared with SAN (84.4), ELMo (85.8), and BERT$_{Base}$ (88.5).",
        "pdf_link": "https://arxiv.org/pdf/2212.04909v1.pdf"
    },
    {
        "title": "Towards Better Long-range Time Series Forecasting using Generative Forecasting",
        "authors": [
            "Shiyu Liu",
            "Rohan Ghosh",
            "Mehul Motani"
        ],
        "published": "2022-12-09T13:35:39Z",
        "summary": "Long-range time series forecasting is usually based on one of two existing\nforecasting strategies: Direct Forecasting and Iterative Forecasting, where the\nformer provides low bias, high variance forecasts and the latter leads to low\nvariance, high bias forecasts. In this paper, we propose a new forecasting\nstrategy called Generative Forecasting (GenF), which generates synthetic data\nfor the next few time steps and then makes long-range forecasts based on\ngenerated and observed data. We theoretically prove that GenF is able to better\nbalance the forecasting variance and bias, leading to a much smaller\nforecasting error. We implement GenF via three components: (i) a novel\nconditional Wasserstein Generative Adversarial Network (GAN) based generator\nfor synthetic time series data generation, called CWGAN-TS. (ii) a transformer\nbased predictor, which makes long-range predictions using both generated and\nobserved data. (iii) an information theoretic clustering algorithm to improve\nthe training of both the CWGAN-TS and the transformer based predictor. The\nexperimental results on five public datasets demonstrate that GenF\nsignificantly outperforms a diverse range of state-of-the-art benchmarks and\nclassical approaches. Specifically, we find a 5% - 11% improvement in\npredictive performance (mean absolute error) while having a 15% - 50% reduction\nin parameters compared to the benchmarks. Lastly, we conduct an ablation study\nto further explore and demonstrate the effectiveness of the components\ncomprising GenF.",
        "pdf_link": "https://arxiv.org/pdf/2212.06142v1.pdf"
    },
    {
        "title": "From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader",
        "authors": [
            "Weiwen Xu",
            "Xin Li",
            "Wenxuan Zhang",
            "Meng Zhou",
            "Wai Lam",
            "Luo Si",
            "Lidong Bing"
        ],
        "published": "2022-12-09T10:21:56Z",
        "summary": "We present Pre-trained Machine Reader (PMR), a novel method for retrofitting\npre-trained masked language models (MLMs) to pre-trained machine reading\ncomprehension (MRC) models without acquiring labeled data. PMR can resolve the\ndiscrepancy between model pre-training and downstream fine-tuning of existing\nMLMs. To build the proposed PMR, we constructed a large volume of\ngeneral-purpose and high-quality MRC-style training data by using Wikipedia\nhyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style\npre-training. Apart from its simplicity, PMR effectively solves extraction\ntasks, such as Extractive Question Answering and Named Entity Recognition. PMR\nshows tremendous improvements over existing approaches, especially in\nlow-resource scenarios. When applied to the sequence classification task in the\nMRC formulation, PMR enables the extraction of high-quality rationales to\nexplain the classification process, thereby providing greater prediction\nexplainability. PMR also has the potential to serve as a unified model for\ntackling various extraction and classification tasks in the MRC formulation.",
        "pdf_link": "https://arxiv.org/pdf/2212.04755v3.pdf"
    },
    {
        "title": "The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",
        "authors": [
            "Alexandre Blanco-Gonzalez",
            "Alfonso Cabezon",
            "Alejandro Seco-Gonzalez",
            "Daniel Conde-Torres",
            "Paula Antelo-Riveiro",
            "Angel Pineiro",
            "Rebeca Garcia-Fandino"
        ],
        "published": "2022-12-08T23:23:39Z",
        "summary": "Artificial intelligence (AI) has the potential to revolutionize the drug\ndiscovery process, offering improved efficiency, accuracy, and speed. However,\nthe successful application of AI is dependent on the availability of\nhigh-quality data, the addressing of ethical concerns, and the recognition of\nthe limitations of AI-based approaches. In this article, the benefits,\nchallenges and drawbacks of AI in this field are reviewed, and possible\nstrategies and approaches for overcoming the present obstacles are proposed.\nThe use of data augmentation, explainable AI, and the integration of AI with\ntraditional experimental methods, as well as the potential advantages of AI in\npharmaceutical research are also discussed. Overall, this review highlights the\npotential of AI in drug discovery and provides insights into the challenges and\nopportunities for realizing its potential in this field.\n  Note from the human-authors: This article was created to test the ability of\nChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors\nin writing review articles. The text generated by the AI following our\ninstructions (see Supporting Information) was used as a starting point, and its\nability to automatically generate content was evaluated. After conducting a\nthorough review, human authors practically rewrote the manuscript, striving to\nmaintain a balance between the original proposal and scientific criteria. The\nadvantages and limitations of using AI for this purpose are discussed in the\nlast section.",
        "pdf_link": "https://arxiv.org/pdf/2212.08104v1.pdf"
    },
    {
        "title": "Explain to me like I am five -- Sentence Simplification Using Transformers",
        "authors": [
            "Aman Agarwal"
        ],
        "published": "2022-12-08T22:57:18Z",
        "summary": "Sentence simplification aims at making the structure of text easier to read\nand understand while maintaining its original meaning. This can be helpful for\npeople with disabilities, new language learners, or those with low literacy.\nSimplification often involves removing difficult words and rephrasing the\nsentence. Previous research have focused on tackling this task by either using\nexternal linguistic databases for simplification or by using control tokens for\ndesired fine-tuning of sentences. However, in this paper we purely use\npre-trained transformer models. We experiment with a combination of GPT-2 and\nBERT models, achieving the best SARI score of 46.80 on the Mechanical Turk\ndataset, which is significantly better than previous state-of-the-art results.\nThe code can be found at https://github.com/amanbasu/sentence-simplification.",
        "pdf_link": "https://arxiv.org/pdf/2212.04595v1.pdf"
    },
    {
        "title": "Structured Like a Language Model: Analysing AI as an Automated Subject",
        "authors": [
            "Liam Magee",
            "Vanicka Arora",
            "Luke Munn"
        ],
        "published": "2022-12-08T21:58:43Z",
        "summary": "Drawing from the resources of psychoanalysis and critical media studies, in\nthis paper we develop an analysis of Large Language Models (LLMs) as automated\nsubjects. We argue the intentional fictional projection of subjectivity onto\nLLMs can yield an alternate frame through which AI behaviour, including its\nproductions of bias and harm, can be analysed. First, we introduce language\nmodels, discuss their significance and risks, and outline our case for\ninterpreting model design and outputs with support from psychoanalytic\nconcepts. We trace a brief history of language models, culminating with the\nreleases, in 2022, of systems that realise state-of-the-art natural language\nprocessing performance. We engage with one such system, OpenAI's InstructGPT,\nas a case study, detailing the layers of its construction and conducting\nexploratory and semi-structured interviews with chatbots. These interviews\nprobe the model's moral imperatives to be helpful, truthful and harmless by\ndesign. The model acts, we argue, as the condensation of often competing social\ndesires, articulated through the internet and harvested into training data,\nwhich must then be regulated and repressed. This foundational structure can\nhowever be redirected via prompting, so that the model comes to identify with,\nand transfer, its commitments to the immediate human subject before it. In\nturn, these automated productions of language can lead to the human subject\nprojecting agency upon the model, effecting occasionally further forms of\ncountertransference. We conclude that critical media methods and psychoanalytic\ntheory together offer a productive frame for grasping the powerful new\ncapacities of AI-driven language systems.",
        "pdf_link": "https://arxiv.org/pdf/2212.05058v1.pdf"
    },
    {
        "title": "SpeechLMScore: Evaluating speech generation using speech language model",
        "authors": [
            "Soumi Maiti",
            "Yifan Peng",
            "Takaaki Saeki",
            "Shinji Watanabe"
        ],
        "published": "2022-12-08T21:00:15Z",
        "summary": "While human evaluation is the most reliable metric for evaluating speech\ngeneration systems, it is generally costly and time-consuming. Previous studies\non automatic speech quality assessment address the problem by predicting human\nevaluation scores with machine learning models. However, they rely on\nsupervised learning and thus suffer from high annotation costs and domain-shift\nproblems. We propose SpeechLMScore, an unsupervised metric to evaluate\ngenerated speech using a speech-language model. SpeechLMScore computes the\naverage log-probability of a speech signal by mapping it into discrete tokens\nand measures the average probability of generating the sequence of tokens.\nTherefore, it does not require human annotation and is a highly scalable\nframework. Evaluation results demonstrate that the proposed metric shows a\npromising correlation with human evaluation scores on different speech\ngeneration tasks including voice conversion, text-to-speech, and speech\nenhancement.",
        "pdf_link": "https://arxiv.org/pdf/2212.04559v1.pdf"
    },
    {
        "title": "Learning Video Representations from Large Language Models",
        "authors": [
            "Yue Zhao",
            "Ishan Misra",
            "Philipp Kr\u00e4henb\u00fchl",
            "Rohit Girdhar"
        ],
        "published": "2022-12-08T18:59:59Z",
        "summary": "We introduce LaViLa, a new approach to learning video-language\nrepresentations by leveraging Large Language Models (LLMs). We repurpose\npre-trained LLMs to be conditioned on visual input, and finetune them to create\nautomatic video narrators. Our auto-generated narrations offer a number of\nadvantages, including dense coverage of long videos, better temporal\nsynchronization of the visual information and text, and much higher diversity\nof text. The video-text embedding learned contrastively with these additional\nauto-generated narrations outperforms the previous state-of-the-art on multiple\nfirst-person and third-person video tasks, both in zero-shot and finetuned\nsetups. Most notably, LaViLa obtains an absolute gain of 10.1% on EGTEA\nclassification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks.\nFurthermore, LaViLa trained with only half the narrations from the Ego4D\ndataset outperforms baseline models trained on the full set, and shows positive\nscaling behavior on increasing pre-training data and model size.",
        "pdf_link": "https://arxiv.org/pdf/2212.04501v1.pdf"
    },
    {
        "title": "Implicit causality in GPT-2: a case study",
        "authors": [
            "Hien Huynh",
            "Tomas O. Lentz",
            "Emiel van Miltenburg"
        ],
        "published": "2022-12-08T15:42:38Z",
        "summary": "This case study investigates the extent to which a language model (GPT-2) is\nable to capture native speakers' intuitions about implicit causality in a\nsentence completion task. We first reproduce earlier results (showing lower\nsurprisal values for pronouns that are congruent with either the subject or\nobject, depending on which one corresponds to the implicit causality bias of\nthe verb), and then examine the effects of gender and verb frequency on model\nperformance. Our second study examines the reasoning ability of GPT-2: is the\nmodel able to produce more sensible motivations for why the subject VERBed the\nobject if the verbs have stronger causality biases? We also developed a\nmethodology to avoid human raters being biased by obscenities and disfluencies\ngenerated by the model.",
        "pdf_link": "https://arxiv.org/pdf/2212.04348v1.pdf"
    },
    {
        "title": "Model-based trajectory stitching for improved behavioural cloning and its applications",
        "authors": [
            "Charles A. Hepburn",
            "Giovanni Montana"
        ],
        "published": "2022-12-08T14:18:04Z",
        "summary": "Behavioural cloning (BC) is a commonly used imitation learning method to\ninfer a sequential decision-making policy from expert demonstrations. However,\nwhen the quality of the data is not optimal, the resulting behavioural policy\nalso performs sub-optimally once deployed. Recently, there has been a surge in\noffline reinforcement learning methods that hold the promise to extract\nhigh-quality policies from sub-optimal historical data. A common approach is to\nperform regularisation during training, encouraging updates during policy\nevaluation and/or policy improvement to stay close to the underlying data. In\nthis work, we investigate whether an offline approach to improving the quality\nof the existing data can lead to improved behavioural policies without any\nchanges in the BC algorithm. The proposed data improvement approach -\nTrajectory Stitching (TS) - generates new trajectories (sequences of states and\nactions) by `stitching' pairs of states that were disconnected in the original\ndata and generating their connecting new action. By construction, these new\ntransitions are guaranteed to be highly plausible according to probabilistic\nmodels of the environment, and to improve a state-value function. We\ndemonstrate that the iterative process of replacing old trajectories with new\nones incrementally improves the underlying behavioural policy. Extensive\nexperimental results show that significant performance gains can be achieved\nusing TS over BC policies extracted from the original data. Furthermore, using\nthe D4RL benchmarking suite, we demonstrate that state-of-the-art results are\nobtained by combining TS with two existing offline learning methodologies\nreliant on BC, model-based offline planning (MBOP) and policy constraint\n(TD3+BC).",
        "pdf_link": "https://arxiv.org/pdf/2212.04280v1.pdf"
    },
    {
        "title": "Learning Domain Invariant Prompt for Vision-Language Models",
        "authors": [
            "Cairong Zhao",
            "Yubin Wang",
            "Xinyang Jiang",
            "Yifei Shen",
            "Kaitao Song",
            "Dongsheng Li",
            "Duoqian Miao"
        ],
        "published": "2022-12-08T11:23:24Z",
        "summary": "Prompt learning is one of the most effective and trending ways to adapt\npowerful vision-language foundation models like CLIP to downstream datasets by\ntuning learnable prompt vectors with very few samples. However, although prompt\nlearning achieves excellent performance over in-domain data, it still faces the\nmajor challenge of generalizing to unseen classes and domains. Some existing\nprompt learning methods tackle this issue by adaptively generating different\nprompts for different tokens or domains but neglecting the ability of learned\nprompts to generalize to unseen domains. In this paper, we propose a novel\nprompt learning paradigm that directly generates \\emph{domain invariant} prompt\nthat can be generalized to unseen domains, called MetaPrompt. Specifically, a\ndual-modality prompt tuning network is proposed to generate prompts for input\nfrom both image and text modalities. With a novel asymmetric contrastive loss,\nthe representation from the original pre-trained vision-language model acts as\nsupervision to enhance the generalization ability of the learned prompt. More\nimportantly, we propose a meta-learning-based prompt tuning algorithm that\nexplicitly constrains the task-specific prompt tuned for one domain or class to\nalso achieve good performance in another domain or class. Extensive experiments\non 11 datasets for base-to-new generalization and 4 datasets for domain\ngeneralization demonstrate that our method consistently and significantly\noutperforms existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.04196v2.pdf"
    },
    {
        "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset",
        "authors": [
            "Young-Jun Lee",
            "Byungsoo Ko",
            "Han-Gyu Kim",
            "Jonghwan Hyeon",
            "Ho-Jin Choi"
        ],
        "published": "2022-12-08T07:29:07Z",
        "summary": "As sharing images in an instant message is a crucial factor, there has been\nactive research on learning an image-text multi-modal dialogue models. However,\ntraining a well-generalized multi-modal dialogue model remains challenging due\nto the low quality and limited diversity of images per dialogue in existing\nmulti-modal dialogue datasets. In this paper, we propose an automated pipeline\nto construct a multi-modal dialogue dataset, ensuring both dialogue quality and\nimage diversity without requiring minimum human effort. In our pipeline, to\nguarantee the coherence between images and dialogue, we prompt GPT-4 to infer\npotential image-sharing moments - specifically, the utterance, speaker,\nrationale, and image description. Furthermore, we leverage CLIP similarity to\nmaintain consistency between aligned multiple images to the utterance. Through\nthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modal\ndialogue dataset that surpasses existing datasets in terms of quality and\ndiversity in human evaluation. Our comprehensive experiments highlight that\nwhen multi-modal dialogue models are trained using our dataset, their\ngeneralization performance on unseen dialogue datasets is significantly\nenhanced. We make our source code and dataset publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2212.04119v2.pdf"
    },
    {
        "title": "NP4G : Network Programming for Generalization",
        "authors": [
            "Shoichiro Hara",
            "Yuji Watanabe"
        ],
        "published": "2022-12-08T06:18:44Z",
        "summary": "Automatic programming has been actively studied for a long time by various\napproaches including genetic programming. In recent years, automatic\nprogramming using neural networks such as GPT-3 has been actively studied and\nis attracting a lot of attention. However, these methods are illogical\ninference based on experience by enormous learning, and their thinking process\nis unclear. Even using the method by logical inference with a clear thinking\nprocess, the system that automatically generates any programs has not yet been\nrealized. Especially, the inductive inference generalized by logical inference\nfrom one example is an important issue that the artificial intelligence can\nacquire knowledge by itself. In this study, we propose NP4G: Network\nProgramming for Generalization, which can automatically generate programs by\ninductive inference. Because the proposed method can realize \"sequence\",\n\"selection\", and \"iteration\" in programming and can satisfy the conditions of\nthe structured program theorem, it is expected that NP4G is a method\nautomatically acquire any programs by inductive inference. As an example, we\nautomatically construct a bitwise NOT operation program from several training\ndata by generalization using NP4G. Although NP4G only randomly selects and\nconnects nodes, by adjusting the number of nodes and the number of phase of\n\"Phased Learning\", we show the bitwise NOT operation programs are acquired in a\ncomparatively short time and at a rate of about 7 in 10 running. The source\ncode of NP4G is available on GitHub as a public repository.",
        "pdf_link": "https://arxiv.org/pdf/2212.11118v1.pdf"
    },
    {
        "title": "Successive Prompting for Decomposing Complex Questions",
        "authors": [
            "Dheeru Dua",
            "Shivanshu Gupta",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "published": "2022-12-08T06:03:38Z",
        "summary": "Answering complex questions that require making latent decisions is a\nchallenging task, especially when limited supervision is available. Recent\nworks leverage the capabilities of large language models (LMs) to perform\ncomplex question answering in a few-shot setting by demonstrating how to output\nintermediate rationalizations while solving the complex question in a single\npass. We introduce ``Successive Prompting'', where we iteratively break down a\ncomplex task into a simple task, solve it, and then repeat the process until we\nget the final solution. Successive prompting decouples the supervision for\ndecomposing complex questions from the supervision for answering simple\nquestions, allowing us to (1) have multiple opportunities to query in-context\nexamples at each reasoning step (2) learn question decomposition separately\nfrom question answering, including using synthetic data, and (3) use bespoke\n(fine-tuned) components for reasoning steps where a large LM does not perform\nwell. The intermediate supervision is typically manually written, which can be\nexpensive to collect. We introduce a way to generate a synthetic dataset which\ncan be used to bootstrap a model's ability to decompose and answer intermediate\nquestions. Our best model (with successive prompting) achieves an improvement\nof ~5% absolute F1 on a few-shot version of the DROP dataset when compared with\na state-of-the-art model with the same supervision.",
        "pdf_link": "https://arxiv.org/pdf/2212.04092v1.pdf"
    },
    {
        "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
        "authors": [
            "Chan Hee Song",
            "Jiaman Wu",
            "Clayton Washington",
            "Brian M. Sadler",
            "Wei-Lun Chao",
            "Yu Su"
        ],
        "published": "2022-12-08T05:46:32Z",
        "summary": "This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner",
        "pdf_link": "https://arxiv.org/pdf/2212.04088v3.pdf"
    },
    {
        "title": "RainUNet for Super-Resolution Rain Movie Prediction under Spatio-temporal Shifts",
        "authors": [
            "Jinyoung Park",
            "Minseok Son",
            "Seungju Cho",
            "Inyoung Lee",
            "Changick Kim"
        ],
        "published": "2022-12-07T23:42:39Z",
        "summary": "This paper presents a solution to the Weather4cast 2022 Challenge Stage 2.\nThe goal of the challenge is to forecast future high-resolution rainfall events\nobtained from ground radar using low-resolution multiband satellite images. We\nsuggest a solution that performs data preprocessing appropriate to the\nchallenge and then predicts rainfall movies using a novel RainUNet. RainUNet is\na hierarchical U-shaped network with temporal-wise separable block (TS block)\nusing a decoupled large kernel 3D convolution to improve the prediction\nperformance. Various evaluation metrics show that our solution is effective\ncompared to the baseline method. The source codes are available at\nhttps://github.com/jinyxp/Weather4cast-2022",
        "pdf_link": "https://arxiv.org/pdf/2212.04005v1.pdf"
    },
    {
        "title": "TweetDrought: A Deep-Learning Drought Impacts Recognizer based on Twitter Data",
        "authors": [
            "Beichen Zhang",
            "Frank Schilder",
            "Kelly Helm Smith",
            "Michael J. Hayes",
            "Sherri Harms",
            "Tsegaye Tadesse"
        ],
        "published": "2022-12-07T23:21:36Z",
        "summary": "Acquiring a better understanding of drought impacts becomes increasingly\nvital under a warming climate. Traditional drought indices describe mainly\nbiophysical variables and not impacts on social, economic, and environmental\nsystems. We utilized natural language processing and bidirectional encoder\nrepresentation from Transformers (BERT) based transfer learning to fine-tune\nthe model on the data from the news-based Drought Impact Report (DIR) and then\napply it to recognize seven types of drought impacts based on the filtered\nTwitter data from the United States. Our model achieved a satisfying macro-F1\nscore of 0.89 on the DIR test set. The model was then applied to California\ntweets and validated with keyword-based labels. The macro-F1 score was 0.58.\nHowever, due to the limitation of keywords, we also spot-checked tweets with\ncontroversial labels. 83.5% of BERT labels were correct compared to the keyword\nlabels. Overall, the fine-tuned BERT-based recognizer provided proper\npredictions and valuable information on drought impacts. The interpretation and\nanalysis of the model were consistent with experiential domain expertise.",
        "pdf_link": "https://arxiv.org/pdf/2212.04001v1.pdf"
    },
    {
        "title": "Discovering Latent Knowledge in Language Models Without Supervision",
        "authors": [
            "Collin Burns",
            "Haotian Ye",
            "Dan Klein",
            "Jacob Steinhardt"
        ],
        "published": "2022-12-07T18:17:56Z",
        "summary": "Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a method for accurately answering yes-no questions\ngiven only unlabeled model activations. It works by finding a direction in\nactivation space that satisfies logical consistency properties, such as that a\nstatement and its negation have opposite truth values. We show that despite\nusing no supervision and no model outputs, our method can recover diverse\nknowledge represented in large language models: across 6 models and 10\nquestion-answering datasets, it outperforms zero-shot accuracy by 4\\% on\naverage. We also find that it cuts prompt sensitivity in half and continues to\nmaintain high accuracy even when models are prompted to generate incorrect\nanswers. Our results provide an initial step toward discovering what language\nmodels know, distinct from what they say, even when we don't have access to\nexplicit ground truth labels.",
        "pdf_link": "https://arxiv.org/pdf/2212.03827v2.pdf"
    },
    {
        "title": "Robustness of Learning from Task Instructions",
        "authors": [
            "Jiasheng Gu",
            "Hongyu Zhao",
            "Hanzi Xu",
            "Liangyu Nie",
            "Hongyuan Mei",
            "Wenpeng Yin"
        ],
        "published": "2022-12-07T17:54:59Z",
        "summary": "Traditional supervised learning mostly works on individual tasks and requires\ntraining on a large set of task-specific examples. This paradigm seriously\nhinders the development of task generalization since preparing a task-specific\nexample set is costly. To build a system that can quickly and easily generalize\nto new tasks, task instructions have been adopted as an emerging trend of\nsupervision recently. These instructions give the model the definition of the\ntask and allow the model to output the appropriate answer based on the\ninstructions and inputs. However, task instructions are often expressed in\ndifferent forms, which can be interpreted from two threads: first, some\ninstructions are short sentences and are pretrained language model (PLM)\noriented, such as prompts, while other instructions are paragraphs and are\nhuman-oriented, such as those in Amazon MTurk; second, different end-users very\nlikely explain the same task with instructions of different textual\nexpressions. A robust system for task generalization should be able to handle\nany new tasks regardless of the variability of instructions.\n  However, the system robustness in dealing with instruction-driven task\ngeneralization is still unexplored. This work investigates the system\nrobustness when the instructions of new tasks are (i) manipulated, (ii)\nparaphrased, or (iii) from different levels of conciseness. To our knowledge,\nthis is the first work that systematically studies how robust a PLM is when it\nis supervised by instructions with different factors of variability.",
        "pdf_link": "https://arxiv.org/pdf/2212.03813v4.pdf"
    },
    {
        "title": "Pre-Training With Scientific Text Improves Educational Question Generation",
        "authors": [
            "Hamze Muse",
            "Sahan Bulathwela",
            "Emine Yilmaz"
        ],
        "published": "2022-12-07T17:17:58Z",
        "summary": "With the boom of digital educational materials and scalable e-learning\nsystems, the potential for realising AI-assisted personalised learning has\nskyrocketed. In this landscape, the automatic generation of educational\nquestions will play a key role, enabling scalable self-assessment when a global\npopulation is manoeuvring their personalised learning journeys. We develop\nEduQG, a novel educational question generation model built by adapting a large\nlanguage model. Our initial experiments demonstrate that EduQG can produce\nsuperior educational questions by pre-training on scientific text.",
        "pdf_link": "https://arxiv.org/pdf/2212.03869v1.pdf"
    },
    {
        "title": "Memorization of Named Entities in Fine-tuned BERT Models",
        "authors": [
            "Andor Diera",
            "Nicolas Lell",
            "Aygul Garifullina",
            "Ansgar Scherp"
        ],
        "published": "2022-12-07T16:20:50Z",
        "summary": "Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets, which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differentially Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a detrimental\neffect on the text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.",
        "pdf_link": "https://arxiv.org/pdf/2212.03749v2.pdf"
    },
    {
        "title": "G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks",
        "authors": [
            "Zhongwei Wan",
            "Yichun Yin",
            "Wei Zhang",
            "Jiaxin Shi",
            "Lifeng Shang",
            "Guangyong Chen",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022-12-07T13:07:24Z",
        "summary": "Recently, domain-specific PLMs have been proposed to boost the task\nperformance of specific domains (e.g., biomedical and computer science) by\ncontinuing to pre-train general PLMs with domain-specific corpora. However,\nthis Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to\nforget the previous general knowledge acquired by general PLMs, which leads to\na catastrophic forgetting phenomenon and sub-optimal performance. To alleviate\nthis problem, we propose a new framework of General Memory Augmented\nPre-trained Language Model (G-MAP), which augments the domain-specific PLM by a\nmemory representation built from the frozen general PLM without losing any\ngeneral knowledge. Specifically, we propose a new memory-augmented layer, and\nbased on it, different augmented strategies are explored to build the memory\nrepresentation and then adaptively fuse it into the domain-specific PLM. We\ndemonstrate the effectiveness of G-MAP on various domains (biomedical and\ncomputer science publications, news, and reviews) and different kinds (text\nclassification, QA, NER) of tasks, and the extensive results show that the\nproposed G-MAP can achieve SOTA results on all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.03613v3.pdf"
    },
    {
        "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
        "authors": [
            "Conglong Li",
            "Zhewei Yao",
            "Xiaoxia Wu",
            "Minjia Zhang",
            "Connor Holmes",
            "Cheng Li",
            "Yuxiong He"
        ],
        "published": "2022-12-07T12:27:28Z",
        "summary": "Recent advances on deep learning models come at the price of formidable\ntraining cost. The increasing model size is one of the root causes, but another\nless-emphasized fact is that data scale is actually increasing at a similar\nspeed as model scale, and the training cost is proportional to both of them.\nCompared to the rapidly evolving model architecture, how to efficiently use the\ntraining data (especially for the expensive foundation model pretraining) is\nboth less explored and difficult to realize due to the lack of a convenient\nframework that focuses on data efficiency capabilities. To this end, we present\nDeepSpeed Data Efficiency, a framework that makes better use of data, increases\ntraining efficiency, and improves model quality. Specifically, we propose and\ncombine two data efficiency techniques: efficient data sampling via a general\ncurriculum learning library, and efficient data routing via a novel random\nlayerwise token dropping technique. For GPT-3 1.3B language model pretraining,\nour work achieves 12.5x less data/time/cost (\\$3.7K if rent on Azure), while\nstill maintaining 95% of model quality compared to baseline with full data and\ncost (\\$46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also\nachieve the same model quality with up to 2x less data/time/cost, or achieve\nbetter model quality under same data/time/cost. DeepSpeed Data Efficiency is\neasy to use and tune, enabling us to easily apply it and verify its benefit on\nadditional tasks including GPT-3 MoE model pretraining and small-scale\nGPT-2/ViT finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2212.03597v3.pdf"
    },
    {
        "title": "Talking About Large Language Models",
        "authors": [
            "Murray Shanahan"
        ],
        "published": "2022-12-07T10:01:44Z",
        "summary": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.",
        "pdf_link": "https://arxiv.org/pdf/2212.03551v5.pdf"
    },
    {
        "title": "A Generative Approach for Script Event Prediction via Contrastive Fine-tuning",
        "authors": [
            "Fangqi Zhu",
            "Jun Gao",
            "Changlong Yu",
            "Wei Wang",
            "Chen Xu",
            "Xin Mu",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2022-12-07T07:32:47Z",
        "summary": "Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.",
        "pdf_link": "https://arxiv.org/pdf/2212.03496v3.pdf"
    },
    {
        "title": "SimVTP: Simple Video Text Pre-training with Masked Autoencoders",
        "authors": [
            "Yue Ma",
            "Tianyu Yang",
            "Yin Shan",
            "Xiu Li"
        ],
        "published": "2022-12-07T07:14:22Z",
        "summary": "This paper presents SimVTP: a Simple Video-Text Pretraining framework via\nmasked autoencoders. We randomly mask out the spatial-temporal tubes of input\nvideo and the word tokens of input text and then feed them into a unified\nautencoder to reconstruct the missing pixels and words. Our SimVTP has several\nproperties: 1) Thanks to the unified autoencoder, SimVTP reconstructs the\nmasked signal of one modality with the help from another modality, which\nimplicitly learns the cross-modal alignment between video tubes and text\ntokens. 2) SimVTP not only benefits from a high video masking ratio (e.g. 90%)\ndue to the temporal redundancy of video, but also needs a high text masking\nratio (e.g. 75%), which is much higher than BERT (e.g. 15%), to achieve optimal\nperformance. This is because the aid of video modality makes text\nreconstruction less challenging, which thus needs a higher mask ratio to make\nthe pretext harder for useful feature learning. 3) Equipping SimVTP with\nvideo-text contrastive learning (VTC) and video-text matching (VTM), which are\ntwo commonly used cross-modal training strategies, could further improve the\ntransferable performance significantly. 4) SimVTP is dataefficent, e.g.,\npre-training only on 10% data of WebVid-2M, SimVTP achieves surprisingly good\nresults (43.8 R@1) on MSRVTT, which is far above recent state-of-the-art\nmethods pre-trained on both CC3M and WebVid-2M. We transfer our pre-trained\nmodel to various downstream tasks and achieve superior performance. The codes\nand models will be released at https://github.com/mayuelala/SimVTP.",
        "pdf_link": "https://arxiv.org/pdf/2212.03490v1.pdf"
    },
    {
        "title": "Towards using Few-Shot Prompt Learning for Automating Model Completion",
        "authors": [
            "Meriem Ben Chaaben",
            "Lola Burgue\u00f1o",
            "Houari Sahraoui"
        ],
        "published": "2022-12-07T02:11:26Z",
        "summary": "We propose a simple yet a novel approach to improve completion in domain\nmodeling activities. Our approach exploits the power of large language models\nby using few-shot prompt learning without the need to train or fine-tune those\nmodels with large datasets that are scarce in this field. We implemented our\napproach and tested it on the completion of static and dynamic domain diagrams.\nOur initial evaluation shows that such an approach is effective and can be\nintegrated in different ways during the modeling activities.",
        "pdf_link": "https://arxiv.org/pdf/2212.03404v1.pdf"
    },
    {
        "title": "Contactless Oxygen Monitoring with Gated Transformer",
        "authors": [
            "Hao He",
            "Yuan Yuan",
            "Ying-Cong Chen",
            "Peng Cao",
            "Dina Katabi"
        ],
        "published": "2022-12-06T22:43:59Z",
        "summary": "With the increasing popularity of telehealth, it becomes critical to ensure\nthat basic physiological signals can be monitored accurately at home, with\nminimal patient overhead. In this paper, we propose a contactless approach for\nmonitoring patients' blood oxygen at home, simply by analyzing the radio\nsignals in the room, without any wearable devices. We extract the patients'\nrespiration from the radio signals that bounce off their bodies and devise a\nnovel neural network that infers a patient's oxygen estimates from their\nbreathing signal. Our model, called \\emph{Gated BERT-UNet}, is designed to\nadapt to the patient's medical indices (e.g., gender, sleep stages). It has\nmultiple predictive heads and selects the most suitable head via a gate\ncontrolled by the person's physiological indices. Extensive empirical results\nshow that our model achieves high accuracy on both medical and radio datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.03357v1.pdf"
    },
    {
        "title": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?",
        "authors": [
            "Jiaxuan Li",
            "Lang Yu",
            "Allyson Ettinger"
        ],
        "published": "2022-12-06T19:22:25Z",
        "summary": "Current pre-trained language models have enabled remarkable improvements in\ndownstream tasks, but it remains difficult to distinguish effects of\nstatistical correlation from more systematic logical reasoning grounded on\nunderstanding of the real world. In this paper we tease these factors apart by\nleveraging counterfactual conditionals, which force language models to predict\nunusual consequences based on hypothetical propositions. We introduce a set of\ntests drawn from psycholinguistic experiments, as well as larger-scale\ncontrolled datasets, to probe counterfactual predictions from a variety of\npopular pre-trained language models. We find that models are consistently able\nto override real-world knowledge in counterfactual scenarios, and that this\neffect is more robust in case of stronger baseline world knowledge -- however,\nwe also find that for most models this effect appears largely to be driven by\nsimple lexical cues. When we mitigate effects of both world knowledge and\nlexical cues to test knowledge of linguistic nuances of counterfactuals, we\nfind that only GPT-3 shows sensitivity to these nuances, though this\nsensitivity is also non-trivially impacted by lexical associative factors.",
        "pdf_link": "https://arxiv.org/pdf/2212.03278v1.pdf"
    },
    {
        "title": "P\u00d8DA: Prompt-driven Zero-shot Domain Adaptation",
        "authors": [
            "Mohammad Fahes",
            "Tuan-Hung Vu",
            "Andrei Bursuc",
            "Patrick P\u00e9rez",
            "Raoul de Charette"
        ],
        "published": "2022-12-06T18:59:58Z",
        "summary": "Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a general description in natural language of the target domain,\ni.e., a prompt. First, we leverage a pretrained contrastive vision-language\nmodel (CLIP) to optimize affine transformations of source features, steering\nthem towards the target text embedding while preserving their content and\nsemantics. To achieve this, we propose Prompt-driven Instance Normalization\n(PIN). Second, we show that these prompt-driven augmentations can be used to\nperform zero-shot domain adaptation for semantic segmentation. Experiments\ndemonstrate that our method significantly outperforms CLIP-based style transfer\nbaselines on several datasets for the downstream task at hand, even surpassing\none-shot unsupervised domain adaptation. A similar boost is observed on object\ndetection and image classification. The code is available at\nhttps://github.com/astra-vision/PODA .",
        "pdf_link": "https://arxiv.org/pdf/2212.03241v3.pdf"
    },
    {
        "title": "ADIR: Adaptive Diffusion for Image Reconstruction",
        "authors": [
            "Shady Abu-Hussein",
            "Tom Tirer",
            "Raja Giryes"
        ],
        "published": "2022-12-06T18:39:58Z",
        "summary": "In recent years, denoising diffusion models have demonstrated outstanding\nimage generation performance. The information on natural images captured by\nthese models is useful for many image reconstruction applications, where the\ntask is to restore a clean image from its degraded observations. In this work,\nwe propose a conditional sampling scheme that exploits the prior learned by\ndiffusion models while retaining agreement with the observations. We then\ncombine it with a novel approach for adapting pretrained diffusion denoising\nnetworks to their input. We examine two adaption strategies: the first uses\nonly the degraded image, while the second, which we advocate, is performed\nusing images that are ``nearest neighbors'' of the degraded image, retrieved\nfrom a diverse dataset using an off-the-shelf visual-language model. To\nevaluate our method, we test it on two state-of-the-art publicly available\ndiffusion models, Stable Diffusion and Guided Diffusion. We show that our\nproposed `adaptive diffusion for image reconstruction' (ADIR) approach achieves\na significant improvement in the super-resolution, deblurring, and text-based\nediting tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.03221v1.pdf"
    },
    {
        "title": "Style transfer and classification in hebrew news items",
        "authors": [
            "Nir Weingarten"
        ],
        "published": "2022-12-06T14:47:29Z",
        "summary": "Hebrew is a Morphological rich language, making its modeling harder than\nsimpler language. Recent developments such as Transformers in general and Bert\nin particular opened a path for Hebrew models that reach SOTA results, not\nfalling short from other non-MRL languages. We explore the cutting edge in this\nfield performing style transfer, text generation and classification over news\narticles collected from online archives. Furthermore, the news portals that\nfeed our collective consciousness are an interesting corpus to study, as their\nanalysis and tracing might reveal insights about our society and discourse.",
        "pdf_link": "https://arxiv.org/pdf/2212.03019v1.pdf"
    },
    {
        "title": "SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies",
        "authors": [
            "Zehao Yu",
            "Xi Yang",
            "Chong Dang",
            "Prakash Adekkanattu",
            "Braja Gopal Patra",
            "Yifan Peng",
            "Jyotishman Pathak",
            "Debbie L. Wilson",
            "Ching-Yuan Chang",
            "Wei-Hsuan Lo-Ciganic",
            "Thomas J. George",
            "William R. Hogan",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2022-12-06T14:23:38Z",
        "summary": "Objective: We aim to develop an open-source natural language processing (NLP)\npackage, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models\nto extract social determinants of health (SDoH) for cancer patients, examine\nthe generalizability of SODA to a new disease domain (i.e., opioid use), and\nevaluate the extraction rate of SDoH using cancer populations.\n  Methods: We identified SDoH categories and attributes and developed an SDoH\ncorpus using clinical notes from a general cancer cohort. We compared four\ntransformer-based NLP models to extract SDoH, examined the generalizability of\nNLP models to a cohort of patients prescribed with opioids, and explored\ncustomization strategies to improve performance. We applied the best NLP model\nto extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804),\nand colorectal cancer (n=6,240) cohorts.\n  Results and Conclusion: We developed a corpus of 629 cancer patients notes\nwith annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH.\nThe Bidirectional Encoder Representations from Transformers (BERT) model\nachieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH\nconcept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts.\nFine-tuning the NLP models using new annotations from opioid use patients\nimproved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The\nextraction rates among 19 categories of SDoH varied greatly, where 10 SDoH\ncould be extracted from >70% of cancer patients, but 9 SDoH had a low\nextraction rate (<70% of cancer patients). The SODA package with pre-trained\ntransformer models is publicly available at\nhttps://github.com/uf-hobiinformatics-lab/SDoH_SODA.",
        "pdf_link": "https://arxiv.org/pdf/2212.03000v2.pdf"
    },
    {
        "title": "CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain",
        "authors": [
            "Markus Bayer",
            "Philipp Kuehn",
            "Ramin Shanehsaz",
            "Christian Reuter"
        ],
        "published": "2022-12-06T13:49:12Z",
        "summary": "The field of cybersecurity is evolving fast. Experts need to be informed\nabout past, current and - in the best case - upcoming threats, because attacks\nare becoming more advanced, targets bigger and systems more complex. As this\ncannot be addressed manually, cybersecurity experts need to rely on machine\nlearning techniques. In the texutual domain, pre-trained language models like\nBERT have shown to be helpful, by providing a good baseline for further\nfine-tuning. However, due to the domain-knowledge and many technical terms in\ncybersecurity general language models might miss the gist of textual\ninformation, hence doing more harm than good. For this reason, we create a\nhigh-quality dataset and present a language model specifically tailored to the\ncybersecurity domain, which can serve as a basic building block for\ncybersecurity systems that deal with natural language. The model is compared\nwith other models based on 15 different domain-dependent extrinsic and\nintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the\none hand, the results of the intrinsic tasks show that our model improves the\ninternal representation space of words compared to the other models. On the\nother hand, the extrinsic, domain-dependent tasks, consisting of sequence\ntagging and classification, show that the model is best in specific application\nscenarios, in contrast to the others. Furthermore, we show that our approach\nagainst catastrophic forgetting works, as the model is able to retrieve the\npreviously trained domain-independent knowledge. The used dataset and trained\nmodel are made publicly available",
        "pdf_link": "https://arxiv.org/pdf/2212.02974v1.pdf"
    },
    {
        "title": "M-VADER: A Model for Diffusion with Multimodal Context",
        "authors": [
            "Samuel Weinbach",
            "Marco Bellagente",
            "Constantin Eichenberg",
            "Andrew Dai",
            "Robert Baldock",
            "Souradeep Nanda",
            "Bj\u00f6rn Deiseroth",
            "Koen Oostermeijer",
            "Hannah Teufel",
            "Andres Felipe Cruz-Salinas"
        ],
        "published": "2022-12-06T12:45:21Z",
        "summary": "We introduce M-VADER: a diffusion model (DM) for image generation where the\noutput can be specified using arbitrary combinations of images and text. We\nshow how M-VADER enables the generation of images specified using combinations\nof image and text, and combinations of multiple images. Previously, a number of\nsuccessful DM image generation algorithms have been introduced that make it\npossible to specify the output image using a text prompt. Inspired by the\nsuccess of those models, and led by the notion that language was already\ndeveloped to describe the elements of visual contexts that humans find most\nimportant, we introduce an embedding model closely related to a vision-language\nmodel. Specifically, we introduce the embedding model S-MAGMA: a 13 billion\nparameter multimodal decoder combining components from an autoregressive\nvision-language model MAGMA and biases finetuned for semantic search.",
        "pdf_link": "https://arxiv.org/pdf/2212.02936v2.pdf"
    },
    {
        "title": "Modern French Poetry Generation with RoBERTa and GPT-2",
        "authors": [
            "Mika H\u00e4m\u00e4l\u00e4inen",
            "Khalid Alnajjar",
            "Thierry Poibeau"
        ],
        "published": "2022-12-06T12:10:14Z",
        "summary": "We present a novel neural model for modern poetry generation in French. The\nmodel consists of two pretrained neural models that are fine-tuned for the poem\ngeneration task. The encoder of the model is a RoBERTa based one while the\ndecoder is based on GPT-2. This way the model can benefit from the superior\nnatural language understanding performance of RoBERTa and the good natural\nlanguage generation performance of GPT-2. Our evaluation shows that the model\ncan create French poetry successfully. On a 5 point scale, the lowest score of\n3.57 was given by human judges to typicality and emotionality of the output\npoetry while the best score of 3.79 was given to understandability.",
        "pdf_link": "https://arxiv.org/pdf/2212.02911v1.pdf"
    },
    {
        "title": "Adaptive Testing of Computer Vision Models",
        "authors": [
            "Irena Gao",
            "Gabriel Ilharco",
            "Scott Lundberg",
            "Marco Tulio Ribeiro"
        ],
        "published": "2022-12-06T05:52:31Z",
        "summary": "Vision models often fail systematically on groups of data that share common\nsemantic characteristics (e.g., rare objects or unusual scenes), but\nidentifying these failure modes is a challenge. We introduce AdaVision, an\ninteractive process for testing vision models which helps users identify and\nfix coherent failure modes. Given a natural language description of a coherent\ngroup, AdaVision retrieves relevant images from LAION-5B with CLIP. The user\nthen labels a small amount of data for model correctness, which is used in\nsuccessive retrieval rounds to hill-climb towards high-error regions, refining\nthe group definition. Once a group is saturated, AdaVision uses GPT-3 to\nsuggest new group descriptions for the user to explore. We demonstrate the\nusefulness and generality of AdaVision in user studies, where users find major\nbugs in state-of-the-art classification, object detection, and image captioning\nmodels. These user-discovered groups have failure rates 2-3x higher than those\nsurfaced by automatic error clustering methods. Finally, finetuning on examples\nfound with AdaVision fixes the discovered bugs when evaluated on unseen\nexamples, without degrading in-distribution accuracy, and while also improving\nperformance on out-of-distribution datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.02774v2.pdf"
    },
    {
        "title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
        "authors": [
            "Hongwei Han",
            "Jialiang Xu",
            "Mengyu Zhou",
            "Yijia Shao",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022-12-06T01:31:37Z",
        "summary": "Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -> 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).",
        "pdf_link": "https://arxiv.org/pdf/2212.02691v2.pdf"
    },
    {
        "title": "INCLUSIFY: A benchmark and a model for gender-inclusive German",
        "authors": [
            "David Pomerenke"
        ],
        "published": "2022-12-05T19:37:48Z",
        "summary": "Gender-inclusive language is important for achieving gender equality in\nlanguages with gender inflections, such as German. While stirring some\ncontroversy, it is increasingly adopted by companies and political\ninstitutions. A handful of tools have been developed to help people use\ngender-inclusive language by identifying instances of the generic masculine and\nproviding suggestions for more inclusive reformulations. In this report, we\ndefine the underlying tasks in terms of natural language processing, and\npresent a dataset and measures for benchmarking them. We also present a model\nthat implements these tasks, by combining an inclusive language database with\nan elaborate sequence of processing steps via standard pre-trained models. Our\nmodel achieves a recall of 0.89 and a precision of 0.82 in our benchmark for\nidentifying exclusive language; and one of its top five suggestions is chosen\nin real-world texts in 44% of cases. We sketch how the area could be further\nadvanced by training end-to-end models and using large language models; and we\nurge the community to include more gender-inclusive texts in their training\ndata in order to not present an obstacle to the adoption of gender-inclusive\nlanguage. Through these efforts, we hope to contribute to restoring justice in\nlanguage and, to a small extent, in reality.",
        "pdf_link": "https://arxiv.org/pdf/2212.02564v1.pdf"
    },
    {
        "title": "In-context Examples Selection for Machine Translation",
        "authors": [
            "Sweta Agrawal",
            "Chunting Zhou",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Marjan Ghazvininejad"
        ],
        "published": "2022-12-05T17:25:15Z",
        "summary": "Large-scale generative models show an impressive ability to perform a wide\nrange of Natural Language Processing (NLP) tasks using in-context learning,\nwhere a few examples are used to describe a task to the model. For Machine\nTranslation (MT), these examples are typically randomly sampled from the\ndevelopment dataset with a similar distribution as the evaluation set. However,\nit is unclear how the choice of these in-context examples and their ordering\nimpacts the output translation quality. In this work, we aim to understand the\nproperties of good in-context examples for MT in both in-domain and\nout-of-domain settings. We show that the translation quality and the domain of\nthe in-context examples matter and that 1-shot noisy unrelated example can have\na catastrophic impact on output quality. While concatenating multiple random\nexamples reduces the effect of noise, a single good prompt optimized to\nmaximize translation quality on the development dataset can elicit learned\ninformation from the pre-trained language model. Adding similar examples based\non an n-gram overlap with the test source significantly and consistently\nimproves the translation quality of the outputs, outperforming a strong kNN-MT\nbaseline in 2 out of 4 out-of-domain datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.02437v1.pdf"
    },
    {
        "title": "Audio-Driven Co-Speech Gesture Video Generation",
        "authors": [
            "Xian Liu",
            "Qianyi Wu",
            "Hang Zhou",
            "Yuanqi Du",
            "Wayne Wu",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "published": "2022-12-05T15:28:22Z",
        "summary": "Co-speech gesture is crucial for human-machine interaction and digital\nentertainment. While previous works mostly map speech audio to human skeletons\n(e.g., 2D keypoints), directly generating speakers' gestures in the image\ndomain remains unsolved. In this work, we formally define and study this\nchallenging problem of audio-driven co-speech gesture video generation, i.e.,\nusing a unified framework to generate speaker image sequence driven by speech\naudio. Our key insight is that the co-speech gestures can be decomposed into\ncommon motion patterns and subtle rhythmic dynamics. To this end, we propose a\nnovel framework, Audio-driveN Gesture vIdeo gEneration (ANGIE), to effectively\ncapture the reusable co-speech gesture patterns as well as fine-grained\nrhythmic movements. To achieve high-fidelity image sequence generation, we\nleverage an unsupervised motion representation instead of a structural human\nbody prior (e.g., 2D skeletons). Specifically, 1) we propose a vector quantized\nmotion extractor (VQ-Motion Extractor) to summarize common co-speech gesture\npatterns from implicit motion representation to codebooks. 2) Moreover, a\nco-speech gesture GPT with motion refinement (Co-Speech GPT) is devised to\ncomplement the subtle prosodic motion details. Extensive experiments\ndemonstrate that our framework renders realistic and vivid co-speech gesture\nvideo. Demo video and more resources can be found in:\nhttps://alvinliu0.github.io/projects/ANGIE",
        "pdf_link": "https://arxiv.org/pdf/2212.02350v1.pdf"
    },
    {
        "title": "I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification",
        "authors": [
            "Muhammad Ferjad Naeem",
            "Muhammad Gul Zain Ali Khan",
            "Yongqin Xian",
            "Muhammad Zeshan Afzal",
            "Didier Stricker",
            "Luc Van Gool",
            "Federico Tombari"
        ],
        "published": "2022-12-05T14:11:36Z",
        "summary": "Recent works have shown that unstructured text (documents) from online\nsources can serve as useful auxiliary information for zero-shot image\nclassification. However, these methods require access to a high-quality source\nlike Wikipedia and are limited to a single source of information. Large\nLanguage Models (LLM) trained on web-scale text show impressive abilities to\nrepurpose their learned knowledge for a multitude of tasks. In this work, we\nprovide a novel perspective on using an LLM to provide text supervision for a\nzero-shot image classification model. The LLM is provided with a few text\ndescriptions from different annotators as examples. The LLM is conditioned on\nthese examples to generate multiple text descriptions for each class(referred\nto as views). Our proposed model, I2MVFormer, learns multi-view semantic\nembeddings for zero-shot image classification with these class views. We show\nthat each text view of a class provides complementary information allowing a\nmodel to learn a highly discriminative class embedding. Moreover, we show that\nI2MVFormer is better at consuming the multi-view text supervision from LLM\ncompared to baseline models. I2MVFormer establishes a new state-of-the-art on\nthree public benchmark datasets for zero-shot image classification with\nunsupervised semantic embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2212.02291v1.pdf"
    },
    {
        "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction",
        "authors": [
            "Dietrich Trautmann",
            "Alina Petrova",
            "Frank Schilder"
        ],
        "published": "2022-12-05T12:17:02Z",
        "summary": "Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide and\nassist a large language model (LLM) with performing a natural legal language\nprocessing (NLLP) skill. Our goal is to use LPE with LLMs over long legal\ndocuments for the Legal Judgement Prediction (LJP) task. We investigate the\nperformance of zero-shot LPE for given facts in case-texts from the European\nCourt of Human Rights (in English) and the Federal Supreme Court of Switzerland\n(in German, French and Italian). Our results show that zero-shot LPE is better\ncompared to the baselines, but it still falls short compared to current state\nof the art supervised approaches. Nevertheless, the results are important,\nsince there was 1) no explicit domain-specific data used - so we show that the\ntransfer to the legal domain is possible for general-purpose LLMs, and 2) the\nLLMs where directly applied without any further training or fine-tuning - which\nin turn saves immensely in terms of additional computational costs.",
        "pdf_link": "https://arxiv.org/pdf/2212.02199v1.pdf"
    },
    {
        "title": "Automatic Generation of Factual News Headlines in Finnish",
        "authors": [
            "Maximilian Koppatz",
            "Khalid Alnajjar",
            "Mika H\u00e4m\u00e4l\u00e4inen",
            "Thierry Poibeau"
        ],
        "published": "2022-12-05T11:12:14Z",
        "summary": "We present a novel approach to generating news headlines in Finnish for a\ngiven news story. We model this as a summarization task where a model is given\na news article, and its task is to produce a concise headline describing the\nmain topic of the article. Because there are no openly available GPT-2 models\nfor Finnish, we will first build such a model using several corpora. The model\nis then fine-tuned for the headline generation task using a massive news\ncorpus. The system is evaluated by 3 expert journalists working in a Finnish\nmedia house. The results showcase the usability of the presented approach as a\nheadline suggestion tool to facilitate the news production process.",
        "pdf_link": "https://arxiv.org/pdf/2212.02170v1.pdf"
    },
    {
        "title": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas Dialog",
        "authors": [
            "Mika H\u00e4m\u00e4l\u00e4inen",
            "Khalid Alnajjar",
            "Thierry Poibeau"
        ],
        "published": "2022-12-05T11:09:05Z",
        "summary": "We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.",
        "pdf_link": "https://arxiv.org/pdf/2212.02168v1.pdf"
    },
    {
        "title": "Human-in-the-Loop Hate Speech Classification in a Multilingual Context",
        "authors": [
            "Ana Kotarcic",
            "Dominik Hangartner",
            "Fabrizio Gilardi",
            "Selina Kurer",
            "Karsten Donnay"
        ],
        "published": "2022-12-05T09:05:40Z",
        "summary": "The shift of public debate to the digital sphere has been accompanied by a\nrise in online hate speech. While many promising approaches for hate speech\nclassification have been proposed, studies often focus only on a single\nlanguage, usually English, and do not address three key concerns:\npost-deployment performance, classifier maintenance and infrastructural\nlimitations. In this paper, we introduce a new human-in-the-loop BERT-based\nhate speech classification pipeline and trace its development from initial data\ncollection and annotation all the way to post-deployment. Our classifier,\ntrained using data from our original corpus of over 422k examples, is\nspecifically developed for the inherently multilingual setting of Switzerland\nand outperforms with its F1 score of 80.5 the currently best-performing\nBERT-based multilingual classifier by 5.8 F1 points in German and 3.6 F1 points\nin French. Our systematic evaluations over a 12-month period further highlight\nthe vital importance of continuous, human-in-the-loop classifier maintenance to\nensure robust hate speech classification post-deployment.",
        "pdf_link": "https://arxiv.org/pdf/2212.02108v2.pdf"
    },
    {
        "title": "Fast and accurate factorized neural transducer for text adaption of end-to-end speech recognition models",
        "authors": [
            "Rui Zhao",
            "Jian Xue",
            "Partha Parthasarathy",
            "Veljko Miljanic",
            "Jinyu Li"
        ],
        "published": "2022-12-05T02:52:21Z",
        "summary": "Neural transducer is now the most popular end-to-end model for speech\nrecognition, due to its naturally streaming ability. However, it is challenging\nto adapt it with text-only data. Factorized neural transducer (FNT) model was\nproposed to mitigate this problem. The improved adaptation ability of FNT on\ntext-only adaptation data came at the cost of lowered accuracy compared to the\nstandard neural transducer model. We propose several methods to improve the\nperformance of the FNT model. They are: adding CTC criterion during training,\nadding KL divergence loss during adaptation, using a pre-trained language model\nto seed the vocabulary predictor, and an efficient adaptation approach by\ninterpolating the vocabulary predictor with the n-gram language model. A\ncombination of these approaches results in a relative word-error-rate reduction\nof 9.48\\% from the standard FNT model. Furthermore, n-gram interpolation with\nthe vocabulary predictor improves the adaptation speed hugely with satisfactory\nadaptation performance.",
        "pdf_link": "https://arxiv.org/pdf/2212.01992v2.pdf"
    },
    {
        "title": "Building Metadata Inference Using a Transducer Based Language Model",
        "authors": [
            "David Waterworth",
            "Subbu Sethuvenkatraman",
            "Quan Z. Sheng"
        ],
        "published": "2022-12-05T00:37:59Z",
        "summary": "Solving the challenges of automatic machine translation of Building\nAutomation System text metadata is a crucial first step in efficiently\ndeploying smart building applications. The vocabulary used to describe building\nmetadata appears small compared to general natural languages, but each term has\nmultiple commonly used abbreviations. Conventional machine learning techniques\nare inefficient since they need to learn many different forms for the same\nword, and large amounts of data must be used to train these models. It is also\ndifficult to apply standard techniques such as tokenisation since this commonly\nresults in multiple output tags being associated with a single input token,\nsomething traditional sequence labelling models do not allow. Finite State\nTransducers can model sequence-to-sequence tasks where the input and output\nsequences are different lengths, and they can be combined with language models\nto ensure a valid output sequence is generated. We perform a preliminary\nanalysis into the use of transducer-based language models to parse and\nnormalise building point metadata.",
        "pdf_link": "https://arxiv.org/pdf/2212.01964v1.pdf"
    },
    {
        "title": "Applying Multilingual Models to Question Answering (QA)",
        "authors": [
            "Ayrton San Joaquin",
            "Filip Skubacz"
        ],
        "published": "2022-12-04T21:58:33Z",
        "summary": "We study the performance of monolingual and multilingual language models on\nthe task of question-answering (QA) on three diverse languages: English,\nFinnish and Japanese. We develop models for the tasks of (1) determining if a\nquestion is answerable given the context and (2) identifying the answer texts\nwithin the context using IOB tagging. Furthermore, we attempt to evaluate the\neffectiveness of a pre-trained multilingual encoder (Multilingual BERT) on\ncross-language zero-shot learning for both the answerability and IOB sequence\nclassifiers.",
        "pdf_link": "https://arxiv.org/pdf/2212.01933v1.pdf"
    },
    {
        "title": "Understanding How Model Size Affects Few-shot Instruction Prompting",
        "authors": [
            "Ayrton San Joaquin",
            "Ardy Haroen"
        ],
        "published": "2022-12-04T19:59:52Z",
        "summary": "Large Language Models are affected by the phenomena of memorizing and\nforgetting their training data. But how do these vary by model size? We work\ntowards this question by investigating how the model size affects the model's\nability to discriminate a word's meaning in a given context. We introduce a\ndataset called DeltaWords, which evaluates a model's ability to follow\ninstructions to select a sentence which replaces the target word with its\nantonym. We show a weak inverse scaling trend, where task accuracy degrades as\nmodel size increase, under extremely few-shot prompting regimes. We show that\nincreasing the number of examples tend to disproportionately benefit larger\nmodels than smaller models.",
        "pdf_link": "https://arxiv.org/pdf/2212.01907v1.pdf"
    },
    {
        "title": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Yibing Zhan",
            "Yu Qiao",
            "Yonggang Wen",
            "Li Shen",
            "Juhua Liu",
            "Baosheng Yu",
            "Bo Du",
            "Yixin Chen",
            "Xinbo Gao",
            "Chunyan Miao",
            "Xiaoou Tang",
            "Dacheng Tao"
        ],
        "published": "2022-12-04T15:36:18Z",
        "summary": "This technical report briefly describes our JDExplore d-team's Vega v2\nsubmission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the\nwidely used general language understanding evaluation (GLUE) benchmark,\ncontaining eight difficult language understanding tasks, including question\nanswering, natural language inference, word sense disambiguation, coreference\nresolution, and reasoning. [Method] Instead of arbitrarily increasing the size\nof a pretrained language model (PLM), our aim is to 1) fully extract knowledge\nfrom the input pretraining data given a certain parameter budget, e.g., 6B, and\n2) effectively transfer this knowledge to downstream tasks. To achieve goal 1),\nwe propose self-evolution learning for PLMs to wisely predict the informative\ntokens that should be masked, and supervise the masked language modeling (MLM)\nprocess with rectified smooth labels. For goal 2), we leverage the prompt\ntransfer technique to improve the low-resource tasks by transferring the\nknowledge from the foundation model and related downstream tasks to the target\ntask. [Results] According to our submission record (Oct. 2022), with our\noptimized pretraining and fine-tuning strategies, our 6B Vega method achieved\nnew state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE\nleaderboard on Oct. 8, 2022, with an average score of 91.3.",
        "pdf_link": "https://arxiv.org/pdf/2212.01853v1.pdf"
    },
    {
        "title": "Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E",
        "authors": [
            "James Brusseau"
        ],
        "published": "2022-12-04T14:54:13Z",
        "summary": "One objection to conventional AI ethics is that it slows innovation. This\npresentation responds by reconfiguring ethics as an innovation accelerator. The\ncritical elements develop from a contrast between Stability AI's Diffusion and\nOpenAI's Dall-E. By analyzing the divergent values underlying their opposed\nstrategies for development and deployment, five conceptions are identified as\ncommon to acceleration ethics. Uncertainty is understood as positive and\nencouraging, rather than discouraging. Innovation is conceived as intrinsically\nvaluable, instead of worthwhile only as mediated by social effects. AI problems\nare solved by more AI, not less. Permissions and restrictions governing AI\nemerge from a decentralized process, instead of a unified authority. The work\nof ethics is embedded in AI development and application, instead of functioning\nfrom outside. Together, these attitudes and practices remake ethics as\nprovoking rather than restraining artificial intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2212.01834v2.pdf"
    },
    {
        "title": "MiLMo:Minority Multilingual Pre-trained Language Model",
        "authors": [
            "Junjie Deng",
            "Hanru Shi",
            "Xinhe Yu",
            "Wugedele Bao",
            "Yuan Sun",
            "Xiaobing Zhao"
        ],
        "published": "2022-12-04T09:28:17Z",
        "summary": "Pre-trained language models are trained on large-scale unsupervised data, and\nthey can fine-turn the model only on small-scale labeled datasets, and achieve\ngood results. Multilingual pre-trained language models can be trained on\nmultiple languages, and the model can understand multiple languages at the same\ntime. At present, the search on pre-trained models mainly focuses on rich\nresources, while there is relatively little research on low-resource languages\nsuch as minority languages, and the public multilingual pre-trained language\nmodel can not work well for minority languages. Therefore, this paper\nconstructs a multilingual pre-trained model named MiLMo that performs better on\nminority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and\nKorean. To solve the problem of scarcity of datasets on minority languages and\nverify the effectiveness of the MiLMo model, this paper constructs a minority\nmultilingual text classification dataset named MiTC, and trains a word2vec\nmodel for each language. By comparing the word2vec model and the pre-trained\nmodel in the text classification task, this paper provides an optimal scheme\nfor the downstream task research of minority languages. The final experimental\nresults show that the performance of the pre-trained model is better than that\nof the word2vec model, and it has achieved the best results in minority\nmultilingual text classification. The multilingual pre-trained model MiLMo,\nmultilingual word2vec model and multilingual text classification dataset MiTC\nare published on http://milmo.cmli-nlp.com/.",
        "pdf_link": "https://arxiv.org/pdf/2212.01779v2.pdf"
    },
    {
        "title": "KPT: Keyword-guided Pre-training for Grounded Dialog Generation",
        "authors": [
            "Qi Zhu",
            "Fei Mi",
            "Zheng Zhang",
            "Yasheng Wang",
            "Yitong Li",
            "Xin Jiang",
            "Qun Liu",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022-12-04T04:05:01Z",
        "summary": "Incorporating external knowledge into the response generation process is\nessential to building more helpful and reliable dialog agents. However,\ncollecting knowledge-grounded conversations is often costly, calling for a\nbetter pre-trained model for grounded dialog generation that generalizes well\nw.r.t. different types of knowledge. In this work, we propose KPT\n(Keyword-guided Pre-Training), a novel self-supervised pre-training method for\ngrounded dialog generation without relying on extra knowledge annotation.\nSpecifically, we use a pre-trained language model to extract the most uncertain\ntokens in the dialog as keywords. With these keywords, we construct two kinds\nof knowledge and pre-train a knowledge-grounded response generation model,\naiming at handling two different scenarios: (1) the knowledge should be\nfaithfully grounded; (2) it can be selectively used. For the former, the\ngrounding knowledge consists of keywords extracted from the response. For the\nlatter, the grounding knowledge is additionally augmented with keywords\nextracted from other utterances in the same dialog. Since the knowledge is\nextracted from the dialog itself, KPT can be easily performed on a large volume\nand variety of dialogue data. We considered three data sources (open-domain,\ntask-oriented, conversational QA) with a total of 2.5M dialogues. We conduct\nextensive experiments on various few-shot knowledge-grounded generation tasks,\nincluding grounding on dialog acts, knowledge graphs, persona descriptions, and\nWikipedia passages. Our comprehensive experiments and analyses demonstrate that\nKPT consistently outperforms state-of-the-art methods on these tasks with\ndiverse grounding knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2212.01739v1.pdf"
    },
    {
        "title": "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models",
        "authors": [
            "Minghua Liu",
            "Yinhao Zhu",
            "Hong Cai",
            "Shizhong Han",
            "Zhan Ling",
            "Fatih Porikli",
            "Hao Su"
        ],
        "published": "2022-12-03T06:59:01Z",
        "summary": "Generalizable 3D part segmentation is important but challenging in vision and\nrobotics. Training deep models via conventional supervised methods requires\nlarge-scale 3D datasets with fine-grained part annotations, which are costly to\ncollect. This paper explores an alternative way for low-shot part segmentation\nof 3D point clouds by leveraging a pretrained image-language model, GLIP, which\nachieves superior performance on open-vocabulary 2D detection. We transfer the\nrich knowledge from 2D to 3D through GLIP-based part detection on point cloud\nrendering and a novel 2D-to-3D label lifting algorithm. We also utilize\nmulti-view 3D priors and few-shot prompt tuning to boost performance\nsignificantly. Extensive evaluation on PartNet and PartNet-Mobility datasets\nshows that our method enables excellent zero-shot 3D part segmentation. Our\nfew-shot version not only outperforms existing few-shot approaches by a large\nmargin but also achieves highly competitive results compared to the fully\nsupervised counterpart. Furthermore, we demonstrate that our method can be\ndirectly applied to iPhone-scanned point clouds without significant domain\ngaps.",
        "pdf_link": "https://arxiv.org/pdf/2212.01558v2.pdf"
    },
    {
        "title": "Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping",
        "authors": [
            "Jiyan He",
            "Xuechen Li",
            "Da Yu",
            "Huishuai Zhang",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Arturs Backurs",
            "Nenghai Yu",
            "Jiang Bian"
        ],
        "published": "2022-12-03T05:20:15Z",
        "summary": "Differentially private deep learning has recently witnessed advances in\ncomputational efficiency and privacy-utility trade-off. We explore whether\nfurther improvements along the two axes are possible and provide affirmative\nanswers leveraging two instantiations of \\emph{group-wise clipping}. To reduce\nthe compute time overhead of private learning, we show that \\emph{per-layer\nclipping}, where the gradient of each neural network layer is clipped\nseparately, allows clipping to be performed in conjunction with backpropagation\nin differentially private optimization. This results in private learning that\nis as memory-efficient and almost as fast per training update as non-private\nlearning for many workflows of interest. While per-layer clipping with constant\nthresholds tends to underperform standard flat clipping, per-layer clipping\nwith adaptive thresholds matches or outperforms flat clipping under given\ntraining epoch constraints, hence attaining similar or better task performance\nwithin less wall time. To explore the limits of scaling (pretrained) models in\ndifferentially private deep learning, we privately fine-tune the 175\nbillion-parameter GPT-3. We bypass scaling challenges associated with clipping\ngradients that are distributed across multiple devices with \\emph{per-device\nclipping} that clips the gradient of each model piece separately on its host\ndevice. Privately fine-tuning GPT-3 with per-device clipping achieves a task\nperformance at $\\epsilon=1$ better than what is attainable by non-privately\nfine-tuning the largest GPT-2 on a summarization task.",
        "pdf_link": "https://arxiv.org/pdf/2212.01539v1.pdf"
    },
    {
        "title": "iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models",
        "authors": [
            "Jiahao Li",
            "Zhourun Wu",
            "Wenhao Lin",
            "Jiawei Luo",
            "Jun Zhang",
            "Qingcai Chen",
            "Junjie Chen"
        ],
        "published": "2022-12-03T00:50:51Z",
        "summary": "Motivation: Enhancers are important cis-regulatory elements that regulate a\nwide range of biological functions and enhance the transcription of target\ngenes. Although many feature extraction methods have been proposed to improve\nthe performance of enhancer identification, they cannot learn position-related\nmultiscale contextual information from raw DNA sequences.\n  Results: In this article, we propose a novel enhancer identification method\n(iEnhancer-ELM) based on BERT-like enhancer language models. iEnhancer-ELM\ntokenizes DNA sequences with multi-scale k-mers and extracts contextual\ninformation of different scale k-mers related with their positions via an\nmulti-head attention mechanism. We first evaluate the performance of different\nscale k-mers, then ensemble them to improve the performance of enhancer\nidentification. The experimental results on two popular benchmark datasets show\nthat our model outperforms stateof-the-art methods. We further illustrate the\ninterpretability of iEnhancer-ELM. For a case study, we discover 30 enhancer\nmotifs via a 3-mer-based model, where 12 of motifs are verified by STREME and\nJASPAR, demonstrating our model has a potential ability to unveil the\nbiological mechanism of enhancer.\n  Availability and implementation: The models and associated code are available\nat https://github.com/chen-bioinfo/iEnhancer-ELM\n  Contact: junjiechen@hit.edu.cn\n  Supplementary information: Supplementary data are available at Bioinformatics\nAdvances online.",
        "pdf_link": "https://arxiv.org/pdf/2212.01495v2.pdf"
    },
    {
        "title": "Event knowledge in large language models: the gap between the impossible and the unlikely",
        "authors": [
            "Carina Kauf",
            "Anna A. Ivanova",
            "Giulia Rambelli",
            "Emmanuele Chersoni",
            "Jingyuan Selena She",
            "Zawad Chowdhury",
            "Evelina Fedorenko",
            "Alessandro Lenci"
        ],
        "published": "2022-12-02T23:43:18Z",
        "summary": "Word co-occurrence patterns in language corpora contain a surprising amount\nof conceptual knowledge. Large language models (LLMs), trained to predict words\nin context, leverage these patterns to achieve impressive performance on\ndiverse semantic tasks requiring world knowledge. An important but understudied\nquestion about LLMs' semantic abilities is whether they acquire generalized\nknowledge of common events. Here, we test whether five pre-trained LLMs (from\n2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions\nof agent-patient interactions than to minimally different implausible versions\nof the same event. Using three curated sets of minimal sentence pairs (total\nn=1,215), we found that pre-trained LLMs possess substantial event knowledge,\noutperforming other distributional language models. In particular, they almost\nalways assign higher likelihood to possible vs. impossible events (The teacher\nbought the laptop vs. The laptop bought the teacher). However, LLMs show less\nconsistent preferences for likely vs. unlikely events (The nanny tutored the\nboy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM\nscores are driven by both plausibility and surface-level sentence features,\n(ii) LLM scores generalize well across syntactic variants (active vs. passive\nconstructions) but less well across semantic variants (synonymous sentences),\n(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence\nplausibility serves as an organizing dimension in internal LLM representations.\nOverall, our results show that important aspects of event knowledge naturally\nemerge from distributional linguistic patterns, but also highlight a gap\nbetween representations of possible/impossible and likely/unlikely events.",
        "pdf_link": "https://arxiv.org/pdf/2212.01488v4.pdf"
    },
    {
        "title": "Twitter Data Analysis: Izmir Earthquake Case",
        "authors": [
            "\u00d6zg\u00fcr Agrali",
            "Hakan S\u00f6k\u00fcn",
            "Enis Karaarslan"
        ],
        "published": "2022-12-02T21:30:34Z",
        "summary": "T\\\"urkiye is located on a fault line; earthquakes often occur on a large and\nsmall scale. There is a need for effective solutions for gathering current\ninformation during disasters. We can use social media to get insight into\npublic opinion. This insight can be used in public relations and disaster\nmanagement. In this study, Twitter posts on Izmir Earthquake that took place on\nOctober 2020 are analyzed. We question if this analysis can be used to make\nsocial inferences on time. Data mining and natural language processing (NLP)\nmethods are used for this analysis. NLP is used for sentiment analysis and\ntopic modelling. The latent Dirichlet Allocation (LDA) algorithm is used for\ntopic modelling. We used the Bidirectional Encoder Representations from\nTransformers (BERT) model working with Transformers architecture for sentiment\nanalysis. It is shown that the users shared their goodwill wishes and aimed to\ncontribute to the initiated aid activities after the earthquake. The users\ndesired to make their voices heard by competent institutions and organizations.\nThe proposed methods work effectively. Future studies are also discussed.",
        "pdf_link": "https://arxiv.org/pdf/2212.01453v1.pdf"
    },
    {
        "title": "Compound Tokens: Channel Fusion for Vision-Language Representation Learning",
        "authors": [
            "Maxwell Mbabilla Aladago",
            "AJ Piergiovanni"
        ],
        "published": "2022-12-02T21:09:52Z",
        "summary": "We present an effective method for fusing visual-and-language representations\nfor several question answering tasks including visual question answering and\nvisual entailment. In contrast to prior works that concatenate unimodal\nrepresentations or use only cross-attention, we compose multimodal\nrepresentations via channel fusion. By fusing on the channels, the model is\nable to more effectively align the tokens compared to standard methods. These\nmultimodal representations, which we call compound tokens are generated with\ncross-attention transformer layers. First, vision tokens are used as queries to\nretrieve compatible text tokens through cross-attention. We then chain the\nvision tokens and the queried text tokens along the channel dimension. We call\nthe resulting representations compound tokens. A second group of compound\ntokens are generated using an analogous process where the text tokens serve as\nqueries to the cross-attention layer. We concatenate all the compound tokens\nfor further processing with multimodal encoder. We demonstrate the\neffectiveness of compound tokens using an encoder-decoder vision-language model\ntrained end-to-end in the open-vocabulary setting. Compound Tokens achieve\nhighly competitive performance across a range of question answering tasks\nincluding GQA, VQA2.0, and SNLI-VE.",
        "pdf_link": "https://arxiv.org/pdf/2212.01447v1.pdf"
    },
    {
        "title": "An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws",
        "authors": [
            "Hong Jun Jeon",
            "Benjamin Van Roy"
        ],
        "published": "2022-12-02T18:46:41Z",
        "summary": "We study the compute-optimal trade-off between model and training data set\nsizes for large neural networks. Our result suggests a linear relation similar\nto that supported by the empirical analysis of chinchilla. While that work\nstudies transformer-based large language models trained on the MassiveText\ncorpus gopher, as a starting point for development of a mathematical theory, we\nfocus on a simpler learning model and data generating process, each based on a\nneural network with a sigmoidal output unit and single hidden layer of ReLU\nactivation units. We introduce general error upper bounds for a class of\nalgorithms which incrementally update a statistic (for example gradient\ndescent). For a particular learning model inspired by barron 1993, we establish\nan upper bound on the minimal information-theoretically achievable expected\nerror as a function of model and data set sizes. We then derive allocations of\ncomputation that minimize this bound. We present empirical results which\nsuggest that this approximation correctly identifies an asymptotic linear\ncompute-optimal scaling. This approximation also generates new insights. Among\nother things, it suggests that, as the input dimension or latent space\ncomplexity grows, as might be the case for example if a longer history of\ntokens is taken as input to a language model, a larger fraction of the compute\nbudget should be allocated to growing the learning model rather than training\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2212.01365v2.pdf"
    },
    {
        "title": "Nonparametric Masked Language Modeling",
        "authors": [
            "Sewon Min",
            "Weijia Shi",
            "Mike Lewis",
            "Xilun Chen",
            "Wen-tau Yih",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022-12-02T18:10:42Z",
        "summary": "Existing language models (LMs) predict tokens with a softmax over a finite\nvocabulary, which can make it difficult to predict rare tokens or phrases. We\nintroduce NPM, the first nonparametric masked language model that replaces this\nsoftmax with a nonparametric distribution over every phrase in a reference\ncorpus. NPM fills in the [MASK] solely from retrieving a token from a text\ncorpus. We show that NPM can be efficiently trained with a contrastive\nobjective and an in-batch approximation to full corpus retrieval. Zero-shot\nevaluation on 16 tasks including classification, fact probing and question\nanswering demonstrates that NPM outperforms significantly larger parametric\nmodels, either with or without a retrieve-and-generate approach. It is\nparticularly better at dealing with rare patterns (word senses or facts) and\npredicting rare or nearly unseen words (e.g., non-Latin script). We release the\nmodel and code at github.com/facebookresearch/NPM.",
        "pdf_link": "https://arxiv.org/pdf/2212.01349v2.pdf"
    },
    {
        "title": "CT-DQN: Control-Tutored Deep Reinforcement Learning",
        "authors": [
            "Francesco De Lellis",
            "Marco Coraggio",
            "Giovanni Russo",
            "Mirco Musolesi",
            "Mario di Bernardo"
        ],
        "published": "2022-12-02T17:59:43Z",
        "summary": "One of the major challenges in Deep Reinforcement Learning for control is the\nneed for extensive training to learn the policy. Motivated by this, we present\nthe design of the Control-Tutored Deep Q-Networks (CT-DQN) algorithm, a Deep\nReinforcement Learning algorithm that leverages a control tutor, i.e., an\nexogenous control law, to reduce learning time. The tutor can be designed using\nan approximate model of the system, without any assumption about the knowledge\nof the system's dynamics. There is no expectation that it will be able to\nachieve the control objective if used stand-alone. During learning, the tutor\noccasionally suggests an action, thus partially guiding exploration. We\nvalidate our approach on three scenarios from OpenAI Gym: the inverted\npendulum, lunar lander, and car racing. We demonstrate that CT-DQN is able to\nachieve better or equivalent data efficiency with respect to the classic\nfunction approximation solutions.",
        "pdf_link": "https://arxiv.org/pdf/2212.01343v1.pdf"
    },
    {
        "title": "Legal Prompting: Teaching a Language Model to Think Like a Lawyer",
        "authors": [
            "Fangyi Yu",
            "Lee Quartey",
            "Frank Schilder"
        ],
        "published": "2022-12-02T17:41:22Z",
        "summary": "Large language models that are capable of zero or few-shot prompting\napproaches have given rise to the new research area of prompt engineering.\nRecent advances showed that for example Chain-of-Thought (CoT) prompts can\nimprove arithmetic or common sense tasks significantly. We explore how such\napproaches fare with legal reasoning tasks and take the COLIEE entailment task\nbased on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning\napproaches. Our findings show that while CoT prompting and fine-tuning with\nexplanations approaches show improvements, the best results are produced by\nprompts that are derived from specific legal reasoning techniques such as IRAC\n(Issue, Rule, Application, Conclusion). Based on our experiments we improve the\n2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best\nsystem of 0.6789 accuracy with an accuracy of 0.7431.",
        "pdf_link": "https://arxiv.org/pdf/2212.01326v2.pdf"
    },
    {
        "title": "SumREN: Summarizing Reported Speech about Events in News",
        "authors": [
            "Revanth Gangi Reddy",
            "Heba Elfardy",
            "Hou Pong Chan",
            "Kevin Small",
            "Heng Ji"
        ],
        "published": "2022-12-02T12:51:39Z",
        "summary": "A primary objective of news articles is to establish the factual record for\nan event, frequently achieved by conveying both the details of the specified\nevent (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and\nhow people reacted to it (i.e., reported statements). However, existing work on\nnews summarization almost exclusively focuses on the event details. In this\nwork, we propose the novel task of summarizing the reactions of different\nspeakers, as expressed by their reported statements, to a given event. To this\nend, we create a new multi-document summarization benchmark, SUMREN, comprising\n745 summaries of reported statements from various public figures obtained from\n633 news articles discussing 132 events. We propose an automatic silver\ntraining data generation approach for our task, which helps smaller models like\nBART achieve GPT-3 level performance on this task. Finally, we introduce a\npipeline-based framework for summarizing reported speech, which we empirically\nshow to generate summaries that are more abstractive and factual than baseline\nquery-focused summarization approaches.",
        "pdf_link": "https://arxiv.org/pdf/2212.01146v2.pdf"
    },
    {
        "title": "SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition",
        "authors": [
            "Yichong Leng",
            "Xu Tan",
            "Wenjie Liu",
            "Kaitao Song",
            "Rui Wang",
            "Xiang-Yang Li",
            "Tao Qin",
            "Edward Lin",
            "Tie-Yan Liu"
        ],
        "published": "2022-12-02T09:11:32Z",
        "summary": "Error correction in automatic speech recognition (ASR) aims to correct those\nincorrect words in sentences generated by ASR models. Since recent ASR models\nusually have low word error rate (WER), to avoid affecting originally correct\ntokens, error correction models should only modify incorrect words, and\ntherefore detecting incorrect words is important for error correction. Previous\nworks on error correction either implicitly detect error words through\ntarget-source attention or CTC (connectionist temporal classification) loss, or\nexplicitly locate specific deletion/substitution/insertion errors. However,\nimplicit error detection does not provide clear signal about which tokens are\nincorrect and explicit error detection suffers from low detection accuracy. In\nthis paper, we propose SoftCorrect with a soft error detection mechanism to\navoid the limitations of both explicit and implicit error detection.\nSpecifically, we first detect whether a token is correct or not through a\nprobability produced by a dedicatedly designed language model, and then design\na constrained CTC loss that only duplicates the detected incorrect tokens to\nlet the decoder focus on the correction of error tokens. Compared with implicit\nerror detection with CTC loss, SoftCorrect provides explicit signal about which\nwords are incorrect and thus does not need to duplicate every token but only\nincorrect tokens; compared with explicit error detection, SoftCorrect does not\ndetect specific deletion/substitution/insertion errors but just leaves it to\nCTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that\nSoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming\nprevious works by a large margin, while still enjoying fast speed of parallel\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2212.01039v2.pdf"
    },
    {
        "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2022-12-02T04:08:09Z",
        "summary": "Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the\nanswer entities that are multiple hops away from the topic entities mentioned\nin a natural language question on a large-scale Knowledge Graph (KG). To cope\nwith the vast search space, existing work usually adopts a two-stage approach:\nit first retrieves a relatively small subgraph related to the question and then\nperforms the reasoning on the subgraph to find the answer entities accurately.\nAlthough these two stages are highly related, previous work employs very\ndifferent technical solutions for developing the retrieval and reasoning\nmodels, neglecting their relatedness in task essence. In this paper, we propose\nUniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and\nreasoning in both model architecture and parameter learning. For model\narchitecture, UniKGQA consists of a semantic matching module based on a\npre-trained language model~(PLM) for question-relation semantic matching, and a\nmatching information propagation module to propagate the matching information\nalong the directed edges on KGs. For parameter learning, we design a shared\npre-training task based on question-relation matching for both retrieval and\nreasoning models, and then propose retrieval- and reasoning-oriented\nfine-tuning strategies. Compared with previous studies, our approach is more\nunified, tightly relating the retrieval and reasoning stages. Extensive\nexperiments on three benchmark datasets have demonstrated the effectiveness of\nour method on the multi-hop KGQA task. Our codes and data are publicly\navailable at~\\url{https://github.com/RUCAIBox/UniKGQA}.",
        "pdf_link": "https://arxiv.org/pdf/2212.00959v2.pdf"
    },
    {
        "title": "a survey on GPT-3",
        "authors": [
            "Mingyu Zong",
            "Bhaskar Krishnamachari"
        ],
        "published": "2022-12-01T20:24:19Z",
        "summary": "This paper provides an introductory survey to GPT-3. We cover some of the\nhistorical development behind this technology, some of the key features of\nGPT-3, and discuss the machine learning model and the datasets used. We survey\nboth academic and commercial efforts applying GPT-3 in diverse domains such as\ndeveloping conversational AI chatbots, software development, creative work,\ndomain knowledge, and business productivity. We discuss some of the challenges\nthat GPT-3 faces such as the problems of training complexity, bias, and\nhallucination/incorrect answers. We also discuss the future research\nopportunities in this area.",
        "pdf_link": "https://arxiv.org/pdf/2212.00857v1.pdf"
    },
    {
        "title": "Analogical Math Word Problems Solving with Enhanced Problem-Solution Association",
        "authors": [
            "Zhenwen Liang",
            "Jipeng Zhang",
            "Xiangliang Zhang"
        ],
        "published": "2022-12-01T19:50:30Z",
        "summary": "Math word problem (MWP) solving is an important task in question answering\nwhich requires human-like reasoning ability. Analogical reasoning has long been\nused in mathematical education, as it enables students to apply common\nrelational structures of mathematical situations to solve new problems. In this\npaper, we propose to build a novel MWP solver by leveraging analogical MWPs,\nwhich advance the solver's generalization ability across different kinds of\nMWPs. The key idea, named analogy identification, is to associate the\nanalogical MWP pairs in a latent space, i.e., encoding an MWP close to another\nanalogical MWP, while moving away from the non-analogical ones. Moreover, a\nsolution discriminator is integrated into the MWP solver to enhance the\nassociation between the representations of MWPs and their true solutions. The\nevaluation results verify that our proposed analogical learning strategy\npromotes the performance of MWP-BERT on Math23k over the state-of-the-art model\nGenerate2Rank, with 5 times fewer parameters in the encoder. We also find that\nour model has a stronger generalization ability in solving difficult MWPs due\nto the analogical learning from easy MWPs.",
        "pdf_link": "https://arxiv.org/pdf/2212.00837v1.pdf"
    },
    {
        "title": "CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text",
        "authors": [
            "B. Jalalzadeh Fard",
            "S. A. Hasan",
            "J. E. Bell"
        ],
        "published": "2022-12-01T17:44:09Z",
        "summary": "Climate change is threatening human health in unprecedented orders and many\nways. These threats are expected to grow unless effective and evidence-based\npolicies are developed and acted upon to minimize or eliminate them. Attaining\nsuch a task requires the highest degree of the flow of knowledge from science\ninto policy. The multidisciplinary, location-specific, and vastness of\npublished science makes it challenging to keep track of novel work in this\narea, as well as making the traditional knowledge synthesis methods inefficient\nin infusing science into policy. To this end, we consider developing multiple\ndomain-specific language models (LMs) with different variations from Climate-\nand Health-related information, which can serve as a foundational step toward\ncapturing available knowledge to enable solving different tasks, such as\ndetecting similarities between climate- and health-related concepts,\nfact-checking, relation extraction, evidence of health effects to policy text\ngeneration, and more. To our knowledge, this is the first work that proposes\ndeveloping multiple domain-specific language models for the considered domains.\nWe will make the developed models, resources, and codebase available for the\nresearchers.",
        "pdf_link": "https://arxiv.org/pdf/2212.00689v1.pdf"
    },
    {
        "title": "Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis",
        "authors": [
            "Odysseas S. Chlapanis",
            "Georgios Paraskevopoulos",
            "Alexandros Potamianos"
        ],
        "published": "2022-12-01T17:31:42Z",
        "summary": "Multimodal learning pipelines have benefited from the success of pretrained\nlanguage models. However, this comes at the cost of increased model parameters.\nIn this work, we propose Adapted Multimodal BERT (AMB), a BERT-based\narchitecture for multimodal tasks that uses a combination of adapter modules\nand intermediate fusion layers. The adapter adjusts the pretrained language\nmodel for the task at hand, while the fusion layers perform task-specific,\nlayer-wise fusion of audio-visual information with textual BERT\nrepresentations. During the adaptation process the pre-trained language model\nparameters remain frozen, allowing for fast, parameter-efficient training. In\nour ablations we see that this approach leads to efficient models, that can\noutperform their fine-tuned counterparts and are robust to input noise. Our\nexperiments on sentiment analysis with CMU-MOSEI show that AMB outperforms the\ncurrent state-of-the-art across metrics, with 3.4% relative reduction in the\nresulting error and 2.1% relative improvement in 7-class classification\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2212.00678v1.pdf"
    },
    {
        "title": "Extensible Prompts for Language Models on Zero-shot Language Style Customization",
        "authors": [
            "Tao Ge",
            "Jing Hu",
            "Li Dong",
            "Shaoguang Mao",
            "Yan Xia",
            "Xun Wang",
            "Si-Qing Chen",
            "Furu Wei"
        ],
        "published": "2022-12-01T16:11:56Z",
        "summary": "We propose eXtensible Prompt (X-Prompt) for prompting a large language model\n(LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL\nbut also an extensible vocabulary of imaginary words. Registering new imaginary\nwords allows us to instruct the LLM to comprehend concepts that are difficult\nto describe with NL words, thereby making a prompt more descriptive. Also,\nthese imaginary words are designed to be out-of-distribution (OOD) robust so\nthat they can be (re)used like NL words in various prompts, distinguishing\nX-Prompt from soft prompt that is for fitting in-distribution data. We propose\ncontext-augmented learning (CAL) to learn imaginary words for general\nusability, enabling them to work properly in OOD (unseen) prompts. We\nexperiment X-Prompt for zero-shot language style customization as a case study.\nThe promising results of X-Prompt demonstrate its potential to facilitate\nadvanced interaction beyond the natural language interface, bridging the\ncommunication gap between humans and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2212.00616v2.pdf"
    },
    {
        "title": "Language models and brain alignment: beyond word-level semantics and prediction",
        "authors": [
            "Gabriele Merlin",
            "Mariya Toneva"
        ],
        "published": "2022-12-01T15:48:51Z",
        "summary": "Pretrained language models that have been trained to predict the next word\nover billions of text documents have been shown to also significantly predict\nbrain recordings of people comprehending language. Understanding the reasons\nbehind the observed similarities between language in machines and language in\nthe brain can lead to more insight into both systems. Recent works suggest that\nthe prediction of the next word is a key mechanism that contributes to the\nalignment between the two. What is not yet understood is whether prediction of\nthe next word is necessary for this observed alignment or simply sufficient,\nand whether there are other shared mechanisms or information that is similarly\nimportant. In this work, we take a first step towards a better understanding\nvia two simple perturbations in a popular pretrained language model. The first\nperturbation is to improve the model's ability to predict the next word in the\nspecific naturalistic stimulus text that the brain recordings correspond to. We\nshow that this indeed improves the alignment with the brain recordings.\nHowever, this improved alignment may also be due to any improved word-level or\nmulti-word level semantics for the specific world that is described by the\nstimulus narrative. We aim to disentangle the contribution of next word\nprediction and semantic knowledge via our second perturbation: scrambling the\nword order at inference time, which reduces the ability to predict the next\nword, but maintains any newly learned word-level semantics. By comparing the\nalignment with brain recordings of these differently perturbed models, we show\nthat improvements in alignment with brain recordings are due to more than\nimprovements in next word prediction and word-level semantics.",
        "pdf_link": "https://arxiv.org/pdf/2212.00596v1.pdf"
    },
    {
        "title": "Language Model Pre-training on True Negatives",
        "authors": [
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Masao Utiyama",
            "Eiichiro Sumita"
        ],
        "published": "2022-12-01T12:24:19Z",
        "summary": "Discriminative pre-trained language models (PLMs) learn to predict original\ntexts from intentionally corrupted ones. Taking the former text as positive and\nthe latter as negative samples, the PLM can be trained effectively for\ncontextualized representation. However, the training of such a type of PLMs\nhighly relies on the quality of the automatically constructed samples. Existing\nPLMs simply treat all corrupted texts as equal negative without any\nexamination, which actually lets the resulting model inevitably suffer from the\nfalse negative issue where training is carried out on pseudo-negative data and\nleads to less efficiency and less robustness in the resulting PLMs. In this\nwork, on the basis of defining the false negative issue in discriminative PLMs\nthat has been ignored for a long time, we design enhanced pre-training methods\nto counteract false negative predictions and encourage pre-training language\nmodels on true negatives by correcting the harmful gradient updates subject to\nfalse negative predictions. Experimental results on GLUE and SQuAD benchmarks\nshow that our counter-false-negative pre-training methods indeed bring about\nbetter performance together with stronger robustness.",
        "pdf_link": "https://arxiv.org/pdf/2212.00460v1.pdf"
    },
    {
        "title": "A Commonsense-Infused Language-Agnostic Learning Framework for Enhancing Prediction of Political Polarity in Multilingual News Headlines",
        "authors": [
            "Swati Swati",
            "Adrian Mladeni\u0107 Grobelnik",
            "Dunja Mladeni\u0107",
            "Marko Grobelnik"
        ],
        "published": "2022-12-01T06:07:01Z",
        "summary": "Predicting the political polarity of news headlines is a challenging task\nthat becomes even more challenging in a multilingual setting with low-resource\nlanguages. To deal with this, we propose to utilise the Inferential Commonsense\nKnowledge via a Translate-Retrieve-Translate strategy to introduce a learning\nframework. To begin with, we use the method of translation and retrieval to\nacquire the inferential knowledge in the target language. We then employ an\nattention mechanism to emphasise important inferences. We finally integrate the\nattended inferences into a multilingual pre-trained language model for the task\nof bias prediction. To evaluate the effectiveness of our framework, we present\na dataset of over 62.6K multilingual news headlines in five European languages\nannotated with their respective political polarities. We evaluate several\nstate-of-the-art multilingual pre-trained language models since their\nperformance tends to vary across languages (low/high resource). Evaluation\nresults demonstrate that our proposed framework is effective regardless of the\nmodels employed. Overall, the best performing model trained with only headlines\nshow 0.90 accuracy and F1, and 0.83 jaccard score. With attended knowledge in\nour framework, the same model show an increase in 2.2% accuracy and F1, and\n3.6% jaccard score. Extending our experiments to individual languages reveals\nthat the models we analyze for Slovenian perform significantly worse than other\nlanguages in our dataset. To investigate this, we assess the effect of\ntranslation quality on prediction performance. It indicates that the disparity\nin performance is most likely due to poor translation quality. We release our\ndataset and scripts at: https://github.com/Swati17293/KG-Multi-Bias for future\nresearch. Our framework has the potential to benefit journalists, social\nscientists, news producers, and consumers.",
        "pdf_link": "https://arxiv.org/pdf/2212.00298v1.pdf"
    },
    {
        "title": "Distilling Reasoning Capabilities into Smaller Language Models",
        "authors": [
            "Kumar Shridhar",
            "Alessandro Stolfo",
            "Mrinmaya Sachan"
        ],
        "published": "2022-12-01T00:39:56Z",
        "summary": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM",
        "pdf_link": "https://arxiv.org/pdf/2212.00193v2.pdf"
    },
    {
        "title": "Task-Specific Embeddings for Ante-Hoc Explainable Text Classification",
        "authors": [
            "Kishaloy Halder",
            "Josip Krapac",
            "Alan Akbik",
            "Anthony Brew",
            "Matti Lyra"
        ],
        "published": "2022-11-30T19:56:25Z",
        "summary": "Current state-of-the-art approaches to text classification typically leverage\nBERT-style Transformer models with a softmax classifier, jointly fine-tuned to\npredict class labels of a target task. In this paper, we instead propose an\nalternative training objective in which we learn task-specific embeddings of\ntext: our proposed objective learns embeddings such that all texts that share\nthe same target class label should be close together in the embedding space,\nwhile all others should be far apart. This allows us to replace the softmax\nclassifier with a more interpretable k-nearest-neighbor classification\napproach. In a series of experiments, we show that this yields a number of\ninteresting benefits: (1) The resulting order induced by distances in the\nembedding space can be used to directly explain classification decisions. (2)\nThis facilitates qualitative inspection of the training data, helping us to\nbetter understand the problem space and identify labelling quality issues. (3)\nThe learned distances to some degree generalize to unseen classes, allowing us\nto incrementally add new classes without retraining the model. We present\nextensive experiments which show that the benefits of ante-hoc explainability\nand incremental learning come at no cost in overall classification accuracy,\nthus pointing to practical applicability of our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2212.00086v1.pdf"
    },
    {
        "title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT",
        "authors": [
            "Rui Pan",
            "Shizhe Diao",
            "Jianlin Chen",
            "Tong Zhang"
        ],
        "published": "2022-11-30T17:50:35Z",
        "summary": "In this paper, we present ExtremeBERT, a toolkit for accelerating and\ncustomizing BERT pretraining. Our goal is to provide an easy-to-use BERT\npretraining toolkit for the research community and industry. Thus, the\npretraining of popular language models on customized datasets is affordable\nwith limited resources. Experiments show that, to achieve the same or better\nGLUE scores, the time cost of our toolkit is over $6\\times$ times less for BERT\nBase and $9\\times$ times less for BERT Large when compared with the original\nBERT paper. The documentation and code are released at\nhttps://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.",
        "pdf_link": "https://arxiv.org/pdf/2211.17201v1.pdf"
    },
    {
        "title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?",
        "authors": [
            "Joel Niklaus",
            "Daniele Giofr\u00e9"
        ],
        "published": "2022-11-30T16:09:20Z",
        "summary": "Pretrained transformer models have achieved state-of-the-art results in many\ntasks and benchmarks recently. Many state-of-the-art Language Models (LMs),\nhowever, do not scale well above the threshold of 512 input tokens. In\nspecialized domains though (such as legal, scientific or biomedical), models\noften need to process very long text (sometimes well above 10000 tokens). Even\nthough many efficient transformers have been proposed (such as Longformer,\nBigBird or FNet), so far, only very few such efficient models are available for\nspecialized domains. Additionally, since the pretraining process is extremely\ncostly in general - but even more so as the sequence length increases - it is\noften only in reach of large research labs. One way of making pretraining\ncheaper is the Replaced Token Detection (RTD) task, by providing more signal\nduring training, since the loss can be computed over all tokens. In this work,\nwe train Longformer models with the efficient RTD task on legal data to\nshowcase that pretraining efficient LMs is possible using much less compute. We\nevaluate the trained models on challenging summarization tasks requiring the\nmodel to summarize long texts to show to what extent the models can achieve\ngood performance on downstream tasks. We find that both the small and base\nmodels outperform their baselines on the in-domain BillSum and out-of-domain\nPubMed tasks in their respective parameter range. We publish our code and\nmodels for research purposes.",
        "pdf_link": "https://arxiv.org/pdf/2211.17135v1.pdf"
    },
    {
        "title": "Rationale-Guided Few-Shot Classification to Detect Abusive Language",
        "authors": [
            "Punyajoy Saha",
            "Divyanshu Sheth",
            "Kushal Kedia",
            "Binny Mathew",
            "Animesh Mukherjee"
        ],
        "published": "2022-11-30T14:47:14Z",
        "summary": "Abusive language is a concerning problem in online social media. Past\nresearch on detecting abusive language covers different platforms, languages,\ndemographies, etc. However, models trained using these datasets do not perform\nwell in cross-domain evaluation settings. To overcome this, a common strategy\nis to use a few samples from the target domain to train models to get better\nperformance in that domain (cross-domain few-shot training). However, this\nmight cause the models to overfit the artefacts of those samples. A compelling\nsolution could be to guide the models toward rationales, i.e., spans of text\nthat justify the text's label. This method has been found to improve model\nperformance in the in-domain setting across various NLP tasks. In this paper,\nwe propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language\ndetection. We first build a multitask learning setup to jointly learn\nrationales, targets, and labels, and find a significant improvement of 6% macro\nF1 on the rationale detection task over training solely rationale classifiers.\nWe introduce two rationale-integrated BERT-based architectures (the RGFS\nmodels) and evaluate our systems over five different abusive language datasets,\nfinding that in the few-shot classification setting, RGFS-based models\noutperform baseline models by about 7% in macro F1 scores and perform\ncompetitively to models finetuned on other source domains. Furthermore,\nRGFS-based models outperform LIME/SHAP-based approaches in terms of\nplausibility and are close in performance in terms of faithfulness.",
        "pdf_link": "https://arxiv.org/pdf/2211.17046v2.pdf"
    },
    {
        "title": "Quadapter: Adapter for GPT-2 Quantization",
        "authors": [
            "Minseop Park",
            "Jaeseong You",
            "Markus Nagel",
            "Simyung Chang"
        ],
        "published": "2022-11-30T11:20:33Z",
        "summary": "Transformer language models such as GPT-2 are difficult to quantize because\nof outliers in activations leading to a large quantization error. To adapt to\nthe error, one must use quantization-aware training, which entails a\nfine-tuning process based on the dataset and the training pipeline identical to\nthose for the original model. Pretrained language models, however, often do not\ngrant access to their datasets and training pipelines, forcing us to rely on\narbitrary ones for fine-tuning. In that case, it is observed that\nquantization-aware training overfits the model to the fine-tuning data. For\nquantization without overfitting, we introduce a quantization adapter\n(Quadapter), a small set of parameters that are learned to make activations\nquantization-friendly by scaling them channel-wise. It keeps the model\nparameters unchanged. By applying our method to the challenging task of\nquantizing GPT-2, we demonstrate that it effectively prevents the overfitting\nand improves the quantization performance.",
        "pdf_link": "https://arxiv.org/pdf/2211.16912v1.pdf"
    },
    {
        "title": "xTrimoABFold: De novo Antibody Structure Prediction without MSA",
        "authors": [
            "Yining Wang",
            "Xumeng Gong",
            "Shaochuan Li",
            "Bing Yang",
            "YiWu Sun",
            "Chuan Shi",
            "Yangang Wang",
            "Cheng Yang",
            "Hui Li",
            "Le Song"
        ],
        "published": "2022-11-30T09:26:08Z",
        "summary": "In the field of antibody engineering, an essential task is to design a novel\nantibody whose paratopes bind to a specific antigen with correct epitopes.\nUnderstanding antibody structure and its paratope can facilitate a mechanistic\nunderstanding of its function. Therefore, antibody structure prediction from\nits sequence alone has always been a highly valuable problem for de novo\nantibody design. AlphaFold2, a breakthrough in the field of structural biology,\nprovides a solution to predict protein structure based on protein sequences and\ncomputationally expensive coevolutionary multiple sequence alignments (MSAs).\nHowever, the computational efficiency and undesirable prediction accuracy of\nantibodies, especially on the complementarity-determining regions (CDRs) of\nantibodies limit their applications in the industrially high-throughput drug\ndesign. To learn an informative representation of antibodies, we employed a\ndeep antibody language model (ALM) on curated sequences from the observed\nantibody space database via a transformer model. We also developed a novel\nmodel named xTrimoABFold to predict antibody structure from antibody sequence\nbased on the pretrained ALM as well as efficient evoformers and structural\nmodules. The model was trained end-to-end on the antibody structures in PDB by\nminimizing the ensemble loss of domain-specific focal loss on CDR and the\nframe-aligned point loss. xTrimoABFold outperforms AlphaFold2 and other protein\nlanguage model based SOTAs, e.g., OmegaFold, HelixFold-Single, and IgFold with\na large significant margin (30+\\% improvement on RMSD) while performing 151\ntimes faster than AlphaFold2. To the best of our knowledge, xTrimoABFold\nachieved state-of-the-art antibody structure prediction. Its improvement in\nboth accuracy and efficiency makes it a valuable tool for de novo antibody\ndesign and could make further improvements in immuno-theory.",
        "pdf_link": "https://arxiv.org/pdf/2212.00735v3.pdf"
    },
    {
        "title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
        "authors": [
            "Xiao Yu",
            "Qingyang Wu",
            "Kun Qian",
            "Zhou Yu"
        ],
        "published": "2022-11-30T06:27:46Z",
        "summary": "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train\na model to directly optimize response for task-related metrics. However, RL\nneeds to perform exploration, which can be time-consuming due to the slow\nauto-regressive sequence generation process. We investigate an approach to\ncreate a more efficient RL-based algorithm to improve TOD performance in an\noffline setting. First, we use a faster generation procedure that samples from\nindependent next-word distributions after training the language model (LM) with\nsupervised learning. We then introduce a fine-grained reward function to help\nthe model focus on learning key information in a dialog, by measuring the\nimportance and semantic closeness of each generated token. Experiments on the\nMultiWoZ dataset show our new training algorithm, Keywords Reinforcement\nLearning with Next-word Sampling (KRLS), achieves state-of-the-art performance\non the end-to-end response generation task, with a 15% training time reduction\ncompared to a standard RL algorithm using auto-regressive generation.",
        "pdf_link": "https://arxiv.org/pdf/2211.16773v5.pdf"
    },
    {
        "title": "HEAT: Hardware-Efficient Automatic Tensor Decomposition for Transformer Compression",
        "authors": [
            "Jiaqi Gu",
            "Ben Keller",
            "Jean Kossaifi",
            "Anima Anandkumar",
            "Brucek Khailany",
            "David Z. Pan"
        ],
        "published": "2022-11-30T05:31:45Z",
        "summary": "Transformers have attained superior performance in natural language\nprocessing and computer vision. Their self-attention and feedforward layers are\noverparameterized, limiting inference speed and energy efficiency. Tensor\ndecomposition is a promising technique to reduce parameter redundancy by\nleveraging tensor algebraic properties to express the parameters in a\nfactorized form. Prior efforts used manual or heuristic factorization settings\nwithout hardware-aware customization, resulting in poor hardware efficiencies\nand large performance degradation.\n  In this work, we propose a hardware-aware tensor decomposition framework,\ndubbed HEAT, that enables efficient exploration of the exponential space of\npossible decompositions and automates the choice of tensorization shape and\ndecomposition rank with hardware-aware co-optimization. We jointly investigate\ntensor contraction path optimizations and a fused Einsum mapping strategy to\nbridge the gap between theoretical benefits and real hardware efficiency\nimprovement. Our two-stage knowledge distillation flow resolves the\ntrainability bottleneck and thus significantly boosts the final accuracy of\nfactorized Transformers. Overall, we experimentally show that our\nhardware-aware factorized BERT variants reduce the energy-delay product by 5.7x\nwith less than 1.1% accuracy loss and achieve a better efficiency-accuracy\nPareto frontier than hand-tuned and heuristic baselines.",
        "pdf_link": "https://arxiv.org/pdf/2211.16749v1.pdf"
    },
    {
        "title": "Protein Language Models and Structure Prediction: Connection and Progression",
        "authors": [
            "Bozhen Hu",
            "Jun Xia",
            "Jiangbin Zheng",
            "Cheng Tan",
            "Yufei Huang",
            "Yongjie Xu",
            "Stan Z. Li"
        ],
        "published": "2022-11-30T04:58:54Z",
        "summary": "The prediction of protein structures from sequences is an important task for\nfunction prediction, drug design, and related biological processes\nunderstanding. Recent advances have proved the power of language models (LMs)\nin processing the protein sequence databases, which inherit the advantages of\nattention networks and capture useful information in learning representations\nfor proteins. The past two years have witnessed remarkable success in tertiary\nprotein structure prediction (PSP), including evolution-based and\nsingle-sequence-based PSP. It seems that instead of using energy-based models\nand sampling procedures, protein language model (pLM)-based pipelines have\nemerged as mainstream paradigms in PSP. Despite the fruitful progress, the PSP\ncommunity needs a systematic and up-to-date survey to help bridge the gap\nbetween LMs in the natural language processing (NLP) and PSP domains and\nintroduce their methodologies, advancements and practical applications. To this\nend, in this paper, we first introduce the similarities between protein and\nhuman languages that allow LMs extended to pLMs, and applied to protein\ndatabases. Then, we systematically review recent advances in LMs and pLMs from\nthe perspectives of network architectures, pre-training strategies,\napplications, and commonly-used protein databases. Next, different types of\nmethods for PSP are discussed, particularly how the pLM-based architectures\nfunction in the process of protein folding. Finally, we identify challenges\nfaced by the PSP community and foresee promising research directions along with\nthe advances of pLMs. This survey aims to be a hands-on guide for researchers\nto understand PSP methods, develop pLMs and tackle challenging problems in this\nfield for practical purposes.",
        "pdf_link": "https://arxiv.org/pdf/2211.16742v1.pdf"
    },
    {
        "title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation",
        "authors": [
            "Zhangir Azerbayev",
            "Ansong Ni",
            "Hailey Schoelkopf",
            "Dragomir Radev"
        ],
        "published": "2022-11-30T04:51:26Z",
        "summary": "Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.",
        "pdf_link": "https://arxiv.org/pdf/2211.16740v3.pdf"
    },
    {
        "title": "Findings of the WMT 2022 Shared Task on Translation Suggestion",
        "authors": [
            "Zhen Yang",
            "Fandong Meng",
            "Yingxue Zhang",
            "Ernan Li",
            "Jie Zhou"
        ],
        "published": "2022-11-30T03:48:36Z",
        "summary": "We report the result of the first edition of the WMT shared task on\nTranslation Suggestion (TS). The task aims to provide alternatives for specific\nwords or phrases given the entire documents generated by machine translation\n(MT). It consists two sub-tasks, namely, the naive translation suggestion and\ntranslation suggestion with hints. The main difference is that some hints are\nprovided in sub-task two, therefore, it is easier for the model to generate\nmore accurate suggestions. For sub-task one, we provide the corpus for the\nlanguage pairs English-German and English-Chinese. And only English-Chinese\ncorpus is provided for the sub-task two.\n  We received 92 submissions from 5 participating teams in sub-task one and 6\nsubmissions for the sub-task 2, most of them covering all of the translation\ndirections. We used the automatic metric BLEU for evaluating the performance of\neach submission.",
        "pdf_link": "https://arxiv.org/pdf/2211.16717v1.pdf"
    },
    {
        "title": "Coder Reviewer Reranking for Code Generation",
        "authors": [
            "Tianyi Zhang",
            "Tao Yu",
            "Tatsunori B. Hashimoto",
            "Mike Lewis",
            "Wen-tau Yih",
            "Daniel Fried",
            "Sida I. Wang"
        ],
        "published": "2022-11-29T18:56:33Z",
        "summary": "Sampling diverse programs from a code language model and reranking with model\nlikelihood is a popular method for code generation but it is prone to\npreferring degenerate solutions. Inspired by collaborative programming, we\npropose Coder-Reviewer reranking. We augment Coder language models from past\nwork, which generate programs given language instructions, with Reviewer\nmodels, which evaluate the likelihood of the instruction given the generated\nprograms. We perform an extensive study across six datasets with eight models\nfrom three model families. Experimental results show that Coder-Reviewer\nreranking leads to consistent and significant improvement (up to 17% absolute\naccuracy gain) over reranking with the Coder model only. When combined with\nexecutability filtering, Coder-Reviewer reranking can often outperform the\nminimum Bayes risk method. Coder-Reviewer reranking is easy to implement by\nprompting, can generalize to different programming languages, and works well\nwith off-the-shelf hyperparameters.",
        "pdf_link": "https://arxiv.org/pdf/2211.16490v1.pdf"
    },
    {
        "title": "Better Transcription of UK Supreme Court Hearings",
        "authors": [
            "Hadeel Saadany",
            "Catherine Breslin",
            "Constantin Or\u0103san",
            "Sophie Walker"
        ],
        "published": "2022-11-29T17:02:00Z",
        "summary": "Transcription of legal proceedings is very important to enable access to\njustice. However, speech transcription is an expensive and slow process. In\nthis paper we describe part of a combined research and industrial project for\nbuilding an automated transcription tool designed specifically for the Justice\nsector in the UK. We explain the challenges involved in transcribing court room\nhearings and the Natural Language Processing (NLP) techniques we employ to\ntackle these challenges. We will show that fine-tuning a generic off-the-shelf\npre-trained Automatic Speech Recognition (ASR) system with an in-domain\nlanguage model as well as infusing common phrases extracted with a collocation\ndetection model can improve not only the Word Error Rate (WER) of the\ntranscribed hearings but avoid critical errors that are specific of the legal\njargon and terminology commonly used in British courts.",
        "pdf_link": "https://arxiv.org/pdf/2211.17094v2.pdf"
    },
    {
        "title": "Outfit Generation and Recommendation -- An Experimental Study",
        "authors": [
            "Marjan Celikik",
            "Matthias Kirmse",
            "Timo Denk",
            "Pierre Gagliardi",
            "Sahar Mbarek",
            "Duy Pham",
            "Ana Peleteiro Ramallo"
        ],
        "published": "2022-11-29T16:36:00Z",
        "summary": "Over the past years, fashion-related challenges have gained a lot of\nattention in the research community. Outfit generation and recommendation,\ni.e., the composition of a set of items of different types (e.g., tops, bottom,\nshoes, accessories) that go well together, are among the most challenging ones.\nThat is because items have to be both compatible amongst each other and also\npersonalized to match the taste of the customer. Recently there has been a\nplethora of work targeted at tackling these problems by adopting various\ntechniques and algorithms from the machine learning literature. However, to\ndate, there is no extensive comparison of the performance of the different\nalgorithms for outfit generation and recommendation. In this paper, we close\nthis gap by providing a broad evaluation and comparison of various algorithms,\nincluding both personalized and non-personalized approaches, using online,\nreal-world user data from one of Europe's largest fashion stores. We present\nthe adaptations we made to some of those models to make them suitable for\npersonalized outfit generation. Moreover, we provide insights for models that\nhave not yet been evaluated on this task, specifically, GPT, BERT and\nSeq-to-Seq LSTM.",
        "pdf_link": "https://arxiv.org/pdf/2211.16353v1.pdf"
    },
    {
        "title": "Improving astroBERT using Semantic Textual Similarity",
        "authors": [
            "Felix Grezes",
            "Thomas Allen",
            "Sergi Blanco-Cuaresma",
            "Alberto Accomazzi",
            "Michael J. Kurtz",
            "Golnaz Shapurian",
            "Edwin Henneken",
            "Carolyn S. Grant",
            "Donna M. Thompson",
            "Timothy W. Hostetler",
            "Matthew R. Templeton",
            "Kelly E. Lockhart",
            "Shinyi Chen",
            "Jennifer Koch",
            "Taylor Jacovich",
            "Pavlos Protopapas"
        ],
        "published": "2022-11-29T16:15:32Z",
        "summary": "The NASA Astrophysics Data System (ADS) is an essential tool for researchers\nthat allows them to explore the astronomy and astrophysics scientific\nliterature, but it has yet to exploit recent advances in natural language\nprocessing. At ADASS 2021, we introduced astroBERT, a machine learning language\nmodel tailored to the text used in astronomy papers in ADS. In this work we:\n  - announce the first public release of the astroBERT language model;\n  - show how astroBERT improves over existing public language models on\nastrophysics specific tasks;\n  - and detail how ADS plans to harness the unique structure of scientific\npapers, the citation graph and citation context, to further improve astroBERT.",
        "pdf_link": "https://arxiv.org/pdf/2212.00744v1.pdf"
    },
    {
        "title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
        "authors": [
            "Jasper Jian",
            "Siva Reddy"
        ],
        "published": "2022-11-29T09:01:37Z",
        "summary": "Syntax is a latent hierarchical structure which underpins the robust and\ncompositional nature of human language. In this work, we explore the hypothesis\nthat syntactic dependencies can be represented in language model attention\ndistributions and propose a new method to induce these structures\ntheory-agnostically. Instead of modeling syntactic relations as defined by\nannotation schemata, we model a more general property implicit in the\ndefinition of dependency relations, syntactic substitutability. This property\ncaptures the fact that words at either end of a dependency can be substituted\nwith words from the same category. Substitutions can be used to generate a set\nof syntactically invariant sentences whose representations are then used for\nparsing. We show that increasing the number of substitutions used improves\nparsing accuracy on natural data. On long-distance subject-verb agreement\nconstructions, our method achieves 79.5% recall compared to 8.9% using a\nprevious method. Our method also provides improvements when transferred to a\ndifferent parsing setup, demonstrating that it generalizes.",
        "pdf_link": "https://arxiv.org/pdf/2211.16031v3.pdf"
    },
    {
        "title": "Diverse Multi-Answer Retrieval with Determinantal Point Processes",
        "authors": [
            "Poojitha Nandigam",
            "Nikhil Rayaprolu",
            "Manish Shrivastava"
        ],
        "published": "2022-11-29T08:54:05Z",
        "summary": "Often questions provided to open-domain question answering systems are\nambiguous. Traditional QA systems that provide a single answer are incapable of\nanswering ambiguous questions since the question may be interpreted in several\nways and may have multiple distinct answers. In this paper, we address\nmulti-answer retrieval which entails retrieving passages that can capture\nmajority of the diverse answers to the question. We propose a re-ranking based\napproach using Determinantal point processes utilizing BERT as kernels. Our\nmethod jointly considers query-passage relevance and passage-passage\ncorrelation to retrieve passages that are both query-relevant and diverse.\nResults demonstrate that our re-ranking technique outperforms state-of-the-art\nmethod on the AmbigQA dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.16029v1.pdf"
    },
    {
        "title": "UDE: A Unified Driving Engine for Human Motion Generation",
        "authors": [
            "Zixiang Zhou",
            "Baoyuan Wang"
        ],
        "published": "2022-11-29T08:30:52Z",
        "summary": "Generating controllable and editable human motion sequences is a key\nchallenge in 3D Avatar generation. It has been labor-intensive to generate and\nanimate human motion for a long time until learning-based approaches have been\ndeveloped and applied recently. However, these approaches are still\ntask-specific or modality-specific\\cite\n{ahuja2019language2pose}\\cite{ghosh2021synthesis}\\cite{ferreira2021learning}\\cite{li2021ai}.\nIn this paper, we propose ``UDE\", the first unified driving engine that enables\ngenerating human motion sequences from natural language or audio sequences (see\nFig.~\\ref{fig:teaser}). Specifically, UDE consists of the following key\ncomponents: 1) a motion quantization module based on VQVAE that represents\ncontinuous motion sequence as discrete latent code\\cite{van2017neural}, 2) a\nmodality-agnostic transformer encoder\\cite{vaswani2017attention} that learns to\nmap modality-aware driving signals to a joint space, and 3) a unified token\ntransformer (GPT-like\\cite{radford2019language}) network to predict the\nquantized latent code index in an auto-regressive manner. 4) a diffusion motion\ndecoder that takes as input the motion tokens and decodes them into motion\nsequences with high diversity. We evaluate our method on\nHumanML3D\\cite{Guo_2022_CVPR} and AIST++\\cite{li2021learn} benchmarks, and the\nexperiment results demonstrate our method achieves state-of-the-art\nperformance. Project website: \\url{https://github.com/zixiangzhou916/UDE/",
        "pdf_link": "https://arxiv.org/pdf/2211.16016v1.pdf"
    },
    {
        "title": "Prompted Opinion Summarization with GPT-3.5",
        "authors": [
            "Adithya Bhaskar",
            "Alexander R. Fabbri",
            "Greg Durrett"
        ],
        "published": "2022-11-29T04:06:21Z",
        "summary": "Large language models have shown impressive performance across a wide variety\nof tasks, including text summarization. In this paper, we show that this strong\nperformance extends to opinion summarization. We explore several pipeline\nmethods for applying GPT-3.5 to summarize a large collection of user reviews in\na prompted fashion. To handle arbitrarily large numbers of user reviews, we\nexplore recursive summarization as well as methods for selecting salient\ncontent to summarize through supervised clustering or extraction. On two\ndatasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and\na generic summarization dataset of Amazon and Yelp reviews (FewSum), we show\nthat GPT-3.5 models achieve very strong performance in human evaluation. We\nargue that standard evaluation metrics do not reflect this, and introduce three\nnew metrics targeting faithfulness, factuality, and genericity to contrast\nthese different methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.15914v2.pdf"
    },
    {
        "title": "Composition based oxidation state prediction of materials using deep learning",
        "authors": [
            "Nihang Fu",
            "Jeffrey Hu",
            "Ying Feng",
            "Gregory Morrison",
            "Hans-Conrad zur Loye",
            "Jianjun Hu"
        ],
        "published": "2022-11-29T03:24:53Z",
        "summary": "Oxidation states are the charges of atoms after their ionic approximation of\ntheir bonds, which have been widely used in charge-neutrality verification,\ncrystal structure determination, and reaction estimation. Currently only\nheuristic rules exist for guessing the oxidation states of a given compound\nwith many exceptions. Recent work has developed machine learning models based\non heuristic structural features for predicting the oxidation states of metal\nions. However, composition based oxidation state prediction still remains\nelusive so far, which is more important in new material discovery for which the\nstructures are not even available. This work proposes a novel deep learning\nbased BERT transformer language model BERTOS for predicting the oxidation\nstates of all elements of inorganic compounds given only their chemical\ncomposition. Our model achieves 96.82\\% accuracy for all-element oxidation\nstates prediction benchmarked on the cleaned ICSD dataset and achieves 97.61\\%\naccuracy for oxide materials. We also demonstrate how it can be used to conduct\nlarge-scale screening of hypothetical material compositions for materials\ndiscovery.",
        "pdf_link": "https://arxiv.org/pdf/2211.15895v1.pdf"
    },
    {
        "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
        "authors": [
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2022-11-28T19:03:35Z",
        "summary": "In many task settings, text classification models are likely to encounter\nexamples from novel classes on which they cannot predict correctly. Selective\nprediction, in which models abstain on low-confidence examples, provides a\npossible solution, but existing models are often overly confident on unseen\nclasses. To remedy this overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-step method that generates OOD\nexamples representative of novel classes, then trains to decrease confidence on\nthem. First, we generate OOD examples by prompting a large language model\ntwice: we prompt it to enumerate relevant novel classes, then generate examples\nfrom each novel class matching the task format. Second, we train a classifier\nwith a novel contrastive objective that encourages lower confidence on\ngenerated OOD examples than training examples. When trained with CoNAL,\nclassifiers improve in their ability to detect and abstain on novel class\nexamples over prior methods by an average of 2.3% in terms of accuracy under\nthe accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with\nno cost to in-distribution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2211.15718v2.pdf"
    },
    {
        "title": "Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation",
        "authors": [
            "Sai Shashank Kalakonda",
            "Shubh Maheshwari",
            "Ravi Kiran Sarvadevabhatla"
        ],
        "published": "2022-11-28T17:57:48Z",
        "summary": "We introduce Action-GPT, a plug-and-play framework for incorporating Large\nLanguage Models (LLMs) into text-based action generation models. Action phrases\nin current motion capture datasets contain minimal and to-the-point\ninformation. By carefully crafting prompts for LLMs, we generate richer and\nfine-grained descriptions of the action. We show that utilizing these detailed\ndescriptions instead of the original action phrases leads to better alignment\nof text and motion spaces. We introduce a generic approach compatible with\nstochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion\nmodels. In addition, the approach enables multiple text descriptions to be\nutilized. Our experiments show (i) noticeable qualitative and quantitative\nimprovement in the quality of synthesized motions, (ii) benefits of utilizing\nmultiple LLM-generated descriptions, (iii) suitability of the prompt function,\nand (iv) zero-shot generation capabilities of the proposed approach. Project\npage: https://actiongpt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2211.15603v3.pdf"
    },
    {
        "title": "GPT-Neo for commonsense reasoning -- a theoretical and practical lens",
        "authors": [
            "Rohan Kashyap",
            "Vivek Kashyap",
            "Narendra C. P."
        ],
        "published": "2022-11-28T17:49:38Z",
        "summary": "Recent work has demonstrated substantial gains in pre-training large-language\nmodels (LLMs) followed by supervised fine-tuning on the downstream task. In\nthis paper, we evaluate the performance of the GPT-neo model using $6$\ncommonsense reasoning benchmark tasks. We aim to examine the performance of\nsmaller models using the GPT-neo models against several larger model baselines\nsuch as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the\nappropriate set of hyperparameters, our model achieves competitive accuracy on\nseveral tasks. We also investigate and substantiate our results using\nattention-head visualization to better understand the model performance.\nFinally, we conduct various robustness tests using various methods to gauge the\nmodel performance under numerous settings.",
        "pdf_link": "https://arxiv.org/pdf/2211.15593v2.pdf"
    },
    {
        "title": "Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling",
        "authors": [
            "Xinshi Wang",
            "Daniel Tang"
        ],
        "published": "2022-11-28T16:49:13Z",
        "summary": "Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.",
        "pdf_link": "https://arxiv.org/pdf/2211.15544v2.pdf"
    },
    {
        "title": "Hypernetworks for Zero-shot Transfer in Reinforcement Learning",
        "authors": [
            "Sahand Rezaei-Shoshtari",
            "Charlotte Morissette",
            "Francois Robert Hogan",
            "Gregory Dudek",
            "David Meger"
        ],
        "published": "2022-11-28T15:48:35Z",
        "summary": "In this paper, hypernetworks are trained to generate behaviors across a range\nof unseen task conditions, via a novel TD-based training objective and data\nfrom a set of near-optimal RL solutions for training tasks. This work relates\nto meta RL, contextual RL, and transfer learning, with a particular focus on\nzero-shot performance at test time, enabled by knowledge of the task parameters\n(also known as context). Our technical approach is based upon viewing each RL\nalgorithm as a mapping from the MDP specifics to the near-optimal value\nfunction and policy and seek to approximate it with a hypernetwork that can\ngenerate near-optimal value functions and policies, given the parameters of the\nMDP. We show that, under certain conditions, this mapping can be considered as\na supervised learning problem. We empirically evaluate the effectiveness of our\nmethod for zero-shot transfer to new reward and transition dynamics on a series\nof continuous control tasks from DeepMind Control Suite. Our method\ndemonstrates significant improvements over baselines from multitask and meta RL\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2211.15457v2.pdf"
    },
    {
        "title": "Scientific and Creative Analogies in Pretrained Language Models",
        "authors": [
            "Tamara Czinczoll",
            "Helen Yannakoudakis",
            "Pushkar Mishra",
            "Ekaterina Shutova"
        ],
        "published": "2022-11-28T12:49:44Z",
        "summary": "This paper examines the encoding of analogy in large-scale pretrained\nlanguage models, such as BERT and GPT-2. Existing analogy datasets typically\nfocus on a limited set of analogical relations, with a high similarity of the\ntwo domains between which the analogy holds. As a more realistic setup, we\nintroduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy\ndataset containing systematic mappings of multiple attributes and relational\nstructures across dissimilar domains. Using this dataset, we test the\nanalogical reasoning capabilities of several widely-used pretrained language\nmodels (LMs). We find that state-of-the-art LMs achieve low performance on\nthese complex analogy tasks, highlighting the challenges still posed by analogy\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2211.15268v1.pdf"
    },
    {
        "title": "Tackling Visual Control via Multi-View Exploration Maximization",
        "authors": [
            "Mingqi Yuan",
            "Xin Jin",
            "Bo Li",
            "Wenjun Zeng"
        ],
        "published": "2022-11-28T11:29:56Z",
        "summary": "We present MEM: Multi-view Exploration Maximization for tackling complex\nvisual control tasks. To the best of our knowledge, MEM is the first approach\nthat combines multi-view representation learning and intrinsic reward-driven\nexploration in reinforcement learning (RL). More specifically, MEM first\nextracts the specific and shared information of multi-view observations to form\nhigh-quality features before performing RL on the learned features, enabling\nthe agent to fully comprehend the environment and yield better actions.\nFurthermore, MEM transforms the multi-view features into intrinsic rewards\nbased on entropy maximization to encourage exploration. As a result, MEM can\nsignificantly promote the sample-efficiency and generalization ability of the\nRL agent, facilitating solving real-world problems with high-dimensional\nobservations and spare-reward space. We evaluate MEM on various tasks from\nDeepMind Control Suite and Procgen games. Extensive simulation results\ndemonstrate that MEM can achieve superior performance and outperform the\nbenchmarking schemes with simple architecture and higher efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2211.15233v1.pdf"
    },
    {
        "title": "Is it Required? Ranking the Skills Required for a Job-Title",
        "authors": [
            "Sarthak Anand",
            "Jens-Joris Decorte",
            "Niels Lowie"
        ],
        "published": "2022-11-28T10:27:11Z",
        "summary": "In this paper, we describe our method for ranking the skills required for a\ngiven job title. Our analysis shows that important/relevant skills appear more\nfrequently in similar job titles. We train a Language-agnostic BERT Sentence\nEncoder (LaBSE) model to predict the importance of the skills using weak\nsupervision. We show the model can learn the importance of skills and perform\nwell in other languages. Furthermore, we show how the Inverse Document\nFrequency factor of skill boosts the specialised skills.",
        "pdf_link": "https://arxiv.org/pdf/2212.08553v1.pdf"
    },
    {
        "title": "Revisiting Distance Metric Learning for Few-Shot Natural Language Classification",
        "authors": [
            "Witold Sosnowski",
            "Anna Wr\u00f3blewska",
            "Karolina Seweryn",
            "Piotr Gawrysiak"
        ],
        "published": "2022-11-28T10:19:31Z",
        "summary": "Distance Metric Learning (DML) has attracted much attention in image\nprocessing in recent years. This paper analyzes its impact on supervised\nfine-tuning language models for Natural Language Processing (NLP)\nclassification tasks under few-shot learning settings. We investigated several\nDML loss functions in training RoBERTa language models on known SentEval\nTransfer Tasks datasets. We also analyzed the possibility of using proxy-based\nDML losses during model inference.\n  Our systematic experiments have shown that under few-shot learning settings,\nparticularly proxy-based DML losses can positively affect the fine-tuning and\ninference of a supervised language model. Models tuned with a combination of\nCCE (categorical cross-entropy loss) and ProxyAnchor Loss have, on average, the\nbest performance and outperform models with only CCE by about 3.27 percentage\npoints -- up to 10.38 percentage points depending on the training dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.15202v1.pdf"
    },
    {
        "title": "Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All",
        "authors": [
            "Eylon Gueta",
            "Avi Shmidman",
            "Shaltiel Shmidman",
            "Cheyn Shmuel Shmidman",
            "Joshua Guedalia",
            "Moshe Koppel",
            "Dan Bareket",
            "Amit Seker",
            "Reut Tsarfaty"
        ],
        "published": "2022-11-28T10:17:35Z",
        "summary": "We present a new pre-trained language model (PLM) for modern Hebrew, termed\nAlephBERTGimmel, which employs a much larger vocabulary (128K items) than\nstandard Hebrew PLMs before. We perform a contrastive analysis of this model\nagainst all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the\neffects of larger vocabularies on task performance. Our experiments show that\nlarger vocabularies lead to fewer splits, and that reducing splits is better\nfor model performance, across different tasks. All in all this new model\nachieves new SOTA on all available Hebrew benchmarks, including Morphological\nSegmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment\nAnalysis. Subsequently we advocate for PLMs that are larger not only in terms\nof number of layers or training data, but also in terms of their vocabulary. We\nrelease the new model publicly for unrestricted use.",
        "pdf_link": "https://arxiv.org/pdf/2211.15199v2.pdf"
    },
    {
        "title": "Handling and extracting key entities from customer conversations using Speech recognition and Named Entity recognition",
        "authors": [
            "Sharvi Endait",
            "Ruturaj Ghatage",
            "Prof. DD Kadam"
        ],
        "published": "2022-11-28T06:41:29Z",
        "summary": "In this modern era of technology with e-commerce developing at a rapid pace,\nit is very important to understand customer requirements and details from a\nbusiness conversation. It is very crucial for customer retention and\nsatisfaction. Extracting key insights from these conversations is very\nimportant when it comes to developing their product or solving their issue.\nUnderstanding customer feedback, responses, and important details of the\nproduct are essential and it would be done using Named entity recognition\n(NER). For extracting the entities we would be converting the conversations to\ntext using the optimal speech-to-text model. The model would be a two-stage\nnetwork in which the conversation is converted to text. Then, suitable entities\nare extracted using robust techniques using a NER BERT transformer model. This\nwill aid in the enrichment of customer experience when there is an issue which\nis faced by them. If a customer faces a problem he will call and register his\ncomplaint. The model will then extract the key features from this conversation\nwhich will be necessary to look into the problem. These features would include\ndetails like the order number, and the exact problem. All these would be\nextracted directly from the conversation and this would reduce the effort of\ngoing through the conversation again.",
        "pdf_link": "https://arxiv.org/pdf/2211.17107v1.pdf"
    },
    {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "authors": [
            "Zhengfu He",
            "Tianxiang Sun",
            "Kuanning Wang",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2022-11-28T03:25:49Z",
        "summary": "We present DiffusionBERT, a new generative masked language model based on\ndiscrete diffusion models. Diffusion models and many pre-trained language\nmodels have a shared training objective, i.e., denoising, making it possible to\ncombine the two powerful models and enjoy the best of both worlds. On the one\nhand, diffusion models offer a promising training strategy that helps improve\nthe generation quality. On the other hand, pre-trained denoising language\nmodels (e.g., BERT) can be used as a good initialization that accelerates\nconvergence. We explore training BERT to learn the reverse process of a\ndiscrete diffusion process with an absorbing state and elucidate several\ndesigns to improve it. First, we propose a new noise schedule for the forward\ndiffusion process that controls the degree of noise added at each step based on\nthe information of each token. Second, we investigate several designs of\nincorporating the time step into BERT. Experiments on unconditional text\ngeneration demonstrate that DiffusionBERT achieves significant improvement over\nexisting diffusion models for text (e.g., D3PM and Diffusion-LM) and previous\ngenerative masked language models in terms of perplexity and BLEU score.",
        "pdf_link": "https://arxiv.org/pdf/2211.15029v2.pdf"
    },
    {
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "authors": [
            "Michiel A. Bakker",
            "Martin J. Chadwick",
            "Hannah R. Sheahan",
            "Michael Henry Tessler",
            "Lucy Campbell-Gillingham",
            "Jan Balaguer",
            "Nat McAleese",
            "Amelia Glaese",
            "John Aslanides",
            "Matthew M. Botvinick",
            "Christopher Summerfield"
        ],
        "published": "2022-11-28T02:24:14Z",
        "summary": "Recent work in large language modeling (LLMs) has used fine-tuning to align\noutputs with the preferences of a prototypical user. This work assumes that\nhuman preferences are static and homogeneous across individuals, so that\naligning to a a single \"generic\" user will confer more general alignment. Here,\nwe embrace the heterogeneity of human preferences to consider a different\nchallenge: how might a machine help people with diverse views find agreement?\nWe fine-tune a 70 billion parameter LLM to generate statements that maximize\nthe expected approval for a group of people with potentially diverse opinions.\nHuman participants provide written opinions on thousands of questions touching\non moral and political issues (e.g., \"should we raise taxes on the rich?\"), and\nrate the LLM's generated candidate consensus statements for agreement and\nquality. A reward model is then trained to predict individual preferences,\nenabling it to quantify and rank consensus statements in terms of their appeal\nto the overall group, defined according to different aggregation (social\nwelfare) functions. The model produces consensus statements that are preferred\nby human users over those from prompted LLMs (>70%) and significantly\noutperforms a tight fine-tuned baseline that lacks the final ranking step.\nFurther, our best model's consensus statements are preferred over the best\nhuman-generated opinions (>65%). We find that when we silently constructed\nconsensus statements from only a subset of group members, those who were\nexcluded were more likely to dissent, revealing the sensitivity of the\nconsensus to individual contributions. These results highlight the potential to\nuse LLMs to help groups of humans align their values with one another.",
        "pdf_link": "https://arxiv.org/pdf/2211.15006v1.pdf"
    },
    {
        "title": "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models",
        "authors": [
            "Peter Henderson",
            "Eric Mitchell",
            "Christopher D. Manning",
            "Dan Jurafsky",
            "Chelsea Finn"
        ],
        "published": "2022-11-27T21:43:45Z",
        "summary": "A growing ecosystem of large, open-source foundation models has reduced the\nlabeled data and technical expertise necessary to apply machine learning to\nmany new problems. Yet foundation models pose a clear dual-use risk,\nindiscriminately reducing the costs of building both harmful and beneficial\nmachine learning systems. Policy tools such as restricted model access and\nexport controls are the primary methods currently used to mitigate such\ndual-use risks. In this work, we review potential safe-release strategies and\nargue that both policymakers and AI researchers would benefit from\nfundamentally new technologies enabling more precise control over the\ndownstream usage of open-source foundation models. We propose one such\napproach: the task blocking paradigm, in which foundation models are trained\nwith an additional mechanism to impede adaptation to harmful tasks without\nsacrificing performance on desirable tasks. We call the resulting models\nself-destructing models, inspired by mechanisms that prevent adversaries from\nusing tools for harmful purposes. We present an algorithm for training\nself-destructing models leveraging techniques from meta-learning and\nadversarial learning, which we call meta-learned adversarial censoring (MLAC).\nIn a small-scale experiment, we show MLAC can largely prevent a BERT-style\nmodel from being re-purposed to perform gender identification without harming\nthe model's ability to perform profession classification.",
        "pdf_link": "https://arxiv.org/pdf/2211.14946v2.pdf"
    },
    {
        "title": "Multi-Modal Few-Shot Temporal Action Detection",
        "authors": [
            "Sauradip Nag",
            "Mengmeng Xu",
            "Xiatian Zhu",
            "Juan-Manuel Perez-Rua",
            "Bernard Ghanem",
            "Yi-Zhe Song",
            "Tao Xiang"
        ],
        "published": "2022-11-27T18:13:05Z",
        "summary": "Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET",
        "pdf_link": "https://arxiv.org/pdf/2211.14905v2.pdf"
    },
    {
        "title": "Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5",
        "authors": [
            "Nghi D. Q. Bui",
            "Yue Wang",
            "Steven Hoi"
        ],
        "published": "2022-11-27T16:11:29Z",
        "summary": "Automated software debugging is a crucial task for improving the productivity\nof software developers. Many neural-based techniques have been proven effective\nfor debugging-related tasks such as bug localization and program repair (or bug\nfixing). However, these techniques often focus only on either one of them or\napproach them in a stage-wise manner, ignoring the mutual benefits between\nthem. In this work, we propose a novel unified \\emph{Detect-Localize-Repair}\nframework based on a pretrained programming language model CodeT5 to seamlessly\naddress these tasks, named CodeT5-DLR. Specifically, we propose three\nobjectives to adapt the generic CodeT5 for debugging: a bug detection objective\nto determine whether a given code snippet is buggy or not, a bug localization\nobjective to identify the buggy lines, and a program repair objective to\ntranslate the buggy code to its fixed version. We evaluate it on each of these\ntasks and their combined setting on two newly collected line-level debugging\ndatasets in Java and Python. Extensive results show that our model\nsignificantly outperforms existing baselines from both NLP and software\nengineering domains.",
        "pdf_link": "https://arxiv.org/pdf/2211.14875v3.pdf"
    },
    {
        "title": "Understanding BLOOM: An empirical study on diverse NLP tasks",
        "authors": [
            "Parag Pravin Dakle",
            "SaiKrishna Rallabandi",
            "Preethi Raghavan"
        ],
        "published": "2022-11-27T15:48:14Z",
        "summary": "We view the landscape of large language models (LLMs) through the lens of the\nrecently released BLOOM model to understand the performance of BLOOM and other\ndecoder-only LLMs compared to BERT-style encoder-only models. We achieve this\nby evaluating the smaller BLOOM model variants (\\textit{350m/560m} and\n\\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards.\nWe make the following observations: (1) BLOOM performance does not scale with\nparameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning\nBLOOM models show that the 560m variant performs similarly to or better than\nthe 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning\nexperiments show that BLOOM is at par or worse than monolingual GPT-2 models,\nand (3) Toxicity analysis of prompt-based text generation using the\nRealToxicityPrompts dataset shows that the text generated by BLOOM is at least\n17\\% less toxic than GPT-2 and GPT-3 models.",
        "pdf_link": "https://arxiv.org/pdf/2211.14865v2.pdf"
    },
    {
        "title": "ESIE-BERT: Enriching Sub-words Information Explicitly with BERT for Joint Intent Classification and SlotFilling",
        "authors": [
            "Yu Guo",
            "Zhilong Xie",
            "Xingyan Chen",
            "Huangen Chen",
            "Leilei Wang",
            "Huaming Du",
            "Shaopeng Wei",
            "Yu Zhao",
            "Qing Li",
            "Gang Wu"
        ],
        "published": "2022-11-27T13:49:19Z",
        "summary": "Natural language understanding (NLU) has two core tasks: intent\nclassification and slot filling. The success of pre-training language models\nresulted in a significant breakthrough in the two tasks. One of the promising\nsolutions called BERT can jointly optimize the two tasks. We note that\nBERT-based models convert each complex token into multiple sub-tokens by\nwordpiece algorithm, which generates a mismatch between the lengths of the\ntokens and the labels. This leads to BERT-based models do not do well in label\nprediction which limits model performance improvement. Many existing models can\nbe compatible with this issue but some hidden semantic information is discarded\nin the fine-tuning process. We address the problem by introducing a novel joint\nmethod on top of BERT which explicitly models the multiple sub-tokens features\nafter wordpiece tokenization, thereby contributing to the two tasks. Our method\ncan well extract the contextual features from complex tokens by the proposed\nsub-words attention adapter (SAA), which preserves overall utterance\ninformation. Additionally, we propose an intent attention adapter (IAA) to\nobtain the full sentence features to aid users to predict intent. Experimental\nresults confirm that our proposed model is significantly improved on two public\nbenchmark datasets. In particular, the slot filling F1 score is improved from\n96.1 to 98.2 (2.1% absolute) on the Airline Travel Information Systems (ATIS)\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.14829v3.pdf"
    },
    {
        "title": "An Automatic SOAP Classification System Using Weakly Supervision And Transfer Learning",
        "authors": [
            "Sunjae Kwon",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2022-11-26T10:58:18Z",
        "summary": "In this paper, we introduce a comprehensive framework for developing a\nmachine learning-based SOAP (Subjective, Objective, Assessment, and Plan)\nclassification system without manually SOAP annotated training data or with\nless manually SOAP annotated training data. The system is composed of the\nfollowing two parts: 1) Data construction, 2) A neural network-based SOAP\nclassifier, and 3) Transfer learning framework. In data construction, since a\nmanual construction of a large size training dataset is expensive, we propose a\nrule-based weak labeling method utilizing the structured information of an EHR\nnote. Then, we present a SOAP classifier composed of a pre-trained language\nmodel and bi-directional long-short term memory with conditional random field\n(Bi-LSTM-CRF). Finally, we propose a transfer learning framework that re-uses\nthe trained parameters of the SOAP classifier trained with the weakly labeled\ndataset for datasets collected from another hospital. The proposed weakly\nlabel-based learning model successfully performed SOAP classification (89.99\nF1-score) on the notes collected from the target hospital. Otherwise, in the\nnotes collected from other hospitals and departments, the performance\ndramatically decreased. Meanwhile, we verified that the transfer learning\nframework is advantageous for inter-hospital adaptation of the model increasing\nthe models' performance in every cases. In particular, the transfer learning\napproach was more efficient when the manually annotated data size was smaller.\nWe showed that SOAP classification models trained with our weakly labeling\nalgorithm can perform SOAP classification without manually annotated data on\nthe EHR notes from the same hospital. The transfer learning framework helps\nSOAP classification model's inter-hospital migration with a minimal size of the\nmanually annotated dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.14539v1.pdf"
    },
    {
        "title": "SKDBERT: Compressing BERT via Stochastic Knowledge Distillation",
        "authors": [
            "Zixiang Ding",
            "Guoqing Jiang",
            "Shuai Zhang",
            "Lin Guo",
            "Wei Lin"
        ],
        "published": "2022-11-26T03:18:55Z",
        "summary": "In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain\ncompact BERT-style language model dubbed SKDBERT. In each iteration, SKD\nsamples a teacher model from a pre-defined teacher ensemble, which consists of\nmultiple teacher models with multi-level capacities, to transfer knowledge into\nstudent model in an one-to-one manner. Sampling distribution plays an important\nrole in SKD. We heuristically present three types of sampling distributions to\nassign appropriate probabilities for multi-level teacher models. SKD has two\nadvantages: 1) it can preserve the diversities of multi-level teacher models\nvia stochastically sampling single teacher model in each iteration, and 2) it\ncan also improve the efficacy of knowledge distillation via multi-level teacher\nmodels when large capacity gap exists between the teacher model and the student\nmodel. Experimental results on GLUE benchmark show that SKDBERT reduces the\nsize of a BERT$_{\\rm BASE}$ model by 40% while retaining 99.5% performances of\nlanguage understanding and being 100% faster.",
        "pdf_link": "https://arxiv.org/pdf/2211.14466v2.pdf"
    },
    {
        "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
        "authors": [
            "Aristides Milios",
            "Parishad BehnamGhader"
        ],
        "published": "2022-11-25T23:38:08Z",
        "summary": "Although large pre-trained language models have achieved great success in\nmany NLP tasks, it has been shown that they reflect human biases from their\npre-training corpora. This bias may lead to undesirable outcomes when these\nmodels are applied in real-world settings. In this paper, we investigate the\nbias present in monolingual BERT models across a diverse set of languages\n(English, Greek, and Persian). While recent research has mostly focused on\ngender-related biases, we analyze religious and ethnic biases as well and\npropose a template-based method to measure any kind of bias, based on sentence\npseudo-likelihood, that can handle morphologically complex languages with\ngender-based adjective declensions. We analyze each monolingual model via this\nmethod and visualize cultural similarities and differences across different\ndimensions of bias. Ultimately, we conclude that current methods of probing for\nbias are highly language-dependent, necessitating cultural insights regarding\nthe unique ways bias is expressed in each language and culture (e.g. through\ncoded language, synecdoche, and other similar linguistic concepts). We also\nhypothesize that higher measured social biases in the non-English BERT models\ncorrelate with user-generated content in their training.",
        "pdf_link": "https://arxiv.org/pdf/2211.14402v1.pdf"
    },
    {
        "title": "Finetuning BERT on Partially Annotated NER Corpora",
        "authors": [
            "Viktor Scherbakov",
            "Vladimir Mayorov"
        ],
        "published": "2022-11-25T19:54:30Z",
        "summary": "Most Named Entity Recognition (NER) models operate under the assumption that\ntraining datasets are fully labelled. While it is valid for established\ndatasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain\nthe complete dataset annotation. These situations may occur, for instance,\nafter selective annotation of entities for cost reduction. This work presents\nan approach to finetuning BERT on such partially labelled datasets using\nself-supervision and label preprocessing. Our approach outperforms the previous\nLSTM-based label preprocessing baseline, significantly improving the\nperformance on poorly labelled datasets. We demonstrate that following our\napproach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total\nentities labelled is enough to reach the performance of the baseline trained on\nthe same dataset with 50% of the entities labelled.",
        "pdf_link": "https://arxiv.org/pdf/2211.14360v1.pdf"
    },
    {
        "title": "GPT-3-driven pedagogical agents for training children's curious question-asking skills",
        "authors": [
            "Rania Abdelghani",
            "Yen-Hsiang Wang",
            "Xingdi Yuan",
            "Tong Wang",
            "Pauline Lucas",
            "H\u00e9l\u00e8ne Sauz\u00e9on",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2022-11-25T16:41:59Z",
        "summary": "In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.",
        "pdf_link": "https://arxiv.org/pdf/2211.14228v6.pdf"
    },
    {
        "title": "PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices",
        "authors": [
            "Kazuki Osawa",
            "Shigang Li",
            "Torsten Hoefler"
        ],
        "published": "2022-11-25T14:16:35Z",
        "summary": "Pipeline parallelism enables efficient training of Large Language Models\n(LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles\nduring startup and tear-down reduce the utilization of accelerators. Although\nefficient pipeline schemes with micro-batching and bidirectional pipelines have\nbeen proposed to maximize utilization, a significant number of bubbles cannot\nbe filled using synchronous forward and backward passes. To address this\nproblem, we suggest that extra work be assigned to the bubbles to gain\nauxiliary benefits in LLM training. As an example in this direction, we propose\nPipeFisher, which assigns the work of K-FAC, a second-order optimization method\nbased on the Fisher information matrix, to the bubbles to accelerate\nconvergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher\nreduces the (simulated) training time to 50-75% compared to training with a\nfirst-order optimizer by greatly improving the accelerator utilization and\nbenefiting from the improved convergence by K-FAC.",
        "pdf_link": "https://arxiv.org/pdf/2211.14133v2.pdf"
    },
    {
        "title": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels",
        "authors": [
            "Siyuan Li",
            "Li Sun",
            "Qingli Li"
        ],
        "published": "2022-11-25T09:41:57Z",
        "summary": "Pre-trained vision-language models like CLIP have recently shown superior\nperformances on various downstream tasks, including image classification and\nsegmentation. However, in fine-grained image re-identification (ReID), the\nlabels are indexes, lacking concrete text descriptions. Therefore, it remains\nto be determined how such models could be applied to these tasks. This paper\nfirst finds out that simply fine-tuning the visual model initialized by the\nimage encoder in CLIP, has already obtained competitive performances in various\nReID tasks. Then we propose a two-stage strategy to facilitate a better visual\nrepresentation. The key idea is to fully exploit the cross-modal description\nability in CLIP through a set of learnable text tokens for each ID and give\nthem to the text encoder to form ambiguous descriptions. In the first training\nstage, image and text encoders from CLIP keep fixed, and only the text tokens\nare optimized from scratch by the contrastive loss computed within a batch. In\nthe second stage, the ID-specific text tokens and their encoder become static,\nproviding constraints for fine-tuning the image encoder. With the help of the\ndesigned loss in the downstream task, the image encoder is able to represent\ndata as vectors in the feature embedding accurately. The effectiveness of the\nproposed strategy is validated on several datasets for the person or vehicle\nReID tasks. Code is available at https://github.com/Syliz517/CLIP-ReID.",
        "pdf_link": "https://arxiv.org/pdf/2211.13977v4.pdf"
    },
    {
        "title": "The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future",
        "authors": [
            "Philipp Hacker"
        ],
        "published": "2022-11-25T09:08:11Z",
        "summary": "As ChatGPT et al. conquer the world, the optimal liability framework for AI\nsystems remains an unsolved problem across the globe. In a much-anticipated\nmove, the European Commission advanced two proposals outlining the European\napproach to AI liability in September 2022: a novel AI Liability Directive and\na revision of the Product Liability Directive. They constitute the final\ncornerstone of EU AI regulation. Crucially, the liability proposals and the EU\nAI Act are inherently intertwined: the latter does not contain any individual\nrights of affected persons, and the former lack specific, substantive rules on\nAI development and deployment. Taken together, these acts may well trigger a\nBrussels Effect in AI regulation, with significant consequences for the US and\nbeyond.\n  This paper makes three novel contributions. First, it examines in detail the\nCommission proposals and shows that, while making steps in the right direction,\nthey ultimately represent a half-hearted approach: if enacted as foreseen, AI\nliability in the EU will primarily rest on disclosure of evidence mechanisms\nand a set of narrowly defined presumptions concerning fault, defectiveness and\ncausality. Hence, second, the article suggests amendments, which are collected\nin an Annex at the end of the paper. Third, based on an analysis of the key\nrisks AI poses, the final part of the paper maps out a road for the future of\nAI liability and regulation, in the EU and beyond. This includes: a\ncomprehensive framework for AI liability; provisions to support innovation; an\nextension to non-discrimination/algorithmic fairness, as well as explainable\nAI; and sustainability. I propose to jump-start sustainable AI regulation via\nsustainability impact assessments in the AI Act and sustainable design defects\nin the liability regime. In this way, the law may help spur not only fair AI\nand XAI, but potentially also sustainable AI (SAI).",
        "pdf_link": "https://arxiv.org/pdf/2211.13960v6.pdf"
    },
    {
        "title": "Comparison Study Between Token Classification and Sequence Classification In Text Classification",
        "authors": [
            "Amir Jafari"
        ],
        "published": "2022-11-25T05:14:58Z",
        "summary": "Unsupervised Machine Learning techniques have been applied to Natural\nLanguage Processing tasks and surpasses the benchmarks such as GLUE with great\nsuccess. Building language models approach achieves good results in one\nlanguage and it can be applied to multiple NLP task such as classification,\nsummarization, generation and etc as an out of box model. Among all the of the\nclassical approaches used in NLP, the masked language modeling is the most\nused. In general, the only requirement to build a language model is presence of\nthe large corpus of textual data. Text classification engines uses a variety of\nmodels from classical and state of art transformer models to classify texts for\nin order to save costs. Sequence Classifiers are mostly used in the domain of\ntext classification. However Token classifiers also are viable candidate models\nas well. Sequence Classifiers and Token Classifier both tend to improve the\nclassification predictions due to the capturing the context information\ndifferently. This work aims to compare the performance of Sequence Classifier\nand Token Classifiers and evaluate each model on the same set of data. In this\nwork, we are using a pre-trained model as the base model and Token Classifier\nand Sequence Classier heads results of these two scoring paradigms with be\ncompared..",
        "pdf_link": "https://arxiv.org/pdf/2211.13899v1.pdf"
    },
    {
        "title": "Complementary Explanations for Effective In-Context Learning",
        "authors": [
            "Xi Ye",
            "Srinivasan Iyer",
            "Asli Celikyilmaz",
            "Ves Stoyanov",
            "Greg Durrett",
            "Ramakanth Pasunuru"
        ],
        "published": "2022-11-25T04:40:47Z",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts, but there has been limited understanding\nof exactly how these explanations function or why they are effective. This work\naims to better understand the mechanisms by which explanations are used for\nin-context learning. We first study the impact of two different factors on the\nperformance of prompts with explanations: the computation trace (the way the\nsolution is decomposed) and the natural language used to express the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set:\ndiverse reasoning skills shown by different exemplars can lead to better\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\nselection approach for constructing exemplar sets that are both relevant as\nwell as complementary, which successfully improves the in-context learning\nperformance across three real-world tasks on multiple LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2211.13892v2.pdf"
    },
    {
        "title": "Using Selective Masking as a Bridge between Pre-training and Fine-tuning",
        "authors": [
            "Tanish Lad",
            "Himanshu Maheshwari",
            "Shreyas Kottukkal",
            "Radhika Mamidi"
        ],
        "published": "2022-11-24T22:25:27Z",
        "summary": "Pre-training a language model and then fine-tuning it for downstream tasks\nhas demonstrated state-of-the-art results for various NLP tasks. Pre-training\nis usually independent of the downstream task, and previous works have shown\nthat this pre-training alone might not be sufficient to capture the\ntask-specific nuances. We propose a way to tailor a pre-trained BERT model for\nthe downstream task via task-specific masking before the standard supervised\nfine-tuning. For this, a word list is first collected specific to the task. For\nexample, if the task is sentiment classification, we collect a small sample of\nwords representing both positive and negative sentiments. Next, a word's\nimportance for the task, called the word's task score, is measured using the\nword list. Each word is then assigned a probability of masking based on its\ntask score. We experiment with different masking functions that assign the\nprobability of masking based on the word's task score. The BERT model is\nfurther trained on MLM objective, where masking is done using the above\nstrategy. Following this standard supervised fine-tuning is done for different\ndownstream tasks. Results on these tasks show that the selective masking\nstrategy outperforms random masking, indicating its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2211.13815v1.pdf"
    },
    {
        "title": "Question-type Identification for Academic Questions in Online Learning Platform",
        "authors": [
            "Azam Rabiee",
            "Alok Goel",
            "Johnson D'Souza",
            "Saurabh Khanwalkar"
        ],
        "published": "2022-11-24T17:28:29Z",
        "summary": "Online learning platforms provide learning materials and answers to students'\nacademic questions by experts, peers, or systems. This paper explores\nquestion-type identification as a step in content understanding for an online\nlearning platform. The aim of the question-type identifier is to categorize\nquestion types based on their structure and complexity, using the question\ntext, subject, and structural features. We have defined twelve question-type\nclasses, including Multiple-Choice Question (MCQ), essay, and others. We have\ncompiled an internal dataset of students' questions and used a combination of\nweak-supervision techniques and manual annotation. We then trained a BERT-based\nensemble model on this dataset and evaluated this model on a separate\nhuman-labeled test set. Our experiments yielded an F1-score of 0.94 for MCQ\nbinary classification and promising results for 12-class multilabel\nclassification. We deployed the model in our online learning platform as a\ncrucial enabler for content understanding to enhance the student learning\nexperience.",
        "pdf_link": "https://arxiv.org/pdf/2211.13727v1.pdf"
    },
    {
        "title": "Undesirable Biases in NLP: Addressing Challenges of Measurement",
        "authors": [
            "Oskar van der Wal",
            "Dominik Bachmann",
            "Alina Leidinger",
            "Leendert van Maanen",
            "Willem Zuidema",
            "Katrin Schulz"
        ],
        "published": "2022-11-24T16:53:18Z",
        "summary": "As Large Language Models and Natural Language Processing (NLP) technology\nrapidly develop and spread into daily life, it becomes crucial to anticipate\nhow their use could harm people. One problem that has received a lot of\nattention in recent years is that this technology has displayed harmful biases,\nfrom generating derogatory stereotypes to producing disparate outcomes for\ndifferent social groups. Although a lot of effort has been invested in\nassessing and mitigating these biases, our methods of measuring the biases of\nNLP models have serious problems and it is often unclear what they actually\nmeasure. In this paper, we provide an interdisciplinary approach to discussing\nthe issue of NLP model bias by adopting the lens of psychometrics -- a field\nspecialized in the measurement of concepts like bias that are not directly\nobservable. In particular, we will explore two central notions from\npsychometrics, the construct validity and the reliability of measurement tools,\nand discuss how they can be applied in the context of measuring model bias. Our\ngoal is to provide NLP practitioners with methodological tools for designing\nbetter bias measures, and to inspire them more generally to explore tools from\npsychometrics when working on bias measurement tools.",
        "pdf_link": "https://arxiv.org/pdf/2211.13709v4.pdf"
    },
    {
        "title": "InDEX: Indonesian Idiom and Expression Dataset for Cloze Test",
        "authors": [
            "Xinying Qiu",
            "Guofeng Shi"
        ],
        "published": "2022-11-24T02:05:47Z",
        "summary": "We propose InDEX, an Indonesian Idiom and Expression dataset for cloze test.\nThe dataset contains 10438 unique sentences for 289 idioms and expressions for\nwhich we generate 15 different types of distractors, resulting in a large\ncloze-style corpus. Many baseline models of cloze test reading comprehension\napply BERT with random initialization to learn embedding representation. But\nidioms and fixed expressions are different such that the literal meaning of the\nphrases may or may not be consistent with their contextual meaning. Therefore,\nwe explore different ways to combine static and contextual representations for\na stronger baseline model. Experimentations show that combining definition and\nrandom initialization will better support cloze test model performance for\nidioms whether independently or mixed with fixed expressions. While for fixed\nexpressions with no special meaning, static embedding with random\ninitialization is sufficient for cloze test model.",
        "pdf_link": "https://arxiv.org/pdf/2211.13376v1.pdf"
    },
    {
        "title": "Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring",
        "authors": [
            "Xinying Qiu",
            "Shuxuan Liao",
            "Jiajun Xie",
            "Jian-Yun Nie"
        ],
        "published": "2022-11-24T02:00:03Z",
        "summary": "In the prompt-specific holistic score prediction task for Automatic Essay\nScoring, the general approaches include pre-trained neural model, coherence\nmodel, and hybrid model that incorporate syntactic features with neural model.\nIn this paper, we propose a novel approach to extract and represent essay\ncoherence features with prompt-learning NSP that shows to match the\nstate-of-the-art AES coherence model, and achieves the best performance for\nlong essays. We apply syntactic feature dense embedding to augment BERT-based\nmodel and achieve the best performance for hybrid methodology for AES. In\naddition, we explore various ideas to combine coherence, syntactic information\nand semantic embeddings, which no previous study has done before. Our combined\nmodel also performs better than the SOTA available for combined model, even\nthough it does not outperform our syntactic enhanced neural model. We further\noffer analyses that can be useful for future study.",
        "pdf_link": "https://arxiv.org/pdf/2211.13373v1.pdf"
    },
    {
        "title": "SEAT: Stable and Explainable Attention",
        "authors": [
            "Lijie Hu",
            "Yixin Liu",
            "Ninghao Liu",
            "Mengdi Huai",
            "Lichao Sun",
            "Di Wang"
        ],
        "published": "2022-11-23T20:33:30Z",
        "summary": "Currently, attention mechanism becomes a standard fixture in most\nstate-of-the-art natural language processing (NLP) models, not only due to\noutstanding performance it could gain, but also due to plausible innate\nexplanation for the behaviors of neural architectures it provides, which is\nnotoriously difficult to analyze. However, recent studies show that attention\nis unstable against randomness and perturbations during training or testing,\nsuch as random seeds and slight perturbation of embedding vectors, which\nimpedes it from becoming a faithful explanation tool. Thus, a natural question\nis whether we can find some substitute of the current attention which is more\nstable and could keep the most important characteristics on explanation and\nprediction of attention. In this paper, to resolve the problem, we provide a\nfirst rigorous definition of such alternate namely SEAT (Stable and Explainable\nAttention). Specifically, a SEAT should has the following three properties: (1)\nIts prediction distribution is enforced to be close to the distribution based\non the vanilla attention; (2) Its top-k indices have large overlaps with those\nof the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any\nslight perturbation on SEAT will not change the prediction distribution too\nmuch, which implicitly indicates that it is stable to randomness and\nperturbations. Finally, through intensive experiments on various datasets, we\ncompare our SEAT with other baseline methods using RNN, BiLSTM and BERT\narchitectures via six different evaluation metrics for model interpretation,\nstability and accuracy. Results show that SEAT is more stable against different\nperturbations and randomness while also keeps the explainability of attention,\nwhich indicates it is a more faithful explanation. Moreover, compared with\nvanilla attention, there is almost no utility (accuracy) degradation for SEAT.",
        "pdf_link": "https://arxiv.org/pdf/2211.13290v1.pdf"
    },
    {
        "title": "SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label",
        "authors": [
            "Aneesha Sampath",
            "Victoria Lin",
            "Louis-Philippe Morency"
        ],
        "published": "2022-11-23T18:35:15Z",
        "summary": "Many machine learning tasks -- particularly those in affective computing --\nare inherently subjective. When asked to classify facial expressions or to rate\nan individual's attractiveness, humans may disagree with one another, and no\nsingle answer may be objectively correct. However, machine learning datasets\ncommonly have just one \"ground truth\" label for each sample, so models trained\non these labels may not perform well on tasks that are subjective in nature.\nThough allowing models to learn from the individual annotators' ratings may\nhelp, most datasets do not provide annotator-specific labels for each sample.\nTo address this issue, we propose SeedBERT, a method for recovering annotator\nrating distributions from a single label by inducing pre-trained models to\nattend to different portions of the input. Our human evaluations indicate that\nSeedBERT's attention mechanism is consistent with human sources of annotator\ndisagreement. Moreover, in our empirical evaluations using large language\nmodels, SeedBERT demonstrates substantial gains in performance on downstream\nsubjective tasks compared both to standard deep learning models and to other\ncurrent models that account explicitly for annotator disagreement.",
        "pdf_link": "https://arxiv.org/pdf/2211.13196v1.pdf"
    },
    {
        "title": "This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",
        "authors": [
            "\u0141ukasz Augustyniak",
            "Kamil Tagowski",
            "Albert Sawczyn",
            "Denis Janiak",
            "Roman Bartusiak",
            "Adrian Szymczak",
            "Marcin W\u0105troba",
            "Arkadiusz Janz",
            "Piotr Szyma\u0144ski",
            "Miko\u0142aj Morzy",
            "Tomasz Kajdanowicz",
            "Maciej Piasecki"
        ],
        "published": "2022-11-23T16:51:09Z",
        "summary": "The availability of compute and data to train larger and larger language\nmodels increases the demand for robust methods of benchmarking the true\nprogress of LM training. Recent years witnessed significant progress in\nstandardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or\nKILT have become de facto standard tools to compare large language models.\nFollowing the trend to replicate GLUE for other languages, the KLEJ benchmark\nhas been released for Polish. In this paper, we evaluate the progress in\nbenchmarking for low-resourced languages. We note that only a handful of\nlanguages have such comprehensive benchmarks. We also note the gap in the\nnumber of tasks being evaluated by benchmarks for resource-rich English/Chinese\nand the rest of the world. In this paper, we introduce LEPISZCZE (the Polish\nword for glew, the Middle English predecessor of glue), a new, comprehensive\nbenchmark for Polish NLP with a large variety of tasks and high-quality\noperationalization of the benchmark. We design LEPISZCZE with flexibility in\nmind. Including new models, datasets, and tasks is as simple as possible while\nstill offering data versioning and model tracking. In the first run of the\nbenchmark, we test 13 experiments (task and dataset pairs) based on the five\nmost recent LMs for Polish. We use five datasets from the Polish benchmark and\nadd eight novel datasets. As the paper's main contribution, apart from\nLEPISZCZE, we provide insights and experiences learned while creating the\nbenchmark for Polish as the blueprint to design similar benchmarks for other\nlow-resourced languages.",
        "pdf_link": "https://arxiv.org/pdf/2211.13112v1.pdf"
    },
    {
        "title": "Improving Visual-textual Sentiment Analysis by Fusing Expert Features",
        "authors": [
            "Junyu Chen",
            "Jie An",
            "Hanjia Lyu",
            "Jiebo Luo"
        ],
        "published": "2022-11-23T14:40:51Z",
        "summary": "Visual-textual sentiment analysis aims to predict sentiment with the input of\na pair of image and text. The main challenge of visual-textual sentiment\nanalysis is how to learn effective visual features for sentiment prediction\nsince input images are often very diverse. To address this challenge, we\npropose a new method that improves visual-textual sentiment analysis by\nintroducing powerful expert visual features. The proposed method consists of\nfour parts: (1) a visual-textual branch to learn features directly from data\nfor sentiment analysis, (2) a visual expert branch with a set of pre-trained\n\"expert\" encoders to extract effective visual features, (3) a CLIP branch to\nimplicitly model visual-textual correspondence, and (4) a multimodal feature\nfusion network based on either BERT or MLP to fuse multimodal features and make\nsentiment prediction. Extensive experiments on three datasets show that our\nmethod produces better visual-textual sentiment analysis performance than\nexisting methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.12981v1.pdf"
    },
    {
        "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
        "authors": [
            "Kumar Shridhar",
            "Jakub Macina",
            "Mennatallah El-Assady",
            "Tanmay Sinha",
            "Manu Kapur",
            "Mrinmaya Sachan"
        ],
        "published": "2022-11-23T10:40:22Z",
        "summary": "Socratic questioning is an educational method that allows students to\ndiscover answers to complex problems by asking them a series of thoughtful\nquestions. Generation of didactically sound questions is challenging, requiring\nunderstanding of the reasoning process involved in the problem. We hypothesize\nthat such questioning strategy can not only enhance the human performance, but\nalso assist the math word problem (MWP) solvers. In this work, we explore the\nability of large language models (LMs) in generating sequential questions for\nguiding math word problem-solving. We propose various guided question\ngeneration schemes based on input conditioning and reinforcement learning. On\nboth automatic and human quality evaluations, we find that LMs constrained with\ndesirable question properties generate superior questions and improve the\noverall performance of a math word problem solver. We conduct a preliminary\nuser study to examine the potential value of such question generation models in\nthe education domain. Results suggest that the difficulty level of problems\nplays an important role in determining whether questioning improves or hinders\nhuman performance. We discuss the future of using such questioning strategies\nin education.",
        "pdf_link": "https://arxiv.org/pdf/2211.12835v1.pdf"
    },
    {
        "title": "Word-Level Representation From Bytes For Language Modeling",
        "authors": [
            "Chu-Tak Lee",
            "Qipeng Guo",
            "Xipeng Qiu"
        ],
        "published": "2022-11-23T03:11:13Z",
        "summary": "Modern language models mostly take sub-words as input, a design that balances\nthe trade-off between vocabulary size, number of parameters, and performance.\nHowever, sub-word tokenization still has disadvantages like not being robust to\nnoise and difficult to generalize to new languages. Also, the current trend of\nscaling up models reveals that larger models require larger embeddings but that\nmakes parallelization hard. Previous work on image classification proves\nsplitting raw input into a sequence of chucks is a strong, model-agnostic\ninductive bias. Based on this observation, we rethink the existing\ncharacter-aware method that takes character-level inputs but makes word-level\nsequence modeling and prediction. We overhaul this method by introducing a\ncross-attention network that builds word-level representation directly from\nbytes, and a sub-word level prediction based on word-level hidden states to\navoid the time and space requirement of word-level prediction. With these two\nimprovements combined, we have a token free model with slim input embeddings\nfor downstream tasks. We name our method Byte2Word and perform evaluations on\nlanguage modeling and text classification. Experiments show that Byte2Word is\non par with the strong sub-word baseline BERT but only takes up 10\\% of\nembedding size. We further test our method on synthetic noise and cross-lingual\ntransfer and find it competitive to baseline methods on both settings.",
        "pdf_link": "https://arxiv.org/pdf/2211.12677v1.pdf"
    },
    {
        "title": "HyperTuning: Toward Adapting Large Language Models without Back-propagation",
        "authors": [
            "Jason Phang",
            "Yi Mao",
            "Pengcheng He",
            "Weizhu Chen"
        ],
        "published": "2022-11-22T18:52:25Z",
        "summary": "Fine-tuning large language models for different tasks can be costly and\ninefficient, and even methods that reduce the number of tuned parameters still\nrequire full gradient-based optimization. We propose HyperTuning, a novel\napproach to model adaptation that uses a hypermodel to generate task-specific\nparameters for a fixed downstream model. We demonstrate a simple setup for\nhypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or\nLoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5\nin two stages: first, hyperpretraining with a modified conditional language\nmodeling objective that trains a hypermodel to generate parameters; second,\nmulti-task fine-tuning (MTF) on a large number of diverse language tasks. We\nevaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and\nshow that it can effectively generate parameters for unseen tasks. Moreover, we\nshow that using hypermodel-generated parameters as initializations for further\nparameter-efficient fine-tuning improves performance. HyperTuning can thus be a\nflexible and efficient way to leverage large language models for diverse\ndownstream applications.",
        "pdf_link": "https://arxiv.org/pdf/2211.12485v1.pdf"
    },
    {
        "title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions",
        "authors": [
            "Zhifang Guo",
            "Yichong Leng",
            "Yihan Wu",
            "Sheng Zhao",
            "Xu Tan"
        ],
        "published": "2022-11-22T10:58:38Z",
        "summary": "Using a text description as prompt to guide the generation of text or images\n(e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and\nimage generation, in this work, we explore the possibility of utilizing text\ndescriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS)\nsystem (dubbed as PromptTTS) that takes a prompt with both style and content\ndescriptions as input to synthesize the corresponding speech. Specifically,\nPromptTTS consists of a style encoder and a content encoder to extract the\ncorresponding representations from the prompt, and a speech decoder to\nsynthesize speech according to the extracted style and content representations.\nCompared with previous works in controllable TTS that require users to have\nacoustic knowledge to understand style factors such as prosody and pitch,\nPromptTTS is more user-friendly since text descriptions are a more natural way\nto express speech style (e.g., ''A lady whispers to her friend slowly''). Given\nthat there is no TTS dataset with prompts, to benchmark the task of PromptTTS,\nwe construct and release a dataset containing prompts with style and content\ninformation and the corresponding speech. Experiments show that PromptTTS can\ngenerate speech with precise style control and high speech quality. Audio\nsamples and our dataset are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2211.12171v1.pdf"
    },
    {
        "title": "OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type",
        "authors": [
            "Suresh Kumar",
            "P Sreenivasa Kumar"
        ],
        "published": "2022-11-22T10:42:07Z",
        "summary": "Machine generation of Arithmetic Word Problems (AWPs) is challenging as they\nexpress quantities and mathematical relationships and need to be consistent.\nML-solvers require a large annotated training set of consistent problems with\nlanguage variations. Exploiting domain-knowledge is needed for consistency\nchecking whereas LSTM-based approaches are good for producing text with\nlanguage variations. Combining these we propose a system, OLGA, to generate\nconsistent word problems of TC (Transfer-Case) type, involving object transfers\namong agents. Though we provide a dataset of consistent 2-agent TC-problems for\ntraining, only about 36% of the outputs of an LSTM-based generator are found\nconsistent. We use an extension of TC-Ontology, proposed by us previously, to\ndetermine the consistency of problems. Among the remaining 64%, about 40% have\nminor errors which we repair using the same ontology. To check consistency and\nfor the repair process, we construct an instance-specific representation (ABox)\nof an auto-generated problem. We use a sentence classifier and BERT models for\nthis task. The training set for these LMs is problem-texts where sentence-parts\nare annotated with ontology class-names. As three-agent problems are longer,\nthe percentage of consistent problems generated by an LSTM-based approach drops\nfurther. Hence, we propose an ontology-based method that extends consistent\n2-agent problems into consistent 3-agent problems. Overall, our approach\ngenerates a large number of consistent TC-type AWPs involving 2 or 3 agents. As\nABox has all the information of a problem, any annotations can also be\ngenerated. Adopting the proposed approach to generate other types of AWPs is\ninteresting future work.",
        "pdf_link": "https://arxiv.org/pdf/2211.12164v1.pdf"
    },
    {
        "title": "Coreference Resolution through a seq2seq Transition-Based System",
        "authors": [
            "Bernd Bohnet",
            "Chris Alberti",
            "Michael Collins"
        ],
        "published": "2022-11-22T10:17:50Z",
        "summary": "Most recent coreference resolution systems use search algorithms over\npossible spans to identify mentions and resolve coreference. We instead present\na coreference resolution system that uses a text-to-text (seq2seq) paradigm to\npredict mentions and links jointly. We implement the coreference system as a\ntransition system and use multilingual T5 as an underlying language model. We\nobtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score\nfor English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))\nusing only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than\nprevious work) and 74.3 F1-score for Chinese (+5.3). In addition we use the\nSemEval-2010 data sets for experiments in the zero-shot setting, a few-shot\nsetting, and supervised setting using all available training data. We get\nsubstantially higher zero-shot F1-scores for 3 out of 4 languages than previous\napproaches and significantly exceed previous supervised state-of-the-art\nresults for all five tested languages.",
        "pdf_link": "https://arxiv.org/pdf/2211.12142v1.pdf"
    },
    {
        "title": "Converge to the Truth: Factual Error Correction via Iterative Constrained Editing",
        "authors": [
            "Jiangjie Chen",
            "Rui Xu",
            "Wenxuan Zeng",
            "Changzhi Sun",
            "Lei Li",
            "Yanghua Xiao"
        ],
        "published": "2022-11-22T10:03:13Z",
        "summary": "Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.12130v3.pdf"
    },
    {
        "title": "Visually Grounded Commonsense Knowledge Acquisition",
        "authors": [
            "Yuan Yao",
            "Tianyu Yu",
            "Ao Zhang",
            "Mengdi Li",
            "Ruobing Xie",
            "Cornelius Weber",
            "Zhiyuan Liu",
            "Hai-Tao Zheng",
            "Stefan Wermter",
            "Tat-Seng Chua",
            "Maosong Sun"
        ],
        "published": "2022-11-22T07:00:16Z",
        "summary": "Large-scale commonsense knowledge bases empower a broad range of AI\napplications, where the automatic extraction of commonsense knowledge (CKE) is\na fundamental and challenging problem. CKE from text is known for suffering\nfrom the inherent sparsity and reporting bias of commonsense in text. Visual\nperception, on the other hand, contains rich commonsense knowledge about\nreal-world entities, e.g., (person, can_hold, bottle), which can serve as\npromising sources for acquiring grounded commonsense knowledge. In this work,\nwe present CLEVER, which formulates CKE as a distantly supervised\nmulti-instance learning problem, where models learn to summarize commonsense\nrelations from a bag of images about an entity pair without any human\nannotation on image instances. To address the problem, CLEVER leverages\nvision-language pre-training models for deep understanding of each image in the\nbag, and selects informative instances from the bag to summarize commonsense\nentity relations via a novel contrastive attention mechanism. Comprehensive\nexperimental results in held-out and human evaluation show that CLEVER can\nextract commonsense knowledge in promising quality, outperforming pre-trained\nlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted\ncommonsense scores show strong correlation with human judgment with a 0.78\nSpearman coefficient. Moreover, the extracted commonsense can also be grounded\ninto images with reasonable interpretability. The data and codes can be\nobtained at https://github.com/thunlp/CLEVER.",
        "pdf_link": "https://arxiv.org/pdf/2211.12054v2.pdf"
    },
    {
        "title": "Knowledge Prompting for Few-shot Action Recognition",
        "authors": [
            "Yuheng Shi",
            "Xinxiao Wu",
            "Hanxi Lin"
        ],
        "published": "2022-11-22T06:05:17Z",
        "summary": "Few-shot action recognition in videos is challenging for its lack of\nsupervision and difficulty in generalizing to unseen actions. To address this\ntask, we propose a simple yet effective method, called knowledge prompting,\nwhich leverages commonsense knowledge of actions from external resources to\nprompt a powerful pre-trained vision-language model for few-shot\nclassification. We first collect large-scale language descriptions of actions,\ndefined as text proposals, to build an action knowledge base. The collection of\ntext proposals is done by filling in handcraft sentence templates with external\naction-related corpus or by extracting action-related phrases from captions of\nWeb instruction videos.Then we feed these text proposals into the pre-trained\nvision-language model along with video frames to generate matching scores of\nthe proposals to each frame, and the scores can be treated as action semantics\nwith strong generalization. Finally, we design a lightweight temporal modeling\nnetwork to capture the temporal evolution of action semantics for\nclassification.Extensive experiments on six benchmark datasets demonstrate that\nour method generally achieves the state-of-the-art performance while reducing\nthe training overhead to 0.001 of existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.12030v1.pdf"
    },
    {
        "title": "TEMPERA: Test-Time Prompting via Reinforcement Learning",
        "authors": [
            "Tianjun Zhang",
            "Xuezhi Wang",
            "Denny Zhou",
            "Dale Schuurmans",
            "Joseph E. Gonzalez"
        ],
        "published": "2022-11-21T22:38:20Z",
        "summary": "Careful prompt design is critical to the use of large language models in\nzero-shot or few-shot learning. As a consequence, there is a growing interest\nin automated methods to design optimal prompts. In this work, we propose\nTest-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to\nprior prompt generation methods, TEMPERA can efficiently leverage prior\nknowledge, is adaptive to different queries and provides an interpretable\nprompt for every query. To achieve this, we design a novel action space that\nallows flexible editing of the initial prompts covering a wide set of\ncommonly-used components like instructions, few-shot exemplars, and\nverbalizers. The proposed method achieves significant gains compared with\nrecent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a\nvariety of tasks including sentiment analysis, topic classification, natural\nlanguage inference, and reading comprehension. Our method achieves 5.33x on\naverage improvement in sample efficiency when compared to the traditional\nfine-tuning methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.11890v1.pdf"
    },
    {
        "title": "Validating Large Language Models with ReLM",
        "authors": [
            "Michael Kuchnik",
            "Virginia Smith",
            "George Amvrosiadis"
        ],
        "published": "2022-11-21T21:40:35Z",
        "summary": "Although large language models (LLMs) have been touted for their ability to\ngenerate natural-sounding text, there are growing concerns around possible\nnegative effects of LLMs such as data memorization, bias, and inappropriate\nlanguage. Unfortunately, the complexity and generation capacities of LLMs make\nvalidating (and correcting) such concerns difficult. In this work, we introduce\nReLM, a system for validating and querying LLMs using standard regular\nexpressions. ReLM formalizes and enables a broad range of language model\nevaluations, reducing complex evaluation rules to simple regular expression\nqueries. Our results exploring queries surrounding memorization, gender bias,\ntoxicity, and language understanding show that ReLM achieves up to 15x higher\nsystem efficiency, 2.5x data efficiency, and increased statistical and\nprompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers\na competitive and general baseline for the increasingly important problem of\nLLM validation.",
        "pdf_link": "https://arxiv.org/pdf/2211.15458v2.pdf"
    },
    {
        "title": "PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning",
        "authors": [
            "Xiangyang Zhu",
            "Renrui Zhang",
            "Bowei He",
            "Ziyu Guo",
            "Ziyao Zeng",
            "Zipeng Qin",
            "Shanghang Zhang",
            "Peng Gao"
        ],
        "published": "2022-11-21T17:52:43Z",
        "summary": "Large-scale pre-trained models have shown promising open-world performance\nfor both vision and language tasks. However, their transferred capacity on 3D\npoint clouds is still limited and only constrained to the classification task.\nIn this paper, we first collaborate CLIP and GPT to be a unified 3D open-world\nlearner, named as PointCLIP V2, which fully unleashes their potential for\nzero-shot 3D classification, segmentation, and detection. To better align 3D\ndata with the pre-trained language knowledge, PointCLIP V2 contains two key\ndesigns. For the visual end, we prompt CLIP via a shape projection module to\ngenerate more realistic depth maps, narrowing the domain gap between projected\npoint clouds with natural images. For the textual end, we prompt the GPT model\nto generate 3D-specific text as the input of CLIP's textual encoder. Without\nany training in 3D domains, our approach significantly surpasses PointCLIP by\n+42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D\nclassification. On top of that, V2 can be extended to few-shot 3D\nclassification, zero-shot 3D part segmentation, and 3D object detection in a\nsimple manner, demonstrating our generalization ability for unified 3D\nopen-world learning.",
        "pdf_link": "https://arxiv.org/pdf/2211.11682v2.pdf"
    },
    {
        "title": "Model-based Trajectory Stitching for Improved Offline Reinforcement Learning",
        "authors": [
            "Charles A. Hepburn",
            "Giovanni Montana"
        ],
        "published": "2022-11-21T16:00:39Z",
        "summary": "In many real-world applications, collecting large and high-quality datasets\nmay be too costly or impractical. Offline reinforcement learning (RL) aims to\ninfer an optimal decision-making policy from a fixed set of data. Getting the\nmost information from historical data is then vital for good performance once\nthe policy is deployed. We propose a model-based data augmentation strategy,\nTrajectory Stitching (TS), to improve the quality of sub-optimal historical\ntrajectories. TS introduces unseen actions joining previously disconnected\nstates: using a probabilistic notion of state reachability, it effectively\n`stitches' together parts of the historical demonstrations to generate new,\nhigher quality ones. A stitching event consists of a transition between a pair\nof observed states through a synthetic and highly probable action. New actions\nare introduced only when they are expected to be beneficial, according to an\nestimated state-value function. We show that using this data augmentation\nstrategy jointly with behavioural cloning (BC) leads to improvements over the\nbehaviour-cloned policy from the original dataset. Improving over the BC policy\ncould then be used as a launchpad for online RL through planning and\ndemonstration-guided RL.",
        "pdf_link": "https://arxiv.org/pdf/2211.11603v1.pdf"
    },
    {
        "title": "ClipCrop: Conditioned Cropping Driven by Vision-Language Model",
        "authors": [
            "Zhihang Zhong",
            "Mingxi Cheng",
            "Zhirong Wu",
            "Yuhui Yuan",
            "Yinqiang Zheng",
            "Ji Li",
            "Han Hu",
            "Stephen Lin",
            "Yoichi Sato",
            "Imari Sato"
        ],
        "published": "2022-11-21T14:27:07Z",
        "summary": "Image cropping has progressed tremendously under the data-driven paradigm.\nHowever, current approaches do not account for the intentions of the user,\nwhich is an issue especially when the composition of the input image is\ncomplex. Moreover, labeling of cropping data is costly and hence the amount of\ndata is limited, leading to poor generalization performance of current\nalgorithms in the wild. In this work, we take advantage of vision-language\nmodels as a foundation for creating robust and user-intentional cropping\nalgorithms. By adapting a transformer decoder with a pre-trained CLIP-based\ndetection model, OWL-ViT, we develop a method to perform cropping with a text\nor image query that reflects the user's intention as guidance. In addition, our\npipeline design allows the model to learn text-conditioned aesthetic cropping\nwith a small cropping dataset, while inheriting the open-vocabulary ability\nacquired from millions of text-image pairs. We validate our model through\nextensive experiments on existing datasets as well as a new cropping test set\nwe compiled that is characterized by content ambiguity.",
        "pdf_link": "https://arxiv.org/pdf/2211.11492v1.pdf"
    },
    {
        "title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?",
        "authors": [
            "Matthew Shardlow",
            "Piotr Przyby\u0142a"
        ],
        "published": "2022-11-21T14:18:25Z",
        "summary": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.",
        "pdf_link": "https://arxiv.org/pdf/2211.11483v4.pdf"
    },
    {
        "title": "L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages",
        "authors": [
            "Raviraj Joshi"
        ],
        "published": "2022-11-21T13:02:52Z",
        "summary": "The monolingual Hindi BERT models currently available on the model hub do not\nperform better than the multi-lingual models on downstream tasks. We present\nL3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.\nFurther, since Indic languages, Hindi and Marathi share the Devanagari script,\nwe train a single model for both languages. We release DevBERT, a Devanagari\nBERT model trained on both Marathi and Hindi monolingual datasets. We evaluate\nthese models on downstream Hindi and Marathi text classification and named\nentity recognition tasks. The HindBERT and DevBERT-based models show\nsignificant improvements over multi-lingual MuRIL, IndicBERT, and XLM-R. Based\non these observations we also release monolingual BERT models for other Indic\nlanguages Kannada, Telugu, Malayalam, Tamil, Gujarati, Assamese, Odia, Bengali,\nand Punjabi. These models are shared at https://huggingface.co/l3cube-pune .",
        "pdf_link": "https://arxiv.org/pdf/2211.11418v4.pdf"
    },
    {
        "title": "AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model",
        "authors": [
            "Yongyu Yan",
            "Kui Xue",
            "Xiaoming Shi",
            "Qi Ye",
            "Jingping Liu",
            "Tong Ruan"
        ],
        "published": "2022-11-21T11:30:13Z",
        "summary": "Continual pretraining is a popular way of building a domain-specific\npretrained language model from a general-domain language model. In spite of its\nhigh efficiency, continual pretraining suffers from catastrophic forgetting,\nwhich may harm the model's performance in downstream tasks. To alleviate the\nissue, in this paper, we propose a continual pretraining method for the\nBERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a\nsmall number of attention heads and hidden units inside each self-attention\nlayer and feed-forward network. Furthermore, we train a domain-specific\nlanguage model named AF Adapter based RoBERTa for the Chinese biomedical\ndomain. In experiments, models are applied to downstream tasks for evaluation.\nThe results demonstrate that with only about 17% of model parameters trained,\nAF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong\nbaselines. Further experimental results show that our method alleviates the\ncatastrophic forgetting problem by 11% compared to the fine-tuning method.",
        "pdf_link": "https://arxiv.org/pdf/2211.11363v2.pdf"
    },
    {
        "title": "TCBERT: A Technical Report for Chinese Topic Classification BERT",
        "authors": [
            "Ting Han",
            "Kunhao Pan",
            "Xinyu Chen",
            "Dingjie Song",
            "Yuchen Fan",
            "Xinyu Gao",
            "Ruyi Gan",
            "Jiaxing Zhang"
        ],
        "published": "2022-11-21T09:45:15Z",
        "summary": "Bidirectional Encoder Representations from Transformers or\nBERT~\\cite{devlin-etal-2019-bert} has been one of the base models for various\nNLP tasks due to its remarkable performance. Variants customized for different\nlanguages and tasks are proposed to further improve the performance. In this\nwork, we investigate supervised continued\npre-training~\\cite{gururangan-etal-2020-dont} on BERT for Chinese topic\nclassification task. Specifically, we incorporate prompt-based learning and\ncontrastive learning into the pre-training. To adapt to the task of Chinese\ntopic classification, we collect around 2.1M Chinese data spanning various\ntopics. The pre-trained Chinese Topic Classification BERTs (TCBERTs) with\ndifferent parameter sizes are open-sourced at\n\\url{https://huggingface.co/IDEA-CCNL}.",
        "pdf_link": "https://arxiv.org/pdf/2211.11304v1.pdf"
    },
    {
        "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "authors": [
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Haonan Yin",
            "B\u00f6rje F. Karlsson",
            "Chin-Yew Lin"
        ],
        "published": "2022-11-21T09:41:25Z",
        "summary": "Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "pdf_link": "https://arxiv.org/pdf/2211.11300v3.pdf"
    },
    {
        "title": "Task-Specific Data Augmentation and Inference Processing for VIPriors Instance Segmentation Challenge",
        "authors": [
            "Bo Yan",
            "Xingran Zhao",
            "Yadong Li",
            "Hongbin Wang"
        ],
        "published": "2022-11-21T09:15:30Z",
        "summary": "Instance segmentation is applied widely in image editing, image analysis and\nautonomous driving, etc. However, insufficient data is a common problem in\npractical applications. The Visual Inductive Priors(VIPriors) Instance\nSegmentation Challenge has focused on this problem. VIPriors for Data-Efficient\nComputer Vision Challenges ask competitors to train models from scratch in a\ndata-deficient setting, but there are some visual inductive priors that can be\nused. In order to address the VIPriors instance segmentation problem, we\ndesigned a Task-Specific Data Augmentation(TS-DA) strategy and Inference\nProcessing(TS-IP) strategy. The main purpose of task-specific data augmentation\nstrategy is to tackle the data-deficient problem. And in order to make the most\nof visual inductive priors, we designed a task-specific inference processing\nstrategy. We demonstrate the applicability of proposed method on VIPriors\nInstance Segmentation Challenge. The segmentation model applied is Hybrid Task\nCascade based detector on the Swin-Base based CBNetV2 backbone. Experimental\nresults demonstrate that proposed method can achieve a competitive result on\nthe test set of 2022 VIPriors Instance Segmentation Challenge, with 0.531\nAP@0.50:0.95.",
        "pdf_link": "https://arxiv.org/pdf/2211.11282v1.pdf"
    },
    {
        "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
        "authors": [
            "Qiushi Zhu",
            "Long Zhou",
            "Ziqiang Zhang",
            "Shujie Liu",
            "Binxing Jiao",
            "Jie Zhang",
            "Lirong Dai",
            "Daxin Jiang",
            "Jinyu Li",
            "Furu Wei"
        ],
        "published": "2022-11-21T09:10:10Z",
        "summary": "Although speech is a simple and effective way for humans to communicate with\nthe outside world, a more realistic speech interaction contains multimodal\ninformation, e.g., vision, text. How to design a unified framework to integrate\ndifferent modal information and leverage different resources (e.g.,\nvisual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to\nfacilitate speech representation learning was not well explored. In this paper,\nwe propose a unified cross-modal representation learning framework VATLM\n(Visual-Audio-Text Language Model). The proposed VATLM employs a unified\nbackbone network to model the modality-independent information and utilizes\nthree simple modality-dependent modules to preprocess visual, speech, and text\ninputs. In order to integrate these three modalities into one shared semantic\nspace, VATLM is optimized with a masked prediction task of unified tokens,\ngiven by our proposed unified tokenizer. We evaluate the pre-trained VATLM on\naudio-visual related downstream tasks, including audio-visual speech\nrecognition (AVSR), visual speech recognition (VSR) tasks. Results show that\nthe proposed VATLM outperforms previous the state-of-the-art models, such as\naudio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that\nVATLM is capable of aligning different modalities into the same space. To\nfacilitate future research, we release the code and pre-trained models at\nhttps://aka.ms/vatlm.",
        "pdf_link": "https://arxiv.org/pdf/2211.11275v2.pdf"
    },
    {
        "title": "Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task",
        "authors": [
            "Shangda Wu",
            "Maosong Sun"
        ],
        "published": "2022-11-21T07:19:17Z",
        "summary": "Benefiting from large-scale datasets and pre-trained models, the field of\ngenerative models has recently gained significant momentum. However, most\ndatasets for symbolic music are very small, which potentially limits the\nperformance of data-driven multimodal models. An intuitive solution to this\nproblem is to leverage pre-trained models from other modalities (e.g., natural\nlanguage) to improve the performance of symbolic music-related multimodal\ntasks. In this paper, we carry out the first study of generating complete and\nsemantically consistent symbolic music scores from text descriptions, and\nexplore the efficacy of using publicly available checkpoints (i.e., BERT,\nGPT-2, and BART) for natural language processing in the task of text-to-music\ngeneration. Our experimental results show that the improvement from using\npre-trained checkpoints is statistically significant in terms of BLEU score and\nedit distance similarity. We analyse the capabilities and limitations of our\nmodel to better understand the potential of language-music models.",
        "pdf_link": "https://arxiv.org/pdf/2211.11216v2.pdf"
    },
    {
        "title": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi",
        "authors": [
            "Ananya Joshi",
            "Aditi Kajale",
            "Janhavi Gadre",
            "Samruddhi Deode",
            "Raviraj Joshi"
        ],
        "published": "2022-11-21T05:15:48Z",
        "summary": "Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.",
        "pdf_link": "https://arxiv.org/pdf/2211.11187v2.pdf"
    },
    {
        "title": "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification",
        "authors": [
            "Yue Yang",
            "Artemis Panagopoulou",
            "Shenghao Zhou",
            "Daniel Jin",
            "Chris Callison-Burch",
            "Mark Yatskar"
        ],
        "published": "2022-11-21T03:05:02Z",
        "summary": "Concept Bottleneck Models (CBM) are inherently interpretable models that\nfactor model decisions into human-readable concepts. They allow people to\neasily understand why a model is failing, a critical feature for high-stakes\napplications. CBMs require manually specified concepts and often under-perform\ntheir black box counterparts, preventing their broad adoption. We address these\nshortcomings and are first to show how to construct high-performance CBMs\nwithout manual specification of similar accuracy to black box models. Our\napproach, Language Guided Bottlenecks (LaBo), leverages a language model,\nGPT-3, to define a large space of possible bottlenecks. Given a problem domain,\nLaBo uses GPT-3 to produce factual sentences about categories to form candidate\nconcepts. LaBo efficiently searches possible bottlenecks through a novel\nsubmodular utility that promotes the selection of discriminative and diverse\ninformation. Ultimately, GPT-3's sentential concepts can be aligned to images\nusing CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a\nhighly effective prior for concepts important to visual recognition. In the\nevaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot\nclassification: they are 11.7% more accurate than black box linear probes at 1\nshot and comparable with more data. Overall, LaBo demonstrates that inherently\ninterpretable models can be widely applied at similar, or better, performance\nthan black box approaches.",
        "pdf_link": "https://arxiv.org/pdf/2211.11158v2.pdf"
    },
    {
        "title": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model",
        "authors": [
            "Shengkun Tang",
            "Yaqing Wang",
            "Zhenglun Kong",
            "Tianchi Zhang",
            "Yao Li",
            "Caiwen Ding",
            "Yanzhi Wang",
            "Yi Liang",
            "Dongkuan Xu"
        ],
        "published": "2022-11-21T02:32:25Z",
        "summary": "Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.11152v2.pdf"
    },
    {
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": [
            "Li S. Yifei",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc"
        ],
        "published": "2022-11-20T21:24:48Z",
        "summary": "Pre-trained large language models (LLMs) reflect the inherent social biases\nof their training corpus. Many methods have been proposed to mitigate this\nissue, but they often fail to debias or they sacrifice model accuracy. We use\nconceptors--a soft projection method--to identify and remove the bias subspace\nin LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)\nbias subspace projection by post-processing by the conceptor NOT operation; and\n(2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly\nincorporates the conceptor projection into all layers during training. We find\nthat conceptor post-processing achieves state-of-the-art (SoTA) debiasing\nresults while maintaining LLMs' performance on the GLUE benchmark. Further, it\nis robust in various scenarios and can mitigate intersectional bias efficiently\nby its AND operation on the existing bias subspaces. Although CI-BERT's\ntraining takes all layers' bias into account and can beat its post-processing\ncounterpart in bias mitigation, CI-BERT reduces the language model accuracy. We\nalso show the importance of carefully constructing the bias subspace. The best\nresults are obtained by removing outliers from the list of biased words,\ncombining them (via the OR operation), and computing their embeddings using the\nsentences from a cleaner corpus.",
        "pdf_link": "https://arxiv.org/pdf/2211.11087v3.pdf"
    },
    {
        "title": "The Stack: 3 TB of permissively licensed source code",
        "authors": [
            "Denis Kocetkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Jia Li",
            "Chenghao Mou",
            "Carlos Mu\u00f1oz Ferrandis",
            "Yacine Jernite",
            "Margaret Mitchell",
            "Sean Hughes",
            "Thomas Wolf",
            "Dzmitry Bahdanau",
            "Leandro von Werra",
            "Harm de Vries"
        ],
        "published": "2022-11-20T18:15:30Z",
        "summary": "Large Language Models (LLMs) play an ever-increasing role in the field of\nArtificial Intelligence (AI)--not only for natural language processing but also\nfor code understanding and generation. To stimulate open and responsible\nresearch on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting\nof permissively licensed source code in 30 programming languages. We describe\nhow we collect the full dataset, construct a permissively licensed subset,\npresent a data governance plan, discuss limitations, and show promising results\non text2code benchmarks by training 350M-parameter decoders on different Python\nsubsets. We find that (1) near-deduplicating the data significantly boosts\nperformance across all experiments, and (2) it is possible to match previously\nreported HumanEval and MBPP performance using only permissively licensed data.\nWe make the dataset available at https://hf.co/BigCode, provide a tool called\n\"Am I in The Stack\" (https://hf.co/spaces/bigcode/in-the-stack) for developers\nto search The Stack for copies of their code, and provide a process for code to\nbe removed from the dataset by following the instructions at\nhttps://www.bigcode-project.org/docs/about/the-stack/.",
        "pdf_link": "https://arxiv.org/pdf/2211.15533v1.pdf"
    },
    {
        "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
        "authors": [
            "Thomas Hartvigsen",
            "Swami Sankaranarayanan",
            "Hamid Palangi",
            "Yoon Kim",
            "Marzyeh Ghassemi"
        ],
        "published": "2022-11-20T17:18:22Z",
        "summary": "Deployed language models decay over time due to shifting inputs, changing\nuser needs, or emergent world-knowledge gaps. When such problems are\nidentified, we want to make targeted edits while avoiding expensive retraining.\nHowever, current model editors, which modify such behaviors of pre-trained\nmodels, degrade model performance quickly across multiple, sequential edits. We\npropose GRACE, a lifelong model editing method, which implements spot-fixes on\nstreaming errors of a deployed model, ensuring minimal impact on unrelated\ninputs. GRACE writes new mappings into a pre-trained model's latent space,\ncreating a discrete, local codebook of edits without altering model weights.\nThis is the first method enabling thousands of sequential edits using only\nstreaming errors. Our experiments on T5, BERT, and GPT models show GRACE's\nstate-of-the-art performance in making and retaining edits, while generalizing\nto unseen inputs. Our code is available at\nhttps://www.github.com/thartvigsen/grace}.",
        "pdf_link": "https://arxiv.org/pdf/2211.11031v5.pdf"
    },
    {
        "title": "Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders",
        "authors": [
            "Minsoo Kim",
            "Sihwa Lee",
            "Sukjin Hong",
            "Du-Seong Chang",
            "Jungwook Choi"
        ],
        "published": "2022-11-20T16:23:23Z",
        "summary": "Knowledge distillation (KD) has been a ubiquitous method for model\ncompression to strengthen the capability of a lightweight model with the\ntransferred knowledge from the teacher. In particular, KD has been employed in\nquantization-aware training (QAT) of Transformer encoders like BERT to improve\nthe accuracy of the student model with the reduced-precision weight parameters.\nHowever, little is understood about which of the various KD approaches best\nfits the QAT of Transformers. In this work, we provide an in-depth analysis of\nthe mechanism of KD on attention recovery of quantized large Transformers. In\nparticular, we reveal that the previously adopted MSE loss on the attention\nscore is insufficient for recovering the self-attention information. Therefore,\nwe propose two KD methods; attention-map and attention-output losses.\nFurthermore, we explore the unification of both losses to address\ntask-dependent preference between attention-map and output losses. The\nexperimental results on various Transformer encoder models demonstrate that the\nproposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit\nweight quantization.",
        "pdf_link": "https://arxiv.org/pdf/2211.11014v1.pdf"
    },
    {
        "title": "Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval",
        "authors": [
            "Taiqiang Wu",
            "Xingyu Bai",
            "Weigang Guo",
            "Weijie Liu",
            "Siheng Li",
            "Yujiu Yang"
        ],
        "published": "2022-11-20T14:37:53Z",
        "summary": "Zero-shot entity retrieval, aiming to link mentions to candidate entities\nunder the zero-shot setting, is vital for many tasks in Natural Language\nProcessing. Most existing methods represent mentions/entities via the sentence\nembeddings of corresponding context from the Pre-trained Language Model.\nHowever, we argue that such coarse-grained sentence embeddings can not fully\nmodel the mentions/entities, especially when the attention scores towards\nmentions/entities are relatively low. In this work, we propose GER, a\n\\textbf{G}raph enhanced \\textbf{E}ntity \\textbf{R}etrieval framework, to\ncapture more fine-grained information as complementary to sentence embeddings.\nWe extract the knowledge units from the corresponding context and then\nconstruct a mention/entity centralized graph. Hence, we can learn the\nfine-grained information about mention/entity by aggregating information from\nthese knowledge units. To avoid the graph information bottleneck for the\ncentral mention/entity node, we construct a hierarchical graph and design a\nnovel Hierarchical Graph Attention Network~(HGAN). Experimental results on\npopular benchmarks demonstrate that our proposed GER framework performs better\nthan previous state-of-the-art models. The code has been available at\nhttps://github.com/wutaiqiang/GER-WSDM2023.",
        "pdf_link": "https://arxiv.org/pdf/2211.10991v1.pdf"
    },
    {
        "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-training",
        "authors": [
            "Yunhao Gou",
            "Tom Ko",
            "Hansi Yang",
            "James Kwok",
            "Yu Zhang",
            "Mingxuan Wang"
        ],
        "published": "2022-11-20T12:10:53Z",
        "summary": "Most existing vision-language pre-training (VLP) approaches adopt cross-modal\nmasked language modeling (CMLM) to learn vision-language associations. However,\nwe find that CMLM is insufficient for this purpose according to our\nobservations: (1) Modality bias: a considerable amount of masked tokens in CMLM\ncan be recovered with only the language information, ignoring the visual\ninputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on\nthe masked token but it cannot simultaneously leverage other tokens to learn\nvision-language associations. To handle those limitations, we propose EPIC\n(lEveraging Per Image-Token Consistency for vision-language pre-training). In\nEPIC, for each image-sentence pair, we mask tokens that are salient to the\nimage (i.e., Saliency-based Masking Strategy) and replace them with\nalternatives sampled from a language model (i.e., Inconsistent Token Generation\nProcedure), and then the model is required to determine for each token in the\nsentence whether it is consistent with the image (i.e., Image-Token Consistency\nTask). The proposed EPIC method is easily combined with pre-training methods.\nExtensive experiments show that the combination of the EPIC method and\nstate-of-the-art pre-training approaches, including ViLT, ALBEF, METER, and\nX-VLM, leads to significant improvements on downstream tasks. The code is\nreleased at https://github.com/gyhdog99/epic.",
        "pdf_link": "https://arxiv.org/pdf/2211.15398v2.pdf"
    },
    {
        "title": "Feature Weaken: Vicinal Data Augmentation for Classification",
        "authors": [
            "Songhao Jiang",
            "Yan Chu",
            "Tianxing Ma",
            "Tianning Zang"
        ],
        "published": "2022-11-20T11:00:23Z",
        "summary": "Deep learning usually relies on training large-scale data samples to achieve\nbetter performance. However, over-fitting based on training data always remains\na problem. Scholars have proposed various strategies, such as feature dropping\nand feature mixing, to improve the generalization continuously. For the same\npurpose, we subversively propose a novel training method, Feature Weaken, which\ncan be regarded as a data augmentation method. Feature Weaken constructs the\nvicinal data distribution with the same cosine similarity for model training by\nweakening features of the original samples. In especially, Feature Weaken\nchanges the spatial distribution of samples, adjusts sample boundaries, and\nreduces the gradient optimization value of back-propagation. This work can not\nonly improve the classification performance and generalization of the model,\nbut also stabilize the model training and accelerate the model convergence. We\nconduct extensive experiments on classical deep convolution neural models with\nfive common image classification datasets and the Bert model with four common\ntext classification datasets. Compared with the classical models or the\ngeneralization improvement methods, such as Dropout, Mixup, Cutout, and CutMix,\nFeature Weaken shows good compatibility and performance. We also use\nadversarial samples to perform the robustness experiments, and the results show\nthat Feature Weaken is effective in improving the robustness of the model.",
        "pdf_link": "https://arxiv.org/pdf/2211.10944v1.pdf"
    },
    {
        "title": "Detecting Conspiracy Theory Against COVID-19 Vaccines",
        "authors": [
            "Md Hasibul Amin",
            "Harika Madanu",
            "Sahithi Lavu",
            "Hadi Mansourifar",
            "Dana Alsagheer",
            "Weidong Shi"
        ],
        "published": "2022-11-20T04:59:33Z",
        "summary": "Since the beginning of the vaccination trial, social media has been flooded\nwith anti-vaccination comments and conspiracy beliefs. As the day passes, the\nnumber of COVID- 19 cases increases, and online platforms and a few news\nportals entertain sharing different conspiracy theories. The most popular\nconspiracy belief was the link between the 5G network spreading COVID-19 and\nthe Chinese government spreading the virus as a bioweapon, which initially\ncreated racial hatred. Although some disbelief has less impact on society,\nothers create massive destruction. For example, the 5G conspiracy led to the\nburn of the 5G Tower, and belief in the Chinese bioweapon story promoted an\nattack on the Asian-Americans. Another popular conspiracy belief was that Bill\nGates spread this Coronavirus disease (COVID-19) by launching a mass\nvaccination program to track everyone. This Conspiracy belief creates distrust\nissues among laypeople and creates vaccine hesitancy. This study aims to\ndiscover the conspiracy theory against the vaccine on social platforms. We\nperformed a sentiment analysis on the 598 unique sample comments related to\nCOVID-19 vaccines. We used two different models, BERT and Perspective API, to\nfind out the sentiment and toxicity of the sentence toward the COVID-19\nvaccine.",
        "pdf_link": "https://arxiv.org/pdf/2211.13003v1.pdf"
    },
    {
        "title": "Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure",
        "authors": [
            "Ke Liang",
            "Yue Liu",
            "Sihang Zhou",
            "Wenxuan Tu",
            "Yi Wen",
            "Xihong Yang",
            "Xiangjun Dong",
            "Xinwang Liu"
        ],
        "published": "2022-11-19T16:30:29Z",
        "summary": "Knowledge graph embedding (KGE) aims at learning powerful representations to\nbenefit various artificial intelligence applications. Meanwhile, contrastive\nlearning has been widely leveraged in graph learning as an effective mechanism\nto enhance the discriminative capacity of the learned representations. However,\nthe complex structures of KG make it hard to construct appropriate contrastive\npairs. Only a few attempts have integrated contrastive learning strategies with\nKGE. But, most of them rely on language models ( e.g., Bert) for contrastive\npair construction instead of fully mining information underlying the graph\nstructure, hindering expressive ability. Surprisingly, we find that the\nentities within a relational symmetrical structure are usually similar and\ncorrelated. To this end, we propose a knowledge graph contrastive learning\nframework based on relation-symmetrical structure, KGE-SymCL, which mines\nsymmetrical structure information in KGs to enhance the discriminative ability\nof KGE models. Concretely, a plug-and-play approach is proposed by taking\nentities in the relation-symmetrical positions as positive pairs. Besides, a\nself-supervised alignment loss is designed to pull together positive pairs.\nExperimental results on link prediction and entity classification datasets\ndemonstrate that our KGE-SymCL can be easily adopted to various KGE models for\nperformance improvements. Moreover, extensive experiments show that our model\ncould outperform other state-of-the-art baselines.",
        "pdf_link": "https://arxiv.org/pdf/2211.10738v4.pdf"
    },
    {
        "title": "Entity-Assisted Language Models for Identifying Check-worthy Sentences",
        "authors": [
            "Ting Su",
            "Craig Macdonald",
            "Iadh Ounis"
        ],
        "published": "2022-11-19T12:03:30Z",
        "summary": "We propose a new uniform framework for text classification and ranking that\ncan automate the process of identifying check-worthy sentences in political\ndebates and speech transcripts. Our framework combines the semantic analysis of\nthe sentences, with additional entity embeddings obtained through the\nidentified entities within the sentences. In particular, we analyse the\nsemantic meaning of each sentence using state-of-the-art neural language models\nsuch as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained\nfrom knowledge graph (KG) embedding models. Specifically, we instantiate our\nframework using five different language models, entity embeddings obtained from\nsix different KG embedding models, as well as two combination methods leading\nto several Entity-Assisted neural language models. We extensively evaluate the\neffectiveness of our framework using two publicly available datasets from the\nCLEF' 2019 & 2020 CheckThat! Labs. Our results show that the neural language\nmodels significantly outperform traditional TF.IDF and LSTM methods. In\naddition, we show that the ALBERT model is consistently the most effective\nmodel among all the tested neural language models. Our entity embeddings\nsignificantly outperform other existing approaches from the literature that are\nbased on similarity and relatedness scores between the entities in a sentence,\nwhen used alongside a KG embedding.",
        "pdf_link": "https://arxiv.org/pdf/2211.10678v1.pdf"
    },
    {
        "title": "ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting",
        "authors": [
            "Shancheng Fang",
            "Zhendong Mao",
            "Hongtao Xie",
            "Yuxin Wang",
            "Chenggang Yan",
            "Yongdong Zhang"
        ],
        "published": "2022-11-19T03:50:33Z",
        "summary": "Scene text spotting is of great importance to the computer vision community\ndue to its wide variety of applications. Recent methods attempt to introduce\nlinguistic knowledge for challenging recognition rather than pure visual\nclassification. However, how to effectively model the linguistic rules in\nend-to-end deep networks remains a research challenge. In this paper, we argue\nthat the limited capacity of language models comes from 1) implicit language\nmodeling; 2) unidirectional feature representation; and 3) language model with\nnoise input. Correspondingly, we propose an autonomous, bidirectional and\niterative ABINet++ for scene text spotting. Firstly, the autonomous suggests\nenforcing explicitly language modeling by decoupling the recognizer into vision\nmodel and language model and blocking gradient flow between both models.\nSecondly, a novel bidirectional cloze network (BCN) as the language model is\nproposed based on bidirectional feature representation. Thirdly, we propose an\nexecution manner of iterative correction for the language model which can\neffectively alleviate the impact of noise input. Finally, to polish ABINet++ in\nlong text recognition, we propose to aggregate horizontal features by embedding\nTransformer units inside a U-Net, and design a position and content attention\nmodule which integrates character order and content to attend to character\nfeatures precisely. ABINet++ achieves state-of-the-art performance on both\nscene text recognition and scene text spotting benchmarks, which consistently\ndemonstrates the superiority of our method in various environments especially\non low-quality images. Besides, extensive experiments including in English and\nChinese also prove that, a text spotter that incorporates our language modeling\nmethod can significantly improve its performance both in accuracy and speed\ncompared with commonly used attention-based recognizers.",
        "pdf_link": "https://arxiv.org/pdf/2211.10578v2.pdf"
    },
    {
        "title": "Knowledge Graph Generation From Text",
        "authors": [
            "Igor Melnyk",
            "Pierre Dognin",
            "Payel Das"
        ],
        "published": "2022-11-18T21:27:13Z",
        "summary": "In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)\ngeneration system from textual inputs, separating the overall process into two\nstages. The graph nodes are generated first using pretrained language model,\nfollowed by a simple edge construction head, enabling efficient KG extraction\nfrom the text. For each stage we consider several architectural choices that\ncan be used depending on the available training resources. We evaluated the\nmodel on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art\nperformance on text-to-RDF generation task, as well as on New York Times (NYT)\nand a large-scale TekGen datasets, showing strong overall performance,\noutperforming the existing baselines. We believe that the proposed system can\nserve as a viable KG construction alternative to the existing linearization or\nsampling-based graph generation approaches. Our code can be found at\nhttps://github.com/IBM/Grapher",
        "pdf_link": "https://arxiv.org/pdf/2211.10511v1.pdf"
    },
    {
        "title": "Knowledge Graph Refinement based on Triplet BERT-Networks",
        "authors": [
            "Armita Khajeh Nassiri",
            "Nathalie Pernelle",
            "Fatiha Sais",
            "Gianluca Quercini"
        ],
        "published": "2022-11-18T19:01:21Z",
        "summary": "Knowledge graph embedding techniques are widely used for knowledge graph\nrefinement tasks such as graph completion and triple classification. These\ntechniques aim at embedding the entities and relations of a Knowledge Graph\n(KG) in a low dimensional continuous feature space. This paper adopts a\ntransformer-based triplet network creating an embedding space that clusters the\ninformation about an entity or relation in the KG. It creates textual sequences\nfrom facts and fine-tunes a triplet network of pre-trained transformer-based\nlanguage models. It adheres to an evaluation paradigm that relies on an\nefficient spatial semantic search technique. We show that this evaluation\nprotocol is more adapted to a few-shot setting for the relation prediction\ntask. Our proposed GilBERT method is evaluated on triplet classification and\nrelation prediction tasks on multiple well-known benchmark knowledge graphs\nsuch as FB13, WN11, and FB15K. We show that GilBERT achieves better or\ncomparable results to the state-of-the-art performance on these two refinement\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.10460v1.pdf"
    },
    {
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
        "authors": [
            "Guangxuan Xiao",
            "Ji Lin",
            "Mickael Seznec",
            "Hao Wu",
            "Julien Demouth",
            "Song Han"
        ],
        "published": "2022-11-18T18:59:33Z",
        "summary": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.",
        "pdf_link": "https://arxiv.org/pdf/2211.10438v7.pdf"
    },
    {
        "title": "PAL: Program-aided Language Models",
        "authors": [
            "Luyu Gao",
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Pengfei Liu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2022-11-18T18:56:13Z",
        "summary": "Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .",
        "pdf_link": "https://arxiv.org/pdf/2211.10435v2.pdf"
    },
    {
        "title": "Visual Programming: Compositional visual reasoning without training",
        "authors": [
            "Tanmay Gupta",
            "Aniruddha Kembhavi"
        ],
        "published": "2022-11-18T18:50:09Z",
        "summary": "We present VISPROG, a neuro-symbolic approach to solving complex and\ncompositional visual tasks given natural language instructions. VISPROG avoids\nthe need for any task-specific training. Instead, it uses the in-context\nlearning ability of large language models to generate python-like modular\nprograms, which are then executed to get both the solution and a comprehensive\nand interpretable rationale. Each line of the generated program may invoke one\nof several off-the-shelf computer vision models, image processing routines, or\npython functions to produce intermediate outputs that may be consumed by\nsubsequent parts of the program. We demonstrate the flexibility of VISPROG on 4\ndiverse tasks - compositional visual question answering, zero-shot reasoning on\nimage pairs, factual knowledge object tagging, and language-guided image\nediting. We believe neuro-symbolic approaches like VISPROG are an exciting\navenue to easily and effectively expand the scope of AI systems to serve the\nlong tail of complex tasks that people may wish to perform.",
        "pdf_link": "https://arxiv.org/pdf/2211.11559v1.pdf"
    },
    {
        "title": "GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation",
        "authors": [
            "Biyang Guo",
            "Yeyun Gong",
            "Yelong Shen",
            "Songqiao Han",
            "Hailiang Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2022-11-18T16:39:45Z",
        "summary": "We introduce GENIUS: a conditional text generation model using sketches as\ninput, which can fill in the missing contexts for a given sketch (key\ninformation consisting of textual spans, phrases, or words, concatenated by\nmask tokens). GENIUS is pre-trained on a large-scale textual corpus with a\nnovel reconstruction from sketch objective using an extreme and selective\nmasking strategy, enabling it to generate diverse and high-quality texts given\nsketches. Comparison with other competitive conditional language models (CLMs)\nreveals the superiority of GENIUS's text generation quality. We further show\nthat GENIUS can be used as a strong and ready-to-use data augmentation tool for\nvarious natural language processing (NLP) tasks. Most existing textual data\naugmentation methods are either too conservative, by making small changes to\nthe original text, or too aggressive, by creating entirely new samples. With\nGENIUS, we propose GeniusAug, which first extracts the target-aware sketches\nfrom the original training set and then generates new samples based on the\nsketches. Empirical experiments on 6 text classification datasets show that\nGeniusAug significantly improves the models' performance in both\nin-distribution (ID) and out-of-distribution (OOD) settings. We also\ndemonstrate the effectiveness of GeniusAug on named entity recognition (NER)\nand machine reading comprehension (MRC) tasks. (Code and models are publicly\navailable at https://github.com/microsoft/SCGLab and\nhttps://github.com/beyondguo/genius)",
        "pdf_link": "https://arxiv.org/pdf/2211.10330v1.pdf"
    },
    {
        "title": "Scaling Native Language Identification with Transformer Adapters",
        "authors": [
            "Ahmet Yavuz Uluslu",
            "Gerold Schneider"
        ],
        "published": "2022-11-18T09:40:16Z",
        "summary": "Native language identification (NLI) is the task of automatically identifying\nthe native language (L1) of an individual based on their language production in\na learned language. It is useful for a variety of purposes including marketing,\nsecurity and educational applications. NLI is usually framed as a multi-label\nclassification task, where numerous designed features are combined to achieve\nstate-of-the-art results. Recently deep generative approach based on\ntransformer decoders (GPT-2) outperformed its counterparts and achieved the\nbest results on the NLI benchmark datasets. We investigate this approach to\ndetermine the practical implications compared to traditional state-of-the-art\nNLI systems. We introduce transformer adapters to address memory limitations\nand improve training/inference speed to scale NLI applications for production.",
        "pdf_link": "https://arxiv.org/pdf/2211.10117v1.pdf"
    },
    {
        "title": "Metadata Might Make Language Models Better",
        "authors": [
            "Kaspar Beelen",
            "Daniel van Strien"
        ],
        "published": "2022-11-18T08:29:00Z",
        "summary": "This paper discusses the benefits of including metadata when training\nlanguage models on historical collections. Using 19th-century newspapers as a\ncase study, we extend the time-masking approach proposed by Rosin et al., 2022\nand compare different strategies for inserting temporal, political and\ngeographical information into a Masked Language Model. After fine-tuning\nseveral DistilBERT on enhanced input data, we provide a systematic evaluation\nof these models on a set of evaluation tasks: pseudo-perplexity, metadata\nmask-filling and supervised classification. We find that showing relevant\nmetadata to a language model has a beneficial impact and may even produce more\nrobust and fairer models.",
        "pdf_link": "https://arxiv.org/pdf/2211.10086v1.pdf"
    },
    {
        "title": "3d human motion generation from the text via gesture action classification and the autoregressive model",
        "authors": [
            "Gwantae Kim",
            "Youngsuk Ryu",
            "Junyeop Lee",
            "David K. Han",
            "Jeongmin Bae",
            "Hanseok Ko"
        ],
        "published": "2022-11-18T03:05:49Z",
        "summary": "In this paper, a deep learning-based model for 3D human motion generation\nfrom the text is proposed via gesture action classification and an\nautoregressive model. The model focuses on generating special gestures that\nexpress human thinking, such as waving and nodding. To achieve the goal, the\nproposed method predicts expression from the sentences using a text\nclassification model based on a pretrained language model and generates\ngestures using the gate recurrent unit-based autoregressive model. Especially,\nwe proposed the loss for the embedding space for restoring raw motions and\ngenerating intermediate motions well. Moreover, the novel data augmentation\nmethod and stop token are proposed to generate variable length motions. To\nevaluate the text classification model and 3D human motion generation model, a\ngesture action classification dataset and action-based gesture dataset are\ncollected. With several experiments, the proposed method successfully generates\nperceptually natural and realistic 3D human motion from the text. Moreover, we\nverified the effectiveness of the proposed method using a public-available\naction recognition dataset to evaluate cross-dataset generalization\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2211.10003v1.pdf"
    },
    {
        "title": "Protein language model rescue mutations highlight variant effects and structure in clinically relevant genes",
        "authors": [
            "Onuralp Soylemez",
            "Pablo Cordero"
        ],
        "published": "2022-11-18T03:00:52Z",
        "summary": "Despite being self-supervised, protein language models have shown remarkable\nperformance in fundamental biological tasks such as predicting impact of\ngenetic variation on protein structure and function. The effectiveness of these\nmodels on diverse set of tasks suggests that they learn meaningful\nrepresentations of fitness landscape that can be useful for downstream clinical\napplications. Here, we interrogate the use of these language models in\ncharacterizing known pathogenic mutations in curated, medically actionable\ngenes through an exhaustive search of putative compensatory mutations on each\nvariant's genetic background. Systematic analysis of the predicted effects of\nthese compensatory mutations reveal unappreciated structural features of\nproteins that are missed by other structure predictors like AlphaFold. While\ndeep mutational scan experiments provide an unbiased estimate of the mutational\nlandscape, we encourage the community to generate and curate rescue mutation\nexperiments to inform the design of more sophisticated co-masking strategies\nand leverage large language models more effectively for downstream clinical\nprediction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.10000v1.pdf"
    },
    {
        "title": "Towards Explaining Subjective Ground of Individuals on Social Media",
        "authors": [
            "Younghun Lee",
            "Dan Goldwasser"
        ],
        "published": "2022-11-18T00:29:05Z",
        "summary": "Large-scale language models have been reducing the gap between machines and\nhumans in understanding the real world, yet understanding an individual's\ntheory of mind and behavior from text is far from being resolved.\n  This research proposes a neural model -- Subjective Ground Attention -- that\nlearns subjective grounds of individuals and accounts for their judgments on\nsituations of others posted on social media. Using simple attention modules as\nwell as taking one's previous activities into consideration, we empirically\nshow that our model provides human-readable explanations of an individual's\nsubjective preference in judging social situations. We further qualitatively\nevaluate the explanations generated by the model and claim that our model\nlearns an individual's subjective orientation towards abstract moral concepts",
        "pdf_link": "https://arxiv.org/pdf/2211.09953v1.pdf"
    },
    {
        "title": "Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers",
        "authors": [
            "Zhewei Yao",
            "Xiaoxia Wu",
            "Conglong Li",
            "Connor Holmes",
            "Minjia Zhang",
            "Cheng Li",
            "Yuxiong He"
        ],
        "published": "2022-11-17T23:14:58Z",
        "summary": "Large-scale transformer models have become the de-facto architectures for\nvarious machine learning applications, e.g., CV and NLP. However, those large\nmodels also introduce prohibitive training costs. To mitigate this issue, we\npropose a novel random and layerwise token dropping method (random-LTD), which\nskips the computation of a subset of the input tokens at all middle layers.\nParticularly, random-LTD achieves considerable speedups and comparable accuracy\nas the standard training baseline. Compared to other token dropping methods,\nrandom-LTD does not require (1) any importance score-based metrics, (2) any\nspecial token treatment (e.g., [CLS]), and (3) many layers in full sequence\nlength training except the first and the last layers. Besides, a new LayerToken\nlearning rate schedule is proposed for pretraining problems that resolve the\nheavy tuning requirement for our proposed training mechanism. Finally, we\ndemonstrate that random-LTD can be applied to broader applications, including\nGPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results\nshow that random-LTD can save about 33.3% theoretical compute cost and 25.6%\nwall-clock training time while achieving similar zero-shot evaluations on\nGPT-31.3B as compared to baseline.",
        "pdf_link": "https://arxiv.org/pdf/2211.11586v1.pdf"
    },
    {
        "title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
        "authors": [
            "Shreyas Sundara Raman",
            "Vanya Cohen",
            "Ifrah Idrees",
            "Eric Rosen",
            "Ray Mooney",
            "Stefanie Tellex",
            "David Paulius"
        ],
        "published": "2022-11-17T23:14:51Z",
        "summary": "Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.",
        "pdf_link": "https://arxiv.org/pdf/2211.09935v3.pdf"
    },
    {
        "title": "ProtSi: Prototypical Siamese Network with Data Augmentation for Few-Shot Subjective Answer Evaluation",
        "authors": [
            "Yining Lu",
            "Jingxi Qiu",
            "Gaurav Gupta"
        ],
        "published": "2022-11-17T19:33:35Z",
        "summary": "Subjective answer evaluation is a time-consuming and tedious task, and the\nquality of the evaluation is heavily influenced by a variety of subjective\npersonal characteristics. Instead, machine evaluation can effectively assist\neducators in saving time while also ensuring that evaluations are fair and\nrealistic. However, most existing methods using regular machine learning and\nnatural language processing techniques are generally hampered by a lack of\nannotated answers and poor model interpretability, making them unsuitable for\nreal-world use. To solve these challenges, we propose ProtSi Network, a unique\nsemi-supervised architecture that for the first time uses few-shot learning to\nsubjective answer evaluation. To evaluate students' answers by similarity\nprototypes, ProtSi Network simulates the natural process of evaluator scoring\nanswers by combining Siamese Network which consists of BERT and encoder layers\nwith Prototypical Network. We employed an unsupervised diverse paraphrasing\nmodel ProtAugment, in order to prevent overfitting for effective few-shot text\nclassification. By integrating contrastive learning, the discriminative text\nissue can be mitigated. Experiments on the Kaggle Short Scoring Dataset\ndemonstrate that the ProtSi Network outperforms the most recent baseline models\nin terms of accuracy and quadratic weighted kappa.",
        "pdf_link": "https://arxiv.org/pdf/2211.09855v1.pdf"
    },
    {
        "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks",
        "authors": [
            "Hao Li",
            "Jinguo Zhu",
            "Xiaohu Jiang",
            "Xizhou Zhu",
            "Hongsheng Li",
            "Chun Yuan",
            "Xiaohua Wang",
            "Yu Qiao",
            "Xiaogang Wang",
            "Wenhai Wang",
            "Jifeng Dai"
        ],
        "published": "2022-11-17T18:59:52Z",
        "summary": "Despite the remarkable success of foundation models, their task-specific\nfine-tuning paradigm makes them inconsistent with the goal of general\nperception modeling. The key to eliminating this inconsistency is to use\ngeneralist models for general task modeling. However, existing attempts at\ngeneralist models are inadequate in both versatility and performance. In this\npaper, we propose Uni-Perceiver v2, which is the first generalist model capable\nof handling major large-scale vision and vision-language tasks with competitive\nperformance. Specifically, images are encoded as general region proposals,\nwhile texts are encoded via a Transformer-based language model. The encoded\nrepresentations are transformed by a task-agnostic decoder. Different tasks are\nformulated as a unified maximum likelihood estimation problem. We further\npropose an improved optimizer to ensure stable multi-task learning with an\nunmixed sampling strategy, which is helpful for tasks requiring large\nbatch-size training. After being jointly trained on various tasks,\nUni-Perceiver v2 is capable of directly handling downstream tasks without any\ntask-specific adaptation. Results show that Uni-Perceiver v2 outperforms all\nexisting generalist models in both versatility and performance. Meanwhile,\ncompared with the commonly-recognized strong baselines that require\ntasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance\non a broad range of vision and vision-language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.09808v1.pdf"
    },
    {
        "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
        "authors": [
            "Tim Brooks",
            "Aleksander Holynski",
            "Alexei A. Efros"
        ],
        "published": "2022-11-17T18:58:43Z",
        "summary": "We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models -- a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) -- to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2211.09800v2.pdf"
    },
    {
        "title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization",
        "authors": [
            "Yulong Chen",
            "Yang Liu",
            "Ruochen Xu",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Michael Zeng",
            "Yue Zhang"
        ],
        "published": "2022-11-17T18:54:47Z",
        "summary": "The high annotation costs and diverse demands of various summarization tasks\nmotivate the development of few-shot summarization. However, despite the\nemergence of many summarization tasks and datasets, the current training\nparadigm for few-shot summarization systems ignores potentially shareable\nknowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm},\na unified few-shot summarization model pre-trained with multiple summarization\ntasks and can be prefix-tuned to excel at any few-shot summarization task.\nMeanwhile, to better evaluate few-shot summarizers, under the principles of\ndiversity and robustness, we assemble and release a new benchmark\n\\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of\nfew-shot samples for each task, covering diverse domains. Experimental results\nand analysis show that \\textsc{UniSumm} outperforms strong baselines by a large\nmargin across all sub-tasks in \\textsc{SummZoo} under both automatic and human\nevaluations and achieves comparable results in human evaluation compared with a\nGPT-3.5 model.",
        "pdf_link": "https://arxiv.org/pdf/2211.09783v6.pdf"
    },
    {
        "title": "Zero-Shot Dynamic Quantization for Transformer Inference",
        "authors": [
            "Yousef El-Kurdi",
            "Jerry Quinn",
            "Avirup Sil"
        ],
        "published": "2022-11-17T18:09:07Z",
        "summary": "We introduce a novel run-time method for significantly reducing the accuracy\nloss associated with quantizing BERT-like models to 8-bit integers. Existing\nmethods for quantizing models either modify the training procedure,or they\nrequire an additional calibration step to adjust parameters that also requires\na selected held-out dataset. Our method permits taking advantage of\nquantization without the need for these adjustments. We present results on\nseveral NLP tasks demonstrating the usefulness of this technique.",
        "pdf_link": "https://arxiv.org/pdf/2211.09744v1.pdf"
    },
    {
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
        "authors": [
            "F\u00e1bio Perez",
            "Ian Ribeiro"
        ],
        "published": "2022-11-17T13:43:20Z",
        "summary": "Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.",
        "pdf_link": "https://arxiv.org/pdf/2211.09527v1.pdf"
    },
    {
        "title": "LongFNT: Long-form Speech Recognition with Factorized Neural Transducer",
        "authors": [
            "Xun Gong",
            "Yu Wu",
            "Jinyu Li",
            "Shujie Liu",
            "Rui Zhao",
            "Xie Chen",
            "Yanmin Qian"
        ],
        "published": "2022-11-17T08:48:27Z",
        "summary": "Traditional automatic speech recognition~(ASR) systems usually focus on\nindividual utterances, without considering long-form speech with useful\nhistorical information, which is more practical in real scenarios. Simply\nattending longer transcription history for a vanilla neural transducer model\nshows no much gain in our preliminary experiments, since the prediction network\nis not a pure language model. This motivates us to leverage the factorized\nneural transducer structure, containing a real language model, the vocabulary\npredictor. We propose the {LongFNT-Text} architecture, which fuses the\nsentence-level long-form features directly with the output of the vocabulary\npredictor and then embeds token-level long-form features inside the vocabulary\npredictor, with a pre-trained contextual encoder RoBERTa to further boost the\nperformance. Moreover, we propose the {LongFNT} architecture by extending the\nlong-form speech to the original speech input and achieve the best performance.\nThe effectiveness of our LongFNT approach is validated on LibriSpeech and\nGigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.09412v1.pdf"
    },
    {
        "title": "A Graph-Based Context-Aware Model to Understand Online Conversations",
        "authors": [
            "Vibhor Agarwal",
            "Anthony P. Young",
            "Sagar Joglekar",
            "Nishanth Sastry"
        ],
        "published": "2022-11-16T20:51:45Z",
        "summary": "Online forums that allow for participatory engagement between users have been\ntransformative for the public discussion of many important issues. However,\nsuch conversations can sometimes escalate into full-blown exchanges of hate and\nmisinformation. Existing approaches in natural language processing (NLP), such\nas deep learning models for classification tasks, use as inputs only a single\ncomment or a pair of comments depending upon whether the task concerns the\ninference of properties of the individual comments or the replies between pairs\nof comments, respectively. But in online conversations, comments and replies\nmay be based on external context beyond the immediately relevant information\nthat is input to the model. Therefore, being aware of the conversations'\nsurrounding contexts should improve the model's performance for the inference\ntask at hand.\n  We propose GraphNLI, a novel graph-based deep learning architecture that uses\ngraph walks to incorporate the wider context of a conversation in a principled\nmanner. Specifically, a graph walk starts from a given comment and samples\n\"nearby\" comments in the same or parallel conversation threads, which results\nin additional embeddings that are aggregated together with the initial\ncomment's embedding. We then use these enriched embeddings for downstream NLP\nprediction tasks that are important for online conversations. We evaluate\nGraphNLI on two such tasks - polarity prediction and misogynistic hate speech\ndetection - and found that our model consistently outperforms all relevant\nbaselines for both tasks. Specifically, GraphNLI with a biased root-seeking\nrandom walk performs with a macro-F1 score of 3 and 6 percentage points better\nthan the best-performing BERT-based baselines for the polarity prediction and\nhate speech detection tasks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.09207v1.pdf"
    },
    {
        "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
        "authors": [
            "David Vilar",
            "Markus Freitag",
            "Colin Cherry",
            "Jiaming Luo",
            "Viresh Ratnakar",
            "George Foster"
        ],
        "published": "2022-11-16T18:42:37Z",
        "summary": "Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.",
        "pdf_link": "https://arxiv.org/pdf/2211.09102v3.pdf"
    },
    {
        "title": "Galactica: A Large Language Model for Science",
        "authors": [
            "Ross Taylor",
            "Marcin Kardas",
            "Guillem Cucurull",
            "Thomas Scialom",
            "Anthony Hartshorn",
            "Elvis Saravia",
            "Andrew Poulton",
            "Viktor Kerkez",
            "Robert Stojnic"
        ],
        "published": "2022-11-16T18:06:33Z",
        "summary": "Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.",
        "pdf_link": "https://arxiv.org/pdf/2211.09085v1.pdf"
    },
    {
        "title": "Technical Report on Neural Language Models and Few-Shot Learning for Systematic Requirements Processing in MDSE",
        "authors": [
            "Vincent Bertram",
            "Miriam Bo\u00df",
            "Evgeny Kusmenko",
            "Imke Helene Nachmann",
            "Bernhard Rumpe",
            "Danilo Trotta",
            "Louis Wachtmeister"
        ],
        "published": "2022-11-16T18:06:25Z",
        "summary": "Systems engineering, in particular in the automotive domain, needs to cope\nwith the massively increasing numbers of requirements that arise during the\ndevelopment process. To guarantee a high product quality and make sure that\nfunctional safety standards such as ISO26262 are fulfilled, the exploitation of\npotentials of model-driven systems engineering in the form of automatic\nanalyses, consistency checks, and tracing mechanisms is indispensable. However,\nthe language in which requirements are written, and the tools needed to operate\non them, are highly individual and require domain-specific tailoring. This\nhinders automated processing of requirements as well as the linking of\nrequirements to models. Introducing formal requirement notations in existing\nprojects leads to the challenge of translating masses of requirements and\nprocess changes on the one hand and to the necessity of the corresponding\ntraining for the requirements engineers.\n  In this paper, based on the analysis of an open-source set of automotive\nrequirements, we derive domain-specific language constructs helping us to avoid\nambiguities in requirements and increase the level of formality. The main\ncontribution is the adoption and evaluation of few-shot learning with large\npretrained language models for the automated translation of informal\nrequirements to structured languages such as a requirement DSL. We show that\nsupport sets of less than ten translation examples can suffice to few-shot\ntrain a language model to incorporate keywords and implement syntactic rules\ninto informal natural language requirements.",
        "pdf_link": "https://arxiv.org/pdf/2211.09084v1.pdf"
    },
    {
        "title": "Towards Computationally Verifiable Semantic Grounding for Language Models",
        "authors": [
            "Chris Alberti",
            "Kuzman Ganchev",
            "Michael Collins",
            "Sebastian Gehrmann",
            "Ciprian Chelba"
        ],
        "published": "2022-11-16T17:35:52Z",
        "summary": "The paper presents an approach to semantic grounding of language models (LMs)\nthat conceptualizes the LM as a conditional model generating text given a\ndesired semantic message formalized as a set of entity-relationship triples. It\nembeds the LM in an auto-encoder by feeding its output to a semantic parser\nwhose output is in the same representation domain as the input message.\nCompared to a baseline that generates text using greedy search, we demonstrate\ntwo techniques that improve the fluency and semantic accuracy of the generated\ntext: The first technique samples multiple candidate text sequences from which\nthe semantic parser chooses. The second trains the language model while keeping\nthe semantic parser frozen to improve the semantic accuracy of the\nauto-encoder. We carry out experiments on the English WebNLG 3.0 data set,\nusing BLEU to measure the fluency of generated text and standard parsing\nmetrics to measure semantic accuracy. We show that our proposed approaches\nsignificantly improve on the greedy search baseline. Human evaluation\ncorroborates the results of the automatic evaluation experiments.",
        "pdf_link": "https://arxiv.org/pdf/2211.09070v1.pdf"
    },
    {
        "title": "TSMind: Alibaba and Soochow University's Submission to the WMT22 Translation Suggestion Task",
        "authors": [
            "Xin Ge",
            "Ke Wang",
            "Jiayi Wang",
            "Nini Xiao",
            "Xiangyu Duan",
            "Yu Zhao",
            "Yuqi Zhang"
        ],
        "published": "2022-11-16T15:43:31Z",
        "summary": "This paper describes the joint submission of Alibaba and Soochow University,\nTSMind, to the WMT 2022 Shared Task on Translation Suggestion (TS). We\nparticipate in the English-German and English-Chinese tasks. Basically, we\nutilize the model paradigm fine-tuning on the downstream tasks based on\nlarge-scale pre-trained models, which has recently achieved great success. We\nchoose FAIR's WMT19 English-German news translation system and MBART50 for\nEnglish-Chinese as our pre-trained models. Considering the task's condition of\nlimited use of training data, we follow the data augmentation strategies\nproposed by WeTS to boost our TS model performance. The difference is that we\nfurther involve the dual conditional cross-entropy model and GPT-2 language\nmodel to filter augmented data. The leader board finally shows that our\nsubmissions are ranked first in three of four language directions in the Naive\nTS task of the WMT22 Translation Suggestion task.",
        "pdf_link": "https://arxiv.org/pdf/2211.08987v1.pdf"
    },
    {
        "title": "A Review of Intelligent Music Generation Systems",
        "authors": [
            "Lei Wang",
            "Ziyi Zhao",
            "Hanwei Liu",
            "Junwei Pang",
            "Yi Qin",
            "Qidi Wu"
        ],
        "published": "2022-11-16T13:43:16Z",
        "summary": "With the introduction of ChatGPT, the public's perception of AI-generated\ncontent (AIGC) has begun to reshape. Artificial intelligence has significantly\nreduced the barrier to entry for non-professionals in creative endeavors,\nenhancing the efficiency of content creation. Recent advancements have seen\nsignificant improvements in the quality of symbolic music generation, which is\nenabled by the use of modern generative algorithms to extract patterns implicit\nin a piece of music based on rule constraints or a musical corpus.\nNevertheless, existing literature reviews tend to present a conventional and\nconservative perspective on future development trajectories, with a notable\nabsence of thorough benchmarking of generative models. This paper provides a\nsurvey and analysis of recent intelligent music generation techniques,\noutlining their respective characteristics and discussing existing methods for\nevaluation. Additionally, the paper compares the different characteristics of\nmusic generation techniques in the East and West as well as analysing the\nfield's development prospects.",
        "pdf_link": "https://arxiv.org/pdf/2211.09124v3.pdf"
    },
    {
        "title": "L2 proficiency assessment using self-supervised speech representations",
        "authors": [
            "Stefano Bann\u00f2",
            "Kate M. Knill",
            "Marco Matassoni",
            "Vyas Raina",
            "Mark J. F. Gales"
        ],
        "published": "2022-11-16T11:47:20Z",
        "summary": "There has been a growing demand for automated spoken language assessment\nsystems in recent years. A standard pipeline for this process is to start with\na speech recognition system and derive features, either hand-crafted or based\non deep-learning, that exploit the transcription and audio. Though these\napproaches can yield high performance systems, they require speech recognition\nsystems that can be used for L2 speakers, and preferably tuned to the specific\nform of test being deployed. Recently a self-supervised speech representation\nbased scheme, requiring no speech recognition, was proposed. This work extends\nthe initial analysis conducted on this approach to a large scale proficiency\ntest, Linguaskill, that comprises multiple parts, each designed to assess\ndifferent attributes of a candidate's speaking proficiency. The performance of\nthe self-supervised, wav2vec 2.0, system is compared to a high performance\nhand-crafted assessment system and a BERT-based text system both of which use\nspeech transcriptions. Though the wav2vec 2.0 based system is found to be\nsensitive to the nature of the response, it can be configured to yield\ncomparable performance to systems requiring a speech transcription, and yields\ngains when appropriately combined with standard approaches.",
        "pdf_link": "https://arxiv.org/pdf/2211.08849v1.pdf"
    },
    {
        "title": "Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT",
        "authors": [
            "Siyuan Lu",
            "Chenchen Zhou",
            "Keli Xie",
            "Jun Lin",
            "Zhongfeng Wang"
        ],
        "published": "2022-11-16T11:43:09Z",
        "summary": "With the development of deep learning and Transformer-based pre-trained\nmodels like BERT, the accuracy of many NLP tasks has been dramatically\nimproved. However, the large number of parameters and computations also pose\nchallenges for their deployment. For instance, using BERT can improve the\npredictions in the financial sentiment analysis (FSA) task but slow it down,\nwhere speed and accuracy are equally important in terms of profits. To address\nthese issues, we first propose an efficient and lightweight BERT (ELBERT) along\nwith a novel confidence-window-based (CWB) early exit mechanism. Based on\nELBERT, an innovative method to accelerate text processing on the GPU platform\nis developed, solving the difficult problem of making the early exit mechanism\nwork more effectively with a large input batch size. Afterward, a fast and\nhigh-accuracy FSA system is built. Experimental results show that the proposed\nCWB early exit mechanism achieves significantly higher accuracy than existing\nearly exit methods on BERT under the same computation cost. By using this\nacceleration method, our FSA system can boost the processing speed by nearly 40\ntimes to over 1000 texts per second with sufficient accuracy, which is nearly\ntwice as fast as FastBERT, thus providing a more powerful text processing\ncapability for modern trading systems.",
        "pdf_link": "https://arxiv.org/pdf/2211.08842v2.pdf"
    },
    {
        "title": "Cognitive Simplification Operations Improve Text Simplification",
        "authors": [
            "Eytan Chamovitz",
            "Omri Abend"
        ],
        "published": "2022-11-16T10:51:03Z",
        "summary": "Text Simplification (TS) is the task of converting a text into a form that is\neasier to read while maintaining the meaning of the original text. A sub-task\nof TS is Cognitive Simplification (CS), converting text to a form that is\nreadily understood by people with cognitive disabilities without rendering it\nchildish or simplistic. This sub-task has yet to be explored with neural\nmethods in NLP, and resources for it are scarcely available. In this paper, we\npresent a method for incorporating knowledge from the cognitive accessibility\ndomain into a TS model, by introducing an inductive bias regarding what\nsimplification operations to use. We show that by adding this inductive bias to\na TS-trained model, it is able to adapt better to CS without ever seeing CS\ndata, and outperform a baseline model on a traditional TS benchmark. In\naddition, we provide a novel test dataset for CS, and analyze the differences\nbetween CS corpora and existing TS corpora, in terms of how simplification\noperations are applied.",
        "pdf_link": "https://arxiv.org/pdf/2211.08825v1.pdf"
    },
    {
        "title": "Streaming Joint Speech Recognition and Disfluency Detection",
        "authors": [
            "Hayato Futami",
            "Emiru Tsunoo",
            "Kentaro Shibata",
            "Yosuke Kashiwagi",
            "Takao Okuda",
            "Siddhant Arora",
            "Shinji Watanabe"
        ],
        "published": "2022-11-16T07:34:20Z",
        "summary": "Disfluency detection has mainly been solved in a pipeline approach, as\npost-processing of speech recognition. In this study, we propose\nTransformer-based encoder-decoder models that jointly solve speech recognition\nand disfluency detection, which work in a streaming manner. Compared to\npipeline approaches, the joint models can leverage acoustic information that\nmakes disfluency detection robust to recognition errors and provide non-verbal\nclues. Moreover, joint modeling results in low-latency and lightweight\ninference. We investigate two joint model variants for streaming disfluency\ndetection: a transcript-enriched model and a multi-task model. The\ntranscript-enriched model is trained on text with special tags indicating the\nstarting and ending points of the disfluent part. However, it has problems with\nlatency and standard language model adaptation, which arise from the additional\ndisfluency tags. We propose a multi-task model to solve such problems, which\nhas two output layers at the Transformer decoder; one for speech recognition\nand the other for disfluency detection. It is modeled to be conditioned on the\ncurrently recognized token with an additional token-dependency mechanism. We\nshow that the proposed joint models outperformed a BERT-based pipeline approach\nin both accuracy and latency, on both the Switchboard and the corpus of\nspontaneous Japanese.",
        "pdf_link": "https://arxiv.org/pdf/2211.08726v2.pdf"
    },
    {
        "title": "Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales",
        "authors": [
            "Saurabh Kulshreshtha",
            "Anna Rumshisky"
        ],
        "published": "2022-11-15T19:36:06Z",
        "summary": "Multi-hop Question Generation is the task of generating questions which\nrequire the reader to reason over and combine information spread across\nmultiple passages using several reasoning steps. Chain-of-thought rationale\ngeneration has been shown to improve performance on multi-step reasoning tasks\nand make model predictions more interpretable. However, few-shot performance\ngains from including rationales have been largely observed only in +100B\nlanguage models, and otherwise require large scale manual rationale annotation.\nIn this work, we introduce a new framework for applying chain-of-thought\ninspired structured rationale generation to multi-hop question generation under\na very low supervision regime (8- to 128-shot). We propose to annotate a small\nnumber of examples following our proposed multi-step rationale schema, treating\neach reasoning step as a separate task to be performed by a generative language\nmodel. We show that our framework leads to improved control over the difficulty\nof the generated questions and better performance compared to baselines trained\nwithout rationales, both on automatic evaluation metrics and in human\nevaluation. Importantly, we show that this is achievable with a modest model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2211.08466v1.pdf"
    },
    {
        "title": "ED-FAITH: Evaluating Dialogue Summarization on Faithfulness",
        "authors": [
            "Sicong Huang",
            "Asli Celikyilmaz",
            "Haoran Li"
        ],
        "published": "2022-11-15T19:33:50Z",
        "summary": "Abstractive summarization models typically generate content unfaithful to the\ninput, thus highlighting the significance of evaluating the faithfulness of\ngenerated summaries. Most faithfulness metrics are only evaluated on news\ndomain, can they be transferred to other summarization tasks? In this work, we\nfirst present a systematic study of faithfulness metrics for dialogue\nsummarization. We evaluate common faithfulness metrics on dialogue datasets and\nobserve that most metrics correlate poorly with human judgements despite\nperforming well on news datasets. Given these findings, to improve existing\nmetrics' performance on dialogue summarization, we first finetune on in-domain\ndataset, then apply unlikelihood training on negative samples, and show that\nthey can successfully improve metric performance on dialogue data. Inspired by\nthe strong zero-shot performance of the T0 language model, we further propose\nT0-Score -- a new metric for faithfulness evaluation, which shows consistent\nimprovement against baseline metrics across multiple domains.",
        "pdf_link": "https://arxiv.org/pdf/2211.08464v1.pdf"
    },
    {
        "title": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
        "authors": [
            "Yushi Hu",
            "Hang Hua",
            "Zhengyuan Yang",
            "Weijia Shi",
            "Noah A Smith",
            "Jiebo Luo"
        ],
        "published": "2022-11-15T19:07:53Z",
        "summary": "Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.",
        "pdf_link": "https://arxiv.org/pdf/2211.09699v4.pdf"
    },
    {
        "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
        "authors": [
            "Derek Tam",
            "Anisha Mascarenhas",
            "Shiyue Zhang",
            "Sarah Kwan",
            "Mohit Bansal",
            "Colin Raffel"
        ],
        "published": "2022-11-15T18:50:34Z",
        "summary": "While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.",
        "pdf_link": "https://arxiv.org/pdf/2211.08412v2.pdf"
    },
    {
        "title": "Large Language Models Struggle to Learn Long-Tail Knowledge",
        "authors": [
            "Nikhil Kandpal",
            "Haikang Deng",
            "Adam Roberts",
            "Eric Wallace",
            "Colin Raffel"
        ],
        "published": "2022-11-15T18:49:27Z",
        "summary": "The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, while certain pieces of information are ubiquitous\non the web, others appear extremely rarely. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in pre-training datasets scraped from the web. In particular, we\nshow that a language model's ability to answer a fact-based question relates to\nhow many documents associated with that question were seen during pre-training.\nWe identify these relevant documents by entity linking pre-training datasets\nand counting documents that contain the same entities as a given\nquestion-answer pair. Our results demonstrate strong correlational and causal\nrelationships between accuracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,\nROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models\nare better at learning long-tail knowledge, we estimate that today's models\nmust be scaled by many orders of magnitude to reach competitive QA performance\non questions with little support in the pre-training data. Finally, we show\nthat retrieval-augmentation can reduce the dependence on relevant pre-training\ninformation, presenting a promising approach for capturing the long-tail.",
        "pdf_link": "https://arxiv.org/pdf/2211.08411v2.pdf"
    },
    {
        "title": "Introducing Semantics into Speech Encoders",
        "authors": [
            "Derek Xu",
            "Shuyan Dong",
            "Changhan Wang",
            "Suyoun Kim",
            "Zhaojiang Lin",
            "Akshat Shrivastava",
            "Shang-Wen Li",
            "Liang-Hsuan Tseng",
            "Alexei Baevski",
            "Guan-Ting Lin",
            "Hung-yi Lee",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2022-11-15T18:44:28Z",
        "summary": "Recent studies find existing self-supervised speech encoders contain\nprimarily acoustic rather than semantic information. As a result, pipelined\nsupervised automatic speech recognition (ASR) to large language model (LLM)\nsystems achieve state-of-the-art results on semantic spoken language tasks by\nutilizing rich semantic representations from the LLM. These systems come at the\ncost of labeled audio transcriptions, which is expensive and time-consuming to\nobtain. We propose a task-agnostic unsupervised way of incorporating semantic\ninformation from LLMs into self-supervised speech encoders without labeled\naudio transcriptions. By introducing semantics, we improve existing speech\nencoder spoken language understanding performance by over 10\\% on intent\nclassification, with modest gains in named entity resolution and slot filling,\nand spoken question answering FF1 score by over 2\\%. Our unsupervised approach\nachieves similar performance as supervised methods trained on over 100 hours of\nlabeled audio transcripts, demonstrating the feasibility of unsupervised\nsemantic augmentations to existing speech encoders.",
        "pdf_link": "https://arxiv.org/pdf/2211.08402v1.pdf"
    },
    {
        "title": "Empowering Language Models with Knowledge Graph Reasoning for Question Answering",
        "authors": [
            "Ziniu Hu",
            "Yichong Xu",
            "Wenhao Yu",
            "Shuohang Wang",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Kai-Wei Chang",
            "Yizhou Sun"
        ],
        "published": "2022-11-15T18:26:26Z",
        "summary": "Answering open-domain questions requires world knowledge about in-context\nentities. As pre-trained Language Models (LMs) lack the power to store all\nrequired knowledge, external knowledge sources, such as knowledge graphs, are\noften used to augment LMs. In this work, we propose knOwledge REasOning\nempowered Language Model (OREO-LM), which consists of a novel Knowledge\nInteraction Layer that can be flexibly plugged into existing Transformer-based\nLMs to interact with a differentiable Knowledge Graph Reasoning module\ncollaboratively. In this way, LM guides KG to walk towards the desired answer,\nwhile the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and\nT5, we show significant performance gain, achieving state-of-art results in the\nClosed-Book setting. The performance enhancement is mainly from the KG\nreasoning's capacity to infer missing relational facts. In addition, OREO-LM\nprovides reasoning paths as rationales to interpret the model's decision.",
        "pdf_link": "https://arxiv.org/pdf/2211.08380v1.pdf"
    },
    {
        "title": "FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery",
        "authors": [
            "Changlong Yu",
            "Weiqi Wang",
            "Xin Liu",
            "Jiaxin Bai",
            "Yangqiu Song",
            "Zheng Li",
            "Yifan Gao",
            "Tianyu Cao",
            "Bing Yin"
        ],
        "published": "2022-11-15T17:20:40Z",
        "summary": "Understanding users' intentions in e-commerce platforms requires commonsense\nknowledge. In this paper, we present FolkScope, an intention knowledge graph\nconstruction framework to reveal the structure of humans' minds about\npurchasing items. As commonsense knowledge is usually ineffable and not\nexpressed explicitly, it is challenging to perform information extraction.\nThus, we propose a new approach that leverages the generation power of large\nlanguage models~(LLMs) and human-in-the-loop annotation to semi-automatically\nconstruct the knowledge graph. LLMs first generate intention assertions via\ne-commerce-specific prompts to explain shopping behaviors, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility\nand typicality labels of sampled intentions as training data in order to\npopulate human judgments to all automatic generations. Last, to structurize the\nassertions, we propose pattern mining and conceptualization to form more\ncondensed and abstract knowledge. Extensive evaluations and studies demonstrate\nthat our constructed knowledge graph can well model e-commerce knowledge and\nhave many potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2211.08316v2.pdf"
    },
    {
        "title": "An FNet based Auto Encoder for Long Sequence News Story Generation",
        "authors": [
            "Paul K. Mandal",
            "Rakeshkumar Mahto"
        ],
        "published": "2022-11-15T16:48:09Z",
        "summary": "In this paper, we design an auto encoder based off of Google's FNet\nArchitecture in order to generate text from a subset of news stories contained\nin Google's C4 dataset. We discuss previous attempts and methods to generate\ntext from autoencoders and non LLM Models. FNET poses multiple advantages to\nBERT based encoders in the realm of efficiency which train 80% faster on GPUs\nand 70% faster on TPUs. We then compare outputs of how this autencoder perfroms\non different epochs. Finally, we analyze what outputs the encoder produces with\ndifferent seed text.",
        "pdf_link": "https://arxiv.org/pdf/2211.08295v2.pdf"
    },
    {
        "title": "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use",
        "authors": [
            "Pieter Delobelle",
            "Thomas Winters",
            "Bettina Berendt"
        ],
        "published": "2022-11-15T14:55:53Z",
        "summary": "Large transformer-based language models, e.g. BERT and GPT-3, outperform\nprevious architectures on most natural language processing tasks. Such language\nmodels are first pre-trained on gigantic corpora of text and later used as\nbase-model for finetuning on a particular task. Since the pre-training step is\nusually not repeated, base models are not up-to-date with the latest\ninformation. In this paper, we update RobBERT, a RoBERTa-based state-of-the-art\nDutch language model, which was trained in 2019. First, the tokenizer of\nRobBERT is updated to include new high-frequent tokens present in the latest\nDutch OSCAR corpus, e.g. corona-related words. Then we further pre-train the\nRobBERT model using this dataset. To evaluate if our new model is a plug-in\nreplacement for RobBERT, we introduce two additional criteria based on concept\ndrift of existing tokens and alignment for novel tokens.We found that for\ncertain language tasks this update results in a significant performance\nincrease. These results highlight the benefit of continually updating a\nlanguage model to account for evolving language use.",
        "pdf_link": "https://arxiv.org/pdf/2211.08192v1.pdf"
    },
    {
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective",
        "authors": [
            "Linyi Yang",
            "Shuibai Zhang",
            "Libo Qin",
            "Yafu Li",
            "Yidong Wang",
            "Hanmeng Liu",
            "Jindong Wang",
            "Xing Xie",
            "Yue Zhang"
        ],
        "published": "2022-11-15T11:53:55Z",
        "summary": "Pre-trained language models (PLMs) are known to improve the generalization\nperformance of natural language understanding models by leveraging large\namounts of data during the pre-training phase. However, the out-of-distribution\n(OOD) generalization problem remains a challenge in many NLP tasks, limiting\nthe real-world deployment of these methods. This paper presents the first\nattempt at creating a unified benchmark named GLUE-X for evaluating OOD\nrobustness in NLP models, highlighting the importance of OOD robustness and\nproviding insights on how to measure the robustness of a model and how to\nimprove it. The benchmark includes 13 publicly available datasets for OOD\ntesting, and evaluations are conducted on 8 classic NLP tasks over 21 popularly\nused PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for\nimproved OOD accuracy in NLP tasks, as significant performance degradation was\nobserved in all settings compared to in-distribution (ID) accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2211.08073v4.pdf"
    },
    {
        "title": "FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers",
        "authors": [
            "Jinyu Chen",
            "Wenchao Xu",
            "Song Guo",
            "Junxiao Wang",
            "Jie Zhang",
            "Haozhao Wang"
        ],
        "published": "2022-11-15T10:16:13Z",
        "summary": "Federated Learning (FL) is an emerging paradigm that enables distributed\nusers to collaboratively and iteratively train machine learning models without\nsharing their private data. Motivated by the effectiveness and robustness of\nself-attention-based architectures, researchers are turning to using\npre-trained Transformers (i.e., foundation models) instead of traditional\nconvolutional neural networks in FL to leverage their excellent transfer\nlearning capabilities. Despite recent progress, how pre-trained Transformer\nmodels play a role in FL remains obscure, that is, how to efficiently fine-tune\nthese pre-trained models in FL and how FL users could benefit from this new\nparadigm. In this paper, we explore this issue and demonstrate that the\nfine-tuned Transformers achieve extraordinary performance on FL, and that the\nlightweight fine-tuning method facilitates a fast convergence rate and low\ncommunication costs. Concretely, we conduct a rigorous empirical study of three\ntuning methods (i.e., modifying the input, adding extra modules, and adjusting\nthe backbone) using two types of pre-trained models (i.e., vision-language\nmodels and vision models) for FL. Our experiments show that 1) Fine-tuning the\nbias term of the backbone performs best when relying on a strong pre-trained\nmodel; 2) The vision-language model (e.g., CLIP) outperforms the pure vision\nmodel (e.g., ViT) and is more robust to the few-shot settings; 3) Compared to\npure local training, FL with pre-trained models has a higher accuracy because\nit alleviates the problem of over-fitting. We will release our code and\nencourage further exploration of pre-trained Transformers and FL.",
        "pdf_link": "https://arxiv.org/pdf/2211.08025v1.pdf"
    },
    {
        "title": "Teaching Algorithmic Reasoning via In-context Learning",
        "authors": [
            "Hattie Zhou",
            "Azade Nova",
            "Hugo Larochelle",
            "Aaron Courville",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ],
        "published": "2022-11-15T06:12:28Z",
        "summary": "Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2211.09066v1.pdf"
    },
    {
        "title": "Relationship of the language distance to English ability of a country",
        "authors": [
            "Cao Xinxin",
            "Lei Xiaolan",
            "Murtadha Ahmed"
        ],
        "published": "2022-11-15T02:40:00Z",
        "summary": "Language difference is one of the factors that hinder the acquisition of\nsecond language skills. In this article, we introduce a novel solution that\nleverages the strength of deep neural networks to measure the semantic\ndissimilarity between languages based on their word distributions in the\nembedding space of the multilingual pre-trained language model (e.g.,BERT).\nThen, we empirically examine the effectiveness of the proposed semantic\nlanguage distance (SLD) in explaining the consistent variation in English\nability of countries, which is proxied by their performance in the\nInternet-Based Test of English as Foreign Language (TOEFL iBT). The\nexperimental results show that the language distance demonstrates negative\ninfluence on a country's average English ability. Interestingly, the effect is\nmore significant on speaking and writing subskills, which pertain to the\nproductive aspects of language learning. Besides, we provide specific\nrecommendations for future research directions.",
        "pdf_link": "https://arxiv.org/pdf/2211.07855v1.pdf"
    },
    {
        "title": "UGIF: UI Grounded Instruction Following",
        "authors": [
            "Sagar Gubbi Venkatesh",
            "Partha Talukdar",
            "Srini Narayanan"
        ],
        "published": "2022-11-14T18:36:19Z",
        "summary": "Smartphone users often find it difficult to navigate myriad menus to perform\ncommon tasks such as \"How to block calls from unknown numbers?\". Currently,\nhelp documents with step-by-step instructions are manually written to aid the\nuser. The user experience can be further enhanced by grounding the instructions\nin the help document to the UI and overlaying a tutorial on the phone UI. To\nbuild such tutorials, several natural language processing components including\nretrieval, parsing, and grounding are necessary, but there isn't any relevant\ndataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual,\nmulti-modal UI grounded dataset for step-by-step task completion on the\nsmartphone containing 4,184 tasks across 8 languages. As an initial approach to\nthis problem, we propose retrieving the relevant instruction steps based on the\nuser's query and parsing the steps using Large Language Models (LLMs) to\ngenerate macros that can be executed on-device. The instruction steps are often\navailable only in English, so the challenge includes cross-modal, cross-lingual\nretrieval of English how-to pages from user queries in many languages and\nmapping English instruction steps to UI in a potentially different language. We\ncompare the performance of different LLMs including PaLM and GPT-3 and find\nthat the end-to-end task completion rate is 48% for English UI but the\nperformance drops to 32% for other languages. We analyze the common failure\nmodes of existing models on this task and point out areas for improvement.",
        "pdf_link": "https://arxiv.org/pdf/2211.07615v2.pdf"
    },
    {
        "title": "Towards a Mathematics Formalisation Assistant using Large Language Models",
        "authors": [
            "Ayush Agrawal",
            "Siddhartha Gadgil",
            "Navin Goyal",
            "Ashvni Narayanan",
            "Anand Tadipatri"
        ],
        "published": "2022-11-14T16:52:32Z",
        "summary": "Mathematics formalisation is the task of writing mathematics (i.e.,\ndefinitions, theorem statements, proofs) in natural language, as found in books\nand papers, into a formal language that can then be checked for correctness by\na program. It is a thriving activity today, however formalisation remains\ncumbersome. In this paper, we explore the abilities of a large language model\n(Codex) to help with formalisation in the Lean theorem prover. We find that\nwith careful input-dependent prompt selection and postprocessing, Codex is able\nto formalise short mathematical statements at undergrad level with nearly 75\\%\naccuracy for $120$ theorem statements. For proofs quantitative analysis is\ninfeasible and we undertake a detailed case study. We choose a diverse set of\n$13$ theorems at undergrad level with proofs that fit in two-three paragraphs.\nWe show that with a new prompting strategy Codex can formalise these proofs in\nnatural language with at least one out of twelve Codex completion being easy to\nrepair into a complete proof. This is surprising as essentially no aligned data\nexists for formalised mathematics, particularly for proofs. These results\nsuggest that large language models are a promising avenue towards fully or\npartially automating formalisation.",
        "pdf_link": "https://arxiv.org/pdf/2211.07524v1.pdf"
    },
    {
        "title": "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations",
        "authors": [
            "Swarnadeep Saha",
            "Peter Hase",
            "Nazneen Rajani",
            "Mohit Bansal"
        ],
        "published": "2022-11-14T16:46:14Z",
        "summary": "Recent work on explainable NLP has shown that few-shot prompting can enable\nlarge pretrained language models (LLMs) to generate grammatical and factual\nnatural language explanations for data labels. In this work, we study the\nconnection between explainability and sample hardness by investigating the\nfollowing research question - \"Are LLMs and humans equally good at explaining\ndata labels for both easy and hard samples?\" We answer this question by first\ncollecting human-written explanations in the form of generalizable commonsense\nrules on the task of Winograd Schema Challenge (Winogrande dataset). We compare\nthese explanations with those generated by GPT-3 while varying the hardness of\nthe test samples as well as the in-context samples. We observe that (1) GPT-3\nexplanations are as grammatical as human explanations regardless of the\nhardness of the test samples, (2) for easy examples, GPT-3 generates highly\nsupportive explanations but human explanations are more generalizable, and (3)\nfor hard examples, human explanations are significantly better than GPT-3\nexplanations both in terms of label-supportiveness and generalizability\njudgements. We also find that hardness of the in-context examples impacts the\nquality of GPT-3 explanations. Finally, we show that the supportiveness and\ngeneralizability aspects of human explanations are also impacted by sample\nhardness, although by a much smaller margin than models. Supporting code and\ndata are available at https://github.com/swarnaHub/ExplanationHardness",
        "pdf_link": "https://arxiv.org/pdf/2211.07517v1.pdf"
    },
    {
        "title": "AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT",
        "authors": [
            "Aman Priyanshu",
            "Supriti Vijay"
        ],
        "published": "2022-11-14T16:29:03Z",
        "summary": "Keyword extraction has been an important topic for modern natural language\nprocessing. With its applications ranging from ontology generation, fact\nverification in summarized text, and recommendation systems. While it has had\nsignificant data-intensive applications, it is often hampered when the data set\nis small. Downstream training for keyword extractors is a lengthy process and\nrequires a significant amount of data. Recently, Few-shot Learning (FSL) and\nZero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,\nwe propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM\nbases by incorporating the concept of regularized attention into a pre-training\nphase for downstream domain adaptation. As we believe our work has implications\nto be utilized in the pipeline of FSL/ZSL and keyword extraction, we\nopen-source our code as well as provide the fine-tuning library of the same\nname AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.",
        "pdf_link": "https://arxiv.org/pdf/2211.07499v2.pdf"
    },
    {
        "title": "Semantic Decomposition Improves Learning of Large Language Models on EHR Data",
        "authors": [
            "David A. Bloore",
            "Romane Gauriau",
            "Anna L. Decker",
            "Jacob Oppenheim"
        ],
        "published": "2022-11-14T14:59:16Z",
        "summary": "Electronic health records (EHR) are widely believed to hold a profusion of\nactionable insights, encrypted in an irregular, semi-structured format, amidst\na loud noise background. To simplify learning patterns of health and disease,\nmedical codes in EHR can be decomposed into semantic units connected by\nhierarchical graphs. Building on earlier synergy between Bidirectional Encoder\nRepresentations from Transformers (BERT) and Graph Attention Networks (GAT), we\npresent H-BERT, which ingests complete graph tree expansions of hierarchical\nmedical codes as opposed to only ingesting the leaves and pushes patient-level\nlabels down to each visit. This methodology significantly improves prediction\nof patient membership in over 500 medical diagnosis classes as measured by\naggregated AUC and APS, and creates distinct representations of patients in\nclosely related but clinically distinct phenotypes.",
        "pdf_link": "https://arxiv.org/pdf/2212.06040v1.pdf"
    },
    {
        "title": "Does Debiasing Inevitably Degrade the Model Performance",
        "authors": [
            "Yiran Liu",
            "Xiao Liu",
            "Haotian Chen",
            "Yang Yu"
        ],
        "published": "2022-11-14T13:46:13Z",
        "summary": "Gender bias in language models has attracted sufficient attention because it\nthreatens social justice. However, most of the current debiasing methods\ndegraded the model's performance on other tasks while the degradation mechanism\nis still mysterious. We propose a theoretical framework explaining the three\ncandidate mechanisms of the language model's gender bias. We use our\ntheoretical framework to explain why the current debiasing methods cause\nperformance degradation. We also discover a pathway through which debiasing\nwill not degrade the model performance. We further develop a\ncausality-detection fine-tuning approach to correct gender bias. The numerical\nexperiment demonstrates that our method is able to lead to double dividends:\npartially mitigating gender bias while avoiding performance degradation.",
        "pdf_link": "https://arxiv.org/pdf/2211.07350v2.pdf"
    },
    {
        "title": "Replacing Language Model for Style Transfer",
        "authors": [
            "Pengyu Cheng",
            "Ruineng Li"
        ],
        "published": "2022-11-14T13:35:55Z",
        "summary": "We introduce replacing language model (RLM), a sequence-to-sequence language\nmodeling framework for text style transfer (TST). Our method autoregressively\nreplaces each token of the source sentence with a text span that has a similar\nmeaning but in the target style. The new span is generated via a\nnon-autoregressive masked language model, which can better preserve the\nlocal-contextual meaning of the replaced token. This RLM generation scheme\ngathers the flexibility of autoregressive models and the accuracy of\nnon-autoregressive models, which bridges the gap between sentence-level and\nword-level style transfer methods. To control the generation style more\nprecisely, we conduct a token-level style-content disentanglement on the hidden\nrepresentations of RLM. Empirical results on real-world text datasets\ndemonstrate the effectiveness of RLM compared with other TST baselines. The\ncode is at https://github.com/Linear95/RLM.",
        "pdf_link": "https://arxiv.org/pdf/2211.07343v2.pdf"
    },
    {
        "title": "Hope Speech Detection on Social Media Platforms",
        "authors": [
            "Pranjal Aggarwal",
            "Pasupuleti Chandana",
            "Jagrut Nemade",
            "Shubham Sharma",
            "Sunil Saumya",
            "Shankar Biradar"
        ],
        "published": "2022-11-14T10:58:22Z",
        "summary": "Since personal computers became widely available in the consumer market, the\namount of harmful content on the internet has significantly expanded. In simple\nterms, harmful content is anything online which causes a person distress or\nharm. It may include hate speech, violent content, threats, non-hope speech,\netc. The online content must be positive, uplifting and supportive. Over the\npast few years, many studies have focused on solving this problem through hate\nspeech detection, but very few focused on identifying hope speech. This paper\ndiscusses various machine learning approaches to identify a sentence as Hope\nSpeech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study\ncontains English YouTube comments and is released as a part of the shared task\n\"EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion\".\nInitially, the dataset obtained from the shared task had three classes: Hope\nSpeech, non-Hope speech, and not in English; however, upon deeper inspection,\nwe discovered that dataset relabeling is required. A group of undergraduates\nwas hired to help perform the entire dataset's relabeling task. We experimented\nwith conventional machine learning models (such as Na\\\"ive Bayes, logistic\nregression and support vector machine) and pre-trained models (such as BERT) on\nrelabeled data. According to the experimental results, the relabeled data has\nachieved a better accuracy for Hope speech identification than the original\ndata set.",
        "pdf_link": "https://arxiv.org/pdf/2212.07424v1.pdf"
    },
    {
        "title": "Grafting Pre-trained Models for Multimodal Headline Generation",
        "authors": [
            "Lingfeng Qiao",
            "Chen Wu",
            "Ye Liu",
            "Haoyuan Peng",
            "Di Yin",
            "Bo Ren"
        ],
        "published": "2022-11-14T08:59:59Z",
        "summary": "Multimodal headline utilizes both video frames and transcripts to generate\nthe natural language title of the videos. Due to a lack of large-scale,\nmanually annotated data, the task of annotating grounded headlines for video is\nlabor intensive and impractical. Previous researches on pre-trained language\nmodels and video-language models have achieved significant progress in related\ndownstream tasks. However, none of them can be directly applied to multimodal\nheadline architecture where we need both multimodal encoder and sentence\ndecoder. A major challenge in simply gluing language model and video-language\nmodel is the modality balance, which is aimed at combining visual-language\ncomplementary abilities. In this paper, we propose a novel approach to graft\nthe video encoder from the pre-trained video-language model on the generative\npre-trained language model. We also present a consensus fusion mechanism for\nthe integration of different components, via inter/intra modality relation.\nEmpirically, experiments show that the grafted model achieves strong results on\na brand-new dataset collected from real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2211.07210v1.pdf"
    },
    {
        "title": "Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation",
        "authors": [
            "Ke Wang",
            "Xin Ge",
            "Jiayi Wang",
            "Yu Zhao",
            "Yuqi Zhang"
        ],
        "published": "2022-11-14T03:40:02Z",
        "summary": "Machine translation technology has made great progress in recent years, but\nit cannot guarantee error free results. Human translators perform post editing\non machine translations to correct errors in the scene of computer aided\ntranslation. In favor of expediting the post editing process, many works have\ninvestigated machine translation in interactive modes, in which machines can\nautomatically refine the rest of translations constrained by human's edits.\nTranslation Suggestion (TS), as an interactive mode to assist human\ntranslators, requires machines to generate alternatives for specific incorrect\nwords or phrases selected by human translators. In this paper, we utilize the\nparameterized objective function of neural machine translation (NMT) and\npropose a novel constrained decoding algorithm, namely Prefix Suffix Guided\nDecoding (PSGD), to deal with the TS problem without additional training.\nCompared to the state of the art lexically constrained decoding method, PSGD\nimproves translation quality by an average of $10.87$ BLEU and $8.62$ BLEU on\nthe WeTS and the WMT 2022 Translation Suggestion datasets, respectively, and\nreduces decoding time overhead by an average of 63.4% tested on the WMT\ntranslation datasets. Furthermore, on both of the TS benchmark datasets, it is\nsuperior to other supervised learning systems trained with TS annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2211.07093v2.pdf"
    },
    {
        "title": "Controllable Citation Sentence Generation with Language Models",
        "authors": [
            "Nianlong Gu",
            "Richard H. R. Hahnloser"
        ],
        "published": "2022-11-14T01:54:08Z",
        "summary": "Citation generation aims to generate a citation sentence that refers to a\nchosen paper in the context of a manuscript. However, a rigid citation\ngeneration process is at odds with an author's desire to control specific\nattributes, such as 1) the citation intent, e.g., either introducing background\ninformation or comparing results, and 2) keywords that should appear in the\ncitation text. To provide these degrees of controllability during citation\ngeneration, we propose to integrate the manuscript context, the context of the\nreferenced paper, and the desired control attributes into a structured template\nand use it to fine-tune a language model (LM) via next-token prediction. We\nthen utilize Proximal Policy Optimization to directly optimize the LM in favor\nof a high score of our proposed controllability metric. The proposed workflow\nharmoniously combines citation attribute suggestion and conditional citation\ngeneration into one LM, allowing for better user control.",
        "pdf_link": "https://arxiv.org/pdf/2211.07066v2.pdf"
    },
    {
        "title": "ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for Commonsense Question Answering",
        "authors": [
            "Byeongmin Choi",
            "YongHyun Lee",
            "Yeunwoong Kyung",
            "Eunchan Kim"
        ],
        "published": "2022-11-14T01:39:26Z",
        "summary": "Recently, pre-trained language representation models such as bidirectional\nencoder representations from transformers (BERT) have been performing well in\ncommonsense question answering (CSQA). However, there is a problem that the\nmodels do not directly use explicit information of knowledge sources existing\noutside. To augment this, additional methods such as knowledge-aware graph\nnetwork (KagNet) and multi-hop graph relation network (MHGRN) have been\nproposed. In this study, we propose to use the latest pre-trained language\nmodel a lite bidirectional encoder representations from transformers (ALBERT)\nwith knowledge graph information extraction technique. We also propose to\napplying the novel method, schema graph expansion to recent language models.\nThen, we analyze the effect of applying knowledge graph-based knowledge\nextraction techniques to recent pre-trained language models and confirm that\nschema graph expansion is effective in some extent. Furthermore, we show that\nour proposed model can achieve better performance than existing KagNet and\nMHGRN models in CommonsenseQA dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.07065v1.pdf"
    },
    {
        "title": "Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction",
        "authors": [
            "Grace Yang",
            "Ming Cao",
            "Lavender Y. Jiang",
            "Xujin C. Liu",
            "Alexander T. M. Cheung",
            "Hannah Weiss",
            "David Kurland",
            "Kyunghyun Cho",
            "Eric K. Oermann"
        ],
        "published": "2022-11-13T23:59:11Z",
        "summary": "Traditional evaluation metrics for classification in natural language\nprocessing such as accuracy and area under the curve fail to differentiate\nbetween models with different predictive behaviors despite their similar\nperformance metrics. We introduce sensitivity score, a metric that scrutinizes\nmodels' behaviors at the vocabulary level to provide insights into disparities\nin their decision-making logic. We assess the sensitivity score on a set of\nrepresentative words in the test set using two classifiers trained for hospital\nreadmission classification with similar performance statistics. Our experiments\ncompare the decision-making logic of clinicians and classifiers based on rank\ncorrelations of sensitivity scores. The results indicate that the language\nmodel's sensitivity score aligns better with the professionals than the xgboost\nclassifier on tf-idf embeddings, which suggests that xgboost uses some spurious\nfeatures. Overall, this metric offers a novel perspective on assessing models'\nrobustness by quantifying their discrepancy with professional opinions. Our\ncode is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).",
        "pdf_link": "https://arxiv.org/pdf/2211.07047v2.pdf"
    },
    {
        "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost",
        "authors": [
            "Qingcheng Zeng",
            "Lucas Garay",
            "Peilin Zhou",
            "Dading Chong",
            "Yining Hua",
            "Jiageng Wu",
            "Yikang Pan",
            "Han Zhou",
            "Rob Voigt",
            "Jie Yang"
        ],
        "published": "2022-11-13T18:59:15Z",
        "summary": "Large pre-trained models have revolutionized natural language processing\n(NLP) research and applications, but high training costs and limited data\nresources have prevented their benefits from being shared equally amongst\nspeakers of all the world's languages. To address issues of cross-linguistic\naccess to such models and reduce energy consumption for sustainability during\nlarge-scale model training, this study proposes an effective and\nenergy-efficient framework called GreenPLM that uses bilingual lexicons to\ndirectly \"translate\" pre-trained language models of one language into another\nat almost no additional cost. We validate this approach in 18 languages' BERT\nmodels and show that this framework is comparable to, if not better than, other\nheuristics with high training costs. In addition, given lightweight continued\npre-training on limited data where available, this framework outperforms the\noriginal monolingual language models in six out of seven tested languages with\nup to 200x less pre-training efforts. Aiming at the Leave No One Behind\nPrinciple (LNOB), our approach manages to reduce inequalities between languages\nand energy consumption greatly. We make our codes and models publicly available\nhere: \\url{https://github.com/qcznlp/GreenPLMs}",
        "pdf_link": "https://arxiv.org/pdf/2211.06993v3.pdf"
    },
    {
        "title": "Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT RoBERTa Approach for Patronizing and Condescending Language Detection",
        "authors": [
            "Jinghua Xu"
        ],
        "published": "2022-11-13T10:59:45Z",
        "summary": "This paper describes my participation in the SemEval-2022 Task 4: Patronizing\nand Condescending Language Detection. I participate in both subtasks:\nPatronizing and Condescending Language (PCL) Identification and Patronizing and\nCondescending Language Categorization, with the main focus put on subtask 1.\nThe experiments compare pre-BERT neural network (NN) based systems against\npost-BERT pretrained language model RoBERTa. This research finds NN-based\nsystems in the experiments perform worse on the task compared to the pretrained\nlanguage models. The top-performing RoBERTa system is ranked 26 out of 78 teams\n(F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in\nsubtask 2.",
        "pdf_link": "https://arxiv.org/pdf/2211.06874v1.pdf"
    },
    {
        "title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters",
        "authors": [
            "Nuo Chen",
            "Yan Wang",
            "Haiyun Jiang",
            "Deng Cai",
            "Yuhan Li",
            "Ziyang Chen",
            "Longyue Wang",
            "Jia Li"
        ],
        "published": "2022-11-13T10:16:39Z",
        "summary": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.",
        "pdf_link": "https://arxiv.org/pdf/2211.06869v4.pdf"
    },
    {
        "title": "Textual Data Augmentation for Patient Outcomes Prediction",
        "authors": [
            "Qiuhao Lu",
            "Dejing Dou",
            "Thien Huu Nguyen"
        ],
        "published": "2022-11-13T01:07:23Z",
        "summary": "Deep learning models have demonstrated superior performance in various\nhealthcare applications. However, the major limitation of these deep models is\nusually the lack of high-quality training data due to the private and sensitive\nnature of this field. In this study, we propose a novel textual data\naugmentation method to generate artificial clinical notes in patients'\nElectronic Health Records (EHRs) that can be used as additional training data\nfor patient outcomes prediction. Essentially, we fine-tune the generative\nlanguage model GPT-2 to synthesize labeled text with the original training\ndata. More specifically, We propose a teacher-student framework where we first\npre-train a teacher model on the original data, and then train a student model\non the GPT-augmented data under the guidance of the teacher. We evaluate our\nmethod on the most common patient outcome, i.e., the 30-day readmission rate.\nThe experimental results show that deep models can improve their predictive\nperformance with the augmented data, indicating the effectiveness of the\nproposed architecture.",
        "pdf_link": "https://arxiv.org/pdf/2211.06778v1.pdf"
    },
    {
        "title": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
        "authors": [
            "Zhongzhi Chen",
            "Guang Liu",
            "Bo-Wen Zhang",
            "Fulong Ye",
            "Qinghong Yang",
            "Ledell Wu"
        ],
        "published": "2022-11-12T14:48:55Z",
        "summary": "In this work, we present a conceptually simple and effective method to train\na strong bilingual/multilingual multimodal representation model. Starting from\nthe pre-trained multimodal representation model CLIP released by OpenAI, we\naltered its text encoder with a pre-trained multilingual text encoder XLM-R,\nand aligned both languages and image representations by a two-stage training\nschema consisting of teacher learning and contrastive learning. We validate our\nmethod through evaluations of a wide range of tasks. We set new\nstate-of-the-art performances on a bunch of tasks including ImageNet-CN,\nFlicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder\nin CLIP for extended capabilities such as multilingual understanding. Our\nmodels and code are available at https://github.com/FlagAI-Open/FlagAI.",
        "pdf_link": "https://arxiv.org/pdf/2211.06679v2.pdf"
    },
    {
        "title": "Dark patterns in e-commerce: a dataset and its baseline evaluations",
        "authors": [
            "Yuki Yada",
            "Jiaying Feng",
            "Tsuneo Matsumoto",
            "Nao Fukushima",
            "Fuyuko Kido",
            "Hayato Yamana"
        ],
        "published": "2022-11-12T01:53:49Z",
        "summary": "Dark patterns, which are user interface designs in online services, induce\nusers to take unintended actions. Recently, dark patterns have been raised as\nan issue of privacy and fairness. Thus, a wide range of research on detecting\ndark patterns is eagerly awaited. In this work, we constructed a dataset for\ndark pattern detection and prepared its baseline detection performance with\nstate-of-the-art machine learning methods. The original dataset was obtained\nfrom Mathur et al.'s study in 2019, which consists of 1,818 dark pattern texts\nfrom shopping sites. Then, we added negative samples, i.e., non-dark pattern\ntexts, by retrieving texts from the same websites as Mathur et al.'s dataset.\nWe also applied state-of-the-art machine learning methods to show the automatic\ndetection accuracy as baselines, including BERT, RoBERTa, ALBERT, and XLNet. As\na result of 5-fold cross-validation, we achieved the highest accuracy of 0.975\nwith RoBERTa. The dataset and baseline source codes are available at\nhttps://github.com/yamanalab/ec-darkpattern.",
        "pdf_link": "https://arxiv.org/pdf/2211.06543v1.pdf"
    },
    {
        "title": "The Architectural Bottleneck Principle",
        "authors": [
            "Tiago Pimentel",
            "Josef Valvoda",
            "Niklas Stoehr",
            "Ryan Cotterell"
        ],
        "published": "2022-11-11T18:58:08Z",
        "summary": "In this paper, we seek to measure how much information a component in a\nneural network could extract from the representations fed into it. Our work\nstands in contrast to prior probing work, most of which investigates how much\ninformation a model's representations contain. This shift in perspective leads\nus to propose a new principle for probing, the architectural bottleneck\nprinciple: In order to estimate how much information a given component could\nextract, a probe should look exactly like the component. Relying on this\nprinciple, we estimate how much syntactic information is available to\ntransformers through our attentional probe, a probe that exactly resembles a\ntransformer's self-attention head. Experimentally, we find that, in three\nmodels (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly\nextractable by our probe, suggesting these models have access to syntactic\ninformation while composing their contextual representations. Whether this\ninformation is actually used by these models, however, remains an open\nquestion.",
        "pdf_link": "https://arxiv.org/pdf/2211.06420v1.pdf"
    },
    {
        "title": "Controlling Commercial Cooling Systems Using Reinforcement Learning",
        "authors": [
            "Jerry Luo",
            "Cosmin Paduraru",
            "Octavian Voicu",
            "Yuri Chervonyi",
            "Scott Munns",
            "Jerry Li",
            "Crystal Qian",
            "Praneet Dutta",
            "Jared Quincy Davis",
            "Ningjia Wu",
            "Xingwei Yang",
            "Chu-Ming Chang",
            "Ted Li",
            "Rob Rose",
            "Mingyan Fan",
            "Hootan Nakhost",
            "Tinglin Liu",
            "Brian Kirkman",
            "Frank Altamura",
            "Lee Cline",
            "Patrick Tonker",
            "Joel Gouker",
            "Dave Uden",
            "Warren Buddy Bryan",
            "Jason Law",
            "Deeni Fatiha",
            "Neil Satra",
            "Juliet Rothenberg",
            "Mandeep Waraich",
            "Molly Carlin",
            "Satish Tallapaka",
            "Sims Witherspoon",
            "David Parish",
            "Peter Dolan",
            "Chenyu Zhao",
            "Daniel J. Mankowitz"
        ],
        "published": "2022-11-11T17:48:13Z",
        "summary": "This paper is a technical overview of DeepMind and Google's recent work on\nreinforcement learning for controlling commercial cooling systems. Building on\nexpertise that began with cooling Google's data centers more efficiently, we\nrecently conducted live experiments on two real-world facilities in partnership\nwith Trane Technologies, a building management system provider. These live\nexperiments had a variety of challenges in areas such as evaluation, learning\nfrom offline data, and constraint satisfaction. Our paper describes these\nchallenges in the hope that awareness of them will benefit future applied RL\nwork. We also describe the way we adapted our RL system to deal with these\nchallenges, resulting in energy savings of approximately 9% and 13%\nrespectively at the two live experiment sites.",
        "pdf_link": "https://arxiv.org/pdf/2211.07357v2.pdf"
    },
    {
        "title": "Improving word mover's distance by leveraging self-attention matrix",
        "authors": [
            "Hiroaki Yamagiwa",
            "Sho Yokoi",
            "Hidetoshi Shimodaira"
        ],
        "published": "2022-11-11T14:25:08Z",
        "summary": "Measuring the semantic similarity between two sentences is still an important\ntask. The word mover's distance (WMD) computes the similarity via the optimal\nalignment between the sets of word embeddings. However, WMD does not utilize\nword order, making it challenging to distinguish sentences with significant\noverlaps of similar words, even if they are semantically very different. Here,\nwe attempt to improve WMD by incorporating the sentence structure represented\nby BERT's self-attention matrix (SAM). The proposed method is based on the\nFused Gromov-Wasserstein distance, which simultaneously considers the\nsimilarity of the word embedding and the SAM for calculating the optimal\ntransport between two sentences. Experiments demonstrate the proposed method\nenhances WMD and its variants in paraphrase identification with near-equivalent\nperformance in semantic textual similarity. Our code is available at\n\\url{https://github.com/ymgw55/WSMD}.",
        "pdf_link": "https://arxiv.org/pdf/2211.06229v2.pdf"
    },
    {
        "title": "DocuT5: Seq2seq SQL Generation with Table Documentation",
        "authors": [
            "Elena Soare",
            "Iain Mackie",
            "Jeffrey Dalton"
        ],
        "published": "2022-11-11T13:31:55Z",
        "summary": "Current SQL generators based on pre-trained language models struggle to\nanswer complex questions requiring domain context or understanding fine-grained\ntable structure. Humans would deal with these unknowns by reasoning over the\ndocumentation of the tables. Based on this hypothesis, we propose DocuT5, which\nuses off-the-shelf language model architecture and injects knowledge from\nexternal `documentation' to improve domain generalization. We perform\nexperiments on the Spider family of datasets that contain complex questions\nthat are cross-domain and multi-table. Specifically, we develop a new\ntext-to-SQL failure taxonomy and find that 19.6% of errors are due to foreign\nkey mistakes, and 49.2% are due to a lack of domain knowledge. We proposed\nDocuT5, a method that captures knowledge from (1) table structure context of\nforeign keys and (2) domain knowledge through contextualizing tables and\ncolumns. Both types of knowledge improve over state-of-the-art T5 with\nconstrained decoding on Spider, and domain knowledge produces state-of-the-art\ncomparable effectiveness on Spider-DK and Spider-SYN datasets.",
        "pdf_link": "https://arxiv.org/pdf/2211.06193v1.pdf"
    },
    {
        "title": "Towards automating Numerical Consistency Checks in Financial Reports",
        "authors": [
            "Lars Hillebrand",
            "Tobias Deu\u00dfer",
            "Tim Dilmaghani",
            "Bernd Kliem",
            "R\u00fcdiger Loitz",
            "Christian Bauckhage",
            "Rafet Sifa"
        ],
        "published": "2022-11-11T10:35:07Z",
        "summary": "We introduce KPI-Check, a novel system that automatically identifies and\ncross-checks semantically equivalent key performance indicators (KPIs), e.g.\n\"revenue\" or \"total costs\", in real-world German financial reports. It combines\na financial named entity and relation extraction module with a BERT-based\nfiltering and text pair classification component to extract KPIs from\nunstructured sentences before linking them to synonymous occurrences in the\nbalance sheet and profit & loss statement. The tool achieves a high matching\nperformance of $73.00$% micro F$_1$ on a hold out test set and is currently\nbeing deployed for a globally operating major auditing firm to assist the\nauditing procedure of financial statements.",
        "pdf_link": "https://arxiv.org/pdf/2211.06112v1.pdf"
    },
    {
        "title": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation",
        "authors": [
            "Danial Kamali",
            "Joseph Romain",
            "Huiyi Liu",
            "Wei Peng",
            "Jingbo Meng",
            "Parisa Kordjamshidi"
        ],
        "published": "2022-11-11T03:26:37Z",
        "summary": "Nowadays, the spread of misinformation is a prominent problem in society. Our\nresearch focuses on aiding the automatic identification of misinformation by\nanalyzing the persuasive strategies employed in textual documents. We introduce\na novel annotation scheme encompassing common persuasive writing tactics to\nachieve our objective. Additionally, we provide a dataset on health\nmisinformation, thoroughly annotated by experts utilizing our proposed scheme.\nOur contribution includes proposing a new task of annotating pieces of text\nwith their persuasive writing strategy types. We evaluate fine-tuning and\nprompt-engineering techniques with pre-trained language models of the BERT\nfamily and the generative large language models of the GPT family using\npersuasive strategies as an additional source of information. We evaluate the\neffects of employing persuasive strategies as intermediate labels in the\ncontext of misinformation detection. Our results show that those strategies\nenhance accuracy and improve the explainability of misinformation detection\nmodels. The persuasive strategies can serve as valuable insights and\nexplanations, enabling other models or even humans to make more informed\ndecisions regarding the trustworthiness of the information.",
        "pdf_link": "https://arxiv.org/pdf/2211.05985v4.pdf"
    },
    {
        "title": "pyRDDLGym: From RDDL to Gym Environments",
        "authors": [
            "Ayal Taitler",
            "Michael Gimelfarb",
            "Jihwan Jeong",
            "Sriram Gopalakrishnan",
            "Martin Mladenov",
            "Xiaotian Liu",
            "Scott Sanner"
        ],
        "published": "2022-11-11T00:58:16Z",
        "summary": "We present pyRDDLGym, a Python framework for auto-generation of OpenAI Gym\nenvironments from RDDL declerative description. The discrete time step\nevolution of variables in RDDL is described by conditional probability\nfunctions, which fits naturally into the Gym step scheme. Furthermore, since\nRDDL is a lifted description, the modification and scaling up of environments\nto support multiple entities and different configurations becomes trivial\nrather than a tedious process prone to errors. We hope that pyRDDLGym will\nserve as a new wind in the reinforcement learning community by enabling easy\nand rapid development of benchmarks due to the unique expressive power of RDDL.\nBy providing explicit access to the model in the RDDL description, pyRDDLGym\ncan also facilitate research on hybrid approaches for learning from interaction\nwhile leveraging model knowledge. We present the design and built-in examples\nof pyRDDLGym, and the additions made to the RDDL language that were\nincorporated into the framework.",
        "pdf_link": "https://arxiv.org/pdf/2211.05939v5.pdf"
    },
    {
        "title": "Steps towards prompt-based creation of virtual worlds",
        "authors": [
            "Jasmine Roberts",
            "Andrzej Banburski-Fahey",
            "Jaron Lanier"
        ],
        "published": "2022-11-10T21:13:04Z",
        "summary": "Large language models trained for code generation can be applied to speaking\nvirtual worlds into existence (creating virtual worlds). In this work we show\nthat prompt-based methods can both accelerate in-VR level editing, as well as\ncan become part of gameplay rather than just part of game development. As an\nexample, we present Codex VR Pong which shows non-deterministic game mechanics\nusing generative processes to not only create static content but also\nnon-trivial interactions between 3D objects. This demonstration naturally leads\nto an integral discussion on how one would evaluate and benchmark experiences\ncreated by generative models - as there are no qualitative or quantitative\nmetrics that apply in these scenarios. We conclude by discussing impending\nchallenges of AI-assisted co-creation in VR.",
        "pdf_link": "https://arxiv.org/pdf/2211.05875v1.pdf"
    },
    {
        "title": "Measuring Reliability of Large Language Models through Semantic Consistency",
        "authors": [
            "Harsh Raj",
            "Domenic Rosati",
            "Subhabrata Majumdar"
        ],
        "published": "2022-11-10T20:21:07Z",
        "summary": "While large pretrained language models (PLMs) demonstrate incredible fluency\nand performance on many natural language tasks, recent work has shown that\nwell-performing PLMs are very sensitive to what prompts are feed into them.\nEven when prompts are semantically identical, language models may give very\ndifferent answers. When considering safe and trustworthy deployments of PLMs we\nwould like their outputs to be consistent under prompts that mean the same\nthing or convey the same intent. While some work has looked into how\nstate-of-the-art PLMs address this need, they have been limited to only\nevaluating lexical equality of single- or multi-word answers and do not address\nconsistency of generative text sequences. In order to understand consistency of\nPLMs under text generation settings, we develop a measure of semantic\nconsistency that allows the comparison of open-ended text outputs. We implement\nseveral versions of this consistency metric to evaluate the performance of a\nnumber of PLMs on paraphrased versions of questions in the TruthfulQA dataset,\nwe find that our proposed metrics are considerably more consistent than\ntraditional metrics embodying lexical consistency, and also correlate with\nhuman evaluation of output consistency to a higher degree.",
        "pdf_link": "https://arxiv.org/pdf/2211.05853v2.pdf"
    },
    {
        "title": "The CRINGE Loss: Learning what language not to model",
        "authors": [
            "Leonard Adolphs",
            "Tianyu Gao",
            "Jing Xu",
            "Kurt Shuster",
            "Sainbayar Sukhbaatar",
            "Jason Weston"
        ],
        "published": "2022-11-10T19:30:08Z",
        "summary": "Standard language model training employs gold human documents or human-human\ninteraction data, and treats all training data as positive examples. Growing\nevidence shows that even with very large amounts of positive training data,\nissues remain that can be alleviated with relatively small amounts of negative\ndata -- examples of what the model should not do. In this work, we propose a\nnovel procedure to train with such data called the CRINGE loss (ContRastive\nIterative Negative GEneration). We show the effectiveness of this approach\nacross three different experiments on the tasks of safe generation,\ncontradiction avoidance, and open-domain dialogue. Our models outperform\nmultiple strong baselines and are conceptually simple, easy to train and\nimplement.",
        "pdf_link": "https://arxiv.org/pdf/2211.05826v1.pdf"
    },
    {
        "title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control",
        "authors": [
            "Xiang Fan",
            "Yiwei Lyu",
            "Paul Pu Liang",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ],
        "published": "2022-11-10T18:31:56Z",
        "summary": "Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2211.05750v3.pdf"
    },
    {
        "title": "BERT in Plutarch's Shadows",
        "authors": [
            "Ivan P. Yamshchikov",
            "Alexey Tikhonov",
            "Yorgos Pantis",
            "Charlotte Schubert",
            "J\u00fcrgen Jost"
        ],
        "published": "2022-11-10T16:21:42Z",
        "summary": "The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea\n(ca. 45-120 CE) also contains several texts which, according to current\nscholarly opinion, did not originate with him and are therefore attributed to\nan anonymous author Pseudo-Plutarch. These include, in particular, the work\nPlacita Philosophorum (Quotations and Opinions of the Ancient Philosophers),\nwhich is extremely important for the history of ancient philosophy. Little is\nknown about the identity of that anonymous author and its relation to other\nauthors from the same period. This paper presents a BERT language model for\nAncient Greek. The model discovers previously unknown statistical properties\nrelevant to these literary, philosophical, and historical problems and can shed\nnew light on this authorship question. In particular, the Placita\nPhilosophorum, together with one of the other Pseudo-Plutarch texts, shows\nsimilarities with the texts written by authors from an Alexandrian context\n(2nd/3rd century CE).",
        "pdf_link": "https://arxiv.org/pdf/2211.05673v1.pdf"
    },
    {
        "title": "BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning",
        "authors": [
            "Mohsen Fayyaz",
            "Ehsan Aghazadeh",
            "Ali Modarressi",
            "Mohammad Taher Pilehvar",
            "Yadollah Yaghoobzadeh",
            "Samira Ebrahimi Kahou"
        ],
        "published": "2022-11-10T14:37:23Z",
        "summary": "Current pre-trained language models rely on large datasets for achieving\nstate-of-the-art performance. However, past research has shown that not all\nexamples in a dataset are equally important during training. In fact, it is\nsometimes possible to prune a considerable fraction of the training set while\nmaintaining the test performance. Established on standard vision benchmarks,\ntwo gradient-based scoring metrics for finding important examples are GraNd and\nits estimated version, EL2N. In this work, we employ these two metrics for the\nfirst time in NLP. We demonstrate that these metrics need to be computed after\nat least one epoch of fine-tuning and they are not reliable in early steps.\nFurthermore, we show that by pruning a small portion of the examples with the\nhighest GraNd/EL2N scores, we can not only preserve the test accuracy, but also\nsurpass it. This paper details adjustments and implementation choices which\nenable GraNd and EL2N to be applied to NLP.",
        "pdf_link": "https://arxiv.org/pdf/2211.05610v2.pdf"
    },
    {
        "title": "Prompt Learning for Domain Adaptation in Task-Oriented Dialogue",
        "authors": [
            "Makesh Narsimhan Sreedhar",
            "Christopher Parisien"
        ],
        "published": "2022-11-10T14:16:00Z",
        "summary": "Conversation designers continue to face significant obstacles when creating\nproduction quality task-oriented dialogue systems. The complexity and cost\ninvolved in schema development and data collection is often a major barrier for\nsuch designers, limiting their ability to create natural, user-friendly\nexperiences. We frame the classification of user intent as the generation of a\ncanonical form, a lightweight semantic representation using natural language.\nWe show that canonical forms offer a promising alternative to traditional\nmethods for intent classification. By tuning soft prompts for a frozen large\nlanguage model, we show that canonical forms generalize very well to new,\nunseen domains in a zero- or few-shot setting. The method is also\nsample-efficient, reducing the complexity and effort of developing new\ntask-oriented dialogue domains.",
        "pdf_link": "https://arxiv.org/pdf/2211.05596v1.pdf"
    },
    {
        "title": "PAD-Net: An Efficient Framework for Dynamic Networks",
        "authors": [
            "Shwai He",
            "Liang Ding",
            "Daize Dong",
            "Boan Liu",
            "Fuqiang Yu",
            "Dacheng Tao"
        ],
        "published": "2022-11-10T12:42:43Z",
        "summary": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of\nExperts (MoE), have been extensively explored as they can considerably improve\nthe model's representation power with acceptable computational cost. The common\npractice in implementing dynamic networks is to convert the given static layers\ninto fully dynamic ones where all parameters are dynamic (at least within a\nsingle layer) and vary with the input. However, such a fully dynamic setting\nmay cause redundant parameters and high deployment costs, limiting the\napplicability of dynamic networks to a broader range of tasks and models. The\nmain contributions of our work are challenging the basic commonsense in dynamic\nnetworks and proposing a partially dynamic network, namely PAD-Net, to\ntransform the redundant dynamic parameters into static ones. Also, we further\ndesign Iterative Mode Partition to partition dynamic and static parameters\nefficiently. Our method is comprehensively supported by large-scale experiments\nwith two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both\nimage classification and GLUE benchmarks. Encouragingly, we surpass the fully\ndynamic networks by $+0.7\\%$ top-1 acc with only $30\\%$ dynamic parameters for\nResNet-50 and $+1.9\\%$ average score in language understanding with only $50\\%$\ndynamic parameters for BERT. Code will be released at:\n\\url{https://github.com/Shwai-He/PAD-Net}.",
        "pdf_link": "https://arxiv.org/pdf/2211.05528v4.pdf"
    },
    {
        "title": "Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis",
        "authors": [
            "Anguo Dong",
            "Cuiyun Gao",
            "Yan Jia",
            "Qing Liao",
            "Xuan Wang",
            "Lei Wang",
            "Jing Xiao"
        ],
        "published": "2022-11-10T10:09:33Z",
        "summary": "Aspect-based sentiment analysis (ABSA) aims at extracting opinionated aspect\nterms in review texts and determining their sentiment polarities, which is\nwidely studied in both academia and industry. As a fine-grained classification\ntask, the annotation cost is extremely high. Domain adaptation is a popular\nsolution to alleviate the data deficiency issue in new domains by transferring\ncommon knowledge across domains. Most cross-domain ABSA studies are based on\nstructure correspondence learning (SCL), and use pivot features to construct\nauxiliary tasks for narrowing down the gap between domains. However, their\npivot-based auxiliary tasks can only transfer knowledge of aspect terms but not\nsentiment, limiting the performance of existing models. In this work, we\npropose a novel Syntax-guided Domain Adaptation Model, named SDAM, for more\neffective cross-domain ABSA. SDAM exploits syntactic structure similarities for\nbuilding pseudo training instances, during which aspect terms of target domain\nare explicitly related to sentiment polarities. Besides, we propose a\nsyntax-based BERT mask language model for further capturing domain-invariant\nfeatures. Finally, to alleviate the sentiment inconsistency issue in multi-gram\naspect terms, we introduce a span-based joint aspect term and sentiment\nanalysis module into the cross-domain End2End ABSA. Experiments on five\nbenchmark datasets show that our model consistently outperforms the\nstate-of-the-art baselines with respect to Micro-F1 metric for the cross-domain\nEnd2End ABSA task.",
        "pdf_link": "https://arxiv.org/pdf/2211.05457v2.pdf"
    },
    {
        "title": "ADEPT: A DEbiasing PrompT Framework",
        "authors": [
            "Ke Yang",
            "Charles Yu",
            "Yi Fung",
            "Manling Li",
            "Heng Ji"
        ],
        "published": "2022-11-10T08:41:40Z",
        "summary": "Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects.",
        "pdf_link": "https://arxiv.org/pdf/2211.05414v2.pdf"
    },
    {
        "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
        "authors": [
            "Evangelia Spiliopoulou",
            "Artidoro Pagnoni",
            "Yonatan Bisk",
            "Eduard Hovy"
        ],
        "published": "2022-11-10T07:48:01Z",
        "summary": "This paper investigates models of event implications. Specifically, how well\nmodels predict entity state-changes, by targeting their understanding of\nphysical attributes. Nominally, Large Language models (LLM) have been exposed\nto procedural knowledge about how objects interact, yet our benchmarking shows\nthey fail to reason about the world. Conversely, we also demonstrate that\nexisting approaches often misrepresent the surprising abilities of LLMs via\nimproper task encodings and that proper model prompting can dramatically\nimprove performance of reported baseline results across multiple tasks. In\nparticular, our results indicate that our prompting technique is especially\nuseful for unseen attributes (out-of-domain) or when only limited data is\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2211.05392v1.pdf"
    },
    {
        "title": "MSDT: Masked Language Model Scoring Defense in Text Domain",
        "authors": [
            "Jaechul Roh",
            "Minhao Cheng",
            "Yajun Fang"
        ],
        "published": "2022-11-10T06:46:47Z",
        "summary": "Pre-trained language models allowed us to process downstream tasks with the\nhelp of fine-tuning, which aids the model to achieve fairly high accuracy in\nvarious Natural Language Processing (NLP) tasks. Such easily-downloaded\nlanguage models from various websites empowered the public users as well as\nsome major institutions to give a momentum to their real-life application.\nHowever, it was recently proven that models become extremely vulnerable when\nthey are backdoor attacked with trigger-inserted poisoned datasets by malicious\nusers. The attackers then redistribute the victim models to the public to\nattract other users to use them, where the models tend to misclassify when\ncertain triggers are detected within the training sample. In this paper, we\nwill introduce a novel improved textual backdoor defense method, named MSDT,\nthat outperforms the current existing defensive algorithms in specific\ndatasets. The experimental results illustrate that our method can be effective\nand constructive in terms of defending against backdoor attack in text domain.\nCode is available at https://github.com/jcroh0508/MSDT.",
        "pdf_link": "https://arxiv.org/pdf/2211.05371v1.pdf"
    },
    {
        "title": "LERT: A Linguistically-motivated Pre-trained Language Model",
        "authors": [
            "Yiming Cui",
            "Wanxiang Che",
            "Shijin Wang",
            "Ting Liu"
        ],
        "published": "2022-11-10T05:09:16Z",
        "summary": "Pre-trained Language Model (PLM) has become a representative foundation model\nin the natural language processing field. Most PLMs are trained with\nlinguistic-agnostic pre-training tasks on the surface form of the text, such as\nthe masked language model (MLM). To further empower the PLMs with richer\nlinguistic features, in this paper, we aim to propose a simple but effective\nway to learn linguistic features for pre-trained language models. We propose\nLERT, a pre-trained language model that is trained on three types of linguistic\nfeatures along with the original MLM pre-training task, using a\nlinguistically-informed pre-training (LIP) strategy. We carried out extensive\nexperiments on ten Chinese NLU tasks, and the experimental results show that\nLERT could bring significant improvements over various comparable baselines.\nFurthermore, we also conduct analytical experiments in various linguistic\naspects, and the results prove that the design of LERT is valid and effective.\nResources are available at https://github.com/ymcui/LERT",
        "pdf_link": "https://arxiv.org/pdf/2211.05344v1.pdf"
    },
    {
        "title": "On Optimizing the Communication of Model Parallelism",
        "authors": [
            "Yonghao Zhuang",
            "Hexu Zhao",
            "Lianmin Zheng",
            "Zhuohan Li",
            "Eric P. Xing",
            "Qirong Ho",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Hao Zhang"
        ],
        "published": "2022-11-10T03:56:48Z",
        "summary": "We study a novel and important communication pattern in large-scale\nmodel-parallel deep learning (DL), which we call cross-mesh resharding. This\npattern emerges when the two paradigms of model parallelism - intra-operator\nand inter-operator parallelism - are combined to support large models on large\nclusters. In cross-mesh resharding, a sharded tensor needs to be sent from a\nsource device mesh to a destination device mesh, on which the tensor may be\ndistributed with the same or different layouts. We formalize this as a\nmany-to-many multicast communication problem, and show that existing approaches\neither are sub-optimal or do not generalize to different network topologies or\ntensor layouts, which result from different model architectures and parallelism\nstrategies. We then propose two contributions to address cross-mesh resharding:\nan efficient broadcast-based communication system, and an\n\"overlapping-friendly\" pipeline schedule. On microbenchmarks, our overall\nsystem outperforms existing ones by up to 10x across various tensor and mesh\nlayouts. On end-to-end training of two large models, GPT-3 and U-Transformer,\nwe improve throughput by 10% and 50%, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.05322v1.pdf"
    },
    {
        "title": "FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information",
        "authors": [
            "Yijia Shao",
            "Mengyu Zhou",
            "Yifan Zhong",
            "Tao Wu",
            "Hongwei Han",
            "Shi Han",
            "Gideon Huang",
            "Dongmei Zhang"
        ],
        "published": "2022-11-10T01:32:55Z",
        "summary": "Online forms are widely used to collect data from human and have a\nmulti-billion market. Many software products provide online services for\ncreating semi-structured forms where questions and descriptions are organized\nby pre-defined structures. However, the design and creation process of forms is\nstill tedious and requires expert knowledge. To assist form designers, in this\nwork we present FormLM to model online forms (by enhancing pre-trained language\nmodel with form structural information) and recommend form creation ideas\n(including question / options recommendations and block type suggestion). For\nmodel training and evaluation, we collect the first public online form dataset\nwith 62K online forms. Experiment results show that FormLM significantly\noutperforms general-purpose language models on all tasks, with an improvement\nby 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms\nof ROUGE-1 and Macro-F1, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.05284v1.pdf"
    },
    {
        "title": "BERT-Based Combination of Convolutional and Recurrent Neural Network for Indonesian Sentiment Analysis",
        "authors": [
            "Hendri Murfi",
            "Syamsyuriani",
            "Theresia Gowandi",
            "Gianinna Ardaneswari",
            "Siti Nurrohmah"
        ],
        "published": "2022-11-10T00:32:40Z",
        "summary": "Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.",
        "pdf_link": "https://arxiv.org/pdf/2211.05273v1.pdf"
    },
    {
        "title": "Training self-supervised peptide sequence models on artificially chopped proteins",
        "authors": [
            "Gil Sadeh",
            "Zichen Wang",
            "Jasleen Grewal",
            "Huzefa Rangwala",
            "Layne Price"
        ],
        "published": "2022-11-09T22:22:17Z",
        "summary": "Representation learning for proteins has primarily focused on the global\nunderstanding of protein sequences regardless of their length. However, shorter\nproteins (known as peptides) take on distinct structures and functions compared\nto their longer counterparts. Unfortunately, there are not as many naturally\noccurring peptides available to be sequenced and therefore less\npeptide-specific data to train with. In this paper, we propose a new peptide\ndata augmentation scheme, where we train peptide language models on\nartificially constructed peptides that are small contiguous subsets of longer,\nwild-type proteins; we refer to the training peptides as \"chopped proteins\". We\nevaluate the representation potential of models trained with chopped proteins\nversus natural peptides and find that training language models with chopped\nproteins results in more generalized embeddings for short protein sequences.\nThese peptide-specific models also retain information about the original\nprotein they were derived from better than language models trained on\nfull-length proteins. We compare masked language model training objectives to\nthree novel peptide-specific training objectives: next-peptide prediction,\ncontrastive peptide selection and evolution-weighted MLM. We demonstrate\nimproved zero-shot learning performance for a deep mutational scan peptides\nbenchmark.",
        "pdf_link": "https://arxiv.org/pdf/2211.06428v1.pdf"
    },
    {
        "title": "Collateral facilitation in humans and language models",
        "authors": [
            "James A. Michaelov",
            "Benjamin K. Bergen"
        ],
        "published": "2022-11-09T21:08:08Z",
        "summary": "Are the predictions of humans and language models affected by similar things?\nResearch suggests that while comprehending language, humans make predictions\nabout upcoming words, with more predictable words being processed more easily.\nHowever, evidence also shows that humans display a similar processing advantage\nfor highly anomalous words when these words are semantically related to the\npreceding context or to the most probable continuation. Using stimuli from 3\npsycholinguistic experiments, we find that this is also almost always also the\ncase for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa,\nXLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of\nthis phenomenon for our understanding of both human language comprehension and\nthe predictions made by language models.",
        "pdf_link": "https://arxiv.org/pdf/2211.05198v1.pdf"
    },
    {
        "title": "Large Language Models with Controllable Working Memory",
        "authors": [
            "Daliang Li",
            "Ankit Singh Rawat",
            "Manzil Zaheer",
            "Xin Wang",
            "Michal Lukasik",
            "Andreas Veit",
            "Felix Yu",
            "Sanjiv Kumar"
        ],
        "published": "2022-11-09T18:58:29Z",
        "summary": "Large language models (LLMs) have led to a series of breakthroughs in natural\nlanguage processing (NLP), owing to their excellent understanding and\ngeneration abilities. Remarkably, what further sets these models apart is the\nmassive amounts of world knowledge they internalize during pretraining. While\nmany downstream applications provide the model with an informational context to\naid its performance on the underlying task, how the model's world knowledge\ninteracts with the factual information presented in the context remains under\nexplored. As a desirable behavior, an LLM should give precedence to the context\nwhenever it contains task-relevant information that conflicts with the model's\nmemorized knowledge. This enables model predictions to be grounded in the\ncontext, which can then be used to update or correct specific model predictions\nwithout frequent retraining. By contrast, when the context is irrelevant to the\ntask, the model should ignore it and fall back on its internal knowledge. In\nthis paper, we undertake a first joint study of the aforementioned two\nproperties, namely controllability and robustness, in the context of LLMs. We\ndemonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)\ncould exhibit poor controllability and robustness, which do not scale with\nincreasing model size. As a solution, we propose a novel method - Knowledge\nAware FineTuning (KAFT) - to strengthen both controllability and robustness by\nincorporating counterfactual and irrelevant contexts to standard supervised\ndatasets. Our comprehensive evaluation showcases the utility of KAFT across\nmodel architectures and sizes.",
        "pdf_link": "https://arxiv.org/pdf/2211.05110v1.pdf"
    },
    {
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
        "authors": [
            "BigScience Workshop",
            ":",
            "Teven Le Scao",
            "Angela Fan",
            "Christopher Akiki",
            "Ellie Pavlick",
            "Suzana Ili\u0107",
            "Daniel Hesslow",
            "Roman Castagn\u00e9",
            "Alexandra Sasha Luccioni",
            "Fran\u00e7ois Yvon",
            "Matthias Gall\u00e9",
            "Jonathan Tow",
            "Alexander M. Rush",
            "Stella Biderman",
            "Albert Webson",
            "Pawan Sasanka Ammanamanchi",
            "Thomas Wang",
            "Beno\u00eet Sagot",
            "Niklas Muennighoff",
            "Albert Villanova del Moral",
            "Olatunji Ruwase",
            "Rachel Bawden",
            "Stas Bekman",
            "Angelina McMillan-Major",
            "Iz Beltagy",
            "Huu Nguyen",
            "Lucile Saulnier",
            "Samson Tan",
            "Pedro Ortiz Suarez",
            "Victor Sanh",
            "Hugo Lauren\u00e7on",
            "Yacine Jernite",
            "Julien Launay",
            "Margaret Mitchell",
            "Colin Raffel",
            "Aaron Gokaslan",
            "Adi Simhi",
            "Aitor Soroa",
            "Alham Fikri Aji",
            "Amit Alfassy",
            "Anna Rogers",
            "Ariel Kreisberg Nitzav",
            "Canwen Xu",
            "Chenghao Mou",
            "Chris Emezue",
            "Christopher Klamm",
            "Colin Leong",
            "Daniel van Strien",
            "David Ifeoluwa Adelani",
            "Dragomir Radev",
            "Eduardo Gonz\u00e1lez Ponferrada",
            "Efrat Levkovizh",
            "Ethan Kim",
            "Eyal Bar Natan",
            "Francesco De Toni",
            "G\u00e9rard Dupont",
            "Germ\u00e1n Kruszewski",
            "Giada Pistilli",
            "Hady Elsahar",
            "Hamza Benyamina",
            "Hieu Tran",
            "Ian Yu",
            "Idris Abdulmumin",
            "Isaac Johnson",
            "Itziar Gonzalez-Dios",
            "Javier de la Rosa",
            "Jenny Chim",
            "Jesse Dodge",
            "Jian Zhu",
            "Jonathan Chang",
            "J\u00f6rg Frohberg",
            "Joseph Tobing",
            "Joydeep Bhattacharjee",
            "Khalid Almubarak",
            "Kimbo Chen",
            "Kyle Lo",
            "Leandro Von Werra",
            "Leon Weber",
            "Long Phan",
            "Loubna Ben allal",
            "Ludovic Tanguy",
            "Manan Dey",
            "Manuel Romero Mu\u00f1oz",
            "Maraim Masoud",
            "Mar\u00eda Grandury",
            "Mario \u0160a\u0161ko",
            "Max Huang",
            "Maximin Coavoux",
            "Mayank Singh",
            "Mike Tian-Jian Jiang",
            "Minh Chien Vu",
            "Mohammad A. Jauhar",
            "Mustafa Ghaleb",
            "Nishant Subramani",
            "Nora Kassner",
            "Nurulaqilla Khamis",
            "Olivier Nguyen",
            "Omar Espejel",
            "Ona de Gibert",
            "Paulo Villegas",
            "Peter Henderson",
            "Pierre Colombo",
            "Priscilla Amuok",
            "Quentin Lhoest",
            "Rheza Harliman",
            "Rishi Bommasani",
            "Roberto Luis L\u00f3pez",
            "Rui Ribeiro",
            "Salomey Osei",
            "Sampo Pyysalo",
            "Sebastian Nagel",
            "Shamik Bose",
            "Shamsuddeen Hassan Muhammad",
            "Shanya Sharma",
            "Shayne Longpre",
            "Somaieh Nikpoor",
            "Stanislav Silberberg",
            "Suhas Pai",
            "Sydney Zink",
            "Tiago Timponi Torrent",
            "Timo Schick",
            "Tristan Thrush",
            "Valentin Danchev",
            "Vassilina Nikoulina",
            "Veronika Laippala",
            "Violette Lepercq",
            "Vrinda Prabhu",
            "Zaid Alyafeai",
            "Zeerak Talat",
            "Arun Raja",
            "Benjamin Heinzerling",
            "Chenglei Si",
            "Davut Emre Ta\u015far",
            "Elizabeth Salesky",
            "Sabrina J. Mielke",
            "Wilson Y. Lee",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Debajyoti Datta",
            "Eliza Szczechla",
            "Gunjan Chhablani",
            "Han Wang",
            "Harshit Pandey",
            "Hendrik Strobelt",
            "Jason Alan Fries",
            "Jos Rozen",
            "Leo Gao",
            "Lintang Sutawika",
            "M Saiful Bari",
            "Maged S. Al-shaibani",
            "Matteo Manica",
            "Nihal Nayak",
            "Ryan Teehan",
            "Samuel Albanie",
            "Sheng Shen",
            "Srulik Ben-David",
            "Stephen H. Bach",
            "Taewoon Kim",
            "Tali Bers",
            "Thibault Fevry",
            "Trishala Neeraj",
            "Urmish Thakker",
            "Vikas Raunak",
            "Xiangru Tang",
            "Zheng-Xin Yong",
            "Zhiqing Sun",
            "Shaked Brody",
            "Yallow Uri",
            "Hadar Tojarieh",
            "Adam Roberts",
            "Hyung Won Chung",
            "Jaesung Tae",
            "Jason Phang",
            "Ofir Press",
            "Conglong Li",
            "Deepak Narayanan",
            "Hatim Bourfoune",
            "Jared Casper",
            "Jeff Rasley",
            "Max Ryabinin",
            "Mayank Mishra",
            "Minjia Zhang",
            "Mohammad Shoeybi",
            "Myriam Peyrounette",
            "Nicolas Patry",
            "Nouamane Tazi",
            "Omar Sanseviero",
            "Patrick von Platen",
            "Pierre Cornette",
            "Pierre Fran\u00e7ois Lavall\u00e9e",
            "R\u00e9mi Lacroix",
            "Samyam Rajbhandari",
            "Sanchit Gandhi",
            "Shaden Smith",
            "St\u00e9phane Requena",
            "Suraj Patil",
            "Tim Dettmers",
            "Ahmed Baruwa",
            "Amanpreet Singh",
            "Anastasia Cheveleva",
            "Anne-Laure Ligozat",
            "Arjun Subramonian",
            "Aur\u00e9lie N\u00e9v\u00e9ol",
            "Charles Lovering",
            "Dan Garrette",
            "Deepak Tunuguntla",
            "Ehud Reiter",
            "Ekaterina Taktasheva",
            "Ekaterina Voloshina",
            "Eli Bogdanov",
            "Genta Indra Winata",
            "Hailey Schoelkopf",
            "Jan-Christoph Kalo",
            "Jekaterina Novikova",
            "Jessica Zosa Forde",
            "Jordan Clive",
            "Jungo Kasai",
            "Ken Kawamura",
            "Liam Hazan",
            "Marine Carpuat",
            "Miruna Clinciu",
            "Najoung Kim",
            "Newton Cheng",
            "Oleg Serikov",
            "Omer Antverg",
            "Oskar van der Wal",
            "Rui Zhang",
            "Ruochen Zhang",
            "Sebastian Gehrmann",
            "Shachar Mirkin",
            "Shani Pais",
            "Tatiana Shavrina",
            "Thomas Scialom",
            "Tian Yun",
            "Tomasz Limisiewicz",
            "Verena Rieser",
            "Vitaly Protasov",
            "Vladislav Mikhailov",
            "Yada Pruksachatkun",
            "Yonatan Belinkov",
            "Zachary Bamberger",
            "Zden\u011bk Kasner",
            "Alice Rueda",
            "Amanda Pestana",
            "Amir Feizpour",
            "Ammar Khan",
            "Amy Faranak",
            "Ana Santos",
            "Anthony Hevia",
            "Antigona Unldreaj",
            "Arash Aghagol",
            "Arezoo Abdollahi",
            "Aycha Tammour",
            "Azadeh HajiHosseini",
            "Bahareh Behroozi",
            "Benjamin Ajibade",
            "Bharat Saxena",
            "Carlos Mu\u00f1oz Ferrandis",
            "Daniel McDuff",
            "Danish Contractor",
            "David Lansky",
            "Davis David",
            "Douwe Kiela",
            "Duong A. Nguyen",
            "Edward Tan",
            "Emi Baylor",
            "Ezinwanne Ozoani",
            "Fatima Mirza",
            "Frankline Ononiwu",
            "Habib Rezanejad",
            "Hessie Jones",
            "Indrani Bhattacharya",
            "Irene Solaiman",
            "Irina Sedenko",
            "Isar Nejadgholi",
            "Jesse Passmore",
            "Josh Seltzer",
            "Julio Bonis Sanz",
            "Livia Dutra",
            "Mairon Samagaio",
            "Maraim Elbadri",
            "Margot Mieskes",
            "Marissa Gerchick",
            "Martha Akinlolu",
            "Michael McKenna",
            "Mike Qiu",
            "Muhammed Ghauri",
            "Mykola Burynok",
            "Nafis Abrar",
            "Nazneen Rajani",
            "Nour Elkott",
            "Nour Fahmy",
            "Olanrewaju Samuel",
            "Ran An",
            "Rasmus Kromann",
            "Ryan Hao",
            "Samira Alizadeh",
            "Sarmad Shubber",
            "Silas Wang",
            "Sourav Roy",
            "Sylvain Viguier",
            "Thanh Le",
            "Tobi Oyebade",
            "Trieu Le",
            "Yoyo Yang",
            "Zach Nguyen",
            "Abhinav Ramesh Kashyap",
            "Alfredo Palasciano",
            "Alison Callahan",
            "Anima Shukla",
            "Antonio Miranda-Escalada",
            "Ayush Singh",
            "Benjamin Beilharz",
            "Bo Wang",
            "Caio Brito",
            "Chenxi Zhou",
            "Chirag Jain",
            "Chuxin Xu",
            "Cl\u00e9mentine Fourrier",
            "Daniel Le\u00f3n Peri\u00f1\u00e1n",
            "Daniel Molano",
            "Dian Yu",
            "Enrique Manjavacas",
            "Fabio Barth",
            "Florian Fuhrimann",
            "Gabriel Altay",
            "Giyaseddin Bayrak",
            "Gully Burns",
            "Helena U. Vrabec",
            "Imane Bello",
            "Ishani Dash",
            "Jihyun Kang",
            "John Giorgi",
            "Jonas Golde",
            "Jose David Posada",
            "Karthik Rangasai Sivaraman",
            "Lokesh Bulchandani",
            "Lu Liu",
            "Luisa Shinzato",
            "Madeleine Hahn de Bykhovetz",
            "Maiko Takeuchi",
            "Marc P\u00e0mies",
            "Maria A Castillo",
            "Marianna Nezhurina",
            "Mario S\u00e4nger",
            "Matthias Samwald",
            "Michael Cullan",
            "Michael Weinberg",
            "Michiel De Wolf",
            "Mina Mihaljcic",
            "Minna Liu",
            "Moritz Freidank",
            "Myungsun Kang",
            "Natasha Seelam",
            "Nathan Dahlberg",
            "Nicholas Michio Broad",
            "Nikolaus Muellner",
            "Pascale Fung",
            "Patrick Haller",
            "Ramya Chandrasekhar",
            "Renata Eisenberg",
            "Robert Martin",
            "Rodrigo Canalli",
            "Rosaline Su",
            "Ruisi Su",
            "Samuel Cahyawijaya",
            "Samuele Garda",
            "Shlok S Deshmukh",
            "Shubhanshu Mishra",
            "Sid Kiblawi",
            "Simon Ott",
            "Sinee Sang-aroonsiri",
            "Srishti Kumar",
            "Stefan Schweter",
            "Sushil Bharati",
            "Tanmay Laud",
            "Th\u00e9o Gigant",
            "Tomoya Kainuma",
            "Wojciech Kusa",
            "Yanis Labrak",
            "Yash Shailesh Bajaj",
            "Yash Venkatraman",
            "Yifan Xu",
            "Yingxin Xu",
            "Yu Xu",
            "Zhe Tan",
            "Zhongli Xie",
            "Zifan Ye",
            "Mathilde Bras",
            "Younes Belkada",
            "Thomas Wolf"
        ],
        "published": "2022-11-09T18:48:09Z",
        "summary": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.",
        "pdf_link": "https://arxiv.org/pdf/2211.05100v4.pdf"
    },
    {
        "title": "Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter",
        "authors": [
            "Maram Hasanain",
            "Tamer Elsayed"
        ],
        "published": "2022-11-09T18:18:53Z",
        "summary": "Misinformation spread over social media has become an undeniable infodemic.\nHowever, not all spreading claims are made equal. If propagated, some claims\ncan be destructive, not only on the individual level, but to organizations and\neven countries. Detecting claims that should be prioritized for fact-checking\nis considered the first step to fight against spread of fake news. With\ntraining data limited to a handful of languages, developing supervised models\nto tackle the problem over lower-resource languages is currently infeasible.\nTherefore, our work aims to investigate whether we can use existing datasets to\ntrain models for predicting worthiness of verification of claims in tweets in\nother languages. We present a systematic comparative study of six approaches\nfor cross-lingual check-worthiness estimation across pairs of five diverse\nlanguages with the help of Multilingual BERT (mBERT) model. We run our\nexperiments using a state-of-the-art multilingual Twitter dataset. Our results\nshow that for some language pairs, zero-shot cross-lingual transfer is possible\nand can perform as good as monolingual models that are trained on the target\nlanguage. We also show that in some languages, this approach outperforms (or at\nleast is comparable to) state-of-the-art models.",
        "pdf_link": "https://arxiv.org/pdf/2211.05087v1.pdf"
    },
    {
        "title": "Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions",
        "authors": [
            "Michele Cafagna",
            "Kees van Deemter",
            "Albert Gatt"
        ],
        "published": "2022-11-09T15:33:51Z",
        "summary": "Image captioning models tend to describe images in an object-centric way,\nemphasising visible objects. But image descriptions can also abstract away from\nobjects and describe the type of scene depicted. In this paper, we explore the\npotential of a state-of-the-art Vision and Language model, VinVL, to caption\nimages at the scene level using (1) a novel dataset which pairs images with\nboth object-centric and scene descriptions. Through (2) an in-depth analysis of\nthe effect of the fine-tuning, we show (3) that a small amount of curated data\nsuffices to generate scene descriptions without losing the capability to\nidentify object-level concepts in the scene; the model acquires a more holistic\nview of the image compared to when object-centric descriptions are generated.\nWe discuss the parallels between these results and insights from computational\nand cognitive science research on scene perception.",
        "pdf_link": "https://arxiv.org/pdf/2211.04971v2.pdf"
    },
    {
        "title": "Sentiment Analysis of Persian Language: Review of Algorithms, Approaches and Datasets",
        "authors": [
            "Ali Nazarizadeh",
            "Touraj Banirostam",
            "Minoo Sayyadpour"
        ],
        "published": "2022-11-09T13:08:46Z",
        "summary": "Sentiment analysis aims to extract people's emotions and opinion from their\ncomments on the web. It widely used in businesses to detect sentiment in social\ndata, gauge brand reputation, and understand customers. Most of articles in\nthis area have concentrated on the English language whereas there are limited\nresources for Persian language. In this review paper, recent published articles\nbetween 2018 and 2022 in sentiment analysis in Persian Language have been\ncollected and their methods, approach and dataset will be explained and\nanalyzed. Almost all the methods used to solve sentiment analysis are machine\nlearning and deep learning. The purpose of this paper is to examine 40\ndifferent approach sentiment analysis in the Persian Language, analysis\ndatasets along with the accuracy of the algorithms applied to them and also\nreview strengths and weaknesses of each. Among all the methods, transformers\nsuch as BERT and RNN Neural Networks such as LSTM and Bi-LSTM have achieved\nhigher accuracy in the sentiment analysis. In addition to the methods and\napproaches, the datasets reviewed are listed between 2018 and 2022 and\ninformation about each dataset and its details are provided.",
        "pdf_link": "https://arxiv.org/pdf/2212.06041v1.pdf"
    },
    {
        "title": "Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition",
        "authors": [
            "Yu Chen",
            "Wen Ding",
            "Junjie Lai"
        ],
        "published": "2022-11-09T07:23:15Z",
        "summary": "Noisy Student Training (NST) has recently demonstrated extremely strong\nperformance in Automatic Speech Recognition(ASR). In this paper, we propose a\ndata selection strategy named LM Filter to improve the performance of NST on\nnon-target domain data in ASR tasks. Hypotheses with and without a Language\nModel are generated and the CER differences between them are utilized as a\nfilter threshold. Results reveal that significant improvements of 10.4%\ncompared with no data filtering baselines. We can achieve 3.31% CER in\nAISHELL-1 test set, which is best result from our knowledge without any other\nsupervised data. We also perform evaluations on the supervised 1000 hour\nAISHELL-2 dataset and competitive results of 4.73% CER can be achieved.",
        "pdf_link": "https://arxiv.org/pdf/2211.04717v2.pdf"
    },
    {
        "title": "Adaptive Multi-Corpora Language Model Training for Speech Recognition",
        "authors": [
            "Yingyi Ma",
            "Zhe Liu",
            "Xuedong Zhang"
        ],
        "published": "2022-11-09T06:54:50Z",
        "summary": "Neural network language model (NNLM) plays an essential role in automatic\nspeech recognition (ASR) systems, especially in adaptation tasks when text-only\ndata is available. In practice, an NNLM is typically trained on a combination\nof data sampled from multiple corpora. Thus, the data sampling strategy is\nimportant to the adaptation performance. Most existing works focus on designing\nstatic sampling strategies. However, each corpus may show varying impacts at\ndifferent NNLM training stages. In this paper, we introduce a novel adaptive\nmulti-corpora training algorithm that dynamically learns and adjusts the\nsampling probability of each corpus along the training process. The algorithm\nis robust to corpora sizes and domain relevance. Compared with static sampling\nstrategy baselines, the proposed approach yields remarkable improvement by\nachieving up to relative 7% and 9% word error rate (WER) reductions on\nin-domain and out-of-domain adaptation tasks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.05121v1.pdf"
    },
    {
        "title": "FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration",
        "authors": [
            "Yangjun Wu",
            "Kebin Fang",
            "Yao Zhao",
            "Hao Zhang",
            "Lifeng Shi",
            "Mengqi Zhang"
        ],
        "published": "2022-11-09T06:18:17Z",
        "summary": "To accomplish punctuation restoration, most existing methods focus on\nintroducing extra information (e.g., part-of-speech) or addressing the class\nimbalance problem. Recently, large-scale transformer-based pre-trained language\nmodels (PLMS) have been utilized widely and obtained remarkable success.\nHowever, the PLMS are trained on the large dataset with marks, which may not\nfit well with the small dataset without marks, causing the convergence to be\nnot ideal. In this study, we propose a Feature Fusion two-stream framework\n(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained\nlanguage model to capture the semantic feature, while another auxiliary module\ncaptures the feature at hand. We also modify the computation of multi-head\nattention to encourage communication among heads. Then, two features with\ndifferent perspectives are aggregated to fuse information and enhance context\nawareness. Without additional data, the experimental results on the popular\nbenchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which\nverifies that our approach is effective.",
        "pdf_link": "https://arxiv.org/pdf/2211.04699v1.pdf"
    },
    {
        "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
        "authors": [
            "Mo Yu",
            "Qiujing Wang",
            "Shunchi Zhang",
            "Yisi Sang",
            "Kangsheng Pu",
            "Zekai Wei",
            "Han Wang",
            "Liyan Xu",
            "Jing Li",
            "Yue Yu",
            "Jie Zhou"
        ],
        "published": "2022-11-09T05:06:12Z",
        "summary": "When reading a story, humans can quickly understand new fictional characters\nwith a few observations, mainly by drawing analogies to fictional and real\npeople they already know. This reflects the few-shot and meta-learning essence\nof humans' inference of characters' mental states, i.e., theory-of-mind (ToM),\nwhich is largely ignored in existing research. We fill this gap with a novel\nNLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM\nin a realistic narrative understanding scenario. Our dataset consists of ~1,000\nparsed movie scripts, each corresponding to a few-shot character understanding\ntask that requires models to mimic humans' ability of fast digesting characters\nwith a few starting scenes in a new movie.\n  We propose a novel ToM prompting approach designed to explicitly assess the\ninfluence of multiple ToM dimensions. It surpasses existing baseline models,\nunderscoring the significance of modeling multiple ToM dimensions for our task.\nOur extensive human study verifies that humans are capable of solving our\nproblem by inferring characters' mental states based on their previously seen\nmovies. In comparison, our systems based on either state-of-the-art large\nlanguage models (GPT-4) or meta-learning algorithms lags >20% behind,\nhighlighting a notable limitation in existing approaches' ToM capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2211.04684v2.pdf"
    },
    {
        "title": "Zero-Label Prompt Selection",
        "authors": [
            "Chonghua Liao",
            "Yanan Zheng",
            "Zhilin Yang"
        ],
        "published": "2022-11-09T04:13:31Z",
        "summary": "Natural language prompts have been shown to facilitate cross-task\ngeneralization for large language models. However, with no or limited labeled\nexamples, the cross-task performance is highly sensitive to the choice of\nprompts, while selecting a high-performing prompt is challenging given the\nscarcity of labels. To address the issue, we propose a Zero-Label Prompt\nSelection (ZPS) method that selects prompts without any labeled data or\ngradient update. Specifically, given the candidate human-written prompts for a\ntask, ZPS labels a set of unlabeled data with a prompt ensemble and uses the\npseudo-labels for prompt selection. Experiments show that ZPS improves over\nprior methods by a sizeable margin in zero-label performance. We also extend\nZPS to a few-shot setting and show its advantages over strong baselines such as\nprompt tuning and model tuning.",
        "pdf_link": "https://arxiv.org/pdf/2211.04668v1.pdf"
    },
    {
        "title": "Classification of Colorectal Cancer Polyps via Transfer Learning and Vision-Based Tactile Sensing",
        "authors": [
            "Nethra Venkatayogi",
            "Ozdemir Can Kara",
            "Jeff Bonyun",
            "Naruhiko Ikoma",
            "Farshid Alambeigi"
        ],
        "published": "2022-11-08T21:47:36Z",
        "summary": "In this study, to address the current high earlydetection miss rate of\ncolorectal cancer (CRC) polyps, we explore the potentials of utilizing transfer\nlearning and machine learning (ML) classifiers to precisely and sensitively\nclassify the type of CRC polyps. Instead of using the common colonoscopic\nimages, we applied three different ML algorithms on the 3D textural image\noutputs of a unique vision-based surface tactile sensor (VS-TS). To collect\nrealistic textural images of CRC polyps for training the utilized ML\nclassifiers and evaluating their performance, we first designed and additively\nmanufactured 48 types of realistic polyp phantoms with different hardness,\ntype, and textures. Next, the performance of the used three ML algorithms in\nclassifying the type of fabricated polyps was quantitatively evaluated using\nvarious statistical metrics.",
        "pdf_link": "https://arxiv.org/pdf/2211.04573v1.pdf"
    },
    {
        "title": "Active Example Selection for In-Context Learning",
        "authors": [
            "Yiming Zhang",
            "Shi Feng",
            "Chenhao Tan"
        ],
        "published": "2022-11-08T19:00:02Z",
        "summary": "With a handful of demonstration examples, large-scale language models show\nstrong capability to perform various tasks by in-context learning from these\nexamples, without any fine-tuning. We demonstrate that in-context learning\nperformance can be highly unstable across samples of examples, indicating the\nidiosyncrasies of how language models acquire information. We formulate example\nselection for in-context learning as a sequential decision problem, and propose\na reinforcement learning algorithm for identifying generalizable policies to\nselect demonstration examples. For GPT-2, our learned policies demonstrate\nstrong abilities of generalizing to unseen tasks in training, with a $5.8\\%$\nimprovement on average. Examples selected from our learned policies can even\nachieve a small improvement on GPT-3 Ada. However, the improvement diminishes\non larger GPT-3 models, suggesting emerging capabilities of large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2211.04486v1.pdf"
    },
    {
        "title": "A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer",
        "authors": [
            "Loukas Ilias",
            "Dimitris Askounis",
            "John Psarras"
        ],
        "published": "2022-11-08T16:43:58Z",
        "summary": "Alzheimer's disease (AD) is a progressive neurological disorder, meaning that\nthe symptoms develop gradually throughout the years. It is also the main cause\nof dementia, which affects memory, thinking skills, and mental abilities.\nNowadays, researchers have moved their interest towards AD detection from\nspontaneous speech, since it constitutes a time-effective procedure. However,\nexisting state-of-the-art works proposing multimodal approaches do not take\ninto consideration the inter- and intra-modal interactions and propose early\nand late fusion approaches. To tackle these limitations, we propose deep neural\nnetworks, which can be trained in an end-to-end trainable way and capture the\ninter- and intra-modal interactions. Firstly, each audio file is converted to\nan image consisting of three channels, i.e., log-Mel spectrogram, delta, and\ndelta-delta. Next, each transcript is passed through a BERT model followed by a\ngated self-attention layer. Similarly, each image is passed through a Swin\nTransformer followed by an independent gated self-attention layer. Acoustic\nfeatures are extracted also from each audio file. Finally, the representation\nvectors from the different modalities are fed to a tensor fusion layer for\ncapturing the inter-modal interactions. Extensive experiments conducted on the\nADReSS Challenge dataset indicate that our introduced approaches obtain\nvaluable advantages over existing research initiatives reaching Accuracy and\nF1-score up to 86.25% and 85.48% respectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.04368v1.pdf"
    },
    {
        "title": "An Ensemble-based approach for assigning text to correct Harmonized system code",
        "authors": [
            "Shubham",
            "Avinash Arya",
            "Subarna Roy",
            "Sridhar Jonnala"
        ],
        "published": "2022-11-08T15:32:36Z",
        "summary": "Industries must follow government rules and regulations around the world to\nclassify products when assessing duties and taxes for international shipment.\nHarmonized System (HS) is the most standardized numerical method of classifying\ntraded products among industry classification systems. A hierarchical ensemble\nmodel comprising of Bert-transformer, NER, distance-based approaches, and\nknowledge-graphs have been developed to address scalability, coverage, ability\nto capture nuances, automation and auditing requirements when classifying\nunknown text-descriptions as per HS method.",
        "pdf_link": "https://arxiv.org/pdf/2211.04313v1.pdf"
    },
    {
        "title": "Third-Party Aligner for Neural Word Alignments",
        "authors": [
            "Jinpeng Zhang",
            "Chuanqi Dong",
            "Xiangyu Duan",
            "Yuqi Zhang",
            "Min Zhang"
        ],
        "published": "2022-11-08T12:30:08Z",
        "summary": "Word alignment is to find translationally equivalent words between source and\ntarget sentences. Previous work has demonstrated that self-training can achieve\ncompetitive word alignment results. In this paper, we propose to use word\nalignments generated by a third-party word aligner to supervise the neural word\nalignment training. Specifically, source word and target word of each word pair\naligned by the third-party aligner are trained to be close neighbors to each\nother in the contextualized embedding space when fine-tuning a pre-trained\ncross-lingual language model. Experiments on the benchmarks of various language\npairs show that our approach can surprisingly do self-correction over the\nthird-party supervision by finding more accurate word alignments and deleting\nwrong word alignments, leading to better performance than various third-party\nword aligners, including the currently best one. When we integrate all\nsupervisions from various third-party aligners, we achieve state-of-the-art\nword alignment performances, with averagely more than two points lower\nalignment error rates than the best third-party aligner. We released our code\nat https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner.",
        "pdf_link": "https://arxiv.org/pdf/2211.04198v1.pdf"
    },
    {
        "title": "Active Learning with Tabular Language Models",
        "authors": [
            "Martin Ringsquandl",
            "Aneta Koleva"
        ],
        "published": "2022-11-08T09:50:30Z",
        "summary": "Despite recent advancements in tabular language model research, real-world\napplications are still challenging. In industry, there is an abundance of\ntables found in spreadsheets, but acquisition of substantial amounts of labels\nis expensive, since only experts can annotate the often highly technical and\ndomain-specific tables. Active learning could potentially reduce labeling\ncosts, however, so far there are no works related to active learning in\nconjunction with tabular language models. In this paper we investigate\ndifferent acquisition functions in a real-world industrial tabular language\nmodel use case for sub-cell named entity recognition. Our results show that\ncell-level acquisition functions with built-in diversity can significantly\nreduce the labeling effort, while enforced table diversity is detrimental. We\nfurther see open fundamental questions concerning computational efficiency and\nthe perspective of human annotators.",
        "pdf_link": "https://arxiv.org/pdf/2211.04128v1.pdf"
    },
    {
        "title": "ABC: Adversarial Behavioral Cloning for Offline Mode-Seeking Imitation Learning",
        "authors": [
            "Eddy Hudson",
            "Ishan Durugkar",
            "Garrett Warnell",
            "Peter Stone"
        ],
        "published": "2022-11-08T04:54:54Z",
        "summary": "Given a dataset of expert agent interactions with an environment of interest,\na viable method to extract an effective agent policy is to estimate the maximum\nlikelihood policy indicated by this data. This approach is commonly referred to\nas behavioral cloning (BC). In this work, we describe a key disadvantage of BC\nthat arises due to the maximum likelihood objective function; namely that BC is\nmean-seeking with respect to the state-conditional expert action distribution\nwhen the learner's policy is represented with a Gaussian. To address this\nissue, we introduce a modified version of BC, Adversarial Behavioral Cloning\n(ABC), that exhibits mode-seeking behavior by incorporating elements of GAN\n(generative adversarial network) training. We evaluate ABC on toy domains and a\ndomain based on Hopper from the DeepMind Control suite, and show that it\noutperforms standard BC by being mode-seeking in nature.",
        "pdf_link": "https://arxiv.org/pdf/2211.04005v1.pdf"
    },
    {
        "title": "Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps",
        "authors": [
            "Hiroki Iida",
            "Naoaki Okazaki"
        ],
        "published": "2022-11-08T03:58:26Z",
        "summary": "IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.",
        "pdf_link": "https://arxiv.org/pdf/2211.03988v1.pdf"
    },
    {
        "title": "Parameter and Data Efficient Continual Pre-training for Robustness to Dialectal Variance in Arabic",
        "authors": [
            "Soumajyoti Sarkar",
            "Kaixiang Lin",
            "Sailik Sengupta",
            "Leonard Lausen",
            "Sheng Zha",
            "Saab Mansour"
        ],
        "published": "2022-11-08T02:51:57Z",
        "summary": "The use of multilingual language models for tasks in low and high-resource\nlanguages has been a success story in deep learning. In recent times, Arabic\nhas been receiving widespread attention on account of its dialectal variance.\nWhile prior research studies have tried to adapt these multilingual models for\ndialectal variants of Arabic, it still remains a challenging problem owing to\nthe lack of sufficient monolingual dialectal data and parallel translation data\nof such dialectal variants. It remains an open problem on whether the limited\ndialectical data can be used to improve the models trained in Arabic on its\ndialectal variants. First, we show that multilingual-BERT (mBERT) incrementally\npretrained on Arabic monolingual data takes less training time and yields\ncomparable accuracy when compared to our custom monolingual Arabic model and\nbeat existing models (by an avg metric of +$6.41$). We then explore two\ncontinual pre-training methods -- (1) using small amounts of dialectical data\nfor continual finetuning and (2) parallel Arabic to English data and a\nTranslation Language Modeling loss function. We show that both approaches help\nimprove performance on dialectal classification tasks ($+4.64$ avg. gain) when\nused on monolingual models.",
        "pdf_link": "https://arxiv.org/pdf/2211.03966v1.pdf"
    },
    {
        "title": "Facial Tic Detection in Untrimmed Videos of Tourette Syndrome Patients",
        "authors": [
            "Yutao Tang",
            "Benjam\u00edn B\u00e9jar",
            "Joey K. -Y. Essoe",
            "Joseph F. McGuire",
            "Ren\u00e9 Vidal"
        ],
        "published": "2022-11-07T22:59:58Z",
        "summary": "Tourette Syndrome (TS) is a behavior disorder that onsets in childhood and is\ncharacterized by the expression of involuntary movements and sounds commonly\nreferred to as tics. Behavioral therapy is the first-line treatment for\npatients with TS, and it helps patients raise awareness about tic occurrence as\nwell as develop tic inhibition strategies. However, the limited availability of\ntherapists and the difficulties for in-home follow up work limits its\neffectiveness. An automatic tic detection system that is easy to deploy could\nalleviate the difficulties of home-therapy by providing feedback to the\npatients while exercising tic awareness. In this work, we propose a novel\narchitecture (T-Net) for automatic tic detection and classification from\nuntrimmed videos. T-Net combines temporal detection and segmentation and\noperates on features that are interpretable to a clinician. We compare T-Net to\nseveral state-of-the-art systems working on deep features extracted from the\nraw videos and T-Net achieves comparable performance in terms of average\nprecision while relying on interpretable features needed in clinical practice.",
        "pdf_link": "https://arxiv.org/pdf/2211.03895v1.pdf"
    },
    {
        "title": "AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis",
        "authors": [
            "Sabyasachi Kamila",
            "Walid Magdy",
            "Sourav Dutta",
            "MingXue Wang"
        ],
        "published": "2022-11-07T19:44:42Z",
        "summary": "Aspect Based Sentiment Analysis is a dominant research area with potential\napplications in social media analytics, business, finance, and health. Prior\nworks in this area are primarily based on supervised methods, with a few\ntechniques using weak supervision limited to predicting a single aspect\ncategory per review sentence. In this paper, we present an extremely weakly\nsupervised multi-label Aspect Category Sentiment Analysis framework which does\nnot use any labelled data. We only rely on a single word per class as an\ninitial indicative information. We further propose an automatic word selection\ntechnique to choose these seed categories and sentiment words. We explore\nunsupervised language model post-training to improve the overall performance,\nand propose a multi-label generator model to generate multiple aspect\ncategory-sentiment pairs per review sentence. Experiments conducted on four\nbenchmark datasets showcase our method to outperform other weakly supervised\nbaselines by a significant margin.",
        "pdf_link": "https://arxiv.org/pdf/2211.03837v1.pdf"
    },
    {
        "title": "Retrieval augmentation of large language models for lay language generation",
        "authors": [
            "Yue Guo",
            "Wei Qiu",
            "Gondy Leroy",
            "Sheng Wang",
            "Trevor Cohen"
        ],
        "published": "2022-11-07T19:06:53Z",
        "summary": "Recent lay language generation systems have used Transformer models trained\non a parallel corpus to increase health information accessibility. However, the\napplicability of these models is constrained by the limited size and topical\nbreadth of available corpora. We introduce CELLS, the largest (63k pairs) and\nbroadest-ranging (12 journals) parallel corpus for lay language generation. The\nabstract and the corresponding lay language summary are written by domain\nexperts, assuring the quality of our dataset. Furthermore, qualitative\nevaluation of expert-authored plain language summaries has revealed background\nexplanation as a key strategy to increase accessibility. Such explanation is\nchallenging for neural models to generate because it goes beyond simplification\nby adding content absent from the source. We derive two specialized paired\ncorpora from CELLS to address key challenges in lay language generation:\ngenerating background explanations and simplifying the original abstract. We\nadopt retrieval-augmented models as an intuitive fit for the task of background\nexplanation generation, and show improvements in summary quality and simplicity\nwhile maintaining factual correctness. Taken together, this work presents the\nfirst comprehensive study of background explanation for lay language\ngeneration, paving the path for disseminating scientific knowledge to a broader\naudience. CELLS is publicly available at:\nhttps://github.com/LinguisticAnomalies/pls_retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2211.03818v2.pdf"
    },
    {
        "title": "Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy",
        "authors": [
            "Michael J. Smith",
            "James E. Geach"
        ],
        "published": "2022-11-07T19:00:00Z",
        "summary": "In this review, we explore the historical development and future prospects of\nartificial intelligence (AI) and deep learning in astronomy. We trace the\nevolution of connectionism in astronomy through its three waves, from the early\nuse of multilayer perceptrons, to the rise of convolutional and recurrent\nneural networks, and finally to the current era of unsupervised and generative\ndeep learning methods. With the exponential growth of astronomical data, deep\nlearning techniques offer an unprecedented opportunity to uncover valuable\ninsights and tackle previously intractable problems. As we enter the\nanticipated fourth wave of astronomical connectionism, we argue for the\nadoption of GPT-like foundation models fine-tuned for astronomical\napplications. Such models could harness the wealth of high-quality, multimodal\nastronomical data to serve state-of-the-art downstream tasks. To keep pace with\nadvancements driven by Big Tech, we propose a collaborative, open-source\napproach within the astronomy community to develop and maintain these\nfoundation models, fostering a symbiotic relationship between AI and astronomy\nthat capitalizes on the unique strengths of both fields.",
        "pdf_link": "https://arxiv.org/pdf/2211.03796v2.pdf"
    },
    {
        "title": "Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach",
        "authors": [
            "Jiayao Zhang",
            "Hongming Zhang",
            "Zhun Deng",
            "Dan Roth"
        ],
        "published": "2022-11-07T16:19:42Z",
        "summary": "Double-blind peer review mechanism has become the skeleton of academic\nresearch across multiple disciplines including computer science, yet several\nstudies have questioned the quality of peer reviews and raised concerns on\npotential biases in the process. In this paper, we conduct a thorough and\nrigorous study on fairness disparities in peer review with the help of large\nlanguage models (LMs). We collect, assemble, and maintain a comprehensive\nrelational database for the International Conference on Learning\nRepresentations (ICLR) conference from 2017 to date by aggregating data from\nOpenReview, Google Scholar, arXiv, and CSRanking, and extracting high-level\nfeatures using language models. We postulate and study fairness disparities on\nmultiple protective attributes of interest, including author gender, geography,\nauthor, and institutional prestige. We observe that the level of disparity\ndiffers and textual features are essential in reducing biases in the predictive\nmodeling. We distill several insights from our analysis on study the peer\nreview process with the help of large LMs. Our database also provides avenues\nfor studying new natural language processing (NLP) methods that facilitate the\nunderstanding of the peer review mechanism. We study a concrete example towards\nautomatic machine review systems and provide baseline models for the review\ngeneration and scoring tasks such that the database can be used as a benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2211.06398v1.pdf"
    },
    {
        "title": "Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables",
        "authors": [
            "Erxin Yu",
            "Lan Du",
            "Yuan Jin",
            "Zhepei Wei",
            "Yi Chang"
        ],
        "published": "2022-11-07T15:09:58Z",
        "summary": "Recently, discrete latent variable models have received a surge of interest\nin both Natural Language Processing (NLP) and Computer Vision (CV), attributed\nto their comparable performance to the continuous counterparts in\nrepresentation learning, while being more interpretable in their predictions.\nIn this paper, we develop a topic-informed discrete latent variable model for\nsemantic textual similarity, which learns a shared latent space for\nsentence-pair representation via vector quantization. Compared with previous\nmodels limited to local semantic contexts, our model can explore richer\nsemantic information via topic modeling. We further boost the performance of\nsemantic similarity by injecting the quantized representation into a\ntransformer-based language model with a well-designed semantic-driven attention\nmechanism. We demonstrate, through extensive experiments across various English\nlanguage datasets, that our model is able to surpass several strong neural\nbaselines in semantic textual similarity tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.03616v1.pdf"
    },
    {
        "title": "A Targeted Sampling Strategy for Compressive Cryo Focused Ion Beam Scanning Electron Microscopy",
        "authors": [
            "Daniel Nicholls",
            "Jack Wells",
            "Alex W. Robinson",
            "Amirafshar Moshtaghpour",
            "Maryna Kobylynska",
            "Roland A. Fleck",
            "Angus I. Kirkland",
            "Nigel D. Browning"
        ],
        "published": "2022-11-07T12:27:28Z",
        "summary": "Cryo Focused Ion-Beam Scanning Electron Microscopy (cryo FIB-SEM) enables\nthree-dimensional and nanoscale imaging of biological specimens via a slice and\nview mechanism. The FIB-SEM experiments are, however, limited by a slow\n(typically, several hours) acquisition process and the high electron doses\nimposed on the beam sensitive specimen can cause damage. In this work, we\npresent a compressive sensing variant of cryo FIB-SEM capable of reducing the\noperational electron dose and increasing speed. We propose two Targeted\nSampling (TS) strategies that leverage the reconstructed image of the previous\nsample layer as a prior for designing the next subsampling mask. Our image\nrecovery is based on a blind Bayesian dictionary learning approach, i.e., Beta\nProcess Factor Analysis (BPFA). This method is experimentally viable due to our\nultra-fast GPU-based implementation of BPFA. Simulations on artificial\ncompressive FIB-SEM measurements validate the success of proposed methods: the\noperational electron dose can be reduced by up to 20 times. These methods have\nlarge implications for the cryo FIB-SEM community, in which the imaging of beam\nsensitive biological materials without beam damage is crucial.",
        "pdf_link": "https://arxiv.org/pdf/2211.03494v1.pdf"
    },
    {
        "title": "Generative Transformers for Design Concept Generation",
        "authors": [
            "Qihao Zhu",
            "Jianxi Luo"
        ],
        "published": "2022-11-07T11:29:10Z",
        "summary": "Generating novel and useful concepts is essential during the early design\nstage to explore a large variety of design opportunities, which usually\nrequires advanced design thinking ability and a wide range of knowledge from\ndesigners. Growing works on computer-aided tools have explored the retrieval of\nknowledge and heuristics from design data. However, they only provide stimuli\nto inspire designers from limited aspects. This study explores the recent\nadvance of the natural language generation (NLG) technique in the artificial\nintelligence (AI) field to automate the early-stage design concept generation.\nSpecifically, a novel approach utilizing the generative pre-trained transformer\n(GPT) is proposed to leverage the knowledge and reasoning from textual data and\ntransform them into new concepts in understandable language. Three concept\ngeneration tasks are defined to leverage different knowledge and reasoning:\ndomain knowledge synthesis, problem-driven synthesis, and analogy-driven\nsynthesis. The experiments with both human and data-driven evaluation show good\nperformance in generating novel and useful concepts.",
        "pdf_link": "https://arxiv.org/pdf/2211.03468v1.pdf"
    },
    {
        "title": "Probing neural language models for understanding of words of estimative probability",
        "authors": [
            "Damien Sileo",
            "Marie-Francine Moens"
        ],
        "published": "2022-11-07T08:29:11Z",
        "summary": "Words of estimative probability (WEP) are expressions of a statement's\nplausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...).\nMultiple surveys demonstrate the agreement of human evaluators when assigning\nnumerical probability levels to WEP. For example, highly likely corresponds to\na median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this\nwork, we measure the ability of neural language processing models to capture\nthe consensual probability level associated to each WEP. Firstly, we use the\nUNLI dataset (Chen et al., 2020) which associates premises and hypotheses with\ntheir perceived joint probability p, to construct prompts, e.g. \"[PREMISE].\n[WEP], [HYPOTHESIS].\" and assess whether language models can predict whether\nthe WEP consensual probability level is close to p. Secondly, we construct a\ndataset of WEP-based probabilistic reasoning, to test whether language models\ncan reason with WEP compositions. When prompted \"[EVENTA] is likely. [EVENTB]\nis impossible.\", a causal language model should not express that [EVENTA&B] is\nlikely. We show that both tasks are unsolved by off-the-shelf English language\nmodels, but that fine-tuning leads to transferable improvement.",
        "pdf_link": "https://arxiv.org/pdf/2211.03358v2.pdf"
    },
    {
        "title": "AD-BERT: Using Pre-trained contextualized embeddings to Predict the Progression from Mild Cognitive Impairment to Alzheimer's Disease",
        "authors": [
            "Chengsheng Mao",
            "Jie Xu",
            "Luke Rasmussen",
            "Yikuan Li",
            "Prakash Adekkanattu",
            "Jennifer Pacheco",
            "Borna Bonakdarpour",
            "Robert Vassar",
            "Guoqian Jiang",
            "Fei Wang",
            "Jyotishman Pathak",
            "Yuan Luo"
        ],
        "published": "2022-11-07T04:05:46Z",
        "summary": "Objective: We develop a deep learning framework based on the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model using\nunstructured clinical notes from electronic health records (EHRs) to predict\nthe risk of disease progression from Mild Cognitive Impairment (MCI) to\nAlzheimer's Disease (AD). Materials and Methods: We identified 3657 patients\ndiagnosed with MCI together with their progress notes from Northwestern\nMedicine Enterprise Data Warehouse (NMEDW) between 2000-2020. The progress\nnotes no later than the first MCI diagnosis were used for the prediction. We\nfirst preprocessed the notes by deidentification, cleaning and splitting, and\nthen pretrained a BERT model for AD (AD-BERT) based on the publicly available\nBio+Clinical BERT on the preprocessed notes. The embeddings of all the sections\nof a patient's notes processed by AD-BERT were combined by MaxPooling to\ncompute the probability of MCI-to-AD progression. For replication, we conducted\na similar set of experiments on 2563 MCI patients identified at Weill Cornell\nMedicine (WCM) during the same timeframe. Results: Compared with the 7 baseline\nmodels, the AD-BERT model achieved the best performance on both datasets, with\nArea Under receiver operating characteristic Curve (AUC) of 0.8170 and F1 score\nof 0.4178 on NMEDW dataset and AUC of 0.8830 and F1 score of 0.6836 on WCM\ndataset. Conclusion: We developed a deep learning framework using BERT models\nwhich provide an effective solution for prediction of MCI-to-AD progression\nusing clinical note analysis.",
        "pdf_link": "https://arxiv.org/pdf/2212.06042v1.pdf"
    },
    {
        "title": "Complex Reading Comprehension Through Question Decomposition",
        "authors": [
            "Xiao-Yu Guo",
            "Yuan-Fang Li",
            "Gholamreza Haffari"
        ],
        "published": "2022-11-07T02:54:04Z",
        "summary": "Multi-hop reading comprehension requires not only the ability to reason over\nraw text but also the ability to combine multiple evidence. We propose a novel\nlearning approach that helps language models better understand difficult\nmulti-hop questions and perform \"complex, compositional\" reasoning. Our model\nfirst learns to decompose each multi-hop question into several sub-questions by\na trainable question decomposer. Instead of answering these sub-questions, we\ndirectly concatenate them with the original question and context, and leverage\na reading comprehension model to predict the answer in a sequence-to-sequence\nmanner. By using the same language model for these two components, our best\nseperate/unified t5-base variants outperform the baseline by 7.2/6.1 absolute\nF1 points on a hard subset of DROP dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.03277v1.pdf"
    },
    {
        "title": "Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following",
        "authors": [
            "Yuki Inoue",
            "Hiroki Ohashi"
        ],
        "published": "2022-11-07T02:24:39Z",
        "summary": "Embodied Instruction Following (EIF) studies how autonomous mobile\nmanipulation robots should be controlled to accomplish long-horizon tasks\ndescribed by natural language instructions. While much research on EIF is\nconducted in simulators, the ultimate goal of the field is to deploy the agents\nin real life. This is one of the reasons why recent methods have moved away\nfrom training models end-to-end and take modular approaches, which do not need\nthe costly expert operation data. However, as it is still in the early days of\nimporting modular ideas to EIF, a search for modules effective in the EIF task\nis still far from a conclusion. In this paper, we propose to extend the modular\ndesign using knowledge obtained from two external sources. First, we show that\nembedding the physical constraints of the deployed robots into the module\ndesign is highly effective. Our design also allows the same modular system to\nwork across robots of different configurations with minimal modifications.\nSecond, we show that the landmark-based object search, previously implemented\nby a trained model requiring a dedicated set of data, can be replaced by an\nimplementation that prompts pretrained large language models for\nlandmark-object relationships, eliminating the need for collecting dedicated\ntraining data. Our proposed Prompter achieves 41.53\\% and 45.32\\% on the ALFRED\nbenchmark with high-level instructions only and step-by-step instructions,\nrespectively, significantly outperforming the previous state of the art by\n5.46\\% and 9.91\\%.",
        "pdf_link": "https://arxiv.org/pdf/2211.03267v2.pdf"
    },
    {
        "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages",
        "authors": [
            "Bonaventure F. P. Dossou",
            "Atnafu Lambebo Tonja",
            "Oreen Yousuf",
            "Salomey Osei",
            "Abigail Oppong",
            "Iyanuoluwa Shode",
            "Oluwabusayo Olufunke Awoyomi",
            "Chris Chinenye Emezue"
        ],
        "published": "2022-11-07T02:15:25Z",
        "summary": "In recent years, multilingual pre-trained language models have gained\nprominence due to their remarkable performance on numerous downstream Natural\nLanguage Processing tasks (NLP). However, pre-training these large multilingual\nlanguage models requires a lot of training data, which is not available for\nAfrican Languages. Active learning is a semi-supervised learning algorithm, in\nwhich a model consistently and dynamically learns to identify the most\nbeneficial samples to train itself on, in order to achieve better optimization\nand performance on downstream tasks. Furthermore, active learning effectively\nand practically addresses real-world data scarcity. Despite all its benefits,\nactive learning, in the context of NLP and especially multilingual language\nmodels pretraining, has received little consideration. In this paper, we\npresent AfroLM, a multilingual language model pretrained from scratch on 23\nAfrican languages (the largest effort to date) using our novel self-active\nlearning framework. Pretrained on a dataset significantly (14x) smaller than\nexisting baselines, AfroLM outperforms many multilingual pretrained language\nmodels (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text\nclassification, and sentiment analysis). Additional out-of-domain sentiment\nanalysis experiments show that \\textbf{AfroLM} is able to generalize well\nacross various domains. We release the code source, and our datasets used in\nour framework at https://github.com/bonaventuredossou/MLM_AL.",
        "pdf_link": "https://arxiv.org/pdf/2211.03263v2.pdf"
    },
    {
        "title": "Noisy Channel for Automatic Text Simplification",
        "authors": [
            "Oscar M Cumbicus-Pineda",
            "Iker Guti\u00e9rrez-Fandi\u00f1o",
            "Itziar Gonzalez-Dios",
            "Aitor Soroa"
        ],
        "published": "2022-11-06T15:28:42Z",
        "summary": "In this paper we present a simple re-ranking method for Automatic Sentence\nSimplification based on the noisy channel scheme. Instead of directly computing\nthe best simplification given a complex text, the re-ranking method also\nconsiders the probability of the simple sentence to produce the complex\ncounterpart, as well as the probability of the simple text itself, according to\na language model. Our experiments show that combining these scores outperform\nthe original system in three different English datasets, yielding the best\nknown result in one of them. Adopting the noisy channel scheme opens new ways\nto infuse additional information into ATS systems, and thus to control\nimportant aspects of them, a known limitation of end-to-end neural seq2seq\ngenerative models.",
        "pdf_link": "https://arxiv.org/pdf/2211.03152v1.pdf"
    },
    {
        "title": "Design Process is a Reinforcement Learning Problem",
        "authors": [
            "Reza kakooee",
            "Benjamin Dillunberger"
        ],
        "published": "2022-11-06T14:37:22Z",
        "summary": "While reinforcement learning has been used widely in research during the past\nfew years, it found fewer real-world applications than supervised learning due\nto some weaknesses that the RL algorithms suffer from, such as performance\ndegradation in transitioning from the simulator to the real world. Here, we\nargue the design process is a reinforcement learning problem and can\npotentially be a proper application for RL algorithms as it is an offline\nprocess and conventionally is done in CAD software - a sort of simulator. This\ncreates opportunities for using RL methods and, at the same time, raises\nchallenges. While the design processes are so diverse, here we focus on the\nspace layout planning (SLP), frame it as an RL problem under the Markov\nDecision Process, and use PPO to address the layout design problem. To do so,\nwe developed an environment named RLDesigner, to simulate the SLP. The\nRLDesigner is an OpenAI Gym compatible environment that can be easily\ncustomized to define a diverse range of design scenarios. We publicly share the\nenvironment to encourage both RL and architecture communities to use it for\ntesting different RL algorithms or in their design practice. The codes are\navailable in the following GitHub repository https://github.com/\nRezaKakooee/rldesigner/tree/Second_Paper",
        "pdf_link": "https://arxiv.org/pdf/2211.03136v1.pdf"
    },
    {
        "title": "Improved Target-specific Stance Detection on Social Media Platforms by Delving into Conversation Threads",
        "authors": [
            "Yupeng Li",
            "Haorui He",
            "Shaonan Wang",
            "Francis C. M. Lau",
            "Yunya Song"
        ],
        "published": "2022-11-06T08:40:48Z",
        "summary": "Target-specific stance detection on social media, which aims at classifying a\ntextual data instance such as a post or a comment into a stance class of a\ntarget issue, has become an emerging opinion mining paradigm of importance. An\nexample application would be to overcome vaccine hesitancy in combating the\ncoronavirus pandemic. However, existing stance detection strategies rely merely\non the individual instances which cannot always capture the expressed stance of\na given target. In response, we address a new task called conversational stance\ndetection which is to infer the stance towards a given target (e.g., COVID-19\nvaccination) when given a data instance and its corresponding conversation\nthread. To tackle the task, we first propose a benchmarking conversational\nstance detection (CSD) dataset with annotations of stances and the structures\nof conversation threads among the instances based on six major social media\nplatforms in Hong Kong. To infer the desired stances from both data instances\nand conversation threads, we propose a model called Branch-BERT that\nincorporates contextual information in conversation threads. Extensive\nexperiments on our CSD dataset show that our proposed model outperforms all the\nbaseline models that do not make use of contextual information. Specifically,\nit improves the F1 score by 10.3% compared with the state-of-the-art method in\nthe SemEval-2016 Task 6 competition. This shows the potential of incorporating\nrich contextual information on detecting target-specific stances on social\nmedia platforms and implies a more practical way to construct future stance\ndetection tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.03061v1.pdf"
    },
    {
        "title": "Suffix Retrieval-Augmented Language Modeling",
        "authors": [
            "Zecheng Wang",
            "Yik-Cheung Tam"
        ],
        "published": "2022-11-06T07:53:19Z",
        "summary": "Causal language modeling (LM) uses word history to predict the next word.\nBERT, on the other hand, makes use of bi-directional word information in a\nsentence to predict words at masked positions. While BERT is effective in\nsequence encoding, it is non-causal by nature and is not designed for sequence\ngeneration. In this paper, we propose a novel language model, SUffix\nREtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual\neffect in an autoregressive manner. SUREALM employs an embedding retriever to\nsearch for training sentences in a data store that share similar word history\nduring sequence generation. In particular, the suffix portions of the retrieved\nsentences mimick the \"future\" context. We evaluated our proposed model on the\nDSTC9 spoken dialogue corpus and showed promising word perplexity reduction on\nthe validation and test set compared to competitive baselines.",
        "pdf_link": "https://arxiv.org/pdf/2211.03053v2.pdf"
    },
    {
        "title": "Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust",
        "authors": [
            "Haotian Chen",
            "Lingwei Zhang",
            "Yiran Liu",
            "Fanchao Chen",
            "Yang Yu"
        ],
        "published": "2022-11-06T07:03:31Z",
        "summary": "Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact\ndescriptions according to rule of law, serves as legal assistance to mitigate\nthe great work burden of limited legal practitioners. Most existing methods\napply various large-scale pre-trained language models (PLMs) finetuned in LJP\ntasks to obtain consistent improvements. However, we discover the fact that the\nstate-of-the-art (SOTA) model makes judgment predictions according to\nirrelevant (or non-casual) information. The violation of rule of law not only\nweakens the robustness and generalization ability of models but also results in\nsevere social problems like discrimination. In this paper, we use causal\nstructural models (SCMs) to theoretically analyze how LJP models learn to make\ndecisions and why they can succeed in passing the traditional testing paradigm\nwithout learning causality. According to our analysis, we provide two solutions\nintervening on data and model by causality, respectively. In detail, we first\ndistinguish non-causal information by applying the open information extraction\n(OIE) technique. Then, we propose a method named the Causal Information\nEnhanced SAmpling Method (CIESAM) to eliminate the non-causal information from\ndata. To validate our theoretical analysis, we further propose another method\nusing our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide\nthe model to learn the underlying causality knowledge in legal texts. The\nconfidence of CASAM in learning causal information is higher than that of\nCIESAM. The extensive experimental results show that both our proposed methods\nachieve state-of-the-art (SOTA) performance on three commonly used\nlegal-specific datasets. The stronger performance of CASAM further demonstrates\nthat causality is the key to the robustness and generalization ability of\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2211.03046v2.pdf"
    },
    {
        "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
        "authors": [
            "Mobashir Sadat",
            "Cornelia Caragea"
        ],
        "published": "2022-11-05T20:34:08Z",
        "summary": "Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims\nat predicting the relation between a pair of sentences (premise and hypothesis)\nas entailment, contradiction or semantic independence. Although deep learning\nmodels have shown promising performance for NLI in recent years, they rely on\nlarge scale expensive human-annotated datasets. Semi-supervised learning (SSL)\nis a popular technique for reducing the reliance on human annotation by\nleveraging unlabeled data for training. However, despite its substantial\nsuccess on single sentence classification tasks where the challenge in making\nuse of unlabeled data is to assign \"good enough\" pseudo-labels, for NLI tasks,\nthe nature of unlabeled data is more complex: one of the sentences in the pair\n(usually the hypothesis) along with the class label are missing from the data\nand require human annotations, which makes SSL for NLI more challenging. In\nthis paper, we propose a novel way to incorporate unlabeled data in SSL for NLI\nwhere we use a conditional language model, BART to generate the hypotheses for\nthe unlabeled sentences (used as premises). Our experiments show that our SSL\nframework successfully exploits unlabeled data and substantially improves the\nperformance of four NLI datasets in low-resource settings. We release our code\nat: https://github.com/msadat3/SSL_for_NLI.",
        "pdf_link": "https://arxiv.org/pdf/2211.02971v1.pdf"
    },
    {
        "title": "BEKG: A Built Environment Knowledge Graph",
        "authors": [
            "Xiaojun Yang",
            "Haoyu Zhong",
            "Penglin Du",
            "Keyi Zhou",
            "Xingjin Lai",
            "Zhengdong Wang",
            "Yik Lun Lau",
            "Yangqiu Song",
            "Liyaning Tang"
        ],
        "published": "2022-11-05T09:52:45Z",
        "summary": "Practices in the built environment have become more digitalized with the\nrapid development of modern design and construction technologies. However, the\nrequirement of practitioners or scholars to gather complicated professional\nknowledge in the built environment has not been satisfied yet. In this paper,\nmore than 80,000 paper abstracts in the built environment field were obtained\nto build a knowledge graph, a knowledge base storing entities and their\nconnective relations in a graph-structured data model. To ensure the retrieval\naccuracy of the entities and relations in the knowledge graph, two\nwell-annotated datasets have been created, containing 2,000 instances and 1,450\ninstances each in 29 relations for the named entity recognition task and\nrelation extraction task respectively. These two tasks were solved by two\nBERT-based models trained on the proposed dataset. Both models attained an\naccuracy above 85% on these two tasks. More than 200,000 high-quality relations\nand entities were obtained using these models to extract all abstract data.\nFinally, this knowledge graph is presented as a self-developed visualization\nsystem to reveal relations between various entities in the domain. Both the\nsource code and the annotated dataset can be found here:\nhttps://github.com/HKUST-KnowComp/BEKG.",
        "pdf_link": "https://arxiv.org/pdf/2211.02864v1.pdf"
    },
    {
        "title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction",
        "authors": [
            "Jason Youn",
            "Ilias Tagkopoulos"
        ],
        "published": "2022-11-04T20:38:12Z",
        "summary": "The ability of knowledge graphs to represent complex relationships at scale\nhas led to their adoption for various needs including knowledge representation,\nquestion-answering, and recommendation systems. Knowledge graphs are often\nincomplete in the information they represent, necessitating the need for\nknowledge graph completion tasks. Pre-trained and fine-tuned language models\nhave shown promise in these tasks although these models ignore the intrinsic\ninformation encoded in the knowledge graph, namely the entity and relation\ntypes. In this work, we propose the Knowledge Graph Language Model (KGLM)\narchitecture, where we introduce a new entity/relation embedding layer that\nlearns to differentiate distinctive entity and relation types, therefore\nallowing the model to learn the structure of the knowledge graph. In this work,\nwe show that further pre-training the language models with this additional\nembedding layer using the triples extracted from the knowledge graph, followed\nby the standard fine-tuning phase sets a new state-of-the-art performance for\nthe link prediction task on the benchmark datasets.",
        "pdf_link": "https://arxiv.org/pdf/2211.02744v2.pdf"
    },
    {
        "title": "Measuring Progress on Scalable Oversight for Large Language Models",
        "authors": [
            "Samuel R. Bowman",
            "Jeeyoon Hyun",
            "Ethan Perez",
            "Edwin Chen",
            "Craig Pettit",
            "Scott Heiner",
            "Kamil\u0117 Luko\u0161i\u016bt\u0117",
            "Amanda Askell",
            "Andy Jones",
            "Anna Chen",
            "Anna Goldie",
            "Azalia Mirhoseini",
            "Cameron McKinnon",
            "Christopher Olah",
            "Daniela Amodei",
            "Dario Amodei",
            "Dawn Drain",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Jackson Kernion",
            "Jamie Kerr",
            "Jared Mueller",
            "Jeffrey Ladish",
            "Joshua Landau",
            "Kamal Ndousse",
            "Liane Lovitt",
            "Nelson Elhage",
            "Nicholas Schiefer",
            "Nicholas Joseph",
            "Noem\u00ed Mercado",
            "Nova DasSarma",
            "Robin Larson",
            "Sam McCandlish",
            "Sandipan Kundu",
            "Scott Johnston",
            "Shauna Kravec",
            "Sheer El Showk",
            "Stanislav Fort",
            "Timothy Telleen-Lawton",
            "Tom Brown",
            "Tom Henighan",
            "Tristan Hume",
            "Yuntao Bai",
            "Zac Hatfield-Dodds",
            "Ben Mann",
            "Jared Kaplan"
        ],
        "published": "2022-11-04T17:03:49Z",
        "summary": "Developing safe and useful general-purpose AI systems will require us to make\nprogress on scalable oversight: the problem of supervising systems that\npotentially outperform us on most skills relevant to the task at hand.\nEmpirical work on this problem is not straightforward, since we do not yet have\nsystems that broadly exceed our abilities. This paper discusses one of the\nmajor ways we think about this problem, with a focus on ways it can be studied\nempirically. We first present an experimental design centered on tasks for\nwhich human specialists succeed but unaided humans and current general AI\nsystems fail. We then present a proof-of-concept experiment meant to\ndemonstrate a key feature of this experimental design and show its viability\nwith two question-answering tasks: MMLU and time-limited QuALITY. On these\ntasks, we find that human participants who interact with an unreliable\nlarge-language-model dialog assistant through chat -- a trivial baseline\nstrategy for scalable oversight -- substantially outperform both the model\nalone and their own unaided performance. These results are an encouraging sign\nthat scalable oversight will be tractable to study with present models and\nbolster recent findings that large language models can productively assist\nhumans with difficult tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.03540v2.pdf"
    },
    {
        "title": "Multi-blank Transducers for Speech Recognition",
        "authors": [
            "Hainan Xu",
            "Fei Jia",
            "Somshubra Majumdar",
            "Shinji Watanabe",
            "Boris Ginsburg"
        ],
        "published": "2022-11-04T16:24:46Z",
        "summary": "This paper proposes a modification to RNN-Transducer (RNN-T) models for\nautomatic speech recognition (ASR). In standard RNN-T, the emission of a blank\nsymbol consumes exactly one input frame; in our proposed method, we introduce\nadditional blank symbols, which consume two or more input frames when emitted.\nWe refer to the added symbols as big blanks, and the method multi-blank RNN-T.\nFor training multi-blank RNN-Ts, we propose a novel logit under-normalization\nmethod in order to prioritize emissions of big blanks. With experiments on\nmultiple languages and datasets, we show that multi-blank RNN-T methods could\nbring relative speedups of over +90%/+139% to model inference for English\nLibrispeech and German Multilingual Librispeech datasets, respectively. The\nmulti-blank RNN-T method also improves ASR accuracy consistently. We will\nrelease our implementation of the method in the NeMo\n(\\url{https://github.com/NVIDIA/NeMo}) toolkit.",
        "pdf_link": "https://arxiv.org/pdf/2211.03541v1.pdf"
    },
    {
        "title": "BERT for Long Documents: A Case Study of Automated ICD Coding",
        "authors": [
            "Arash Afkanpour",
            "Shabir Adeel",
            "Hansenclever Bassani",
            "Arkady Epshteyn",
            "Hongbo Fan",
            "Isaac Jones",
            "Mahan Malihi",
            "Adrian Nauth",
            "Raj Sinha",
            "Sanjana Woonna",
            "Shiva Zamani",
            "Elli Kanal",
            "Mikhail Fomitchev",
            "Donny Cheung"
        ],
        "published": "2022-11-04T15:24:19Z",
        "summary": "Transformer models have achieved great success across many NLP problems.\nHowever, previous studies in automated ICD coding concluded that these models\nfail to outperform some of the earlier solutions such as CNN-based models. In\nthis paper we challenge this conclusion. We present a simple and scalable\nmethod to process long text with the existing transformer models such as BERT.\nWe show that this method significantly improves the previous results reported\nfor transformer models in ICD coding, and is able to outperform one of the\nprominent CNN-based methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.02519v1.pdf"
    },
    {
        "title": "BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets",
        "authors": [
            "Javad Hassannataj Joloudari",
            "Sadiq Hussain",
            "Mohammad Ali Nematollahi",
            "Rouhollah Bagheri",
            "Fatemeh Fazl",
            "Roohallah Alizadehsani",
            "Reza Lashgari",
            "Ashis Talukder"
        ],
        "published": "2022-11-04T14:35:56Z",
        "summary": "The free flow of information has been accelerated by the rapid development of\nsocial media technology. There has been a significant social and psychological\nimpact on the population due to the outbreak of Coronavirus disease (COVID-19).\nThe COVID-19 pandemic is one of the current events being discussed on social\nmedia platforms. In order to safeguard societies from this pandemic, studying\npeople's emotions on social media is crucial. As a result of their particular\ncharacteristics, sentiment analysis of texts like tweets remains challenging.\nSentiment analysis is a powerful text analysis tool. It automatically detects\nand analyzes opinions and emotions from unstructured data. Texts from a wide\nrange of sources are examined by a sentiment analysis tool, which extracts\nmeaning from them, including emails, surveys, reviews, social media posts, and\nweb articles. To evaluate sentiments, natural language processing (NLP) and\nmachine learning techniques are used, which assign weights to entities, topics,\nthemes, and categories in sentences or phrases. Machine learning tools learn\nhow to detect sentiment without human intervention by examining examples of\nemotions in text. In a pandemic situation, analyzing social media texts to\nuncover sentimental trends can be very helpful in gaining a better\nunderstanding of society's needs and predicting future trends. We intend to\nstudy society's perception of the COVID-19 pandemic through social media using\nstate-of-the-art BERT and Deep CNN models. The superiority of BERT models over\nother deep models in sentiment analysis is evident and can be concluded from\nthe comparison of the various research studies mentioned in this article.",
        "pdf_link": "https://arxiv.org/pdf/2211.09733v2.pdf"
    },
    {
        "title": "Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing",
        "authors": [
            "Yibo Wang",
            "Congying Xia",
            "Guan Wang",
            "Philip Yu"
        ],
        "published": "2022-11-04T14:20:40Z",
        "summary": "The explosion of e-commerce has caused the need for processing and analysis\nof product titles, like entity typing in product titles. However, the rapid\nactivity in e-commerce has led to the rapid emergence of new entities, which is\ndifficult to be solved by general entity typing. Besides, product titles in\ne-commerce have very different language styles from text data in general\ndomain. In order to handle new entities in product titles and address the\nspecial language styles problem of product titles in e-commerce domain, we\npropose our textual entailment model with continuous prompt tuning based\nhypotheses and fusion embeddings for e-commerce entity typing. First, we\nreformulate the entity typing task into a textual entailment problem to handle\nnew entities that are not present during training. Second, we design a model to\nautomatically generate textual entailment hypotheses using a continuous prompt\ntuning method, which can generate better textual entailment hypotheses without\nmanual design. Third, we utilize the fusion embeddings of BERT embedding and\nCharacterBERT embedding with a two-layer MLP classifier to solve the problem\nthat the language styles of product titles in e-commerce are different from\nthat of general domain. To analyze the effect of each contribution, we compare\nthe performance of entity typing and textual entailment model, and conduct\nablation studies on continuous prompt tuning and fusion embeddings. We also\nevaluate the impact of different prompt template initialization for the\ncontinuous prompt tuning. We show our proposed model improves the average F1\nscore by around 2% compared to the baseline BERT entity typing model.",
        "pdf_link": "https://arxiv.org/pdf/2211.02483v1.pdf"
    },
    {
        "title": "OSIC: A New One-Stage Image Captioner Coined",
        "authors": [
            "Bo Wang",
            "Zhao Zhang",
            "Mingbo Zhao",
            "Xiaojie Jin",
            "Mingliang Xu",
            "Meng Wang"
        ],
        "published": "2022-11-04T08:50:09Z",
        "summary": "Mainstream image caption models are usually two-stage captioners, i.e.,\ncalculating object features by pre-trained detector, and feeding them into a\nlanguage model to generate text descriptions. However, such an operation will\ncause a task-based information gap to decrease the performance, since the\nobject features in detection task are suboptimal representation and cannot\nprovide all necessary information for subsequent text generation. Besides,\nobject features are usually represented by the last layer features that lose\nthe local details of input images. In this paper, we propose a novel One-Stage\nImage Captioner (OSIC) with dynamic multi-sight learning, which directly\ntransforms input image into descriptive sentences in one stage. As a result,\nthe task-based information gap can be greatly reduced. To obtain rich features,\nwe use the Swin Transformer to calculate multi-level features, and then feed\nthem into a novel dynamic multi-sight embedding module to exploit both global\nstructure and local texture of input images. To enhance the global modeling of\nencoder for caption, we propose a new dual-dimensional refining module to\nnon-locally model the interaction of the embedded features. Finally, OSIC can\nobtain rich and useful information to improve the image caption task. Extensive\ncomparisons on benchmark MS-COCO dataset verified the superior performance of\nour method.",
        "pdf_link": "https://arxiv.org/pdf/2211.02321v1.pdf"
    },
    {
        "title": "MolE: a molecular foundation model for drug discovery",
        "authors": [
            "Oscar M\u00e9ndez-Lucio",
            "Christos Nicolaou",
            "Berton Earnshaw"
        ],
        "published": "2022-11-03T21:22:05Z",
        "summary": "Models that accurately predict properties based on chemical structure are\nvaluable tools in drug discovery. However, for many properties, public and\nprivate training sets are typically small, and it is difficult for the models\nto generalize well outside of the training data. Recently, large language\nmodels have addressed this problem by using self-supervised pretraining on\nlarge unlabeled datasets, followed by fine-tuning on smaller, labeled datasets.\nIn this paper, we report MolE, a molecular foundation model that adapts the\nDeBERTa architecture to be used on molecular graphs together with a two-step\npretraining strategy. The first step of pretraining is a self-supervised\napproach focused on learning chemical structures, and the second step is a\nmassive multi-task approach to learn biological information. We show that\nfine-tuning pretrained MolE achieves state-of-the-art results on 9 of the 22\nADMET tasks included in the Therapeutic Data Commons.",
        "pdf_link": "https://arxiv.org/pdf/2211.02657v1.pdf"
    },
    {
        "title": "Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2022-11-03T18:53:30Z",
        "summary": "Through their transfer learning abilities, highly-parameterized large\npre-trained language models have dominated the NLP landscape for a multitude of\ndownstream language tasks. Though linguistically proficient, the inability of\nthese models to incorporate the learning of non-linguistic entities (numerals\nand arithmetic reasoning) limits their usage for tasks that require numeric\ncomprehension or strict mathematical reasoning. However, as we illustrate in\nthis paper, building a general purpose language model that also happens to be\nproficient in mathematical reasoning is not as straight-forward as training it\non a numeric dataset. In this work, we develop a novel framework that enables\nlanguage models to be mathematically proficient while retaining their\nlinguistic prowess. Specifically, we offer information-theoretic interventions\nto overcome the catastrophic forgetting of linguistic skills that occurs while\ninjecting non-linguistic skills into language models.",
        "pdf_link": "https://arxiv.org/pdf/2211.02098v1.pdf"
    },
    {
        "title": "LMentry: A Language Model Benchmark of Elementary Language Tasks",
        "authors": [
            "Avia Efrat",
            "Or Honovich",
            "Omer Levy"
        ],
        "published": "2022-11-03T18:01:12Z",
        "summary": "As the performance of large language models rapidly improves, benchmarks are\ngetting larger and more complex as well. We present LMentry, a benchmark that\navoids this \"arms race\" by focusing on a compact set of tasks that are trivial\nto humans, e.g. writing a sentence containing a specific word, identifying\nwhich words in a list belong to a specific category, or choosing which of two\nwords is longer. LMentry is specifically designed to provide quick and\ninterpretable insights into the capabilities and robustness of large language\nmodels. Our experiments reveal a wide variety of failure cases that, while\nimmediately obvious to humans, pose a considerable challenge for large language\nmodels, including OpenAI's latest 175B-parameter instruction-tuned model,\nTextDavinci002. LMentry complements contemporary evaluation approaches of large\nlanguage models, providing a quick, automatic, and easy-to-run \"unit test\",\nwithout resorting to large benchmark suites of complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.02069v2.pdf"
    },
    {
        "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",
        "authors": [
            "Alexandra Sasha Luccioni",
            "Sylvain Viguier",
            "Anne-Laure Ligozat"
        ],
        "published": "2022-11-03T17:13:48Z",
        "summary": "Progress in machine learning (ML) comes with a cost to the environment, given\nthat training ML models requires significant computational resources, energy\nand materials. In the present article, we aim to quantify the carbon footprint\nof BLOOM, a 176-billion parameter language model, across its life cycle. We\nestimate that BLOOM's final training emitted approximately 24.7 tonnes\nof~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes\nif we account for all processes ranging from equipment manufacturing to\nenergy-based operational consumption. We also study the energy requirements and\ncarbon emissions of its deployment for inference via an API endpoint receiving\nuser queries in real-time. We conclude with a discussion regarding the\ndifficulty of precisely estimating the carbon footprint of ML models and future\nresearch directions that can contribute towards improving carbon emissions\nreporting.",
        "pdf_link": "https://arxiv.org/pdf/2211.02001v1.pdf"
    },
    {
        "title": "Probing Statistical Representations For End-To-End ASR",
        "authors": [
            "Anna Ollerenshaw",
            "Md Asif Jalal",
            "Thomas Hain"
        ],
        "published": "2022-11-03T17:08:14Z",
        "summary": "End-to-End automatic speech recognition (ASR) models aim to learn a\ngeneralised speech representation to perform recognition. In this domain there\nis little research to analyse internal representation dependencies and their\nrelationship to modelling approaches. This paper investigates cross-domain\nlanguage model dependencies within transformer architectures using SVCCA and\nuses these insights to exploit modelling approaches. It was found that specific\nneural representations within the transformer layers exhibit correlated\nbehaviour which impacts recognition performance.\n  Altogether, this work provides analysis of the modelling approaches affecting\ncontextual dependencies and ASR performance, and can be used to create or adapt\nbetter performing End-to-End ASR models and also for downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.01993v1.pdf"
    },
    {
        "title": "Large Language Models Are Human-Level Prompt Engineers",
        "authors": [
            "Yongchao Zhou",
            "Andrei Ioan Muresanu",
            "Ziwen Han",
            "Keiran Paster",
            "Silviu Pitis",
            "Harris Chan",
            "Jimmy Ba"
        ],
        "published": "2022-11-03T15:43:03Z",
        "summary": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.",
        "pdf_link": "https://arxiv.org/pdf/2211.01910v2.pdf"
    },
    {
        "title": "Contextual information integration for stance detection via cross-attention",
        "authors": [
            "Tilman Beck",
            "Andreas Waldis",
            "Iryna Gurevych"
        ],
        "published": "2022-11-03T15:04:29Z",
        "summary": "Stance detection deals with identifying an author's stance towards a target.\nMost existing stance detection models are limited because they do not consider\nrelevant contextual information which allows for inferring the stance\ncorrectly. Complementary context can be found in knowledge bases but\nintegrating the context into pretrained language models is non-trivial due to\nthe graph structure of standard knowledge bases. To overcome this, we explore\nan approach to integrate contextual information as text which allows for\nintegrating contextual information from heterogeneous sources, such as\nstructured knowledge sources and by prompting large language models. Our\napproach can outperform competitive baselines on a large and diverse stance\ndetection benchmark in a cross-target setup, i.e. for targets unseen during\ntraining. We demonstrate that it is more robust to noisy context and can\nregularize for unwanted correlations between labels and target-specific\nvocabulary. Finally, it is independent of the pretrained language model in use.",
        "pdf_link": "https://arxiv.org/pdf/2211.01874v2.pdf"
    },
    {
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "authors": [
            "Niklas Muennighoff",
            "Thomas Wang",
            "Lintang Sutawika",
            "Adam Roberts",
            "Stella Biderman",
            "Teven Le Scao",
            "M Saiful Bari",
            "Sheng Shen",
            "Zheng-Xin Yong",
            "Hailey Schoelkopf",
            "Xiangru Tang",
            "Dragomir Radev",
            "Alham Fikri Aji",
            "Khalid Almubarak",
            "Samuel Albanie",
            "Zaid Alyafeai",
            "Albert Webson",
            "Edward Raff",
            "Colin Raffel"
        ],
        "published": "2022-11-03T13:19:32Z",
        "summary": "Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.",
        "pdf_link": "https://arxiv.org/pdf/2211.01786v2.pdf"
    },
    {
        "title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively",
        "authors": [
            "Haojie Zhang",
            "Ge Li",
            "Jia Li",
            "Zhongjin Zhang",
            "Yuqi Zhu",
            "Zhi Jin"
        ],
        "published": "2022-11-03T08:32:12Z",
        "summary": "Large-scale pre-trained language models have achieved impressive results on a\nwide range of downstream tasks recently. However, fine-tuning an extremely\nlarge-scale pre-trained language model on limited target datasets is often\nplagued by overfitting and representation degradation. In this paper, we\npropose a Dynamic Parameter Selection (DPS) algorithm for the large-scale\npre-trained models during fine-tuning, which adaptively selects a more\npromising subnetwork to perform staging updates based on gradients of\nback-propagation. Experiments on the GLUE benchmark show that DPS outperforms\nprevious fine-tuning methods in terms of overall performance and stability, and\nconsistently achieves better results with variable pre-trained language models.\nIn addition, DPS brings a large magnitude of improvement in out-of-domain\ntransferring experiments and low-resource scenarios, which shows that it can\nmaintain stable general contextual features and reduce the representation\ncollapse. We release our code at https://github.com/ZhangHaojie077/DPS",
        "pdf_link": "https://arxiv.org/pdf/2211.01642v1.pdf"
    },
    {
        "title": "Using Large Pre-Trained Language Model to Assist FDA in Premarket Medical Device",
        "authors": [
            "Zongzhe Xu"
        ],
        "published": "2022-11-03T04:18:05Z",
        "summary": "This paper proposes a possible method using natural language processing that\nmight assist in the FDA medical device marketing process. Actual device\ndescriptions are taken and matched with the device description in FDA Title 21\nof CFR to determine their corresponding device type. Both pre-trained word\nembeddings such as FastText and large pre-trained sentence embedding models\nsuch as sentence transformers are evaluated on their accuracy in characterizing\na piece of device description. An experiment is also done to test whether these\nmodels can identify the devices wrongly classified in the FDA database. The\nresult shows that sentence transformer with T5 and MPNet and GPT-3 semantic\nsearch embedding show high accuracy in identifying the correct classification\nby narrowing down the correct label to be contained in the first 15 most likely\nresults, as compared to 2585 types of device descriptions that must be manually\nsearched through. On the other hand, all methods demonstrate high accuracy in\nidentifying completely incorrectly labeled devices, but all fail to identify\nfalse device classifications that are wrong but closely related to the true\nlabel.",
        "pdf_link": "https://arxiv.org/pdf/2212.01217v1.pdf"
    },
    {
        "title": "Open-Vocabulary Argument Role Prediction for Event Extraction",
        "authors": [
            "Yizhu Jiao",
            "Sha Li",
            "Yiqing Xie",
            "Ming Zhong",
            "Heng Ji",
            "Jiawei Han"
        ],
        "published": "2022-11-03T04:13:37Z",
        "summary": "The argument role in event extraction refers to the relation between an event\nand an argument participating in it. Despite the great progress in event\nextraction, existing studies still depend on roles pre-defined by domain\nexperts. These studies expose obvious weakness when extending to emerging event\ntypes or new domains without available roles. Therefore, more attention and\neffort needs to be devoted to automatically customizing argument roles. In this\npaper, we define this essential but under-explored task: open-vocabulary\nargument role prediction. The goal of this task is to infer a set of argument\nroles for a given event type. We propose a novel unsupervised framework,\nRolePred for this task. Specifically, we formulate the role prediction problem\nas an in-filling task and construct prompts for a pre-trained language model to\ngenerate candidate roles. By extracting and analyzing the candidate arguments,\nthe event-specific roles are further merged and selected. To standardize the\nresearch of this task, we collect a new event extraction dataset from\nWikiPpedia including 142 customized argument roles with rich semantics. On this\ndataset, RolePred outperforms the existing methods by a large margin. Source\ncode and dataset are available on our GitHub repository:\nhttps://github.com/yzjiao/RolePred",
        "pdf_link": "https://arxiv.org/pdf/2211.01577v1.pdf"
    },
    {
        "title": "Fine-Tuning Language Models via Epistemic Neural Networks",
        "authors": [
            "Ian Osband",
            "Seyed Mohammad Asghari",
            "Benjamin Van Roy",
            "Nat McAleese",
            "John Aslanides",
            "Geoffrey Irving"
        ],
        "published": "2022-11-03T03:24:46Z",
        "summary": "Language models often pre-train on large unsupervised text corpora, then\nfine-tune on additional task-specific data. However, typical fine-tuning\nschemes do not prioritize the examples that they tune on. We show that, if you\ncan prioritize informative training data, you can achieve better performance\nwhile using fewer labels. To do this we augment a language model with an\nepinet: a small additional network that helps to estimate model uncertainty and\nforms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that\ncan know what they don't know. Using an epinet to prioritize uncertain data, we\ncan fine-tune BERT on GLUE tasks to the same performance while using 2x less\ndata than training without prioritization. We also investigate performance in\nsynthetic neural network generative models designed to build understanding. In\neach setting, using an epinet outperforms heuristic active learning schemes.",
        "pdf_link": "https://arxiv.org/pdf/2211.01568v2.pdf"
    },
    {
        "title": "Towards Zero-Shot Code-Switched Speech Recognition",
        "authors": [
            "Brian Yan",
            "Matthew Wiesner",
            "Ondrej Klejch",
            "Preethi Jyothi",
            "Shinji Watanabe"
        ],
        "published": "2022-11-02T19:52:54Z",
        "summary": "In this work, we seek to build effective code-switched (CS) automatic speech\nrecognition systems (ASR) under the zero-shot setting where no transcribed CS\nspeech data is available for training. Previously proposed frameworks which\nconditionally factorize the bilingual task into its constituent monolingual\nparts are a promising starting point for leveraging monolingual data\nefficiently. However, these methods require the monolingual modules to perform\nlanguage segmentation. That is, each monolingual module has to simultaneously\ndetect CS points and transcribe speech segments of one language while ignoring\nthose of other languages -- not a trivial task. We propose to simplify each\nmonolingual module by allowing them to transcribe all speech segments\nindiscriminately with a monolingual script (i.e. transliteration). This simple\nmodification passes the responsibility of CS point detection to subsequent\nbilingual modules which determine the final output by considering multiple\nmonolingual transliterations along with external language model information. We\napply this transliteration-based approach in an end-to-end differentiable\nneural network and demonstrate its efficacy for zero-shot CS ASR on\nMandarin-English SEAME test sets.",
        "pdf_link": "https://arxiv.org/pdf/2211.01458v2.pdf"
    },
    {
        "title": "data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup",
        "authors": [
            "Vasista Sai Lodagala",
            "Sreyan Ghosh",
            "S. Umesh"
        ],
        "published": "2022-11-02T16:29:59Z",
        "summary": "In this paper, we propose a new Self-Supervised Learning (SSL) algorithm\ncalled data2vec-aqc, for speech representation learning from unlabeled speech\ndata. Our goal is to improve SSL for speech in domains where both unlabeled and\nlabeled data are limited. Building on the recently introduced data2vec, we\nintroduce additional modules to the data2vec framework that leverage the\nbenefit of data augmentations, quantized representations, and clustering. The\ninteraction between these modules helps solve the cross-contrastive loss as an\nadditional self-supervised objective. data2vec-aqc achieves up to 14.1% and\n20.9% relative WER improvement over the existing state-of-the-art data2vec\nsystem over the test-clean and test-other sets, respectively of LibriSpeech,\nwithout the use of any language model (LM). Our proposed model also achieves up\nto 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a\nsubset of the Switchboard dataset. Code:\nhttps://github.com/Speech-Lab-IITM/data2vec-aqc.",
        "pdf_link": "https://arxiv.org/pdf/2211.01246v2.pdf"
    },
    {
        "title": "Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model",
        "authors": [
            "Mingqi Li",
            "Fei Ding",
            "Dan Zhang",
            "Long Cheng",
            "Hongxin Hu",
            "Feng Luo"
        ],
        "published": "2022-11-02T15:23:13Z",
        "summary": "Pre-trained multilingual language models play an important role in\ncross-lingual natural language understanding tasks. However, existing methods\ndid not focus on learning the semantic structure of representation, and thus\ncould not optimize their performance. In this paper, we propose Multi-level\nMultilingual Knowledge Distillation (MMKD), a novel method for improving\nmultilingual language models. Specifically, we employ a teacher-student\nframework to adopt rich semantic representation knowledge in English BERT. We\npropose token-, word-, sentence-, and structure-level alignment objectives to\nencourage multiple levels of consistency between source-target pairs and\ncorrelation similarity between teacher and student models. We conduct\nexperiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and\nXQuAD. Experimental results show that MMKD outperforms other baseline models of\nsimilar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.\nEspecially, MMKD obtains significant performance gains on low-resource\nlanguages.",
        "pdf_link": "https://arxiv.org/pdf/2211.01200v1.pdf"
    },
    {
        "title": "Transformer-based encoder-encoder architecture for Spoken Term Detection",
        "authors": [
            "Jan \u0160vec",
            "Lubo\u0161 \u0160m\u00eddl",
            "Jan Lehe\u010dka"
        ],
        "published": "2022-11-02T13:03:15Z",
        "summary": "The paper presents a method for spoken term detection based on the\nTransformer architecture. We propose the encoder-encoder architecture employing\ntwo BERT-like encoders with additional modifications, including convolutional\nand upsampling layers, attention masking, and shared parameters. The encoders\nproject a recognized hypothesis and a searched term into a shared embedding\nspace, where the score of the putative hit is computed using the calibrated dot\nproduct. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the\nproposed system outperformed a baseline method based on deep LSTMs on the\nEnglish and Czech STD datasets based on USC Shoah Foundation Visual History\nArchive (MALACH).",
        "pdf_link": "https://arxiv.org/pdf/2211.01089v1.pdf"
    },
    {
        "title": "Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer",
        "authors": [
            "Dimitris Mamakas",
            "Petros Tsotsi",
            "Ion Androutsopoulos",
            "Ilias Chalkidis"
        ],
        "published": "2022-11-02T09:27:01Z",
        "summary": "Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.",
        "pdf_link": "https://arxiv.org/pdf/2211.00974v2.pdf"
    },
    {
        "title": "Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation",
        "authors": [
            "Rao Ma",
            "Xiaobo Wu",
            "Jin Qiu",
            "Yanan Qin",
            "Haihua Xu",
            "Peihao Wu",
            "Zejun Ma"
        ],
        "published": "2022-11-02T09:15:20Z",
        "summary": "ASR model deployment environment is ever-changing, and the incoming speech\ncan be switched across different domains during a session. This brings a\nchallenge for effective domain adaptation when only target domain text data is\navailable, and our objective is to obtain obviously improved performance on the\ntarget domain while the performance on the general domain is less undermined.\nIn this paper, we propose an adaptive LM fusion approach called internal\nlanguage model estimation based adaptive domain adaptation (ILME-ADA). To\nrealize such an ILME-ADA, an interpolated log-likelihood score is calculated\nbased on the maximum of the scores from the internal LM and the external LM\n(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method\nwith both RNN-T and LAS modeling frameworks employing neural network and n-gram\nLMs as ELMs respectively on two domain specific (target) test sets. The\nproposed method can achieve significantly better performance on the target test\nsets while it gets minimal performance degradation on the general test set,\ncompared with both shallow and ILME-based LM fusion methods.",
        "pdf_link": "https://arxiv.org/pdf/2211.00968v2.pdf"
    },
    {
        "title": "Numerical Optimizations for Weighted Low-rank Estimation on Language Model",
        "authors": [
            "Ting Hua",
            "Yen-Chang Hsu",
            "Felicity Wang",
            "Qian Lou",
            "Yilin Shen",
            "Hongxia Jin"
        ],
        "published": "2022-11-02T00:58:02Z",
        "summary": "Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.",
        "pdf_link": "https://arxiv.org/pdf/2211.09718v2.pdf"
    },
    {
        "title": "BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder",
        "authors": [
            "Yosuke Higuchi",
            "Tetsuji Ogawa",
            "Tetsunori Kobayashi",
            "Shinji Watanabe"
        ],
        "published": "2022-11-02T00:10:43Z",
        "summary": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2211.00792v2.pdf"
    },
    {
        "title": "Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions",
        "authors": [
            "Alexey Skrynnik",
            "Zoya Volovikova",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Anton Voronov",
            "Artem Zholus",
            "Negar Arabzadeh",
            "Shrestha Mohanty",
            "Milagro Teruel",
            "Ahmed Awadallah",
            "Aleksandr Panov",
            "Mikhail Burtsev",
            "Julia Kiseleva"
        ],
        "published": "2022-11-01T18:30:42Z",
        "summary": "The adoption of pre-trained language models to generate action plans for\nembodied agents is a promising research strategy. However, execution of\ninstructions in real or simulated environments requires verification of the\nfeasibility of actions as well as their relevance to the completion of a goal.\nWe propose a new method that combines a language model and reinforcement\nlearning for the task of building objects in a Minecraft-like environment\naccording to the natural language instructions. Our method first generates a\nset of consistently achievable sub-goals from the instructions and then\ncompletes associated sub-tasks with a pre-trained RL policy. The proposed\nmethod formed the RL baseline at the IGLU 2022 competition.",
        "pdf_link": "https://arxiv.org/pdf/2211.00688v1.pdf"
    },
    {
        "title": "Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation",
        "authors": [
            "Cody Blakeney",
            "Jessica Zosa Forde",
            "Jonathan Frankle",
            "Ziliang Zong",
            "Matthew L. Leavitt"
        ],
        "published": "2022-11-01T18:16:00Z",
        "summary": "Methods for improving the efficiency of deep network training (i.e. the\nresources required to achieve a given level of model quality) are of immediate\nbenefit to deep learning practitioners. Distillation is typically used to\ncompress models or improve model quality, but it's unclear if distillation\nactually improves training efficiency. Can the quality improvements of\ndistillation be converted into training speed-ups, or do they simply increase\nfinal model quality with no resource savings? We conducted a series of\nexperiments to investigate whether and how distillation can be used to\naccelerate training using ResNet-50 trained on ImageNet and BERT trained on C4\nwith a masked language modeling objective and evaluated on GLUE, using common\nenterprise hardware (8x NVIDIA A100). We found that distillation can speed up\ntraining by up to 1.96x in ResNet-50 trained on ImageNet and up to 1.42x on\nBERT when evaluated on GLUE. Furthermore, distillation for BERT yields optimal\nresults when it is only performed for the first 20-50% of training. We also\nobserved that training with distillation is almost always more efficient than\ntraining without distillation, even when using the poorest-quality model as a\nteacher, in both ResNet-50 and BERT. Finally, we found that it's possible to\ngain the benefit of distilling from an ensemble of teacher models, which has\nO(n) runtime cost, by randomly sampling a single teacher from the pool of\nteacher models on each step, which only has a O(1) runtime cost. Taken\ntogether, these results show that distillation can substantially improve\ntraining efficiency in both image classification and language modeling, and\nthat a few simple optimizations to distillation protocols can further enhance\nthese efficiency improvements.",
        "pdf_link": "https://arxiv.org/pdf/2211.00683v1.pdf"
    },
    {
        "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
        "authors": [
            "Yihan Wang",
            "Si Si",
            "Daliang Li",
            "Michal Lukasik",
            "Felix Yu",
            "Cho-Jui Hsieh",
            "Inderjit S Dhillon",
            "Sanjiv Kumar"
        ],
        "published": "2022-11-01T17:56:57Z",
        "summary": "Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.",
        "pdf_link": "https://arxiv.org/pdf/2211.00635v3.pdf"
    },
    {
        "title": "Machine learning can guide experimental approaches for protein digestibility estimations",
        "authors": [
            "Sara Malvar",
            "Anvita Bhagavathula",
            "Maria Angels de Luis Balaguer",
            "Swati Sharma",
            "Ranveer Chandra"
        ],
        "published": "2022-11-01T17:43:58Z",
        "summary": "Food protein digestibility and bioavailability are critical aspects in\naddressing human nutritional demands, particularly when seeking sustainable\nalternatives to animal-based proteins. In this study, we propose a machine\nlearning approach to predict the true ileal digestibility coefficient of food\nitems. The model makes use of a unique curated dataset that combines\nnutritional information from different foods with FASTA sequences of some of\ntheir protein families. We extracted the biochemical properties of the proteins\nand combined these properties with embeddings from a Transformer-based protein\nLanguage Model (pLM). In addition, we used SHAP to identify features that\ncontribute most to the model prediction and provide interpretability. This\nfirst AI-based model for predicting food protein digestibility has an accuracy\nof 90% compared to existing experimental techniques. With this accuracy, our\nmodel can eliminate the need for lengthy in-vivo or in-vitro experiments,\nmaking the process of creating new foods faster, cheaper, and more ethical.",
        "pdf_link": "https://arxiv.org/pdf/2211.00625v1.pdf"
    },
    {
        "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small",
        "authors": [
            "Kevin Wang",
            "Alexandre Variengien",
            "Arthur Conmy",
            "Buck Shlegeris",
            "Jacob Steinhardt"
        ],
        "published": "2022-11-01T17:08:44Z",
        "summary": "Research in mechanistic interpretability seeks to explain behaviors of\nmachine learning models in terms of their internal components. However, most\nprevious work either focuses on simple behaviors in small models, or describes\ncomplicated behaviors in larger models with broad strokes. In this work, we\nbridge this gap by presenting an explanation for how GPT-2 small performs a\nnatural language task called indirect object identification (IOI). Our\nexplanation encompasses 26 attention heads grouped into 7 main classes, which\nwe discovered using a combination of interpretability approaches relying on\ncausal interventions. To our knowledge, this investigation is the largest\nend-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a\nlanguage model. We evaluate the reliability of our explanation using three\nquantitative criteria--faithfulness, completeness and minimality. Though these\ncriteria support our explanation, they also point to remaining gaps in our\nunderstanding. Our work provides evidence that a mechanistic understanding of\nlarge ML models is feasible, opening opportunities to scale our understanding\nto both larger models and more complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.00593v1.pdf"
    },
    {
        "title": "T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5",
        "authors": [
            "Chan-Jan Hsu",
            "Ho-Lam Chung",
            "Hung-yi Lee",
            "Yu Tsao"
        ],
        "published": "2022-11-01T17:00:23Z",
        "summary": "In Spoken language understanding (SLU), a natural solution is concatenating\npre-trained speech models (e.g. HuBERT) and pretrained language models (PLM,\ne.g. T5). Most previous works use pretrained language models with subword-based\ntokenization. However, the granularity of input units affects the alignment of\nspeech model outputs and language model inputs, and PLM with character-based\ntokenization is underexplored. In this work, we conduct extensive studies on\nhow PLMs with different tokenization strategies affect spoken language\nunderstanding task including spoken question answering (SQA) and speech\ntranslation (ST). We further extend the idea to create T5lephone(pronounced as\ntelephone), a variant of T5 that is pretrained using phonemicized text. We\ninitialize T5lephone with existing PLMs to pretrain it using relatively\nlightweight computational resources. We reached state-of-the-art on NMSQA, and\nthe T5lephone model exceeds T5 with other types of units on end-to-end SQA and\nST.",
        "pdf_link": "https://arxiv.org/pdf/2211.00586v1.pdf"
    },
    {
        "title": "VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding",
        "authors": [
            "Dou Hu",
            "Xiaolong Hou",
            "Xiyang Du",
            "Mengyuan Zhou",
            "Lianxin Jiang",
            "Yang Mo",
            "Xiaofeng Shi"
        ],
        "published": "2022-11-01T12:51:51Z",
        "summary": "Pre-trained language models have achieved promising performance on general\nbenchmarks, but underperform when migrated to a specific domain. Recent works\nperform pre-training from scratch or continual pre-training on domain corpora.\nHowever, in many specific domains, the limited corpus can hardly support\nobtaining precise representations. To address this issue, we propose a novel\nTransformer-based language model named VarMAE for domain-adaptive language\nunderstanding. Under the masked autoencoding objective, we design a context\nuncertainty learning module to encode the token's context into a smooth latent\ndistribution. The module can produce diverse and well-formed contextual\nrepresentations. Experiments on science- and finance-domain NLU tasks\ndemonstrate that VarMAE can be efficiently adapted to new domains with limited\nresources.",
        "pdf_link": "https://arxiv.org/pdf/2211.00430v1.pdf"
    },
    {
        "title": "The future is different: Large pre-trained language models fail in prediction tasks",
        "authors": [
            "Kostadin Cvejoski",
            "Rams\u00e9s J. S\u00e1nchez",
            "C\u00e9sar Ojeda"
        ],
        "published": "2022-11-01T11:01:36Z",
        "summary": "Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021",
        "pdf_link": "https://arxiv.org/pdf/2211.00384v2.pdf"
    },
    {
        "title": "Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features",
        "authors": [
            "Alexandra Vioni",
            "Georgia Maniati",
            "Nikolaos Ellinas",
            "June Sig Sung",
            "Inchul Hwang",
            "Aimilios Chalamandaris",
            "Pirros Tsiakoulis"
        ],
        "published": "2022-11-01T09:18:50Z",
        "summary": "Current state-of-the-art methods for automatic synthetic speech evaluation\nare based on MOS prediction neural models. Such MOS prediction models include\nMOSNet and LDNet that use spectral features as input, and SSL-MOS that relies\non a pretrained self-supervised learning model that directly uses the speech\nsignal as input. In modern high-quality neural TTS systems, prosodic\nappropriateness with regard to the spoken content is a decisive factor for\nspeech naturalness. For this reason, we propose to include prosodic and\nlinguistic features as additional inputs in MOS prediction systems, and\nevaluate their impact on the prediction outcome. We consider phoneme level F0\nand duration features as prosodic inputs, as well as Tacotron encoder outputs,\nPOS tags and BERT embeddings as higher-level linguistic inputs. All MOS\nprediction systems are trained on SOMOS, a neural TTS-only dataset with\ncrowdsourced naturalness MOS evaluations. Results show that the proposed\nadditional features are beneficial in the MOS prediction task, by improving the\npredicted MOS scores' correlation with the ground truths, both at\nutterance-level and system-level predictions.",
        "pdf_link": "https://arxiv.org/pdf/2211.00342v2.pdf"
    },
    {
        "title": "Training Vision-Language Models with Less Bimodal Supervision",
        "authors": [
            "Elad Segal",
            "Ben Bogin",
            "Jonathan Berant"
        ],
        "published": "2022-11-01T04:07:11Z",
        "summary": "Standard practice in pretraining multimodal models, such as vision-language\nmodels, is to rely on pairs of aligned inputs from both modalities, for\nexample, aligned image-text pairs. However, such pairs can be difficult to\nobtain in low-resource settings and for some modality pairs (e.g., structured\ntables and images). In this work, we investigate the extent to which we can\nreduce the reliance on such parallel data, which we term \\emph{bimodal\nsupervision}, and use models that are pretrained on each modality\nindependently. We experiment with a high-performing vision-language model, and\nanalyze the effect of bimodal supervision on three vision-language tasks. We\nfind that on simpler tasks, such as VQAv2 and GQA, one can eliminate bimodal\nsupervision completely, suffering only a minor loss in performance. Conversely,\nfor NLVR2, which requires more complex reasoning, training without bimodal\nsupervision leads to random performance. Nevertheless, using only 5\\% of the\nbimodal data (142K images along with their captions), or leveraging weak\nsupervision in the form of a list of machine-generated labels for each image,\nleads to only a moderate degradation compared to using 3M image-text pairs:\n74\\%$\\rightarrow$$\\sim$70\\%. Our code is available at\nhttps://github.com/eladsegal/less-bimodal-sup.",
        "pdf_link": "https://arxiv.org/pdf/2211.00262v1.pdf"
    },
    {
        "title": "WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain",
        "authors": [
            "Raj Sanjay Shah",
            "Kunal Chawla",
            "Dheeraj Eidnani",
            "Agam Shah",
            "Wendi Du",
            "Sudheer Chava",
            "Natraj Raman",
            "Charese Smiley",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "published": "2022-10-31T18:35:18Z",
        "summary": "Pre-trained language models have shown impressive performance on a variety of\ntasks and domains. Previous research on financial language models usually\nemploys a generic training scheme to train standard model architectures,\nwithout completely leveraging the richness of the financial data. We propose a\nnovel domain specific Financial LANGuage model (FLANG) which uses financial\nkeywords and phrases for better masking, together with span boundary objective\nand in-filing objective. Additionally, the evaluation benchmarks in the field\nhave been limited. To this end, we contribute the Financial Language\nUnderstanding Evaluation (FLUE), an open-source comprehensive suite of\nbenchmarks for the financial domain. These include new benchmarks across 5 NLP\ntasks in financial domain as well as common benchmarks used in the previous\nresearch. Experiments on these benchmarks suggest that our model outperforms\nthose in prior literature on a variety of NLP tasks. Our models, code and\nbenchmark data are publicly available on Github and Huggingface.",
        "pdf_link": "https://arxiv.org/pdf/2211.00083v1.pdf"
    },
    {
        "title": "Generating Sequences by Learning to Self-Correct",
        "authors": [
            "Sean Welleck",
            "Ximing Lu",
            "Peter West",
            "Faeze Brahman",
            "Tianxiao Shen",
            "Daniel Khashabi",
            "Yejin Choi"
        ],
        "published": "2022-10-31T18:09:51Z",
        "summary": "Sequence generation applications require satisfying semantic constraints,\nsuch as ensuring that programs are correct, using certain keywords, or avoiding\nundesirable content. Language models, whether fine-tuned or prompted with\nfew-shot demonstrations, frequently violate these constraints, and lack a\nmechanism to iteratively revise their outputs. Moreover, some powerful language\nmodels are of extreme scale or inaccessible, making it inefficient, if not\ninfeasible, to update their parameters for task-specific adaptation. We present\nSelf-Correction, an approach that decouples an imperfect base generator (an\noff-the-shelf language model or supervised sequence-to-sequence model) from a\nseparate corrector that learns to iteratively correct imperfect generations. To\ntrain the corrector, we propose an online training procedure that can use\neither scalar or natural language feedback on intermediate imperfect\ngenerations. We show that Self-Correction improves upon the base generator in\nthree diverse generation tasks - mathematical program synthesis,\nlexically-constrained generation, and toxicity control - even when the\ncorrector is much smaller than the base generator.",
        "pdf_link": "https://arxiv.org/pdf/2211.00053v1.pdf"
    },
    {
        "title": "Query Refinement Prompts for Closed-Book Long-Form Question Answering",
        "authors": [
            "Reinald Kim Amplayo",
            "Kellie Webster",
            "Michael Collins",
            "Dipanjan Das",
            "Shashi Narayan"
        ],
        "published": "2022-10-31T17:44:42Z",
        "summary": "Large language models (LLMs) have been shown to perform well in answering\nquestions and in producing long-form texts, both in few-shot closed-book\nsettings. While the former can be validated using well-known evaluation\nmetrics, the latter is difficult to evaluate. We resolve the difficulties to\nevaluate long-form output by doing both tasks at once -- to do question\nanswering that requires long-form answers. Such questions tend to be\nmultifaceted, i.e., they may have ambiguities and/or require information from\nmultiple sources. To this end, we define query refinement prompts that\nencourage LLMs to explicitly express the multifacetedness in questions and\ngenerate long-form answers covering multiple facets of the question. Our\nexperiments on two long-form question answering datasets, ASQA and AQuAMuSe,\nshow that using our prompts allows us to outperform fully finetuned models in\nthe closed book setting, as well as achieve results comparable to\nretrieve-then-generate open-book models.",
        "pdf_link": "https://arxiv.org/pdf/2210.17525v1.pdf"
    },
    {
        "title": "Leveraging Pre-trained Models for Failure Analysis Triplets Generation",
        "authors": [
            "Kenneth Ezukwoke",
            "Anis Hoayek",
            "Mireille Batton-Hubert",
            "Xavier Boucher",
            "Pascal Gounet",
            "Jerome Adrian"
        ],
        "published": "2022-10-31T17:21:15Z",
        "summary": "Pre-trained Language Models recently gained traction in the Natural Language\nProcessing (NLP) domain for text summarization, generation and\nquestion-answering tasks. This stems from the innovation introduced in\nTransformer models and their overwhelming performance compared with Recurrent\nNeural Network Models (Long Short Term Memory (LSTM)). In this paper, we\nleverage the attention mechanism of pre-trained causal language models such as\nTransformer model for the downstream task of generating Failure Analysis\nTriplets (FATs) - a sequence of steps for analyzing defected components in the\nsemiconductor industry. We compare different transformer models for this\ngenerative task and observe that Generative Pre-trained Transformer 2 (GPT2)\noutperformed other transformer model for the failure analysis triplet\ngeneration (FATG) task. In particular, we observe that GPT2 (trained on 1.5B\nparameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on\nROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric\n(LESE) for better evaluation of the structured FAT data and show that it\ncompares exactly with human judgment than existing metrics.",
        "pdf_link": "https://arxiv.org/pdf/2210.17497v1.pdf"
    },
    {
        "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes",
        "authors": [
            "Avyav Kumar Singh",
            "Ekaterina Shutova",
            "Helen Yannakoudakis"
        ],
        "published": "2022-10-31T16:06:48Z",
        "summary": "Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2210.17437v3.pdf"
    },
    {
        "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "authors": [
            "Xiaochuang Han",
            "Sachin Kumar",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-31T16:02:00Z",
        "summary": "Despite the growing success of diffusion models in continuous-valued domains\n(e.g., images), similar efforts for discrete domains such as text have yet to\nmatch the performance of autoregressive language models. In this work, we\npresent SSD-LM -- a diffusion-based language model with two key design choices.\nFirst, SSD-LM is semi-autoregressive, iteratively generating blocks of text,\nallowing for flexible output length at decoding time while enabling local\nbidirectional context updates. Second, it is simplex-based, performing\ndiffusion on the natural vocabulary space rather than a learned latent space,\nallowing us to incorporate classifier guidance and modular control using\noff-the-shelf classifiers without any adaptation. We evaluate SSD-LM on\nunconstrained text generation benchmarks, and show that it matches or\noutperforms strong autoregressive GPT-2 models across standard quality and\ndiversity metrics, while vastly outperforming diffusion-based baselines. On\ncontrolled text generation, SSD-LM also outperforms competitive baselines, with\nan extra advantage in modularity.",
        "pdf_link": "https://arxiv.org/pdf/2210.17432v2.pdf"
    },
    {
        "title": "Emergent Linguistic Structures in Neural Networks are Fragile",
        "authors": [
            "Emanuele La Malfa",
            "Matthew Wicker",
            "Marta Kwiatkowska"
        ],
        "published": "2022-10-31T15:43:57Z",
        "summary": "Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structures. In this paper, focusing on\nthe ability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.17406v8.pdf"
    },
    {
        "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation",
        "authors": [
            "Spyridon Mouselinos",
            "Mateusz Malinowski",
            "Henryk Michalewski"
        ],
        "published": "2022-10-31T15:06:15Z",
        "summary": "Recently, high-performing code generation systems based on large language\nmodels have surfaced. They are trained on massive corpora containing much more\nnatural text than actual executable computer code. This work shows that current\ncode generation systems exhibit undesired biases inherited from their large\nlanguage model backbones, which can reduce the quality of the generated code\nunder specific circumstances.\n  To investigate the effect, we propose the \"block of influence\" concept, which\nenables a modular decomposition and analysis of the coding challenges. We\nintroduce an automated intervention mechanism reminiscent of adversarial\ntesting that exposes undesired biases through the failure modes of the models\nunder test. Finally, we demonstrate how our framework can be used as a data\ntransformation technique during fine-tuning, acting as a mitigation strategy\nfor these biases.",
        "pdf_link": "https://arxiv.org/pdf/2211.00609v2.pdf"
    },
    {
        "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
        "authors": [
            "Elias Frantar",
            "Saleh Ashkboos",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "published": "2022-10-31T13:42:40Z",
        "summary": "Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.",
        "pdf_link": "https://arxiv.org/pdf/2210.17323v2.pdf"
    },
    {
        "title": "Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3",
        "authors": [
            "Pragya Srivastava",
            "Tanuja Ganu",
            "Saikat Guha"
        ],
        "published": "2022-10-31T13:08:55Z",
        "summary": "We present very early results on using GPT-3 to perform question answering on\ntabular data. We find that stock pre-trained GPT-3 is able to zero-shot learn\nthe table structure from a serialized JSON array-of-arrays representation, and\nable to answer lookup queries and simple comparison questions in natural\nlanguage without any fine-tuning. We further find that simple prompt\nengineering to include few-shot static Q&A examples significantly improves\naccuracy. Lastly, we find that intermixing passage text improves accuracy even\nfurther on heterogeneous data. We apply our approach on a novel dataset of\nsimple tables in newspaper infographics with promising results. Overall, we\nfind much cause for optimism in this basic approach.",
        "pdf_link": "https://arxiv.org/pdf/2210.17284v1.pdf"
    },
    {
        "title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task",
        "authors": [
            "Nyoungwoo Lee",
            "ChaeHun Park",
            "Ho-Jin Choi",
            "Jaegul Choo"
        ],
        "published": "2022-10-31T11:49:49Z",
        "summary": "In retrieval-based dialogue systems, a response selection model acts as a\nranker to select the most appropriate response among several candidates.\nHowever, such selection models tend to rely on context-response content\nsimilarity, which makes models vulnerable to adversarial responses that are\nsemantically similar but not relevant to the dialogue context. Recent studies\nhave shown that leveraging these adversarial responses as negative training\nsamples is useful for improving the discriminating power of the selection\nmodel. Nevertheless, collecting human-written adversarial responses is\nexpensive, and existing synthesizing methods often have limited scalability. To\novercome these limitations, this paper proposes a simple but efficient method\nfor generating adversarial negative responses leveraging a large-scale language\nmodel. Experimental results on dialogue selection tasks show that our method\noutperforms other methods of synthesizing adversarial negative responses. These\nresults suggest that our method can be an effective alternative to human\nannotators in generating adversarial responses. Our dataset and generation code\nis available at https://github.com/leenw23/generating-negatives-by-gpt3.",
        "pdf_link": "https://arxiv.org/pdf/2210.17238v1.pdf"
    },
    {
        "title": "When Language Model Meets Private Library",
        "authors": [
            "Daoguang Zan",
            "Bei Chen",
            "Zeqi Lin",
            "Bei Guan",
            "Yongji Wang",
            "Jian-Guang Lou"
        ],
        "published": "2022-10-31T11:42:06Z",
        "summary": "With the rapid development of pre-training techniques, a number of language\nmodels have been pre-trained on large-scale code corpora and perform well in\ncode generation. In this paper, we investigate how to equip pre-trained\nlanguage models with the ability of code generation for private libraries. In\npractice, it is common for programmers to write code using private libraries.\nHowever, this is a challenge for language models since they have never seen\nprivate APIs during training. Motivated by the fact that private libraries\nusually come with elaborate API documentation, we propose a novel framework\nwith two modules: the APIRetriever finds useful APIs, and then the APICoder\ngenerates code using these APIs. For APIRetriever, we present a dense retrieval\nsystem and also design a friendly interaction to involve uses. For APICoder, we\ncan directly use off-the-shelf language models, or continually pre-train the\nbase model on a code corpus containing API information. Both modules are\ntrained with data from public libraries and can be generalized to private ones.\nFurthermore, we craft three benchmarks for private libraries, named\nTorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate\nthe impressive performance of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2210.17236v1.pdf"
    },
    {
        "title": "SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking",
        "authors": [
            "Xiaotian Zhang",
            "Hang Yan",
            "Yu Sun",
            "Xipeng Qiu"
        ],
        "published": "2022-10-31T09:29:21Z",
        "summary": "Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has\nwidespread applications. Existing systems typically utilize BERT for text\nencoding. However, CSC requires the model to account for both phonetic and\ngraphemic information. To adapt BERT to the CSC task, we propose a token-level\nself-distillation contrastive learning method. We employ BERT to encode both\nthe corrupted and corresponding correct sentence. Then, we use contrastive\nlearning loss to regularize corrupted tokens' hidden states to be closer to\ncounterparts in the correct sentence. On three CSC datasets, we confirmed our\nmethod provides a significant improvement above baselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.17168v4.pdf"
    },
    {
        "title": "Improving Cause-of-Death Classification from Verbal Autopsy Reports",
        "authors": [
            "Thokozile Manaka",
            "Terence van Zyl",
            "Deepak Kar"
        ],
        "published": "2022-10-31T09:14:08Z",
        "summary": "In many lower-and-middle income countries including South Africa, data access\nin health facilities is restricted due to patient privacy and confidentiality\npolicies. Further, since clinical data is unique to individual institutions and\nlaboratories, there are insufficient data annotation standards and conventions.\nAs a result of the scarcity of textual data, natural language processing (NLP)\ntechniques have fared poorly in the health sector. A cause of death (COD) is\noften determined by a verbal autopsy (VA) report in places without reliable\ndeath registration systems. A non-clinician field worker does a VA report using\na set of standardized questions as a guide to uncover symptoms of a COD. This\nanalysis focuses on the textual part of the VA report as a case study to\naddress the challenge of adapting NLP techniques in the health domain. We\npresent a system that relies on two transfer learning paradigms of monolingual\nlearning and multi-source domain adaptation to improve VA narratives for the\ntarget task of the COD classification. We use the Bidirectional Encoder\nRepresentations from Transformers (BERT) and Embeddings from Language Models\n(ELMo) models pre-trained on the general English and health domains to extract\nfeatures from the VA narratives. Our findings suggest that this transfer\nlearning system improves the COD classification tasks and that the narrative\ntext contains valuable information for figuring out a COD. Our results further\nshow that combining binary VA features and narrative text features learned via\nthis framework boosts the classification task of COD.",
        "pdf_link": "https://arxiv.org/pdf/2210.17161v1.pdf"
    },
    {
        "title": "1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector",
        "authors": [
            "Xingran Chen",
            "Ge Zhang",
            "Adam Nik",
            "Mingyu Li",
            "Jie Fu"
        ],
        "published": "2022-10-31T09:09:59Z",
        "summary": "In this paper, we present our approach and empirical observations for\nCause-Effect Signal Span Detection -- Subtask 2 of Shared task\n3~\\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the\ncause, effect, and signal spans from a given causal sentence. We model the task\nas a reading comprehension (RC) problem and apply a token-level RC-based span\nprediction paradigm to the task as the baseline. We explore different training\nobjectives to fine-tune the model, as well as data augmentation (DA) tricks\nbased on the language model (LM) for performance improvement. Additionally, we\npropose an efficient beam-search post-processing strategy to due with the\ndrawbacks of span detection to obtain a further performance gain. Our approach\nachieves an average $F_1$ score of 54.15 and ranks \\textbf{$1^{st}$} in the\nCASE competition. Our code is available at\n\\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.",
        "pdf_link": "https://arxiv.org/pdf/2210.17157v1.pdf"
    },
    {
        "title": "Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change",
        "authors": [
            "Zhaochen Su",
            "Zecheng Tang",
            "Xinyan Guan",
            "Juntao Li",
            "Lijun Wu",
            "Min Zhang"
        ],
        "published": "2022-10-31T08:12:41Z",
        "summary": "Recent research has revealed that neural language models at scale suffer from\npoor temporal generalization capability, i.e., the language model pre-trained\non static data from past years performs worse over time on emerging data.\nExisting methods mainly perform continual training to mitigate such a\nmisalignment. While effective to some extent but is far from being addressed on\nboth the language modeling and downstream tasks. In this paper, we empirically\nobserve that temporal generalization is closely affiliated with lexical\nsemantic change, which is one of the essential phenomena of natural languages.\nBased on this observation, we propose a simple yet effective lexical-level\nmasking strategy to post-train a converged language model. Experiments on two\npre-trained language models, two different classification tasks, and four\nbenchmark datasets demonstrate the effectiveness of our proposed method over\nexisting temporal adaptation methods, i.e., continual training with new data.\nOur code is available at \\url{https://github.com/zhaochen0110/LMLM}.",
        "pdf_link": "https://arxiv.org/pdf/2210.17127v1.pdf"
    },
    {
        "title": "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM",
        "authors": [
            "Shira Guskin",
            "Moshe Wasserblat",
            "Chang Wang",
            "Haihao Shen"
        ],
        "published": "2022-10-31T07:42:52Z",
        "summary": "Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with <1%\naccuracy loss). The code to reproduce this work is publicly available on\nGithub.",
        "pdf_link": "https://arxiv.org/pdf/2210.17114v3.pdf"
    },
    {
        "title": "Modular Hybrid Autoregressive Transducer",
        "authors": [
            "Zhong Meng",
            "Tongzhou Chen",
            "Rohit Prabhavalkar",
            "Yu Zhang",
            "Gary Wang",
            "Kartik Audhkhasi",
            "Jesse Emond",
            "Trevor Strohman",
            "Bhuvana Ramabhadran",
            "W. Ronny Huang",
            "Ehsan Variani",
            "Yinghui Huang",
            "Pedro J. Moreno"
        ],
        "published": "2022-10-31T03:56:37Z",
        "summary": "Text-only adaptation of a transducer model remains challenging for end-to-end\nspeech recognition since the transducer has no clearly separated acoustic model\n(AM), language model (LM) or blank model. In this work, we propose a modular\nhybrid autoregressive transducer (MHAT) that has structurally separated label\nand blank decoders to predict label and blank distributions, respectively,\nalong with a shared acoustic encoder. The encoder and label decoder outputs are\ndirectly projected to AM and internal LM scores and then added to compute label\nposteriors. We train MHAT with an internal LM loss and a HAT loss to ensure\nthat its internal LM becomes a standalone neural LM that can be effectively\nadapted to text. Moreover, text adaptation of MHAT fosters a much better LM\nfusion than internal LM subtraction-based methods. On Google's large-scale\nproduction data, a multi-domain MHAT adapted with 100B sentences achieves\nrelative WER reductions of up to 12.4% without LM fusion and 21.5% with LM\nfusion from 400K-hour trained HAT.",
        "pdf_link": "https://arxiv.org/pdf/2210.17049v2.pdf"
    },
    {
        "title": "Blank Collapse: Compressing CTC emission for the faster decoding",
        "authors": [
            "Minkyu Jung",
            "Ohhyeok Kwon",
            "Seunghyun Seo",
            "Soonshin Seo"
        ],
        "published": "2022-10-31T02:12:51Z",
        "summary": "Connectionist Temporal Classification (CTC) model is a very efficient method\nfor modeling sequences, especially for speech data. In order to use CTC model\nas an Automatic Speech Recognition (ASR) task, the beam search decoding with an\nexternal language model like n-gram LM is necessary to obtain reasonable\nresults. In this paper we analyze the blank label in CTC beam search deeply and\npropose a very simple method to reduce the amount of calculation resulting in\nfaster beam search decoding speed. With this method, we can get up to 78%\nfaster decoding speed than ordinary beam search decoding with a very small loss\nof accuracy in LibriSpeech datasets. We prove this method is effective not only\npractically by experiments but also theoretically by mathematical reasoning. We\nalso observe that this reduction is more obvious if the accuracy of the model\nis higher.",
        "pdf_link": "https://arxiv.org/pdf/2210.17017v2.pdf"
    },
    {
        "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
        "authors": [
            "Ben Zhou",
            "Kyle Richardson",
            "Xiaodong Yu",
            "Dan Roth"
        ],
        "published": "2022-10-30T15:38:03Z",
        "summary": "Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2210.16865v1.pdf"
    },
    {
        "title": "A Solvable Model of Neural Scaling Laws",
        "authors": [
            "Alexander Maloney",
            "Daniel A. Roberts",
            "James Sully"
        ],
        "published": "2022-10-30T15:13:18Z",
        "summary": "Large language models with a huge number of parameters, when trained on near\ninternet-sized number of tokens, have been empirically shown to obey neural\nscaling laws: specifically, their performance behaves predictably as a power\nlaw in either parameters or dataset size until bottlenecked by the other\nresource. To understand this better, we first identify the necessary properties\nallowing such scaling laws to arise and then propose a statistical model -- a\njoint generative data model and random feature model -- that captures this\nneural scaling phenomenology. By solving this model in the dual limit of large\ntraining set size and large number of parameters, we gain insight into (i) the\nstatistical structure of datasets and tasks that lead to scaling laws, (ii) the\nway nonlinear feature maps, such as those provided by neural networks, enable\nscaling laws when trained on these datasets, (iii) the optimality of the\nequiparameterization scaling of training sets and parameters, and (iv) whether\nsuch scaling laws can break down and how they behave when they do. Key findings\nare the manner in which the power laws that occur in the statistics of natural\ndatasets are extended by nonlinear random feature maps and then translated into\npower-law scalings of the test loss and how the finite extent of the data's\nspectral power law causes the model's performance to plateau.",
        "pdf_link": "https://arxiv.org/pdf/2210.16859v1.pdf"
    },
    {
        "title": "Parameter-Efficient Tuning Makes a Good Classification Head",
        "authors": [
            "Zhuoyi Yang",
            "Ming Ding",
            "Yanhui Guo",
            "Qingsong Lv",
            "Jie Tang"
        ],
        "published": "2022-10-30T08:29:20Z",
        "summary": "In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.",
        "pdf_link": "https://arxiv.org/pdf/2210.16771v2.pdf"
    },
    {
        "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model",
        "authors": [
            "Yosuke Higuchi",
            "Brian Yan",
            "Siddhant Arora",
            "Tetsuji Ogawa",
            "Tetsunori Kobayashi",
            "Shinji Watanabe"
        ],
        "published": "2022-10-29T18:19:44Z",
        "summary": "This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.16663v2.pdf"
    },
    {
        "title": "Empirical Evaluation of Post-Training Quantization Methods for Language Tasks",
        "authors": [
            "Ting Hu",
            "Christoph Meinel",
            "Haojin Yang"
        ],
        "published": "2022-10-29T14:51:41Z",
        "summary": "Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.",
        "pdf_link": "https://arxiv.org/pdf/2210.16621v1.pdf"
    },
    {
        "title": "NTULM: Enriching Social Media Text Representations with Non-Textual Units",
        "authors": [
            "Jinning Li",
            "Shubhanshu Mishra",
            "Ahmed El-Kishky",
            "Sneha Mehta",
            "Vivek Kulkarni"
        ],
        "published": "2022-10-29T12:18:04Z",
        "summary": "On social media, additional context is often present in the form of\nannotations and meta-data such as the post's author, mentions, Hashtags, and\nhyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit\nthat NTUs provide social context beyond their textual semantics and leveraging\nthese units can enrich social media text representations. In this work we\nconstruct an NTU-centric social heterogeneous network to co-embed NTUs. We then\nprincipally integrate these NTU embeddings into a large pretrained language\nmodel by fine-tuning with these additional units. This adds context to noisy\nshort-text social media. Experiments show that utilizing NTU-augmented text\nrepresentations significantly outperforms existing text-only baselines by 2-5\\%\nrelative points on many downstream tasks highlighting the importance of context\nto social media NLP. We also highlight that including NTU context into the\ninitial layers of language model alongside text is better than using it after\nthe text embedding is generated. Our work leads to the generation of holistic\ngeneral purpose social media content embedding.",
        "pdf_link": "https://arxiv.org/pdf/2210.16586v1.pdf"
    },
    {
        "title": "Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection",
        "authors": [
            "Yi Wang",
            "Jiajun Deng",
            "Tianzi Wang",
            "Bo Zheng",
            "Shoukang Hu",
            "Xunying Liu",
            "Helen Meng"
        ],
        "published": "2022-10-29T09:18:41Z",
        "summary": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The decision voting based combination among systems using\ndifferent PLMs (BERT and RoBERTa) or systems with different fine-tuning\nparadigms (conventional masked-language modelling fine-tuning and prompt-based\nfine-tuning) is further applied. Mean, standard deviation and the maximum among\naccuracy scores over 15 experiment runs are adopted as performance measurements\nfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,\nbest 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual\nand ASR speech transcripts respectively on the ADReSS20 test set consisting of\n48 elderly speakers.",
        "pdf_link": "https://arxiv.org/pdf/2210.16539v2.pdf"
    },
    {
        "title": "Differentiable Data Augmentation for Contrastive Sentence Representation Learning",
        "authors": [
            "Tianduo Wang",
            "Wei Lu"
        ],
        "published": "2022-10-29T08:57:45Z",
        "summary": "Fine-tuning a pre-trained language model via the contrastive learning\nframework with a large amount of unlabeled sentences or labeled sentence pairs\nis a common way to obtain high-quality sentence representations. Although the\ncontrastive learning framework has shown its superiority on sentence\nrepresentation learning over previous methods, the potential of such a\nframework is under-explored so far due to the simple method it used to\nconstruct positive pairs. Motivated by this, we propose a method that makes\nhard positives from the original training examples. A pivotal ingredient of our\napproach is the use of prefix that is attached to a pre-trained language model,\nwhich allows for differentiable data augmentation during contrastive learning.\nOur method can be summarized in two steps: supervised prefix-tuning followed by\njoint contrastive fine-tuning with unlabeled or labeled examples. Our\nexperiments confirm the effectiveness of our data augmentation approach. The\nproposed method yields significant improvements over existing methods under\nboth semi-supervised and supervised settings. Our experiments under a low\nlabeled data setting also show that our method is more label-efficient than the\nstate-of-the-art contrastive learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.16536v1.pdf"
    },
    {
        "title": "Aligning Offline Metrics and Human Judgments of Value for Code Generation Models",
        "authors": [
            "Victor Dibia",
            "Adam Fourney",
            "Gagan Bansal",
            "Forough Poursabzi-Sangdeh",
            "Han Liu",
            "Saleema Amershi"
        ],
        "published": "2022-10-29T05:03:28Z",
        "summary": "Large language models have demonstrated great potential to assist programmers\nin generating code. For such human-AI pair programming scenarios, we\nempirically demonstrate that while generated code is most often evaluated in\nterms of their functional correctness (i.e., whether generations pass available\nunit tests), correctness does not fully capture (e.g., may underestimate) the\nproductivity gains these models may provide. Through a user study with N = 49\nexperienced programmers, we show that while correctness captures high-value\ngenerations, programmers still rate code that fails unit tests as valuable if\nit reduces the overall effort needed to complete a coding task. Finally, we\npropose a hybrid metric that combines functional correctness and syntactic\nsimilarity and show that it achieves a 14% stronger correlation with value and\ncan therefore better represent real-world gains when evaluating and comparing\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2210.16494v2.pdf"
    },
    {
        "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models",
        "authors": [
            "Xiaoman Pan",
            "Wenlin Yao",
            "Hongming Zhang",
            "Dian Yu",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2022-10-28T23:18:43Z",
        "summary": "Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.",
        "pdf_link": "https://arxiv.org/pdf/2210.16433v3.pdf"
    },
    {
        "title": "DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention",
        "authors": [
            "Fenglin Liu",
            "Xian Wu",
            "Shen Ge",
            "Xuancheng Ren",
            "Wei Fan",
            "Xu Sun",
            "Yuexian Zou"
        ],
        "published": "2022-10-28T23:00:40Z",
        "summary": "Vision-and-language (V-L) tasks require the system to understand both vision\ncontent and natural language, thus learning fine-grained joint representations\nof vision and language (a.k.a. V-L representations) is of paramount importance.\nRecently, various pre-trained V-L models are proposed to learn V-L\nrepresentations and achieve improved results in many tasks. However, the\nmainstream models process both vision and language inputs with the same set of\nattention matrices. As a result, the generated V-L representations are\nentangled in one common latent space. To tackle this problem, we propose\nDiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel\nframework that applies separated attention spaces for vision and language, and\nthe representations of multi-modalities can thus be disentangled explicitly. To\nenhance the correlation between vision and language in disentangled spaces, we\nintroduce the visual concepts to DiMBERT which represent visual information in\ntextual format. In this manner, visual concepts help to bridge the gap between\nthe two modalities. We pre-train DiMBERT on a large amount of image-sentence\npairs on two tasks: bidirectional language modeling and sequence-to-sequence\nlanguage modeling. After pre-train, DiMBERT is further fine-tuned for the\ndownstream tasks. Experiments show that DiMBERT sets new state-of-the-art\nperformance on three tasks (over four datasets), including both generation\ntasks (image captioning and visual storytelling) and classification tasks\n(referring expressions). The proposed DiM (short for Disentangled\nMultimodal-Attention) module can be easily incorporated into existing\npre-trained V-L models to boost their performance, up to a 5% increase on the\nrepresentative task. Finally, we conduct a systematic analysis and demonstrate\nthe effectiveness of our DiM and the introduced visual concepts.",
        "pdf_link": "https://arxiv.org/pdf/2210.16431v1.pdf"
    },
    {
        "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
        "authors": [
            "Xinyu Zhu",
            "Junjie Wang",
            "Lin Zhang",
            "Yuxiang Zhang",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yujiu Yang"
        ],
        "published": "2022-10-28T16:47:03Z",
        "summary": "Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenging problems, especially those that need high-level intelligence, such\nas the math word problem (MWPs). However, directly applying existing PLMs to\nMWPs can fail as the generation process lacks sufficient supervision and thus\nlacks fast adaptivity as humans. We notice that human reasoning has a dual\nreasoning framework that consists of an immediate reaction system (system 1)\nand a delicate reasoning system (system 2), where the entire reasoning is\ndetermined by their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.6% increase over best\nbaselines. Our codes are available at https://github.com/TianHongZXY/CoRe",
        "pdf_link": "https://arxiv.org/pdf/2210.16257v5.pdf"
    },
    {
        "title": "Probing for targeted syntactic knowledge through grammatical error detection",
        "authors": [
            "Christopher Davis",
            "Christopher Bryant",
            "Andrew Caines",
            "Marek Rei",
            "Paula Buttery"
        ],
        "published": "2022-10-28T16:01:25Z",
        "summary": "Targeted studies testing knowledge of subject-verb agreement (SVA) indicate\nthat pre-trained language models encode syntactic information. We assert that\nif models robustly encode subject-verb agreement, they should be able to\nidentify when agreement is correct and when it is incorrect. To that end, we\npropose grammatical error detection as a diagnostic probe to evaluate\ntoken-level contextual representations for their knowledge of SVA. We evaluate\ncontextual representations at each layer from five pre-trained English language\nmodels: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated\ntraining data from both English second language learners and Wikipedia edits,\nand report results on manually crafted stimuli for subject-verb agreement. We\nfind that masked language models linearly encode information relevant to the\ndetection of SVA errors, while the autoregressive models perform on par with\nour baseline. However, we also observe a divergence in performance when probes\nare trained on different training sets, and when they are evaluated on\ndifferent syntactic constructions, suggesting the information pertaining to SVA\nerror detection is not robustly encoded.",
        "pdf_link": "https://arxiv.org/pdf/2210.16228v1.pdf"
    },
    {
        "title": "Feature Engineering vs BERT on Twitter Data",
        "authors": [
            "Ryiaadh Gani",
            "Lisa Chalaguine"
        ],
        "published": "2022-10-28T14:43:13Z",
        "summary": "In this paper, we compare the performances of traditional machine learning\nmodels using feature engineering and word vectors and the state-of-the-art\nlanguage model BERT using word embeddings on three datasets. We also consider\nthe time and cost efficiency of feature engineering compared to BERT. From our\nresults we conclude that the use of the BERT model was only worth the time and\ncost trade-off for one of the three datasets we used for comparison, where the\nBERT model significantly outperformed any kind of traditional classifier that\nuses feature vectors, instead of embeddings. Using the BERT model for the other\ndatasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score\nrespectively, which could be argued makes its use not worth the time and cost\nof GPU.",
        "pdf_link": "https://arxiv.org/pdf/2210.16168v1.pdf"
    },
    {
        "title": "Modeling structure-building in the brain with CCG parsing and large language models",
        "authors": [
            "Milo\u0161 Stanojevi\u0107",
            "Jonathan R. Brennan",
            "Donald Dunagan",
            "Mark Steedman",
            "John T. Hale"
        ],
        "published": "2022-10-28T14:21:29Z",
        "summary": "To model behavioral and neural correlates of language comprehension in\nnaturalistic environments researchers have turned to broad-coverage tools from\nnatural-language processing and machine learning. Where syntactic structure is\nexplicitly modeled, prior work has relied predominantly on context-free\ngrammars (CFG), yet such formalisms are not sufficiently expressive for human\nlanguages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive\ndirectly compositional models of grammar with flexible constituency that\naffords incremental interpretation. In this work we evaluate whether a more\nexpressive CCG provides a better model than a CFG for human neural signals\ncollected with fMRI while participants listen to an audiobook story. We further\ntest between variants of CCG that differ in how they handle optional adjuncts.\nThese evaluations are carried out against a baseline that includes estimates of\nnext-word predictability from a Transformer neural network language model. Such\na comparison reveals unique contributions of CCG structure-building\npredominantly in the left posterior temporal lobe: CCG-derived measures offer a\nsuperior fit to neural signals compared to those derived from a CFG. These\neffects are spatially distinct from bilateral superior temporal effects that\nare unique to predictability. Neural effects for structure-building are thus\nseparable from predictability during naturalistic listening, and those effects\nare best characterized by a grammar whose expressive power is motivated on\nindependent linguistic grounds.",
        "pdf_link": "https://arxiv.org/pdf/2210.16147v3.pdf"
    },
    {
        "title": "Zero-Shot Text Matching for Automated Auditing using Sentence Transformers",
        "authors": [
            "David Biesner",
            "Maren Pielka",
            "Rajkumar Ramamurthy",
            "Tim Dilmaghani",
            "Bernd Kliem",
            "R\u00fcdiger Loitz",
            "Rafet Sifa"
        ],
        "published": "2022-10-28T11:52:16Z",
        "summary": "Natural language processing methods have several applications in automated\nauditing, including document or passage classification, information retrieval,\nand question answering. However, training such models requires a large amount\nof annotated data which is scarce in industrial settings. At the same time,\ntechniques like zero-shot and unsupervised learning allow for application of\nmodels pre-trained using general domain data to unseen domains.\n  In this work, we study the efficiency of unsupervised text matching using\nSentence-Bert, a transformer-based model, by applying it to the semantic\nsimilarity of financial passages. Experimental results show that this model is\nrobust to documents from in- and out-of-domain data.",
        "pdf_link": "https://arxiv.org/pdf/2211.07716v1.pdf"
    },
    {
        "title": "UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance",
        "authors": [
            "Wei Li",
            "Xue Xu",
            "Xinyan Xiao",
            "Jiachen Liu",
            "Hu Yang",
            "Guohao Li",
            "Zhanpeng Wang",
            "Zhifan Feng",
            "Qiaoqiao She",
            "Yajuan Lyu",
            "Hua Wu"
        ],
        "published": "2022-10-28T10:07:25Z",
        "summary": "Diffusion generative models have recently greatly improved the power of\ntext-conditioned image generation. Existing image generation models mainly\ninclude text conditional diffusion model and cross-modal guided diffusion\nmodel, which are good at small scene image generation and complex scene image\ngeneration respectively. In this work, we propose a simple yet effective\napproach, namely UPainting, to unify simple and complex scene image generation,\nas shown in Figure 1. Based on architecture improvements and diverse guidance\nschedules, UPainting effectively integrates cross-modal guidance from a\npretrained image-text matching model into a text conditional diffusion model\nthat utilizes a pretrained Transformer language model as the text encoder. Our\nkey findings is that combining the power of large-scale Transformer language\nmodel in understanding language and image-text matching model in capturing\ncross-modal semantics and style, is effective to improve sample fidelity and\nimage-text alignment of image generation. In this way, UPainting has a more\ngeneral image generation capability, which can generate images of both simple\nand complex scenes more effectively. To comprehensively compare text-to-image\nmodels, we further create a more general benchmark, UniBench, with well-written\nChinese and English prompts in both simple and complex scenes. We compare\nUPainting with recent models and find that UPainting greatly outperforms other\nmodels in terms of caption similarity and image fidelity in both simple and\ncomplex scenes. UPainting project page \\url{https://upainting.github.io/}.",
        "pdf_link": "https://arxiv.org/pdf/2210.16031v3.pdf"
    },
    {
        "title": "BEBERT: Efficient and Robust Binary Ensemble BERT",
        "authors": [
            "Jiayi Tian",
            "Chao Fang",
            "Haonan Wang",
            "Zhongfeng Wang"
        ],
        "published": "2022-10-28T08:15:26Z",
        "summary": "Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.",
        "pdf_link": "https://arxiv.org/pdf/2210.15976v2.pdf"
    },
    {
        "title": "RoChBert: Towards Robust BERT Fine-tuning for Chinese",
        "authors": [
            "Zihan Zhang",
            "Jinfeng Li",
            "Ning Shi",
            "Bo Yuan",
            "Xiangyu Liu",
            "Rong Zhang",
            "Hui Xue",
            "Donghong Sun",
            "Chao Zhang"
        ],
        "published": "2022-10-28T07:08:00Z",
        "summary": "Despite of the superb performance on a wide range of tasks, pre-trained\nlanguage models (e.g., BERT) have been proved vulnerable to adversarial texts.\nIn this paper, we present RoChBERT, a framework to build more Robust BERT-based\nmodels by utilizing a more comprehensive adversarial graph to fuse Chinese\nphonetic and glyph features into pre-trained representations during\nfine-tuning. Inspired by curriculum learning, we further propose to augment the\ntraining dataset with adversarial texts in combination with intermediate\nsamples. Extensive experiments demonstrate that RoChBERT outperforms previous\nmethods in significant ways: (i) robust -- RoChBERT greatly improves the model\nrobustness without sacrificing accuracy on benign texts. Specifically, the\ndefense lowers the success rates of unlimited and limited attacks by 59.43% and\n39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible --\nRoChBERT can easily extend to various language models to solve different\ndownstream tasks with excellent performance; and (iii) efficient -- RoChBERT\ncan be directly applied to the fine-tuning stage without pre-training language\nmodel from scratch, and the proposed data augmentation method is also low-cost.",
        "pdf_link": "https://arxiv.org/pdf/2210.15944v1.pdf"
    },
    {
        "title": "On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis",
        "authors": [
            "Atsushi Ando",
            "Ryo Masumura",
            "Akihiko Takashima",
            "Satoshi Suzuki",
            "Naoki Makishima",
            "Keita Suzuki",
            "Takafumi Moriya",
            "Takanori Ashihara",
            "Hiroshi Sato"
        ],
        "published": "2022-10-28T06:48:35Z",
        "summary": "This paper investigates the effectiveness and implementation of\nmodality-specific large-scale pre-trained encoders for multimodal sentiment\nanalysis~(MSA). Although the effectiveness of pre-trained encoders in various\nfields has been reported, conventional MSA methods employ them for only\nlinguistic modality, and their application has not been investigated. This\npaper compares the features yielded by large-scale pre-trained encoders with\nconventional heuristic features. One each of the largest pre-trained encoders\npublicly available for each modality are used; CLIP-ViT, WavLM, and BERT for\nvisual, acoustic, and linguistic modalities, respectively. Experiments on two\ndatasets reveal that methods with domain-specific pre-trained encoders attain\nbetter performance than those with conventional features in both unimodal and\nmultimodal scenarios. We also find it better to use the outputs of the\nintermediate layers of the encoders than those of the output layer. The codes\nare available at https://github.com/ando-hub/MSA_Pretrain.",
        "pdf_link": "https://arxiv.org/pdf/2210.15937v1.pdf"
    },
    {
        "title": "Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion",
        "authors": [
            "Georgios Chochlakis",
            "Gireesh Mahajan",
            "Sabyasachee Baruah",
            "Keith Burghardt",
            "Kristina Lerman",
            "Shrikanth Narayanan"
        ],
        "published": "2022-10-28T02:27:18Z",
        "summary": "Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.",
        "pdf_link": "https://arxiv.org/pdf/2210.15842v2.pdf"
    },
    {
        "title": "QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation",
        "authors": [
            "Krishna Srinivasan",
            "Karthik Raman",
            "Anupam Samanta",
            "Lingrui Liao",
            "Luca Bertelli",
            "Mike Bendersky"
        ],
        "published": "2022-10-27T18:44:58Z",
        "summary": "Large Language Models (LLMs) have shown impressive results on a variety of\ntext understanding tasks. Search queries though pose a unique challenge, given\ntheir short-length and lack of nuance or context. Complicated feature\nengineering efforts do not always lead to downstream improvements as their\nperformance benefits may be offset by increased complexity of knowledge\ndistillation. Thus, in this paper we make the following contributions: (1) We\ndemonstrate that Retrieval Augmentation of queries provides LLMs with valuable\nadditional context enabling improved understanding. While Retrieval\nAugmentation typically increases latency of LMs (thus hurting distillation\nefficacy), (2) we provide a practical and effective way of distilling Retrieval\nAugmentation LLMs. Specifically, we use a novel two-stage distillation approach\nthat allows us to carry over the gains of retrieval augmentation, without\nsuffering the increased compute typically associated with it. (3) We\ndemonstrate the benefits of the proposed approach (QUILL) on a billion-scale,\nreal-world query understanding system resulting in huge gains. Via extensive\nexperiments, including on public benchmarks, we believe this work offers a\nrecipe for practical use of retrieval-augmented query understanding.",
        "pdf_link": "https://arxiv.org/pdf/2210.15718v1.pdf"
    },
    {
        "title": "Simulating realistic speech overlaps improves multi-talker ASR",
        "authors": [
            "Muqiao Yang",
            "Naoyuki Kanda",
            "Xiaofei Wang",
            "Jian Wu",
            "Sunit Sivasankaran",
            "Zhuo Chen",
            "Jinyu Li",
            "Takuya Yoshioka"
        ],
        "published": "2022-10-27T18:29:39Z",
        "summary": "Multi-talker automatic speech recognition (ASR) has been studied to generate\ntranscriptions of natural conversation including overlapping speech of multiple\nspeakers. Due to the difficulty in acquiring real conversation data with\nhigh-quality human transcriptions, a na\\\"ive simulation of multi-talker speech\nby randomly mixing multiple utterances was conventionally used for model\ntraining. In this work, we propose an improved technique to simulate\nmulti-talker overlapping speech with realistic speech overlaps, where an\narbitrary pattern of speech overlaps is represented by a sequence of discrete\ntokens. With this representation, speech overlapping patterns can be learned\nfrom real conversations based on a statistical language model, such as N-gram,\nwhich can be then used to generate multi-talker speech for training. In our\nexperiments, multi-talker ASR models trained with the proposed method show\nconsistent improvement on the word error rates across multiple datasets.",
        "pdf_link": "https://arxiv.org/pdf/2210.15715v2.pdf"
    },
    {
        "title": "COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models",
        "authors": [
            "Bowen Shen",
            "Zheng Lin",
            "Yuanxin Liu",
            "Zhengxiao Liu",
            "Lei Wang",
            "Weiping Wang"
        ],
        "published": "2022-10-27T15:06:40Z",
        "summary": "Transformer-based pre-trained language models (PLMs) mostly suffer from\nexcessive overhead despite their advanced capacity. For resource-constrained\ndevices, there is an urgent need for a spatially and temporally efficient model\nwhich retains the major capacity of PLMs. However, existing statically\ncompressed models are unaware of the diverse complexities between input\ninstances, potentially resulting in redundancy and inadequacy for simple and\ncomplex inputs. Also, miniature models with early exiting encounter challenges\nin the trade-off between making predictions and serving the deeper layers.\nMotivated by such considerations, we propose a collaborative optimization for\nPLMs that integrates static model compression and dynamic inference\nacceleration. Specifically, the PLM is slenderized in width while the depth\nremains intact, complementing layer-wise early exiting to speed up inference\ndynamically. To address the trade-off of early exiting, we propose a joint\ntraining approach that calibrates slenderization and preserves contributive\nstructures to each exit instead of only the final layer. Experiments are\nconducted on GLUE benchmark and the results verify the Pareto optimality of our\napproach at high compression and acceleration rate with 1/8 parameters and 1/19\nFLOPs of BERT.",
        "pdf_link": "https://arxiv.org/pdf/2210.15523v1.pdf"
    },
    {
        "title": "What Language Model to Train if You Have One Million GPU Hours?",
        "authors": [
            "Teven Le Scao",
            "Thomas Wang",
            "Daniel Hesslow",
            "Lucile Saulnier",
            "Stas Bekman",
            "M Saiful Bari",
            "Stella Biderman",
            "Hady Elsahar",
            "Niklas Muennighoff",
            "Jason Phang",
            "Ofir Press",
            "Colin Raffel",
            "Victor Sanh",
            "Sheng Shen",
            "Lintang Sutawika",
            "Jaesung Tae",
            "Zheng Xin Yong",
            "Julien Launay",
            "Iz Beltagy"
        ],
        "published": "2022-10-27T13:43:27Z",
        "summary": "The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .",
        "pdf_link": "https://arxiv.org/pdf/2210.15424v2.pdf"
    },
    {
        "title": "FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis",
        "authors": [
            "Yifan Hu",
            "Rui Liu",
            "Guanglai Gao",
            "Haizhou Li"
        ],
        "published": "2022-10-27T12:20:20Z",
        "summary": "Conversational Text-to-Speech (TTS) aims to synthesis an utterance with the\nright linguistic and affective prosody in a conversational context. The\ncorrelation between the current utterance and the dialogue history at the\nutterance level was used to improve the expressiveness of synthesized speech.\nHowever, the fine-grained information in the dialogue history at the word level\nalso has an important impact on the prosodic expression of an utterance, which\nhas not been well studied in the prior work. Therefore, we propose a novel\nexpressive conversational TTS model, termed as FCTalker, that learn the fine\nand coarse grained context dependency at the same time during speech\ngeneration. Specifically, the FCTalker includes fine and coarse grained\nencoders to exploit the word and utterance-level context dependency. To model\nthe word-level dependencies between an utterance and its dialogue history, the\nfine-grained dialogue encoder is built on top of a dialogue BERT model. The\nexperimental results show that the proposed method outperforms all baselines\nand generates more expressive speech that is contextually appropriate. We\nrelease the source code at: https://github.com/walker-hyf/FCTalker.",
        "pdf_link": "https://arxiv.org/pdf/2210.15360v1.pdf"
    },
    {
        "title": "SAN: a robust end-to-end ASR model architecture",
        "authors": [
            "Zeping Min",
            "Qian Ge",
            "Guanhua Huang"
        ],
        "published": "2022-10-27T09:36:25Z",
        "summary": "In this paper, we propose a novel Siamese Adversarial Network (SAN)\narchitecture for automatic speech recognition, which aims at solving the\ndifficulty of fuzzy audio recognition. Specifically, SAN constructs two\nsub-networks to differentiate the audio feature input and then introduces a\nloss to unify the output distribution of these sub-networks. Adversarial\nlearning enables the network to capture more essential acoustic features and\nhelps the models achieve better performance when encountering fuzzy audio\ninput. We conduct numerical experiments with the SAN model on several datasets\nfor the automatic speech recognition task. All experimental results show that\nthe siamese adversarial nets significantly reduce the character error rate\n(CER). Specifically, we achieve a new state of art 4.37 CER without language\nmodel on the AISHELL-1 dataset, which leads to around 5% relative CER\nreduction. To reveal the generality of the siamese adversarial net, we also\nconduct experiments on the phoneme recognition task, which also shows the\nsuperiority of the siamese adversarial network.",
        "pdf_link": "https://arxiv.org/pdf/2210.15285v1.pdf"
    },
    {
        "title": "Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model",
        "authors": [
            "Ju-Hyung Lee",
            "Dong-Ho Lee",
            "Eunsoo Sheen",
            "Thomas Choi",
            "Jay Pujara"
        ],
        "published": "2022-10-27T07:48:18Z",
        "summary": "In this work, we propose a realistic semantic network called seq2seq-SC,\ndesigned to be compatible with 5G NR and capable of working with generalized\ntext datasets using a pre-trained language model. The goal is to achieve\nunprecedented communication efficiency by focusing on the meaning of messages\nin semantic communication. We employ a performance metric called semantic\nsimilarity, measured by BLEU for lexical similarity and SBERT for semantic\nsimilarity. Our findings demonstrate that seq2seq-SC outperforms previous\nmodels in extracting semantically meaningful information while maintaining\nsuperior performance. This study paves the way for continued advancements in\nsemantic communication and its prospective incorporation with future wireless\nsystems in 6G networks.",
        "pdf_link": "https://arxiv.org/pdf/2210.15237v2.pdf"
    },
    {
        "title": "Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling",
        "authors": [
            "Peijie Jiang",
            "Dingkun Long",
            "Yanzhao Zhang",
            "Pengjun Xie",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "published": "2022-10-27T07:38:50Z",
        "summary": "Boundary information is critical for various Chinese language processing\ntasks, such as word segmentation, part-of-speech tagging, and named entity\nrecognition. Previous studies usually resorted to the use of a high-quality\nexternal lexicon, where lexicon items can offer explicit boundary information.\nHowever, to ensure the quality of the lexicon, great human effort is always\nnecessary, which has been generally ignored. In this work, we suggest\nunsupervised statistical boundary information instead, and propose an\narchitecture to encode the information directly into pre-trained language\nmodels, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature\ninduction of Chinese sequence labeling tasks. Experimental results on ten\nbenchmarks of Chinese sequence labeling demonstrate that BABERT can provide\nconsistent improvements on all datasets. In addition, our method can complement\nprevious supervised lexicon exploration, where further improvements can be\nachieved when integrated with external lexicon information.",
        "pdf_link": "https://arxiv.org/pdf/2210.15231v1.pdf"
    },
    {
        "title": "BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification",
        "authors": [
            "Ziwen Liu",
            "Josep Grau-Bove",
            "Scott Allan Orr"
        ],
        "published": "2022-10-27T07:18:56Z",
        "summary": "Multi-label Text Classification (MLTC) is the task of categorizing documents\ninto one or more topics. Considering the large volumes of data and varying\ndomains of such tasks, fully supervised learning requires manually fully\nannotated datasets which is costly and time-consuming. In this paper, we\npropose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text\nClassification (WSMLTC) model that reduces the need for full supervision. This\nnew model (1) produces BERT sentence embeddings and calibrates them using a\nflow model, (2) generates an initial topic-document matrix by averaging results\nof a seeded sparse topic model and a textual entailment model which only\nrequire surface name of topics and 4-6 seed words per topic, and (3) adopts a\nVAE framework to reconstruct the embeddings under the guidance of the\ntopic-document matrix. Finally, (4) it uses the means produced by the encoder\nmodel in the VAE architecture as predictions for MLTC. Experimental results on\n6 multi-label datasets show that BFV can substantially outperform other\nbaseline WSMLTC models in key metrics and achieve approximately 84% performance\nof a fully-supervised model.",
        "pdf_link": "https://arxiv.org/pdf/2210.15225v1.pdf"
    },
    {
        "title": "COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
        "authors": [
            "Yue Yu",
            "Chenyan Xiong",
            "Si Sun",
            "Chao Zhang",
            "Arnold Overwijk"
        ],
        "published": "2022-10-27T06:51:39Z",
        "summary": "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to\nimprove the generalization ability of dense retrieval by combating the\ndistribution shifts between source training tasks and target scenarios. To\nmitigate the impact of document differences, COCO-DR continues pretraining the\nlanguage model on the target corpora to adapt the model to target distributions\nvia COtinuous COtrastive learning. To prepare for unseen target queries,\nCOCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to\nreweight samples from different source query clusters for improving model\nrobustness over rare queries during fine-tuning. COCO-DR achieves superior\naverage performance on BEIR, the zero-shot retrieval benchmark. At BERT Base\nscale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At\nBERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model\nwhich has 500x more parameters. Our analysis show the correlation between\nCOCO-DR's effectiveness in combating distribution shifts and improving\nzero-shot accuracy. Our code and model can be found at\n\\url{https://github.com/OpenMatch/COCO-DR}.",
        "pdf_link": "https://arxiv.org/pdf/2210.15212v2.pdf"
    },
    {
        "title": "Truncation Sampling as Language Model Desmoothing",
        "authors": [
            "John Hewitt",
            "Christopher D. Manning",
            "Percy Liang"
        ],
        "published": "2022-10-27T05:52:35Z",
        "summary": "Long samples of text from neural language models can be of poor quality.\nTruncation sampling algorithms--like top-$p$ or top-$k$ -- address this by\nsetting some words' probabilities to zero at each step. This work provides\nframing for the aim of truncation, and an improved algorithm for that aim. We\npropose thinking of a neural language model as a mixture of a true distribution\nand a smoothing distribution that avoids infinite perplexity. In this light,\ntruncation algorithms aim to perform desmoothing, estimating a subset of the\nsupport of the true distribution. Finding a good subset is crucial: we show\nthat top-$p$ unnecessarily truncates high-probability words, for example\ncausing it to truncate all words but Trump for a document that starts with\nDonald. We introduce $\\eta$-sampling, which truncates words below an\nentropy-dependent probability threshold. Compared to previous algorithms,\n$\\eta$-sampling generates more plausible long English documents according to\nhumans, is better at breaking out of repetition, and behaves more reasonably on\na battery of test distributions.",
        "pdf_link": "https://arxiv.org/pdf/2210.15191v1.pdf"
    },
    {
        "title": "Learning Joint Representation of Human Motion and Language",
        "authors": [
            "Jihoon Kim",
            "Youngjae Yu",
            "Seungyoun Shin",
            "Taehyun Byun",
            "Sungjoon Choi"
        ],
        "published": "2022-10-27T05:32:20Z",
        "summary": "In this work, we present MoLang (a Motion-Language connecting model) for\nlearning joint representation of human motion and language, leveraging both\nunpaired and paired datasets of motion and language modalities. To this end, we\npropose a motion-language model with contrastive learning, empowering our model\nto learn better generalizable representations of the human motion domain.\nEmpirical results show that our model learns strong representations of human\nmotion data through navigating language modality. Our proposed method is able\nto perform both action recognition and motion retrieval tasks with a single\nmodel where it outperforms state-of-the-art approaches on a number of action\nrecognition benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2210.15187v1.pdf"
    },
    {
        "title": "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
        "authors": [
            "Paul Denny",
            "Viraj Kumar",
            "Nasser Giacaman"
        ],
        "published": "2022-10-27T03:48:24Z",
        "summary": "GitHub Copilot is an artificial intelligence model for automatically\ngenerating source code from natural language problem descriptions. Since June\n2022, Copilot has officially been available for free to all students as a\nplug-in to development environments like Visual Studio Code. Prior work\nexploring OpenAI Codex, the underlying model that powers Copilot, has shown it\nperforms well on typical CS1 problems thus raising concerns about the impact it\nwill have on how introductory programming courses are taught. However, little\nis known about the types of problems for which Copilot does not perform well,\nor about the natural language interactions that a student might have with\nCopilot when resolving errors. We explore these questions by evaluating the\nperformance of Copilot on a publicly available dataset of 166 programming\nproblems. We find that it successfully solves around half of these problems on\nits very first attempt, and that it solves 60\\% of the remaining problems using\nonly natural language changes to the problem description. We argue that this\ntype of prompt engineering, which we believe will become a standard interaction\nbetween human and Copilot when it initially fails, is a potentially useful\nlearning activity that promotes computational thinking skills, and is likely to\nchange the nature of code writing skill development.",
        "pdf_link": "https://arxiv.org/pdf/2210.15157v1.pdf"
    },
    {
        "title": "Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models",
        "authors": [
            "Chaofan Ma",
            "Yuhuan Yang",
            "Yanfeng Wang",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "published": "2022-10-27T02:57:26Z",
        "summary": "When trained at a sufficient scale, self-supervised learning has exhibited a\nnotable ability to solve a wide range of visual or language understanding\ntasks. In this paper, we investigate simple, yet effective approaches for\nadapting the pre-trained foundation models to the downstream task of interest,\nnamely, open-vocabulary semantic segmentation. To this end, we make the\nfollowing contributions: (i) we introduce Fusioner, with a lightweight,\ntransformer-based fusion module, that pairs the frozen visual representation\nwith language concept through a handful of image segmentation data. As a\nconsequence, the model gains the capability of zero-shot transfer to segment\nnovel categories; (ii) without loss of generality, we experiment on a broad\nrange of self-supervised models that have been pre-trained with different\nschemes, e.g. visual-only models (MoCo v3, DINO), language-only models (BERT),\nvisual-language model (CLIP), and show that, the proposed fusion approach is\neffective to any pair of visual and language models, even those pre-trained on\na corpus of uni-modal data; (iii) we conduct thorough ablation studies to\nanalyze the critical components in our proposed Fusioner, while evaluating on\nstandard benchmarks, e.g. PASCAL-5i and COCO-20i , it surpasses existing\nstate-of-the-art models by a large margin, despite only being trained on frozen\nvisual and language features; (iv) to measure the model's robustness on\nlearning visual-language correspondence, we further evaluate on synthetic\ndataset, named Mosaic-4, where images are constructed by mosaicking the samples\nfrom FSS-1000. Fusioner demonstrates superior performance over previous models.",
        "pdf_link": "https://arxiv.org/pdf/2210.15138v1.pdf"
    },
    {
        "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
        "authors": [
            "Dingkun Long",
            "Yanzhao Zhang",
            "Guangwei Xu",
            "Pengjun Xie"
        ],
        "published": "2022-10-27T02:43:48Z",
        "summary": "Pre-trained language model (PTM) has been shown to yield powerful text\nrepresentations for dense passage retrieval task. The Masked Language Modeling\n(MLM) is a major sub-task of the pre-training process. However, we found that\nthe conventional random masking strategy tend to select a large number of\ntokens that have limited effect on the passage retrieval task (e,g. stop-words\nand punctuation). By noticing the term importance weight can provide valuable\ninformation for passage retrieval, we hereby propose alternative retrieval\noriented masking (dubbed as ROM) strategy where more important tokens will have\na higher probability of being masked out, to capture this straightforward yet\nessential information to facilitate the language model pre-training process.\nNotably, the proposed new token masking method will not change the architecture\nand learning objective of original PTM. Our experiments verify that the\nproposed ROM enables term importance information to help language model\npre-training thus achieving better performance on multiple passage retrieval\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2210.15133v1.pdf"
    },
    {
        "title": "Masked Vision-Language Transformer in Fashion",
        "authors": [
            "Ge-Peng Ji",
            "Mingcheng Zhuge",
            "Dehong Gao",
            "Deng-Ping Fan",
            "Christos Sakaridis",
            "Luc Van Gool"
        ],
        "published": "2022-10-27T01:44:08Z",
        "summary": "We present a masked vision-language transformer (MVLT) for fashion-specific\nmulti-modal representation. Technically, we simply utilize vision transformer\narchitecture for replacing the BERT in the pre-training model, making MVLT the\nfirst end-to-end framework for the fashion domain. Besides, we designed masked\nimage reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is\nan extensible and convenient architecture that admits raw multi-modal inputs\nwithout extra pre-processing models (e.g., ResNet), implicitly modeling the\nvision-language alignments. More importantly, MVLT can easily generalize to\nvarious matching and generative tasks. Experimental results show obvious\nimprovements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks\nover the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at\nhttps://github.com/GewelsJI/MVLT.",
        "pdf_link": "https://arxiv.org/pdf/2210.15110v1.pdf"
    },
    {
        "title": "TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and Punctuation model evaluation and selection",
        "authors": [
            "Piyush Behre",
            "Sharman Tan",
            "Amy Shah",
            "Harini Kesavamoorthy",
            "Shuangyu Chang",
            "Fei Zuo",
            "Chris Basoglu",
            "Sayan Pathak"
        ],
        "published": "2022-10-27T01:11:32Z",
        "summary": "Punctuation and Segmentation are key to readability in Automatic Speech\nRecognition (ASR), often evaluated using F1 scores that require high-quality\nhuman transcripts and do not reflect readability well. Human evaluation is\nexpensive, time-consuming, and suffers from large inter-observer variability,\nespecially in conversational speech devoid of strict grammatical structures.\nLarge pre-trained models capture a notion of grammatical structure. We present\nTRScore, a novel readability measure using the GPT model to evaluate different\nsegmentation and punctuation systems. We validate our approach with human\nexperts. Additionally, our approach enables quantitative assessment of text\npost-processing techniques such as capitalization, inverse text normalization\n(ITN), and disfluency on overall readability, which traditional word error rate\n(WER) and slot error rate (SER) metrics fail to capture. TRScore is strongly\ncorrelated to traditional F1 and human readability scores, with Pearson's\ncorrelation coefficients of 0.67 and 0.98, respectively. It also eliminates the\nneed for human transcriptions for model selection.",
        "pdf_link": "https://arxiv.org/pdf/2210.15104v1.pdf"
    },
    {
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
        "authors": [
            "Xiang Lisa Li",
            "Ari Holtzman",
            "Daniel Fried",
            "Percy Liang",
            "Jason Eisner",
            "Tatsunori Hashimoto",
            "Luke Zettlemoyer",
            "Mike Lewis"
        ],
        "published": "2022-10-27T00:58:21Z",
        "summary": "Given a language model (LM), maximum probability is a poor decoding objective\nfor open-ended generation, because it produces short and repetitive text. On\nthe other hand, sampling can often produce incoherent text that drifts from the\noriginal topics. We propose contrastive decoding (CD), a reliable decoding\napproach that optimizes a contrastive objective subject to a plausibility\nconstraint. The contrastive objective returns the difference between the\nlikelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM\n(called the amateur, e.g. OPT-125M), and the constraint ensures that the\noutputs are plausible. CD is inspired by the fact that the failures of larger\nLMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and\nthat this difference signals which texts should be preferred. CD requires zero\nadditional training, and produces higher quality text than decoding from the\nlarger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and\nsignificantly outperforms four strong decoding algorithms (e.g., nucleus,\ntop-k) in automatic and human evaluations across wikipedia, news and story\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2210.15097v2.pdf"
    },
    {
        "title": "Privately Fine-Tuning Large Language Models with Differential Privacy",
        "authors": [
            "Rouzbeh Behnia",
            "Mohamamdreza Ebrahimi",
            "Jason Pacheco",
            "Balaji Padmanabhan"
        ],
        "published": "2022-10-26T21:18:31Z",
        "summary": "Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.",
        "pdf_link": "https://arxiv.org/pdf/2210.15042v3.pdf"
    },
    {
        "title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
        "authors": [
            "Laura Ruis",
            "Akbir Khan",
            "Stella Biderman",
            "Sara Hooker",
            "Tim Rockt\u00e4schel",
            "Edward Grefenstette"
        ],
        "published": "2022-10-26T19:04:23Z",
        "summary": "Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context -- incorporating its pragmatics. Humans interpret language\nusing beliefs and prior knowledge about the world. For example, we intuitively\nunderstand the response \"I wore gloves\" to the question \"Did you leave\nfingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to\nmake this type of inference, known as an implicature, we design a simple task\nand evaluate four categories of widely used state-of-the-art models. We find\nthat, despite only evaluating on utterances that require a binary inference\n(yes or no), models in three of these categories perform close to random.\nHowever, LLMs instruction-tuned at the example-level perform significantly\nbetter. These results suggest that certain fine-tuning strategies are far\nbetter at inducing pragmatic understanding in models. We present our findings\nas the starting point for further research into evaluating how LLMs interpret\nlanguage in context and to drive the development of more pragmatic and useful\nmodels of human discourse.",
        "pdf_link": "https://arxiv.org/pdf/2210.14986v2.pdf"
    },
    {
        "title": "Causality Detection using Multiple Annotation Decisions",
        "authors": [
            "Quynh Anh Nguyen",
            "Arka Mitra"
        ],
        "published": "2022-10-26T16:50:10Z",
        "summary": "The paper describes the work that has been submitted to the 5th workshop on\nChallenges and Applications of Automated Extraction of socio-political events\nfrom text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3\nthat aims to detect causality in protest news corpus. The authors used\ndifferent large language models with customized cross-entropy loss functions\nthat exploit annotation information. The experiments showed that\nbert-based-uncased with refined cross-entropy outperformed the others,\nachieving a F1 score of 0.8501 on the Causal News Corpus dataset.",
        "pdf_link": "https://arxiv.org/pdf/2210.14852v2.pdf"
    },
    {
        "title": "Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models",
        "authors": [
            "Mozes van de Kar",
            "Mengzhou Xia",
            "Danqi Chen",
            "Mikel Artetxe"
        ],
        "published": "2022-10-26T15:52:30Z",
        "summary": "Masked language models like BERT can perform text classification in a\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\nthis approach is highly sensitive to the template used to prompt the model, yet\npractitioners are blind when designing them in strict zero-shot settings. In\nthis paper, we propose an alternative mining-based approach for zero-shot\nlearning. Instead of prompting language models, we use regular expressions to\nmine labeled examples from unlabeled corpora, which can optionally be filtered\nthrough prompting, and used to finetune a pretrained model. Our method is more\nflexible and interpretable than prompting, and outperforms it on a wide range\nof tasks when using comparable templates. Our results suggest that the success\nof prompting can partly be explained by the model being exposed to similar\nexamples during pretraining, which can be directly retrieved through regular\nexpressions.",
        "pdf_link": "https://arxiv.org/pdf/2210.14803v1.pdf"
    },
    {
        "title": "Incorporating Pre-training Paradigm for Antibody Sequence-Structure Co-design",
        "authors": [
            "Kaiyuan Gao",
            "Lijun Wu",
            "Jinhua Zhu",
            "Tianbo Peng",
            "Yingce Xia",
            "Liang He",
            "Shufang Xie",
            "Tao Qin",
            "Haiguang Liu",
            "Kun He",
            "Tie-Yan Liu"
        ],
        "published": "2022-10-26T15:31:36Z",
        "summary": "Antibodies are versatile proteins that can bind to pathogens and provide\neffective protection for human body. Recently, deep learning-based\ncomputational antibody design has attracted popular attention since it\nautomatically mines the antibody patterns from data that could be complementary\nto human experiences. However, the computational methods heavily rely on\nhigh-quality antibody structure data, which is quite limited. Besides, the\ncomplementarity-determining region (CDR), which is the key component of an\nantibody that determines the specificity and binding affinity, is highly\nvariable and hard to predict. Therefore, the data limitation issue further\nraises the difficulty of CDR generation for antibodies. Fortunately, there\nexists a large amount of sequence data of antibodies that can help model the\nCDR and alleviate the reliance on structure data. By witnessing the success of\npre-training models for protein modeling, in this paper, we develop the\nantibody pre-training language model and incorporate it into the\n(antigen-specific) antibody design model in a systemic way. Specifically, we\nfirst pre-train an antibody language model based on the sequence data, then\npropose a one-shot way for sequence and structure generation of CDR to avoid\nthe heavy cost and error propagation from an autoregressive manner, and finally\nleverage the pre-trained antibody model for the antigen-specific antibody\ngeneration model with some carefully designed modules. Through various\nexperiments, we show that our method achieves superior performances over\nprevious baselines on different tasks, such as sequence and structure\ngeneration and antigen-binding CDR-H3 design.",
        "pdf_link": "https://arxiv.org/pdf/2211.08406v2.pdf"
    },
    {
        "title": "A Case for Business Process-Specific Foundation Models",
        "authors": [
            "Yara Rizk",
            "Praveen Venkateswaran",
            "Vatche Isahagian",
            "Vinod Muthusamy"
        ],
        "published": "2022-10-26T14:17:47Z",
        "summary": "The inception of large language models has helped advance state-of-the-art\nperformance on numerous natural language tasks. This has also opened the door\nfor the development of foundation models for other domains and data modalities\nsuch as images, code, and music. In this paper, we argue that business process\ndata representations have unique characteristics that warrant the development\nof a new class of foundation models to handle tasks like process mining,\noptimization, and decision making. These models should also tackle the unique\nchallenges of applying AI to business processes which include data scarcity,\nmulti-modal representations, domain specific terminology, and privacy concerns.",
        "pdf_link": "https://arxiv.org/pdf/2210.14739v2.pdf"
    },
    {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "authors": [
            "Jianan Zhao",
            "Meng Qu",
            "Chaozhuo Li",
            "Hao Yan",
            "Qian Liu",
            "Rui Li",
            "Xing Xie",
            "Jian Tang"
        ],
        "published": "2022-10-26T13:40:57Z",
        "summary": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by training large language models and GNNs together. In this paper, we\npropose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows training the two modules separately while simultaneously\nallowing the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2210.14709v2.pdf"
    },
    {
        "title": "Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?",
        "authors": [
            "Jean-Baptiste D\u00f6derlein",
            "Mathieu Acher",
            "Djamel Eddine Khelladi",
            "Benoit Combemale"
        ],
        "published": "2022-10-26T13:28:14Z",
        "summary": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently attracted attention in code\nassistants, with programs automatically written in a given programming language\nfrom a programming task description in natural language. They have the\npotential to save time and effort when writing code. However, these systems are\ncurrently poorly understood, preventing them from being used optimally. In this\npaper, we investigate the various input parameters of two language models, and\nconduct a study to understand if variations of these input parameters (e.g.\nprogramming task description and the surrounding context, creativity of the\nlanguage model, number of generated solutions) can have a significant impact on\nthe quality of the generated programs. We design specific operators for varying\ninput parameters and apply them over two code assistants (Copilot and Codex)\nand two benchmarks representing algorithmic problems (HumanEval and LeetCode).\nOur results showed that varying the input parameters can significantly improve\nthe performance of language models. However, there is a tight dependency when\nvarying the temperature, the prompt and the number of generated solutions,\nmaking potentially hard for developers to properly control the parameters to\nobtain an optimal result. This work opens opportunities to propose (automated)\nstrategies for improving performance.",
        "pdf_link": "https://arxiv.org/pdf/2210.14699v2.pdf"
    },
    {
        "title": "TSUP Speaker Diarization System for Conversational Short-phrase Speaker Diarization Challenge",
        "authors": [
            "Bowen Pang",
            "Huan Zhao",
            "Gaosheng Zhang",
            "Xiaoyue Yang",
            "Yang Sun",
            "Li Zhang",
            "Qing Wang",
            "Lei Xie"
        ],
        "published": "2022-10-26T12:01:24Z",
        "summary": "This paper describes the TSUP team's submission to the ISCSLP 2022\nconversational short-phrase speaker diarization (CSSD) challenge which\nparticularly focuses on short-phrase conversations with a new evaluation metric\ncalled conversational diarization error rate (CDER). In this challenge, we\nexplore three kinds of typical speaker diarization systems, which are spectral\nclustering(SC) based diarization, target-speaker voice activity\ndetection(TS-VAD) and end-to-end neural diarization(EEND) respectively. Our\nmajor findings are summarized as follows. First, the SC approach is more\nfavored over the other two approaches under the new CDER metric. Second, tuning\non hyperparameters is essential to CDER for all three types of speaker\ndiarization systems. Specifically, CDER becomes smaller when the length of\nsub-segments setting longer. Finally, multi-system fusion through DOVER-LAP\nwill worsen the CDER metric on the challenge data. Our submitted SC system\neventually ranks the third place in the challenge.",
        "pdf_link": "https://arxiv.org/pdf/2210.14653v1.pdf"
    },
    {
        "title": "A Robust Bias Mitigation Procedure Based on the Stereotype Content Model",
        "authors": [
            "Eddie L. Ungless",
            "Amy Rafferty",
            "Hrichika Nag",
            "Bj\u00f6rn Ross"
        ],
        "published": "2022-10-26T08:13:58Z",
        "summary": "The Stereotype Content model (SCM) states that we tend to perceive minority\ngroups as cold, incompetent or both. In this paper we adapt existing work to\ndemonstrate that the Stereotype Content model holds for contextualised word\nembeddings, then use these results to evaluate a fine-tuning process designed\nto drive a language model away from stereotyped portrayals of minority groups.\nWe find the SCM terms are better able to capture bias than demographic agnostic\nterms related to pleasantness. Further, we were able to reduce the presence of\nstereotypes in the model through a simple fine-tuning procedure that required\nminimal human and computer resources, without harming downstream performance.\nWe present this work as a prototype of a debiasing procedure that aims to\nremove the need for a priori knowledge of the specifics of bias in the model.",
        "pdf_link": "https://arxiv.org/pdf/2210.14552v1.pdf"
    },
    {
        "title": "Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning",
        "authors": [
            "Yifan Chen",
            "Devamanyu Hazarika",
            "Mahdi Namazifar",
            "Yang Liu",
            "Di Jin",
            "Dilek Hakkani-Tur"
        ],
        "published": "2022-10-26T04:39:42Z",
        "summary": "Prefix-tuning, or more generally continuous prompt tuning, has become an\nessential paradigm of parameter-efficient transfer learning. Using a large\npre-trained language model (PLM), prefix-tuning can obtain strong performance\nby training only a small portion of parameters. In this paper, we propose to\nunderstand and further develop prefix-tuning through the kernel lens.\nSpecifically, we make an analogy between \\textit{prefixes} and \\textit{inducing\nvariables} in kernel methods and hypothesize that \\textit{prefixes} serving as\n\\textit{inducing variables} would improve their overall mechanism. From the\nkernel estimator perspective, we suggest a new variant of prefix-tuning --\n\\textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning\nwhile leveraging the residual form found in adapter-tuning. This mitigates the\ninitialization issue in prefix-tuning. Through comprehensive empirical\nexperiments on natural language understanding and generation tasks, we\ndemonstrate that inducer-tuning can close the performance gap between\nprefix-tuning and fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2210.14469v1.pdf"
    },
    {
        "title": "Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts",
        "authors": [
            "Bohua Peng",
            "Shihao Liang",
            "Mobarakol Islam"
        ],
        "published": "2022-10-26T04:31:07Z",
        "summary": "Inductive knowledge graph completion requires models to comprehend the\nunderlying semantics and logic patterns of relations. With the advance of\npretrained language models, recent research have designed transformers for link\nprediction tasks. However, empirical studies show that linearizing triples\naffects the learning of relational patterns, such as inversion and symmetry. In\nthis paper, we propose Bi-Link, a contrastive learning framework with\nprobabilistic syntax prompts for link predictions. Using grammatical knowledge\nof BERT, we efficiently search for relational prompts according to learnt\nsyntactical patterns that generalize to large knowledge graphs. To better\nexpress symmetric relations, we design a symmetric link prediction model,\nestablishing bidirectional linking between forward prediction and backward\nprediction. This bidirectional linking accommodates flexible self-ensemble\nstrategies at test time. In our experiments, Bi-Link outperforms recent\nbaselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).\nFurthermore, we construct Zeshel-Ind as an in-domain inductive entity linking\nthe environment to evaluate Bi-Link. The experimental results demonstrate that\nour method yields robust representations which can generalize under domain\nshift.",
        "pdf_link": "https://arxiv.org/pdf/2210.14463v1.pdf"
    },
    {
        "title": "$N$-gram Is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model",
        "authors": [
            "Huayang Li",
            "Deng Cai",
            "Jin Xu",
            "Taro Watanabe"
        ],
        "published": "2022-10-26T02:42:53Z",
        "summary": "$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.",
        "pdf_link": "https://arxiv.org/pdf/2210.14431v3.pdf"
    },
    {
        "title": "Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis",
        "authors": [
            "Sudhandar Balakrishnan",
            "Yihao Fang",
            "Xioadan Zhu"
        ],
        "published": "2022-10-26T01:13:41Z",
        "summary": "The invention of transformer-based models such as BERT, GPT, and RoBERTa has\nenabled researchers and financial companies to finetune these powerful models\nand use them in different downstream tasks to achieve state-of-the-art\nperformance. Recently, a lightweight alternative (approximately 0.1% - 3% of\nthe original model parameters) to fine-tuning, known as prefix tuning has been\nintroduced. This method freezes the model parameters and only updates the\nprefix to achieve performance comparable to full fine-tuning. Prefix tuning\nenables researchers and financial practitioners to achieve similar results with\nmuch fewer parameters. In this paper, we explore the robustness of prefix\ntuning when facing noisy data. Our experiments demonstrate that fine-tuning is\nmore robust to noise than prefix tuning -- the latter method faces a\nsignificant decrease in performance on most corrupted data sets with increasing\nnoise levels. Furthermore, prefix tuning has high variances in the F1 scores\ncompared to fine-tuning in many corruption methods. We strongly advocate that\ncaution should be carefully taken when applying the state-of-the-art prefix\ntuning method to noisy data.",
        "pdf_link": "https://arxiv.org/pdf/2211.05584v1.pdf"
    },
    {
        "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
        "authors": [
            "Victor Zhong",
            "Weijia Shi",
            "Wen-tau Yih",
            "Luke Zettlemoyer"
        ],
        "published": "2022-10-25T21:39:36Z",
        "summary": "We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2210.14353v2.pdf"
    },
    {
        "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
        "authors": [
            "Xiang Yue",
            "Huseyin A. Inan",
            "Xuechen Li",
            "Girish Kumar",
            "Julia McAnallen",
            "Hoda Shajari",
            "Huan Sun",
            "David Levitan",
            "Robert Sim"
        ],
        "published": "2022-10-25T21:21:17Z",
        "summary": "Privacy concerns have attracted increasing attention in data-driven products\ndue to the tendency of machine learning models to memorize sensitive training\ndata. Generating synthetic versions of such data with a formal privacy\nguarantee, such as differential privacy (DP), provides a promising path to\nmitigating these privacy concerns, but previous approaches in this direction\nhave typically failed to produce synthetic data of high quality. In this work,\nwe show that a simple and practical recipe in the text domain is effective:\nsimply fine-tuning a pretrained generative language model with DP enables the\nmodel to generate useful synthetic text with strong privacy protection. Through\nextensive empirical analyses on both benchmark and private customer data, we\ndemonstrate that our method produces synthetic text that is competitive in\nterms of utility with its non-private counterpart, meanwhile providing strong\nprotection against potential privacy leakages.",
        "pdf_link": "https://arxiv.org/pdf/2210.14348v3.pdf"
    },
    {
        "title": "A single-cell gene expression language model",
        "authors": [
            "William Connell",
            "Umair Khan",
            "Michael J. Keiser"
        ],
        "published": "2022-10-25T20:52:19Z",
        "summary": "Gene regulation is a dynamic process that connects genotype and phenotype.\nGiven the difficulty of physically mapping mammalian gene circuitry, we require\nnew computational methods to learn regulatory rules. Natural language is a\nvaluable analogy to the communication of regulatory control. Machine learning\nsystems model natural language by explicitly learning context dependencies\nbetween words. We propose a similar system applied to single-cell RNA\nexpression profiles to learn context dependencies between genes. Our model,\nExceiver, is trained across a diversity of cell types using a self-supervised\ntask formulated for discrete count data, accounting for feature sparsity. We\nfound agreement between the similarity profiles of latent sample\nrepresentations and learned gene embeddings with respect to biological\nannotations. We evaluated Exceiver on a new dataset and a downstream prediction\ntask and found that pretraining supports transfer learning. Our work provides a\nframework to model gene regulation on a single-cell level and transfer\nknowledge to downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.14330v1.pdf"
    },
    {
        "title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models",
        "authors": [
            "Aaron Mueller",
            "Yu Xia",
            "Tal Linzen"
        ],
        "published": "2022-10-25T20:43:36Z",
        "summary": "Structural probing work has found evidence for latent syntactic information\nin pre-trained language models. However, much of this analysis has focused on\nmonolingual models, and analyses of multilingual models have employed\ncorrelational methods that are confounded by the choice of probing tasks. In\nthis study, we causally probe multilingual language models (XGLM and\nmultilingual BERT) as well as monolingual BERT-based models across various\nlanguages; we do this by performing counterfactual perturbations on neuron\nactivations and observing the effect on models' subject-verb agreement\nprobabilities. We observe where in the model and to what extent syntactic\nagreement is encoded in each language. We find significant neuron overlap\nacross languages in autoregressive multilingual language models, but not masked\nlanguage models. We also find two distinct layer-wise effect patterns and two\ndistinct sets of neurons used for syntactic agreement, depending on whether the\nsubject and verb are separated by other tokens. Finally, we find that\nbehavioral analyses of language models are likely underestimating how sensitive\nmasked language models are to syntactic information.",
        "pdf_link": "https://arxiv.org/pdf/2210.14328v1.pdf"
    },
    {
        "title": "Learning Better Intent Representations for Financial Open Intent Classification",
        "authors": [
            "Xianzhi Li",
            "Will Aitken",
            "Xiaodan Zhu",
            "Stephen W. Thomas"
        ],
        "published": "2022-10-25T20:01:13Z",
        "summary": "With the recent surge of NLP technologies in the financial domain, banks and\nother financial entities have adopted virtual agents (VA) to assist customers.\nA challenging problem for VAs in this domain is determining a user's reason or\nintent for contacting the VA, especially when the intent was unseen or open\nduring the VA's training. One method for handling open intents is adaptive\ndecision boundary (ADB) post-processing, which learns tight decision boundaries\nfrom intent representations to separate known and open intents. We propose\nincorporating two methods for supervised pre-training of intent\nrepresentations: prefix-tuning and fine-tuning just the last layer of a large\nlanguage model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher\nthan the prior state-of-the-art ADB method for open intent classification on\nthe banking77 benchmark amongst others. Notably, we only supplement the\noriginal ADB model with 0.1% additional trainable parameters. Ablation studies\nalso determine that our method yields better results than full fine-tuning the\nentire model. We hypothesize that our findings could stimulate a new optimal\nmethod of downstream tuning that combines parameter efficient tuning modules\nwith fine-tuning a subset of the base model's layers.",
        "pdf_link": "https://arxiv.org/pdf/2210.14304v1.pdf"
    },
    {
        "title": "Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios",
        "authors": [
            "Zhuohao Chen",
            "Nikolaos Flemotomos",
            "Zac E. Imel",
            "David C. Atkins",
            "Shrikanth Narayanan"
        ],
        "published": "2022-10-25T18:15:25Z",
        "summary": "In psychotherapy interactions, the quality of a session is assessed by\ncodifying the communicative behaviors of participants during the conversation\nthrough manual observation and annotation. Developing computational approaches\nfor automated behavioral coding can reduce the burden on human coders and\nfacilitate the objective evaluation of the intervention. In the real world,\nhowever, implementing such algorithms is associated with data sparsity\nchallenges since privacy concerns lead to limited available in-domain data. In\nthis paper, we leverage a publicly available conversation-based dataset and\ntransfer knowledge to the low-resource behavioral coding task by performing an\nintermediate language model training via meta-learning. We introduce a task\naugmentation method to produce a large number of \"analogy tasks\" - tasks\nsimilar to the target one - and demonstrate that the proposed framework\npredicts target behaviors more accurately than all the other baseline models.",
        "pdf_link": "https://arxiv.org/pdf/2210.14254v1.pdf"
    },
    {
        "title": "Contrastive Search Is What You Need For Neural Text Generation",
        "authors": [
            "Yixuan Su",
            "Nigel Collier"
        ],
        "published": "2022-10-25T16:40:48Z",
        "summary": "Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous studies. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\n  In this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of the 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations. Our code and other related resources are publicly\navailable at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.",
        "pdf_link": "https://arxiv.org/pdf/2210.14140v3.pdf"
    },
    {
        "title": "IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models",
        "authors": [
            "Chenguang Wang",
            "Xiao Liu",
            "Dawn Song"
        ],
        "published": "2022-10-25T16:25:00Z",
        "summary": "We introduce a new open information extraction (OIE) benchmark for\npre-trained language models (LM). Recent studies have demonstrated that\npre-trained LMs, such as BERT and GPT, may store linguistic and relational\nknowledge. In particular, LMs are able to answer ``fill-in-the-blank''\nquestions when given a pre-defined relation category. Instead of focusing on\npre-defined relations, we create an OIE benchmark aiming to fully examine the\nopen relational information present in the pre-trained LMs. We accomplish this\nby turning pre-trained LMs into zero-shot OIE systems. Surprisingly,\npre-trained LMs are able to obtain competitive performance on both standard OIE\ndatasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets\n(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For\ninstance, the zero-shot pre-trained LMs outperform the F1 score of the\nstate-of-the-art supervised OIE methods on our factual OIE datasets without\nneeding to use any training sets. Our code and datasets are available at\nhttps://github.com/cgraywang/IELM",
        "pdf_link": "https://arxiv.org/pdf/2210.14128v1.pdf"
    },
    {
        "title": "MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction",
        "authors": [
            "Pengtao Zhang",
            "Junlin Zhang"
        ],
        "published": "2022-10-25T12:08:14Z",
        "summary": "New findings in natural language processing (NLP) demonstrate that the strong\nmemorization capability contributes a lot to the success of Large Language\nModels (LLM). This inspires us to explicitly bring an independent memory\nmechanism into CTR ranking model to learn and memorize cross features'\nrepresentations. In this paper, we propose multi-Hash Codebook NETwork (HCNet)\nas the memory mechanism for efficiently learning and memorizing representations\nof cross features in CTR tasks. HCNet uses a multi-hash codebook as the main\nmemory place and the whole memory procedure consists of three phases:\nmulti-hash addressing, memory restoring, and feature shrinking. We also propose\na new CTR model named MemoNet which combines HCNet with a DNN backbone.\nExtensive experimental results on three public datasets and online test show\nthat MemoNet reaches superior performance over state-of-the-art approaches.\nBesides, MemoNet shows scaling law of large language model in NLP, which means\nwe can enlarge the size of the codebook in HCNet to sustainably obtain\nperformance gains. Our work demonstrates the importance and feasibility of\nlearning and memorizing representations of cross features, which sheds light on\na new promising research direction.",
        "pdf_link": "https://arxiv.org/pdf/2211.01334v3.pdf"
    },
    {
        "title": "Dual Mechanism Priming Effects in Hindi Word Order",
        "authors": [
            "Sidharth Ranjan",
            "Marten van Schijndel",
            "Sumeet Agarwal",
            "Rajakrishnan Rajkumar"
        ],
        "published": "2022-10-25T11:49:22Z",
        "summary": "Word order choices during sentence production can be primed by preceding\nsentences. In this work, we test the DUAL MECHANISM hypothesis that priming is\ndriven by multiple different sources. Using a Hindi corpus of text productions,\nwe model lexical priming with an n-gram cache model and we capture more\nabstract syntactic priming with an adaptive neural language model. We permute\nthe preverbal constituents of corpus sentences, and then use a logistic\nregression model to predict which sentences actually occurred in the corpus\nagainst artificially generated meaning-equivalent variants. Our results\nindicate that lexical priming and lexically-independent syntactic priming\naffect complementary sets of verb classes. By showing that different priming\ninfluences are separable from one another, our results support the hypothesis\nthat multiple different cognitive mechanisms underlie priming.",
        "pdf_link": "https://arxiv.org/pdf/2210.13938v1.pdf"
    },
    {
        "title": "Cloning Ideology and Style using Deep Learning",
        "authors": [
            "Dr. Omer Beg",
            "Muhammad Nasir Zafar",
            "Waleed Anjum"
        ],
        "published": "2022-10-25T11:37:19Z",
        "summary": "Text generation tasks have gotten the attention of researchers in the last\nfew years because of their applications on a large scale.In the past, many\nresearchers focused on task-based text generations.Our research focuses on text\ngeneration based on the ideology and style of a specific author, and text\ngeneration on a topic that was not written by the same author in the past.Our\ntrained model requires an input prompt containing initial few words of text to\nproduce a few paragraphs of text based on the ideology and style of the author\non which the model is trained.Our methodology to accomplish this task is based\non Bi-LSTM.The Bi-LSTM model is used to make predictions at the character\nlevel, during the training corpus of a specific author is used along with the\nground truth corpus.A pre-trained model is used to identify the sentences of\nground truth having contradiction with the author's corpus to make our language\nmodel inclined.During training, we have achieved a perplexity score of 2.23 at\nthe character level. The experiments show a perplexity score of around 3 over\nthe test dataset.",
        "pdf_link": "https://arxiv.org/pdf/2211.07712v1.pdf"
    },
    {
        "title": "Differentially Private Language Models for Secure Data Sharing",
        "authors": [
            "Justus Mattern",
            "Zhijing Jin",
            "Benjamin Weggenmann",
            "Bernhard Schoelkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2022-10-25T11:12:56Z",
        "summary": "To protect the privacy of individuals whose data is being shared, it is of\nhigh importance to develop methods allowing researchers and companies to\nrelease textual data while providing formal privacy guarantees to its\noriginators. In the field of NLP, substantial efforts have been directed at\nbuilding mechanisms following the framework of local differential privacy,\nthereby anonymizing individual text samples before releasing them. In practice,\nthese approaches are often dissatisfying in terms of the quality of their\noutput language due to the strong noise required for local differential\nprivacy. In this paper, we approach the problem at hand using global\ndifferential privacy, particularly by training a generative language model in a\ndifferentially private manner and consequently sampling data from it. Using\nnatural language prompts and a new prompt-mismatch loss, we are able to create\nhighly accurate and fluent textual datasets taking on specific desired\nattributes such as sentiment or topic and resembling statistical properties of\nthe training data. We perform thorough experiments indicating that our\nsynthetic datasets do not leak information from our original data and are of\nhigh language quality and highly suitable for training models for further\nanalysis on real-world data. Notably, we also demonstrate that training\nclassifiers on private synthetic data outperforms directly training classifiers\non real data with DP-SGD.",
        "pdf_link": "https://arxiv.org/pdf/2210.13918v2.pdf"
    },
    {
        "title": "Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition",
        "authors": [
            "Xulong Zhang",
            "Jianzong Wang",
            "Ning Cheng",
            "Mengyuan Zhao",
            "Zhiyong Zhang",
            "Jing Xiao"
        ],
        "published": "2022-10-25T08:12:59Z",
        "summary": "The recent emergence of joint CTC-Attention model shows significant\nimprovement in automatic speech recognition (ASR). The improvement largely lies\nin the modeling of linguistic information by decoder. The decoder\njoint-optimized with an acoustic encoder renders the language model from\nground-truth sequences in an auto-regressive manner during training. However,\nthe training corpus of the decoder is limited to the speech transcriptions,\nwhich is far less than the corpus needed to train an acceptable language model.\nThis leads to poor robustness of decoder. To alleviate this problem, we propose\nlinguistic-enhanced transformer, which introduces refined CTC information to\ndecoder during training process, so that the decoder can be more robust. Our\nexperiments on AISHELL-1 speech corpus show that the character error rate (CER)\nis relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR\nmodel, decoder is more sensitive to linguistic information than acoustic\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2210.14725v1.pdf"
    },
    {
        "title": "Parameter-Efficient Legal Domain Adaptation",
        "authors": [
            "Jonathan Li",
            "Rohan Bhambhoria",
            "Xiaodan Zhu"
        ],
        "published": "2022-10-25T02:14:15Z",
        "summary": "Seeking legal advice is often expensive. Recent advancements in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models in the legal\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2210.13712v2.pdf"
    },
    {
        "title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
        "authors": [
            "Hung-Ting Chen",
            "Michael J. Q. Zhang",
            "Eunsol Choi"
        ],
        "published": "2022-10-25T01:46:00Z",
        "summary": "Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.",
        "pdf_link": "https://arxiv.org/pdf/2210.13701v1.pdf"
    },
    {
        "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
        "authors": [
            "Peng Shi",
            "Rui Zhang",
            "He Bai",
            "Jimmy Lin"
        ],
        "published": "2022-10-25T01:33:49Z",
        "summary": "In-context learning using large language models has recently shown surprising\nresults for semantic parsing tasks such as Text-to-SQL translation. Prompting\nGPT-3 or Codex using several examples of question-SQL pairs can produce\nexcellent results, comparable to state-of-the-art finetuning-based models.\nHowever, existing work primarily focuses on English datasets, and it is unknown\nwhether large language models can serve as competitive semantic parsers for\nother languages. To bridge this gap, our work focuses on cross-lingual\nText-to-SQL semantic parsing for translating non-English utterances into SQL\nqueries based on an English schema. We consider a zero-shot transfer learning\nsetting with the assumption that we do not have any labeled examples in the\ntarget language (but have annotated examples in English). This work introduces\nthe XRICL framework, which learns to retrieve relevant English exemplars for a\ngiven query to construct prompts. We also include global translation exemplars\nfor a target language to facilitate the translation process for large language\nmodels. To systematically evaluate our model, we construct two new benchmark\ndatasets, XSpider and XKaggle-dbqa, which include questions in Chinese,\nVietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively\nleverages large pre-trained language models to outperform existing baselines.\nData and code are publicly available at https://github.com/Impavidity/XRICL.",
        "pdf_link": "https://arxiv.org/pdf/2210.13693v1.pdf"
    },
    {
        "title": "Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
        "authors": [
            "Tuhin Chakrabarty",
            "Vishakh Padmakumar",
            "He He"
        ],
        "published": "2022-10-25T00:07:10Z",
        "summary": "Recent work in training large language models (LLMs) to follow natural\nlanguage instructions has opened up exciting opportunities for natural language\ninterface design. Building on the prior success of LLMs in the realm of\ncomputer-assisted creativity, we aim to study if LLMs can improve the quality\nof user-generated content through collaboration. We present CoPoet, a\ncollaborative poetry writing system. In contrast to auto-completing a user's\ntext, CoPoet is controlled by user instructions that specify the attributes of\nthe desired text, such as Write a sentence about `love' or Write a sentence\nending in `fly'. The core component of our system is a language model\nfine-tuned on a diverse collection of instructions for poetry writing. Our\nmodel is not only competitive with publicly available LLMs trained on\ninstructions (InstructGPT), but is also capable of satisfying unseen\ncompositional instructions. A study with 15 qualified crowdworkers shows that\nusers successfully write poems with CoPoet on diverse topics ranging from\nMonarchy to Climate change. Further, the collaboratively written poems are\npreferred by third-party evaluators over those written without the system.",
        "pdf_link": "https://arxiv.org/pdf/2210.13669v1.pdf"
    },
    {
        "title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge",
        "authors": [
            "Sahithya Ravi",
            "Aditya Chinchure",
            "Leonid Sigal",
            "Renjie Liao",
            "Vered Shwartz"
        ],
        "published": "2022-10-24T22:01:17Z",
        "summary": "There has been a growing interest in solving Visual Question Answering (VQA)\ntasks that require the model to reason beyond the content present in the image.\nIn this work, we focus on questions that require commonsense reasoning. In\ncontrast to previous methods which inject knowledge from static knowledge\nbases, we investigate the incorporation of contextualized knowledge using\nCommonsense Transformer (COMET), an existing knowledge model trained on\nhuman-curated knowledge bases. We propose a method to generate, select, and\nencode external commonsense knowledge alongside visual and textual cues in a\nnew pre-trained Vision-Language-Commonsense transformer model, VLC-BERT.\nThrough our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets,\nwe show that VLC-BERT is capable of outperforming existing models that utilize\nstatic knowledge bases. Furthermore, through a detailed analysis, we explain\nwhich questions benefit, and which don't, from contextualized commonsense\nknowledge from COMET.",
        "pdf_link": "https://arxiv.org/pdf/2210.13626v1.pdf"
    },
    {
        "title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text",
        "authors": [
            "Yifan Hou",
            "Wenxiang Jiao",
            "Meizhen Liu",
            "Carl Allen",
            "Zhaopeng Tu",
            "Mrinmaya Sachan"
        ],
        "published": "2022-10-24T21:33:42Z",
        "summary": "Large language models appear to learn facts from the large text corpora they\nare trained on. Such facts are encoded implicitly within their many parameters,\nmaking it difficult to verify or manipulate what knowledge has been learned.\nLanguage models have recently been extended to multilingual language models\n(MLLMs), enabling knowledge to be learned across hundreds of languages.\nMeanwhile, knowledge graphs contain facts in an explicit triple format, which\nrequire careful and costly curation and are only available in a few\nhigh-resource languages, restricting their research and application. To address\nthese issues, we propose to enhance MLLMs with knowledge from multilingual\nknowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks\nacross many languages, including low-resource ones. Specifically, we introduce\na lightweight adapter set to enhance MLLMs with cross-lingual entity alignment\nand facts from MLKGs for many languages. Experiments on common benchmarks show\nthat such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable\nor improved performance for knowledge graph completion and entity alignment\nrelative to baselines, especially for low-resource languages (for which\nknowledge graphs are unavailable); and (2) improved MLLM performance on\nlanguage understanding tasks that require multilingual factual knowledge; all\nwhile maintaining performance on other general language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.13617v2.pdf"
    },
    {
        "title": "Speeding Up Question Answering Task of Language Models via Inverted Index",
        "authors": [
            "Xiang Ji",
            "Yesim Sungu-Eryilmaz",
            "Elaheh Momeni",
            "Reza Rawassizadeh"
        ],
        "published": "2022-10-24T19:59:17Z",
        "summary": "Natural language processing applications, such as conversational agents and\ntheir question-answering capabilities, are widely used in the real world.\nDespite the wide popularity of large language models (LLMs), few real-world\nconversational agents take advantage of LLMs. Extensive resources consumed by\nLLMs disable developers from integrating them into end-user applications. In\nthis study, we leverage an inverted indexing mechanism combined with LLMs to\nimprove the efficiency of question-answering models for closed-domain\nquestions. Our experiments show that using the index improves the average\nresponse time by 97.44%. In addition, due to the reduced search scope, the\naverage BLEU score improved by 0.23 while using the inverted index.",
        "pdf_link": "https://arxiv.org/pdf/2210.13578v1.pdf"
    },
    {
        "title": "Characterizing Verbatim Short-Term Memory in Neural Language Models",
        "authors": [
            "Kristijan Armeni",
            "Christopher Honey",
            "Tal Linzen"
        ],
        "published": "2022-10-24T19:47:56Z",
        "summary": "When a language model is trained to predict natural language sequences, its\nprediction at each moment depends on a representation of prior context. What\nkind of information about the prior context can language models retrieve? We\ntested whether language models could retrieve the exact words that occurred\npreviously in a text. In our paradigm, language models (transformers and an\nLSTM) processed English text in which a list of nouns occurred twice. We\noperationalized retrieval as the reduction in surprisal from the first to the\nsecond list. We found that the transformers retrieved both the identity and\nordering of nouns from the first list. Further, the transformers' retrieval was\nmarkedly enhanced when they were trained on a larger corpus and with greater\nmodel depth. Lastly, their ability to index prior tokens was dependent on\nlearned attention patterns. In contrast, the LSTM exhibited less precise\nretrieval, which was limited to list-initial tokens and to short intervening\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\nimproved when the list was semantically coherent. We conclude that transformers\nimplemented something akin to a working memory system that could flexibly\nretrieve individual token representations across arbitrary delays; conversely,\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\ntokens, weighted toward the earliest items.",
        "pdf_link": "https://arxiv.org/pdf/2210.13569v2.pdf"
    },
    {
        "title": "Effective Pre-Training Objectives for Transformer-based Autoencoders",
        "authors": [
            "Luca Di Liello",
            "Matteo Gabburo",
            "Alessandro Moschitti"
        ],
        "published": "2022-10-24T18:39:44Z",
        "summary": "In this paper, we study trade-offs between efficiency, cost and accuracy when\npre-training Transformer encoders with different pre-training objectives. For\nthis purpose, we analyze features of common objectives and combine them to\ncreate new effective pre-training approaches. Specifically, we designed light\ntoken generators based on a straightforward statistical approach, which can\nreplace ELECTRA computationally heavy generators, thus highly reducing cost.\nOur experiments also show that (i) there are more efficient alternatives to\nBERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based\nmodels using lighter generators without a significant drop in performance.",
        "pdf_link": "https://arxiv.org/pdf/2210.13536v1.pdf"
    },
    {
        "title": "Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models",
        "authors": [
            "Hao Liu",
            "Xinyang Geng",
            "Lisa Lee",
            "Igor Mordatch",
            "Sergey Levine",
            "Sharan Narang",
            "Pieter Abbeel"
        ],
        "published": "2022-10-24T17:46:57Z",
        "summary": "Large language models (LLM) trained using the next-token-prediction\nobjective, such as GPT3 and PaLM, have revolutionized natural language\nprocessing in recent years by showing impressive zero-shot and few-shot\ncapabilities across a wide range of tasks. In this work, we propose a simple\ntechnique that significantly boosts the performance of LLMs without adding\ncomputational cost. Our key observation is that, by performing the next token\nprediction task with randomly selected past tokens masked out, we can improve\nthe quality of the learned representations for downstream language\nunderstanding tasks. We hypothesize that randomly masking past tokens prevents\nover-attending to recent tokens and encourages attention to tokens in the\ndistant past. We find that our method, Forgetful Causal Masking (FCM),\nsignificantly improves both few-shot and finetuning performance of PaLM. We\nfurther consider a simple extension, T-FCM, which introduces bidirectional\ncontext to causal language model without altering the sequence order, and\nfurther improves finetuning performance.",
        "pdf_link": "https://arxiv.org/pdf/2210.13432v2.pdf"
    },
    {
        "title": "Perfectly Secure Steganography Using Minimum Entropy Coupling",
        "authors": [
            "Christian Schroeder de Witt",
            "Samuel Sokota",
            "J. Zico Kolter",
            "Jakob Foerster",
            "Martin Strohmeier"
        ],
        "published": "2022-10-24T17:40:07Z",
        "summary": "Steganography is the practice of encoding secret information into innocuous\ncontent in such a manner that an adversarial third party would not realize that\nthere is hidden meaning. While this problem has classically been studied in\nsecurity literature, recent advances in generative models have led to a shared\ninterest among security and machine learning researchers in developing scalable\nsteganography techniques. In this work, we show that a steganography procedure\nis perfectly secure under Cachin (1998)'s information-theoretic model of\nsteganography if and only if it is induced by a coupling. Furthermore, we show\nthat, among perfectly secure procedures, a procedure maximizes information\nthroughput if and only if it is induced by a minimum entropy coupling. These\ninsights yield what are, to the best of our knowledge, the first steganography\nalgorithms to achieve perfect security guarantees for arbitrary covertext\ndistributions. To provide empirical validation, we compare a minimum entropy\ncoupling-based approach to three modern baselines -- arithmetic coding, Meteor,\nand adaptive dynamic grouping -- using GPT-2, WaveRNN, and Image Transformer as\ncommunication channels. We find that the minimum entropy coupling-based\napproach achieves superior encoding efficiency, despite its stronger security\nconstraints. In aggregate, these results suggest that it may be natural to view\ninformation-theoretic steganography through the lens of minimum entropy\ncoupling.",
        "pdf_link": "https://arxiv.org/pdf/2210.14889v4.pdf"
    },
    {
        "title": "Explaining Translationese: why are Neural Classifiers Better and what do they Learn?",
        "authors": [
            "Kwabena Amponsah-Kaakyire",
            "Daria Pylypenko",
            "Josef van Genabith",
            "Cristina Espa\u00f1a-Bonet"
        ],
        "published": "2022-10-24T16:43:28Z",
        "summary": "Recent work has shown that neural feature- and representation-learning, e.g.\nBERT, achieves superior performance over traditional manual feature engineering\nbased approaches, with e.g. SVMs, in translationese classification tasks.\nPrevious research did not show $(i)$ whether the difference is because of the\nfeatures, the classifiers or both, and $(ii)$ what the neural classifiers\nactually learn. To address $(i)$, we carefully design experiments that swap\nfeatures between BERT- and SVM-based classifiers. We show that an SVM fed with\nBERT representations performs at the level of the best BERT classifiers, while\nBERT learning and using handcrafted features performs at the level of an SVM\nusing handcrafted features. This shows that the performance differences are due\nto the features. To address $(ii)$ we use integrated gradients and find that\n$(a)$ there is indication that information captured by hand-crafted features is\nonly a subset of what BERT learns, and $(b)$ part of BERT's top performance\nresults are due to BERT learning topic differences and spurious correlations\nwith translationese.",
        "pdf_link": "https://arxiv.org/pdf/2210.13391v1.pdf"
    },
    {
        "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
        "authors": [
            "Kenneth Li",
            "Aspen K. Hopkins",
            "David Bau",
            "Fernanda Vi\u00e9gas",
            "Hanspeter Pfister",
            "Martin Wattenberg"
        ],
        "published": "2022-10-24T16:29:55Z",
        "summary": "Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.",
        "pdf_link": "https://arxiv.org/pdf/2210.13382v4.pdf"
    },
    {
        "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
        "authors": [
            "Maarten Sap",
            "Ronan LeBras",
            "Daniel Fried",
            "Yejin Choi"
        ],
        "published": "2022-10-24T14:58:58Z",
        "summary": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.",
        "pdf_link": "https://arxiv.org/pdf/2210.13312v2.pdf"
    },
    {
        "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation",
        "authors": [
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2022-10-24T14:46:47Z",
        "summary": "We study the text generation task under the approach of pre-trained language\nmodels (PLMs). Typically, an auto-regressive (AR) method is adopted for\ngenerating texts in a token-by-token manner. Despite many advantages of AR\ngeneration, it usually suffers from inefficient inference. Therefore,\nnon-autoregressive (NAR) models are proposed to generate all target tokens\nsimultaneously. However, NAR models usually generate texts of lower quality due\nto the absence of token dependency in the output text. In this paper, we\npropose ELMER: an efficient and effective PLM for NAR text generation to\nexplicitly model the token dependency during NAR generation. By leveraging the\nearly exit technique, ELMER enables the token generations at different layers,\naccording to their prediction confidence (a more confident token will exit at a\nlower layer). Besides, we propose a novel pre-training objective, Layer\nPermutation Language Modeling, to pre-train ELMER by permuting the exit layer\nfor each token in sequences. Experiments on three text generation tasks show\nthat ELMER significantly outperforms NAR models and further narrows the\nperformance gap with AR PLMs (\\eg ELMER (29.92) vs BART (30.61) ROUGE-L in\nXSUM) while achieving over 10 times inference speedup.",
        "pdf_link": "https://arxiv.org/pdf/2210.13304v2.pdf"
    },
    {
        "title": "The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",
        "authors": [
            "Leonie Weissweiler",
            "Valentin Hofmann",
            "Abdullatif K\u00f6ksal",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2022-10-24T13:01:24Z",
        "summary": "Construction Grammar (CxG) is a paradigm from cognitive linguistics\nemphasising the connection between syntax and semantics. Rather than rules that\noperate on lexical items, it posits constructions as the central building\nblocks of language, i.e., linguistic units of different granularity that\ncombine syntax and semantics. As a first step towards assessing the\ncompatibility of CxG with the syntactic and semantic knowledge demonstrated by\nstate-of-the-art pretrained language models (PLMs), we present an investigation\nof their capability to classify and understand one of the most commonly studied\nconstructions, the English comparative correlative (CC). We conduct experiments\nexamining the classification accuracy of a syntactic probe on the one hand and\nthe models' behaviour in a semantic application task on the other, with BERT,\nRoBERTa, and DeBERTa as the example PLMs. Our results show that all three\ninvestigated PLMs are able to recognise the structure of the CC but fail to use\nits meaning. While human-like performance of PLMs on many NLP tasks has been\nalleged, this indicates that PLMs still suffer from substantial shortcomings in\ncentral domains of linguistic knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2210.13181v1.pdf"
    },
    {
        "title": "Proficiency assessment of L2 spoken English using wav2vec 2.0",
        "authors": [
            "Stefano Bann\u00f2",
            "Marco Matassoni"
        ],
        "published": "2022-10-24T12:36:49Z",
        "summary": "The increasing demand for learning English as a second language has led to a\ngrowing interest in methods for automatically assessing spoken language\nproficiency. Most approaches use hand-crafted features, but their efficacy\nrelies on their particular underlying assumptions and they risk discarding\npotentially salient information about proficiency. Other approaches rely on\ntranscriptions produced by ASR systems which may not provide a faithful\nrendition of a learner's utterance in specific scenarios (e.g., non-native\nchildren's spontaneous speech). Furthermore, transcriptions do not yield any\ninformation about relevant aspects such as intonation, rhythm or prosody. In\nthis paper, we investigate the use of wav2vec 2.0 for assessing overall and\nindividual aspects of proficiency on two small datasets, one of which is\npublicly available. We find that this approach significantly outperforms the\nBERT-based baseline system trained on ASR and manual transcriptions used for\ncomparison.",
        "pdf_link": "https://arxiv.org/pdf/2210.13168v1.pdf"
    },
    {
        "title": "An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks",
        "authors": [
            "Changlong Yu",
            "Tianyi Xiao",
            "Lingpeng Kong",
            "Yangqiu Song",
            "Wilfred Ng"
        ],
        "published": "2022-10-24T07:47:32Z",
        "summary": "Though linguistic knowledge emerges during large-scale language model\npretraining, recent work attempt to explicitly incorporate human-defined\nlinguistic priors into task-specific fine-tuning. Infusing language models with\nsyntactic or semantic knowledge from parsers has shown improvements on many\nlanguage understanding tasks. To further investigate the effectiveness of\nstructural linguistic priors, we conduct empirical study of replacing parsed\ngraphs or trees with trivial ones (rarely carrying linguistic knowledge e.g.,\nbalanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs\nachieves competitive or even better performance in fully-supervised and\nfew-shot settings. It reveals that the gains might not be significantly\nattributed to explicit linguistic priors but rather to more feature\ninteractions brought by fusion layers. Hence we call for attention to using\ntrivial graphs as necessary baselines to design advanced knowledge fusion\nmethods in the future.",
        "pdf_link": "https://arxiv.org/pdf/2210.13002v1.pdf"
    },
    {
        "title": "Abductive Action Inference",
        "authors": [
            "Clement Tan",
            "Chai Kiat Yeo",
            "Cheston Tan",
            "Basura Fernando"
        ],
        "published": "2022-10-24T07:43:59Z",
        "summary": "Abductive reasoning aims to make the most likely inference for a given set of\nincomplete observations. In this paper, we introduce a novel research task\nknown as \"abductive action inference\" which addresses the question of which\nactions were executed by a human to reach a specific state shown in a single\nsnapshot. The research explores three key abductive inference problems: action\nset prediction, action sequence prediction, and abductive action verification.\nTo tackle these challenging tasks, we investigate various models, including\nestablished ones such as Transformers, Graph Neural Networks, CLIP, BLIP, GPT3,\nend-to-end trained Slow-Fast, Resnet50-3D, and ViT models. Furthermore, the\npaper introduces several innovative models tailored for abductive action\ninference, including a relational graph neural network, a relational bilinear\npooling model, a relational rule-based inference model, a relational GPT-3\nprompt method, and a relational Transformer model. Notably, the newly proposed\nobject-relational bilinear graph encoder-decoder (BiGED) model emerges as the\nmost effective among all methods evaluated, demonstrating good proficiency in\nhandling the intricacies of the Action Genome dataset. The contributions of\nthis research offer significant progress toward comprehending the implications\nof human actions and making highly plausible inferences concerning the outcomes\nof these actions.",
        "pdf_link": "https://arxiv.org/pdf/2210.13984v4.pdf"
    },
    {
        "title": "Exploring Euphemism Detection in Few-Shot and Zero-Shot Settings",
        "authors": [
            "Sedrick Scott Keh"
        ],
        "published": "2022-10-24T02:43:43Z",
        "summary": "This work builds upon the Euphemism Detection Shared Task proposed in the\nEMNLP 2022 FigLang Workshop, and extends it to few-shot and zero-shot settings.\nWe demonstrate a few-shot and zero-shot formulation using the dataset from the\nshared task, and we conduct experiments in these settings using RoBERTa and\nGPT-3. Our results show that language models are able to classify euphemistic\nterms relatively well even on new terms unseen during training, indicating that\nit is able to capture higher-level concepts related to euphemisms.",
        "pdf_link": "https://arxiv.org/pdf/2210.12926v1.pdf"
    },
    {
        "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
        "authors": [
            "Xingyao Wang",
            "Sha Li",
            "Heng Ji"
        ],
        "published": "2022-10-23T18:18:51Z",
        "summary": "Large Language Model (LLM) trained on a mixture of text and code has\ndemonstrated impressive capability in translating natural language (NL) into\nstructured code. We observe that semantic structures can be conveniently\ntranslated into code and propose Code4Struct to leverage such text-to-structure\ntranslation capability to tackle structured prediction tasks. As a case study,\nwe formulate Event Argument Extraction (EAE) as converting text into\nevent-argument structures that can be represented as a class object using code.\nThis alignment between structures and code enables us to take advantage of\nProgramming Language (PL) features such as inheritance and type annotation to\nintroduce external knowledge or add constraints. We show that, with sufficient\nin-context examples, formulating EAE as a code generation problem is\nadvantageous over using variants of text-based prompts. Despite only using 20\ntraining event instances for each event type, Code4Struct is comparable to\nsupervised models trained on 4,202 instances and outperforms current\nstate-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1.\nCode4Struct can use 10-shot training data from a sibling event type to predict\narguments for zero-resource event types and outperforms the zero-shot baseline\nby 12% absolute F1.",
        "pdf_link": "https://arxiv.org/pdf/2210.12810v2.pdf"
    },
    {
        "title": "Data Augmentation for Automated Essay Scoring using Transformer Models",
        "authors": [
            "Kshitij Gupta"
        ],
        "published": "2022-10-23T18:13:30Z",
        "summary": "Automated essay scoring is one of the most important problem in Natural\nLanguage Processing. It has been explored for a number of years, and it remains\npartially solved. In addition to its economic and educational usefulness, it\npresents research problems. Transfer learning has proved to be beneficial in\nNLP. Data augmentation techniques have also helped build state-of-the-art\nmodels for automated essay scoring. Many works in the past have attempted to\nsolve this problem by using RNNs, LSTMs, etc. This work examines the\ntransformer models like BERT, RoBERTa, etc. We empirically demonstrate the\neffectiveness of transformer models and data augmentation for automated essay\ngrading across many topics using a single model.",
        "pdf_link": "https://arxiv.org/pdf/2210.12809v5.pdf"
    },
    {
        "title": "Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition",
        "authors": [
            "Samuel Belkadi",
            "Lifeng Han",
            "Yuping Wu",
            "Goran Nenadic"
        ],
        "published": "2022-10-23T16:27:31Z",
        "summary": "The practice of fine-tuning Pre-trained Language Models (PLMs) from general\nor domain-specific data to a specific task with limited resources, has gained\npopularity within the field of natural language processing (NLP). In this work,\nwe re-visit this assumption and carry out an investigation in clinical NLP,\nspecifically Named Entity Recognition on drugs and their related attributes. We\ncompare Transformer models that are trained from scratch to fine-tuned\nBERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine\nthe impact of an additional CRF layer on such models to encourage contextual\nlearning. We use n2c2-2018 shared task data for model development and\nevaluations. The experimental outcomes show that 1) CRF layers improved all\nlanguage models; 2) referring to BIO-strict span level evaluation using\nmacro-average F1 score, although the fine-tuned LLMs achieved 0.83+ scores, the\nTransformerCRF model trained from scratch achieved 0.78+, demonstrating\ncomparable performances with much lower cost - e.g. with 39.80\\% less training\nparameters; 3) referring to BIO-strict span-level evaluation using\nweighted-average F1 score, ClinicalBERT-CRF, BERT-CRF, and TransformerCRF\nexhibited lower score differences, with 97.59\\%/97.44\\%/96.84\\% respectively.\n4) applying efficient training by down-sampling for better data distribution\nfurther reduced the training cost and need for data, while maintaining similar\nscores - i.e. around 0.02 points lower compared to using the full dataset. Our\nmodels will be hosted at \\url{https://github.com/HECTA-UoM/TransformerCRF}",
        "pdf_link": "https://arxiv.org/pdf/2210.12770v4.pdf"
    },
    {
        "title": "Discriminative Language Model as Semantic Consistency Scorer for Prompt-based Few-Shot Text Classification",
        "authors": [
            "Zhipeng Xie",
            "Yahe Li"
        ],
        "published": "2022-10-23T16:10:48Z",
        "summary": "This paper proposes a novel prompt-based finetuning method (called DLM-SCS)\nfor few-shot text classification by utilizing the discriminative language model\nELECTRA that is pretrained to distinguish whether a token is original or\ngenerated. The underlying idea is that the prompt instantiated with the true\nlabel should have higher semantic consistency score than other prompts with\nfalse labels. Since a prompt usually consists of several components (or parts),\nits semantic consistency can be decomposed accordingly. The semantic\nconsistency of each component is then computed by making use of the pretrained\nELECTRA model, without introducing extra parameters. Extensive experiments have\nshown that our model outperforms several state-of-the-art prompt-based few-shot\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2210.12763v1.pdf"
    },
    {
        "title": "Do Language Models Understand Measurements?",
        "authors": [
            "Sungjin Park",
            "Seungwoo Ryu",
            "Edward Choi"
        ],
        "published": "2022-10-23T10:52:52Z",
        "summary": "Recent success of pre-trained language models (PLMs) has stimulated interest\nin their ability to understand and work with numbers. Yet, the numerical\nreasoning over measurements has not been formally studied despite their\nimportance. In this study, we show that PLMs lack the capability required for\nreasoning over measurements. Furthermore, we find that a language model trained\non a measurement-rich corpus shows better performance on understanding\nmeasurements. We propose a simple embedding strategy to better distinguish\nbetween numbers and units, which leads to a significant improvement in the\nprobing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.12694v1.pdf"
    },
    {
        "title": "A BERT-based Deep Learning Approach for Reputation Analysis in Social Media",
        "authors": [
            "Mohammad Wali Ur Rahman",
            "Sicong Shao",
            "Pratik Satam",
            "Salim Hariri",
            "Chris Padilla",
            "Zoe Taylor",
            "Carlos Nevarez"
        ],
        "published": "2022-10-23T02:04:03Z",
        "summary": "Social media has become an essential part of the modern lifestyle, with its\nusage being highly prevalent. This has resulted in unprecedented amounts of\ndata generated from users in social media, such as users' attitudes, opinions,\ninterests, purchases, and activities across various aspects of their lives.\nTherefore, in a world of social media, where its power has shifted to users,\nactions taken by companies and public figures are subject to constantly being\nunder scrutiny by influential global audiences. As a result, reputation\nmanagement in social media has become essential as companies and public figures\nneed to maintain their reputation to preserve their reputation capital.\nHowever, domain experts still face the challenge of lacking appropriate\nsolutions to automate reliable online reputation analysis. To tackle this\nchallenge, we proposed a novel reputation analysis approach based on the\npopular language model BERT (Bidirectional Encoder Representations from\nTransformers). The proposed approach was evaluated on the reputational polarity\ntask using RepLab 2013 dataset. Compared to previous works, we achieved 5.8%\nimprovement in accuracy, 26.9% improvement in balanced accuracy, and 21.8%\nimprovement in terms of F-score.",
        "pdf_link": "https://arxiv.org/pdf/2211.01954v1.pdf"
    },
    {
        "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
        "authors": [
            "Xiangyu Peng",
            "Chen Xing",
            "Prafulla Kumar Choubey",
            "Chien-Sheng Wu",
            "Caiming Xiong"
        ],
        "published": "2022-10-23T01:33:16Z",
        "summary": "Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2210.12587v3.pdf"
    },
    {
        "title": "Language Model Pre-Training with Sparse Latent Typing",
        "authors": [
            "Liliang Ren",
            "Zixuan Zhang",
            "Han Wang",
            "Clare R. Voss",
            "Chengxiang Zhai",
            "Heng Ji"
        ],
        "published": "2022-10-23T00:37:08Z",
        "summary": "Modern large-scale Pre-trained Language Models (PLMs) have achieved\ntremendous success on a wide range of downstream tasks. However, most of the LM\npre-training objectives only focus on text reconstruction, but have not sought\nto learn latent-level interpretable representations of sentences. In this\npaper, we manage to push the language models to obtain a deeper understanding\nof sentences by proposing a new pre-training objective, Sparse Latent Typing,\nwhich enables the model to sparsely extract sentence-level keywords with\ndiverse latent types. Experimental results show that our model is able to learn\ninterpretable latent type categories in a self-supervised manner without using\nany external knowledge. Besides, the language model pre-trained with such an\nobjective also significantly improves Information Extraction related downstream\ntasks in both supervised and few-shot settings. Our code is publicly available\nat: https://github.com/renll/SparseLT.",
        "pdf_link": "https://arxiv.org/pdf/2210.12582v2.pdf"
    },
    {
        "title": "Understanding Domain Learning in Language Models Through Subpopulation Analysis",
        "authors": [
            "Zheng Zhao",
            "Yftah Ziser",
            "Shay B. Cohen"
        ],
        "published": "2022-10-22T21:12:57Z",
        "summary": "We investigate how different domains are encoded in modern neural network\narchitectures. We analyze the relationship between natural language domains,\nmodel size, and the amount of training data used. The primary analysis tool we\ndevelop is based on subpopulation analysis with Singular Vector Canonical\nCorrelation Analysis (SVCCA), which we apply to Transformer-based language\nmodels (LMs). We compare the latent representations of such a language model at\nits different layers from a pair of models: a model trained on multiple domains\n(an experimental model) and a model trained on a single domain (a control\nmodel). Through our method, we find that increasing the model capacity impacts\nhow domain information is stored in upper and lower layers differently. In\naddition, we show that larger experimental models simultaneously embed\ndomain-specific information as if they were conjoined control models. These\nfindings are confirmed qualitatively, demonstrating the validity of our method.",
        "pdf_link": "https://arxiv.org/pdf/2210.12553v1.pdf"
    },
    {
        "title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors",
        "authors": [
            "Kristy Choi",
            "Chris Cundy",
            "Sanjari Srivastava",
            "Stefano Ermon"
        ],
        "published": "2022-10-22T19:09:18Z",
        "summary": "Particularly in low-data regimes, an outstanding challenge in machine\nlearning is developing principled techniques for augmenting our models with\nsuitable priors. This is to encourage them to learn in ways that are compatible\nwith our understanding of the world. But in contrast to generic priors such as\nshrinkage or sparsity, we draw inspiration from the recent successes of\nlarge-scale language models (LMs) to construct task-specific priors distilled\nfrom the rich knowledge of LMs. Our method, Language Model Priors (LMPriors),\nincorporates auxiliary natural language metadata about the task -- such as\nvariable names and descriptions -- to encourage downstream model outputs to be\nconsistent with the LM's common-sense reasoning based on the metadata.\nEmpirically, we demonstrate that LMPriors improve model performance in settings\nwhere such natural language descriptions are available, and perform well on\nseveral tasks that benefit from such prior knowledge, such as feature\nselection, causal inference, and safe reinforcement learning.",
        "pdf_link": "https://arxiv.org/pdf/2210.12530v1.pdf"
    },
    {
        "title": "Spectrum-BERT: Pre-training of Deep Bidirectional Transformers for Spectral Classification of Chinese Liquors",
        "authors": [
            "Yansong Wang",
            "Yundong Sun",
            "Yansheng Fu",
            "Dongjie Zhu",
            "Zhaoshuo Tian"
        ],
        "published": "2022-10-22T13:11:25Z",
        "summary": "Spectral detection technology, as a non-invasive method for rapid detection\nof substances, combined with deep learning algorithms, has been widely used in\nfood detection. However, in real scenarios, acquiring and labeling spectral\ndata is an extremely labor-intensive task, which makes it impossible to provide\nenough high-quality data for training efficient supervised deep learning\nmodels. To better leverage limited samples, we apply pre-training & fine-tuning\nparadigm to the field of spectral detection for the first time and propose a\npre-training method of deep bidirectional transformers for spectral\nclassification of Chinese liquors, abbreviated as Spectrum-BERT. Specifically,\nfirst, to retain the model's sensitivity to the characteristic peak position\nand local information of the spectral curve, we innovatively partition the\ncurve into multiple blocks and obtain the embeddings of different blocks, as\nthe feature input for the next calculation. Second, in the pre-training stage,\nwe elaborately design two pre-training tasks, Next Curve Prediction (NCP) and\nMasked Curve Model (MCM), so that the model can effectively utilize unlabeled\nsamples to capture the potential knowledge of spectral data, breaking the\nrestrictions of the insufficient labeled samples, and improving the\napplicability and performance of the model in practical scenarios. Finally, we\nconduct a large number of experiments on the real liquor spectral dataset. In\nthe comparative experiments, the proposed Spectrum-BERT significantly\noutperforms the baselines in multiple metrics and this advantage is more\nsignificant on the imbalanced dataset. Moreover, in the parameter sensitivity\nexperiment, we also analyze the model performance under different parameter\nsettings, to provide a reference for subsequent research.",
        "pdf_link": "https://arxiv.org/pdf/2210.12440v1.pdf"
    },
    {
        "title": "Hindering Adversarial Attacks with Implicit Neural Representations",
        "authors": [
            "Andrei A. Rusu",
            "Dan A. Calian",
            "Sven Gowal",
            "Raia Hadsell"
        ],
        "published": "2022-10-22T13:10:24Z",
        "summary": "We introduce the Lossy Implicit Network Activation Coding (LINAC) defence, an\ninput transformation which successfully hinders several common adversarial\nattacks on CIFAR-$10$ classifiers for perturbations up to $\\epsilon = 8/255$ in\n$L_\\infty$ norm and $\\epsilon = 0.5$ in $L_2$ norm. Implicit neural\nrepresentations are used to approximately encode pixel colour intensities in\n$2\\text{D}$ images such that classifiers trained on transformed data appear to\nhave robustness to small perturbations without adversarial training or large\ndrops in performance. The seed of the random number generator used to\ninitialise and train the implicit neural representation turns out to be\nnecessary information for stronger generic attacks, suggesting its role as a\nprivate key. We devise a Parametric Bypass Approximation (PBA) attack strategy\nfor key-based defences, which successfully invalidates an existing method in\nthis category. Interestingly, our LINAC defence also hinders some transfer and\nadaptive attacks, including our novel PBA strategy. Our results emphasise the\nimportance of a broad range of customised attacks despite apparent robustness\naccording to standard evaluations. LINAC source code and parameters of defended\nclassifier evaluated throughout this submission are available:\nhttps://github.com/deepmind/linac",
        "pdf_link": "https://arxiv.org/pdf/2210.13982v1.pdf"
    },
    {
        "title": "Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and Reliable Language Model",
        "authors": [
            "Dongkyu Lee",
            "Zhiliang Tian",
            "Yingxiu Zhao",
            "Ka Chun Cheung",
            "Nevin L. Zhang"
        ],
        "published": "2022-10-22T11:57:10Z",
        "summary": "In knowledge distillation, a student model is trained with supervisions from\nboth knowledge from a teacher and observations drawn from a training data\ndistribution. Knowledge of a teacher is considered a subject that holds\ninter-class relations which send a meaningful supervision to a student; hence,\nmuch effort has been put to find such knowledge to be distilled. In this paper,\nwe explore a question that has been given little attention: \"when to distill\nsuch knowledge.\" The question is answered in our work with the concept of model\ncalibration; we view a teacher model not only as a source of knowledge but also\nas a gauge to detect miscalibration of a student. This simple and yet novel\nview leads to a hard gate knowledge distillation scheme that switches between\nlearning from a teacher model and training data. We verify the gating mechanism\nin the context of natural language generation at both the token-level and the\nsentence-level. Empirical comparisons with strong baselines show that hard gate\nknowledge distillation not only improves model generalization, but also\nsignificantly lowers model calibration error.",
        "pdf_link": "https://arxiv.org/pdf/2210.12427v1.pdf"
    },
    {
        "title": "Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling",
        "authors": [
            "Vidhisha Balachandran",
            "Hannaneh Hajishirzi",
            "William W. Cohen",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-22T07:16:19Z",
        "summary": "Abstractive summarization models often generate inconsistent summaries\ncontaining factual errors or hallucinated content. Recent works focus on\ncorrecting factual errors in generated summaries via post-editing. Such\ncorrection models are trained using adversarial non-factual summaries\nconstructed using heuristic rules for injecting errors. However, generating\nnon-factual summaries using heuristics often does not generalize well to actual\nmodel errors. In this work, we propose to generate hard, representative\nsynthetic examples of non-factual summaries through infilling language models.\nWith this data, we train a more robust fact-correction model to post-edit the\nsummaries to improve factual consistency. Through quantitative and qualitative\nexperiments on two popular summarization datasets -- CNN/DM and XSum -- we show\nthat our approach vastly outperforms prior methods in correcting erroneous\nsummaries. Our model -- FactEdit -- improves factuality scores by over ~11\npoints on CNN/DM and over ~31 points on XSum on average across multiple\nsummarization models, producing more factual summaries while maintaining\ncompetitive summarization quality.",
        "pdf_link": "https://arxiv.org/pdf/2210.12378v2.pdf"
    },
    {
        "title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation",
        "authors": [
            "Phillip Howard",
            "Gadi Singer",
            "Vasudev Lal",
            "Yejin Choi",
            "Swabha Swayamdipta"
        ],
        "published": "2022-10-22T06:29:21Z",
        "summary": "While counterfactual data augmentation offers a promising step towards robust\ngeneralization in natural language processing, producing a set of\ncounterfactuals that offer valuable inductive bias for models remains a\nchallenge. Most existing approaches for producing counterfactuals, manual or\nautomated, rely on small perturbations via minimal edits, resulting in\nsimplistic changes. We introduce NeuroCounterfactuals, designed as loose\ncounterfactuals, allowing for larger edits which result in naturalistic\ngenerations containing linguistic diversity, while still bearing similarity to\nthe original document. Our novel generative approach bridges the benefits of\nconstrained decoding, with those of language model adaptation for sentiment\nsteering. Training data augmentation with our generations results in both\nin-domain and out-of-domain improvements for sentiment classification,\noutperforming even manually curated counterfactuals, under select settings. We\nfurther present detailed analyses to show the advantages of\nNeuroCounterfactuals over approaches involving simple, minimal edits.",
        "pdf_link": "https://arxiv.org/pdf/2210.12365v1.pdf"
    },
    {
        "title": "Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks",
        "authors": [
            "Arijit Sehanobish",
            "Kawshik Kannan",
            "Nabila Abraham",
            "Anasuya Das",
            "Benjamin Odry"
        ],
        "published": "2022-10-22T05:22:29Z",
        "summary": "Large pretrained Transformer-based language models like BERT and GPT have\nchanged the landscape of Natural Language Processing (NLP). However, fine\ntuning such models still requires a large number of training examples for each\ntarget task, thus annotating multiple datasets and training these models on\nvarious downstream tasks becomes time consuming and expensive. In this work, we\npropose a simple extension of the Prototypical Networks for few-shot text\nclassification. Our main idea is to replace the class prototypes by Gaussians\nand introduce a regularization term that encourages the examples to be\nclustered near the appropriate class centroids. Experimental results show that\nour method outperforms various strong baselines on 13 public and 4 internal\ndatasets. Furthermore, we use the class distributions as a tool for detecting\npotential out-of-distribution (OOD) data points during deployment.",
        "pdf_link": "https://arxiv.org/pdf/2210.13979v2.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
        "authors": [
            "Joshua Robinson",
            "Christopher Michael Rytting",
            "David Wingate"
        ],
        "published": "2022-10-22T05:04:54Z",
        "summary": "While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.",
        "pdf_link": "https://arxiv.org/pdf/2210.12353v3.pdf"
    },
    {
        "title": "P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training",
        "authors": [
            "Junwei Bao",
            "Yifan Wang",
            "Jiangyong Ying",
            "Yeyun Gong",
            "Jing Zhao",
            "Youzheng Wu",
            "Xiaodong He"
        ],
        "published": "2022-10-22T03:50:59Z",
        "summary": "Conventional autoregressive left-to-right (L2R) sequence generation faces two\nissues during decoding: limited to unidirectional target sequence modeling, and\nconstrained on strong local dependencies. To address the aforementioned\nproblem, we propose P$^3$LM, a probabilistically permuted prophet language\nmodel, which strengthens the modeling of bidirectional information and long\ntoken dependencies for sequence generation. Specifically, P$^3$LM learns to\ngenerate tokens in permuted order upon an order-aware transformer decoder, as\nwell as to generate the corresponding future $N$ tokens with a multi-stream\nattention mechanism. Extensive experiments are conducted on the GLGE benchmark,\nwhich includes four datasets for summarization, two for question generation,\none for conversational question answering, and one for dialog response\ngeneration, where P$^3$LM achieves state-of-the-art results compared with\nstrong publicly available generative pre-training methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.12339v1.pdf"
    },
    {
        "title": "PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding",
        "authors": [
            "Niranjan Uma Naresh",
            "Ziyan Jiang",
            "Ankit",
            "Sungjin Lee",
            "Jie Hao",
            "Xing Fan",
            "Chenlei Guo"
        ],
        "published": "2022-10-22T00:14:47Z",
        "summary": "Conversational understanding is an integral part of modern intelligent\ndevices. In a large fraction of the global traffic from customers using smart\ndigital assistants, frictions in dialogues may be attributed to incorrect\nunderstanding of the entities in a customer's query due to factors including\nambiguous mentions, mispronunciation, background noise and faulty on-device\nsignal processing. Such errors are compounded by two common deficiencies from\nintelligent devices namely, (1) the device not being tailored to individual\ncustomers, and (2) the device responses being unaware of the context in the\nconversation session. Viewing this problem via the lens of retrieval-based\nsearch engines, we build and evaluate a scalable entity correction system,\nPENTATRON. The system leverages a parametric transformer-based language model\nto learn patterns from in-session customer-device interactions coupled with a\nnon-parametric personalized entity index to compute the correct query, which\naids downstream components in reasoning about the best response. In addition to\nestablishing baselines and demonstrating the value of personalized and\ncontext-aware systems, we use multitasking to learn the domain of the correct\nentity. We also investigate the utility of language model prompts. Through\nextensive experiments, we show a significant upward movement of the key metric\n(Exact Match) by up to 500.97% (relative to the baseline).",
        "pdf_link": "https://arxiv.org/pdf/2210.12308v1.pdf"
    },
    {
        "title": "What do Large Language Models Learn beyond Language?",
        "authors": [
            "Avinash Madasu",
            "Shashank Srivastava"
        ],
        "published": "2022-10-21T23:43:13Z",
        "summary": "Large language models (LMs) have rapidly become a mainstay in Natural\nLanguage Processing. These models are known to acquire rich linguistic\nknowledge from training on large amounts of text. In this paper, we investigate\nif pre-training on text also confers these models with helpful `inductive\nbiases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic\ntasks involving quantitative computations, recognizing regular expressions and\nreasoning over strings. We find that pretrained models significantly outperform\ncomparable non-pretrained neural models. This remains true also in experiments\nwith training non-pretrained models with fewer parameters to account for model\nregularization effects. We further explore the effect of text domain on LMs by\npretraining models from text from different domains and provenances. Our\nexperiments surprisingly reveal that the positive effects of pre-training\npersist even when pretraining on multi-lingual text or computer code, and even\nfor text generated from synthetic languages. Our findings suggest a hitherto\nunexplored deep connection between pre-training and inductive learning\nabilities of language models.",
        "pdf_link": "https://arxiv.org/pdf/2210.12302v1.pdf"
    },
    {
        "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",
        "authors": [
            "Albert Q. Jiang",
            "Sean Welleck",
            "Jin Peng Zhou",
            "Wenda Li",
            "Jiacheng Liu",
            "Mateja Jamnik",
            "Timoth\u00e9e Lacroix",
            "Yuhuai Wu",
            "Guillaume Lample"
        ],
        "published": "2022-10-21T22:37:22Z",
        "summary": "The formalization of existing mathematical proofs is a notoriously difficult\nprocess. Despite decades of research on automation and proof assistants,\nwriting formal proofs remains arduous and only accessible to a few experts.\nWhile previous studies to automate formalization focused on powerful search\nalgorithms, no attempts were made to take advantage of available informal\nproofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method\nthat maps informal proofs to formal proof sketches, and uses the sketches to\nguide an automated prover by directing its search to easier sub-problems. We\ninvestigate two relevant setups where informal proofs are either written by\nhumans or generated by a language model. Our experiments and ablation studies\nshow that large language models are able to produce well-structured formal\nsketches that follow the same reasoning steps as the informal proofs. Guiding\nan automated prover with these sketches enhances its performance from 20.9% to\n39.3% on a collection of mathematical competition problems.",
        "pdf_link": "https://arxiv.org/pdf/2210.12283v3.pdf"
    },
    {
        "title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
        "authors": [
            "Yue Yang",
            "Wenlin Yao",
            "Hongming Zhang",
            "Xiaoyang Wang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2022-10-21T21:33:10Z",
        "summary": "Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.12261v1.pdf"
    },
    {
        "title": "SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation",
        "authors": [
            "Zekun Li",
            "Jina Kim",
            "Yao-Yi Chiang",
            "Muhao Chen"
        ],
        "published": "2022-10-21T19:42:32Z",
        "summary": "Named geographic entities (geo-entities for short) are the building blocks of\nmany geographic datasets. Characterizing geo-entities is integral to various\napplication domains, such as geo-intelligence and map comprehension, while a\nkey challenge is to capture the spatial-varying context of an entity. We\nhypothesize that we shall know the characteristics of a geo-entity by its\nsurrounding entities, similar to knowing word meanings by their linguistic\ncontext. Accordingly, we propose a novel spatial language model, SpaBERT, which\nprovides a general-purpose geo-entity representation based on neighboring\nentities in geospatial data. SpaBERT extends BERT to capture linearized spatial\ncontext, while incorporating a spatial coordinate embedding mechanism to\npreserve spatial relations of entities in the 2-dimensional space. SpaBERT is\npretrained with masked language modeling and masked entity prediction tasks to\nlearn spatial dependencies. We apply SpaBERT to two downstream tasks:\ngeo-entity typing and geo-entity linking. Compared with the existing language\nmodels that do not use spatial context, SpaBERT shows significant performance\nimprovement on both tasks. We also analyze the entity representation from\nSpaBERT in various settings and the effect of spatial coordinate embedding.",
        "pdf_link": "https://arxiv.org/pdf/2210.12213v1.pdf"
    },
    {
        "title": "Probing with Noise: Unpicking the Warp and Weft of Embeddings",
        "authors": [
            "Filip Klubi\u010dka",
            "John D. Kelleher"
        ],
        "published": "2022-10-21T19:33:33Z",
        "summary": "Improving our understanding of how information is encoded in vector space can\nyield valuable interpretability insights. Alongside vector dimensions, we argue\nthat it is possible for the vector norm to also carry linguistic information.\nWe develop a method to test this: an extension of the probing framework which\nallows for relative intrinsic interpretations of probing results. It relies on\nintroducing noise that ablates information encoded in embeddings, grounded in\nrandom baselines and confidence intervals. We apply the method to\nwell-established probing tasks and find evidence that confirms the existence of\nseparate information containers in English GloVe and BERT embeddings. Our\ncorrelation analysis aligns with the experimental findings that different\nencoders use the norm to encode different kinds of information: GloVe stores\nsyntactic and sentence length information in the vector norm, while BERT uses\nit to encode contextual incongruity.",
        "pdf_link": "https://arxiv.org/pdf/2210.12206v1.pdf"
    },
    {
        "title": "Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities",
        "authors": [
            "Suhas Arehalli",
            "Brian Dillon",
            "Tal Linzen"
        ],
        "published": "2022-10-21T18:30:56Z",
        "summary": "Humans exhibit garden path effects: When reading sentences that are\ntemporarily structurally ambiguous, they slow down when the structure is\ndisambiguated in favor of the less preferred alternative. Surprisal theory\n(Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes\nthat these slowdowns are due to the unpredictability of each of the words that\noccur in these sentences. Challenging this hypothesis, van Schijndel & Linzen\n(2021) find that estimates of the cost of word predictability derived from\nlanguage models severely underestimate the magnitude of human garden path\neffects. In this work, we consider whether this underestimation is due to the\nfact that humans weight syntactic factors in their predictions more highly than\nlanguage models do. We propose a method for estimating syntactic predictability\nfrom a language model, allowing us to weigh the cost of lexical and syntactic\npredictability independently. We find that treating syntactic predictability\nindependently from lexical predictability indeed results in larger estimates of\ngarden path. At the same time, even when syntactic predictability is\nindependently weighted, surprisal still greatly underestimate the magnitude of\nhuman garden path effects. Our results support the hypothesis that\npredictability is not the only factor responsible for the processing cost\nassociated with garden path sentences.",
        "pdf_link": "https://arxiv.org/pdf/2210.12187v2.pdf"
    },
    {
        "title": "Discovering Differences in the Representation of People using Contextualized Semantic Axes",
        "authors": [
            "Li Lucy",
            "Divya Tadimeti",
            "David Bamman"
        ],
        "published": "2022-10-21T18:02:19Z",
        "summary": "A common paradigm for identifying semantic differences across social and\ntemporal contexts is the use of static word embeddings and their distances. In\nparticular, past work has compared embeddings against \"semantic axes\" that\nrepresent two opposing concepts. We extend this paradigm to BERT embeddings,\nand construct contextualized axes that mitigate the pitfall where antonyms have\nneighboring representations. We validate and demonstrate these axes on two\npeople-centric datasets: occupations from Wikipedia, and multi-platform\ndiscussions in extremist, men's communities over fourteen years. In both\nstudies, contextualized semantic axes can characterize differences among\ninstances of the same word type. In the latter study, we show that references\nto women and the contexts around them have become more detestable over time.",
        "pdf_link": "https://arxiv.org/pdf/2210.12170v1.pdf"
    },
    {
        "title": "WikiWhy: Answering and Explaining Cause-and-Effect Questions",
        "authors": [
            "Matthew Ho",
            "Aditya Sharma",
            "Justin Chang",
            "Michael Saxon",
            "Sharon Levy",
            "Yujie Lu",
            "William Yang Wang"
        ],
        "published": "2022-10-21T17:59:03Z",
        "summary": "As large language models (LLMs) grow larger and more sophisticated, assessing\ntheir \"reasoning\" capabilities in natural language grows more challenging.\nRecent question answering (QA) benchmarks that attempt to assess reasoning are\noften limited by a narrow scope of covered situations and subject matters. We\nintroduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining\nwhy an answer is true in natural language. WikiWhy contains over 9,000 \"why\"\nquestion-answer-rationale triples, grounded on Wikipedia facts across a diverse\nset of topics. Each rationale is a set of supporting statements connecting the\nquestion to the answer. WikiWhy serves as a benchmark for the reasoning\ncapabilities of LLMs because it demands rigorous explicit rationales for each\nanswer to demonstrate the acquisition of implicit commonsense knowledge, which\nis unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%\nhuman-evaluated correctness in the end-to-end answer & explain condition,\nleaving significant room for future improvements.",
        "pdf_link": "https://arxiv.org/pdf/2210.12152v2.pdf"
    },
    {
        "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
        "authors": [
            "Alessandro Stolfo",
            "Zhijing Jin",
            "Kumar Shridhar",
            "Bernhard Sch\u00f6lkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2022-10-21T15:12:37Z",
        "summary": "We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen generating a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands, and\nmath operators on the output solution. By grounding the behavioral analysis in\na causal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of math\nword problems. Our analysis shows that robustness does not appear to\ncontinuously improve as a function of size, but the GPT-3 Davinci models (175B)\nachieve a dramatic improvement in both robustness and sensitivity compared to\nall other GPT variants.",
        "pdf_link": "https://arxiv.org/pdf/2210.12023v3.pdf"
    },
    {
        "title": "LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling",
        "authors": [
            "Dongsheng Chen",
            "Chaofan Tao",
            "Lu Hou",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022-10-21T13:03:49Z",
        "summary": "Recent large-scale video-language pre-trained models have shown appealing\nperformance on various downstream tasks. However, the pre-training process is\ncomputationally expensive due to the requirement of millions of video-text\npairs and the redundant data structure of each video. To mitigate these\nproblems, we propose LiteVL, which adapts a pre-trained image-language model\nBLIP into a video-text model directly on downstream tasks, without heavy\npre-training. To enhance the temporal modeling lacking in the image-language\nmodel, we propose to add temporal attention modules in the image encoder of\nBLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also\npropose a non-parametric pooling mechanism to adaptively reweight the\nfine-grained video embedding conditioned on the text. Experimental results on\ntext-video retrieval and video question answering show that the proposed LiteVL\neven outperforms previous video-language pre-trained models by a clear margin,\nthough without any video-language pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2210.11929v1.pdf"
    },
    {
        "title": "A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT",
        "authors": [
            "Hadeel Saadany",
            "Constantin Orasan",
            "Emad Mohamed",
            "Ashraf Tantawy"
        ],
        "published": "2022-10-21T11:55:55Z",
        "summary": "In the online world, Machine Translation (MT) systems are extensively used to\ntranslate User-Generated Text (UGT) such as reviews, tweets, and social media\nposts, where the main message is often the author's positive or negative\nattitude towards the topic of the text. However, MT systems still lack accuracy\nin some low-resource languages and sometimes make critical translation errors\nthat completely flip the sentiment polarity of the target word or phrase and\nhence delivers a wrong affect message. This is particularly noticeable in texts\nthat do not follow common lexico-grammatical standards such as the dialectical\nArabic (DA) used on online platforms. In this research, we aim to improve the\ntranslation of sentiment in UGT written in the dialectical versions of the\nArabic language to English. Given the scarcity of gold-standard parallel data\nfor DA-EN in the UGT domain, we introduce a semi-supervised approach that\nexploits both monolingual and parallel data for training an NMT system\ninitialised by a cross-lingual language model trained with supervised and\nunsupervised modeling objectives. We assess the accuracy of sentiment\ntranslation by our proposed system through a numerical 'sentiment-closeness'\nmeasure as well as human evaluation. We will show that our semi-supervised MT\nsystem can significantly help with correcting sentiment errors detected in the\nonline translation of dialectical Arabic UGT.",
        "pdf_link": "https://arxiv.org/pdf/2210.11899v2.pdf"
    },
    {
        "title": "Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction Classification",
        "authors": [
            "Andrey Sakhovskiy",
            "Elena Tutubalina"
        ],
        "published": "2022-10-21T11:41:45Z",
        "summary": "In this paper, we focus on the classification of tweets as sources of\npotential signals for adverse drug effects (ADEs) or drug reactions (ADRs).\nFollowing the intuition that text and drug structure representations are\ncomplementary, we introduce a multimodal model with two components. These\ncomponents are state-of-the-art BERT-based models for language understanding\nand molecular property prediction. Experiments were carried out on multilingual\nbenchmarks of the Social Media Mining for Health Research and Applications\n(#SMM4H) initiative. Our models obtained state-of-the-art results of 0.61 F1\nand 0.57 F1 on #SMM4H 2021 Shared Tasks 1a and 2 in English and Russian,\nrespectively. On the classification of French tweets from SMM4H 2020 Task 1,\nour approach pushes the state of the art by an absolute gain of 8% F1. Our\nexperiments show that the molecular information obtained from neural networks\nis more beneficial for ADE classification than traditional molecular\ndescriptors. The source code for our models is freely available at\nhttps://github.com/Andoree/smm4h_2021_classification.",
        "pdf_link": "https://arxiv.org/pdf/2210.13238v1.pdf"
    },
    {
        "title": "Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer",
        "authors": [
            "Jan \u0160vec",
            "Jan Lehe\u010dka",
            "Lubo\u0161 \u0160m\u00eddl"
        ],
        "published": "2022-10-21T11:26:59Z",
        "summary": "In recent years, the standard hybrid DNN-HMM speech recognizers are\noutperformed by the end-to-end speech recognition systems. One of the very\npromising approaches is the grapheme Wav2Vec 2.0 model, which uses the\nself-supervised pretraining approach combined with transfer learning of the\nfine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and\nlanguage model, the approach is suitable for tasks where obtaining such models\nis not easy or almost impossible.\n  In this paper, we use the Wav2Vec speech recognizer in the task of spoken\nterm detection over a large set of spoken documents. The method employs a deep\nLSTM network which maps the recognized hypothesis and the searched term into a\nshared pronunciation embedding space in which the term occurrences and the\nassigned scores are easily computed.\n  The paper describes a bootstrapping approach that allows the transfer of the\nknowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid\nASR into the context of grapheme-based Wav2Vec. The proposed method outperforms\nthe previously published system based on the combination of the DNN-HMM hybrid\nASR and phoneme recognizer by a large margin on the MALACH data in both English\nand Czech languages.",
        "pdf_link": "https://arxiv.org/pdf/2210.11885v1.pdf"
    },
    {
        "title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering",
        "authors": [
            "Minchul Lee",
            "Kijong Han",
            "Myeong Cheol Shin"
        ],
        "published": "2022-10-21T10:46:41Z",
        "summary": "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a\nlimitation dealing with long inputs due to its attention mechanism. Longformer,\nETC and BigBird addressed this issue and effectively solved the quadratic\ndependency problem. However we find that these models are not sufficient, and\npropose LittleBird, a novel model based on BigBird with improved speed and\nmemory footprint while maintaining accuracy. In particular, we devise a more\nflexible and efficient position representation method based on Attention with\nLinear Biases (ALiBi). We also show that replacing the method of global\ninformation represented in the BigBird with pack and unpack attention is more\neffective. The proposed model can work on long inputs even after being\npre-trained on short inputs, and can be trained efficiently reusing existing\npre-trained language model for short inputs. This is a significant benefit for\nlow-resource languages where large amounts of long text data are difficult to\nobtain. As a result, our experiments show that LittleBird works very well in a\nvariety of languages, achieving high performance in question answering tasks,\nparticularly in KorQuAD2.0, Korean Question Answering Dataset for long\nparagraphs.",
        "pdf_link": "https://arxiv.org/pdf/2210.11870v2.pdf"
    },
    {
        "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
        "authors": [
            "Yingbo Gao",
            "Christian Herold",
            "Zijian Yang",
            "Hermann Ney"
        ],
        "published": "2022-10-21T08:33:55Z",
        "summary": "Encoder-decoder architecture is widely adopted for sequence-to-sequence\nmodeling tasks. For machine translation, despite the evolution from long\nshort-term memory networks to Transformer networks, plus the introduction and\ndevelopment of attention mechanism, encoder-decoder is still the de facto\nneural network architecture for state-of-the-art models. While the motivation\nfor decoding information from some hidden space is straightforward, the strict\nseparation of the encoding and decoding steps into an encoder and a decoder in\nthe model architecture is not necessarily a must. Compared to the task of\nautoregressive language modeling in the target language, machine translation\nsimply has an additional source sentence as context. Given the fact that neural\nlanguage models nowadays can already handle rather long contexts in the target\nlanguage, it is natural to ask whether simply concatenating the source and\ntarget sentences and training a language model to do translation would work. In\nthis work, we investigate the aforementioned concept for machine translation.\nSpecifically, we experiment with bilingual translation, translation with\nadditional target monolingual data, and multilingual translation. In all cases,\nthis alternative approach performs on par with the baseline encoder-decoder\nTransformer, suggesting that an encoder-decoder architecture might be redundant\nfor neural machine translation.",
        "pdf_link": "https://arxiv.org/pdf/2210.11807v1.pdf"
    },
    {
        "title": "InforMask: Unsupervised Informative Masking for Language Model Pretraining",
        "authors": [
            "Nafis Sadeq",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2022-10-21T07:10:56Z",
        "summary": "Masked language modeling is widely used for pretraining large language models\nfor natural language understanding (NLU). However, random masking is\nsuboptimal, allocating an equal masking rate for all tokens. In this paper, we\npropose InforMask, a new unsupervised masking strategy for training masked\nlanguage models. InforMask exploits Pointwise Mutual Information (PMI) to\nselect the most informative tokens to mask. We further propose two\noptimizations for InforMask to improve its efficiency. With a one-off\npreprocessing step, InforMask outperforms random masking and previously\nproposed masking strategies on the factual recall benchmark LAMA and the\nquestion answering benchmark SQuAD v1 and v2.",
        "pdf_link": "https://arxiv.org/pdf/2210.11771v1.pdf"
    },
    {
        "title": "Dissecting Deep Metric Learning Losses for Image-Text Retrieval",
        "authors": [
            "Hong Xuan",
            "Xi Chen"
        ],
        "published": "2022-10-21T06:48:27Z",
        "summary": "Visual-Semantic Embedding (VSE) is a prevalent approach in image-text\nretrieval by learning a joint embedding space between the image and language\nmodalities where semantic similarities would be preserved. The triplet loss\nwith hard-negative mining has become the de-facto objective for most VSE\nmethods. Inspired by recent progress in deep metric learning (DML) in the image\ndomain which gives rise to new loss functions that outperform triplet loss, in\nthis paper, we revisit the problem of finding better objectives for VSE in\nimage-text matching. Despite some attempts in designing losses based on\ngradient movement, most DML losses are defined empirically in the embedding\nspace. Instead of directly applying these loss functions which may lead to\nsub-optimal gradient updates in model parameters, in this paper we present a\nnovel Gradient-based Objective AnaLysis framework, or \\textit{GOAL}, to\nsystematically analyze the combinations and reweighting of the gradients in\nexisting DML functions. With the help of this analysis framework, we further\npropose a new family of objectives in the gradient space exploring different\ngradient combinations. In the event that the gradients are not integrable to a\nvalid loss function, we implement our proposed objectives such that they would\ndirectly operate in the gradient space instead of on the losses in the\nembedding space. Comprehensive experiments have demonstrated that our novel\nobjectives have consistently improved performance over baselines across\ndifferent visual/text features and model frameworks. We also showed the\ngeneralizability of the GOAL framework by extending it to other models using\ntriplet family losses including vision-language model with heavy cross-modal\ninteractions and have achieved state-of-the-art results on the image-text\nretrieval tasks on COCO and Flick30K.",
        "pdf_link": "https://arxiv.org/pdf/2210.13188v1.pdf"
    },
    {
        "title": "Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale",
        "authors": [
            "Ran Tian",
            "Ankur P. Parikh"
        ],
        "published": "2022-10-21T02:37:58Z",
        "summary": "We present Amos, a stochastic gradient-based optimizer designed for training\ndeep neural networks. It can be viewed as an Adam optimizer with theoretically\nsupported, adaptive learning-rate decay and weight decay. A key insight behind\nAmos is that it leverages model-specific information to determine the initial\nlearning-rate and decaying schedules. When used for pre-training BERT variants\nand T5, Amos consistently converges faster than the state-of-the-art settings\nof AdamW, achieving better validation loss within <=70% training steps and\ntime, while requiring <=51% memory for slot variables. Our code is open-sourced\nat: https://github.com/google-research/jestimator",
        "pdf_link": "https://arxiv.org/pdf/2210.11693v2.pdf"
    },
    {
        "title": "SLING: Sino Linguistic Evaluation of Large Language Models",
        "authors": [
            "Yixiao Song",
            "Kalpesh Krishna",
            "Rajesh Bhatt",
            "Mohit Iyyer"
        ],
        "published": "2022-10-21T02:29:39Z",
        "summary": "To understand what kinds of linguistic knowledge are encoded by pretrained\nChinese language models (LMs), we introduce the benchmark of Sino LINGuistics\n(SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese\ngrouped into 9 high-level linguistic phenomena. Each pair demonstrates the\nacceptability contrast of a specific syntactic or semantic phenomenon (e.g.,\nThe keys are lost vs. The keys is lost), and an LM should assign lower\nperplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang\net al., 2021), which also contains Chinese minimal pairs and was created by\ntranslating the vocabulary of the English BLiMP dataset, the minimal pairs in\nSLING are derived primarily by applying syntactic and lexical transformations\nto naturally-occurring, linguist-annotated sentences from the Chinese Treebank\n9.0, thus addressing severe issues in CLiMP's data generation process. We test\n18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and\nmulti-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show\nthat the average accuracy for LMs is far below human performance (69.7% vs.\n97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested\nLMs, even much larger ones. Additionally, we find that most LMs have a strong\ngender and number (singular/plural) bias, and they perform better on local\nphenomena than hierarchical ones.",
        "pdf_link": "https://arxiv.org/pdf/2210.11689v1.pdf"
    },
    {
        "title": "Using Large Language Models to Enhance Programming Error Messages",
        "authors": [
            "Juho Leinonen",
            "Arto Hellas",
            "Sami Sarsa",
            "Brent Reeves",
            "Paul Denny",
            "James Prather",
            "Brett A. Becker"
        ],
        "published": "2022-10-20T23:17:26Z",
        "summary": "A key part of learning to program is learning to understand programming error\nmessages. They can be hard to interpret and identifying the cause of errors can\nbe time-consuming. One factor in this challenge is that the messages are\ntypically intended for an audience that already knows how to program, or even\nfor programming environments that then use the information to highlight areas\nin code. Researchers have been working on making these errors more novice\nfriendly since the 1960s, however progress has been slow. The present work\ncontributes to this stream of research by using large language models to\nenhance programming error messages with explanations of the errors and\nsuggestions on how to fix the error. Large language models can be used to\ncreate useful and novice-friendly enhancements to programming error messages\nthat sometimes surpass the original programming error messages in\ninterpretability and actionability. These results provide further evidence of\nthe benefits of large language models for computing educators, highlighting\ntheir use in areas known to be challenging for students. We further discuss the\nbenefits and downsides of large language models and highlight future streams of\nresearch for enhancing programming error messages.",
        "pdf_link": "https://arxiv.org/pdf/2210.11630v1.pdf"
    },
    {
        "title": "Large Language Models Can Self-Improve",
        "authors": [
            "Jiaxin Huang",
            "Shixiang Shane Gu",
            "Le Hou",
            "Yuexin Wu",
            "Xuezhi Wang",
            "Hongkun Yu",
            "Jiawei Han"
        ],
        "published": "2022-10-20T21:53:54Z",
        "summary": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2210.11610v2.pdf"
    },
    {
        "title": "3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows",
        "authors": [
            "Vivian Liu",
            "Jo Vermeulen",
            "George Fitzmaurice",
            "Justin Matejka"
        ],
        "published": "2022-10-20T21:28:34Z",
        "summary": "Text-to-image AI are capable of generating novel images for inspiration, but\ntheir applications for 3D design workflows and how designers can build 3D\nmodels using AI-provided inspiration have not yet been explored. To investigate\nthis, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a\nplugin that generates 2D image inspiration for 3D design. 3DALL-E allows users\nto construct text and image prompts based on what they are modeling. In a study\nwith 13 designers, we found that designers saw great potential in 3DALL-E\nwithin their workflows and could use text-to-image AI to produce reference\nimages, prevent design fixation, and inspire design considerations. We\nelaborate on prompting patterns observed across 3D modeling tasks and provide\nmeasures of prompt complexity observed across participants. From our findings,\nwe discuss how 3DALL-E can merge with existing generative design workflows and\npropose prompt bibliographies as a form of human-AI design history.",
        "pdf_link": "https://arxiv.org/pdf/2210.11603v2.pdf"
    },
    {
        "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus",
        "authors": [
            "Shuang Li",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Igor Mordatch"
        ],
        "published": "2022-10-20T18:46:31Z",
        "summary": "Large pre-trained models exhibit distinct and complementary capabilities\ndependent on the data they are trained on. Language models such as GPT-3 are\ncapable of textual reasoning but cannot understand visual information, while\nvision models such as DALL-E can generate photorealistic photos but fail to\nunderstand complex language descriptions. In this work, we propose a unified\nframework for composing ensembles of different pre-trained models -- combining\nthe strengths of each individual model to solve various multimodal problems in\na zero-shot manner. We use pre-trained models as \"generators\" or \"scorers\" and\ncompose them via closed-loop iterative consensus optimization. The generator\nconstructs proposals and the scorers iteratively provide feedback to refine the\ngenerated result. Such closed-loop communication enables models to correct\nerrors caused by other models, significantly boosting performance on downstream\ntasks, e.g. improving accuracy on grade school math problems by 7.5%, without\nrequiring any model finetuning. We demonstrate that consensus achieved by an\nensemble of scorers outperforms the feedback of a single scorer, by leveraging\nthe strengths of each expert model. Results show that the proposed method can\nbe used as a general purpose framework for a wide range of zero-shot multimodal\ntasks, such as image generation, video question answering, mathematical\nreasoning, and robotic manipulation. Project page:\nhttps://energy-based-model.github.io/composing-pretrained-models.",
        "pdf_link": "https://arxiv.org/pdf/2210.11522v1.pdf"
    },
    {
        "title": "ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications",
        "authors": [
            "Alex Gu",
            "Tamara Mitrovska",
            "Daniela Velez",
            "Jacob Andreas",
            "Armando Solar-Lezama"
        ],
        "published": "2022-10-20T17:59:19Z",
        "summary": "We introduce ObSynth, an interactive system leveraging the domain knowledge\nembedded in large language models (LLMs) to help users design object models\nfrom high level natural language prompts. This is an example of specification\nreification, the process of taking a high-level, potentially vague\nspecification and reifying it into a more concrete form. We evaluate ObSynth\nvia a user study, leading to three key findings: first, object models designed\nusing ObSynth are more detailed, showing that it often synthesizes fields users\nmight have otherwise omitted. Second, a majority of objects, methods, and\nfields generated by ObSynth are kept by the user in the final object model,\nhighlighting the quality of generated components. Third, ObSynth altered the\nworkflow of participants: they focus on checking that synthesized components\nwere correct rather than generating them from scratch, though ObSynth did not\nreduce the time participants took to generate object models.",
        "pdf_link": "https://arxiv.org/pdf/2210.11468v1.pdf"
    },
    {
        "title": "Transcending Scaling Laws with 0.1% Extra Compute",
        "authors": [
            "Yi Tay",
            "Jason Wei",
            "Hyung Won Chung",
            "Vinh Q. Tran",
            "David R. So",
            "Siamak Shakeri",
            "Xavier Garcia",
            "Huaixiu Steven Zheng",
            "Jinfeng Rao",
            "Aakanksha Chowdhery",
            "Denny Zhou",
            "Donald Metzler",
            "Slav Petrov",
            "Neil Houlsby",
            "Quoc V. Le",
            "Mostafa Dehghani"
        ],
        "published": "2022-10-20T16:46:41Z",
        "summary": "Scaling language models improves performance but comes with significant\ncomputational costs. This paper proposes UL2R, a method that substantially\nimproves existing language models and their scaling curves with a relatively\ntiny amount of extra compute. The key idea is to continue training a\nstate-of-the-art large language model (e.g., PaLM) on a few more steps with\nUL2's mixture-of-denoiser objective. We show that, with almost negligible extra\ncomputational costs and no new sources of data, we are able to substantially\nimprove the scaling properties of large language models on downstream metrics.\nIn this paper, we continue training PaLM with UL2R, introducing a new set of\nmodels at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B\nscale, we show an approximately 2x computational savings rate where U-PaLM\nachieves the same performance as the final PaLM 540B model at around half its\ncomputational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further\nshow that this improved scaling curve leads to 'emergent abilities' on\nchallenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM\non some tasks or demonstrates better quality at much smaller scale (62B as\nopposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many\nfew-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question\nanswering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual\ntasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide\nqualitative examples showing the new capabilities of U-PaLM for single and\nmulti-span infilling.",
        "pdf_link": "https://arxiv.org/pdf/2210.11399v2.pdf"
    },
    {
        "title": "Tele-Knowledge Pre-training for Fault Analysis",
        "authors": [
            "Zhuo Chen",
            "Wen Zhang",
            "Yufeng Huang",
            "Mingyang Chen",
            "Yuxia Geng",
            "Hongtao Yu",
            "Zhen Bi",
            "Yichi Zhang",
            "Zhen Yao",
            "Wenting Song",
            "Xinliang Wu",
            "Yi Yang",
            "Mingyi Chen",
            "Zhaoyang Lian",
            "Yingying Li",
            "Lei Cheng",
            "Huajun Chen"
        ],
        "published": "2022-10-20T14:31:48Z",
        "summary": "In this work, we share our experience on tele-knowledge pre-training for\nfault analysis, a crucial task in telecommunication applications that requires\na wide range of knowledge normally found in both machine log data and product\ndocuments. To organize this knowledge from experts uniformly, we propose to\ncreate a Tele-KG (tele-knowledge graph). Using this valuable data, we further\npropose a tele-domain language pre-training model TeleBERT and its\nknowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which\nincludes effective prompt hints, adaptive numerical data encoding, and two\nknowledge injection paradigms. Concretely, our proposal includes two stages:\nfirst, pre-training TeleBERT on 20 million tele-related corpora, and then\nre-training it on 1 million causal and machine-related corpora to obtain\nKTeleBERT. Our evaluation on multiple tasks related to fault analysis in\ntele-applications, including root-cause analysis, event association prediction,\nand fault chain tracing, shows that pre-training a language model with\ntele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT\nre-training further improves the performance of task models, highlighting the\neffectiveness of incorporating diverse tele-knowledge into the model.",
        "pdf_link": "https://arxiv.org/pdf/2210.11298v2.pdf"
    },
    {
        "title": "Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble",
        "authors": [
            "Hyunsoo Cho",
            "Choonghyun Park",
            "Jaewook Kang",
            "Kang Min Yoo",
            "Taeuk Kim",
            "Sang-goo Lee"
        ],
        "published": "2022-10-20T06:05:58Z",
        "summary": "Out-of-distribution (OOD) detection aims to discern outliers from the\nintended data distribution, which is crucial to maintaining high reliability\nand a good user experience. Most recent studies in OOD detection utilize the\ninformation from a single representation that resides in the penultimate layer\nto determine whether the input is anomalous or not. Although such a method is\nstraightforward, the potential of diverse information in the intermediate\nlayers is overlooked. In this paper, we propose a novel framework based on\ncontrastive learning that encourages intermediate features to learn\nlayer-specialized representations and assembles them implicitly into a single\nrepresentation to absorb rich information in the pre-trained language model.\nExtensive experiments in various intent classification and OOD datasets\ndemonstrate that our approach is significantly more effective than other works.",
        "pdf_link": "https://arxiv.org/pdf/2210.11034v1.pdf"
    },
    {
        "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
        "authors": [
            "Stefan Hegselmann",
            "Alejandro Buendia",
            "Hunter Lang",
            "Monica Agrawal",
            "Xiaoyi Jiang",
            "David Sontag"
        ],
        "published": "2022-10-19T17:08:13Z",
        "summary": "We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.",
        "pdf_link": "https://arxiv.org/pdf/2210.10723v2.pdf"
    },
    {
        "title": "Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages",
        "authors": [
            "Idris Abdulmumin",
            "Michael Beukman",
            "Jesujoba O. Alabi",
            "Chris Emezue",
            "Everlyn Asiko",
            "Tosin Adewumi",
            "Shamsuddeen Hassan Muhammad",
            "Mofetoluwa Adeyemi",
            "Oreen Yousuf",
            "Sahib Singh",
            "Tajuddeen Rabiu Gwadabe"
        ],
        "published": "2022-10-19T16:12:27Z",
        "summary": "We participated in the WMT 2022 Large-Scale Machine Translation Evaluation\nfor the African Languages Shared Task. This work describes our approach, which\nis based on filtering the given noisy data using a sentence-pair classifier\nthat was built by fine-tuning a pre-trained language model. To train the\nclassifier, we obtain positive samples (i.e. high-quality parallel sentences)\nfrom a gold-standard curated dataset and extract negative samples (i.e.\nlow-quality parallel sentences) from automatically aligned parallel data by\nchoosing sentences with low alignment scores. Our final machine translation\nmodel was then trained on filtered data, instead of the entire noisy dataset.\nWe empirically validate our approach by evaluating on two common datasets and\nshow that data filtering generally improves overall translation quality, in\nsome cases even significantly.",
        "pdf_link": "https://arxiv.org/pdf/2210.10692v2.pdf"
    },
    {
        "title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
        "authors": [
            "Jiuzhou Han",
            "Ehsan Shareghi"
        ],
        "published": "2022-10-19T14:44:56Z",
        "summary": "Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text\n(G2T) generation by processing the linearised version of a graph. However, the\nlinearisation is known to ignore the structural information. Additionally, PLMs\nare typically pre-trained on free text which introduces domain mismatch between\npre-training and downstream G2T generation tasks. To address these\nshortcomings, we propose graph masking pre-training strategies that neither\nrequire supervision signals nor adjust the architecture of the underlying\npre-trained encoder-decoder model. When used with a pre-trained T5, our\napproach achieves new state-of-the-art results on WebNLG+2020 and\nEventNarrative G2T generation datasets. Our method also shows to be very\neffective in the low-resource setting.",
        "pdf_link": "https://arxiv.org/pdf/2210.10599v1.pdf"
    },
    {
        "title": "DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation",
        "authors": [
            "Alessandro Palmas"
        ],
        "published": "2022-10-19T14:39:10Z",
        "summary": "The recent advances in reinforcement learning have led to effective methods\nable to obtain above human-level performances in very complex environments.\nHowever, once solved, these environments become less valuable, and new\nchallenges with different or more complex scenarios are needed to support\nresearch advances. This work presents DIAMBRA Arena, a new platform for\nreinforcement learning research and experimentation, featuring a collection of\nhigh-quality environments exposing a Python API fully compliant with OpenAI Gym\nstandard. They are episodic tasks with discrete actions and observations\ncomposed by raw pixels plus additional numerical values, all supporting both\nsingle player and two players mode, allowing to work on standard reinforcement\nlearning, competitive multi-agent, human-agent competition, self-play,\nhuman-in-the-loop training and imitation learning. Software capabilities are\ndemonstrated by successfully training multiple deep reinforcement learning\nagents with proximal policy optimization obtaining human-like behavior. Results\nconfirm the utility of DIAMBRA Arena as a reinforcement learning research tool,\nproviding environments designed to study some of the most challenging topics in\nthe field.",
        "pdf_link": "https://arxiv.org/pdf/2210.10595v1.pdf"
    },
    {
        "title": "Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing",
        "authors": [
            "Frank van der Velde"
        ],
        "published": "2022-10-19T13:31:26Z",
        "summary": "Recently, a number of articles have argued that deep learning models such as\nGPT could also capture key aspects of language processing in the human mind and\nbrain. However, I will argue that these models are not suitable as neural\nmodels of human language. Firstly, because they fail on fundamental boundary\nconditions, such as the amount of learning they require. This would in fact\nimply that the mechanisms of GPT and brain language processing are\nfundamentally different. Secondly, because they do not possess the logistics of\naccess needed for compositional and productive human language processing.\nNeural architectures could possess logistics of access based on small-world\nlike network structures, in which processing does not consist of symbol\nmanipulation but of controlling the flow of activation. In this view, two\ncomplementary approaches would be needed to investigate the relation between\nbrain and cognition. Investigating learning methods could reveal how 'learned\ncognition' as found in deep learning could develop in the brain. However,\nneural architectures with logistics of access should also be developed to\naccount for 'productive cognition' as required for natural or artificial human\nlanguage processing. Later on, these approaches could perhaps be combined to\nsee how such architectures could develop by learning and development from a\nsimpler basis.",
        "pdf_link": "https://arxiv.org/pdf/2210.10543v1.pdf"
    },
    {
        "title": "PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting",
        "authors": [
            "Thomas Lucas",
            "Fabien Baradel",
            "Philippe Weinzaepfel",
            "Gr\u00e9gory Rogez"
        ],
        "published": "2022-10-19T13:30:39Z",
        "summary": "We address the problem of action-conditioned generation of human motion\nsequences. Existing work falls into two categories: forecast models conditioned\non observed past motions, or generative models conditioned on action labels and\nduration only. In contrast, we generate motion conditioned on observations of\narbitrary length, including none. To solve this generalized problem, we propose\nPoseGPT, an auto-regressive transformer-based approach which internally\ncompresses human motion into quantized latent sequences. An auto-encoder first\nmaps human motion to latent index sequences in a discrete space, and\nvice-versa. Inspired by the Generative Pretrained Transformer (GPT), we propose\nto train a GPT-like model for next-index prediction in that space; this allows\nPoseGPT to output distributions on possible futures, with or without\nconditioning on past motion. The discrete and compressed nature of the latent\nspace allows the GPT-like model to focus on long-range signal, as it removes\nlow-level redundancy in the input signal. Predicting discrete indices also\nalleviates the common pitfall of predicting averaged poses, a typical failure\ncase when regressing continuous values, as the average of discrete targets is\nnot a target itself. Our experimental results show that our proposed approach\nachieves state-of-the-art results on HumanAct12, a standard but small scale\ndataset, as well as on BABEL, a recent large scale MoCap dataset, and on GRAB,\na human-object interactions dataset.",
        "pdf_link": "https://arxiv.org/pdf/2210.10542v1.pdf"
    },
    {
        "title": "Leveraging a New Spanish Corpus for Multilingual and Crosslingual Metaphor Detection",
        "authors": [
            "Elisa Sanchez-Bayona",
            "Rodrigo Agerri"
        ],
        "published": "2022-10-19T07:55:36Z",
        "summary": "The lack of wide coverage datasets annotated with everyday metaphorical\nexpressions for languages other than English is striking. This means that most\nresearch on supervised metaphor detection has been published only for that\nlanguage. In order to address this issue, this work presents the first corpus\nannotated with naturally occurring metaphors in Spanish large enough to develop\nsystems to perform metaphor detection. The presented dataset, CoMeta, includes\ntexts from various domains, namely, news, political discourse, Wikipedia and\nreviews. In order to label CoMeta, we apply the MIPVU method, the guidelines\nmost commonly used to systematically annotate metaphor on real data. We use our\nnewly created dataset to provide competitive baselines by fine-tuning several\nmultilingual and monolingual state-of-the-art large language models.\nFurthermore, by leveraging the existing VUAM English data in addition to\nCoMeta, we present the, to the best of our knowledge, first cross-lingual\nexperiments on supervised metaphor detection. Finally, we perform a detailed\nerror analysis that explores the seemingly high transfer of everyday metaphor\nacross these two languages and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2210.10358v2.pdf"
    },
    {
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
        "authors": [
            "Renqian Luo",
            "Liai Sun",
            "Yingce Xia",
            "Tao Qin",
            "Sheng Zhang",
            "Hoifung Poon",
            "Tie-Yan Liu"
        ],
        "published": "2022-10-19T07:17:39Z",
        "summary": "Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.",
        "pdf_link": "https://arxiv.org/pdf/2210.10341v3.pdf"
    },
    {
        "title": "Language Detoxification with Attribute-Discriminative Latent Space",
        "authors": [
            "Jin Myung Kwak",
            "Minseon Kim",
            "Sung Ju Hwang"
        ],
        "published": "2022-10-19T06:54:42Z",
        "summary": "Transformer-based Language Models (LMs) have achieved impressive results on\nnatural language understanding tasks, but they can also generate toxic text\nsuch as insults, threats, and profanity, limiting their real-world\napplications. To overcome this issue, a few text generation approaches aim to\ndetoxify toxic texts using additional LMs or perturbations. However, previous\nmethods require excessive memory, computations, and time which are serious\nbottlenecks in their real-world application. To address such limitations, we\npropose an effective yet efficient method for language detoxification using an\nattribute-discriminative latent space. Specifically, we project the latent\nspace of an original Transformer LM onto a discriminative latent space that\nwell-separates texts by their attributes using a projection block and an\nattribute discriminator. This allows the LM to control the text generation to\nbe non-toxic with minimal memory and computation overhead. We validate our\nmodel, Attribute-Discriminative Language Model (ADLM) on detoxified language\nand dialogue generation tasks, on which our method significantly outperforms\nbaselines both in performance and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2210.10329v2.pdf"
    },
    {
        "title": "A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss",
        "authors": [
            "Wenbiao Li",
            "Ziyang Wang",
            "Yunfang Wu"
        ],
        "published": "2022-10-19T05:33:27Z",
        "summary": "For readability assessment, traditional methods mainly employ machine\nlearning classifiers with hundreds of linguistic features. Although the deep\nlearning model has become the prominent approach for almost all NLP tasks, it\nis less explored for readability assessment. In this paper, we propose a\nBERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)\nfor readability assessment. Specially, we present a new difficulty knowledge\nguided semi-supervised method to extract topic features to complement the\ntraditional linguistic features. From the linguistic features, we employ\nprojection filtering to extract orthogonal features to supplement BERT\nrepresentations. Furthermore, we design a new length-balanced loss to handle\nthe greatly varying length distribution of data. Our model achieves\nstate-of-the-art performances on two English benchmark datasets and one dataset\nof Chinese textbooks, and also achieves the near-perfect accuracy of 99\\% on\none English dataset. Moreover, our proposed model obtains comparable results\nwith human experts in consistency test.",
        "pdf_link": "https://arxiv.org/pdf/2210.10305v2.pdf"
    },
    {
        "title": "Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning",
        "authors": [
            "Hongqiu Wu",
            "Ruixue Ding",
            "Hai Zhao",
            "Boli Chen",
            "Pengjun Xie",
            "Fei Huang",
            "Min Zhang"
        ],
        "published": "2022-10-19T04:38:26Z",
        "summary": "Multiple pre-training objectives fill the vacancy of the understanding\ncapability of single-objective language modeling, which serves the ultimate\npurpose of pre-trained language models (PrLMs), generalizing well on a mass of\nscenarios. However, learning multiple training objectives in a single model is\nchallenging due to the unknown relative significance as well as the potential\ncontrariety between them. Empirical studies have shown that the current\nobjective sampling in an ad-hoc manual setting makes the learned language\nrepresentation barely converge to the desired optimum. Thus, we propose\n\\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns\nthe latent sampling pattern on arbitrary pre-training objectives. Such a design\nis lightweight with negligible additional training overhead. To validate our\napproach, we adopt five objectives and conduct continual pre-training with\nBERT-base and BERT-large models, where MOMETAS demonstrates universal\nperformance gain over other rule-based sampling strategies on 14 natural\nlanguage processing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.10293v1.pdf"
    },
    {
        "title": "Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation",
        "authors": [
            "Mengting Hu",
            "Yike Wu",
            "Hang Gao",
            "Yinhao Bai",
            "Shiwan Zhao"
        ],
        "published": "2022-10-19T04:31:08Z",
        "summary": "Recently, aspect sentiment quad prediction (ASQP) has become a popular task\nin the field of aspect-level sentiment analysis. Previous work utilizes a\npredefined template to paraphrase the original sentence into a structure target\nsequence, which can be easily decoded as quadruplets of the form (aspect\ncategory, aspect term, opinion term, sentiment polarity). The template involves\nthe four elements in a fixed order. However, we observe that this solution\ncontradicts with the order-free property of the ASQP task, since there is no\nneed to fix the template order as long as the quadruplet is extracted\ncorrectly. Inspired by the observation, we study the effects of template orders\nand find that some orders help the generative model achieve better performance.\nIt is hypothesized that different orders provide various views of the\nquadruplet. Therefore, we propose a simple but effective method to identify the\nmost proper orders, and further combine multiple proper templates as data\naugmentation to improve the ASQP task. Specifically, we use the pre-trained\nlanguage model to select the orders with minimal entropy. By fine-tuning the\npre-trained language model with these template orders, our approach improves\nthe performance of quad prediction, and outperforms state-of-the-art methods\nsignificantly in low-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2210.10291v1.pdf"
    },
    {
        "title": "Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models",
        "authors": [
            "Hao Zhang"
        ],
        "published": "2022-10-19T04:28:19Z",
        "summary": "Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its\nvariants, have led to significant improvements on various NLP tasks in past\nyears. However, a theoretical framework for studying their relationships is\nstill missing. In this paper, we fill this gap by investigating the linear\ndependency between pre-trained LMs. The linear dependency of LMs is defined\nanalogously to the linear dependency of vectors. We propose Language Model\nDecomposition (LMD) to represent a LM using a linear combination of other LMs\nas basis, and derive the closed-form solution. A goodness-of-fit metric for LMD\nsimilar to the coefficient of determination is defined and used to measure the\nlinear dependency of a set of LMs. In experiments, we find that BERT and eleven\n(11) BERT-like LMs are 91% linearly dependent. This observation suggests that\ncurrent state-of-the-art (SOTA) LMs are highly \"correlated\". To further advance\nSOTA we need more diverse and novel LMs that are less dependent on existing\nLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.10289v2.pdf"
    },
    {
        "title": "Continued Pretraining for Better Zero- and Few-Shot Promptability",
        "authors": [
            "Zhaofeng Wu",
            "Robert L. Logan IV",
            "Pete Walsh",
            "Akshita Bhagia",
            "Dirk Groeneveld",
            "Sameer Singh",
            "Iz Beltagy"
        ],
        "published": "2022-10-19T02:41:51Z",
        "summary": "Recently introduced language model prompting methods can achieve high\naccuracy in zero- and few-shot settings while requiring few to no learned\ntask-specific parameters. Nevertheless, these methods still often trail behind\nfull model finetuning. In this work, we investigate if a dedicated continued\npretraining stage could improve \"promptability\", i.e., zero-shot performance\nwith natural language prompts or few-shot performance with prompt tuning. We\nreveal settings where existing continued pretraining methods lack\npromptability. We also identify current methodological gaps, which we fill with\nthorough large-scale experiments. We demonstrate that a simple recipe,\ncontinued pretraining that incorporates a trainable prompt during multi-task\nlearning, leads to improved promptability in both zero- and few-shot settings\ncompared to existing methods, up to 31% relative. On the other hand, we find\nthat continued pretraining using MAML-style meta-learning, a method that\ndirectly optimizes few-shot promptability, yields subpar performance. We\nvalidate our findings with two prompt tuning methods, and, based on our\nresults, we provide concrete recommendations to optimize promptability for\ndifferent use cases.",
        "pdf_link": "https://arxiv.org/pdf/2210.10258v2.pdf"
    },
    {
        "title": "Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction",
        "authors": [
            "Muralidhar Andoorveedu",
            "Zhanda Zhu",
            "Bojian Zheng",
            "Gennady Pekhimenko"
        ],
        "published": "2022-10-19T01:59:37Z",
        "summary": "Training deep learning models can be computationally expensive. Prior works\nhave shown that increasing the batch size can potentially lead to better\noverall throughput. However, the batch size is frequently limited by the\naccelerator memory capacity due to the activations/feature maps stored for the\ntraining backward pass, as larger batch sizes require larger feature maps to be\nstored. Transformer-based models, which have recently seen a surge in\npopularity due to their good performance and applicability to a variety of\ntasks, have a similar problem. To remedy this issue, we propose Tempo, a new\napproach to efficiently use accelerator (e.g., GPU) memory resources for\ntraining Transformer-based models. Our approach provides drop-in replacements\nfor the GELU, LayerNorm, and Attention layers, reducing the memory usage and\nultimately leading to more efficient training. We implement Tempo and evaluate\nthe throughput, memory usage, and accuracy/loss on the BERT Large pre-training\ntask. We demonstrate that Tempo enables up to 2x higher batch sizes and 16%\nhigher training throughput over the state-of-the-art baseline. We also evaluate\nTempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the\nbaseline.",
        "pdf_link": "https://arxiv.org/pdf/2210.10246v2.pdf"
    },
    {
        "title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
        "authors": [
            "Jean-Charles Layoun",
            "Alexis Roger",
            "Irina Rish"
        ],
        "published": "2022-10-18T22:20:47Z",
        "summary": "The goal of vision-language modeling is to allow models to tie language\nunderstanding with visual inputs. The aim of this paper is to evaluate and\nalign the Visual Language Model (VLM) called Multimodal Augmentation of\nGenerative Models through Adapter-based finetuning (MAGMA) with human values.\nMAGMA is a VLM that is capable of image captioning and visual\nquestion-answering. We will evaluate its alignment in three different\nscenarios. To begin, we assess MAGMA's out-of-the-box alignment through the\ncheckpoint provided by Hugging Face. Then, we measure if few-shot learning\nmanages to improve the results. Finally, we finetune the model on aligned\nexamples and evaluate its behavior.",
        "pdf_link": "https://arxiv.org/pdf/2210.14161v1.pdf"
    },
    {
        "title": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models",
        "authors": [
            "Luke Vilnis",
            "Yury Zemlyanskiy",
            "Patrick Murray",
            "Alexandre Passos",
            "Sumit Sanghai"
        ],
        "published": "2022-10-18T22:19:41Z",
        "summary": "Decoding methods for large language models often trade-off between diversity\nof outputs and parallelism of computation. Methods such as beam search and\nGumbel top-k sampling can guarantee a different output for each element of the\nbeam, but are not easy to parallelize. Alternatively, methods such as\ntemperature sampling and its modifications (top-k sampling, nucleus sampling,\ntypical decoding, and others), are embarrassingly parallel, but have no\nguarantees about duplicate samples. We present a framework for sampling\naccording to an arithmetic code book implicitly defined by a large language\nmodel, compatible with common sampling variations, with provable beam diversity\nunder certain conditions, as well as being embarrassingly parallel and\nproviding unbiased and consistent expectations from the original model. We\ndemonstrate the effectiveness of our approach on WMT machine translation, more\nthan halving the standard deviation when estimating expected BLEU score reward,\nand closing the BLEU score gap between independent sampling and beam search by\nup to 63%.",
        "pdf_link": "https://arxiv.org/pdf/2210.15458v2.pdf"
    },
    {
        "title": "JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions",
        "authors": [
            "Mo Yu",
            "Yi Gu",
            "Xiaoxiao Guo",
            "Yufei Feng",
            "Xiaodan Zhu",
            "Michael Greenspan",
            "Murray Campbell",
            "Chuang Gan"
        ],
        "published": "2022-10-18T19:20:53Z",
        "summary": "Commonsense reasoning simulates the human ability to make presumptions about\nour physical world, and it is an essential cornerstone in building general AI\nsystems. We propose a new commonsense reasoning dataset based on human's\nInteractive Fiction (IF) gameplay walkthroughs as human players demonstrate\nplentiful and diverse commonsense reasoning. The new dataset provides a natural\nmixture of various reasoning types and requires multi-hop reasoning. Moreover,\nthe IF game-based construction procedure requires much less human interventions\nthan previous ones. Different from existing benchmarks, our dataset focuses on\nthe assessment of functional commonsense knowledge rules rather than factual\nknowledge. Hence, in order to achieve higher performance on our tasks, models\nneed to effectively utilize such functional knowledge to infer the outcomes of\nactions, rather than relying solely on memorizing facts. Experiments show that\nthe introduced dataset is challenging to previous machine reading models as\nwell as the new large language models with a significant 20% performance gap\ncompared to human experts.",
        "pdf_link": "https://arxiv.org/pdf/2210.15456v2.pdf"
    },
    {
        "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models",
        "authors": [
            "Sharon Levy",
            "Emily Allaway",
            "Melanie Subbiah",
            "Lydia Chilton",
            "Desmond Patton",
            "Kathleen McKeown",
            "William Yang Wang"
        ],
        "published": "2022-10-18T17:59:31Z",
        "summary": "Understanding what constitutes safe text is an important issue in natural\nlanguage processing and can often prevent the deployment of models deemed\nharmful and unsafe. One such type of safety that has been scarcely studied is\ncommonsense physical safety, i.e. text that is not explicitly violent and\nrequires additional commonsense knowledge to comprehend that it leads to\nphysical harm. We create the first benchmark dataset, SafeText, comprising\nreal-life scenarios with paired safe and physically unsafe pieces of advice. We\nutilize SafeText to empirically study commonsense physical safety across\nvarious models designed for text generation and commonsense reasoning tasks. We\nfind that state-of-the-art large language models are susceptible to the\ngeneration of unsafe text and have difficulty rejecting unsafe advice. As a\nresult, we argue for further studies of safety and the assessment of\ncommonsense physical safety in models before release.",
        "pdf_link": "https://arxiv.org/pdf/2210.10045v1.pdf"
    },
    {
        "title": "Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning",
        "authors": [
            "Shuo Xie",
            "Jiahao Qiu",
            "Ankita Pasad",
            "Li Du",
            "Qing Qu",
            "Hongyuan Mei"
        ],
        "published": "2022-10-18T17:58:43Z",
        "summary": "While transferring a pretrained language model, common approaches\nconventionally attach their task-specific classifiers to the top layer and\nadapt all the pretrained layers. We investigate whether one could make a\ntask-specific selection on which subset of the layers to adapt and where to\nplace the classifier. The goal is to reduce the computation cost of transfer\nlearning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its\nperformance.\n  We propose to select layers based on the variability of their hidden states\ngiven a task-specific corpus. We say a layer is already \"well-specialized\" in a\ntask if the within-class variability of its hidden states is low relative to\nthe between-class variability. Our variability metric is cheap to compute and\ndoesn't need any training or hyperparameter tuning. It is robust to data\nimbalance and data scarcity. Extensive experiments on the GLUE benchmark\ndemonstrate that selecting layers based on our metric can yield significantly\nstronger performance than using the same number of top layers and often match\nthe performance of fine-tuning or adapter-tuning the entire language model.",
        "pdf_link": "https://arxiv.org/pdf/2210.10041v2.pdf"
    },
    {
        "title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks",
        "authors": [
            "Nikil Roashan Selvam",
            "Sunipa Dev",
            "Daniel Khashabi",
            "Tushar Khot",
            "Kai-Wei Chang"
        ],
        "published": "2022-10-18T17:58:39Z",
        "summary": "How reliably can we trust the scores obtained from social bias benchmarks as\nfaithful indicators of problematic social biases in a given language model? In\nthis work, we study this question by contrasting social biases with non-social\nbiases stemming from choices made during dataset construction that might not\neven be discernible to the human eye. To do so, we empirically simulate various\nalternative constructions for a given benchmark based on innocuous\nmodifications (such as paraphrasing or random-sampling) that maintain the\nessence of their social bias. On two well-known social bias benchmarks\n(Winogender and BiasNLI) we observe that these shallow modifications have a\nsurprising effect on the resulting degree of bias across various models. We\nhope these troubling observations motivate more robust measures of social\nbiases.",
        "pdf_link": "https://arxiv.org/pdf/2210.10040v2.pdf"
    },
    {
        "title": "Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters",
        "authors": [
            "Hongyu Zhao",
            "Hao Tan",
            "Hongyuan Mei"
        ],
        "published": "2022-10-18T15:20:44Z",
        "summary": "Adapter-tuning is a paradigm that transfers a pretrained language model to\ndownstream tasks by adding and tuning a small number of new parameters.\nPreviously proposed adapter architectures are all feed-forward neural networks.\nIn this paper, we investigate the effectiveness of using tiny-attention --\ni.e., attention with extremely small per-head dimensionality -- as adapters.\nOur tiny-attention adapter learns to modify the hidden states at each position\ndirectly conditioned on the hidden states at all the other positions, which is\nmissed by the previously proposed adapters. Moreover, we view its multiple\nattention heads as a mixture of experts and propose to average their weights\nduring deployment, which further reduces its inference computation cost. On the\nGLUE benchmark, our tiny-attention adapter outperforms the other\nparameter-efficient transfer learning methods as well as full fine-tuning while\nonly updating 0.05% of the parameters. On the FewGLUE benchmark, its\nperformance is comparable to that of GPT-3 and PET.",
        "pdf_link": "https://arxiv.org/pdf/2211.01979v1.pdf"
    },
    {
        "title": "Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis",
        "authors": [
            "Shuai Fan",
            "Chen Lin",
            "Haonan Li",
            "Zhenghao Lin",
            "Jinsong Su",
            "Hang Zhang",
            "Yeyun Gong",
            "Jian Guo",
            "Nan Duan"
        ],
        "published": "2022-10-18T12:25:29Z",
        "summary": "Most existing pre-trained language representation models (PLMs) are\nsub-optimal in sentiment analysis tasks, as they capture the sentiment\ninformation from word-level while under-considering sentence-level information.\nIn this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained\nlanguage model with combined Word-level and Sentence-level Pre-training tasks.\nThe word level pre-training task detects replaced sentiment words, via a\ngenerator-discriminator framework, to enhance the PLM's knowledge about\nsentiment words. The sentence level pre-training task further strengthens the\ndiscriminator via a contrastive learning framework, with similar sentences as\nnegative samples, to encode sentiments in a sentence. Extensive experimental\nresults show that SentiWSP achieves new state-of-the-art performance on various\nsentence-level and aspect-level sentiment classification benchmarks. We have\nmade our code and model publicly available at\nhttps://github.com/XMUDM/SentiWSP.",
        "pdf_link": "https://arxiv.org/pdf/2210.09803v2.pdf"
    },
    {
        "title": "Alibaba-Translate China's Submission for WMT 2022 Quality Estimation Shared Task",
        "authors": [
            "Keqin Bao",
            "Yu Wan",
            "Dayiheng Liu",
            "Baosong Yang",
            "Wenqiang Lei",
            "Xiangnan He",
            "Derek F. Wong",
            "Jun Xie"
        ],
        "published": "2022-10-18T08:55:27Z",
        "summary": "In this paper, we present our submission to the sentence-level MQM benchmark\nat Quality Estimation Shared Task, named UniTE (Unified Translation\nEvaluation). Specifically, our systems employ the framework of UniTE, which\ncombined three types of input formats during training with a pre-trained\nlanguage model. First, we apply the pseudo-labeled data examples for the\ncontinuously pre-training phase. Notably, to reduce the gap between\npre-training and fine-tuning, we use data pruning and a ranking-based score\nnormalization strategy. For the fine-tuning phase, we use both Direct\nAssessment (DA) and Multidimensional Quality Metrics (MQM) data from past\nyears' WMT competitions. Finally, we collect the source-only evaluation\nresults, and ensemble the predictions generated by two UniTE models, whose\nbackbones are XLM-R and InfoXLM, respectively. Results show that our models\nreach 1st overall ranking in the Multilingual and English-Russian settings, and\n2nd overall ranking in English-German and Chinese-English settings, showing\nrelatively strong performances in this year's quality estimation competition.",
        "pdf_link": "https://arxiv.org/pdf/2210.10049v2.pdf"
    },
    {
        "title": "Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task",
        "authors": [
            "Yu Wan",
            "Keqin Bao",
            "Dayiheng Liu",
            "Baosong Yang",
            "Derek F. Wong",
            "Lidia S. Chao",
            "Wenqiang Lei",
            "Jun Xie"
        ],
        "published": "2022-10-18T08:51:25Z",
        "summary": "In this report, we present our submission to the WMT 2022 Metrics Shared\nTask. We build our system based on the core idea of UNITE (Unified Translation\nEvaluation), which unifies source-only, reference-only, and\nsource-reference-combined evaluation scenarios into one single model.\nSpecifically, during the model pre-training phase, we first apply the\npseudo-labeled data examples to continuously pre-train UNITE. Notably, to\nreduce the gap between pre-training and fine-tuning, we use data cropping and a\nranking-based score normalization strategy. During the fine-tuning phase, we\nuse both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data\nfrom past years' WMT competitions. Specially, we collect the results from\nmodels with different pre-trained language model backbones, and use different\nensembling strategies for involved translation directions.",
        "pdf_link": "https://arxiv.org/pdf/2210.09683v2.pdf"
    },
    {
        "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
        "authors": [
            "Lan Jiang",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Rui Jiang"
        ],
        "published": "2022-10-18T07:53:15Z",
        "summary": "Even though the large-scale language models have achieved excellent\nperformances, they suffer from various adversarial attacks. A large body of\ndefense methods has been proposed. However, they are still limited due to\nredundant attack search spaces and the inability to defend against various\ntypes of attacks. In this work, we present a novel fine-tuning approach called\n\\textbf{RO}bust \\textbf{SE}letive fine-tuning (\\textbf{ROSE}) to address this\nissue. ROSE conducts selective updates when adapting pre-trained models to\ndownstream tasks, filtering out invaluable and unrobust updates of parameters.\nSpecifically, we propose two strategies: the first-order and second-order ROSE\nfor selecting target robust parameters. The experimental results show that ROSE\nachieves significant improvements in adversarial robustness on various\ndownstream NLP tasks, and the ensemble method even surpasses both variants\nabove. Furthermore, ROSE can be easily incorporated into existing fine-tuning\nmethods to improve their adversarial robustness further. The empirical analysis\nconfirms that ROSE eliminates unrobust spurious updates during fine-tuning,\nleading to solutions corresponding to flatter and wider optima than the\nconventional method. Code is available at\n\\url{https://github.com/jiangllan/ROSE}.",
        "pdf_link": "https://arxiv.org/pdf/2210.09658v1.pdf"
    },
    {
        "title": "Planning for Sample Efficient Imitation Learning",
        "authors": [
            "Zhao-Heng Yin",
            "Weirui Ye",
            "Qifeng Chen",
            "Yang Gao"
        ],
        "published": "2022-10-18T05:19:26Z",
        "summary": "Imitation learning is a class of promising policy learning algorithms that is\nfree from many practical issues with reinforcement learning, such as the reward\ndesign issue and the exploration hardness. However, the current imitation\nalgorithm struggles to achieve both high performance and high in-environment\nsample efficiency simultaneously. Behavioral Cloning (BC) does not need\nin-environment interactions, but it suffers from the covariate shift problem\nwhich harms its performance. Adversarial Imitation Learning (AIL) turns\nimitation learning into a distribution matching problem. It can achieve better\nperformance on some tasks but it requires a large number of in-environment\ninteractions. Inspired by the recent success of EfficientZero in RL, we propose\nEfficientImitate (EI), a planning-based imitation learning method that can\nachieve high in-environment sample efficiency and performance simultaneously.\nOur algorithmic contribution in this paper is two-fold. First, we extend AIL\ninto the MCTS-based RL. Second, we show the seemingly incompatible two classes\nof imitation algorithms (BC and AIL) can be naturally unified under our\nframework, enjoying the benefits of both. We benchmark our method not only on\nthe state-based DeepMind Control Suite, but also on the image version which\nmany previous works find highly challenging. Experimental results show that EI\nachieves state-of-the-art results in performance and sample efficiency. EI\nshows over 4x gain in performance in the limited sample setting on state-based\nand image-based tasks and can solve challenging problems like Humanoid, where\nprevious methods fail with small amount of interactions. Our code is available\nat https://github.com/zhaohengyin/EfficientImitate.",
        "pdf_link": "https://arxiv.org/pdf/2210.09598v1.pdf"
    },
    {
        "title": "Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation",
        "authors": [
            "Ruijun Li",
            "Weihua Li",
            "Yi Yang",
            "Hanyu Wei",
            "Jianhua Jiang",
            "Quan Bai"
        ],
        "published": "2022-10-18T02:50:34Z",
        "summary": "Recently, diffusion models have been proven to perform remarkably well in\ntext-to-image synthesis tasks in a number of studies, immediately presenting\nnew study opportunities for image generation. Google's Imagen follows this\nresearch trend and outperforms DALLE2 as the best model for text-to-image\ngeneration. However, Imagen merely uses a T5 language model for text\nprocessing, which cannot ensure learning the semantic information of the text.\nFurthermore, the Efficient UNet leveraged by Imagen is not the best choice in\nimage processing. To address these issues, we propose the Swinv2-Imagen, a\nnovel text-to-image diffusion model based on a Hierarchical Visual Transformer\nand a Scene Graph incorporating a semantic layout. In the proposed model, the\nfeature vectors of entities and relationships are extracted and involved in the\ndiffusion model, effectively improving the quality of generated images. On top\nof that, we also introduce a Swin-Transformer-based UNet architecture, called\nSwinv2-Unet, which can address the problems stemming from the CNN convolution\noperations. Extensive experiments are conducted to evaluate the performance of\nthe proposed model by using three real-world datasets, i.e., MSCOCO, CUB and\nMM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen\nmodel outperforms several popular state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.09549v1.pdf"
    },
    {
        "title": "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models",
        "authors": [
            "Zhiyuan Zhang",
            "Lingjuan Lyu",
            "Xingjun Ma",
            "Chenguang Wang",
            "Xu Sun"
        ],
        "published": "2022-10-18T02:44:38Z",
        "summary": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.\nIn Natural Language Processing (NLP), DNNs are often backdoored during the\nfine-tuning process of a large-scale Pre-trained Language Model (PLM) with\npoisoned samples. Although the clean weights of PLMs are readily available,\nexisting methods have ignored this information in defending NLP models against\nbackdoor attacks. In this work, we take the first step to exploit the\npre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language\nmodels. Specifically, we leverage the clean pre-trained weights via two\ncomplementary techniques: (1) a two-step Fine-mixing technique, which first\nmixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained\nweights, then fine-tunes the mixed weights on a small subset of clean data; (2)\nan Embedding Purification (E-PUR) technique, which mitigates potential\nbackdoors existing in the word embeddings. We compare Fine-mixing with typical\nbackdoor mitigation methods on three single-sentence sentiment classification\ntasks and two sentence-pair classification tasks and show that it outperforms\nthe baselines by a considerable margin in all scenarios. We also show that our\nE-PUR method can benefit existing mitigation methods. Our work establishes a\nsimple but strong baseline defense for secure fine-tuned NLP models against\nbackdoor attacks.",
        "pdf_link": "https://arxiv.org/pdf/2210.09545v1.pdf"
    },
    {
        "title": "Team Flow at DRC2022: Pipeline System for Travel Destination Recommendation Task in Spoken Dialogue",
        "authors": [
            "Ryu Hirai",
            "Atsumoto Ohashi",
            "Ao Guo",
            "Hideki Shiroma",
            "Xulin Zhou",
            "Yukihiko Tone",
            "Shinya Iizuka",
            "Ryuichiro Higashinaka"
        ],
        "published": "2022-10-18T01:11:16Z",
        "summary": "To improve the interactive capabilities of a dialogue system, e.g., to adapt\nto different customers, the Dialogue Robot Competition (DRC2022) was held. As\none of the teams, we built a dialogue system with a pipeline structure\ncontaining four modules. The natural language understanding (NLU) and natural\nlanguage generation (NLG) modules were GPT-2 based models, and the dialogue\nstate tracking (DST) and policy modules were designed on the basis of\nhand-crafted rules. After the preliminary round of the competition, we found\nthat the low variation in training examples for the NLU and failed\nrecommendation due to the policy used were probably the main reasons for the\nlimited performance of the system.",
        "pdf_link": "https://arxiv.org/pdf/2210.09518v1.pdf"
    },
    {
        "title": "Systematicity in GPT-3's Interpretation of Novel English Noun Compounds",
        "authors": [
            "Siyan Li",
            "Riley Carlson",
            "Christopher Potts"
        ],
        "published": "2022-10-18T00:25:24Z",
        "summary": "Levin et al. (2019) show experimentally that the interpretations of novel\nEnglish noun compounds (e.g., stew skillet), while not fully compositional, are\nhighly predictable based on whether the modifier and head refer to artifacts or\nnatural kinds. Is the large language model GPT-3 governed by the same\ninterpretive principles? To address this question, we first compare Levin et\nal.'s experimental data with GPT-3 generations, finding a high degree of\nsimilarity. However, this evidence is consistent with GPT3 reasoning only about\nspecific lexical items rather than the more abstract conceptual categories of\nLevin et al.'s theory. To probe more deeply, we construct prompts that require\nthe relevant kind of conceptual reasoning. Here, we fail to find convincing\nevidence that GPT-3 is reasoning about more than just individual lexical items.\nThese results highlight the importance of controlling for low-level\ndistributional regularities when assessing whether a large language model\nlatently encodes a deeper theory.",
        "pdf_link": "https://arxiv.org/pdf/2210.09492v1.pdf"
    },
    {
        "title": "Adversarial and Safely Scaled Question Generation",
        "authors": [
            "Sreehari Sankar",
            "Zhihang Dong"
        ],
        "published": "2022-10-17T22:51:45Z",
        "summary": "Question generation has recently gained a lot of research interest,\nespecially with the advent of large language models. In and of itself, question\ngeneration can be considered 'AI-hard', as there is a lack of unanimously\nagreed sense of what makes a question 'good' or 'bad'. In this paper, we tackle\ntwo fundamental problems in parallel: on one hand, we try to solve the scaling\nproblem, where question-generation and answering applications have to be\napplied to a massive amount of text without ground truth labeling. The usual\napproach to solve this problem is to either downsample or summarize. However,\nthere are critical risks of misinformation with these approaches. On the other\nhand, and related to the misinformation problem, we try to solve the 'safety'\nproblem, as many public institutions rely on a much higher level of accuracy\nfor the content they provide. We introduce an adversarial approach to tackle\nthe question generation safety problem with scale. Specifically, we designed a\nquestion-answering system that specifically prunes out unanswerable questions\nthat may be generated, and further increases the quality of the answers that\nare generated. We build a production-ready, easily-plugged pipeline that can be\nused on any given body of text, that is scalable and immune from generating any\nhate speech, profanity, or misinformation. Based on the results, we are able to\ngenerate more than six times the number of quality questions generated by the\nabstractive approach, with a perceived quality being 44% higher, according to a\nsurvey of 168 participants.",
        "pdf_link": "https://arxiv.org/pdf/2210.09467v1.pdf"
    },
    {
        "title": "Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints",
        "authors": [
            "Omid Rohanian",
            "Hannah Jauncey",
            "Mohammadmahdi Nouriborji",
            "Vinod Kumar Chauhan",
            "Bronner P. Gon\u00e7alves",
            "Christiana Kartsonaki",
            "ISARIC Clinical Characterisation Group",
            "Laura Merson",
            "David Clifton"
        ],
        "published": "2022-10-17T21:22:23Z",
        "summary": "Processing information locked within clinical health records is a challenging\ntask that remains an active area of research in biomedical NLP. In this work,\nwe evaluate a broad set of machine learning techniques ranging from simple RNNs\nto specialised transformers such as BioBERT on a dataset containing clinical\nnotes along with a set of annotations indicating whether a sample is\ncancer-related or not.\n  Furthermore, we specifically employ efficient fine-tuning methods from NLP,\nnamely, bottleneck adapters and prompt tuning, to adapt the models to our\nspecialised task. Our evaluations suggest that fine-tuning a frozen BERT model\npre-trained on natural language and with bottleneck adapters outperforms all\nother strategies, including full fine-tuning of the specialised BioBERT model.\nBased on our findings, we suggest that using bottleneck adapters in\nlow-resource situations with limited access to labelled data or processing\ncapacity could be a viable strategy in biomedical text mining. The code used in\nthe experiments are going to be made available at\nhttps://github.com/omidrohanian/bottleneck-adapters.",
        "pdf_link": "https://arxiv.org/pdf/2210.09440v2.pdf"
    },
    {
        "title": "CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model",
        "authors": [
            "Natasha Alkhatib",
            "Maria Mushtaq",
            "Hadi Ghauch",
            "Jean-Luc Danger"
        ],
        "published": "2022-10-17T21:21:37Z",
        "summary": "Due to the rising number of sophisticated customer functionalities,\nelectronic control units (ECUs) are increasingly integrated into modern\nautomotive systems. However, the high connectivity between the in-vehicle and\nthe external networks paves the way for hackers who could exploit in-vehicle\nnetwork protocols' vulnerabilities. Among these protocols, the Controller Area\nNetwork (CAN), known as the most widely used in-vehicle networking technology,\nlacks encryption and authentication mechanisms, making the communications\ndelivered by distributed ECUs insecure. Inspired by the outstanding performance\nof bidirectional encoder representations from transformers (BERT) for improving\nmany natural language processing tasks, we propose in this paper ``CAN-BERT\", a\ndeep learning based network intrusion detection system, to detect cyber attacks\non CAN bus protocol. We show that the BERT model can learn the sequence of\narbitration identifiers (IDs) in the CAN bus for anomaly detection using the\n``masked language model\" unsupervised training objective. The experimental\nresults on the ``Car Hacking: Attack \\& Defense Challenge 2020\" dataset show\nthat ``CAN-BERT\" outperforms state-of-the-art approaches. In addition to being\nable to identify in-vehicle intrusions in real-time within 0.8 ms to 3 ms w.r.t\nCAN ID sequence length, it can also detect a wide variety of cyberattacks with\nan F1-score of between 0.81 and 0.99.",
        "pdf_link": "https://arxiv.org/pdf/2210.09439v1.pdf"
    },
    {
        "title": "Sufficient Exploration for Convex Q-learning",
        "authors": [
            "Fan Lu",
            "Prashant Mehta",
            "Sean Meyn",
            "Gergely Neu"
        ],
        "published": "2022-10-17T20:22:12Z",
        "summary": "In recent years there has been a collective research effort to find new\nformulations of reinforcement learning that are simultaneously more efficient\nand more amenable to analysis. This paper concerns one approach that builds on\nthe linear programming (LP) formulation of optimal control of Manne. A primal\nversion is called logistic Q-learning, and a dual variant is convex Q-learning.\nThis paper focuses on the latter, while building bridges with the former. The\nmain contributions follow: (i) The dual of convex Q-learning is not precisely\nManne's LP or a version of logistic Q-learning, but has similar structure that\nreveals the need for regularization to avoid over-fitting. (ii) A sufficient\ncondition is obtained for a bounded solution to the Q-learning LP. (iii)\nSimulation studies reveal numerical challenges when addressing sampled-data\nsystems based on a continuous time model. The challenge is addressed using\nstate-dependent sampling. The theory is illustrated with applications to\nexamples from OpenAI gym. It is shown that convex Q-learning is successful in\ncases where standard Q-learning diverges, such as the LQR problem.",
        "pdf_link": "https://arxiv.org/pdf/2210.09409v1.pdf"
    },
    {
        "title": "Deep Bidirectional Language-Knowledge Graph Pretraining",
        "authors": [
            "Michihiro Yasunaga",
            "Antoine Bosselut",
            "Hongyu Ren",
            "Xikun Zhang",
            "Christopher D Manning",
            "Percy Liang",
            "Jure Leskovec"
        ],
        "published": "2022-10-17T18:02:52Z",
        "summary": "Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.",
        "pdf_link": "https://arxiv.org/pdf/2210.09338v2.pdf"
    },
    {
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
        "authors": [
            "Mirac Suzgun",
            "Nathan Scales",
            "Nathanael Sch\u00e4rli",
            "Sebastian Gehrmann",
            "Yi Tay",
            "Hyung Won Chung",
            "Aakanksha Chowdhery",
            "Quoc V. Le",
            "Ed H. Chi",
            "Denny Zhou",
            "Jason Wei"
        ],
        "published": "2022-10-17T17:08:26Z",
        "summary": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\nfocuses on tasks believed to be beyond the capabilities of current language\nmodels. Language models have already made good progress on this benchmark, with\nthe best model in the BIG-Bench paper outperforming average reported\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\non what tasks do language models fall short of average human-rater performance,\nand are those tasks actually unsolvable by current language models?\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\nevaluations did not outperform the average human-rater. We find that applying\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\naverage human-rater performance on 10 of the 23 tasks, and Codex\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\n2022), substantially underestimates the best performance and capabilities of\nlanguage models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH,\nfinding that CoT enables emergent task performance on several BBH tasks with\notherwise flat scaling curves.",
        "pdf_link": "https://arxiv.org/pdf/2210.09261v1.pdf"
    },
    {
        "title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization",
        "authors": [
            "Yuxian Gu",
            "Pei Ke",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022-10-17T15:25:24Z",
        "summary": "Training language models to learn from human instructions for zero-shot\ncross-task generalization has attracted much attention in NLP communities.\nRecently, instruction tuning (IT), which fine-tunes a pre-trained language\nmodel on a massive collection of tasks described via human-craft instructions,\nhas been shown effective in instruction learning for unseen tasks. However, IT\nrelies on a large amount of human-annotated samples, which restricts its\ngeneralization. Unlike labeled data, unlabeled data are often massive and cheap\nto obtain. In this work, we study how IT can be improved with unlabeled data.\nWe first empirically explore the IT performance trends versus the number of\nlabeled data, instructions, and training tasks. We find it critical to enlarge\nthe number of training instructions, and the instructions can be underutilized\ndue to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented\nInstruction Tuning (UDIT) to take better advantage of the instructions during\nIT by constructing pseudo-labeled data from unlabeled plain texts. We conduct\nextensive experiments to show UDIT's effectiveness in various scenarios of\ntasks and datasets. We also comprehensively analyze the key factors of UDIT to\ninvestigate how to better improve IT with unlabeled data. The code is publicly\navailable at https://github.com/thu-coai/UDIT.",
        "pdf_link": "https://arxiv.org/pdf/2210.09175v1.pdf"
    },
    {
        "title": "Prompting GPT-3 To Be Reliable",
        "authors": [
            "Chenglei Si",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Shuohang Wang",
            "Jianfeng Wang",
            "Jordan Boyd-Graber",
            "Lijuan Wang"
        ],
        "published": "2022-10-17T14:52:39Z",
        "summary": "Large language models (LLMs) show impressive abilities via few-shot\nprompting. Commercialized APIs such as OpenAI GPT-3 further increase their use\nin real-world language applications. However, the crucial problem of how to\nimprove the reliability of GPT-3 is still under-explored. While reliability is\na broad and vaguely defined term, we decompose reliability into four main\nfacets that correspond to the existing framework of ML safety and are\nwell-recognized to be important: generalizability, social biases, calibration,\nand factuality. Our core contribution is to establish simple and effective\nprompts that improve GPT-3's reliability as it: 1) generalizes\nout-of-distribution, 2) balances demographic distribution and uses natural\nlanguage instructions to reduce social biases, 3) calibrates output\nprobabilities, and 4) updates the LLM's factual knowledge and reasoning chains.\nWith appropriate prompts, GPT-3 is more reliable than smaller-scale supervised\nmodels on all these facets. We release all processed datasets, evaluation\nscripts, and model predictions. Our systematic empirical study not only sheds\nnew insights on the reliability of prompting LLMs, but more importantly, our\nprompting strategies can help practitioners more reliably use LLMs like GPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2210.09150v2.pdf"
    },
    {
        "title": "Exposing Influence Campaigns in the Age of LLMs: A Behavioral-Based AI Approach to Detecting State-Sponsored Trolls",
        "authors": [
            "Fatima Ezzeddine",
            "Luca Luceri",
            "Omran Ayoub",
            "Ihab Sbeity",
            "Gianluca Nogara",
            "Emilio Ferrara",
            "Silvia Giordano"
        ],
        "published": "2022-10-17T07:01:17Z",
        "summary": "The detection of state-sponsored trolls operating in influence campaigns on\nsocial media is a critical and unsolved challenge for the research community,\nwhich has significant implications beyond the online realm. To address this\nchallenge, we propose a new AI-based solution that identifies troll accounts\nsolely through behavioral cues associated with their sequences of sharing\nactivity, encompassing both their actions and the feedback they receive from\nothers. Our approach does not incorporate any textual content shared and\nconsists of two steps: First, we leverage an LSTM-based classifier to determine\nwhether account sequences belong to a state-sponsored troll or an organic,\nlegitimate user. Second, we employ the classified sequences to calculate a\nmetric named the \"Troll Score\", quantifying the degree to which an account\nexhibits troll-like behavior. To assess the effectiveness of our method, we\nexamine its performance in the context of the 2016 Russian interference\ncampaign during the U.S. Presidential election. Our experiments yield\ncompelling results, demonstrating that our approach can identify account\nsequences with an AUC close to 99% and accurately differentiate between Russian\ntrolls and organic users with an AUC of 91%. Notably, our behavioral-based\napproach holds a significant advantage in the ever-evolving landscape, where\ntextual and linguistic properties can be easily mimicked by Large Language\nModels (LLMs): In contrast to existing language-based techniques, it relies on\nmore challenging-to-replicate behavioral cues, ensuring greater resilience in\nidentifying influence campaigns, especially given the potential increase in the\nusage of LLMs for generating inauthentic content. Finally, we assessed the\ngeneralizability of our solution to various entities driving different\ninformation operations and found promising results that will guide future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2210.08786v6.pdf"
    },
    {
        "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
        "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Y. Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
        ],
        "published": "2022-10-17T03:44:30Z",
        "summary": "Language models (LMs) now excel at many tasks such as few-shot learning,\nquestion answering, reasoning, and dialog. However, they sometimes generate\nunsupported or misleading content. A user cannot easily determine whether their\noutputs are trustworthy or not, because most LMs do not have any built-in\nmechanism for attribution to external evidence. To enable attribution while\nstill preserving all the powerful advantages of recent generation models, we\npropose RARR (Retrofit Attribution using Research and Revision), a system that\n1) automatically finds attribution for the output of any text generation model\nand 2) post-edits the output to fix unsupported content while preserving the\noriginal output as much as possible. When applied to the output of several\nstate-of-the-art LMs on a diverse set of generation tasks, we find that RARR\nsignificantly improves attribution while otherwise preserving the original\ninput to a much greater degree than previously explored edit models.\nFurthermore, the implementation of RARR requires only a handful of training\nexamples, a large language model, and standard web search.",
        "pdf_link": "https://arxiv.org/pdf/2210.08726v3.pdf"
    },
    {
        "title": "Continuous Pseudo-Labeling from the Start",
        "authors": [
            "Dan Berrebbi",
            "Ronan Collobert",
            "Samy Bengio",
            "Navdeep Jaitly",
            "Tatiana Likhomanenko"
        ],
        "published": "2022-10-17T03:04:06Z",
        "summary": "Self-training (ST), or pseudo-labeling has sparked significant interest in\nthe automatic speech recognition (ASR) community recently because of its\nsuccess in harnessing unlabeled data. Unlike prior semi-supervised learning\napproaches that relied on iteratively regenerating pseudo-labels (PLs) from a\ntrained model and using them to train a new model, recent state-of-the-art\nmethods perform `continuous training' where PLs are generated using a very\nrecent version of the model being trained. Nevertheless, these approaches still\nrely on bootstrapping the ST using an initial supervised learning phase where\nthe model is trained on labeled data alone. We believe this has the potential\nfor over-fitting to the labeled dataset in low resource settings and that ST\nfrom the start of training should reduce over-fitting. In this paper we show\nhow we can do this by dynamically controlling the evolution of PLs during the\ntraining process in ASR. To the best of our knowledge, this is the first study\nthat shows the feasibility of generating PLs from the very start of the\ntraining. We are able to achieve this using two techniques that avoid\ninstabilities which lead to degenerate models that do not generalize. Firstly,\nwe control the evolution of PLs through a curriculum that uses the online\nchanges in PLs to control the membership of the cache of PLs and improve\ngeneralization. Secondly, we find that by sampling transcriptions from the\npredictive distribution, rather than only using the best transcription, we can\nstabilize training further. With these techniques, our ST models match prior\nworks without an external language model.",
        "pdf_link": "https://arxiv.org/pdf/2210.08711v2.pdf"
    },
    {
        "title": "A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems",
        "authors": [
            "Hong Liu",
            "Yucheng Cai",
            "Zhijian Ou",
            "Yi Huang",
            "Junlan Feng"
        ],
        "published": "2022-10-17T01:57:50Z",
        "summary": "Building user simulators (USs) for reinforcement learning (RL) of\ntask-oriented dialog systems (DSs) has gained more and more attention, which,\nhowever, still faces several fundamental challenges. First, it is unclear\nwhether we can leverage pretrained language models to design, for example,\nGPT-2 based USs, to catch up and interact with the recently advanced GPT-2\nbased DSs. Second, an important ingredient in a US is that the user goal can be\neffectively incorporated and tracked; but how to flexibly integrate goal state\ntracking and develop an end-to-end trainable US for multi-domains has remained\nto be a challenge. In this work, we propose a generative user simulator (GUS)\nwith GPT-2 based architecture and goal state tracking towards addressing the\nabove two challenges. Extensive experiments are conducted on MultiWOZ2.1.\nDifferent DSs are trained via RL with GUS, the classic agenda-based user\nsimulator (ABUS) and other ablation simulators respectively, and are compared\nfor cross-model evaluation, corpus-based evaluation and human evaluation. The\nGUS achieves superior results in all three evaluation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.08692v2.pdf"
    },
    {
        "title": "SGRAM: Improving Scene Graph Parsing via Abstract Meaning Representation",
        "authors": [
            "Woo Suk Choi",
            "Yu-Jung Heo",
            "Byoung-Tak Zhang"
        ],
        "published": "2022-10-17T00:37:00Z",
        "summary": "Scene graph is structured semantic representation that can be modeled as a\nform of graph from images and texts. Image-based scene graph generation\nresearch has been actively conducted until recently, whereas text-based scene\ngraph generation research has not. In this paper, we focus on the problem of\nscene graph parsing from textual description of a visual scene. The core idea\nis to use abstract meaning representation (AMR) instead of the dependency\nparsing mainly used in previous studies. AMR is a graph-based semantic\nformalism of natural language which abstracts concepts of words in a sentence\ncontrary to the dependency parsing which considers dependency relationships on\nall words in a sentence. To this end, we design a simple yet effective\ntwo-stage scene graph parsing framework utilizing abstract meaning\nrepresentation, SGRAM (Scene GRaph parsing via Abstract Meaning\nrepresentation): 1) transforming a textual description of an image into an AMR\ngraph (Text-to-AMR) and 2) encoding the AMR graph into a Transformer-based\nlanguage model to generate a scene graph (AMR-to-SG). Experimental results show\nthe scene graphs generated by our framework outperforms the dependency\nparsing-based model by 11.61\\% and the previous state-of-the-art model using a\npre-trained Transformer language model by 3.78\\%. Furthermore, we apply SGRAM\nto image retrieval task which is one of downstream tasks for scene graph, and\nconfirm the effectiveness of scene graphs generated by our framework.",
        "pdf_link": "https://arxiv.org/pdf/2210.08675v1.pdf"
    },
    {
        "title": "NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly",
        "authors": [
            "Yi R. Fung",
            "Tuhin Chakraborty",
            "Hao Guo",
            "Owen Rambow",
            "Smaranda Muresan",
            "Heng Ji"
        ],
        "published": "2022-10-16T18:30:05Z",
        "summary": "Norm discovery is important for understanding and reasoning about the\nacceptable behaviors and potential violations in human communication and\ninteractions. We introduce NormSage, a framework for addressing the novel task\nof conversation-grounded multi-lingual, multi-cultural norm discovery, based on\nlanguage model prompting and self-verification. NormSAGE leverages the\nexpressiveness and implicit knowledge of the pretrained GPT-3 language model\nbackbone, to elicit knowledge about norms through directed questions\nrepresenting the norm discovery task and conversation context. It further\naddresses the risk of language model hallucination with a self-verification\nmechanism ensuring that the norms discovered are correct and are substantially\ngrounded to their source conversations. Evaluation results show that our\napproach discovers significantly more relevant and insightful norms for\nconversations on-the-fly compared to baselines (>10+% in Likert scale rating).\nThe norms discovered from Chinese conversation are also comparable to the norms\ndiscovered from English conversation in terms of insightfulness and correctness\n(<3% difference). In addition, the culture-specific norms are promising\nquality, allowing for 80% accuracy in culture pair human identification.\nFinally, our grounding process in norm discovery self-verification can be\nextended for instantiating the adherence and violation of any norm for a given\nconversation on-the-fly, with explainability and transparency. NormSAGE\nachieves an AUC of 95.4% in grounding, with natural language explanation\nmatching human-written quality.",
        "pdf_link": "https://arxiv.org/pdf/2210.08604v2.pdf"
    },
    {
        "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding",
        "authors": [
            "Jianing Wang",
            "Wenkang Huang",
            "Qiuhui Shi",
            "Hongbin Wang",
            "Minghui Qiu",
            "Xiang Li",
            "Ming Gao"
        ],
        "published": "2022-10-16T13:36:57Z",
        "summary": "Knowledge-enhanced Pre-trained Language Model (PLM) has recently received\nsignificant attention, which aims to incorporate factual knowledge into PLMs.\nHowever, most existing methods modify the internal structures of fixed types of\nPLMs by stacking complicated modules, and introduce redundant and irrelevant\nfactual knowledge from knowledge bases (KBs). In this paper, to address these\nproblems, we introduce a seminal knowledge prompting paradigm and further\npropose a knowledge-prompting-based PLM framework KP-PLM. This framework can be\nflexibly combined with existing mainstream PLMs. Specifically, we first\nconstruct a knowledge sub-graph from KBs for each context. Then we design\nmultiple continuous prompts rules and transform the knowledge sub-graph into\nnatural language prompts. To further leverage the factual knowledge from these\nprompts, we propose two novel knowledge-aware self-supervised tasks including\nprompt relevance inspection and masked prompt modeling. Extensive experiments\non multiple natural language understanding (NLU) tasks show the superiority of\nKP-PLM over other state-of-the-art methods in both full-resource and\nlow-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2210.08536v1.pdf"
    },
    {
        "title": "Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion",
        "authors": [
            "Jian Song",
            "Di Liang",
            "Rumei Li",
            "Yuntao Li",
            "Sirui Wang",
            "Minlong Peng",
            "Wei Wu",
            "Yongxin Yu"
        ],
        "published": "2022-10-16T07:17:27Z",
        "summary": "Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.",
        "pdf_link": "https://arxiv.org/pdf/2210.08471v5.pdf"
    },
    {
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
        "authors": [
            "Christoph Schuhmann",
            "Romain Beaumont",
            "Richard Vencu",
            "Cade Gordon",
            "Ross Wightman",
            "Mehdi Cherti",
            "Theo Coombes",
            "Aarush Katta",
            "Clayton Mullis",
            "Mitchell Wortsman",
            "Patrick Schramowski",
            "Srivatsa Kundurthy",
            "Katherine Crowson",
            "Ludwig Schmidt",
            "Robert Kaczmarczyk",
            "Jenia Jitsev"
        ],
        "published": "2022-10-16T00:08:18Z",
        "summary": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the\nutility of training on large amounts of noisy image-text data, without relying\non expensive accurate labels used in standard vision unimodal supervised\nlearning. The resulting models showed capabilities of strong text-guided image\ngeneration and transfer to downstream tasks, while performing remarkably at\nzero-shot classification with noteworthy out-of-distribution robustness. Since\nthen, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and\nImagen made further improvements. Studying the training and capabilities of\nsuch models requires datasets containing billions of image-text pairs. Until\nnow, no datasets of this size have been made openly available for the broader\nresearch community. To address this problem and democratize research on\nlarge-scale multi-modal models, we present LAION-5B - a dataset consisting of\n5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English\nlanguage. We show successful replication and fine-tuning of foundational models\nlike CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further\nexperiments enabled with an openly available dataset of this scale.\nAdditionally we provide several nearest neighbor indices, an improved\nweb-interface for dataset exploration and subset generation, and detection\nscores for watermark, NSFW, and toxic content detection. Announcement page\nhttps://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/",
        "pdf_link": "https://arxiv.org/pdf/2210.08402v1.pdf"
    },
    {
        "title": "Construction Repetition Reduces Information Rate in Dialogue",
        "authors": [
            "Mario Giulianelli",
            "Arabella Sinclair",
            "Raquel Fern\u00e1ndez"
        ],
        "published": "2022-10-15T15:44:00Z",
        "summary": "Speakers repeat constructions frequently in dialogue. Due to their peculiar\ninformation-theoretic properties, repetitions can be thought of as a strategy\nfor cost-effective communication. In this study, we focus on the repetition of\nlexicalised constructions -- i.e., recurring multi-word units -- in English\nopen-domain spoken dialogues. We hypothesise that speakers use construction\nrepetition to mitigate information rate, leading to an overall decrease in\nutterance information content over the course of a dialogue. We conduct a\nquantitative analysis, measuring the information content of constructions and\nthat of their containing utterances, estimating information content with an\nadaptive neural language model. We observe that construction usage lowers the\ninformation content of utterances. This facilitating effect (i) increases\nthroughout dialogues, (ii) is boosted by repetition, (iii) grows as a function\nof repetition frequency and density, and (iv) is stronger for repetitions of\nreferential constructions.",
        "pdf_link": "https://arxiv.org/pdf/2210.08321v1.pdf"
    },
    {
        "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text",
        "authors": [
            "Muhammad AL-Qurishi",
            "Sarah AlQaseemi",
            "Riad Soussi"
        ],
        "published": "2022-10-15T13:08:40Z",
        "summary": "The effectiveness of the BERT model on multiple linguistic tasks has been\nwell documented. On the other hand, its potentials for narrow and specific\ndomains such as Legal, have not been fully explored. In this paper, we examine\nhow BERT can be used in the Arabic legal domain and try customizing this\nlanguage model for several downstream tasks using several different\ndomain-relevant training and testing datasets to train BERT from scratch. We\nintroduce the AraLegal-BERT, a bidirectional encoder Transformer-based model\nthat have been thoroughly tested and carefully optimized with the goal to\namplify the impact of NLP-driven solution concerning jurisprudence, legal\ndocuments, and legal practice. We fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variations for Arabic language in three natural languages\nunderstanding (NLU) tasks. The results show that the base version of\nAraLegal-BERT achieve better accuracy than the general and original BERT over\nthe Legal text.",
        "pdf_link": "https://arxiv.org/pdf/2210.08284v1.pdf"
    },
    {
        "title": "Large Language Models for Multi-label Propaganda Detection",
        "authors": [
            "Tanmay Chavan",
            "Aditya Kane"
        ],
        "published": "2022-10-15T06:47:31Z",
        "summary": "The spread of propaganda through the internet has increased drastically over\nthe past years. Lately, propaganda detection has started gaining importance\nbecause of the negative impact it has on society. In this work, we describe our\napproach for the WANLP 2022 shared task which handles the task of propaganda\ndetection in a multi-label setting. The task demands the model to label the\ngiven text as having one or more types of propaganda techniques. There are a\ntotal of 21 propaganda techniques to be detected. We show that an ensemble of\nfive models performs the best on the task, scoring a micro-F1 score of 59.73%.\nWe also conduct comprehensive ablations and propose various future directions\nfor this work.",
        "pdf_link": "https://arxiv.org/pdf/2210.08209v2.pdf"
    },
    {
        "title": "Temporal Word Meaning Disambiguation using TimeLMs",
        "authors": [
            "Mihir Godbole",
            "Parth Dandavate",
            "Aditya Kane"
        ],
        "published": "2022-10-15T06:34:59Z",
        "summary": "Meaning of words constantly changes given the events in modern civilization.\nLarge Language Models use word embeddings, which are often static and thus\ncannot cope with this semantic change. Thus,it is important to resolve\nambiguity in word meanings. This paper is an effort in this direction, where we\nexplore methods for word sense disambiguation for the EvoNLP shared task. We\nconduct rigorous ablations for two solutions to this problem. We see that an\napproach using time-aware language models helps this task. Furthermore, we\nexplore possible future directions to this problem.",
        "pdf_link": "https://arxiv.org/pdf/2210.08207v2.pdf"
    },
    {
        "title": "TestAug: A Framework for Augmenting Capability-based NLP Tests",
        "authors": [
            "Guanqun Yang",
            "Mirazul Haque",
            "Qiaochu Song",
            "Wei Yang",
            "Xueqing Liu"
        ],
        "published": "2022-10-14T20:42:16Z",
        "summary": "The recently proposed capability-based NLP testing allows model developers to\ntest the functional capabilities of NLP models, revealing functional failures\nthat cannot be detected by the traditional heldout mechanism. However, existing\nwork on capability-based testing requires extensive manual efforts and domain\nexpertise in creating the test cases. In this paper, we investigate a low-cost\napproach for the test case generation by leveraging the GPT-3 engine. We\nfurther propose to use a classifier to remove the invalid outputs from GPT-3\nand expand the outputs into templates to generate more test cases. Our\nexperiments show that TestAug has three advantages over the existing work on\nbehavioral testing: (1) TestAug can find more bugs than existing work; (2) The\ntest cases in TestAug are more diverse; and (3) TestAug largely saves the\nmanual efforts in creating the test suites. The code and data for TestAug can\nbe found at our project website (https://guanqun-yang.github.io/testaug/) and\nGitHub (https://github.com/guanqun-yang/testaug).",
        "pdf_link": "https://arxiv.org/pdf/2210.08097v1.pdf"
    },
    {
        "title": "MiQA: A Benchmark for Inference on Metaphorical Questions",
        "authors": [
            "Iulia-Maria Comsa",
            "Julian Martin Eisenschlos",
            "Srini Narayanan"
        ],
        "published": "2022-10-14T17:46:05Z",
        "summary": "We propose a benchmark to assess the capability of large language models to\nreason with conventional metaphors. Our benchmark combines the previously\nisolated topics of metaphor detection and commonsense reasoning into a single\ntask that requires a model to make inferences by accurately selecting between\nthe literal and metaphorical register. We examine the performance of\nstate-of-the-art pre-trained models on binary-choice tasks and find a large\ndiscrepancy between the performance of small and very large models, going from\nchance to near-human level. We also analyse the largest model in a generative\nsetting and find that although human performance is approached, careful\nmultiple-shot prompting is required.",
        "pdf_link": "https://arxiv.org/pdf/2210.07993v1.pdf"
    },
    {
        "title": "PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population",
        "authors": [
            "Tianqing Fang",
            "Quyet V. Do",
            "Hongming Zhang",
            "Yangqiu Song",
            "Ginny Y. Wong",
            "Simon See"
        ],
        "published": "2022-10-14T17:37:30Z",
        "summary": "Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen\nentities and assertions on CSKBs, and is an important yet hard commonsense\nreasoning task. One challenge is that it requires out-of-domain generalization\nability as the source CSKB for training is of a relatively smaller scale (1M)\nwhile the whole candidate space for population is way larger (200M). We propose\nPseudoReasoner, a semi-supervised learning framework for CSKB population that\nuses a teacher model pre-trained on CSKBs to provide pseudo labels on the\nunlabeled candidate dataset for a student model to learn from. The teacher can\nbe a generative model rather than restricted to discriminative models as\nprevious works. In addition, we design a new filtering procedure for pseudo\nlabels based on influence function and the student model's prediction to\nfurther improve the performance. The framework can improve the backbone model\nKG-BERT (RoBERTa-large) by 3.3 points on the overall performance and\nespecially, 5.3 points on the out-of-domain performance, and achieves the\nstate-of-the-art. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/PseudoReasoner.",
        "pdf_link": "https://arxiv.org/pdf/2210.07988v1.pdf"
    },
    {
        "title": "The Debate Over Understanding in AI's Large Language Models",
        "authors": [
            "Melanie Mitchell",
            "David C. Krakauer"
        ],
        "published": "2022-10-14T17:04:29Z",
        "summary": "We survey a current, heated debate in the AI research community on whether\nlarge pre-trained language models can be said to \"understand\" language -- and\nthe physical and social situations language encodes -- in any important sense.\nWe describe arguments that have been made for and against such understanding,\nand key questions for the broader sciences of intelligence that have arisen in\nlight of these arguments. We contend that a new science of intelligence can be\ndeveloped that will provide insight into distinct modes of understanding, their\nstrengths and limitations, and the challenge of integrating diverse forms of\ncognition.",
        "pdf_link": "https://arxiv.org/pdf/2210.13966v3.pdf"
    },
    {
        "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
        "authors": [
            "Tiannan Wang",
            "Wangchunshu Zhou",
            "Yan Zeng",
            "Xinsong Zhang"
        ],
        "published": "2022-10-14T13:26:41Z",
        "summary": "Pre-trained vision-language models (VLMs) have achieved impressive results in\na range of vision-language tasks. However, popular VLMs usually consist of\nhundreds of millions of parameters which brings challenges for fine-tuning and\ndeployment in real-world applications due to space, memory, and latency\nconstraints. In this work, we introduce a distilling then pruning framework to\ncompress large vision-language models into smaller, faster, and more accurate\nones. We first shrink the size of a pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training stage to obtain a\ntask-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm\nto automatically infer the importance of vision and language modalities for\ndifferent downstream tasks and adaptively remove redundant structures and\nneurons in different encoders with controllable target sparsity. We apply our\nframework to train EfficientVLM, a fast and accurate vision-language model\nconsisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,\naccounting for only 93 million parameters in total, which is 44.3% of the\nteacher model. EfficientVLM retains 98.4% performance of the teacher model and\naccelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute\nimprovement over previous SoTA efficient VLMs of similar sizes by a large\nmargin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2\n(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation\n(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.07795v1.pdf"
    },
    {
        "title": "Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning",
        "authors": [
            "Louis Castricato",
            "Alexander Havrilla",
            "Shahbuland Matiana",
            "Michael Pieler",
            "Anbang Ye",
            "Ian Yang",
            "Spencer Frazier",
            "Mark Riedl"
        ],
        "published": "2022-10-14T13:21:33Z",
        "summary": "Controlled automated story generation seeks to generate natural language\nstories satisfying constraints from natural language critiques or preferences.\nExisting methods to control for story preference utilize prompt engineering\nwhich is labor intensive and often inconsistent. They may also use\nlogit-manipulation methods which require annotated datasets to exist for the\ndesired attributes. To address these issues, we first train a contrastive\nbi-encoder model to align stories with corresponding human critiques, named\nCARP, building a general purpose preference model. This is subsequently used as\na reward function to fine-tune a generative language model via reinforcement\nlearning. However, simply fine-tuning a generative language model with a\ncontrastive reward model does not always reliably result in a story generation\nsystem capable of generating stories that meet user preferences. To increase\nstory generation robustness we further fine-tune the contrastive reward model\nusing a prompt-learning technique. A human participant study is then conducted\ncomparing generations from our full system, ablations, and two baselines. We\nshow that the full fine-tuning pipeline results in a story generator preferred\nover a LLM 20x as large as well as logit-based methods. This motivates the use\nof contrastive learning for general purpose human preference modeling.",
        "pdf_link": "https://arxiv.org/pdf/2210.07792v2.pdf"
    },
    {
        "title": "Extracting Cultural Commonsense Knowledge at Scale",
        "authors": [
            "Tuan-Phong Nguyen",
            "Simon Razniewski",
            "Aparna Varde",
            "Gerhard Weikum"
        ],
        "published": "2022-10-14T12:53:57Z",
        "summary": "Structured knowledge is important for many AI applications. Commonsense\nknowledge, which is crucial for robust human-centric AI, is covered by a small\nnumber of structured knowledge projects. However, they lack knowledge about\nhuman traits and behaviors conditioned on socio-cultural contexts, which is\ncrucial for situative AI. This paper presents CANDLE, an end-to-end methodology\nfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.\nCANDLE extracts CCSK assertions from a huge web corpus and organizes them into\ncoherent clusters, for 3 domains of subjects (geography, religion, occupation)\nand several cultural facets (food, drinks, clothing, traditions, rituals,\nbehaviors). CANDLE includes judicious techniques for classification-based\nfiltering and scoring of interestingness. Experimental evaluations show the\nsuperiority of the CANDLE CCSK collection over prior works, and an extrinsic\nuse case demonstrates the benefits of CCSK for the GPT-3 language model. Code\nand data can be accessed at https://candle.mpi-inf.mpg.de/.",
        "pdf_link": "https://arxiv.org/pdf/2210.07763v3.pdf"
    },
    {
        "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
        "authors": [
            "Sachin Kumar",
            "Vidhisha Balachandran",
            "Lucille Njoo",
            "Antonios Anastasopoulos",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-14T10:43:39Z",
        "summary": "Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have explored these harms and called for their\nmitigation via development of safer, fairer models. Going beyond enumerating\nthe risks of harms, this work provides a survey of practical methods for\naddressing potential threats and societal harms from language generation\nmodels. We draw on several prior works' taxonomies of language model risks to\npresent a structured overview of strategies for detecting and ameliorating\ndifferent kinds of risks/harms of language generators. Bridging diverse strands\nof research, this survey aims to serve as a practical guide for both LM\nresearchers and practitioners, with explanations of different mitigation\nstrategies' motivations, their limitations, and open problems for future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2210.07700v2.pdf"
    },
    {
        "title": "Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values",
        "authors": [
            "Yejin Bang",
            "Tiezheng Yu",
            "Andrea Madotto",
            "Zhaojiang Lin",
            "Mona Diab",
            "Pascale Fung"
        ],
        "published": "2022-10-14T09:10:49Z",
        "summary": "Many NLP classification tasks, such as sexism/racism detection or toxicity\ndetection, are based on human values. Yet, human values can vary under diverse\ncultural conditions. Therefore, we introduce a framework for value-aligned\nclassification that performs prediction based on explicitly written human\nvalues in the command. Along with the task, we propose a practical approach\nthat distills value-aligned knowledge from large-scale language models (LLMs)\nto construct value-aligned classifiers in two steps. First, we generate\nvalue-aligned training data from LLMs by prompt-based few-shot learning. Next,\nwe fine-tune smaller classification models with the generated data for the\ntask. Empirical results show that our VA-Models surpass multiple baselines by\nat least 15.56% on the F1-score, including few-shot learning with OPT-175B and\nexisting text augmentation methods. We suggest that using classifiers with\nexplicit human value input improves both inclusivity & explainability in AI.",
        "pdf_link": "https://arxiv.org/pdf/2210.07652v1.pdf"
    },
    {
        "title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation",
        "authors": [
            "Tianxiang Sun",
            "Junliang He",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2022-10-14T08:24:11Z",
        "summary": "Automatic evaluation metrics are crucial to the development of generative\nsystems. In recent years, pre-trained language model (PLM) based metrics, such\nas BERTScore, have been commonly adopted in various generation tasks. However,\nit has been demonstrated that PLMs encode a range of stereotypical societal\nbiases, leading to a concern on the fairness of PLMs as metrics. To that end,\nthis work presents the first systematic study on the social bias in PLM-based\nmetrics. We demonstrate that popular PLM-based metrics exhibit significantly\nhigher social bias than traditional metrics on 6 sensitive attributes, namely\nrace, gender, religion, physical appearance, age, and socioeconomic status.\nIn-depth analysis suggests that choosing paradigms (matching, regression, or\ngeneration) of the metric has a greater impact on fairness than choosing PLMs.\nIn addition, we develop debiasing adapters that are injected into PLM layers,\nmitigating bias in PLM-based metrics while retaining high performance for\nevaluating text generation.",
        "pdf_link": "https://arxiv.org/pdf/2210.07626v1.pdf"
    },
    {
        "title": "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation",
        "authors": [
            "Mojtaba Valipour",
            "Mehdi Rezagholizadeh",
            "Ivan Kobyzev",
            "Ali Ghodsi"
        ],
        "published": "2022-10-14T06:29:22Z",
        "summary": "With the ever-growing size of pretrained models (PMs), fine-tuning them has\nbecome more expensive and resource-hungry. As a remedy, low-rank adapters\n(LoRA) keep the main pretrained weights of the model frozen and just introduce\nsome learnable truncated SVD modules (so-called LoRA blocks) to the model.\nWhile LoRA blocks are parameter-efficient, they suffer from two major problems:\nfirst, the size of these blocks is fixed and cannot be modified after training\n(for example, if we need to change the rank of LoRA blocks, then we need to\nre-train them from scratch); second, optimizing their rank requires an\nexhaustive search and effort. In this work, we introduce a dynamic low-rank\nadaptation (DyLoRA) technique to address these two problems together. Our\nDyLoRA method trains LoRA blocks for a range of ranks instead of a single rank\nby sorting the representation learned by the adapter module at different ranks\nduring training. We evaluate our solution on different natural language\nunderstanding (GLUE benchmark) and language generation tasks (E2E, DART and\nWebNLG) using different pretrained models such as RoBERTa and GPT with\ndifferent sizes. Our results show that we can train dynamic search-free models\nwith DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA\nwithout significantly compromising performance. Moreover, our models can\nperform consistently well on a much larger range of ranks compared to LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2210.07558v2.pdf"
    },
    {
        "title": "Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding",
        "authors": [
            "Songyang Gao",
            "Shihan Dou",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022-10-14T05:56:38Z",
        "summary": "Dataset bias has attracted increasing attention recently for its detrimental\neffect on the generalization ability of fine-tuned models. The current\nmainstream solution is designing an additional shallow model to pre-identify\nbiased instances. However, such two-stage methods scale up the computational\ncomplexity of training process and obstruct valid feature information while\nmitigating bias. To address this issue, we utilize the representation\nnormalization method which aims at disentangling the correlations between\nfeatures of encoded sentences. We find it also promising in eliminating the\nbias problem by providing isotropic data distribution. We further propose\nKernel-Whitening, a Nystrom kernel approximation method to achieve more\nthorough debiasing on nonlinear spurious correlations. Our framework is\nend-to-end with similar time consumption to fine-tuning. Experiments show that\nKernel-Whitening significantly improves the performance of BERT on\nout-of-distribution datasets while maintaining in-distribution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2210.07547v1.pdf"
    },
    {
        "title": "MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks",
        "authors": [
            "Zequn Liu",
            "Kefei Duan",
            "Junwei Yang",
            "Hanwen Xu",
            "Ming Zhang",
            "Sheng Wang"
        ],
        "published": "2022-10-14T03:34:09Z",
        "summary": "Heterogeneous Information Network (HIN) is essential to study complicated\nnetworks containing multiple edge types and node types. Meta-path, a sequence\nof node types and edge types, is the core technique to embed HINs. Since\nmanually curating meta-paths is time-consuming, there is a pressing need to\ndevelop automated meta-path generation approaches. Existing meta-path\ngeneration approaches cannot fully exploit the rich textual information in\nHINs, such as node names and edge type names. To address this problem, we\npropose MetaFill, a text-infilling-based approach for meta-path generation. The\nkey idea of MetaFill is to formulate meta-path identification problem as a word\nsequence infilling problem, which can be advanced by Pretrained Language Models\n(PLMs). We observed the superior performance of MetaFill against existing\nmeta-path generation methods and graph embedding methods that do not leverage\nmeta-paths in both link prediction and node classification on two real-world\nHIN datasets. We further demonstrated how MetaFill can accurately classify\nedges in the zero-shot setting, where existing approaches cannot generate any\nmeta-paths. MetaFill exploits PLMs to generate meta-paths for graph embedding,\nopening up new avenues for language model applications in graph analysis.",
        "pdf_link": "https://arxiv.org/pdf/2210.07488v1.pdf"
    },
    {
        "title": "\"John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility",
        "authors": [
            "Himanshu Gupta",
            "Neeraj Varshney",
            "Swaroop Mishra",
            "Kuntal Kumar Pal",
            "Saurabh Arjun Sawant",
            "Kevin Scaria",
            "Siddharth Goyal",
            "Chitta Baral"
        ],
        "published": "2022-10-14T02:46:06Z",
        "summary": "In current NLP research, large-scale language models and their abilities are\nwidely being discussed. Some recent works have also found notable failures of\nthese models. Often these failure examples involve complex reasoning abilities.\nThis work focuses on a simple commonsense ability, reasoning about when an\naction (or its effect) is feasible. To this end, we introduce FeasibilityQA, a\nquestion-answering dataset involving binary classification (BCQ) and\nmulti-choice multi-correct questions (MCQ) that test understanding of\nfeasibility. We show that even state-of-the-art models such as GPT-3, GPT-2,\nand T5 struggle to answer the feasibility questions correctly. Specifically, on\nMCQ and BCQ questions, GPT-3 achieves an accuracy of just (19%, 62%) and (25%,\n64%) in zero-shot and few-shot settings, respectively. We also evaluate models\nby providing relevant knowledge statements required to answer the question. We\nfind that the additional knowledge leads to a 7% gain in performance, but the\noverall performance still remains low. These results make one wonder how much\ncommonsense knowledge about action feasibility is encoded in state-of-the-art\nmodels and how well they can reason about it.",
        "pdf_link": "https://arxiv.org/pdf/2210.07471v2.pdf"
    },
    {
        "title": "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
        "authors": [
            "Evan Crothers",
            "Nathalie Japkowicz",
            "Herna Viktor"
        ],
        "published": "2022-10-13T19:46:14Z",
        "summary": "Machine generated text is increasingly difficult to distinguish from human\nauthored text. Powerful open-source models are freely available, and\nuser-friendly tools that democratize access to generative models are\nproliferating. ChatGPT, which was released shortly after the first edition of\nthis survey, epitomizes these trends. The great potential of state-of-the-art\nnatural language generation (NLG) systems is tempered by the multitude of\navenues for abuse. Detection of machine generated text is a key countermeasure\nfor reducing abuse of NLG models, with significant technical challenges and\nnumerous open problems. We provide a survey that includes both 1) an extensive\nanalysis of threat models posed by contemporary NLG systems, and 2) the most\ncomplete review of machine generated text detection methods to date. This\nsurvey places machine generated text within its cybersecurity and social\ncontext, and provides strong guidance for future work addressing the most\ncritical threat models, and ensuring detection systems themselves demonstrate\ntrustworthiness through fairness, robustness, and accountability.",
        "pdf_link": "https://arxiv.org/pdf/2210.07321v4.pdf"
    },
    {
        "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
        "authors": [
            "Abhijeet Awasthi",
            "Nitish Gupta",
            "Bidisha Samanta",
            "Shachi Dave",
            "Sunita Sarawagi",
            "Partha Talukdar"
        ],
        "published": "2022-10-13T19:34:14Z",
        "summary": "Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be a key mechanism for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, translation services may continue\nto be brittle due to domain mismatch between task-specific input text and\ngeneral-purpose text used for training translation models. For multilingual\nsemantic parsing, we demonstrate the effectiveness and flexibility offered by\nlarge language models (LLMs) for translating English datasets into several\nlanguages via few-shot prompting. Through extensive comparisons on two public\ndatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show\nthat our method of translating data using LLMs outperforms a strong\ntranslate-train baseline on 41 out of 50 languages. We study the key design\nchoices that enable more effective multilingual data translation via prompted\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.07313v2.pdf"
    },
    {
        "title": "Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning",
        "authors": [
            "Md Masudur Rahman",
            "Yexiang Xue"
        ],
        "published": "2022-10-13T19:30:43Z",
        "summary": "This paper proposes an advantage estimation approach based on data\naugmentation for policy optimization. Unlike using data augmentation on the\ninput to learn value and policy function as existing methods use, our method\nuses data augmentation to compute a bootstrap advantage estimation. This\nBootstrap Advantage Estimation (BAE) is then used for learning and updating the\ngradient of policy and value function. To demonstrate the effectiveness of our\napproach, we conducted experiments on several environments. These environments\nare from three benchmarks: Procgen, Deepmind Control, and Pybullet, which\ninclude both image and vector-based observations; discrete and continuous\naction spaces. We observe that our method reduces the policy and the value loss\nbetter than the Generalized advantage estimation (GAE) method and eventually\nimproves cumulative return. Furthermore, our method performs better than two\nrecently proposed data augmentation techniques (RAD and DRAC). Overall, our\nmethod performs better empirically than baselines in sample efficiency and\ngeneralization, where the agent is tested in unseen environments.",
        "pdf_link": "https://arxiv.org/pdf/2210.07312v1.pdf"
    },
    {
        "title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models",
        "authors": [
            "Haozhe An",
            "Zongxia Li",
            "Jieyu Zhao",
            "Rachel Rudinger"
        ],
        "published": "2022-10-13T18:04:48Z",
        "summary": "A common limitation of diagnostic tests for detecting social biases in NLP\nmodels is that they may only detect stereotypic associations that are\npre-specified by the designer of the test. Since enumerating all possible\nproblematic associations is infeasible, it is likely these tests fail to detect\nbiases that are present in a model but not pre-specified by the designer. To\naddress this limitation, we propose SODAPOP (SOcial bias Discovery from Answers\nabout PeOPle) in social commonsense question-answering. Our pipeline generates\nmodified instances from the Social IQa dataset (Sap et al., 2019) by (1)\nsubstituting names associated with different demographic groups, and (2)\ngenerating many distractor answers from a masked language model. By using a\nsocial commonsense model to score the generated distractors, we are able to\nuncover the model's stereotypic associations between demographic groups and an\nopen set of words. We also test SODAPOP on debiased models and show the\nlimitations of multiple state-of-the-art debiasing algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2210.07269v2.pdf"
    },
    {
        "title": "Mass-Editing Memory in a Transformer",
        "authors": [
            "Kevin Meng",
            "Arnab Sen Sharma",
            "Alex Andonian",
            "Yonatan Belinkov",
            "David Bau"
        ],
        "published": "2022-10-13T17:55:53Z",
        "summary": "Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.",
        "pdf_link": "https://arxiv.org/pdf/2210.07229v2.pdf"
    },
    {
        "title": "Language Model Decoding as Likelihood-Utility Alignment",
        "authors": [
            "Martin Josifoski",
            "Maxime Peyrard",
            "Frano Rajic",
            "Jiheng Wei",
            "Debjit Paul",
            "Valentin Hartmann",
            "Barun Patra",
            "Vishrav Chaudhary",
            "Emre K\u0131c\u0131man",
            "Boi Faltings",
            "Robert West"
        ],
        "published": "2022-10-13T17:55:51Z",
        "summary": "A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of a decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios, and their findings do not generalize\nacross tasks. We argue that the misalignment between the model's likelihood and\nthe task-specific notion of utility is the key factor to understanding the\neffectiveness of decoding algorithms. To structure the discussion, we introduce\na taxonomy of misalignment mitigation strategies (MMSs), providing a unifying\nview of decoding as a tool for alignment. The MMS taxonomy groups decoding\nalgorithms based on their implicit assumptions about likelihood--utility\nmisalignment, yielding general statements about their applicability across\ntasks. Specifically, by analyzing the correlation between the likelihood and\nthe utility of predictions across a diverse set of tasks, we provide empirical\nevidence supporting the proposed taxonomy and a set of principles to structure\nreasoning when choosing a decoding algorithm. Crucially, our analysis is the\nfirst to relate likelihood-based decoding algorithms with algorithms that rely\non external information, such as value-guided methods and prompting, and covers\nthe most diverse set of tasks to date. Code, data, and models are available at\nhttps://github.com/epfl-dlab/understanding-decoding.",
        "pdf_link": "https://arxiv.org/pdf/2210.07228v2.pdf"
    },
    {
        "title": "Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods",
        "authors": [
            "Nils Feldhus",
            "Leonhard Hennig",
            "Maximilian Dustin Nasert",
            "Christopher Ebert",
            "Robert Schwarzenberg",
            "Sebastian M\u00f6ller"
        ],
        "published": "2022-10-13T17:48:15Z",
        "summary": "Saliency maps can explain a neural model's predictions by identifying\nimportant input features. They are difficult to interpret for laypeople,\nespecially for instances with many features. In order to make them more\naccessible, we formalize the underexplored task of translating saliency maps\ninto natural language and compare methods that address two key challenges of\nthis approach -- what and how to verbalize. In both automatic and human\nevaluation setups, using token-level attributions from text classification\ntasks, we compare two novel methods (search-based and instruction-based\nverbalizations) against conventional feature importance representations\n(heatmap visualizations and extractive rationales), measuring simulatability,\nfaithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to\ngenerate saliency map verbalizations yields plausible explanations which\ninclude associations, abstractive summarization and commonsense reasoning,\nachieving by far the highest human ratings, but they are not faithfully\ncapturing numeric information and are inconsistent in their interpretation of\nthe task. In comparison, our search-based, model-free verbalization approach\nefficiently completes templated verbalizations, is faithful by design, but\nfalls short in helpfulness and simulatability. Our results suggest that\nsaliency map verbalization makes feature attribution explanations more\ncomprehensible and less cognitively challenging to humans than conventional\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2210.07222v3.pdf"
    },
    {
        "title": "Visual Classification via Description from Large Language Models",
        "authors": [
            "Sachit Menon",
            "Carl Vondrick"
        ],
        "published": "2022-10-13T17:03:46Z",
        "summary": "Vision-language models (VLMs) such as CLIP have shown promising performance\non a variety of recognition tasks using the standard zero-shot classification\nprocedure -- computing similarity between the query image and the embedded\nwords for each category. By only using the category name, they neglect to make\nuse of the rich context of additional information that language affords. The\nprocedure gives no intermediate understanding of why a category is chosen, and\nfurthermore provides no mechanism for adjusting the criteria used towards this\ndecision. We present an alternative framework for classification with VLMs,\nwhich we call classification by description. We ask VLMs to check for\ndescriptive features rather than broad categories: to find a tiger, look for\nits stripes; its claws; and more. By basing decisions on these descriptors, we\ncan provide additional cues that encourage using the features we want to be\nused. In the process, we can get a clear idea of what features the model uses\nto construct its decision; it gains some level of inherent explainability. We\nquery large language models (e.g., GPT-3) for these descriptors to obtain them\nin a scalable way. Extensive experiments show our framework has numerous\nadvantages past interpretability. We show improvements in accuracy on ImageNet\nacross distribution shifts; demonstrate the ability to adapt VLMs to recognize\nconcepts unseen during training; and illustrate how descriptors can be edited\nto effectively mitigate bias compared to the baseline.",
        "pdf_link": "https://arxiv.org/pdf/2210.07183v2.pdf"
    },
    {
        "title": "SQuAT: Sharpness- and Quantization-Aware Training for BERT",
        "authors": [
            "Zheng Wang",
            "Juncheng B Li",
            "Shuhui Qu",
            "Florian Metze",
            "Emma Strubell"
        ],
        "published": "2022-10-13T16:52:19Z",
        "summary": "Quantization is an effective technique to reduce memory footprint, inference\nlatency, and power consumption of deep learning models. However, existing\nquantization methods suffer from accuracy degradation compared to\nfull-precision (FP) models due to the errors introduced by coarse gradient\nestimation through non-differentiable quantization layers. The existence of\nsharp local minima in the loss landscapes of overparameterized models (e.g.,\nTransformers) tends to aggravate such performance penalty in low-bit (2, 4\nbits) settings. In this work, we propose sharpness- and quantization-aware\ntraining (SQuAT), which would encourage the model to converge to flatter minima\nwhile performing quantization-aware training. Our proposed method alternates\ntraining between sharpness objective and step-size objective, which could\npotentially let the model learn the most suitable parameter update magnitude to\nreach convergence near-flat minima. Extensive experiments show that our method\ncan consistently outperform state-of-the-art quantized BERT models under 2, 3,\nand 4-bit settings on GLUE benchmarks by 1%, and can sometimes even outperform\nfull precision (32-bit) models. Our experiments on empirical measurement of\nsharpness also suggest that our method would lead to flatter minima compared to\nother quantization methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.07171v1.pdf"
    },
    {
        "title": "Language Models of Code are Few-Shot Commonsense Learners",
        "authors": [
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Yiming Yang",
            "Graham Neubig"
        ],
        "published": "2022-10-13T16:09:36Z",
        "summary": "We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2210.07128v3.pdf"
    },
    {
        "title": "Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence",
        "authors": [
            "Chris Callison-Burch",
            "Gaurav Singh Tomar",
            "Lara J. Martin",
            "Daphne Ippolito",
            "Suma Bailis",
            "David Reitter"
        ],
        "published": "2022-10-13T15:43:39Z",
        "summary": "AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem\nto test systems on various language-related capabilities. In this paper, we\nframe D&D specifically as a dialogue system challenge, where the tasks are to\nboth generate the next conversational turn in the game and predict the state of\nthe game given the dialogue history. We create a gameplay dataset consisting of\nnearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,\n500,000 dice rolls, and 58 million words. We automatically annotate the data\nwith partial state information about the game play. We train a large language\nmodel (LM) to generate the next game turn, conditioning it on different\ninformation. The LM can respond as a particular character or as the player who\nruns the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue\nthat is either in-character (roleplaying in the fictional world) or\nout-of-character (discussing rules or strategy). We perform a human evaluation\nto determine what factors make the generated output plausible and interesting.\nWe further perform an automatic evaluation to determine how well the model can\npredict the game state given the history and examine how well tracking the game\nstate improves its ability to produce plausible conversational output.",
        "pdf_link": "https://arxiv.org/pdf/2210.07109v1.pdf"
    },
    {
        "title": "Query Expansion Using Contextual Clue Sampling with Language Models",
        "authors": [
            "Linqing Liu",
            "Minghan Li",
            "Jimmy Lin",
            "Sebastian Riedel",
            "Pontus Stenetorp"
        ],
        "published": "2022-10-13T15:18:04Z",
        "summary": "Query expansion is an effective approach for mitigating vocabulary mismatch\nbetween queries and documents in information retrieval. One recent line of\nresearch uses language models to generate query-related contexts for expansion.\nAlong this line, we argue that expansion terms from these contexts should\nbalance two key aspects: diversity and relevance. The obvious way to increase\ndiversity is to sample multiple contexts from the language model. However, this\ncomes at the cost of relevance, because there is a well-known tendency of\nmodels to hallucinate incorrect or irrelevant contexts. To balance these two\nconsiderations, we propose a combination of an effective filtering strategy and\nfusion of the retrieved documents based on the generation probability of each\ncontext. Our lexical matching based approach achieves a similar top-5/top-20\nretrieval accuracy and higher top-100 accuracy compared with the\nwell-established dense retrieval model DPR, while reducing the index size by\nmore than 96%. For end-to-end QA, the reader model also benefits from our\nmethod and achieves the highest Exact-Match score against several competitive\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.07093v1.pdf"
    },
    {
        "title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
        "authors": [
            "Andy Rosenbaum",
            "Saleh Soltan",
            "Wael Hamza",
            "Amir Saffari",
            "Marco Damonte",
            "Isabel Groves"
        ],
        "published": "2022-10-13T15:01:03Z",
        "summary": "A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.07074v2.pdf"
    },
    {
        "title": "Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation",
        "authors": [
            "Jinhui Ye",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu"
        ],
        "published": "2022-10-13T14:25:08Z",
        "summary": "Sign language gloss translation aims to translate the sign glosses into\nspoken language texts, which is challenging due to the scarcity of labeled\ngloss-text parallel data. Back translation (BT), which generates\npseudo-parallel data by translating in-domain spoken language texts into sign\nglosses, has been applied to alleviate the data scarcity problem. However, the\nlack of large-scale high-quality domain spoken language text data limits the\neffect of BT. In this paper, to overcome the limitation, we propose a Prompt\nbased domain text Generation (PGEN) approach to produce the large-scale\nin-domain spoken language text data. Specifically, PGEN randomly concatenates\nsentences from the original in-domain spoken language text data as prompts to\ninduce a pre-trained language model (i.e., GPT-2) to generate spoken language\ntexts in a similar style. Experimental results on three benchmarks of sign\nlanguage gloss translation in varied languages demonstrate that BT with spoken\nlanguage texts generated by PGEN significantly outperforms the compared\nmethods. In addition, as the scale of spoken language texts generated by PGEN\nincreases, the BT technique can achieve further improvements, demonstrating the\neffectiveness of our approach. We release the code and data for facilitating\nfuture research in this field.",
        "pdf_link": "https://arxiv.org/pdf/2210.07054v2.pdf"
    },
    {
        "title": "Spontaneous Emerging Preference in Two-tower Language Model",
        "authors": [
            "Zhengqi He",
            "Taro Toyoizumi"
        ],
        "published": "2022-10-13T13:55:19Z",
        "summary": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.",
        "pdf_link": "https://arxiv.org/pdf/2210.07041v1.pdf"
    },
    {
        "title": "Tone prediction and orthographic conversion for Basaa",
        "authors": [
            "Ilya Nikitin",
            "Brian O'Connor",
            "Anastasia Safonova"
        ],
        "published": "2022-10-13T12:58:39Z",
        "summary": "In this paper, we present a seq2seq approach for transliterating missionary\nBasaa orthographies into the official orthography. Our model uses pre-trained\nBasaa missionary and official orthography corpora using BERT. Since Basaa is a\nlow-resource language, we have decided to use the mT5 model for our project.\nBefore training our model, we pre-processed our corpora by eliminating\none-to-one correspondences between spellings and unifying characters variably\ncontaining either one to two characters into single-character form. Our best\nmT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.",
        "pdf_link": "https://arxiv.org/pdf/2210.06986v1.pdf"
    },
    {
        "title": "Multilingual Zero Resource Speech Recognition Base on Self-Supervise Pre-Trained Acoustic Models",
        "authors": [
            "Haoyu Wang",
            "Wei-Qiang Zhang",
            "Hongbin Suo",
            "Yulong Wan"
        ],
        "published": "2022-10-13T12:11:18Z",
        "summary": "Labeled audio data is insufficient to build satisfying speech recognition\nsystems for most of the languages in the world. There have been some\nzero-resource methods trying to perform phoneme or word-level speech\nrecognition without labeled audio data of the target language, but the error\nrate of these methods is usually too high to be applied in real-world\nscenarios. Recently, the representation ability of self-supervise pre-trained\nmodels has been found to be extremely beneficial in zero-resource phoneme\nrecognition. As far as we are concerned, this paper is the first attempt to\nextend the use of pre-trained models into word-level zero-resource speech\nrecognition. This is done by fine-tuning the pre-trained models on IPA phoneme\ntranscriptions and decoding with a language model trained on extra texts.\nExperiments on Wav2vec 2.0 and HuBERT models show that this method can achieve\nless than 20% word error rate on some languages, and the average error rate on\n8 languages is 33.77%.",
        "pdf_link": "https://arxiv.org/pdf/2210.06936v1.pdf"
    },
    {
        "title": "ImaginaryNet: Learning Object Detectors without Real Images and Annotations",
        "authors": [
            "Minheng Ni",
            "Zitong Huang",
            "Kailai Feng",
            "Wangmeng Zuo"
        ],
        "published": "2022-10-13T10:25:22Z",
        "summary": "Without the demand of training in reality, humans can easily detect a known\nconcept simply based on its language description. Empowering deep learning with\nthis ability undoubtedly enables the neural network to handle complex vision\ntasks, e.g., object detection, without collecting and annotating real images.\nTo this end, this paper introduces a novel challenging learning paradigm\nImaginary-Supervised Object Detection (ISOD), where neither real images nor\nmanual annotations are allowed for training object detectors. To resolve this\nchallenge, we propose ImaginaryNet, a framework to synthesize images by\ncombining pretrained language model and text-to-image synthesis model. Given a\nclass label, the language model is used to generate a full description of a\nscene with a target object, and the text-to-image model deployed to generate a\nphoto-realistic image. With the synthesized images and class labels, weakly\nsupervised object detection can then be leveraged to accomplish ISOD. By\ngradually introducing real images and manual annotations, ImaginaryNet can\ncollaborate with other supervision settings to further boost detection\nperformance. Experiments show that ImaginaryNet can (i) obtain about 70%\nperformance in ISOD compared with the weakly supervised counterpart of the same\nbackbone trained on real data, (ii) significantly improve the baseline while\nachieving state-of-the-art or comparable performance by incorporating\nImaginaryNet with other supervision settings.",
        "pdf_link": "https://arxiv.org/pdf/2210.06886v1.pdf"
    },
    {
        "title": "Sample-Then-Optimize Batch Neural Thompson Sampling",
        "authors": [
            "Zhongxiang Dai",
            "Yao Shu",
            "Bryan Kian Hsiang Low",
            "Patrick Jaillet"
        ],
        "published": "2022-10-13T09:01:58Z",
        "summary": "Bayesian optimization (BO), which uses a Gaussian process (GP) as a surrogate\nto model its objective function, is popular for black-box optimization.\nHowever, due to the limitations of GPs, BO underperforms in some problems such\nas those with categorical, high-dimensional or image inputs. To this end,\nrecent works have used the highly expressive neural networks (NNs) as the\nsurrogate model and derived theoretical guarantees using the theory of neural\ntangent kernel (NTK). However, these works suffer from the limitations of the\nrequirement to invert an extremely large parameter matrix and the restriction\nto the sequential (rather than batch) setting. To overcome these limitations,\nwe introduce two algorithms based on the Thompson sampling (TS) policy named\nSample-Then-Optimize Batch Neural TS (STO-BNTS) and STO-BNTS-Linear. To choose\nan input query, we only need to train an NN (resp. a linear model) and then\nchoose the query by maximizing the trained NN (resp. linear model), which is\nequivalently sampled from the GP posterior with the NTK as the kernel function.\nAs a result, our algorithms sidestep the need to invert the large parameter\nmatrix yet still preserve the validity of the TS policy. Next, we derive regret\nupper bounds for our algorithms with batch evaluations, and use insights from\nbatch BO and NTK to show that they are asymptotically no-regret under certain\nconditions. Finally, we verify their empirical effectiveness using practical\nAutoML and reinforcement learning experiments.",
        "pdf_link": "https://arxiv.org/pdf/2210.06850v1.pdf"
    },
    {
        "title": "Benchmarking Long-tail Generalization with Likelihood Splits",
        "authors": [
            "Ameya Godbole",
            "Robin Jia"
        ],
        "published": "2022-10-13T07:27:14Z",
        "summary": "In order to reliably process natural language, NLP systems must generalize to\nthe long tail of rare utterances. We propose a method to create challenging\nbenchmarks that require generalizing to the tail of the distribution by\nre-splitting existing datasets. We create 'Likelihood Splits' where examples\nthat are assigned lower likelihood by a pre-trained language model (LM) are\nplaced in the test set, and more likely examples are in the training set. This\nsimple approach can be customized to construct meaningful train-test splits for\na wide range of tasks. Likelihood Splits surface more challenges than random\nsplits: relative error rates of state-of-the-art models increase by 59% for\nsemantic parsing on Spider, 93% for natural language inference on SNLI, and 33%\nfor yes/no question answering on BoolQ, on our splits compared with the\ncorresponding random splits. Moreover, Likelihood Splits create fairer\nbenchmarks than adversarial filtering; when the LM used to create the splits is\nalso employed as the task model, our splits do not unfairly penalize the LM.",
        "pdf_link": "https://arxiv.org/pdf/2210.06799v2.pdf"
    },
    {
        "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
        "authors": [
            "Kevin Yang",
            "Yuandong Tian",
            "Nanyun Peng",
            "Dan Klein"
        ],
        "published": "2022-10-13T06:29:57Z",
        "summary": "We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).",
        "pdf_link": "https://arxiv.org/pdf/2210.06774v3.pdf"
    },
    {
        "title": "Policy Gradient With Serial Markov Chain Reasoning",
        "authors": [
            "Edoardo Cetin",
            "Oya Celiktutan"
        ],
        "published": "2022-10-13T06:15:29Z",
        "summary": "We introduce a new framework that performs decision-making in reinforcement\nlearning (RL) as an iterative reasoning process. We model agent behavior as the\nsteady-state distribution of a parameterized reasoning Markov chain (RMC),\noptimized with a new tractable estimate of the policy gradient. We perform\naction selection by simulating the RMC for enough reasoning steps to approach\nits steady-state distribution. We show our framework has several useful\nproperties that are inherently missing from traditional RL. For instance, it\nallows agent behavior to approximate any continuous distribution over actions\nby parameterizing the RMC with a simple Gaussian transition function. Moreover,\nthe number of reasoning steps to reach convergence can scale adaptively with\nthe difficulty of each action selection decision and can be accelerated by\nre-using past solutions. Our resulting algorithm achieves state-of-the-art\nperformance in popular Mujoco and DeepMind Control benchmarks, both for\nproprioceptive and pixel-based tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.06766v1.pdf"
    },
    {
        "title": "Explanations from Large Language Models Make Small Reasoners Better",
        "authors": [
            "Shiyang Li",
            "Jianshu Chen",
            "Yelong Shen",
            "Zhiyu Chen",
            "Xinlu Zhang",
            "Zekun Li",
            "Hong Wang",
            "Jing Qian",
            "Baolin Peng",
            "Yi Mao",
            "Wenhu Chen",
            "Xifeng Yan"
        ],
        "published": "2022-10-13T04:50:02Z",
        "summary": "Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.",
        "pdf_link": "https://arxiv.org/pdf/2210.06726v1.pdf"
    },
    {
        "title": "Assessing Out-of-Domain Language Model Performance from Few Examples",
        "authors": [
            "Prasann Singhal",
            "Jarad Forristal",
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2022-10-13T04:45:26Z",
        "summary": "While pretrained language models have exhibited impressive generalization\ncapabilities, they still behave unpredictably under certain domain shifts. In\nparticular, a model may learn a reasoning process on in-domain training data\nthat does not hold for out-of-domain test data. We address the task of\npredicting out-of-domain (OOD) performance in a few-shot fashion: given a few\ntarget-domain examples and a set of models with similar training performance,\ncan we understand how these models will perform on OOD test data? We benchmark\nthe performance on this task when looking at model accuracy on the few-shot\nexamples, then investigate how to incorporate analysis of the models' behavior\nusing feature attributions to better tackle this problem. Specifically, we\nexplore a set of \"factors\" designed to reveal model agreement with certain\npathological heuristics that may indicate worse generalization capabilities. On\ntextual entailment, paraphrase recognition, and a synthetic classification\ntask, we show that attribution-based factors can help rank relative model OOD\nperformance. However, accuracy on a few-shot test set is a surprisingly strong\nbaseline, particularly when the system designer does not have in-depth prior\nknowledge about the domain shift.",
        "pdf_link": "https://arxiv.org/pdf/2210.06725v1.pdf"
    },
    {
        "title": "Large Language Models are few(1)-shot Table Reasoners",
        "authors": [
            "Wenhu Chen"
        ],
        "published": "2022-10-13T04:08:24Z",
        "summary": "Recent literature has shown that large language models (LLMs) are generally\nexcellent few-shot reasoners to solve text reasoning tasks. However, the\ncapability of LLMs on table reasoning tasks is yet to be explored. In this\npaper, we aim at understanding how well LLMs can perform table-related tasks\nwith few-shot in-context learning. Specifically, we evaluated LLMs on popular\ntable QA and fact verification datasets like WikiTableQuestion, FetaQA,\nTabFact, and FEVEROUS and found that LLMs are competent at complex reasoning\nover table structures, though these models are not pre-trained on any table\ncorpus. When combined with `chain of thoughts' prompting, LLMs can achieve very\nstrong performance with only a 1-shot demonstration, even on par with some SoTA\nmodels. We show that LLMs are even more competent at generating comprehensive\nlong-form answers on FetaQA than tuned T5-large. We further manually studied\nthe reasoning chains elicited from LLMs and found that these reasoning chains\nare highly consistent with the underlying semantic form. We believe that LLMs\ncan serve as a simple yet generic baseline for future research. The code and\ndata are released in https://github.com/wenhuchen/TableCoT.",
        "pdf_link": "https://arxiv.org/pdf/2210.06710v2.pdf"
    },
    {
        "title": "Jointly Reinforced User Simulator and Task-oriented Dialog System with Simplified Generative Architecture",
        "authors": [
            "Hong Liu",
            "Zhijian Ou",
            "Yi Huang",
            "Junlan Feng"
        ],
        "published": "2022-10-13T03:57:17Z",
        "summary": "Recently, there has been progress in supervised funetuning pretrained GPT-2\nto build end-to-end task-oriented dialog (TOD) systems. However, online\nreinforcement learning of a GPT-2 based dialog system (DS), together with a\nend-to-end user simulator (US), has not ever been explored. Moreover, a\ndrawback with existing GPT-2 based TOD systems is that they mostly employ the\nwhole dialog history as input, which brings inefficiencies in memory and\ncompute. In this paper, we first propose Simplified Generative Architectures\n(SGA) for DS and US respectively, both based on GPT-2 but using shortened\nhistory. Then, we successfully develop Jointly Reinforced US and DS, called\nSGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves\nstate-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in\nboth training and generation. Extensive experiments on MultiWOZ2.1 further show\nthe superiority of SGA-JRUD in both offline and online evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2210.06706v1.pdf"
    },
    {
        "title": "Overlooked Video Classification in Weakly Supervised Video Anomaly Detection",
        "authors": [
            "Weijun Tan",
            "Qi Yao",
            "Jingfeng Liu"
        ],
        "published": "2022-10-13T03:00:22Z",
        "summary": "Current weakly supervised video anomaly detection algorithms mostly use\nmultiple instance learning (MIL) or their varieties. Almost all recent\napproaches focus on how to select the correct snippets for training to improve\nthe performance. They overlook or do not realize the power of video\nclassification in boosting the performance of anomaly detection. In this paper,\nwe study explicitly the power of video classification supervision using a BERT\nor LSTM. With this BERT or LSTM, CNN features of all snippets of a video can be\naggregated into a single feature which can be used for video classification.\nThis simple yet powerful video classification supervision, combined into the\nMIL framework, brings extraordinary performance improvement on all three major\nvideo anomaly detection datasets. Particularly it improves the mean average\nprecision (mAP) on the XD-Violence from SOTA 78.84\\% to new 82.10\\%. The source\ncode is available at\nhttps://github.com/wjtan99/BERT_Anomaly_Video_Classification.",
        "pdf_link": "https://arxiv.org/pdf/2210.06688v2.pdf"
    },
    {
        "title": "Neuro-symbolic Explainable Artificial Intelligence Twin for Zero-touch IoE in Wireless Network",
        "authors": [
            "Md. Shirajum Munir",
            "Ki Tae Kim",
            "Apurba Adhikary",
            "Walid Saad",
            "Sachin Shetty",
            "Seong-Bae Park",
            "Choong Seon Hong"
        ],
        "published": "2022-10-13T01:08:06Z",
        "summary": "Explainable artificial intelligence (XAI) twin systems will be a fundamental\nenabler of zero-touch network and service management (ZSM) for sixth-generation\n(6G) wireless networks. A reliable XAI twin system for ZSM requires two\ncomposites: an extreme analytical ability for discretizing the physical\nbehavior of the Internet of Everything (IoE) and rigorous methods for\ncharacterizing the reasoning of such behavior. In this paper, a novel\nneuro-symbolic explainable artificial intelligence twin framework is proposed\nto enable trustworthy ZSM for a wireless IoE. The physical space of the XAI\ntwin executes a neural-network-driven multivariate regression to capture the\ntime-dependent wireless IoE environment while determining unconscious decisions\nof IoE service aggregation. Subsequently, the virtual space of the XAI twin\nconstructs a directed acyclic graph (DAG)-based Bayesian network that can infer\na symbolic reasoning score over unconscious decisions through a first-order\nprobabilistic language model. Furthermore, a Bayesian multi-arm bandits-based\nlearning problem is proposed for reducing the gap between the expected\nexplained score and the current obtained score of the proposed neuro-symbolic\nXAI twin. To address the challenges of extensible, modular, and stateless\nmanagement functions in ZSM, the proposed neuro-symbolic XAI twin framework\nconsists of two learning systems: 1) an implicit learner that acts as an\nunconscious learner in physical space, and 2) an explicit leaner that can\nexploit symbolic reasoning based on implicit learner decisions and prior\nevidence. Experimental results show that the proposed neuro-symbolic XAI twin\ncan achieve around 96.26% accuracy while guaranteeing from 18% to 44% more\ntrust score in terms of reasoning and closed-loop automation.",
        "pdf_link": "https://arxiv.org/pdf/2210.06649v1.pdf"
    },
    {
        "title": "The COVID That Wasn't: Counterfactual Journalism Using GPT",
        "authors": [
            "Sil Hamilton",
            "Andrew Piper"
        ],
        "published": "2022-10-13T00:50:24Z",
        "summary": "In this paper, we explore the use of large language models to assess human\ninterpretations of real world events. To do so, we use a language model trained\nprior to 2020 to artificially generate news articles concerning COVID-19 given\nthe headlines of actual articles written during the pandemic. We then compare\nstylistic qualities of our artificially generated corpus with a news corpus, in\nthis case 5,082 articles produced by CBC News between January 23 and May 5,\n2020. We find our artificially generated articles exhibits a considerably more\nnegative attitude towards COVID and a significantly lower reliance on\ngeopolitical framing. Our methods and results hold importance for researchers\nseeking to simulate large scale cultural processes via recent breakthroughs in\ntext generation.",
        "pdf_link": "https://arxiv.org/pdf/2210.06644v1.pdf"
    },
    {
        "title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses",
        "authors": [
            "Honglei Zhuang",
            "Zhen Qin",
            "Rolf Jagerman",
            "Kai Hui",
            "Ji Ma",
            "Jing Lu",
            "Jianmo Ni",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "published": "2022-10-12T20:51:49Z",
        "summary": "Recently, substantial progress has been made in text ranking based on\npretrained language models such as BERT. However, there are limited studies on\nhow to leverage more powerful sequence-to-sequence models such as T5. Existing\nattempts usually formulate text ranking as classification and rely on\npostprocessing to obtain a ranked list. In this paper, we propose RankT5 and\nstudy two T5-based ranking model structures, an encoder-decoder and an\nencoder-only one, so that they not only can directly output ranking scores for\neach query-document pair, but also can be fine-tuned with \"pairwise\" or\n\"listwise\" ranking losses to optimize ranking performances. Our experiments\nshow that the proposed models with ranking losses can achieve substantial\nranking performance gains on different public text ranking data sets. Moreover,\nwhen fine-tuned with listwise ranking losses, the ranking model appears to have\nbetter zero-shot ranking performance on out-of-domain data sets compared to the\nmodel fine-tuned with classification losses.",
        "pdf_link": "https://arxiv.org/pdf/2210.10634v1.pdf"
    },
    {
        "title": "DATScore: Evaluating Translation with Data Augmented Translations",
        "authors": [
            "Moussa Kamal Eddine",
            "Guokan Shang",
            "Michalis Vazirgiannis"
        ],
        "published": "2022-10-12T20:31:42Z",
        "summary": "The rapid development of large pretrained language models has revolutionized\nnot only the field of Natural Language Generation (NLG) but also its\nevaluation. Inspired by the recent work of BARTScore: a metric leveraging the\nBART language model to evaluate the quality of generated text from various\naspects, we introduce DATScore. DATScore uses data augmentation techniques to\nimprove the evaluation of machine translation. Our main finding is that\nintroducing data augmented translations of the source and reference texts is\ngreatly helpful in evaluating the quality of the generated translation. We also\npropose two novel score averaging and term weighting strategies to improve the\noriginal score computing process of BARTScore. Experimental results on WMT show\nthat DATScore correlates better with human meta-evaluations than the other\nrecent state-of-the-art metrics, especially for low-resource languages.\nAblation studies demonstrate the value added by our new scoring strategies.\nMoreover, we report in our extended experiments the performance of DATScore on\n3 NLG tasks other than translation.",
        "pdf_link": "https://arxiv.org/pdf/2210.06576v1.pdf"
    },
    {
        "title": "Developing a general-purpose clinical language inference model from a large corpus of clinical notes",
        "authors": [
            "Madhumita Sushil",
            "Dana Ludwig",
            "Atul J. Butte",
            "Vivek A. Rudrapatna"
        ],
        "published": "2022-10-12T20:08:45Z",
        "summary": "Several biomedical language models have already been developed for clinical\nlanguage inference. However, these models typically utilize general\nvocabularies and are trained on relatively small clinical corpora. We sought to\nevaluate the impact of using a domain-specific vocabulary and a large clinical\ntraining corpus on the performance of these language models in clinical\nlanguage inference. We trained a Bidirectional Encoder Decoder from\nTransformers (BERT) model using a diverse, deidentified corpus of 75 million\ndeidentified clinical notes authored at the University of California, San\nFrancisco (UCSF). We evaluated this model on several clinical language\ninference benchmark tasks: clinical and temporal concept recognition, relation\nextraction and medical language inference. We also evaluated our model on two\ntasks using discharge summaries from UCSF: diagnostic code assignment and\ntherapeutic class inference. Our model performs at par with the best publicly\navailable biomedical language models of comparable sizes on the public\nbenchmark tasks, and is significantly better than these models in a\nwithin-system evaluation on the two tasks using UCSF data. The use of in-domain\nvocabulary appears to improve the encoding of longer documents. The use of\nlarge clinical corpora appears to enhance document encoding and inferential\naccuracy. However, further research is needed to improve abbreviation\nresolution, and numerical, temporal, and implicitly causal inference.",
        "pdf_link": "https://arxiv.org/pdf/2210.06566v1.pdf"
    },
    {
        "title": "Subword Segmental Language Modelling for Nguni Languages",
        "authors": [
            "Francois Meyer",
            "Jan Buys"
        ],
        "published": "2022-10-12T18:41:00Z",
        "summary": "Subwords have become the standard units of text in NLP, enabling efficient\nopen-vocabulary models. With algorithms like byte-pair encoding (BPE), subword\nsegmentation is viewed as a preprocessing step applied to the corpus before\ntraining. This can lead to sub-optimal segmentations for low-resource languages\nwith complex morphologies. We propose a subword segmental language model (SSLM)\nthat learns how to segment words while being trained for autoregressive\nlanguage modelling. By unifying subword segmentation and language modelling,\nour model learns subwords that optimise LM performance. We train our model on\nthe 4 Nguni languages of South Africa. These are low-resource agglutinative\nlanguages, so subword information is critical. As an LM, SSLM outperforms\nexisting approaches such as BPE-based models on average across the 4 languages.\nFurthermore, it outperforms standard subword segmenters on unsupervised\nmorphological segmentation. We also train our model as a word-level sequence\nmodel, resulting in an unsupervised morphological segmenter that outperforms\nexisting methods by a large margin for all 4 languages. Our results show that\nlearning subword segmentation is an effective alternative to existing subword\nsegmenters, enabling the model to discover morpheme-like subwords that improve\nits LM capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2210.06525v1.pdf"
    },
    {
        "title": "Predictive Querying for Autoregressive Neural Sequence Models",
        "authors": [
            "Alex Boyd",
            "Sam Showalter",
            "Stephan Mandt",
            "Padhraic Smyth"
        ],
        "published": "2022-10-12T17:59:36Z",
        "summary": "In reasoning about sequential events it is natural to pose probabilistic\nqueries such as \"when will event A occur next\" or \"what is the probability of A\noccurring before B\", with applications in areas such as user modeling,\nmedicine, and finance. However, with machine learning shifting towards neural\nautoregressive models such as RNNs and transformers, probabilistic querying has\nbeen largely restricted to simple cases such as next-event prediction. This is\nin part due to the fact that future querying involves marginalization over\nlarge path spaces, which is not straightforward to do efficiently in such\nmodels. In this paper we introduce a general typology for predictive queries in\nneural autoregressive sequence models and show that such queries can be\nsystematically represented by sets of elementary building blocks. We leverage\nthis typology to develop new query estimation methods based on beam search,\nimportance sampling, and hybrids. Across four large-scale sequence datasets\nfrom different application domains, as well as for the GPT-2 language model, we\ndemonstrate the ability to make query answering tractable for arbitrary queries\nin exponentially-large predictive path-spaces, and find clear differences in\ncost-accuracy tradeoffs between search and sampling methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.06464v3.pdf"
    },
    {
        "title": "MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers",
        "authors": [
            "Mohammadmahdi Nouriborji",
            "Omid Rohanian",
            "Samaneh Kouchaki",
            "David A. Clifton"
        ],
        "published": "2022-10-12T17:23:21Z",
        "summary": "Pre-trained Language Models (LMs) have become an integral part of Natural\nLanguage Processing (NLP) in recent years, due to their superior performance in\ndownstream applications. In spite of this resounding success, the usability of\nLMs is constrained by computational and time complexity, along with their\nincreasing size; an issue that has been referred to as `overparameterisation'.\nDifferent strategies have been proposed in the literature to alleviate these\nproblems, with the aim to create effective compact models that nearly match the\nperformance of their bloated counterparts with negligible performance losses.\nOne of the most popular techniques in this area of research is model\ndistillation. Another potent but underutilised technique is cross-layer\nparameter sharing. In this work, we combine these two strategies and present\nMiniALBERT, a technique for converting the knowledge of fully parameterised LMs\n(such as BERT) into a compact recursive student. In addition, we investigate\nthe application of bottleneck adapters for layer-wise adaptation of our\nrecursive student, and also explore the efficacy of adapter tuning for\nfine-tuning of compact models. We test our proposed models on a number of\ngeneral and biomedical NLP tasks to demonstrate their viability and compare\nthem with the state-of-the-art and other existing compact models. All the codes\nused in the experiments are available at\nhttps://github.com/nlpie-research/MiniALBERT. Our pre-trained compact models\ncan be accessed from https://huggingface.co/nlpie.",
        "pdf_link": "https://arxiv.org/pdf/2210.06425v2.pdf"
    },
    {
        "title": "Foundation Transformers",
        "authors": [
            "Hongyu Wang",
            "Shuming Ma",
            "Shaohan Huang",
            "Li Dong",
            "Wenhui Wang",
            "Zhiliang Peng",
            "Yu Wu",
            "Payal Bajaj",
            "Saksham Singhal",
            "Alon Benhaim",
            "Barun Patra",
            "Zhun Liu",
            "Vishrav Chaudhary",
            "Xia Song",
            "Furu Wei"
        ],
        "published": "2022-10-12T17:16:27Z",
        "summary": "A big convergence of model architectures across language, vision, speech, and\nmultimodal is emerging. However, under the same name \"Transformers\", the above\nareas use different implementations for better performance, e.g.,\nPost-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We\ncall for the development of Foundation Transformer for true general-purpose\nmodeling, which serves as a go-to architecture for various tasks and modalities\nwith guaranteed training stability. In this work, we introduce a Transformer\nvariant, named Magneto, to fulfill the goal. Specifically, we propose\nSub-LayerNorm for good expressivity, and the initialization strategy\ntheoretically derived from DeepNet for stable scaling up. Extensive experiments\ndemonstrate its superior performance and better stability than the de facto\nTransformer variants designed for various applications, including language\nmodeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,\nBEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
        "pdf_link": "https://arxiv.org/pdf/2210.06423v2.pdf"
    },
    {
        "title": "On Text Style Transfer via Style Masked Language Models",
        "authors": [
            "Sharan Narasimhan",
            "Pooja Shekar",
            "Suvodip Dey",
            "Maunendra Sankar Desarkar"
        ],
        "published": "2022-10-12T16:44:06Z",
        "summary": "Text Style Transfer (TST) is performable through approaches such as latent\nspace disentanglement, cycle-consistency losses, prototype editing etc. The\nprototype editing approach, which is known to be quite successful in TST,\ninvolves two key phases a) Masking of source style-associated tokens and b)\nReconstruction of this source-style masked sentence conditioned with the target\nstyle. We follow a similar transduction method, in which we transpose the more\ndifficult direct source to target TST task to a simpler Style-Masked Language\nModel (SMLM) Task, wherein, similar to BERT \\cite{bert}, the goal of our model\nis now to reconstruct the source sentence from its style-masked version. We\narrive at the SMLM mechanism naturally by formulating prototype editing/\ntransduction methods in a probabilistic framework, where TST resolves into\nestimating a hypothetical parallel dataset from a partially observed parallel\ndataset, wherein each domain is assumed to have a common latent style-masked\nprior. To generate this style-masked prior, we use \"Explainable Attention\" as\nour choice of attribution for a more precise style-masking step and also\nintroduce a cost-effective and accurate \"Attribution-Surplus\" method of\ndetermining the position of masks from any arbitrary attribution model in O(1)\ntime. We empirically show that this non-generational approach well suites the\n\"content preserving\" criteria for a task like TST, even for a complex style\nlike Discourse Manipulation. Our model, the Style MLM, outperforms strong TST\nbaselines and is on par with state-of-the-art TST models, which use complex\narchitectures and orders of more parameters.",
        "pdf_link": "https://arxiv.org/pdf/2210.06394v1.pdf"
    },
    {
        "title": "GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most BERT-Pruning Methods",
        "authors": [
            "Eldar Kurtic",
            "Dan Alistarh"
        ],
        "published": "2022-10-12T16:35:47Z",
        "summary": "We revisit the performance of the classic gradual magnitude pruning (GMP)\nbaseline for large language models, focusing on the classic BERT benchmark on\nvarious popular tasks. Despite existing evidence in the literature that GMP\nperforms poorly, we show that a simple and general variant, which we call GMP*,\ncan match and sometimes outperform more complex state-of-the-art methods. Our\nresults provide a simple yet strong baseline for future work, highlight the\nimportance of parameter tuning for baselines, and even improve the performance\nof the state-of-the-art second-order pruning method in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2210.06384v3.pdf"
    },
    {
        "title": "Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary",
        "authors": [
            "Daniel Loureiro",
            "Al\u00edpio M\u00e1rio Jorge"
        ],
        "published": "2022-10-12T16:26:59Z",
        "summary": "Progress on commonsense reasoning is usually measured from performance\nimprovements on Question Answering tasks designed to require commonsense\nknowledge. However, fine-tuning large Language Models (LMs) on these specific\ntasks does not directly evaluate commonsense learned during pre-training. The\nmost direct assessments of commonsense knowledge in pre-trained LMs are\narguably cloze-style tasks targeting commonsense assertions (e.g., A pen is\nused for [MASK].). However, this approach is restricted by the LM's vocabulary\navailable for masked predictions, and its precision is subject to the context\nprovided by the assertion. In this work, we present a method for enriching LMs\nwith a grounded sense inventory (i.e., WordNet) available at the vocabulary\nlevel, without further training. This modification augments the prediction\nspace of cloze-style prompts to the size of a large ontology while enabling\nfiner-grained (sense-level) queries and predictions. In order to evaluate LMs\nwith higher precision, we propose SenseLAMA, a cloze-style task featuring\nverbalized relations from disambiguated triples sourced from WordNet, WikiData,\nand ConceptNet. Applying our method to BERT, producing a WordNet-enriched\nversion named SynBERT, we find that LMs can learn non-trivial commonsense\nknowledge from self-supervision, covering numerous relations, and more\neffectively than comparable similarity-based approaches.",
        "pdf_link": "https://arxiv.org/pdf/2210.06376v1.pdf"
    },
    {
        "title": "Context Generation Improves Open Domain Question Answering",
        "authors": [
            "Dan Su",
            "Mostofa Patwary",
            "Shrimai Prabhumoye",
            "Peng Xu",
            "Ryan Prenger",
            "Mohammad Shoeybi",
            "Pascale Fung",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "published": "2022-10-12T16:00:50Z",
        "summary": "Closed-book question answering (QA) requires a model to directly answer an\nopen-domain question without access to any external knowledge. Prior work on\nclosed-book QA either directly finetunes or prompts a pretrained language model\n(LM) to leverage the stored knowledge. However, they do not fully exploit the\nparameterized knowledge. To address this issue, we propose a two-stage,\nclosed-book QA framework which employs a coarse-to-fine approach to extract\nrelevant knowledge and answer a question. Our approach first generates a\nrelated context for a given question by prompting a pretrained LM. We then\nprompt the same LM for answer prediction using the generated context and the\nquestion. Additionally, to eliminate failure caused by context uncertainty, we\nmarginalize over generated contexts. Experimental results on three QA\nbenchmarks show that our method significantly outperforms previous closed-book\nQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book\nmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our\nmethod is able to better exploit the stored knowledge in pretrained LMs without\nadding extra learnable parameters or needing finetuning, and paves the way for\nhybrid models that integrate pretrained LMs with external knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2210.06349v2.pdf"
    },
    {
        "title": "SQuId: Measuring Speech Naturalness in Many Languages",
        "authors": [
            "Thibault Sellam",
            "Ankur Bapna",
            "Joshua Camp",
            "Diana Mackinnon",
            "Ankur P. Parikh",
            "Jason Riesa"
        ],
        "published": "2022-10-12T15:43:09Z",
        "summary": "Much of text-to-speech research relies on human evaluation, which incurs\nheavy costs and slows down the development process. The problem is particularly\nacute in heavily multilingual applications, where recruiting and polling judges\ncan take weeks. We introduce SQuId (Speech Quality Identification), a\nmultilingual naturalness prediction model trained on over a million ratings and\ntested in 65 locales-the largest effort of this type to date. The main insight\nis that training one model on many locales consistently outperforms mono-locale\nbaselines. We present our task, the model, and show that it outperforms a\ncompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then\ndemonstrate the effectiveness of cross-locale transfer during fine-tuning and\nhighlight its effect on zero-shot locales, i.e., locales for which there is no\nfine-tuning data. Through a series of analyses, we highlight the role of\nnon-linguistic effects such as sound artifacts in cross-locale transfer.\nFinally, we present the effect of our design decision, e.g., model size,\npre-training diversity, and language rebalancing with several ablation\nexperiments.",
        "pdf_link": "https://arxiv.org/pdf/2210.06324v2.pdf"
    },
    {
        "title": "Language Models are Realistic Tabular Data Generators",
        "authors": [
            "Vadim Borisov",
            "Kathrin Se\u00dfler",
            "Tobias Leemann",
            "Martin Pawelczyk",
            "Gjergji Kasneci"
        ],
        "published": "2022-10-12T15:03:28Z",
        "summary": "Tabular data is among the oldest and most ubiquitous forms of data. However,\nthe generation of synthetic samples with the original data's characteristics\nremains a significant challenge for tabular data. While many generative models\nfrom the computer vision domain, such as variational autoencoders or generative\nadversarial networks, have been adapted for tabular data generation, less\nresearch has been directed towards recent transformer-based large language\nmodels (LLMs), which are also generative in nature. To this end, we propose\nGReaT (Generation of Realistic Tabular data), which exploits an auto-regressive\ngenerative LLM to sample synthetic and yet highly realistic tabular data.\nFurthermore, GReaT can model tabular data distributions by conditioning on any\nsubset of features; the remaining features are sampled without additional\noverhead. We demonstrate the effectiveness of the proposed approach in a series\nof experiments that quantify the validity and quality of the produced data\nsamples from multiple angles. We find that GReaT maintains state-of-the-art\nperformance across numerous real-world and synthetic data sets with\nheterogeneous feature types coming in various sizes.",
        "pdf_link": "https://arxiv.org/pdf/2210.06280v2.pdf"
    },
    {
        "title": "Zero-Shot On-the-Fly Event Schema Induction",
        "authors": [
            "Rotem Dror",
            "Haoyu Wang",
            "Dan Roth"
        ],
        "published": "2022-10-12T14:37:00Z",
        "summary": "What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.",
        "pdf_link": "https://arxiv.org/pdf/2210.06254v2.pdf"
    },
    {
        "title": "A context-aware knowledge transferring strategy for CTC-based ASR",
        "authors": [
            "Ke-Han Lu",
            "Kuan-Yu Chen"
        ],
        "published": "2022-10-12T14:31:38Z",
        "summary": "Non-autoregressive automatic speech recognition (ASR) modeling has received\nincreasing attention recently because of its fast decoding speed and superior\nperformance. Among representatives, methods based on the connectionist temporal\nclassification (CTC) are still a dominating stream. However, the theoretically\ninherent flaw, the assumption of independence between tokens, creates a\nperformance barrier for the school of works. To mitigate the challenge, we\npropose a context-aware knowledge transferring strategy, consisting of a\nknowledge transferring module and a context-aware training strategy, for\nCTC-based ASR. The former is designed to distill linguistic information from a\npre-trained language model, and the latter is framed to modulate the\nlimitations caused by the conditional independence assumption. As a result, a\nknowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is\npresented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2\ndatasets demonstrate the effectiveness of the proposed method.",
        "pdf_link": "https://arxiv.org/pdf/2210.06244v1.pdf"
    },
    {
        "title": "Improved Data Augmentation for Translation Suggestion",
        "authors": [
            "Hongxiao Zhang",
            "Siyu Lai",
            "Songming Zhang",
            "Hui Huang",
            "Yufeng Chen",
            "Jinan Xu",
            "Jian Liu"
        ],
        "published": "2022-10-12T12:46:43Z",
        "summary": "Translation suggestion (TS) models are used to automatically provide\nalternative suggestions for incorrect spans in sentences generated by machine\ntranslation. This paper introduces the system used in our submission to the\nWMT'22 Translation Suggestion shared task. Our system is based on the ensemble\nof different translation architectures, including Transformer, SA-Transformer,\nand DynamicConv. We use three strategies to construct synthetic data from\nparallel corpora to compensate for the lack of supervised data. In addition, we\nintroduce a multi-phase pre-training strategy, adding an additional\npre-training phase with in-domain data. We rank second and third on the\nEnglish-German and English-Chinese bidirectional tasks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2210.06138v1.pdf"
    },
    {
        "title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning",
        "authors": [
            "Hui-Chi Kuo",
            "Yun-Nung Chen"
        ],
        "published": "2022-10-12T03:33:49Z",
        "summary": "Intelligent virtual assistants are currently designed to perform tasks or\nservices explicitly mentioned by users, so multiple related domains or tasks\nneed to be performed one by one through a long conversation with many explicit\nintents. Instead, human assistants are capable of reasoning (multiple) implicit\nintents based on user utterances via commonsense knowledge, reducing complex\ninteractions and improving practicality. Therefore, this paper proposes a\nframework of multi-domain dialogue systems, which can automatically infer\nimplicit intents based on user utterances and then perform zero-shot prompting\nusing a large pre-trained language model to trigger suitable single\ntask-oriented bots. The proposed framework is demonstrated effective to realize\nimplicit intents and recommend associated bots in a zero-shot manner.",
        "pdf_link": "https://arxiv.org/pdf/2210.05901v2.pdf"
    },
    {
        "title": "AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning",
        "authors": [
            "Tao Yang",
            "Jinghao Deng",
            "Xiaojun Quan",
            "Qifan Wang",
            "Shaoliang Nie"
        ],
        "published": "2022-10-12T02:54:41Z",
        "summary": "Fine-tuning large pre-trained language models on downstream tasks is apt to\nsuffer from overfitting when limited training data is available. While dropout\nproves to be an effective antidote by randomly dropping a proportion of units,\nexisting research has not examined its effect on the self-attention mechanism.\nIn this paper, we investigate this problem through self-attention attribution\nand find that dropping attention positions with low attribution scores can\naccelerate training and increase the risk of overfitting. Motivated by this\nobservation, we propose Attribution-Driven Dropout (AD-DROP), which randomly\ndiscards some high-attribution positions to encourage the model to make\npredictions by relying more on low-attribution positions to reduce overfitting.\nWe also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to\navoid dropping high-attribution positions excessively. Extensive experiments on\nvarious benchmarks show that AD-DROP yields consistent improvements over\nbaselines. Analysis further confirms that AD-DROP serves as a strategic\nregularizer to prevent overfitting during fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2210.05883v1.pdf"
    },
    {
        "title": "MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score",
        "authors": [
            "Sunjae Kwon",
            "Zonghai Yao",
            "Harmon S. Jordan",
            "David A. Levy",
            "Brian Corner",
            "Hong Yu"
        ],
        "published": "2022-10-12T02:27:32Z",
        "summary": "This paper proposes a new natural language processing (NLP) application for\nidentifying medical jargon terms potentially difficult for patients to\ncomprehend from electronic health record (EHR) notes. We first present a novel\nand publicly available dataset with expert-annotated medical jargon terms from\n18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon\nextraction ($MedJEx$) model which has been shown to outperform existing\nstate-of-the-art NLP models. First, MedJEx improved the overall performance\nwhen it was trained on an auxiliary Wikipedia hyperlink span dataset, where\nhyperlink spans provide additional Wikipedia articles to explain the spans (or\nterms), and then fine-tuned on the annotated MedJ data. Secondly, we found that\na contextualized masked language model score was beneficial for detecting\ndomain-specific unfamiliar jargon terms. Moreover, our results show that\ntraining on the auxiliary Wikipedia hyperlink span datasets improved six out of\neight biomedical named entity recognition benchmark datasets. Both MedJ and\nMedJEx are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2210.05875v1.pdf"
    },
    {
        "title": "SEAL : Interactive Tool for Systematic Error Analysis and Labeling",
        "authors": [
            "Nazneen Rajani",
            "Weixin Liang",
            "Lingjiao Chen",
            "Meg Mitchell",
            "James Zou"
        ],
        "published": "2022-10-11T23:51:44Z",
        "summary": "With the advent of Transformers, large language models (LLMs) have saturated\nwell-known NLP benchmarks and leaderboards with high aggregate performance.\nHowever, many times these models systematically fail on tail data or rare\ngroups not obvious in aggregate evaluation. Identifying such problematic data\ngroups is even more challenging when there are no explicit labels (e.g.,\nethnicity, gender, etc.) and further compounded for NLP datasets due to the\nlack of visual features to characterize failure modes (e.g., Asian males,\nanimals indoors, waterbirds on land, etc.). This paper introduces an\ninteractive Systematic Error Analysis and Labeling (\\seal) tool that uses a\ntwo-step approach to first identify high error slices of data and then, in the\nsecond step, introduce methods to give human-understandable semantics to those\nunderperforming slices. We explore a variety of methods for coming up with\ncoherent semantics for the error groups using language models for semantic\nlabeling and a text-to-image model for generating visual features. SEAL toolkit\nand demo screencast is available at https://huggingface.co/spaces/nazneen/seal.",
        "pdf_link": "https://arxiv.org/pdf/2210.05839v1.pdf"
    },
    {
        "title": "CLIP also Understands Text: Prompting CLIP for Phrase Understanding",
        "authors": [
            "An Yan",
            "Jiacheng Li",
            "Wanrong Zhu",
            "Yujie Lu",
            "William Yang Wang",
            "Julian McAuley"
        ],
        "published": "2022-10-11T23:35:18Z",
        "summary": "Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.05836v1.pdf"
    },
    {
        "title": "Cross-Lingual Speaker Identification Using Distant Supervision",
        "authors": [
            "Ben Zhou",
            "Dian Yu",
            "Dong Yu",
            "Dan Roth"
        ],
        "published": "2022-10-11T20:49:44Z",
        "summary": "Speaker identification, determining which character said each utterance in\nliterary text, benefits many downstream tasks. Most existing approaches use\nexpert-defined rules or rule-based features to directly approach this task, but\nthese approaches come with significant drawbacks, such as lack of contextual\nreasoning and poor cross-lingual generalization. In this work, we propose a\nspeaker identification framework that addresses these issues. We first extract\nlarge-scale distant supervision signals in English via general-purpose tools\nand heuristics, and then apply these weakly-labeled instances with a focus on\nencouraging contextual reasoning to train a cross-lingual language model. We\nshow that the resulting model outperforms previous state-of-the-art methods on\ntwo English speaker identification benchmarks by up to 9% in accuracy and 5%\nwith only distant supervision, as well as two Chinese speaker identification\ndatasets by up to 4.7%.",
        "pdf_link": "https://arxiv.org/pdf/2210.05780v1.pdf"
    },
    {
        "title": "Visual Language Maps for Robot Navigation",
        "authors": [
            "Chenguang Huang",
            "Oier Mees",
            "Andy Zeng",
            "Wolfram Burgard"
        ],
        "published": "2022-10-11T18:13:20Z",
        "summary": "Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https://vlmaps.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2210.05714v4.pdf"
    },
    {
        "title": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory",
        "authors": [
            "Nur Muhammad Mahi Shafiullah",
            "Chris Paxton",
            "Lerrel Pinto",
            "Soumith Chintala",
            "Arthur Szlam"
        ],
        "published": "2022-10-11T17:57:10Z",
        "summary": "We propose CLIP-Fields, an implicit scene model that can be used for a\nvariety of tasks, such as segmentation, instance identification, semantic\nsearch over space, and view localization. CLIP-Fields learns a mapping from\nspatial locations to semantic embedding vectors. Importantly, we show that this\nmapping can be trained with supervision coming only from web-image and web-text\ntrained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct\nhuman supervision. When compared to baselines like Mask-RCNN, our method\noutperforms on few-shot instance identification or semantic segmentation on the\nHM3D dataset with only a fraction of the examples. Finally, we show that using\nCLIP-Fields as a scene memory, robots can perform semantic navigation in\nreal-world environments. Our code and demonstration videos are available here:\nhttps://mahis.life/clip-fields",
        "pdf_link": "https://arxiv.org/pdf/2210.05663v3.pdf"
    },
    {
        "title": "A Kernel-Based View of Language Model Fine-Tuning",
        "authors": [
            "Sadhika Malladi",
            "Alexander Wettig",
            "Dingli Yu",
            "Danqi Chen",
            "Sanjeev Arora"
        ],
        "published": "2022-10-11T17:34:32Z",
        "summary": "It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam\nand use Tensor Programs (Yang, 2020) to characterize conditions under which the\nNTK lens may describe fine-tuning updates to pre-trained language models.\nExtensive experiments on 14 NLP tasks validate our theory and show that\nformulating the downstream task as a masked word prediction problem through\nprompting often induces kernel-based dynamics during fine-tuning. Finally, we\nuse this kernel view to propose an explanation for the success of\nparameter-efficient subspace-based fine-tuning methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.05643v4.pdf"
    },
    {
        "title": "Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models",
        "authors": [
            "Isabel Papadimitriou",
            "Kezia Lopez",
            "Dan Jurafsky"
        ],
        "published": "2022-10-11T17:06:38Z",
        "summary": "While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control language model. With our case\nstudies, we hope to bring to light the fine-grained ways in which multilingual\nmodels can be biased,and encourage more linguistically-aware fluency\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2210.05619v2.pdf"
    },
    {
        "title": "Continual Training of Language Models for Few-Shot Learning",
        "authors": [
            "Zixuan Ke",
            "Haowei Lin",
            "Yijia Shao",
            "Hu Xu",
            "Lei Shu",
            "Bing Liu"
        ],
        "published": "2022-10-11T15:43:58Z",
        "summary": "Recent work on applying large language models (LMs) achieves impressive\nperformance in many NLP applications. Adapting or posttraining an LM using an\nunlabeled domain corpus can produce even better performance for end-tasks in\nthe domain. This paper proposes the problem of continually extending an LM by\nincrementally post-train the LM with a sequence of unlabeled domain corpora to\nexpand its knowledge without forgetting its previous skills. The goal is to\nimprove the few-shot end-task learning in these domains. The resulting system\nis called CPT (Continual PostTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental results verify its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2210.05549v1.pdf"
    },
    {
        "title": "Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration",
        "authors": [
            "Matteo Paltenghi",
            "Rahul Pandita",
            "Austin Z. Henley",
            "Albert Ziegler"
        ],
        "published": "2022-10-11T14:58:58Z",
        "summary": "The high effectiveness of neural models of code, such as OpenAI Codex and\nAlphaCode, suggests coding capabilities of models that are at least comparable\nto those of humans. However, previous work has only used these models for their\nraw completion, ignoring how the model reasoning, in the form of attention\nweights, can be used for other downstream tasks. Disregarding the attention\nweights means discarding a considerable portion of what those models compute\nwhen queried. To profit more from the knowledge embedded in these large\npre-trained models, this work compares multiple approaches to post-process\nthese valuable attention weights for supporting code exploration. Specifically,\nwe compare to which extent the transformed attention signal of CodeGen, a large\nand publicly available pretrained neural model, agrees with how developers look\nat and explore code when each answering the same sense-making questions about\ncode. At the core of our experimental evaluation, we collect, manually\nannotate, and open-source a novel eye-tracking dataset comprising 25 developers\nanswering sense-making questions on code over 92 sessions. We empirically\nevaluate five attention-agnostic heuristics and ten attention-based post\nprocessing approaches of the attention signal against our ground truth of\ndevelopers exploring code, including the novel concept of follow-up attention\nwhich exhibits the highest agreement. Beyond the dataset contribution and the\nempirical study, we also introduce a novel practical application of the\nattention signal of pre-trained models with completely analytical solutions,\ngoing beyond how neural models' attention mechanisms have traditionally been\nused.",
        "pdf_link": "https://arxiv.org/pdf/2210.05506v1.pdf"
    },
    {
        "title": "Like a bilingual baby: The advantage of visually grounding a bilingual language model",
        "authors": [
            "Khai-Nguyen Nguyen",
            "Zixin Tang",
            "Ankur Mali",
            "Alex Kelly"
        ],
        "published": "2022-10-11T14:43:26Z",
        "summary": "Unlike most neural language models, humans learn language in a rich,\nmulti-sensory and, often, multi-lingual environment. Current language models\ntypically fail to fully capture the complexities of multilingual language use.\nWe train an LSTM language model on images and captions in English and Spanish\nfrom MS-COCO-ES. We find that the visual grounding improves the model's\nunderstanding of semantic similarity both within and across languages and\nimproves perplexity. However, we find no significant advantage of visual\ngrounding for abstract words. Our results provide additional evidence of the\nadvantages of visually grounded language models and point to the need for more\nnaturalistic language data from multilingual speakers and multilingual datasets\nwith perceptual grounding.",
        "pdf_link": "https://arxiv.org/pdf/2210.05487v2.pdf"
    },
    {
        "title": "Instance Regularization for Discriminative Language Model Pre-training",
        "authors": [
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Ming Zhou"
        ],
        "published": "2022-10-11T14:16:37Z",
        "summary": "Discriminative pre-trained language models (PrLMs) can be generalized as\ndenoising auto-encoders that work with two procedures, ennoising and denoising.\nFirst, an ennoising process corrupts texts with arbitrary noising functions to\nconstruct training instances. Then, a denoising language model is trained to\nrestore the corrupted tokens. Existing studies have made progress by optimizing\nindependent strategies of either ennoising or denosing. They treat training\ninstances equally throughout the training process, with little attention on the\nindividual contribution of those instances. To model explicit signals of\ninstance contribution, this work proposes to estimate the complexity of\nrestoring the original sentences from corrupted ones in language model\npre-training. The estimations involve the corruption degree in the ennoising\ndata construction process and the prediction confidence in the denoising\ncounterpart. Experimental results on natural language understanding and reading\ncomprehension benchmarks show that our approach improves pre-training\nefficiency, effectiveness, and robustness. Code is publicly available at\nhttps://github.com/cooelf/InstanceReg",
        "pdf_link": "https://arxiv.org/pdf/2210.05471v1.pdf"
    },
    {
        "title": "Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization",
        "authors": [
            "Hadi Abdine",
            "Moussa Kamal Eddine",
            "Michalis Vazirgiannis",
            "Davide Buscaldi"
        ],
        "published": "2022-10-11T13:04:06Z",
        "summary": "Word sense induction (WSI) is a difficult problem in natural language\nprocessing that involves the unsupervised automatic detection of a word's\nsenses (i.e. meanings). Recent work achieves significant results on the WSI\ntask by pre-training a language model that can exclusively disambiguate word\nsenses, whereas others employ previously pre-trained language models in\nconjunction with additional strategies to induce senses. In this paper, we\npropose a novel unsupervised method based on hierarchical clustering and\ninvariant information clustering (IIC). The IIC is used to train a small model\nto optimize the mutual information between two vector representations of a\ntarget word occurring in a pair of synthetic paraphrases. This model is later\nused in inference mode to extract a higher quality vector representation to be\nused in the hierarchical clustering. We evaluate our method on two WSI tasks\nand in two distinct clustering configurations (fixed and dynamic number of\nclusters). We empirically demonstrate that, in certain cases, our approach\noutperforms prior WSI state-of-the-art methods, while in others, it achieves a\ncompetitive performance.",
        "pdf_link": "https://arxiv.org/pdf/2210.05422v1.pdf"
    },
    {
        "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation",
        "authors": [
            "Ruibo Liu",
            "Jason Wei",
            "Shixiang Shane Gu",
            "Te-Yen Wu",
            "Soroush Vosoughi",
            "Claire Cui",
            "Denny Zhou",
            "Andrew M. Dai"
        ],
        "published": "2022-10-11T11:39:23Z",
        "summary": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.",
        "pdf_link": "https://arxiv.org/pdf/2210.05359v1.pdf"
    },
    {
        "title": "On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding",
        "authors": [
            "Ga\u00eblle Laperri\u00e8re",
            "Valentin Pelloin",
            "Micka\u00ebl Rouvier",
            "Themos Stafylakis",
            "Yannick Est\u00e8ve"
        ],
        "published": "2022-10-11T09:40:34Z",
        "summary": "In this paper we examine the use of semantically-aligned speech\nrepresentations for end-to-end spoken language understanding (SLU). We employ\nthe recently-introduced SAMU-XLSR model, which is designed to generate a single\nembedding that captures the semantics at the utterance level, semantically\naligned across different languages. This model combines the acoustic\nframe-level speech representation learning model (XLS-R) with the Language\nAgnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the\nSAMU-XLSR model instead of the initial XLS-R model improves significantly the\nperformance in the framework of end-to-end SLU. Finally, we present the\nbenefits of using this model towards language portability in SLU.",
        "pdf_link": "https://arxiv.org/pdf/2210.05291v1.pdf"
    },
    {
        "title": "Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training",
        "authors": [
            "Taolin Zhang",
            "Junwei Dong",
            "Jianing Wang",
            "Chengyu Wang",
            "Ang Wang",
            "Yinghui Liu",
            "Jun Huang",
            "Yong Li",
            "Xiaofeng He"
        ],
        "published": "2022-10-11T09:34:21Z",
        "summary": "Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve\ncontext-aware representations via learning from structured relations in\nknowledge graphs, and/or linguistic knowledge from syntactic or dependency\nanalysis. Unlike English, there is a lack of high-performing open-source\nChinese KEPLMs in the natural language processing (NLP) community to support\nvarious language understanding applications. In this paper, we revisit and\nadvance the development of Chinese natural language understanding with a series\nof novel Chinese KEPLMs released in various parameter sizes, namely CKBERT\n(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic\nknowledge is effectively injected into CKBERT based on two novel pre-training\ntasks, i.e., linguistic-aware masked language modeling and contrastive\nmulti-hop relation modeling. Based on the above two pre-training paradigms and\nour in-house implemented TorchAccelerator, we have pre-trained base (110M),\nlarge (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.\nExperiments demonstrate that CKBERT outperforms strong baselines for Chinese\nover various benchmark NLP tasks and in terms of different model sizes.",
        "pdf_link": "https://arxiv.org/pdf/2210.05287v2.pdf"
    },
    {
        "title": "Rethinking the Event Coding Pipeline with Prompt Entailment",
        "authors": [
            "Cl\u00e9ment Lefebvre",
            "Niklas Stoehr"
        ],
        "published": "2022-10-11T08:38:48Z",
        "summary": "For monitoring crises, political events are extracted from the news. The\nlarge amount of unstructured full-text event descriptions makes a case-by-case\nanalysis unmanageable, particularly for low-resource humanitarian aid\norganizations. This creates a demand to classify events into event types, a\ntask referred to as event coding. Typically, domain experts craft an event type\nontology, annotators label a large dataset and technical experts develop a\nsupervised coding system. In this work, we propose PR-ENT, a new event coding\napproach that is more flexible and resource-efficient, while maintaining\ncompetitive accuracy: first, we extend an event description such as \"Military\ninjured two civilians'' by a template, e.g. \"People were [Z]\" and prompt a\npre-trained (cloze) language model to fill the slot Z. Second, we select answer\ncandidates Z* = {\"injured'', \"hurt\"...} by treating the event description as\npremise and the filled templates as hypothesis in a textual entailment task.\nThis allows domain experts to draft the codebook directly as labeled prompts\nand interpretable answer candidates. This human-in-the-loop process is guided\nby our interactive codebook design tool. We evaluate PR-ENT in several\nrobustness checks: perturbing the event description and prompt template,\nrestricting the vocabulary and removing contextual information.",
        "pdf_link": "https://arxiv.org/pdf/2210.05257v2.pdf"
    },
    {
        "title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models",
        "authors": [
            "Yuanxin Liu",
            "Fandong Meng",
            "Zheng Lin",
            "Jiangnan Li",
            "Peng Fu",
            "Yanan Cao",
            "Weiping Wang",
            "Jie Zhou"
        ],
        "published": "2022-10-11T07:26:34Z",
        "summary": "Despite the remarkable success of pre-trained language models (PLMs), they\nstill face two challenges: First, large-scale PLMs are inefficient in terms of\nmemory footprint and computation. Second, on the downstream tasks, PLMs tend to\nrely on the dataset bias and struggle to generalize to out-of-distribution\n(OOD) data. In response to the efficiency problem, recent studies show that\ndense PLMs can be replaced with sparse subnetworks without hurting the\nperformance. Such subnetworks can be found in three scenarios: 1) the\nfine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even\ninside 3) PLMs without any parameter fine-tuning. However, these results are\nonly obtained in the in-distribution (ID) setting. In this paper, we extend the\nstudy on PLMs subnetworks to the OOD setting, investigating whether sparsity\nand robustness to dataset bias can be achieved simultaneously. To this end, we\nconduct extensive experiments with the pre-trained BERT model on three natural\nlanguage understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse\nand robust subnetworks (SRNets) can consistently be found in BERT}, across the\naforementioned three scenarios, using different training and compression\nmethods. Furthermore, we explore the upper bound of SRNets using the OOD\ninformation and show that \\textbf{there exist sparse and almost unbiased BERT\nsubnetworks}. Finally, we present 1) an analytical study that provides insights\non how to promote the efficiency of SRNets searching process and 2) a solution\nto improve subnetworks' performance at high sparsity. The code is available at\nhttps://github.com/llyx97/sparse-and-robust-PLM.",
        "pdf_link": "https://arxiv.org/pdf/2210.05211v1.pdf"
    },
    {
        "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
        "authors": [
            "Zhaowei Wang"
        ],
        "published": "2022-10-11T06:47:23Z",
        "summary": "Legal case retrieval, which aims to retrieve relevant cases given a query\ncase, plays an essential role in the legal system. While recent research\nefforts improve the performance of traditional ad-hoc retrieval models, legal\ncase retrieval is still challenging since queries are legal cases, which\ncontain hundreds of tokens. Legal cases are much longer and more complicated\nthan keywords queries. Apart from that, the definition of legal relevance is\nbeyond the general definition. In addition to general topical relevance, the\nrelevant cases also involve similar situations and legal elements, which can\nsupport the judgment of the current case. In this paper, we propose an\ninteraction-focused network for legal case retrieval with a multi-view\ncontrastive learning objective. The contrastive learning views, including\ncase-view and element-view, aim to overcome the above challenges. The case-view\ncontrastive learning minimizes the hidden space distance between relevant legal\ncase representations produced by a pre-trained language model (PLM) encoder.\nThe element-view builds positive and negative instances by changing legal\nelements of cases to help the network better compute legal relevance. To\nachieve this, we employ a legal element knowledge-aware indicator to detect\nlegal elements of cases. We conduct extensive experiments on the benchmark of\nrelevant case retrieval. Evaluation results indicate our proposed method\nobtains significant improvement over the existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.05188v1.pdf"
    },
    {
        "title": "Retrieval Augmentation for T5 Re-ranker using External Sources",
        "authors": [
            "Kai Hui",
            "Tao Chen",
            "Zhen Qin",
            "Honglei Zhuang",
            "Fernando Diaz",
            "Mike Bendersky",
            "Don Metzler"
        ],
        "published": "2022-10-11T04:54:19Z",
        "summary": "Retrieval augmentation has shown promising improvements in different tasks.\nHowever, whether such augmentation can assist a large language model based\nre-ranker remains unclear. We investigate how to augment T5-based re-rankers\nusing high-quality information retrieved from two external corpora -- a\ncommercial web search engine and Wikipedia. We empirically demonstrate how\nretrieval augmentation can substantially improve the effectiveness of T5-based\nre-rankers for both in-domain and zero-shot out-of-domain re-ranking tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.05145v1.pdf"
    },
    {
        "title": "HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea",
        "authors": [
            "Haneul Yoo",
            "Jiho Jin",
            "Juhee Son",
            "JinYeong Bak",
            "Kyunghyun Cho",
            "Alice Oh"
        ],
        "published": "2022-10-11T03:04:28Z",
        "summary": "Historical records in Korea before the 20th century were primarily written in\nHanja, an extinct language based on Chinese characters and not understood by\nmodern Korean or Chinese speakers. Historians with expertise in this time\nperiod have been analyzing the documents, but that process is very difficult\nand time-consuming, and language models would significantly speed up the\nprocess. Toward building and evaluating language models for Hanja, we release\nthe Hanja Understanding Evaluation dataset consisting of chronological\nattribution, topic classification, named entity recognition, and summary\nretrieval tasks. We also present BERT-based models continued training on the\ntwo major corpora from the 14th to the 19th centuries: the Annals of the Joseon\nDynasty and Diaries of the Royal Secretariats. We compare the models with\nseveral baselines on all tasks and show there are significant improvements\ngained by training on the two corpora. Additionally, we run zero-shot\nexperiments on the Daily Records of the Royal Court and Important Officials\n(DRRI). The DRRI dataset has not been studied much by the historians, and not\nat all by the NLP community.",
        "pdf_link": "https://arxiv.org/pdf/2210.05112v1.pdf"
    },
    {
        "title": "Pre-Training Representations of Binary Code Using Contrastive Learning",
        "authors": [
            "Yifan Zhang",
            "Chen Huang",
            "Yueke Zhang",
            "Kevin Cao",
            "Scott Thomas Andersen",
            "Huajie Shao",
            "Kevin Leach",
            "Yu Huang"
        ],
        "published": "2022-10-11T02:39:06Z",
        "summary": "Compiled software is delivered as executable binary code. Developers write\nsource code to express the software semantics, but the compiler converts it to\na binary format that the CPU can directly execute. Therefore, binary code\nanalysis is critical to applications in reverse engineering and computer\nsecurity tasks where source code is not available. However, unlike source code\nand natural language that contain rich semantic information, binary code is\ntypically difficult for human engineers to understand and analyze. While\nexisting work uses AI models to assist source code analysis, few studies have\nconsidered binary code. In this paper, we propose a COntrastive learning Model\nfor Binary cOde Analysis, or COMBO, that incorporates source code and comment\ninformation into binary code during representation learning. Specifically, we\npresent three components in COMBO: (1) a primary contrastive learning method\nfor cold-start pre-training, (2) a simplex interpolation method to incorporate\nsource code, comments, and binary code, and (3) an intermediate representation\nlearning algorithm to provide binary code embeddings. Finally, we evaluate the\neffectiveness of the pre-trained representations produced by COMBO using three\nindicative downstream tasks relating to binary code: algorithmic functionality\nclassification, binary code similarity, and vulnerability detection. Our\nexperimental results show that COMBO facilitates representation learning of\nbinary code visualized by distribution analysis, and improves the performance\non all three downstream tasks by 5.45% on average compared to state-of-the-art\nlarge-scale language representation models. To the best of our knowledge, COMBO\nis the first language representation model that incorporates source code,\nbinary code, and comments into contrastive code representation learning and\nunifies multiple tasks for binary code analysis.",
        "pdf_link": "https://arxiv.org/pdf/2210.05102v2.pdf"
    },
    {
        "title": "Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems",
        "authors": [
            "Fan Zhou",
            "Haoyu Dong",
            "Qian Liu",
            "Zhoujun Cheng",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022-10-11T00:57:19Z",
        "summary": "Numerical reasoning over natural language has been a long-standing goal for\nthe research community. However, cutting-edge language models have proven\ndifficult to reliably generalize to a broad range of numbers, although they\nhave shown proficiency in reasoning over common and simple numbers. In this\npaper, we propose a novel method to elicit and exploit the numerical reasoning\nknowledge hidden in pre-trained language models using simple anchor numbers.\nConcretely, we first leverage simple numbers as anchors to probe the implicitly\ninferred arithmetic expressions from language models, and then explicitly apply\nthe expressions on complex numbers to get corresponding answers. To inversely\nelicit arithmetic expressions, we transform and formulate the task as an\nanalytically solvable linear system. Experimental results on several numerical\nreasoning benchmarks demonstrate that our approach significantly improves\nnumerical reasoning capabilities of existing LMs. More importantly, our\napproach is training-free and simply works in the inference phase, making it\nhighly portable and achieving consistent performance benefits across a variety\nof language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and\nfine-tuning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2210.05075v1.pdf"
    },
    {
        "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling",
        "authors": [
            "Haw-Shiuan Chang",
            "Ruei-Yao Sun",
            "Kathryn Ricci",
            "Andrew McCallum"
        ],
        "published": "2022-10-10T23:15:17Z",
        "summary": "Ensembling BERT models often significantly improves accuracy, but at the cost\nof significantly more computation and memory footprint. In this work, we\npropose Multi-CLS BERT, a novel ensembling method for CLS-based prediction\ntasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses\nmultiple CLS tokens with a parameterization and objective that encourages their\ndiversity. Thus instead of fine-tuning each BERT model in an ensemble (and\nrunning them all at test time), we need only fine-tune our single Multi-CLS\nBERT model (and run the one model at test time, ensembling just the multiple\nfinal CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on\ntop of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and\nRudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS\nBERT reliably improves both overall accuracy and confidence estimation. When\nonly 100 training samples are available in GLUE, the Multi-CLS BERT_Base model\ncan even outperform the corresponding BERT_Large model. We analyze the behavior\nof our Multi-CLS BERT, showing that it has many of the same characteristics and\nbehavior as a typical BERT 5-way ensemble, but with nearly 4-times less\ncomputation and memory.",
        "pdf_link": "https://arxiv.org/pdf/2210.05043v2.pdf"
    },
    {
        "title": "Generating Executable Action Plans with Environmentally-Aware Language Models",
        "authors": [
            "Maitrey Gramopadhye",
            "Daniel Szafir"
        ],
        "published": "2022-10-10T18:56:57Z",
        "summary": "Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable,\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat agents are better able to execute. Our approach involves integrating\nenvironmental objects and object relations as additional inputs into LLM action\nplan generation to provide the system with an awareness of its surroundings,\nresulting in plans where each generated action is mapped to objects present in\nthe scene. We also design a novel scoring function that, along with generating\nthe action steps and associating them with objects, helps the system\ndisambiguate among object instances and take into account their states. We\nevaluated our approach using the VirtualHome simulator and the ActivityPrograms\nknowledge base and found that action plans generated from our system had a 310%\nimprovement in executability and a 147% improvement in correctness over prior\nwork. The complete code and a demo of our method is publicly available at\nhttps://github.com/hri-ironlab/scene_aware_language_planner.",
        "pdf_link": "https://arxiv.org/pdf/2210.04964v2.pdf"
    },
    {
        "title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
        "authors": [
            "Tanay Dixit",
            "Bhargavi Paranjape",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022-10-10T17:45:38Z",
        "summary": "Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed\ninputs during training -- helps reduce model reliance on spurious correlations\nand improves generalization to out-of-distribution (OOD) data. Prior work on\ngenerating counterfactuals only considered restricted classes of perturbations,\nlimiting their effectiveness. We present COunterfactual Generation via\nRetrieval and Editing (CORE), a retrieval-augmented generation framework for\ncreating diverse counterfactual perturbations for CDA. For each training\nexample, CORE first performs a dense retrieval over a task-related unlabeled\ntext corpus using a learned bi-encoder and extracts relevant counterfactual\nexcerpts. CORE then incorporates these into prompts to a large language model\nwith few-shot learning capabilities, for counterfactual editing. Conditioning\nlanguage model edits on naturally occurring data results in diverse\nperturbations. Experiments on natural language inference and sentiment analysis\nbenchmarks show that CORE counterfactuals are more effective at improving\ngeneralization to OOD data compared to other DA approaches. We also show that\nthe CORE retrieval framework can be used to encourage diversity in manually\nauthored perturbations",
        "pdf_link": "https://arxiv.org/pdf/2210.04873v2.pdf"
    },
    {
        "title": "Transformer-based Localization from Embodied Dialog with Large-scale Pre-training",
        "authors": [
            "Meera Hahn",
            "James M. Rehg"
        ],
        "published": "2022-10-10T17:25:06Z",
        "summary": "We address the challenging task of Localization via Embodied Dialog (LED).\nGiven a dialog from two agents, an Observer navigating through an unknown\nenvironment and a Locator who is attempting to identify the Observer's\nlocation, the goal is to predict the Observer's final location in a map. We\ndevelop a novel LED-Bert architecture and present an effective pretraining\nstrategy. We show that a graph-based scene representation is more effective\nthan the top-down 2D maps used in prior works. Our approach outperforms\nprevious baselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.04864v1.pdf"
    },
    {
        "title": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks",
        "authors": [
            "Charith Peris",
            "Lizhen Tan",
            "Thomas Gueudre",
            "Turan Gojayev",
            "Pan Wei",
            "Gokmen Oz"
        ],
        "published": "2022-10-10T16:49:52Z",
        "summary": "Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2210.04834v3.pdf"
    },
    {
        "title": "Long N-step Surrogate Stage Reward to Reduce Variances of Deep Reinforcement Learning in Complex Problems",
        "authors": [
            "Junmin Zhong",
            "Ruofan Wu",
            "Jennie Si"
        ],
        "published": "2022-10-10T16:32:10Z",
        "summary": "High variances in reinforcement learning have shown impeding successful\nconvergence and hurting task performance. As reward signal plays an important\nrole in learning behavior, multi-step methods have been considered to mitigate\nthe problem, and are believed to be more effective than single step methods.\nHowever, there is a lack of comprehensive and systematic study on this\nimportant aspect to demonstrate the effectiveness of multi-step methods in\nsolving highly complex continuous control problems. In this study, we introduce\na new long $N$-step surrogate stage (LNSS) reward approach to effectively\naccount for complex environment dynamics while previous methods are usually\nfeasible for limited number of steps. The LNSS method is simple, low\ncomputational cost, and applicable to value based or policy gradient\nreinforcement learning. We systematically evaluate LNSS in OpenAI Gym and\nDeepMind Control Suite to address some complex benchmark environments that have\nbeen challenging to obtain good results by DRL in general. We demonstrate\nperformance improvement in terms of total reward, convergence speed, and\ncoefficient of variation (CV) by LNSS. We also provide analytical insights on\nhow LNSS exponentially reduces the upper bound on the variances of Q value from\na respective single step method",
        "pdf_link": "https://arxiv.org/pdf/2210.04820v1.pdf"
    },
    {
        "title": "Readability Controllable Biomedical Document Summarization",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2022-10-10T14:03:20Z",
        "summary": "Different from general documents, it is recognised that the ease with which\npeople can understand a biomedical text is eminently varied, owing to the\nhighly technical nature of biomedical documents and the variance of readers'\ndomain knowledge. However, existing biomedical document summarization systems\nhave paid little attention to readability control, leaving users with summaries\nthat are incompatible with their levels of expertise. In recognition of this\nurgent demand, we introduce a new task of readability controllable\nsummarization for biomedical documents, which aims to recognise users'\nreadability demands and generate summaries that better suit their needs:\ntechnical summaries for experts and plain language summaries (PLS) for laymen.\nTo establish this task, we construct a corpus consisting of biomedical papers\nwith technical summaries and PLSs written by the authors, and benchmark\nmultiple advanced controllable abstractive and extractive summarization models\nbased on pre-trained language models (PLMs) with prevalent controlling and\ngeneration techniques. Moreover, we propose a novel masked language model (MLM)\nbased metric and its variant to effectively evaluate the readability\ndiscrepancy between lay and technical summaries. Experimental results from\nautomated and human evaluations show that though current control techniques\nallow for a certain degree of readability adjustment during generation, the\nperformance of existing controllable summarization methods is far from\ndesirable in this task.",
        "pdf_link": "https://arxiv.org/pdf/2210.04705v3.pdf"
    },
    {
        "title": "Bridging CLIP and StyleGAN through Latent Alignment for Image Editing",
        "authors": [
            "Wanfeng Zheng",
            "Qiang Li",
            "Xiaoyan Guo",
            "Pengfei Wan",
            "Zhongyuan Wang"
        ],
        "published": "2022-10-10T09:17:35Z",
        "summary": "Text-driven image manipulation is developed since the vision-language model\n(CLIP) has been proposed. Previous work has adopted CLIP to design a text-image\nconsistency-based objective to address this issue. However, these methods\nrequire either test-time optimization or image feature cluster analysis for\nsingle-mode manipulation direction. In this paper, we manage to achieve\ninference-time optimization-free diverse manipulation direction mining by\nbridging CLIP and StyleGAN through Latent Alignment (CSLA). More specifically,\nour efforts consist of three parts: 1) a data-free training strategy to train\nlatent mappers to bridge the latent space of CLIP and StyleGAN; 2) for more\nprecise mapping, temporal relative consistency is proposed to address the\nknowledge distribution bias problem among different latent spaces; 3) to refine\nthe mapped latent in s space, adaptive style mixing is also proposed. With this\nmapping scheme, we can achieve GAN inversion, text-to-image generation and\ntext-driven image manipulation. Qualitative and quantitative comparisons are\nmade to demonstrate the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2210.04506v1.pdf"
    },
    {
        "title": "DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities",
        "authors": [
            "Mohsinul Kabir",
            "Tasnim Ahmed",
            "Md. Bakhtiar Hasan",
            "Md Tahmid Rahman Laskar",
            "Tarun Kumar Joarder",
            "Hasan Mahmud",
            "Kamrul Hasan"
        ],
        "published": "2022-10-10T08:23:57Z",
        "summary": "Mental health research through data-driven methods has been hindered by a\nlack of standard typology and scarcity of adequate data. In this study, we\nleverage the clinical articulation of depression to build a typology for social\nmedia texts for detecting the severity of depression. It emulates the standard\nclinical assessment procedure Diagnostic and Statistical Manual of Mental\nDisorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle\nindications of depressive disorders from tweets. Along with the typology, we\npresent a new dataset of 40191 tweets labeled by expert annotators. Each tweet\nis labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels\nare considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.\nAn associated confidence score is provided with each label to validate the\nquality of annotation. We examine the quality of the dataset via representing\nsummary statistics while setting strong baseline results using attention-based\nmodels like BERT and DistilBERT. Finally, we extensively address the\nlimitations of the study to provide directions for further research.",
        "pdf_link": "https://arxiv.org/pdf/2210.05372v1.pdf"
    },
    {
        "title": "Parameter-Efficient Tuning with Special Token Adaptation",
        "authors": [
            "Xiaocong Yang",
            "James Y. Huang",
            "Wenxuan Zhou",
            "Muhao Chen"
        ],
        "published": "2022-10-10T01:02:51Z",
        "summary": "Parameter-efficient tuning aims at updating only a small subset of parameters\nwhen adapting a pretrained model to downstream tasks. In this work, we\nintroduce PASTA, in which we only modify the special token representations\n(e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer\nin Transformer-based models. PASTA achieves comparable performance to full\nfinetuning in natural language understanding tasks including text\nclassification and NER with up to only 0.029% of total parameters trained. Our\nwork not only provides a simple yet effective way of parameter-efficient\ntuning, which has a wide range of practical applications when deploying\nfinetuned models for multiple tasks, but also demonstrates the pivotal role of\nspecial tokens in pretrained language models",
        "pdf_link": "https://arxiv.org/pdf/2210.04382v2.pdf"
    },
    {
        "title": "FairGer: Using NLP to Measure Support for Women and Migrants in 155 Years of German Parliamentary Debates",
        "authors": [
            "Dominik Beese",
            "Ole P\u00fctz",
            "Steffen Eger"
        ],
        "published": "2022-10-09T22:02:58Z",
        "summary": "We measure support with women and migrants in German political debates over\nthe last 155 years. To do so, we (1) provide a gold standard of 1205 text\nsnippets in context, annotated for support with our target groups, (2) train a\nBERT model on our annotated data, with which (3) we infer large-scale trends.\nThese show that support with women is stronger than support with migrants, but\nboth have steadily increased over time. While we hardly find any direct\nanti-support with women, there is more polarization when it comes to migrants.\nWe also discuss the difficulty of annotation as a result of ambiguity in\npolitical discourse and indirectness, i.e., politicians' tendency to relate\nstances attributed to political opponents. Overall, our results indicate that\nGerman society, as measured from its political elite, has become fairer over\ntime.",
        "pdf_link": "https://arxiv.org/pdf/2210.04359v1.pdf"
    },
    {
        "title": "Quantifying Social Biases Using Templates is Unreliable",
        "authors": [
            "Preethi Seshadri",
            "Pouya Pezeshkpour",
            "Sameer Singh"
        ],
        "published": "2022-10-09T20:05:29Z",
        "summary": "Recently, there has been an increase in efforts to understand how large\nlanguage models (LLMs) propagate and amplify social biases. Several works have\nutilized templates for fairness evaluation, which allow researchers to quantify\nsocial biases in the absence of test sets with protected attribute labels.\nWhile template evaluation can be a convenient and helpful diagnostic tool to\nunderstand model deficiencies, it often uses a simplistic and limited set of\ntemplates. In this paper, we study whether bias measurements are sensitive to\nthe choice of templates used for benchmarking. Specifically, we investigate the\ninstability of bias measurements by manually modifying templates proposed in\nprevious works in a semantically-preserving manner and measuring bias across\nthese modifications. We find that bias values and resulting conclusions vary\nconsiderably across template modifications on four tasks, ranging from an 81%\nreduction (NLI) to a 162% increase (MLM) in (task-specific) bias measurements.\nOur results indicate that quantifying fairness in LLMs, as done in current\npractice, can be brittle and needs to be approached with more care and caution.",
        "pdf_link": "https://arxiv.org/pdf/2210.04337v1.pdf"
    },
    {
        "title": "ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models",
        "authors": [
            "Jiannan Xiang",
            "Zhengzhong Liu",
            "Yucheng Zhou",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "published": "2022-10-09T19:17:43Z",
        "summary": "Data-to-text generation is challenging due to the great variety of the input\ndata in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse\npredicates). Recent end-to-end neural methods thus require substantial training\nexamples to learn to disambiguate and describe the data. Yet, real-world\ndata-to-text problems often suffer from various data-scarce issues: one may\nhave access to only a handful of or no training examples, and/or have to rely\non examples in a different domain or schema. To fill this gap, we propose\nAny-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse\nsettings by making efficient use of any given (or no) examples. ASDOT consists\nof two steps, data disambiguation and sentence fusion, both of which are\namenable to be solved with off-the-shelf pretrained language models (LMs) with\noptional finetuning. In the data disambiguation stage, we employ the prompted\nGPT-3 model to understand possibly ambiguous triples from the input data and\nconvert each into a short sentence with reduced ambiguity. The sentence fusion\nstage then uses an LM like T5 to fuse all the resulting sentences into a\ncoherent paragraph as the final description. We evaluate extensively on various\ndatasets in different scenarios, including the zero-/few-/full-shot settings,\nand generalization to unseen predicates and out-of-domain data. Experimental\nresults show that ASDOT consistently achieves significant improvement over\nbaselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2210.04325v3.pdf"
    },
    {
        "title": "QAScore -- An Unsupervised Unreferenced Metric for the Question Generation Evaluation",
        "authors": [
            "Tianbo Ji",
            "Chenyang Lyu",
            "Gareth Jones",
            "Liting Zhou",
            "Yvette Graham"
        ],
        "published": "2022-10-09T19:00:39Z",
        "summary": "Question Generation (QG) aims to automate the task of composing questions for\na passage with a set of chosen answers found within the passage. In recent\nyears, the introduction of neural generation models has resulted in substantial\nimprovements of automatically generated questions in terms of quality,\nespecially compared to traditional approaches that employ manually crafted\nheuristics. However, the metrics commonly applied in QG evaluations have been\ncriticized for their low agreement with human judgement. We therefore propose a\nnew reference-free evaluation metric that has the potential to provide a better\nmechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a\nlanguage model to maximize its correlation with human judgements, QAScore\nevaluates a question by computing the cross entropy according to the\nprobability that the language model can correctly generate the masked words in\nthe answer to that question. Furthermore, we conduct a new crowd-sourcing human\nevaluation experiment for the QG evaluation to investigate how QAScore and\nother metrics can correlate with human judgements. Experiments show that\nQAScore obtains a stronger correlation with the results of our proposed human\nevaluation method compared to existing traditional word-overlap-based metrics\nsuch as BLEU and ROUGE, as well as the existing pretrained-model-based metric\nBERTScore.",
        "pdf_link": "https://arxiv.org/pdf/2210.04320v1.pdf"
    },
    {
        "title": "Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection",
        "authors": [
            "Omkar Gokhale",
            "Aditya Kane",
            "Shantanu Patankar",
            "Tanmay Chavan",
            "Raviraj Joshi"
        ],
        "published": "2022-10-09T13:53:06Z",
        "summary": "Pre-training large neural language models, such as BERT, has led to\nimpressive gains on many natural language processing (NLP) tasks. Although this\nmethod has proven to be effective for many domains, it might not always provide\ndesirable benefits. In this paper, we study the effects of hateful pre-training\non low-resource hate speech classification tasks. While previous studies on the\nEnglish language have emphasized its importance, we aim to augment their\nobservations with some non-obvious insights. We evaluate different variations\nof tweet-based BERT models pre-trained on hateful, non-hateful, and mixed\nsubsets of a 40M tweet dataset. This evaluation is carried out for the Indian\nlanguages Hindi and Marathi. This paper is empirical evidence that hateful\npre-training is not the best pre-training option for hate speech detection. We\nshow that pre-training on non-hateful text from the target domain provides\nsimilar or better results. Further, we introduce HindTweetBERT and\nMahaTweetBERT, the first publicly available BERT models pre-trained on Hindi\nand Marathi tweets, respectively. We show that they provide state-of-the-art\nperformance on hate speech classification tasks. We also release hateful BERT\nfor the two languages and a gold hate speech evaluation benchmark HateEval-Hi\nand HateEval-Mr consisting of manually labeled 2000 tweets each. The models and\ndata are available at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2210.04267v3.pdf"
    },
    {
        "title": "Better Pre-Training by Reducing Representation Confusion",
        "authors": [
            "Haojie Zhang",
            "Mingfei Liang",
            "Ruobing Xie",
            "Zhenlong Sun",
            "Bo Zhang",
            "Leyu Lin"
        ],
        "published": "2022-10-09T12:35:04Z",
        "summary": "In this work, we revisit the Transformer-based pre-trained language models\nand identify two different types of information confusion in position encoding\nand model representations, respectively. Firstly, we show that in the relative\nposition encoding, the joint modeling about relative distances and directions\nbrings confusion between two heterogeneous information. It may make the model\nunable to capture the associative semantics of the same distance and the\nopposite directions, which in turn affects the performance of downstream tasks.\nSecondly, we notice the BERT with Mask Language Modeling (MLM) pre-training\nobjective outputs similar token representations (last hidden states of\ndifferent tokens) and head representations (attention weights of different\nheads), which may make the diversity of information expressed by different\ntokens and heads limited. Motivated by the above investigation, we propose two\nnovel techniques to improve pre-trained language models: Decoupled Directional\nRelative Position (DDRP) encoding and MTH pre-training objective. DDRP\ndecouples the relative distance features and the directional features in\nclassical relative position encoding. MTH applies two novel auxiliary\nregularizers besides MLM to enlarge the dissimilarities between (a) last hidden\nstates of different tokens, and (b) attention weights of different heads. These\ndesigns allow the model to capture different categories of information more\nclearly, as a way to alleviate information confusion in representation learning\nfor better optimization. Extensive experiments and ablation studies on GLUE\nbenchmark demonstrate the effectiveness of our proposed methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.04246v2.pdf"
    },
    {
        "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
        "authors": [
            "Huanru Henry Mao"
        ],
        "published": "2022-10-09T12:27:25Z",
        "summary": "Autoregressive Transformers are strong language models but incur O(T)\ncomplexity during per-token generation due to the self-attention mechanism.\nRecent work proposes kernel-based methods to approximate causal self-attention\nby replacing it with recurrent formulations with various update rules and\nfeature maps to achieve O(1) time and memory complexity. We explore these\napproaches and find that they are unnecessarily complex, and propose a simple\nalternative - decaying fast weights - that runs fast on GPU, outperforms prior\nmethods, and retains 99% of attention's performance for GPT-2. We also show\ncompetitive performance on WikiText-103 against more complex attention\nsubstitutes.",
        "pdf_link": "https://arxiv.org/pdf/2210.04243v1.pdf"
    },
    {
        "title": "Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT",
        "authors": [
            "Bhavya Bhavya",
            "Jinjun Xiong",
            "Chengxiang Zhai"
        ],
        "published": "2022-10-09T06:35:14Z",
        "summary": "We propose a novel application of prompting Pre-trained Language Models\n(PLMs) to generate analogies and study how to design effective prompts for two\ntask settings: generating a source concept analogous to a given target concept\n(aka Analogous Concept Generation or ACG), and generating an explanation of the\nsimilarity between a given pair of target concept and source concept (aka\nAnalogous Explanation Generation or AEG). We found that it is feasible to\nprompt InstructGPT to generate meaningful analogies and the best prompts tend\nto be precise imperative statements especially with a low temperature setting.\nWe also systematically analyzed the sensitivity of the InstructGPT model to\nprompt design, temperature, and injected spelling errors, and found that the\nmodel is particularly sensitive to certain variations (e.g., questions vs.\nimperative statements). Further, we conducted human evaluation on 1.4k of the\ngenerated analogies and found that the quality of generations varies\nsubstantially by model size. The largest InstructGPT model can achieve\nhuman-level performance at generating meaningful analogies for a given target\nwhile there is still room for improvement on the AEG task.",
        "pdf_link": "https://arxiv.org/pdf/2210.04186v2.pdf"
    },
    {
        "title": "Controllable Dialogue Simulation with In-Context Learning",
        "authors": [
            "Zekun Li",
            "Wenhu Chen",
            "Shiyang Li",
            "Hong Wang",
            "Jing Qian",
            "Xifeng Yan"
        ],
        "published": "2022-10-09T06:32:58Z",
        "summary": "Building dialogue systems requires a large corpus of annotated dialogues.\nSuch datasets are usually created via crowdsourcing, which is expensive and\ntime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue\nsimulation method based on large language model in-context learning to automate\ndataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}\nautomatically selects in-context examples for demonstration and prompts GPT-3\nto generate new dialogues and annotations in a controllable way. Our method can\nrapidly expand a small set of dialogue data with minimum or zero \\textit{human\ninvolvement} and \\textit{parameter update} and is thus much more cost-efficient\nand time-saving than crowdsourcing. Experimental results on the MultiWOZ\ndataset demonstrate that training a model on the simulated dialogues leads to\neven better performance than using the same amount of human-generated dialogues\nunder the challenging low-resource settings, with as few as 85 dialogues as a\nseed. When enough data is available, our method can still serve as an effective\ndata augmentation method. Human evaluation results also show that our simulated\ndialogues have near-human fluency and annotation accuracy. The code and data\nare available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.",
        "pdf_link": "https://arxiv.org/pdf/2210.04185v4.pdf"
    },
    {
        "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
        "authors": [
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Wenqian Zhang",
            "Zhenyu Lei",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-08T20:51:02Z",
        "summary": "With the advent of pretrained language models (LMs), increasing research\nefforts have been focusing on infusing commonsense and domain-specific\nknowledge to prepare LMs for downstream tasks. These works attempt to leverage\nknowledge graphs, the de facto standard of symbolic knowledge representation,\nalong with pretrained LMs. While existing approaches have leveraged external\nknowledge, it remains an open question how to jointly incorporate knowledge\ngraphs representing varying contexts, from local (e.g., sentence), to\ndocument-level, to global knowledge, to enable knowledge-rich exchange across\nthese contexts. Such rich contextualization can be especially beneficial for\nlong document understanding tasks since standard pretrained LMs are typically\nbounded by the input sequence length. In light of these challenges, we propose\nKALM, a Knowledge-Aware Language Model that jointly leverages knowledge in\nlocal, document-level, and global contexts for long document understanding.\nKALM first encodes long documents and knowledge graphs into the three\nknowledge-aware context representations. It then processes each context with\ncontext-specific layers, followed by a context fusion layer that facilitates\nknowledge exchange to derive an overarching document representation. Extensive\nexperiments demonstrate that KALM achieves state-of-the-art performance on six\nlong document understanding tasks and datasets. Further analyses reveal that\nthe three knowledge-aware contexts are complementary and they all contribute to\nmodel performance, while the importance and information exchange patterns of\ndifferent contexts vary with respect to different tasks and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2210.04105v2.pdf"
    },
    {
        "title": "On Task-Adaptive Pretraining for Dialogue Response Selection",
        "authors": [
            "Tzu-Hsiang Lin",
            "Ta-Chung Chi",
            "Anna Rumshisky"
        ],
        "published": "2022-10-08T17:58:49Z",
        "summary": "Recent advancements in dialogue response selection (DRS) are based on the\n\\textit{task-adaptive pre-training (TAP)} approach, by first initializing their\nmodel with BERT~\\cite{devlin-etal-2019-bert}, and adapt to dialogue data with\ndialogue-specific or fine-grained pre-training tasks. However, it is uncertain\nwhether BERT is the best initialization choice, or whether the proposed\ndialogue-specific fine-grained learning tasks are actually better than MLM+NSP.\nThis paper aims to verify assumptions made in previous works and understand the\nsource of improvements for DRS. We show that initializing with RoBERTa achieve\nsimilar performance as BERT, and MLM+NSP can outperform all previously proposed\nTAP tasks, during which we also contribute a new state-of-the-art on the Ubuntu\ncorpus. Additional analyses shows that the main source of improvements comes\nfrom the TAP step, and that the NSP task is crucial to DRS, different from\ncommon NLU tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.04073v1.pdf"
    },
    {
        "title": "InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings",
        "authors": [
            "Xing Wu",
            "Chaochen Gao",
            "Zijia Lin",
            "Jizhong Han",
            "Zhongyuan Wang",
            "Songlin Hu"
        ],
        "published": "2022-10-08T15:53:19Z",
        "summary": "Contrastive learning has been extensively studied in sentence embedding\nlearning, which assumes that the embeddings of different views of the same\nsentence are closer. The constraint brought by this assumption is weak, and a\ngood sentence representation should also be able to reconstruct the original\nsentence fragments. Therefore, this paper proposes an information-aggregated\ncontrastive learning framework for learning unsupervised sentence embeddings,\ntermed InfoCSE. InfoCSE forces the representation of [CLS] positions to\naggregate denser sentence information by introducing an additional Masked\nlanguage model task and a well-designed network. We evaluate the proposed\nInfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)\ntask. Experimental results show that InfoCSE outperforms SimCSE by an average\nSpearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving\nstate-of-the-art results among unsupervised sentence representation learning\nmethods. Our code are available at\nhttps://github.com/caskcsg/sentemb/tree/main/InfoCSE.",
        "pdf_link": "https://arxiv.org/pdf/2210.06432v3.pdf"
    },
    {
        "title": "KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification",
        "authors": [
            "Yong He",
            "Cheng Wang",
            "Shun Zhang",
            "Nan Li",
            "Zhaorong Li",
            "Zhenyu Zeng"
        ],
        "published": "2022-10-08T08:37:44Z",
        "summary": "Medical text learning has recently emerged as a promising area to improve\nhealthcare due to the wide adoption of electronic health record (EHR) systems.\nThe complexity of the medical text such as diverse length, mixed text types,\nand full of medical jargon, poses a great challenge for developing effective\ndeep learning models. BERT has presented state-of-the-art results in many NLP\ntasks, such as text classification and question answering. However, the\nstandalone BERT model cannot deal with the complexity of the medical text,\nespecially the lengthy clinical notes. Herein, we develop a new model called\nKG-MTT-BERT (Knowledge Graph Enhanced Multi-Type Text BERT) by extending the\nBERT model for long and multi-type text with the integration of the medical\nknowledge graph. Our model can outperform all baselines and other\nstate-of-the-art models in diagnosis-related group (DRG) classification, which\nrequires comprehensive medical text for accurate classification. We also\ndemonstrated that our model can effectively handle multi-type text and the\nintegration of medical knowledge graph can significantly improve the\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2210.03970v1.pdf"
    },
    {
        "title": "Understanding HTML with Large Language Models",
        "authors": [
            "Izzeddin Gur",
            "Ofir Nachum",
            "Yingjie Miao",
            "Mustafa Safdari",
            "Austin Huang",
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Noah Fiedel",
            "Aleksandra Faust"
        ],
        "published": "2022-10-08T07:27:17Z",
        "summary": "Large language models (LLMs) have shown exceptional performance on a variety\nof natural language tasks. Yet, their capabilities for HTML understanding --\ni.e., parsing the raw HTML of a webpage, with applications to automation of\nweb-based tasks, crawling, and browser-assisted retrieval -- have not been\nfully explored. We contribute HTML understanding models (fine-tuned LLMs) and\nan in-depth analysis of their capabilities under three tasks: (i) Semantic\nClassification of HTML elements, (ii) Description Generation for HTML inputs,\nand (iii) Autonomous Web Navigation of HTML pages. While previous work has\ndeveloped dedicated architectures and training procedures for HTML\nunderstanding, we show that LLMs pretrained on standard natural language\ncorpora transfer remarkably well to HTML understanding tasks. For instance,\nfine-tuned LLMs are 12% more accurate at semantic classification compared to\nmodels trained exclusively on the task dataset. Moreover, when fine-tuned on\ndata from the MiniWoB benchmark, LLMs successfully complete 50% more tasks\nusing 192x less data compared to the previous best supervised model. Out of the\nLLMs we evaluate, we show evidence that T5-based models are ideal due to their\nbidirectional encoder-decoder architecture. To promote further research on LLMs\nfor HTML understanding, we create and open-source a large-scale HTML dataset\ndistilled and auto-labeled from CommonCrawl.",
        "pdf_link": "https://arxiv.org/pdf/2210.03945v2.pdf"
    },
    {
        "title": "Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling",
        "authors": [
            "Hsin-Ying Lee",
            "Hung-Ting Su",
            "Bing-Chen Tsai",
            "Tsung-Han Wu",
            "Jia-Fong Yeh",
            "Winston H. Hsu"
        ],
        "published": "2022-10-08T07:03:31Z",
        "summary": "While recent large-scale video-language pre-training made great progress in\nvideo question answering, the design of spatial modeling of video-language\nmodels is less fine-grained than that of image-language models; existing\npractices of temporal modeling also suffer from weak and noisy alignment\nbetween modalities. To learn fine-grained visual understanding, we decouple\nspatial-temporal modeling and propose a hybrid pipeline, Decoupled\nSpatial-Temporal Encoders, integrating an image- and a video-language encoder.\nThe former encodes spatial semantics from larger but sparsely sampled frames\nindependently of time, while the latter models temporal dynamics at lower\nspatial but higher temporal resolution. To help the video-language model learn\ntemporal relations for video QA, we propose a novel pre-training objective,\nTemporal Referring Modeling, which requires the model to identify temporal\npositions of events in video sequences. Extensive experiments demonstrate that\nour model outperforms previous work pre-trained on orders of magnitude larger\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2210.03941v1.pdf"
    },
    {
        "title": "Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models",
        "authors": [
            "Alon Albalak",
            "Akshat Shrivastava",
            "Chinnadhurai Sankar",
            "Adithya Sagar",
            "Mike Ross"
        ],
        "published": "2022-10-08T01:45:22Z",
        "summary": "Multi-task learning (MTL), instruction tuning, and prompting have recently\nbeen shown to improve the generalizability of large language models to new\ntasks. However, the benefits of such methods are less well-documented in\nsmaller language models, with some studies finding contradictory results. In\nthis work, we explore and isolate the effects of (i) model size, (ii) general\npurpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot\nfine-tuning for models with fewer than 500 million parameters. Our experiments\nin the zero-shot setting demonstrate that models gain 31% relative improvement,\non average, from general purpose MTL, with an additional 37.6% relative gain\nfrom in-domain MTL. Contradictory to prior works on large models, we find that\ninstruction tuning provides a modest 2% performance improvement for small\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2210.03871v1.pdf"
    },
    {
        "title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models",
        "authors": [
            "Se Jung Kwon",
            "Jeonghoon Kim",
            "Jeongin Bae",
            "Kang Min Yoo",
            "Jin-Hwa Kim",
            "Baeseong Park",
            "Byeongwook Kim",
            "Jung-Woo Ha",
            "Nako Sung",
            "Dongsoo Lee"
        ],
        "published": "2022-10-08T00:36:00Z",
        "summary": "There are growing interests in adapting large-scale language models using\nparameter-efficient fine-tuning methods. However, accelerating the model itself\nand achieving better inference efficiency through model compression has not\nbeen thoroughly explored yet. Model compression could provide the benefits of\nreducing memory footprints, enabling low-precision computations, and ultimately\nachieving cost-effective inference. To combine parameter-efficient adaptation\nand model compression, we propose AlphaTuning consisting of post-training\nquantization of the pre-trained language model and fine-tuning only some parts\nof quantized parameters for a target task. Specifically, AlphaTuning works by\nemploying binary-coding quantization, which factorizes the full-precision\nparameters into binary parameters and a separate set of scaling factors. During\nthe adaptation phase, the binary values are frozen for all tasks, while the\nscaling factors are fine-tuned for the downstream task. We demonstrate that\nAlphaTuning, when applied to GPT-2 and OPT, performs competitively with full\nfine-tuning on a variety of downstream tasks while achieving >10x compression\nratio under 4-bit quantization and >1,000x reduction in the number of trainable\nparameters.",
        "pdf_link": "https://arxiv.org/pdf/2210.03858v1.pdf"
    },
    {
        "title": "Breaking BERT: Evaluating and Optimizing Sparsified Attention",
        "authors": [
            "Siddhartha Brahma",
            "Polina Zablotskaia",
            "David Mimno"
        ],
        "published": "2022-10-07T22:32:27Z",
        "summary": "Transformers allow attention between all pairs of tokens, but there is reason\nto believe that most of these connections - and their quadratic time and memory\n- may not be necessary. But which ones? We evaluate the impact of\nsparsification patterns with a series of ablation experiments. First, we\ncompare masks based on syntax, lexical similarity, and token position to random\nconnections, and measure which patterns reduce performance the least. We find\nthat on three common finetuning tasks even using attention that is at least 78%\nsparse can have little effect on performance if applied at later transformer\nlayers, but that applying sparsity throughout the network reduces performance\nsignificantly. Second, we vary the degree of sparsity for three patterns\nsupported by previous work, and find that connections to neighbouring tokens\nare the most significant. Finally, we treat sparsity as an optimizable\nparameter, and present an algorithm to learn degrees of neighboring connections\nthat gives a fine-grained control over the accuracy-sparsity trade-off while\napproaching the performance of existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.03841v1.pdf"
    },
    {
        "title": "Large Language Models can Implement Policy Iteration",
        "authors": [
            "Ethan Brooks",
            "Logan Walls",
            "Richard L. Lewis",
            "Satinder Singh"
        ],
        "published": "2022-10-07T21:18:22Z",
        "summary": "This work presents In-Context Policy Iteration, an algorithm for performing\nReinforcement Learning (RL), in-context, using foundation models. While the\napplication of foundation models to RL has received considerable attention,\nmost approaches rely on either (1) the curation of expert demonstrations\n(either through manual design or task-specific pretraining) or (2) adaptation\nto the task of interest using gradient methods (either fine-tuning or training\nof adapter layers). Both of these techniques have drawbacks. Collecting\ndemonstrations is labor-intensive, and algorithms that rely on them do not\noutperform the experts from which the demonstrations were derived. All gradient\ntechniques are inherently slow, sacrificing the \"few-shot\" quality that made\nin-context learning attractive to begin with. In this work, we present an\nalgorithm, ICPI, that learns to perform RL tasks without expert demonstrations\nor gradients. Instead we present a policy-iteration method in which the prompt\ncontent is the entire locus of learning. ICPI iteratively updates the contents\nof the prompt from which it derives its policy through trial-and-error\ninteraction with an RL environment. In order to eliminate the role of\nin-weights learning (on which approaches like Decision Transformer rely\nheavily), we demonstrate our algorithm using Codex, a language model with no\nprior knowledge of the domains on which we evaluate it.",
        "pdf_link": "https://arxiv.org/pdf/2210.03821v2.pdf"
    },
    {
        "title": "Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts",
        "authors": [
            "Asahi Ushio",
            "Leonardo Neves",
            "Vitor Silva",
            "Francesco Barbieri",
            "Jose Camacho-Collados"
        ],
        "published": "2022-10-07T19:58:47Z",
        "summary": "Recent progress in language model pre-training has led to important\nimprovements in Named Entity Recognition (NER). Nonetheless, this progress has\nbeen mainly tested in well-formatted documents such as news, Wikipedia, or\nscientific articles. In social media the landscape is different, in which it\nadds another layer of complexity due to its noisy and dynamic nature. In this\npaper, we focus on NER in Twitter, one of the largest social media platforms,\nand construct a new NER dataset, TweetNER7, which contains seven entity types\nannotated over 11,382 tweets from September 2019 to August 2021. The dataset\nwas constructed by carefully distributing the tweets over time and taking\nrepresentative trends as a basis. Along with the dataset, we provide a set of\nlanguage model baselines and perform an analysis on the language model\nperformance on the task, especially analyzing the impact of different time\nperiods. In particular, we focus on three important temporal aspects in our\nanalysis: short-term degradation of NER models over time, strategies to\nfine-tune a language model over different periods, and self-labeling as an\nalternative to lack of recently-labeled data. TweetNER7 is released publicly\n(https://huggingface.co/datasets/tner/tweetner7) along with the models\nfine-tuned on it.",
        "pdf_link": "https://arxiv.org/pdf/2210.03797v2.pdf"
    },
    {
        "title": "Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts",
        "authors": [
            "Nghia T. Le",
            "Fan Bai",
            "Alan Ritter"
        ],
        "published": "2022-10-07T16:51:45Z",
        "summary": "Anaphora resolution is an important task for information extraction across a\nrange of languages, text genres, and domains, motivating the need for methods\nthat do not require large annotated datasets. In-context learning has emerged\nas a promising approach, yet there are a number of challenges in applying\nin-context learning to resolve anaphora. For example, encoding a single\nin-context demonstration that consists of: an anaphor, a paragraph-length\ncontext, and a list of corresponding antecedents, requires conditioning a\nlanguage model on a long sequence of tokens, limiting the number of\ndemonstrations per prompt. In this paper, we present MICE (Mixtures of\nIn-Context Experts), which we demonstrate is effective for few-shot anaphora\nresolution in scientific protocols (Tamari et al., 2021). Given only a handful\nof training examples, MICE combines the predictions of hundreds of in-context\nexperts, yielding a 30% increase in F1 score over a competitive prompt\nretrieval baseline. Furthermore, we show MICE can be used to train compact\nstudent models without sacrificing performance. As far as we are aware, this is\nthe first work to present experimental results demonstrating the effectiveness\nof in-context learning on the task of few-shot anaphora resolution in\nscientific protocols.",
        "pdf_link": "https://arxiv.org/pdf/2210.03690v2.pdf"
    },
    {
        "title": "Novice Type Error Diagnosis with Natural Language Models",
        "authors": [
            "Chuqin Geng",
            "Haolin Ye",
            "Yixuan Li",
            "Tianyu Han",
            "Brigitte Pientka",
            "Xujie Si"
        ],
        "published": "2022-10-07T16:40:53Z",
        "summary": "Strong static type systems help programmers eliminate many errors without\nmuch burden of supplying type annotations. However, this flexibility makes it\nhighly non-trivial to diagnose ill-typed programs, especially for novice\nprogrammers. Compared to classic constraint solving and optimization-based\napproaches, the data-driven approach has shown great promise in identifying the\nroot causes of type errors with higher accuracy. Instead of relying on\nhand-engineered features, this work explores natural language models for type\nerror localization, which can be trained in an end-to-end fashion without\nrequiring any features. We demonstrate that, for novice type error diagnosis,\nthe language model-based approach significantly outperforms the previous\nstate-of-the-art data-driven approach. Specifically, our model could predict\ntype errors correctly 62% of the time, outperforming the state-of-the-art\nNate's data-driven model by 11%, in a more rigorous accuracy metric.\nFurthermore, we also apply structural probes to explain the performance\ndifference between different language models.",
        "pdf_link": "https://arxiv.org/pdf/2210.03682v1.pdf"
    },
    {
        "title": "Reinforcement Learning Approach for Multi-Agent Flexible Scheduling Problems",
        "authors": [
            "Hongjian Zhou",
            "Boyang Gu",
            "Chenghao Jin"
        ],
        "published": "2022-10-07T16:31:01Z",
        "summary": "Scheduling plays an important role in automated production. Its impact can be\nfound in various fields such as the manufacturing industry, the service\nindustry and the technology industry. A scheduling problem (NP-hard) is a task\nof finding a sequence of job assignments on a given set of machines with the\ngoal of optimizing the objective defined. Methods such as Operation Research,\nDispatching Rules, and Combinatorial Optimization have been applied to\nscheduling problems but no solution guarantees to find the optimal solution.\nThe recent development of Reinforcement Learning has shown success in\nsequential decision-making problems. This research presents a Reinforcement\nLearning approach for scheduling problems. In particular, this study delivers\nan OpenAI gym environment with search-space reduction for Job Shop Scheduling\nProblems and provides a heuristic-guided Q-Learning solution with\nstate-of-the-art performance for Multi-agent Flexible Job Shop Problems.",
        "pdf_link": "https://arxiv.org/pdf/2210.03674v1.pdf"
    },
    {
        "title": "How Large Language Models are Transforming Machine-Paraphrased Plagiarism",
        "authors": [
            "Jan Philip Wahle",
            "Terry Ruas",
            "Frederic Kirstein",
            "Bela Gipp"
        ],
        "published": "2022-10-07T14:08:57Z",
        "summary": "The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.",
        "pdf_link": "https://arxiv.org/pdf/2210.03568v3.pdf"
    },
    {
        "title": "Automatic Chain of Thought Prompting in Large Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola"
        ],
        "published": "2022-10-07T12:28:21Z",
        "summary": "Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot",
        "pdf_link": "https://arxiv.org/pdf/2210.03493v1.pdf"
    },
    {
        "title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems",
        "authors": [
            "Parikshit Bansal",
            "Yashoteja Prabhu",
            "Emre Kiciman",
            "Amit Sharma"
        ],
        "published": "2022-10-07T11:16:45Z",
        "summary": "Given a user's input text, text-matching recommender systems output relevant\nitems by comparing the input text to available items' description, such as\nproduct-to-product recommendation on e-commerce platforms. As users' interests\nand item inventory are expected to change, it is important for a text-matching\nsystem to generalize to data shifts, a task known as out-of-distribution (OOD)\ngeneralization. However, we find that the popular approach of fine-tuning a\nlarge, base language model on paired item relevance data (e.g., user clicks)\ncan be counter-productive for OOD generalization. For a product recommendation\ntask, fine-tuning obtains worse accuracy than the base model when recommending\nitems in a new category or for a future time period. To explain this\ngeneralization failure, we consider an intervention-based importance metric,\nwhich shows that a fine-tuned model captures spurious correlations and fails to\nlearn the causal features that determine the relevance between any two text\ninputs. Moreover, standard methods for causal regularization do not apply in\nthis setting, because unlike in images, there exist no universally spurious\nfeatures in a text-matching task (the same token may be spurious or causal\ndepending on the text it is being matched to). For OOD generalization on text\ninputs, therefore, we highlight a different goal: avoiding high importance\nscores for certain features. We do so using an intervention-based regularizer\nthat constraints the causal effect of any token on the model's relevance score\nto be similar to the base model. Results on Amazon product and 3 question\nrecommendation datasets show that our proposed regularizer improves\ngeneralization for both in-distribution and OOD evaluation, especially in\ndifficult scenarios when the base model is not accurate.",
        "pdf_link": "https://arxiv.org/pdf/2210.10636v2.pdf"
    },
    {
        "title": "DABERT: Dual Attention Enhanced BERT for Semantic Matching",
        "authors": [
            "Sirui Wang",
            "Di Liang",
            "Jian Song",
            "Yuntao Li",
            "Wei Wu"
        ],
        "published": "2022-10-07T10:54:49Z",
        "summary": "Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.",
        "pdf_link": "https://arxiv.org/pdf/2210.03454v4.pdf"
    },
    {
        "title": "UU-Tax at SemEval-2022 Task 3: Improving the generalizability of language models for taxonomy classification through data augmentation",
        "authors": [
            "Injy Sarhan",
            "Pablo Mosteiro",
            "Marco Spruit"
        ],
        "published": "2022-10-07T07:41:28Z",
        "summary": "This paper presents our strategy to address the SemEval-2022 Task 3 PreTENS:\nPresupposed Taxonomies Evaluating Neural Network Semantics. The goal of the\ntask is to identify if a sentence is deemed acceptable or not, depending on the\ntaxonomic relationship that holds between a noun pair contained in the\nsentence. For sub-task 1 -- binary classification -- we propose an effective\nway to enhance the robustness and the generalizability of language models for\nbetter classification on this downstream task. We design a two-stage\nfine-tuning procedure on the ELECTRA language model using data augmentation\ntechniques. Rigorous experiments are carried out using multi-task learning and\ndata-enriched fine-tuning. Experimental results demonstrate that our proposed\nmodel, UU-Tax, is indeed able to generalize well for our downstream task. For\nsub-task 2 -- regression -- we propose a simple classifier that trains on\nfeatures obtained from Universal Sentence Encoder (USE). In addition to\ndescribing the submitted systems, we discuss other experiments that employ\npre-trained language models and data augmentation techniques. For both\nsub-tasks, we perform error analysis to further understand the behaviour of the\nproposed models. We achieved a global F1_Binary score of 91.25% in sub-task 1\nand a rho score of 0.221 in sub-task 2.",
        "pdf_link": "https://arxiv.org/pdf/2210.03378v1.pdf"
    },
    {
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "authors": [
            "Ofir Press",
            "Muru Zhang",
            "Sewon Min",
            "Ludwig Schmidt",
            "Noah A. Smith",
            "Mike Lewis"
        ],
        "published": "2022-10-07T06:50:23Z",
        "summary": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2210.03350v3.pdf"
    },
    {
        "title": "Elastic Step DQN: A novel multi-step algorithm to alleviate overestimation in Deep QNetworks",
        "authors": [
            "Adrian Ly",
            "Richard Dazeley",
            "Peter Vamplew",
            "Francisco Cruz",
            "Sunil Aryal"
        ],
        "published": "2022-10-07T04:56:04Z",
        "summary": "Deep Q-Networks algorithm (DQN) was the first reinforcement learning\nalgorithm using deep neural network to successfully surpass human level\nperformance in a number of Atari learning environments. However, divergent and\nunstable behaviour have been long standing issues in DQNs. The unstable\nbehaviour is often characterised by overestimation in the $Q$-values, commonly\nreferred to as the overestimation bias. To address the overestimation bias and\nthe divergent behaviour, a number of heuristic extensions have been proposed.\nNotably, multi-step updates have been shown to drastically reduce unstable\nbehaviour while improving agent's training performance. However, agents are\noften highly sensitive to the selection of the multi-step update horizon ($n$),\nand our empirical experiments show that a poorly chosen static value for $n$\ncan in many cases lead to worse performance than single-step DQN. Inspired by\nthe success of $n$-step DQN and the effects that multi-step updates have on\noverestimation bias, this paper proposes a new algorithm that we call `Elastic\nStep DQN' (ES-DQN). It dynamically varies the step size horizon in multi-step\nupdates based on the similarity of states visited. Our empirical evaluation\nshows that ES-DQN out-performs $n$-step with fixed $n$ updates, Double DQN and\nAverage DQN in several OpenAI Gym environments while at the same time\nalleviating the overestimation bias.",
        "pdf_link": "https://arxiv.org/pdf/2210.03325v1.pdf"
    },
    {
        "title": "HealthE: Classifying Entities in Online Textual Health Advice",
        "authors": [
            "Joseph Gatto",
            "Parker Seegmiller",
            "Garrett Johnston",
            "Sarah M. Preum"
        ],
        "published": "2022-10-06T23:18:24Z",
        "summary": "The processing of entities in natural language is essential to many medical\nNLP systems. Unfortunately, existing datasets vastly under-represent the\nentities required to model public health relevant texts such as health advice\noften found on sites like WebMD. People rely on such information for personal\nhealth management and clinically relevant decision making. In this work, we\nrelease a new annotated dataset, HealthE, consisting of 6,756 health advice.\nHealthE has a more granular label space compared to existing medical NER\ncorpora and contains annotation for diverse health phrases. Additionally, we\nintroduce a new health entity classification model, EP S-BERT, which leverages\ntextual context patterns in the classification of entity classes. EP S-BERT\nprovides a 4-point increase in F1 score over the nearest baseline and a\n34-point increase in F1 when compared to off-the-shelf medical NER tools\ntrained to extract disease and medication mentions from clinical texts. All\ncode and data are publicly available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2210.03246v1.pdf"
    },
    {
        "title": "Improving Large-scale Paraphrase Acquisition and Generation",
        "authors": [
            "Yao Dou",
            "Chao Jiang",
            "Wei Xu"
        ],
        "published": "2022-10-06T22:00:56Z",
        "summary": "This paper addresses the quality issues in existing Twitter-based paraphrase\ndatasets, and discusses the necessity of using two separate definitions of\nparaphrase for identification and generation tasks. We present a new\nMulti-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of\n130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert\n(MultiPIT_expert) annotations using two different paraphrase definitions for\nparaphrase identification, in addition to a multi-reference test set\n(MultiPIT_NMR) and a large automatically constructed training set\n(MultiPIT_Auto) for paraphrase generation. With improved data annotation\nquality and task-specific paraphrase definition, the best pre-trained language\nmodel fine-tuned on our dataset achieves the state-of-the-art performance of\n84.2 F1 for automatic paraphrase identification. Furthermore, our empirical\nresults also demonstrate that the paraphrase generation models trained on\nMultiPIT_Auto generate more diverse and high-quality paraphrases compared to\ntheir counterparts fine-tuned on other corpora such as Quora, MSCOCO, and\nParaNMT.",
        "pdf_link": "https://arxiv.org/pdf/2210.03235v3.pdf"
    },
    {
        "title": "PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection",
        "authors": [
            "Shuyue Stella Li",
            "Xiangyu Zhang",
            "Shu Zhou",
            "Hongchao Shu",
            "Ruixing Liang",
            "Hexin Liu",
            "Leibny Paola Garcia"
        ],
        "published": "2022-10-06T21:29:17Z",
        "summary": "With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nPortable Quantum Language Model (PQLM) that can easily transmit information to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the ad hoc portability of the quantum\nmodel by extracting only the word embeddings and effectively applying them to\ndownstream tasks on classical machines. Our PQLM exhibits comparable\nperformance to its classical counterpart on both intrinsic evaluation (loss,\nperplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)\nmetrics. We also perform ablation studies on the factors affecting PQLM\nperformance to analyze model stability. Our work establishes a theoretical\nfoundation for a portable quantum pre-trained language model that could be\ntrained on private data and made available for public use with privacy\nprotection guarantees.",
        "pdf_link": "https://arxiv.org/pdf/2210.03221v5.pdf"
    },
    {
        "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models",
        "authors": [
            "David Wingate",
            "Mohammad Shoeybi",
            "Taylor Sorensen"
        ],
        "published": "2022-10-06T18:52:24Z",
        "summary": "We explore the idea of compressing the prompts used to condition language\nmodels, and show that compressed prompts can retain a substantive amount of\ninformation about the original prompt. For severely compressed prompts, while\nfine-grained information is lost, abstract information and general sentiments\ncan be retained with surprisingly few parameters, which can be useful in the\ncontext of decode-time algorithms for controllability and toxicity reduction.\nWe explore contrastive conditioning to steer language model generation towards\ndesirable text and away from undesirable text, and find that some complex\nprompts can be effectively compressed into a single token to guide generation.\nWe also show that compressed prompts are largely compositional, and can be\nconstructed such that they can be used to control independent aspects of\ngenerated text.",
        "pdf_link": "https://arxiv.org/pdf/2210.03162v1.pdf"
    },
    {
        "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
        "authors": [
            "Yunfan Jiang",
            "Agrim Gupta",
            "Zichen Zhang",
            "Guanzhi Wang",
            "Yongqiang Dou",
            "Yanjun Chen",
            "Li Fei-Fei",
            "Anima Anandkumar",
            "Yuke Zhu",
            "Linxi Fan"
        ],
        "published": "2022-10-06T17:50:11Z",
        "summary": "Prompt-based learning has emerged as a successful paradigm in natural\nlanguage processing, where a single general-purpose language model can be\ninstructed to perform any task specified by input prompts. Yet task\nspecification in robotics comes in various forms, such as imitating one-shot\ndemonstrations, following language instructions, and reaching visual goals.\nThey are often considered different tasks and tackled by specialized models. We\nshow that a wide spectrum of robot manipulation tasks can be expressed with\nmultimodal prompts, interleaving textual and visual tokens. Accordingly, we\ndevelop a new simulation benchmark that consists of thousands of\nprocedurally-generated tabletop tasks with multimodal prompts, 600K+ expert\ntrajectories for imitation learning, and a four-level evaluation protocol for\nsystematic generalization. We design a transformer-based robot agent, VIMA,\nthat processes these prompts and outputs motor actions autoregressively. VIMA\nfeatures a recipe that achieves strong model scalability and data efficiency.\nIt outperforms alternative designs in the hardest zero-shot generalization\nsetting by up to $2.9\\times$ task success rate given the same training data.\nWith $10\\times$ less training data, VIMA still performs $2.7\\times$ better than\nthe best competing variant. Code and video demos are available at\nhttps://vimalabs.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2210.03094v2.pdf"
    },
    {
        "title": "Explainable Verbal Deception Detection using Transformers",
        "authors": [
            "Loukas Ilias",
            "Felix Soldner",
            "Bennett Kleinberg"
        ],
        "published": "2022-10-06T17:36:00Z",
        "summary": "People are regularly confronted with potentially deceptive statements (e.g.,\nfake news, misleading product reviews, or lies about activities). Only few\nworks on automated text-based deception detection have exploited the potential\nof deep learning approaches. A critique of deep-learning methods is their lack\nof interpretability, preventing us from understanding the underlying\n(linguistic) mechanisms involved in deception. However, recent advancements\nhave made it possible to explain some aspects of such models. This paper\nproposes and evaluates six deep-learning models, including combinations of BERT\n(and RoBERTa), MultiHead Attention, co-attentions, and transformers. To\nunderstand how the models reach their decisions, we then examine the model's\npredictions with LIME. We then zoom in on vocabulary uniqueness and the\ncorrelation of LIWC categories with the outcome class (truthful vs deceptive).\nThe findings suggest that our transformer-based models can enhance automated\ndeception detection performances (+2.11% in accuracy) and show significant\ndifferences pertinent to the usage of LIWC features in truthful and deceptive\nstatements.",
        "pdf_link": "https://arxiv.org/pdf/2210.03080v1.pdf"
    },
    {
        "title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering",
        "authors": [
            "Jiacheng Liu",
            "Skyler Hallinan",
            "Ximing Lu",
            "Pengfei He",
            "Sean Welleck",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "2022-10-06T17:34:06Z",
        "summary": "Knowledge underpins reasoning. Recent research demonstrates that when\nrelevant knowledge is provided as additional context to commonsense question\nanswering (QA), it can substantially enhance the performance even on top of\nstate-of-the-art. The fundamental challenge is where and how to find such\nknowledge that is high quality and on point with respect to the question;\nknowledge retrieved from knowledge bases are incomplete and knowledge generated\nfrom language models are inconsistent. We present Rainier, or Reinforced\nKnowledge Introspector, that learns to generate contextually relevant knowledge\nin response to given questions. Our approach starts by imitating knowledge\ngenerated by GPT-3, then learns to generate its own knowledge via reinforcement\nlearning where rewards are shaped based on the increased performance on the\nresulting question answering. Rainier demonstrates substantial and consistent\nperformance gains when tested over 9 different commonsense benchmarks:\nincluding 5 datasets that are seen during model training, as well as 4 datasets\nthat are kept unseen. Our work is the first to report that knowledge generated\nby models that are orders of magnitude smaller than GPT-3, even without direct\nsupervision on the knowledge itself, can exceed the quality of commonsense\nknowledge elicited from GPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2210.03078v2.pdf"
    },
    {
        "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
        "authors": [
            "Freda Shi",
            "Mirac Suzgun",
            "Markus Freitag",
            "Xuezhi Wang",
            "Suraj Srivats",
            "Soroush Vosoughi",
            "Hyung Won Chung",
            "Yi Tay",
            "Sebastian Ruder",
            "Denny Zhou",
            "Dipanjan Das",
            "Jason Wei"
        ],
        "published": "2022-10-06T17:03:34Z",
        "summary": "We evaluate the reasoning abilities of large language models in multilingual\nsettings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by\nmanually translating 250 grade-school math problems from the GSM8K dataset\n(Cobbe et al., 2021) into ten typologically diverse languages. We find that the\nability to solve MGSM problems via chain-of-thought prompting emerges with\nincreasing model scale, and that models have strikingly strong multilingual\nreasoning abilities, even in underrepresented languages such as Bengali and\nSwahili. Finally, we show that the multilingual reasoning abilities of language\nmodels extend to other tasks such as commonsense reasoning and word-in-context\nsemantic judgment. The MGSM benchmark is publicly available at\nhttps://github.com/google-research/url-nlp.",
        "pdf_link": "https://arxiv.org/pdf/2210.03057v1.pdf"
    },
    {
        "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs",
        "authors": [
            "Yujia Zhai",
            "Chengquan Jiang",
            "Leyuan Wang",
            "Xiaoying Jia",
            "Shang Zhang",
            "Zizhong Chen",
            "Xin Liu",
            "Yibo Zhu"
        ],
        "published": "2022-10-06T16:57:23Z",
        "summary": "Transformers have become keystone models in natural language processing over\nthe past decade. They have achieved great popularity in deep learning\napplications, but the increasing sizes of the parameter spaces required by\ntransformer models generate a commensurate need to accelerate performance.\nNatural language processing problems are also routinely faced with\nvariable-length sequences, as word counts commonly vary among sentences.\nExisting deep learning frameworks pad variable-length sequences to a maximal\nlength, which adds significant memory and computational overhead. In this\npaper, we present ByteTransformer, a high-performance transformer boosted for\nvariable-length inputs. We propose a padding-free algorithm that liberates the\nentire transformer from redundant computations on zero padded tokens. In\naddition to algorithmic-level optimization, we provide architecture-aware\noptimizations for transformer functional modules, especially the\nperformance-critical algorithm Multi-Head Attention (MHA). Experimental results\non an NVIDIA A100 GPU with variable-length sequence inputs validate that our\nfused MHA outperforms PyTorch by 6.13x. The end-to-end performance of\nByteTransformer for a forward BERT transformer surpasses state-of-the-art\ntransformer frameworks, such as PyTorch JIT, TensorFlow XLA, Tencent\nTurboTransformer, Microsoft DeepSpeed-Inference and NVIDIA FasterTransformer,\nby 87\\%, 131\\%, 138\\%, 74\\% and 55\\%, respectively. We also demonstrate the\ngeneral applicability of our optimization methods to other BERT-like models,\nincluding ALBERT, DistilBERT, and DeBERTa.",
        "pdf_link": "https://arxiv.org/pdf/2210.03052v4.pdf"
    },
    {
        "title": "Conversational Semantic Role Labeling with Predicate-Oriented Latent Graph",
        "authors": [
            "Hao Fei",
            "Shengqiong Wu",
            "Meishan Zhang",
            "Yafeng Ren",
            "Donghong Ji"
        ],
        "published": "2022-10-06T16:42:00Z",
        "summary": "Conversational semantic role labeling (CSRL) is a newly proposed task that\nuncovers the shallow semantic structures in a dialogue text. Unfortunately\nseveral important characteristics of the CSRL task have been overlooked by the\nexisting works, such as the structural information integration, near-neighbor\ninfluence. In this work, we investigate the integration of a latent graph for\nCSRL. We propose to automatically induce a predicate-oriented latent graph\n(POLar) with a predicate-centered Gaussian mechanism, by which the nearer and\ninformative words to the predicate will be allocated with more attention. The\nPOLar structure is then dynamically pruned and refined so as to best fit the\ntask need. We additionally introduce an effective dialogue-level pre-trained\nlanguage model, CoDiaBERT, for better supporting multiple utterance sentences\nand handling the speaker coreference issue in CSRL. Our system outperforms\nbest-performing baselines on three benchmark CSRL datasets with big margins,\nespecially achieving over 4% F1 score improvements on the cross-utterance\nargument detection. Further analyses are presented to better understand the\neffectiveness of our proposed methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.03037v1.pdf"
    },
    {
        "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners",
        "authors": [
            "Seonghyeon Ye",
            "Doyoung Kim",
            "Joel Jang",
            "Joongbo Shin",
            "Minjoon Seo"
        ],
        "published": "2022-10-06T15:00:47Z",
        "summary": "Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.",
        "pdf_link": "https://arxiv.org/pdf/2210.02969v4.pdf"
    },
    {
        "title": "Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation",
        "authors": [
            "Xu Guo",
            "Boyang Li",
            "Han Yu"
        ],
        "published": "2022-10-06T14:44:21Z",
        "summary": "Prompt tuning, or the conditioning of a frozen pretrained language model\n(PLM) with soft prompts learned from data, has demonstrated impressive\nperformance on a wide range of NLP tasks. However, prompt tuning requires a\nlarge training dataset to be effective and is outperformed by finetuning the\nentire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al.,\n2022) proposed to transfer soft prompts pretrained on the source domain to the\ntarget domain. In this paper, we explore domain adaptation for prompt tuning, a\nproblem setting where unlabeled data from the target domain are available\nduring pretraining. We propose bOosting Prompt TunIng with doMain Adaptation\n(OPTIMA), which regularizes the decision boundary to be smooth around regions\nwhere source and target data distributions are similar. Extensive experiments\ndemonstrate that OPTIMA significantly enhances the transferability and\nsample-efficiency of prompt tuning compared to strong baselines. Moreover, in\nfew-shot settings, OPTIMA exceeds full-model tuning by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2210.02952v2.pdf"
    },
    {
        "title": "Generative Entity Typing with Curriculum Learning",
        "authors": [
            "Siyu Yuan",
            "Deqing Yang",
            "Jiaqing Liang",
            "Zhixu Li",
            "Jinxi Liu",
            "Jingyue Huang",
            "Yanghua Xiao"
        ],
        "published": "2022-10-06T13:32:50Z",
        "summary": "Entity typing aims to assign types to the entity mentions in given texts. The\ntraditional classification-based entity typing paradigm has two unignorable\ndrawbacks: 1) it fails to assign an entity to the types beyond the predefined\ntype set, and 2) it can hardly handle few-shot and zero-shot situations where\nmany long-tail types only have few or even no training instances. To overcome\nthese drawbacks, we propose a novel generative entity typing (GET) paradigm:\ngiven a text with an entity mention, the multiple types for the role that the\nentity plays in the text are generated with a pre-trained language model (PLM).\nHowever, PLMs tend to generate coarse-grained types after fine-tuning upon the\nentity typing dataset. Besides, we only have heterogeneous training data\nconsisting of a small portion of human-annotated data and a large portion of\nauto-generated but low-quality data. To tackle these problems, we employ\ncurriculum learning (CL) to train our GET model upon the heterogeneous data,\nwhere the curriculum could be self-adjusted with the self-paced learning\naccording to its comprehension of the type granularity and data heterogeneity.\nOur extensive experiments upon the datasets of different languages and\ndownstream tasks justify the superiority of our GET model over the\nstate-of-the-art entity typing models. The code has been released on\nhttps://github.com/siyuyuan/GET.",
        "pdf_link": "https://arxiv.org/pdf/2210.02914v2.pdf"
    },
    {
        "title": "Binding Language Models in Symbolic Languages",
        "authors": [
            "Zhoujun Cheng",
            "Tianbao Xie",
            "Peng Shi",
            "Chengzu Li",
            "Rahul Nadkarni",
            "Yushi Hu",
            "Caiming Xiong",
            "Dragomir Radev",
            "Mari Ostendorf",
            "Luke Zettlemoyer",
            "Noah A. Smith",
            "Tao Yu"
        ],
        "published": "2022-10-06T12:55:17Z",
        "summary": "Though end-to-end neural approaches have recently been dominating NLP tasks\nin both performance and ease-of-use, they lack interpretability and robustness.\nWe propose Binder, a training-free neural-symbolic framework that maps the task\ninput to a program, which (1) allows binding a unified API of language model\n(LM) functionalities to a programming language (e.g., SQL, Python) to extend\nits grammar coverage and thus tackle more diverse questions, (2) adopts an LM\nas both the program parser and the underlying model called by the API during\nexecution, and (3) requires only a few in-context exemplar annotations.\nSpecifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only\na few in-context exemplars, Codex is able to identify the part of the task\ninput that cannot be answerable by the original programming language, correctly\ngenerate API calls to prompt Codex to solve the unanswerable part, and identify\nwhere to place the API calls while being compatible with the original grammar.\nIn the execution stage, Codex can perform versatile functionalities (e.g.,\ncommonsense QA, information extraction) given proper prompts in the API calls.\nBinder achieves state-of-the-art results on WikiTableQuestions and TabFact\ndatasets, with explicit output programs that benefit human debugging. Note that\nprevious best systems are all finetuned on tens of thousands of task-specific\nsamples, while Binder only uses dozens of annotations as in-context exemplars\nwithout any training. Our code is available at https://github.com/HKUNLP/Binder .",
        "pdf_link": "https://arxiv.org/pdf/2210.02875v2.pdf"
    },
    {
        "title": "Vision Transformer Based Model for Describing a Set of Images as a Story",
        "authors": [
            "Zainy M. Malakan",
            "Ghulam Mubashar Hassan",
            "Ajmal Mian"
        ],
        "published": "2022-10-06T09:01:50Z",
        "summary": "Visual Story-Telling is the process of forming a multi-sentence story from a\nset of images. Appropriately including visual variation and contextual\ninformation captured inside the input images is one of the most challenging\naspects of visual storytelling. Consequently, stories developed from a set of\nimages often lack cohesiveness, relevance, and semantic relationship. In this\npaper, we propose a novel Vision Transformer Based Model for describing a set\nof images as a story. The proposed method extracts the distinct features of the\ninput images using a Vision Transformer (ViT). Firstly, input images are\ndivided into 16X16 patches and bundled into a linear projection of flattened\npatches. The transformation from a single image to multiple image patches\ncaptures the visual variety of the input visual patterns. These features are\nused as input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches. Then, an\nattention mechanism is implemented and used to increase the discriminatory\ncapacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The\nperformance of our proposed model is evaluated using the Visual Story-Telling\ndataset (VIST), and the results show that our model outperforms the current\nstate of the art models.",
        "pdf_link": "https://arxiv.org/pdf/2210.02762v3.pdf"
    },
    {
        "title": "Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer",
        "authors": [
            "Jianyi Zhang",
            "Yiran Chen",
            "Jianshu Chen"
        ],
        "published": "2022-10-06T07:39:58Z",
        "summary": "Developing neural architectures that are capable of logical reasoning has\nbecome increasingly important for a wide range of applications (e.g., natural\nlanguage processing). Towards this grand objective, we propose a symbolic\nreasoning architecture that chains many join operators together to model output\nlogical expressions. In particular, we demonstrate that such an ensemble of\njoin-chains can express a broad subset of ''tree-structured'' first-order\nlogical expressions, named FOET, which is particularly useful for modeling\nnatural languages. To endow it with differentiable learning capability, we\nclosely examine various neural operators for approximating the symbolic\njoin-chains. Interestingly, we find that the widely used multi-head\nself-attention module in transformer can be understood as a special neural\noperator that implements the union bound of the join operator in probabilistic\npredicate space. Our analysis not only provides a new perspective on the\nmechanism of the pretrained models such as BERT for natural language\nunderstanding but also suggests several important future improvement\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2210.02729v3.pdf"
    },
    {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik Narasimhan",
            "Yuan Cao"
        ],
        "published": "2022-10-06T01:00:32Z",
        "summary": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io",
        "pdf_link": "https://arxiv.org/pdf/2210.03629v3.pdf"
    },
    {
        "title": "Generalization Properties of Retrieval-based Models",
        "authors": [
            "Soumya Basu",
            "Ankit Singh Rawat",
            "Manzil Zaheer"
        ],
        "published": "2022-10-06T00:33:01Z",
        "summary": "Many modern high-performing machine learning models such as GPT-3 primarily\nrely on scaling up models, e.g., transformer networks. Simultaneously, a\nparallel line of work aims to improve the model performance by augmenting an\ninput instance with other (labeled) instances during inference. Examples of\nsuch augmentations include task-specific prompts and similar examples retrieved\nfrom the training data by a nonparametric component. Remarkably,\nretrieval-based methods have enjoyed success on a wide range of problems,\nranging from standard natural language processing and vision tasks to protein\nfolding, as demonstrated by many recent efforts, including WebGPT and\nAlphaFold. Despite growing literature showcasing the promise of these models,\nthe theoretical underpinning for such models remains underexplored. In this\npaper, we present a formal treatment of retrieval-based models to characterize\ntheir generalization ability. In particular, we focus on two classes of\nretrieval-based classification approaches: First, we analyze a local learning\nframework that employs an explicit local empirical risk minimization based on\nretrieved examples for each input instance. Interestingly, we show that\nbreaking down the underlying learning task into local sub-tasks enables the\nmodel to employ a low complexity parametric component to ensure good overall\naccuracy. The second class of retrieval-based approaches we explore learns a\nglobal model using kernel methods to directly map an input instance and\nretrieved examples to a prediction, without explicitly solving a local learning\ntask.",
        "pdf_link": "https://arxiv.org/pdf/2210.02617v1.pdf"
    },
    {
        "title": "Learning to Reason With Relational Abstractions",
        "authors": [
            "Andrew J. Nam",
            "Mengye Ren",
            "Chelsea Finn",
            "James L. McClelland"
        ],
        "published": "2022-10-06T00:27:50Z",
        "summary": "Large language models have recently shown promising progress in mathematical\nreasoning when fine-tuned with human-generated sequences walking through a\nsequence of solution steps. However, the solution sequences are not formally\nstructured and the resulting model-generated sequences may not reflect the kind\nof systematic reasoning we might expect an expert human to produce. In this\npaper, we study how to build stronger reasoning capability in language models\nusing the idea of relational abstractions. We introduce new types of sequences\nthat more explicitly provide an abstract characterization of the transitions\nthrough intermediate solution steps to the goal state. We find that models that\nare supplied with such sequences as prompts can solve tasks with a\nsignificantly higher accuracy, and models that are trained to produce such\nsequences solve problems better than those that are trained with previously\nused human-generated sequences and other baselines. Our work thus takes several\nsteps toward elucidating and improving how language models perform on tasks\nrequiring multi-step mathematical reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2210.02615v2.pdf"
    },
    {
        "title": "CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations",
        "authors": [
            "Vasista Sai Lodagala",
            "Sreyan Ghosh",
            "S. Umesh"
        ],
        "published": "2022-10-05T22:44:35Z",
        "summary": "While Self-Supervised Learning has helped reap the benefit of the scale from\nthe available unlabeled data, the learning paradigms are continuously being\nbettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which\nuses clustering and an augmentation-based cross-contrastive loss as its\nself-supervised objective. Through the clustering module, we scale down the\ninfluence of those negative examples that are highly similar to the positive.\nThe Cross-Contrastive loss is computed between the encoder output of the\noriginal sample and the quantizer output of its augmentation and vice-versa,\nbringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up\nto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on\nthe test-clean and test-other sets, respectively, of LibriSpeech, without the\nuse of any language model. The proposed method also achieves up to 14.9%\nrelative WER improvement over the baseline wav2vec 2.0 when fine-tuned on\nSwitchboard data. We make all our codes publicly available on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2210.02592v3.pdf"
    },
    {
        "title": "Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption",
        "authors": [
            "Garam Lee",
            "Minsoo Kim",
            "Jai Hyun Park",
            "Seung-won Hwang",
            "Jung Hee Cheon"
        ],
        "published": "2022-10-05T21:46:02Z",
        "summary": "Embeddings, which compress information in raw text into semantics-preserving\nlow-dimensional vectors, have been widely adopted for their efficacy. However,\nrecent research has shown that embeddings can potentially leak private\ninformation about sensitive attributes of the text, and in some cases, can be\ninverted to recover the original input text. To address these growing privacy\nchallenges, we propose a privatization mechanism for embeddings based on\nhomomorphic encryption, to prevent potential leakage of any piece of\ninformation in the process of text classification. In particular, our method\nperforms text classification on the encryption of embeddings from\nstate-of-the-art models like BERT, supported by an efficient GPU implementation\nof CKKS encryption scheme. We show that our method offers encrypted protection\nof BERT embeddings, while largely preserving their utility on downstream text\nclassification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.02574v1.pdf"
    },
    {
        "title": "Reprogramming Pretrained Language Models for Antibody Sequence Infilling",
        "authors": [
            "Igor Melnyk",
            "Vijil Chenthamarakshan",
            "Pin-Yu Chen",
            "Payel Das",
            "Amit Dhurandhar",
            "Inkit Padhi",
            "Devleena Das"
        ],
        "published": "2022-10-05T20:44:55Z",
        "summary": "Antibodies comprise the most versatile class of binding molecules, with\nnumerous applications in biomedicine. Computational design of antibodies\ninvolves generating novel and diverse sequences, while maintaining structural\nconsistency. Unique to antibodies, designing the complementarity-determining\nregion (CDR), which determines the antigen binding affinity and specificity,\ncreates its own unique challenges. Recent deep learning models have shown\nimpressive results, however the limited number of known antibody\nsequence/structure pairs frequently leads to degraded performance, particularly\nlacking diversity in the generated sequences. In our work we address this\nchallenge by leveraging Model Reprogramming (MR), which repurposes pretrained\nmodels on a source language to adapt to the tasks that are in a different\nlanguage and have scarce data - where it may be difficult to train a\nhigh-performing model from scratch or effectively fine-tune an existing\npre-trained model on the specific task. Specifically, we introduce ReprogBert\nin which a pretrained English language model is repurposed for protein sequence\ninfilling - thus considers cross-language adaptation using less data. Results\non antibody design benchmarks show that our model on low-resourced antibody\nsequence dataset provides highly diverse CDR sequences, up to more than a\ntwo-fold increase of diversity over the baselines, without losing structural\nintegrity and naturalness. The generated sequences also demonstrate enhanced\nantigen binding specificity and virus neutralization ability. Code is available\nat https://github.com/IBM/ReprogBERT",
        "pdf_link": "https://arxiv.org/pdf/2210.07144v2.pdf"
    },
    {
        "title": "Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors",
        "authors": [
            "Mohammad Reza Taesiri",
            "Finlay Macklon",
            "Yihe Wang",
            "Hengshuo Shen",
            "Cor-Paul Bezemer"
        ],
        "published": "2022-10-05T18:44:35Z",
        "summary": "Video game testing requires game-specific knowledge as well as common sense\nreasoning about the events in the game. While AI-driven agents can satisfy the\nfirst requirement, it is not yet possible to meet the second requirement\nautomatically. Therefore, video game testing often still relies on manual\ntesting, and human testers are required to play the game thoroughly to detect\nbugs. As a result, it is challenging to fully automate game testing. In this\nstudy, we explore the possibility of leveraging the zero-shot capabilities of\nlarge language models for video game bug detection. By formulating the bug\ndetection problem as a question-answering task, we show that large language\nmodels can identify which event is buggy in a sequence of textual descriptions\nof events from a game. To this end, we introduce the GameBugDescriptions\nbenchmark dataset, which consists of 167 buggy gameplay videos and a total of\n334 question-answer pairs across 8 games. We extensively evaluate the\nperformance of six models across the OPT and InstructGPT large language model\nfamilies on our benchmark dataset. Our results show promising results for\nemploying language models to detect video game bugs. With the proper prompting\ntechnique, we could achieve an accuracy of 70.66%, and on some video games, up\nto 78.94%. Our code, evaluation data and the benchmark can be found on\nhttps://asgaardlab.github.io/LLMxBugs",
        "pdf_link": "https://arxiv.org/pdf/2210.02506v1.pdf"
    },
    {
        "title": "Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model",
        "authors": [
            "Jacob Eisenstein",
            "Daniel Andor",
            "Bernd Bohnet",
            "Michael Collins",
            "David Mimno"
        ],
        "published": "2022-10-05T18:23:49Z",
        "summary": "Explainable question answering systems should produce not only accurate\nanswers but also rationales that justify their reasoning and allow humans to\ncheck their work. But what sorts of rationales are useful and how can we train\nsystems to produce them? We propose a new style of rationale for open-book\nquestion answering, called \\emph{markup-and-mask}, which combines aspects of\nextractive and free-text explanations. In the markup phase, the passage is\naugmented with free-text markup that enables each sentence to stand on its own\noutside the discourse context. In the masking phase, a sub-span of the\nmarked-up passage is selected. To train a system to produce markup-and-mask\nrationales without annotations, we leverage in-context learning. Specifically,\nwe generate silver annotated data by sending a series of prompts to a frozen\npretrained language model, which acts as a teacher. We then fine-tune a smaller\nstudent model by training on the subset of rationales that led to correct\nanswers. The student is \"honest\" in the sense that it is a pipeline: the\nrationale acts as a bottleneck between the passage and the answer, while the\n\"untrusted\" teacher operates under no such constraints. Thus, we offer a new\nway to build trustworthy pipeline systems from a combination of end-task\nannotations and frozen pretrained language models.",
        "pdf_link": "https://arxiv.org/pdf/2210.02498v2.pdf"
    },
    {
        "title": "Ask Me Anything: A simple strategy for prompting language models",
        "authors": [
            "Simran Arora",
            "Avanika Narayan",
            "Mayee F. Chen",
            "Laurel Orr",
            "Neel Guha",
            "Kush Bhatia",
            "Ines Chami",
            "Frederic Sala",
            "Christopher R\u00e9"
        ],
        "published": "2022-10-05T17:59:45Z",
        "summary": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt that demonstrates how to perform the task and\nno additional training. Prompting is a brittle process wherein small\nmodifications to the prompt can cause large variations in the model\npredictions, and therefore significant effort is dedicated towards designing a\npainstakingly \"perfect prompt\" for a task. To mitigate the high degree of\neffort involved in prompt-design, we instead ask whether producing multiple\neffective, yet imperfect, prompts and aggregating them can lead to a high\nquality prompting strategy. Our observations motivate our proposed prompting\nmethod, ASK ME ANYTHING (AMA). We first develop an understanding of the\neffective prompt formats, finding that question-answering (QA) prompts, which\nencourage open-ended generation (\"Who went to the park?\") tend to outperform\nthose that restrict the model outputs (\"John went to the park. Output True or\nFalse.\"). Our approach recursively uses the LLM itself to transform task inputs\nto the effective QA format. We apply the collected prompts to obtain several\nnoisy votes for the input's true label. We find that the prompts can have very\ndifferent accuracies and complex dependencies and thus propose to use weak\nsupervision, a procedure for combining the noisy predictions, to produce the\nfinal predictions for the inputs. We evaluate AMA across open-source model\nfamilies (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B\nparameters), demonstrating an average performance lift of 10.2% over the\nfew-shot baseline. This simple strategy enables the open-source GPT-J-6B model\nto match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular\nbenchmarks. Averaged across these tasks, the GPT-J-6B model outperforms\nfew-shot GPT3-175B. We release our code here:\nhttps://github.com/HazyResearch/ama_prompting",
        "pdf_link": "https://arxiv.org/pdf/2210.02441v3.pdf"
    },
    {
        "title": "GLM-130B: An Open Bilingual Pre-trained Model",
        "authors": [
            "Aohan Zeng",
            "Xiao Liu",
            "Zhengxiao Du",
            "Zihan Wang",
            "Hanyu Lai",
            "Ming Ding",
            "Zhuoyi Yang",
            "Yifan Xu",
            "Wendi Zheng",
            "Xiao Xia",
            "Weng Lam Tam",
            "Zixuan Ma",
            "Yufei Xue",
            "Jidong Zhai",
            "Wenguang Chen",
            "Peng Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2022-10-05T17:34:44Z",
        "summary": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\n\\url{https://github.com/THUDM/GLM-130B/}.",
        "pdf_link": "https://arxiv.org/pdf/2210.02414v2.pdf"
    },
    {
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
        "authors": [
            "Tushar Khot",
            "Harsh Trivedi",
            "Matthew Finlayson",
            "Yao Fu",
            "Kyle Richardson",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2022-10-05T17:28:20Z",
        "summary": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.",
        "pdf_link": "https://arxiv.org/pdf/2210.02406v2.pdf"
    },
    {
        "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
        "authors": [
            "Mohammad Mahdi Derakhshani",
            "Enrique Sanchez",
            "Adrian Bulat",
            "Victor Guilherme Turrisi da Costa",
            "Cees G. M. Snoek",
            "Georgios Tzimiropoulos",
            "Brais Martinez"
        ],
        "published": "2022-10-05T17:05:56Z",
        "summary": "Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\nCode available at: https://github.com/saic-fi/Bayesian-Prompt-Learning",
        "pdf_link": "https://arxiv.org/pdf/2210.02390v3.pdf"
    },
    {
        "title": "Antibody Representation Learning for Drug Discovery",
        "authors": [
            "Lin Li",
            "Esther Gupta",
            "John Spaeth",
            "Leslie Shing",
            "Tristan Bepler",
            "Rajmonda Sulo Caceres"
        ],
        "published": "2022-10-05T13:48:41Z",
        "summary": "Therapeutic antibody development has become an increasingly popular approach\nfor drug development. To date, antibody therapeutics are largely developed\nusing large scale experimental screens of antibody libraries containing\nhundreds of millions of antibody sequences. The high cost and difficulty of\ndeveloping therapeutic antibodies create a pressing need for computational\nmethods to predict antibody properties and create bespoke designs. However, the\nrelationship between antibody sequence and activity is a complex physical\nprocess and traditional iterative design approaches rely on large scale assays\nand random mutagenesis. Deep learning methods have emerged as a promising way\nto learn antibody property predictors, but predicting antibody properties and\ntarget-specific activities depends critically on the choice of antibody\nrepresentations and data linking sequences to properties is often limited.\nExisting works have not yet investigated the value, limitations and\nopportunities of these methods in application to antibody-based drug discovery.\nIn this paper, we present results on a novel SARS-CoV-2 antibody binding\ndataset and an additional benchmark dataset. We compare three classes of\nmodels: conventional statistical sequence models, supervised learning on each\ndataset independently, and fine-tuning an antibody specific pre-trained\nlanguage model. Experimental results suggest that self-supervised pretraining\nof feature representation consistently offers significant improvement in over\nprevious approaches. We also investigate the impact of data size on the model\nperformance, and discuss challenges and opportunities that the machine learning\ncommunity can address to advance in silico engineering and design of\ntherapeutic antibodies.",
        "pdf_link": "https://arxiv.org/pdf/2210.02881v1.pdf"
    },
    {
        "title": "Grounding Language with Visual Affordances over Unstructured Data",
        "authors": [
            "Oier Mees",
            "Jessica Borja-Diaz",
            "Wolfram Burgard"
        ],
        "published": "2022-10-04T21:16:48Z",
        "summary": "Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\nhttp://hulc2.cs.uni-freiburg.de",
        "pdf_link": "https://arxiv.org/pdf/2210.01911v3.pdf"
    },
    {
        "title": "Towards Improving Faithfulness in Abstractive Summarization",
        "authors": [
            "Xiuying Chen",
            "Mingzhe Li",
            "Xin Gao",
            "Xiangliang Zhang"
        ],
        "published": "2022-10-04T19:52:09Z",
        "summary": "Despite the success achieved in neural abstractive summarization based on\npre-trained language models, one unresolved issue is that the generated\nsummaries are not always faithful to the input document. There are two possible\ncauses of the unfaithfulness problem: (1) the summarization model fails to\nunderstand or capture the gist of the input text, and (2) the model over-relies\non the language model to generate fluent but inadequate words. In this work, we\npropose a Faithfulness Enhanced Summarization model (FES), which is designed\nfor addressing these two problems and improving faithfulness in abstractive\nsummarization. For the first problem, we propose to use question-answering (QA)\nto examine whether the encoder fully grasps the input document and can answer\nthe questions on the key information in the input. The QA attention on the\nproper input words can also be used to stipulate how the decoder should attend\nto the source. For the second problem, we introduce a max-margin loss defined\non the difference between the language and the summarization model, aiming to\nprevent the overconfidence of the language model. Extensive experiments on two\nbenchmark summarization datasets, CNN/DM and XSum, demonstrate that our model\nsignificantly outperforms strong baselines. The evaluation of factual\nconsistency also shows that our model generates more faithful summaries than\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.01877v1.pdf"
    },
    {
        "title": "Explaining Patterns in Data with Language Models via Interpretable Autoprompting",
        "authors": [
            "Chandan Singh",
            "John X. Morris",
            "Jyoti Aneja",
            "Alexander M. Rush",
            "Jianfeng Gao"
        ],
        "published": "2022-10-04T18:32:14Z",
        "summary": "Large language models (LLMs) have displayed an impressive ability to harness\nnatural language to perform complex tasks. In this work, we explore whether we\ncan leverage this learned ability to find and explain patterns in data.\nSpecifically, given a pre-trained LLM and data examples, we introduce\ninterpretable autoprompting (iPrompt), an algorithm that generates a\nnatural-language string explaining the data. iPrompt iteratively alternates\nbetween generating explanations with an LLM and reranking them based on their\nperformance when used as a prompt. Experiments on a wide range of datasets,\nfrom synthetic mathematics to natural-language understanding, show that iPrompt\ncan yield meaningful insights by accurately finding groundtruth dataset\ndescriptions. Moreover, the prompts produced by iPrompt are simultaneously\nhuman-interpretable and highly effective for generalization: on real-world\nsentiment classification datasets, iPrompt produces prompts that match or even\nimprove upon human-written prompts for GPT-3. Finally, experiments with an fMRI\ndataset show the potential for iPrompt to aid in scientific discovery. All code\nfor using the methods and data here is made available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2210.01848v2.pdf"
    },
    {
        "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
        "authors": [
            "Zhijing Jin",
            "Sydney Levine",
            "Fernando Gonzalez",
            "Ojasv Kamal",
            "Maarten Sap",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Josh Tenenbaum",
            "Bernhard Sch\u00f6lkopf"
        ],
        "published": "2022-10-04T09:04:27Z",
        "summary": "AI systems are becoming increasingly intertwined with human life. In order to\neffectively collaborate with humans and ensure safety, AI systems need to be\nable to understand, interpret and predict human moral judgments and decisions.\nHuman moral judgments are often guided by rules, but not always. A central\nchallenge for AI safety is capturing the flexibility of the human moral mind --\nthe ability to determine when a rule should be broken, especially in novel or\nunusual situations. In this paper, we present a novel challenge set consisting\nof rule-breaking question answering (RBQA) of cases that involve potentially\npermissible rule-breaking -- inspired by recent moral psychology studies. Using\na state-of-the-art large language model (LLM) as a basis, we propose a novel\nmoral chain of thought (MORALCOT) prompting strategy that combines the\nstrengths of LLMs with theories of moral reasoning developed in cognitive\nscience to predict human moral judgments. MORALCOT outperforms seven existing\nLLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to\ncapture the flexibility of the human moral mind. We also conduct a detailed\nerror analysis to suggest directions for future work to improve AI safety using\nRBQA. Our data is open-sourced at\nhttps://huggingface.co/datasets/feradauto/MoralExceptQA and code at\nhttps://github.com/feradauto/MoralCoT",
        "pdf_link": "https://arxiv.org/pdf/2210.01478v3.pdf"
    },
    {
        "title": "Continuous Monte Carlo Graph Search",
        "authors": [
            "Kalle Kujanp\u00e4\u00e4",
            "Amin Babadi",
            "Yi Zhao",
            "Juho Kannala",
            "Alexander Ilin",
            "Joni Pajarinen"
        ],
        "published": "2022-10-04T07:34:06Z",
        "summary": "Online planning is crucial for high performance in many complex sequential\ndecision-making tasks. Monte Carlo Tree Search (MCTS) employs a principled\nmechanism for trading off exploration for exploitation for efficient online\nplanning, and it outperforms comparison methods in many discrete\ndecision-making domains such as Go, Chess, and Shogi. Subsequently, extensions\nof MCTS to continuous domains have been developed. However, the inherent high\nbranching factor and the resulting explosion of the search tree size are\nlimiting the existing methods. To address this problem, we propose Continuous\nMonte Carlo Graph Search (CMCGS), an extension of MCTS to online planning in\nenvironments with continuous state and action spaces. CMCGS takes advantage of\nthe insight that, during planning, sharing the same action policy between\nseveral states can yield high performance. To implement this idea, at each time\nstep, CMCGS clusters similar states into a limited number of stochastic action\nbandit nodes, which produce a layered directed graph instead of an MCTS search\ntree. Experimental evaluation shows that CMCGS outperforms comparable planning\nmethods in several complex continuous DeepMind Control Suite benchmarks and 2D\nnavigation and exploration tasks with limited sample budgets. Furthermore,\nCMCGS can be scaled up through parallelization, and it outperforms the\nCross-Entropy Method (CEM) in continuous control with learned dynamics models.",
        "pdf_link": "https://arxiv.org/pdf/2210.01426v3.pdf"
    },
    {
        "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression",
        "authors": [
            "Chen Liang",
            "Simiao Zuo",
            "Qingru Zhang",
            "Pengcheng He",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2022-10-04T03:36:53Z",
        "summary": "Layer-wise distillation is a powerful tool to compress large models (i.e.\nteacher models) into small ones (i.e., student models). The student distills\nknowledge from the teacher by mimicking the hidden representations of the\nteacher at every intermediate layer. However, layer-wise distillation is\ndifficult. Since the student has a smaller model capacity than the teacher, it\nis often under-fitted. Furthermore, the hidden representations of the teacher\ncontain redundant information that the student does not necessarily need for\nthe target task's learning. To address these challenges, we propose a novel\nTask-aware layEr-wise Distillation (TED). TED designs task-aware filters to\nalign the hidden representations of the student and the teacher at each layer.\nThe filters select the knowledge that is useful for the target task from the\nhidden representations. As such, TED reduces the knowledge gap between the two\nmodels and helps the student to fit better on the target task. We evaluate TED\nin two scenarios: continual pre-training and fine-tuning. TED demonstrates\nsignificant and consistent improvements over existing distillation methods in\nboth scenarios. Code is available at\nhttps://github.com/cliang1453/task-aware-distillation.",
        "pdf_link": "https://arxiv.org/pdf/2210.01351v3.pdf"
    },
    {
        "title": "Recitation-Augmented Language Models",
        "authors": [
            "Zhiqing Sun",
            "Xuezhi Wang",
            "Yi Tay",
            "Yiming Yang",
            "Denny Zhou"
        ],
        "published": "2022-10-04T00:49:20Z",
        "summary": "We propose a new paradigm to help Large Language Models (LLMs) generate more\naccurate factual knowledge without retrieving from an external corpus, called\nRECITation-augmented gEneration (RECITE). Different from retrieval-augmented\nlanguage models that retrieve relevant documents before generating the outputs,\ngiven an input, RECITE first recites one or several relevant passages from\nLLMs' own memory via sampling, and then produces the final answers. We show\nthat RECITE is a powerful paradigm for knowledge-intensive NLP tasks.\nSpecifically, we show that by utilizing recitation as the intermediate step, a\nrecite-and-answer scheme can achieve new state-of-the-art performance in\nvarious closed-book question answering (CBQA) tasks. In experiments, we verify\nthe effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and\nCodex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our\ncode is available at \"https://github.com/Edward-Sun/RECITE\".",
        "pdf_link": "https://arxiv.org/pdf/2210.01296v2.pdf"
    },
    {
        "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
        "authors": [
            "Batu Ozturkler",
            "Nikolay Malkin",
            "Zhen Wang",
            "Nebojsa Jojic"
        ],
        "published": "2022-10-04T00:34:01Z",
        "summary": "Large language models (LLMs) have a substantial capacity for high-level\nanalogical reasoning: reproducing patterns in linear text that occur in their\ntraining data (zero-shot evaluation) or in the provided context (few-shot\nin-context learning). However, recent studies show that even the more advanced\nLLMs fail in scenarios that require reasoning over multiple objects or facts\nand making sequences of logical deductions. We propose a two-stage\nprobabilistic inference paradigm, ThinkSum, which reasons over sets of objects\nor facts in a structured manner. In the first stage (Think - retrieval of\nassociations), a LLM is queried in parallel over a set of phrases extracted\nfrom the prompt or an auxiliary model call. In the second stage (Sum -\nprobabilistic inference or reasoning), the results of these queries are\naggregated to make the final prediction. We demonstrate the possibilities and\nadvantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks,\nachieving improvements over the state of the art using GPT-family models on\nthirteen difficult tasks, often with far smaller model variants. We also\ncompare and contrast ThinkSum with other proposed modifications to direct\nprompting of LLMs, such as variants of chain-of-thought prompting. Our results\nsuggest that because the probabilistic inference in ThinkSum is performed\noutside of calls to the LLM, ThinkSum is less sensitive to prompt design,\nyields more interpretable predictions, and can be flexibly combined with latent\nvariable models to extract structured knowledge from LLMs. Overall, our\nproposed paradigm represents a promising approach for enhancing the reasoning\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.01293v2.pdf"
    },
    {
        "title": "Robot Task Planning and Situation Handling in Open Worlds",
        "authors": [
            "Yan Ding",
            "Xiaohan Zhang",
            "Saeid Amiri",
            "Nieqing Cao",
            "Hao Yang",
            "Chad Esselink",
            "Shiqi Zhang"
        ],
        "published": "2022-10-04T00:21:00Z",
        "summary": "Automated task planning algorithms have been developed to help robots\ncomplete complex tasks that require multiple actions. Most of those algorithms\nhave been developed for \"closed worlds\" assuming complete world knowledge is\nprovided. However, the real world is generally open, and the robots frequently\nencounter unforeseen situations that can potentially break the planner's\ncompleteness. This paper introduces a novel algorithm (COWP) for open-world\ntask planning and situation handling that dynamically augments the robot's\naction knowledge with task-oriented common sense. In particular, common sense\nis extracted from Large Language Models based on the current task at hand and\nrobot skills. For systematic evaluations, we collected a dataset that includes\n561 execution-time situations in a dining domain, where each situation\ncorresponds to a state instance of a robot being potentially unable to complete\na task using a solution that normally works. Experimental results show that our\napproach significantly outperforms competitive baselines from the literature in\nthe success rate of service tasks. Additionally, we have demonstrated COWP\nusing a mobile manipulator. Supplementary materials are available at:\nhttps://cowplanning.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2210.01287v1.pdf"
    },
    {
        "title": "Enriching Vulnerability Reports Through Automated and Augmented Description Summarization",
        "authors": [
            "Hattan Althebeiti",
            "David Mohaisen"
        ],
        "published": "2022-10-03T22:46:35Z",
        "summary": "Security incidents and data breaches are increasing rapidly, and only a\nfraction of them is being reported. Public vulnerability databases, e.g.,\nnational vulnerability database (NVD) and common vulnerability and exposure\n(CVE), have been leading the effort in documenting vulnerabilities and sharing\nthem to aid defenses. Both are known for many issues, including brief\nvulnerability descriptions. Those descriptions play an important role in\ncommunicating the vulnerability information to security analysts in order to\ndevelop the appropriate countermeasure. Many resources provide additional\ninformation about vulnerabilities, however, they are not utilized to boost\npublic repositories. In this paper, we devise a pipeline to augment\nvulnerability description through third party reference (hyperlink) scrapping.\nTo normalize the description, we build a natural language summarization\npipeline utilizing a pretrained language model that is fine-tuned using labeled\ninstances and evaluate its performance against both human evaluation (golden\nstandard) and computational metrics, showing initial promising results in terms\nof summary fluency, completeness, correctness, and understanding.",
        "pdf_link": "https://arxiv.org/pdf/2210.01260v1.pdf"
    },
    {
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
        "authors": [
            "Rajkumar Ramamurthy",
            "Prithviraj Ammanabrolu",
            "Kiant\u00e9 Brantley",
            "Jack Hessel",
            "Rafet Sifa",
            "Christian Bauckhage",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "2022-10-03T21:38:29Z",
        "summary": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2210.01241v3.pdf"
    },
    {
        "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
        "authors": [
            "Abulhair Saparov",
            "He He"
        ],
        "published": "2022-10-03T21:34:32Z",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.",
        "pdf_link": "https://arxiv.org/pdf/2210.01240v4.pdf"
    },
    {
        "title": "CaiRL: A High-Performance Reinforcement Learning Environment Toolkit",
        "authors": [
            "Per-Arne Andersen",
            "Morten Goodwin",
            "Ole-Christoffer Granmo"
        ],
        "published": "2022-10-03T21:24:04Z",
        "summary": "This paper addresses the dire need for a platform that efficiently provides a\nframework for running reinforcement learning (RL) experiments. We propose the\nCaiRL Environment Toolkit as an efficient, compatible, and more sustainable\nalternative for training learning agents and propose methods to develop more\nefficient environment simulations.\n  There is an increasing focus on developing sustainable artificial\nintelligence. However, little effort has been made to improve the efficiency of\nrunning environment simulations. The most popular development toolkit for\nreinforcement learning, OpenAI Gym, is built using Python, a powerful but slow\nprogramming language. We propose a toolkit written in C++ with the same\nflexibility level but works orders of magnitude faster to make up for Python's\ninefficiency. This would drastically cut climate emissions.\n  CaiRL also presents the first reinforcement learning toolkit with a built-in\nJVM and Flash support for running legacy flash games for reinforcement learning\nresearch. We demonstrate the effectiveness of CaiRL in the classic control\nbenchmark, comparing the execution speed to OpenAI Gym. Furthermore, we\nillustrate that CaiRL can act as a drop-in replacement for OpenAI Gym to\nleverage significantly faster training speeds because of the reduced\nenvironment computation time.",
        "pdf_link": "https://arxiv.org/pdf/2210.01235v1.pdf"
    },
    {
        "title": "ContraCLM: Contrastive Learning For Causal Language Model",
        "authors": [
            "Nihal Jain",
            "Dejiao Zhang",
            "Wasi Uddin Ahmad",
            "Zijian Wang",
            "Feng Nan",
            "Xiaopeng Li",
            "Ming Tan",
            "Ramesh Nallapati",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Bing Xiang"
        ],
        "published": "2022-10-03T18:56:35Z",
        "summary": "Despite exciting progress in causal language models, the expressiveness of\nthe representations is largely limited due to poor discrimination ability. To\nremedy this issue, we present ContraCLM, a novel contrastive learning framework\nat both token-level and sequence-level. We assess ContraCLM on a variety of\ndownstream tasks. We show that ContraCLM enhances discrimination of the\nrepresentations and bridges the gap with the encoder-only models, which makes\ncausal language models better suited for tasks beyond language generation.\nSpecifically, we attain $44\\%$ relative improvement on the Semantic Textual\nSimilarity tasks and $34\\%$ on Code-to-Code Search tasks. Furthermore, by\nimproving the expressiveness of the representations, ContraCLM also boosts the\nsource code generation capability with $9\\%$ relative improvement on execution\naccuracy on the HumanEval benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2210.01185v2.pdf"
    },
    {
        "title": "The (In)Effectiveness of Intermediate Task Training For Domain Adaptation and Cross-Lingual Transfer Learning",
        "authors": [
            "Sovesh Mohapatra",
            "Somesh Mohapatra"
        ],
        "published": "2022-10-03T17:17:07Z",
        "summary": "Transfer learning from large language models (LLMs) has emerged as a powerful\ntechnique to enable knowledge-based fine-tuning for a number of tasks,\nadaptation of models for different domains and even languages. However, it\nremains an open question, if and when transfer learning will work, i.e. leading\nto positive or negative transfer. In this paper, we analyze the knowledge\ntransfer across three natural language processing (NLP) tasks - text\nclassification, sentimental analysis, and sentence similarity, using three LLMs\n- BERT, RoBERTa, and XLNet - and analyzing their performance, by fine-tuning on\ntarget datasets for domain and cross-lingual adaptation tasks, with and without\nan intermediate task training on a larger dataset. Our experiments showed that\nfine-tuning without an intermediate task training can lead to a better\nperformance for most tasks, while more generalized tasks might necessitate a\npreceding intermediate task training step. We hope that this work will act as a\nguide on transfer learning to NLP practitioners.",
        "pdf_link": "https://arxiv.org/pdf/2210.01091v2.pdf"
    },
    {
        "title": "Hypothesis Engineering for Zero-Shot Hate Speech Detection",
        "authors": [
            "Janis Goldzycher",
            "Gerold Schneider"
        ],
        "published": "2022-10-03T13:11:42Z",
        "summary": "Standard approaches to hate speech detection rely on sufficient available\nhate speech annotations. Extending previous work that repurposes natural\nlanguage inference (NLI) models for zero-shot text classification, we propose a\nsimple approach that combines multiple hypotheses to improve English NLI-based\nzero-shot hate speech detection. We first conduct an error analysis for vanilla\nNLI-based zero-shot hate speech detection and then develop four strategies\nbased on this analysis. The strategies use multiple hypotheses to predict\nvarious aspects of an input text and combine these predictions into a final\nverdict. We find that the zero-shot baseline used for the initial error\nanalysis already outperforms commercial systems and fine-tuned BERT-based hate\nspeech detection models on HateCheck. The combination of the proposed\nstrategies further increases the zero-shot accuracy of 79.4% on HateCheck by\n7.9 percentage points (pp), and the accuracy of 69.6% on ETHOS by 10.0pp.",
        "pdf_link": "https://arxiv.org/pdf/2210.00910v1.pdf"
    },
    {
        "title": "Complexity-Based Prompting for Multi-Step Reasoning",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Ashish Sabharwal",
            "Peter Clark",
            "Tushar Khot"
        ],
        "published": "2022-10-03T05:33:27Z",
        "summary": "We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.",
        "pdf_link": "https://arxiv.org/pdf/2210.00720v2.pdf"
    },
    {
        "title": "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model",
        "authors": [
            "Yi-Jen Shih",
            "Hsuan-Fu Wang",
            "Heng-Jui Chang",
            "Layne Berry",
            "Hung-yi Lee",
            "David Harwath"
        ],
        "published": "2022-10-03T04:15:36Z",
        "summary": "Data-driven speech processing models usually perform well with a large amount\nof text supervision, but collecting transcribed speech data is costly.\nTherefore, we propose SpeechCLIP, a novel framework bridging speech and text\nthrough images to enhance speech models without transcriptions. We leverage\nstate-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images\nand spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior\nstate-of-the-art on image-speech retrieval and performs zero-shot speech-text\nretrieval without direct supervision from transcriptions. Moreover, SpeechCLIP\ncan directly retrieve semantically related keywords from speech.",
        "pdf_link": "https://arxiv.org/pdf/2210.00705v2.pdf"
    },
    {
        "title": "Probing of Quantitative Values in Abstractive Summarization Models",
        "authors": [
            "Nathan M. White"
        ],
        "published": "2022-10-03T00:59:50Z",
        "summary": "Abstractive text summarization has recently become a popular approach, but\ndata hallucination remains a serious problem, including with quantitative data.\nWe propose a set of probing tests to evaluate the efficacy of abstract\nsummarization models' modeling of quantitative values found in the input text.\nOur results show that in most cases, the encoders of recent SOTA-performing\nmodels struggle to provide embeddings that adequately represent quantitative\nvalues in the input compared to baselines, and in particular, they outperform\nrandom representations in some, but surprisingly not all, cases. Under our\nassumptions, this suggests that the encoder's performance contributes to the\nquantity hallucination problem. One model type in particular, DistilBART-CDM,\nwas observed to underperform randomly initialized representations for several\nexperiments, and performance versus BERT suggests that standard pretraining and\nfine-tuning approaches for the summarization task may play a role in\nunderperformance for some encoders.",
        "pdf_link": "https://arxiv.org/pdf/2210.00667v1.pdf"
    },
    {
        "title": "A Non-monotonic Self-terminating Language Model",
        "authors": [
            "Eugene Choi",
            "Kyunghyun Cho",
            "Cheolhyoung Lee"
        ],
        "published": "2022-10-03T00:28:44Z",
        "summary": "Recent large-scale neural autoregressive sequence models have shown\nimpressive performances on a variety of natural language generation tasks.\nHowever, their generated sequences often exhibit degenerate properties such as\nnon-termination, undesirable repetition, and premature termination, when\ngenerated with decoding algorithms such as greedy search, beam search, top-$k$\nsampling, and nucleus sampling. In this paper, we focus on the problem of\nnon-terminating sequences resulting from an incomplete decoding algorithm. We\nfirst define an incomplete probable decoding algorithm which includes greedy\nsearch, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding\nalgorithm originally put forward by Welleck et al. (2020). We then propose a\nnon-monotonic self-terminating language model, which significantly relaxes the\nconstraint of monotonically increasing termination probability in the\noriginally proposed self-terminating language model by Welleck et al. (2020),\nto address the issue of non-terminating sequences when using incomplete\nprobable decoding algorithms. We prove that our proposed model prevents\nnon-terminating sequences when using not only incomplete probable decoding\nalgorithms but also beam search. We empirically validate our model on sequence\ncompletion tasks with various architectures.",
        "pdf_link": "https://arxiv.org/pdf/2210.00660v3.pdf"
    },
    {
        "title": "Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis",
        "authors": [
            "Haomiao Ni",
            "Yihao Liu",
            "Sharon X. Huang",
            "Yuan Xue"
        ],
        "published": "2022-10-02T03:09:12Z",
        "summary": "In this paper, we propose a novel dual-branch Transformation-Synthesis\nnetwork (TS-Net), for video motion retargeting. Given one subject video and one\ndriving video, TS-Net can produce a new plausible video with the subject\nappearance of the subject video and motion pattern of the driving video. TS-Net\nconsists of a warp-based transformation branch and a warp-free synthesis\nbranch. The novel design of dual branches combines the strengths of\ndeformation-grid-based transformation and warp-free generation for better\nidentity preservation and robustness to occlusion in the synthesized videos. A\nmask-aware similarity module is further introduced to the transformation branch\nto reduce computational overhead. Experimental results on face and dance\ndatasets show that TS-Net achieves better performance in video motion\nretargeting than several state-of-the-art models as well as its single-branch\nvariants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet.",
        "pdf_link": "https://arxiv.org/pdf/2210.01559v1.pdf"
    },
    {
        "title": "LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings",
        "authors": [
            "Xin Xie",
            "Zhoubo Li",
            "Xiaohan Wang",
            "Zekun Xi",
            "Ningyu Zhang"
        ],
        "published": "2022-10-01T16:01:53Z",
        "summary": "Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph\nstructure and text-rich entity/relation information. Text-based KG embeddings\ncan represent entities by encoding descriptions with pre-trained language\nmodels, but no open-sourced library is specifically designed for KGs with PLMs\nat present. In this paper, we present LambdaKG, a library for KGE that equips\nwith many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and\nsupports various tasks (e.g., knowledge graph completion, question answering,\nrecommendation, and knowledge probing). LambdaKG is publicly open-sourced at\nhttps://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at\nhttp://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.",
        "pdf_link": "https://arxiv.org/pdf/2210.00305v3.pdf"
    },
    {
        "title": "Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles",
        "authors": [
            "Shuhei Mandokoro",
            "Natsuki Oka",
            "Akane Matsushima",
            "Chie Fukada",
            "Yuko Yoshimura",
            "Koji Kawahara",
            "Kazuaki Tanaka"
        ],
        "published": "2022-10-01T13:54:54Z",
        "summary": "Sentence-final particles serve an essential role in spoken Japanese because\nthey express the speaker's mental attitudes toward a proposition and/or an\ninterlocutor. They are acquired at early ages and occur very frequently in\neveryday conversation. However, there has been little proposal for a\ncomputational model of acquiring sentence-final particles. This paper proposes\nSubjective BERT, a self-attention model that takes various subjective senses in\naddition to language and images as input and learns the relationship between\nwords and subjective senses. An evaluation experiment revealed that the model\nunderstands the usage of \"yo\", which expresses the speaker's intention to\ncommunicate new information, and that of \"ne\", which denotes the speaker's\ndesire to confirm that some information is shared.",
        "pdf_link": "https://arxiv.org/pdf/2210.00282v1.pdf"
    },
    {
        "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Xiaoman Pan",
            "Dian Yu",
            "Dong Yu",
            "Jianshu Chen",
            "Heng Ji"
        ],
        "published": "2022-10-01T04:08:50Z",
        "summary": "Although large language models have achieved impressive zero-shot ability,\nthe huge model size generally incurs high cost. Recently, semi-parametric\nlanguage models, which augment a smaller language model with an external\nretriever, have demonstrated promising language modeling capabilities. However,\nit remains unclear whether such semi-parametric language models can perform\ncompetitively well as their fully-parametric counterparts on zero-shot\ngeneralization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a\nzero-shot semi-parametric language model. To our best knowledge, this is the\nfirst semi-parametric language model that can demonstrate strong zero-shot\nperformance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$\nwith a novel semi-parametric multitask prompted training paradigm, which shows\nsignificant improvement compared with the parametric multitask training as\nproposed by T0. Specifically, we augment the multitask training and zero-shot\nevaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In\norder to incorporate multiple potentially noisy retrieved augmentations, we\nfurther propose a novel $\\text{augmentation fusion}$ module leveraging\nperceiver resampler and gated cross-attention. Notably, our proposed\n$\\text{Zemi}_\\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation\ntasks while being 3.9x smaller in model size.",
        "pdf_link": "https://arxiv.org/pdf/2210.00185v2.pdf"
    },
    {
        "title": "Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution",
        "authors": [
            "Emily McMilin"
        ],
        "published": "2022-09-30T23:10:11Z",
        "summary": "Modern language modeling tasks are often underspecified: for a given token\nprediction, many words may satisfy the user's intent of producing natural\nlanguage at inference time, however only one word will minimize the task's loss\nfunction at training time. We introduce a simple causal mechanism to describe\nthe role underspecification plays in the generation of spurious correlations.\nDespite its simplicity, our causal model directly informs the development of\ntwo lightweight black-box evaluation methods, that we apply to gendered pronoun\nresolution tasks on a wide range of LLMs to 1) aid in the detection of\ninference-time task underspecification by exploiting 2) previously unreported\ngender vs. time and gender vs. location spurious correlations on LLMs with a\nrange of A) sizes: from BERT-base to GPT-4 Turbo Preview, B) pre-training\nobjectives: from masked & autoregressive language modeling to a mixture of\nthese objectives, and C) training stages: from pre-training only to\nreinforcement learning from human feedback (RLHF). Code and open-source demos\navailable at https://github.com/2dot71mily/uspec.",
        "pdf_link": "https://arxiv.org/pdf/2210.00131v4.pdf"
    },
    {
        "title": "SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data",
        "authors": [
            "Ziqiang Zhang",
            "Sanyuan Chen",
            "Long Zhou",
            "Yu Wu",
            "Shuo Ren",
            "Shujie Liu",
            "Zhuoyuan Yao",
            "Xun Gong",
            "Lirong Dai",
            "Jinyu Li",
            "Furu Wei"
        ],
        "published": "2022-09-30T09:12:10Z",
        "summary": "How to boost speech pre-training with textual data is an unsolved problem due\nto the fact that speech and text are very different modalities with distinct\ncharacteristics. In this paper, we propose a cross-modal Speech and Language\nModel (SpeechLM) to explicitly align speech and text pre-training with a\npre-defined unified discrete representation. Specifically, we introduce two\nalternative discrete tokenizers to bridge the speech and text modalities,\nincluding phoneme-unit and hidden-unit tokenizers, which can be trained using a\nsmall amount of paired speech-text data. Based on the trained tokenizers, we\nconvert the unlabeled speech and text data into tokens of phoneme units or\nhidden units. The pre-training objective is designed to unify the speech and\nthe text into the same discrete semantic space with a unified Transformer\nnetwork. We evaluate SpeechLM on various spoken language processing tasks\nincluding speech recognition, speech translation, and universal representation\nevaluation framework SUPERB, demonstrating significant improvements on\ncontent-related tasks. Code and models are available at\nhttps://aka.ms/SpeechLM.",
        "pdf_link": "https://arxiv.org/pdf/2209.15329v3.pdf"
    },
    {
        "title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation",
        "authors": [
            "Rita Ramos",
            "Bruno Martins",
            "Desmond Elliott",
            "Yova Kementchedjhieva"
        ],
        "published": "2022-09-30T09:03:22Z",
        "summary": "Recent advances in image captioning have focused on scaling the data and\nmodel size, substantially increasing the cost of pre-training and finetuning.\nAs an alternative to large models, we present SmallCap, which generates a\ncaption conditioned on an input image and related captions retrieved from a\ndatastore. Our model is lightweight and fast to train, as the only learned\nparameters are in newly introduced cross-attention layers between a pre-trained\nCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without\nadditional finetuning and can exploit large-scale data in a training-free\nfashion since the contents of the datastore can be readily replaced. Our\nexperiments show that SmallCap, trained only on COCO, has competitive\nperformance on this benchmark, and also transfers to other domains without\nretraining, solely through retrieval from target-domain data. Further\nimprovement is achieved through the training-free exploitation of diverse\nhuman-labeled and web data, which proves to be effective for a range of\ndomains, including the nocaps benchmark, designed to test generalization to\nunseen visual concepts.",
        "pdf_link": "https://arxiv.org/pdf/2209.15323v2.pdf"
    },
    {
        "title": "On the Impossible Safety of Large AI Models",
        "authors": [
            "El-Mahdi El-Mhamdi",
            "Sadegh Farhadkhani",
            "Rachid Guerraoui",
            "Nirupam Gupta",
            "L\u00ea-Nguy\u00ean Hoang",
            "Rafael Pinot",
            "S\u00e9bastien Rouault",
            "John Stephan"
        ],
        "published": "2022-09-30T06:36:49Z",
        "summary": "Large AI Models (LAIMs), of which large language models are the most\nprominent recent example, showcase some impressive performance. However they\nhave been empirically found to pose serious security issues. This paper\nsystematizes our knowledge about the fundamental impossibility of building\narbitrarily accurate and secure machine learning models. More precisely, we\nidentify key challenging features of many of today's machine learning settings.\nNamely, high accuracy seems to require memorizing large training datasets,\nwhich are often user-generated and highly heterogeneous, with both sensitive\ninformation and fake users. We then survey statistical lower bounds that, we\nargue, constitute a compelling case against the possibility of designing\nhigh-accuracy LAIMs with strong security guarantees.",
        "pdf_link": "https://arxiv.org/pdf/2209.15259v2.pdf"
    },
    {
        "title": "Learning by Distilling Context",
        "authors": [
            "Charlie Snell",
            "Dan Klein",
            "Ruiqi Zhong"
        ],
        "published": "2022-09-30T02:30:15Z",
        "summary": "Language models significantly benefit from context tokens, such as prompts or\nscratchpads. They perform better when prompted with informative instructions,\nand they acquire new reasoning capabilities by generating a scratch-pad before\npredicting the final answers. However, they do not \\textit{internalize} these\nperformance gains, which disappear when the context tokens are gone. Our work\nproposes to apply context distillation so that a language model can improve\nitself by internalizing these gains. Concretely, given a synthetic unlabeled\ninput for the target task, we condition the model on ``[instructions] +\n[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune\nthe same model to predict its own ``[final answer]'' conditioned on the\n``[task-input]'', without seeing the ``[instructions]'' or using the\n``[scratch-pad]''.\n  We show that context distillation is a general method to train language\nmodels, and it can effectively internalize 3 types of training signals. First,\nit can internalize abstract task instructions and explanations, so we can\niteratively update the model parameters with new instructions and overwrite old\nones. Second, it can internalize step-by-step reasoning for complex tasks\n(e.g., 8-digit addition), and such a newly acquired capability proves to be\nuseful for other downstream tasks. Finally, it can internalize concrete\ntraining examples, and it outperforms directly learning with gradient descent\nby 9\\% on the SPIDER Text-to-SQL dataset; furthermore, combining context\ndistillation operations can internalize more training examples than the context\nwindow size allows.",
        "pdf_link": "https://arxiv.org/pdf/2209.15189v1.pdf"
    },
    {
        "title": "Linearly Mapping from Image to Text Space",
        "authors": [
            "Jack Merullo",
            "Louis Castricato",
            "Carsten Eickhoff",
            "Ellie Pavlick"
        ],
        "published": "2022-09-30T01:17:18Z",
        "summary": "The extent to which text-only language models (LMs) learn to represent\nfeatures of the non-linguistic world is an open question. Prior work has shown\nthat pretrained LMs can be taught to caption images when a vision model's\nparameters are optimized to encode images in the language space. We test a\nstronger hypothesis: that the conceptual representations learned by frozen\ntext-only models and vision-only models are similar enough that this can be\nachieved with a linear map. We show that the image representations from vision\nmodels can be transferred as continuous prompts to frozen LMs by training only\na single linear projection. Using these to prompt the LM achieves competitive\nperformance on captioning and visual question answering tasks compared to\nmodels that tune both the image encoder and text decoder (such as the MAGMA\nmodel). We compare three image encoders with increasing amounts of linguistic\nsupervision seen during pretraining: BEIT (no linguistic information),\nNF-ResNET (lexical category information), and CLIP (full natural language\ndescriptions). We find that all three encoders perform equally well at\ntransferring visual property information to the language model (e.g., whether\nan animal is large or small), but that image encoders pretrained with\nlinguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs. elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images. Code is available here:\nhttps://github.com/jmerullo/limber",
        "pdf_link": "https://arxiv.org/pdf/2209.15162v3.pdf"
    },
    {
        "title": "Unpacking Large Language Models with Conceptual Consistency",
        "authors": [
            "Pritish Sahu",
            "Michael Cogswell",
            "Yunye Gong",
            "Ajay Divakaran"
        ],
        "published": "2022-09-29T20:55:57Z",
        "summary": "If a Large Language Model (LLM) answers \"yes\" to the question \"Are mountains\ntall?\" then does it know what a mountain is? Can you rely on it responding\ncorrectly or incorrectly to other questions about mountains? The success of\nLarge Language Models (LLMs) indicates they are increasingly able to answer\nqueries like these accurately, but that ability does not necessarily imply a\ngeneral understanding of concepts relevant to the anchor query. We propose\nconceptual consistency to measure a LLM's understanding of relevant concepts.\nThis novel metric measures how well a model can be characterized by finding out\nhow consistent its responses to queries about conceptually relevant background\nknowledge are. To compute it we extract background knowledge by traversing\npaths between concepts in a knowledge base and then try to predict the model's\nresponse to the anchor query from the background knowledge. We investigate the\nperformance of current LLMs in a commonsense reasoning setting using the CSQA\ndataset and the ConceptNet knowledge base. While conceptual consistency, like\nother metrics, does increase with the scale of the LLM used, we find that\npopular models do not necessarily have high conceptual consistency. Our\nanalysis also shows significant variation in conceptual consistency across\ndifferent kinds of relations, concepts, and prompts. This serves as a step\ntoward building models that humans can apply a theory of mind to, and thus\ninteract with intuitively.",
        "pdf_link": "https://arxiv.org/pdf/2209.15093v1.pdf"
    },
    {
        "title": "Toward Trustworthy Neural Program Synthesis",
        "authors": [
            "Darren Key",
            "Wen-Ding Li",
            "Kevin Ellis"
        ],
        "published": "2022-09-29T20:32:07Z",
        "summary": "We develop an approach to estimate the probability that a program sampled\nfrom a large language model is correct. Given a natural language description of\na programming problem, our method samples both candidate programs as well as\ncandidate predicates specifying how the program should behave. This allows\nlearning a model that forms a well-calibrated probabilistic prediction of\nprogram correctness. Our system also infers which predicates are useful to\nexplain the behavior of the generated code, and humans preferred these in a\nhuman study over raw language model outputs. Our method is simple, easy to\nimplement, and maintains state of the art generation accuracy results.",
        "pdf_link": "https://arxiv.org/pdf/2210.00848v2.pdf"
    },
    {
        "title": "Few-shot Text Classification with Dual Contrastive Consistency",
        "authors": [
            "Liwen Sun",
            "Jiawei Han"
        ],
        "published": "2022-09-29T19:26:23Z",
        "summary": "In this paper, we explore how to utilize pre-trained language model to\nperform few-shot text classification where only a few annotated examples are\ngiven for each class. Since using traditional cross-entropy loss to fine-tune\nlanguage model under this scenario causes serious overfitting and leads to\nsub-optimal generalization of model, we adopt supervised contrastive learning\non few labeled data and consistency-regularization on vast unlabeled data.\nMoreover, we propose a novel contrastive consistency to further boost model\nperformance and refine sentence representation. After conducting extensive\nexperiments on four datasets, we demonstrate that our model (FTCC) can\noutperform state-of-the-art methods and has better robustness.",
        "pdf_link": "https://arxiv.org/pdf/2209.15069v1.pdf"
    },
    {
        "title": "Compositional Semantic Parsing with Large Language Models",
        "authors": [
            "Andrew Drozdov",
            "Nathanael Sch\u00e4rli",
            "Ekin Aky\u00fcrek",
            "Nathan Scales",
            "Xinying Song",
            "Xinyun Chen",
            "Olivier Bousquet",
            "Denny Zhou"
        ],
        "published": "2022-09-29T17:58:28Z",
        "summary": "Humans can reason compositionally when presented with new tasks. Previous\nresearch shows that appropriate prompting techniques enable large language\nmodels (LLMs) to solve artificial compositional generalization tasks such as\nSCAN. In this work, we identify additional challenges in more realistic\nsemantic parsing tasks with larger vocabulary and refine these prompting\ntechniques to address them. Our best method is based on least-to-most\nprompting: it decomposes the problem using prompting-based syntactic parsing,\nthen uses this decomposition to select appropriate exemplars and to\nsequentially generate the semantic parse. This method allows us to set a new\nstate of the art for CFQ while requiring only 1% of the training data used by\ntraditional approaches. Due to the general nature of our approach, we expect\nsimilar efforts will lead to new results in other tasks and domains, especially\nfor knowledge-intensive applications.",
        "pdf_link": "https://arxiv.org/pdf/2209.15003v2.pdf"
    },
    {
        "title": "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging",
        "authors": [
            "Jean Kaddour"
        ],
        "published": "2022-09-29T17:46:13Z",
        "summary": "Training vision or language models on large datasets can take days, if not\nweeks. We show that averaging the weights of the k latest checkpoints, each\ncollected at the end of an epoch, can speed up the training progression in\nterms of loss and accuracy by dozens of epochs, corresponding to time savings\nup to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and\nRoBERTa-Base model on WikiText-103, respectively. We also provide the code and\nmodel checkpoint trajectory to reproduce the results and facilitate research on\nreusing historical weights for faster convergence.",
        "pdf_link": "https://arxiv.org/pdf/2209.14981v2.pdf"
    },
    {
        "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal Features",
        "authors": [
            "Rudra P. K. Poudel",
            "Harit Pandya",
            "Roberto Cipolla"
        ],
        "published": "2022-09-29T16:49:24Z",
        "summary": "In this paper we present a world model, which learns causal features using\nthe invariance principle. In particular, we use contrastive unsupervised\nlearning to learn the invariant causal features, which enforces invariance\nacross augmentations of irrelevant parts or styles of the observation. The\nworld-model-based reinforcement learning methods independently optimize\nrepresentation learning and the policy. Thus naive contrastive loss\nimplementation collapses due to a lack of supervisory signals to the\nrepresentation learning module. We propose an intervention invariant auxiliary\ntask to mitigate this issue. Specifically, we utilize depth prediction to\nexplicitly enforce the invariance and use data augmentation as style\nintervention on the RGB observation space. Our design leverages unsupervised\nrepresentation learning to learn the world model with invariant causal\nfeatures. Our proposed method significantly outperforms current\nstate-of-the-art model-based and model-free reinforcement learning methods on\nout-of-distribution point navigation tasks on the iGibson dataset. Moreover,\nour proposed model excels at the sim-to-real transfer of our perception\nlearning module. Finally, we evaluate our approach on the DeepMind control\nsuite and enforce invariance only implicitly since depth is not available.\nNevertheless, our proposed model performs on par with the state-of-the-art\ncounterpart.",
        "pdf_link": "https://arxiv.org/pdf/2209.14932v1.pdf"
    },
    {
        "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus",
        "authors": [
            "Gang Li",
            "Yang Li"
        ],
        "published": "2022-09-29T16:45:43Z",
        "summary": "Mobile UI understanding is important for enabling various interaction tasks\nsuch as UI automation and accessibility. Previous mobile UI modeling often\ndepends on the view hierarchy information of a screen, which directly provides\nthe structural data of the UI, with the hope to bypass challenging tasks of\nvisual modeling from screen pixels. However, view hierarchies are not always\navailable, and are often corrupted with missing object descriptions or\nmisaligned structure information. As a result, despite the use of view\nhierarchies could offer short-term gains, it may ultimately hinder the\napplicability and performance of the model. In this paper, we propose\nSpotlight, a vision-only approach for mobile UI understanding. Specifically, we\nenhance a vision-language model that only takes the screenshot of the UI and a\nregion of interest on the screen -- the focus -- as the input. This general\narchitecture of Spotlight is easily scalable and capable of performing a range\nof UI modeling tasks. Our experiments show that our model establishes SoTA\nresults on several representative UI tasks and outperforms previous methods\nthat use both screenshots and view hierarchies as inputs. Furthermore, we\nexplore multi-task learning and few-shot prompting capacities of the proposed\nmodels, demonstrating promising results in the multi-task learning direction.",
        "pdf_link": "https://arxiv.org/pdf/2209.14927v4.pdf"
    },
    {
        "title": "Repairing Bugs in Python Assignments Using Large Language Models",
        "authors": [
            "Jialu Zhang",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Ruzica Piskac",
            "Gustavo Soares",
            "Gust Verbruggen"
        ],
        "published": "2022-09-29T15:41:17Z",
        "summary": "Students often make mistakes on their introductory programming assignments as\npart of their learning process. Unfortunately, providing custom repairs for\nthese mistakes can require a substantial amount of time and effort from class\ninstructors. Automated program repair (APR) techniques can be used to\nsynthesize such fixes. Prior work has explored the use of symbolic and neural\ntechniques for APR in the education domain. Both types of approaches require\neither substantial engineering efforts or large amounts of data and training.\nWe propose to use a large language model trained on code, such as Codex, to\nbuild an APR system -- MMAPR -- for introductory Python programming\nassignments. Our system can fix both syntactic and semantic mistakes by\ncombining multi-modal prompts, iterative querying, test-case-based selection of\nfew-shots, and program chunking. We evaluate MMAPR on 286 real student programs\nand compare to a baseline built by combining a state-of-the-art Python syntax\nrepair engine, BIFI, and state-of-the-art Python semantic repair engine for\nstudent assignments, Refactory. We find that MMAPR can fix more programs and\nproduce smaller patches on average.",
        "pdf_link": "https://arxiv.org/pdf/2209.14876v1.pdf"
    },
    {
        "title": "polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics",
        "authors": [
            "Christopher Kuenneth",
            "Rampi Ramprasad"
        ],
        "published": "2022-09-29T14:09:54Z",
        "summary": "Polymers are a vital part of everyday life. Their chemical universe is so\nlarge that it presents unprecedented opportunities as well as significant\nchallenges to identify suitable application-specific candidates. We present a\ncomplete end-to-end machine-driven polymer informatics pipeline that can search\nthis space for suitable candidates at unprecedented speed and accuracy. This\npipeline includes a polymer chemical fingerprinting capability called polyBERT\n(inspired by Natural Language Processing concepts), and a multitask learning\napproach that maps the polyBERT fingerprints to a host of properties. polyBERT\nis a chemical linguist that treats the chemical structure of polymers as a\nchemical language. The present approach outstrips the best presently available\nconcepts for polymer property prediction based on handcrafted fingerprint\nschemes in speed by two orders of magnitude while preserving accuracy, thus\nmaking it a strong candidate for deployment in scalable architectures including\ncloud infrastructures.",
        "pdf_link": "https://arxiv.org/pdf/2209.14803v1.pdf"
    },
    {
        "title": "Prompt-guided Scene Generation for 3D Zero-Shot Learning",
        "authors": [
            "Majid Nasiri",
            "Ali Cheraghian",
            "Townim Faisal Chowdhury",
            "Sahar Ahmadi",
            "Morteza Saberi",
            "Shafin Rahman"
        ],
        "published": "2022-09-29T11:24:33Z",
        "summary": "Zero-shot learning on 3D point cloud data is a related underexplored problem\ncompared to its 2D image counterpart. 3D data brings new challenges for ZSL due\nto the unavailability of robust pre-trained feature extraction models. To\naddress this problem, we propose a prompt-guided 3D scene generation and\nsupervision method that augments 3D data to learn the network better, exploring\nthe complex interplay of seen and unseen objects. First, we merge point clouds\nof two 3D models in certain ways described by a prompt. The prompt acts like\nthe annotation describing each 3D scene. Later, we perform contrastive learning\nto train our proposed architecture in an end-to-end manner. We argue that 3D\nscenes can relate objects more efficiently than single objects because popular\nlanguage models (like BERT) can achieve high performance when objects appear in\na context. Our proposed prompt-guided scene generation method encapsulates data\naugmentation and prompt-based annotation/captioning to improve 3D ZSL\nperformance. We have achieved state-of-the-art ZSL and generalized ZSL\nperformance on synthetic (ModelNet40, ModelNet10) and real-scanned\n(ScanOjbectNN) 3D object datasets.",
        "pdf_link": "https://arxiv.org/pdf/2209.14690v1.pdf"
    },
    {
        "title": "An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation",
        "authors": [
            "Yuqiao Wen",
            "Yongchang Hao",
            "Yanshuai Cao",
            "Lili Mou"
        ],
        "published": "2022-09-29T08:41:32Z",
        "summary": "Open-domain dialogue systems aim to interact with humans through natural\nlanguage texts in an open-ended fashion. Despite the recent success of super\nlarge dialogue systems such as ChatGPT, using medium-to-small-sized dialogue\nsystems remains the common practice as they are more lightweight and\naccessible; however, generating diverse dialogue responses is challenging,\nespecially with smaller models. In this work, we propose an Equal-size Hard\nExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model\nfor diverse dialogue generation. Our algorithm assigns a sample to a decoder in\na hard manner and additionally imposes an equal-assignment constraint to ensure\nthat all decoders are well-trained. We provide detailed theoretical analysis to\njustify our approach. Further, experiments on two large-scale open-domain\ndialogue datasets verify that our EqHard-EM algorithm generates high-quality\ndiverse responses.",
        "pdf_link": "https://arxiv.org/pdf/2209.14627v2.pdf"
    },
    {
        "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Tanmay Rajpurohit",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "published": "2022-09-29T08:01:04Z",
        "summary": "Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nselecting in-context examples.",
        "pdf_link": "https://arxiv.org/pdf/2209.14610v3.pdf"
    },
    {
        "title": "Neural Media Bias Detection Using Distant Supervision With BABE -- Bias Annotations By Experts",
        "authors": [
            "Timo Spinde",
            "Manuel Plank",
            "Jan-David Krieger",
            "Terry Ruas",
            "Bela Gipp",
            "Akiko Aizawa"
        ],
        "published": "2022-09-29T05:32:55Z",
        "summary": "Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of a gold standard data set\nand high context dependencies. This paper presents BABE, a robust and diverse\ndata set created by trained experts, for media bias research. We also analyze\nwhy expert labeling is essential within this domain. Our data set offers better\nannotation quality and higher inter-annotator agreement than existing work. It\nconsists of 3,700 sentences balanced among topics and outlets, containing media\nbias labels on the word and sentence level. Based on our data, we also\nintroduce a way to detect bias-inducing sentences in news articles\nautomatically. Our best performing BERT-based model is pre-trained on a larger\ncorpus consisting of distant labels. Fine-tuning and evaluating the model on\nour proposed supervised data set, we achieve a macro F1-score of 0.804,\noutperforming existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2209.14557v1.pdf"
    },
    {
        "title": "Bidirectional Language Models Are Also Few-shot Learners",
        "authors": [
            "Ajay Patel",
            "Bryan Li",
            "Mohammad Sadegh Rasooli",
            "Noah Constant",
            "Colin Raffel",
            "Chris Callison-Burch"
        ],
        "published": "2022-09-29T01:35:57Z",
        "summary": "Large language models such as GPT-3 (Brown et al., 2020) can perform\narbitrary tasks without undergoing fine-tuning after being prompted with only a\nfew labeled examples. An arbitrary task can be reformulated as a natural\nlanguage prompt, and a language model can be asked to generate the completion,\nindirectly performing the task in a paradigm known as prompt-based learning. To\ndate, emergent prompt-based learning capabilities have mainly been demonstrated\nfor unidirectional language models. However, bidirectional language models\npre-trained on denoising objectives such as masked language modeling produce\nstronger learned representations for transfer learning. This motivates the\npossibility of prompting bidirectional models, but their pre-training\nobjectives have made them largely incompatible with the existing prompting\nparadigm. We present SAP (Sequential Autoregressive Prompting), a technique\nthat enables the prompting of bidirectional models. Utilizing the machine\ntranslation task as a case study, we prompt the bidirectional mT5 model (Xue et\nal., 2021) with SAP and demonstrate its few-shot and zero-shot translations\noutperform the few-shot translations of unidirectional models like GPT-3 and\nXGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We\nfurther show SAP is effective on question answering and summarization. For the\nfirst time, our results demonstrate prompt-based learning is an emergent\nproperty of a broader class of language models, rather than only unidirectional\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2209.14500v2.pdf"
    },
    {
        "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
        "authors": [
            "Kundan Krishna",
            "Saurabh Garg",
            "Jeffrey P. Bigham",
            "Zachary C. Lipton"
        ],
        "published": "2022-09-28T19:28:43Z",
        "summary": "For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.",
        "pdf_link": "https://arxiv.org/pdf/2209.14389v2.pdf"
    },
    {
        "title": "Improving alignment of dialogue agents via targeted human judgements",
        "authors": [
            "Amelia Glaese",
            "Nat McAleese",
            "Maja Tr\u0119bacz",
            "John Aslanides",
            "Vlad Firoiu",
            "Timo Ewalds",
            "Maribeth Rauh",
            "Laura Weidinger",
            "Martin Chadwick",
            "Phoebe Thacker",
            "Lucy Campbell-Gillingham",
            "Jonathan Uesato",
            "Po-Sen Huang",
            "Ramona Comanescu",
            "Fan Yang",
            "Abigail See",
            "Sumanth Dathathri",
            "Rory Greig",
            "Charlie Chen",
            "Doug Fritz",
            "Jaume Sanchez Elias",
            "Richard Green",
            "So\u0148a Mokr\u00e1",
            "Nicholas Fernando",
            "Boxi Wu",
            "Rachel Foley",
            "Susannah Young",
            "Iason Gabriel",
            "William Isaac",
            "John Mellor",
            "Demis Hassabis",
            "Koray Kavukcuoglu",
            "Lisa Anne Hendricks",
            "Geoffrey Irving"
        ],
        "published": "2022-09-28T19:04:43Z",
        "summary": "We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.",
        "pdf_link": "https://arxiv.org/pdf/2209.14375v1.pdf"
    },
    {
        "title": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
        "authors": [
            "Maril\u00f9 Miotto",
            "Nicola Rossberg",
            "Bennett Kleinberg"
        ],
        "published": "2022-09-28T18:07:02Z",
        "summary": "Language models such as GPT-3 have caused a furore in the research community.\nSome studies found that GPT-3 has some creative abilities and makes mistakes\nthat are on par with human behaviour. This paper answers a related question:\nWho is GPT-3? We administered two validated measurement tools to GPT-3 to\nassess its personality, the values it holds and its self-reported demographics.\nOur results show that GPT-3 scores similarly to human samples in terms of\npersonality and - when provided with a model response memory - in terms of the\nvalues it holds. We provide the first evidence of psychological assessment of\nthe GPT-3 model and thereby add to our understanding of this language model. We\nclose with suggestions for future research that moves social science closer to\nlanguage models and vice versa.",
        "pdf_link": "https://arxiv.org/pdf/2209.14338v2.pdf"
    },
    {
        "title": "Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures",
        "authors": [
            "Andrea Lacava",
            "Michele Polese",
            "Rajarajan Sivaraj",
            "Rahul Soundrarajan",
            "Bhawani Shanker Bhati",
            "Tarunjeet Singh",
            "Tommaso Zugno",
            "Francesca Cuomo",
            "Tommaso Melodia"
        ],
        "published": "2022-09-28T15:31:06Z",
        "summary": "5G and beyond mobile networks will support heterogeneous use cases at an\nunprecedented scale, thus demanding automated control and optimization of\nnetwork functionalities customized to the needs of individual users. Such\nfine-grained control of the Radio Access Network (RAN) is not possible with the\ncurrent cellular architecture. To fill this gap, the Open RAN paradigm and its\nspecification introduce an open architecture with abstractions that enable\nclosed-loop control and provide data-driven, and intelligent optimization of\nthe RAN at the user level. This is obtained through custom RAN control\napplications (i.e., xApps) deployed on near-real-time RAN Intelligent\nController (near-RT RIC) at the edge of the network. Despite these premises, as\nof today the research community lacks a sandbox to build data-driven xApps, and\ncreate large-scale datasets for effective AI training. In this paper, we\naddress this by introducing ns-O-RAN, a software framework that integrates a\nreal-world, production-grade near-RT RIC with a 3GPP-based simulated\nenvironment on ns-3, enabling the development of xApps and automated\nlarge-scale data collection and testing of Deep Reinforcement Learning-driven\ncontrol policies for the optimization at the user-level. In addition, we\npropose the first user-specific O-RAN Traffic Steering (TS) intelligent\nhandover framework. It uses Random Ensemble Mixture, combined with a\nstate-of-the-art Convolutional Neural Network architecture, to optimally assign\na serving base station to each user in the network. Our TS xApp, trained with\nmore than 40 million data points collected by ns-O-RAN, runs on the near-RT RIC\nand controls its base stations. We evaluate the performance on a large-scale\ndeployment, showing that the xApp-based handover improves throughput and\nspectral efficiency by an average of 50% over traditional handover heuristics,\nwith less mobility overhead.",
        "pdf_link": "https://arxiv.org/pdf/2209.14171v3.pdf"
    },
    {
        "title": "Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-trained Language Models",
        "authors": [
            "Youness Moukafih",
            "Mounir Ghogho",
            "Kamel Smaili"
        ],
        "published": "2022-09-28T15:13:58Z",
        "summary": "Recently, Supervised Contrastive Learning (SCL) has been shown to achieve\nexcellent performance in most classification tasks. In SCL, a neural network is\ntrained to optimize two objectives: pull an anchor and positive samples\ntogether in the embedding space, and push the anchor apart from the negatives.\nHowever, these two different objectives may conflict, requiring trade-offs\nbetween them during optimization. In this work, we formulate the SCL problem as\na Multi-Objective Optimization problem for the fine-tuning phase of RoBERTa\nlanguage model. Two methods are utilized to solve the optimization problem: (i)\nthe linear scalarization (LS) method, which minimizes a weighted linear\ncombination of pertask losses; and (ii) the Exact Pareto Optimal (EPO) method\nwhich finds the intersection of the Pareto front with a given preference\nvector. We evaluate our approach on several GLUE benchmark tasks, without using\ndata augmentations, memory banks, or generating adversarial examples. The\nempirical results show that the proposed learning strategy significantly\noutperforms a strong competitive contrastive learning baseline",
        "pdf_link": "https://arxiv.org/pdf/2209.14161v1.pdf"
    },
    {
        "title": "Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer",
        "authors": [
            "Piotr P\u0119zik",
            "Agnieszka Miko\u0142ajczyk-Bare\u0142a",
            "Adam Wawrzy\u0144ski",
            "Bart\u0142omiej Nito\u0144",
            "Maciej Ogrodniczuk"
        ],
        "published": "2022-09-28T11:31:43Z",
        "summary": "The paper explores the relevance of the Text-To-Text Transfer Transformer\nlanguage model (T5) for Polish (plT5) to the task of intrinsic and extrinsic\nkeyword extraction from short text passages. The evaluation is carried out on\nthe new Polish Open Science Metadata Corpus (POSMAC), which is released with\nthis paper: a collection of 216,214 abstracts of scientific publications\ncompiled in the CURLICAT project. We compare the results obtained by four\ndifferent methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that\nthe plT5kw model yields particularly promising results for both frequent and\nsparsely represented keywords. Furthermore, a plT5kw keyword generation model\ntrained on the POSMAC also seems to produce highly useful results in\ncross-domain text labelling scenarios. We discuss the performance of the model\non news stories and phone-based dialog transcripts which represent text genres\nand domains extrinsic to the dataset of scientific abstracts. Finally, we also\nattempt to characterize the challenges of evaluating a text-to-text model on\nboth intrinsic and extrinsic keyword extraction.",
        "pdf_link": "https://arxiv.org/pdf/2209.14008v2.pdf"
    },
    {
        "title": "CEFER: A Four Facets Framework based on Context and Emotion embedded features for Implicit and Explicit Emotion Recognition",
        "authors": [
            "Fereshteh Khoshnam",
            "Ahmad Baraani-Dastjerdi",
            "M. J. Liaghatdar"
        ],
        "published": "2022-09-28T11:16:32Z",
        "summary": "People's conduct and reactions are driven by their emotions. Online social\nmedia is becoming a great instrument for expressing emotions in written form.\nPaying attention to the context and the entire sentence help us to detect\nemotion from texts. However, this perspective inhibits us from noticing some\nemotional words or phrases in the text, particularly when the words express an\nemotion implicitly rather than explicitly. On the other hand, focusing only on\nthe words and ignoring the context results in a distorted understanding of the\nsentence meaning and feeling. In this paper, we propose a framework that\nanalyses text at both the sentence and word levels. We name it CEFER (Context\nand Emotion embedded Framework for Emotion Recognition). Our four approach\nfacets are to extracting data by considering the entire sentence and each\nindividual word simultaneously, as well as implicit and explicit emotions. The\nknowledge gained from these data not only mitigates the impact of flaws in the\npreceding approaches but also it strengthens the feature vector. We evaluate\nseveral feature spaces using BERT family and design the CEFER based on them.\nCEFER combines the emotional vector of each word, including explicit and\nimplicit emotions, with the feature vector of each word based on context. CEFER\nperforms better than the BERT family. The experimental results demonstrate that\nidentifying implicit emotions are more challenging than detecting explicit\nemotions. CEFER, improves the accuracy of implicit emotion recognition.\nAccording to the results, CEFER perform 5% better than the BERT family in\nrecognizing explicit emotions and 3% in implicit.",
        "pdf_link": "https://arxiv.org/pdf/2209.13999v1.pdf"
    },
    {
        "title": "Medical Image Captioning via Generative Pretrained Transformers",
        "authors": [
            "Alexander Selivanov",
            "Oleg Y. Rogov",
            "Daniil Chesakov",
            "Artem Shelmanov",
            "Irina Fedulova",
            "Dmitry V. Dylov"
        ],
        "published": "2022-09-28T10:27:10Z",
        "summary": "The automatic clinical caption generation problem is referred to as proposed\nmodel combining the analysis of frontal chest X-Ray scans with structured\npatient information from the radiology records. We combine two language models,\nthe Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive\nradiology records. The proposed combination of these models generates a textual\nsummary with the essential information about pathologies found, their location,\nand the 2D heatmaps localizing each pathology on the original X-Ray scans. The\nproposed model is tested on two medical datasets, the Open-I, MIMIC-CXR, and\nthe general-purpose MS-COCO. The results measured with the natural language\nassessment metrics prove their efficient applicability to the chest X-Ray image\ncaptioning.",
        "pdf_link": "https://arxiv.org/pdf/2209.13983v1.pdf"
    },
    {
        "title": "ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection",
        "authors": [
            "Khloud Al Jallad",
            "Nada Ghneim"
        ],
        "published": "2022-09-28T09:37:16Z",
        "summary": "Natural Language Inference (NLI) is a hot topic research in natural language\nprocessing, contradiction detection between sentences is a special case of NLI.\nThis is considered a difficult NLP task which has a big influence when added as\na component in many NLP applications, such as Question Answering Systems, text\nSummarization. Arabic Language is one of the most challenging low-resources\nlanguages in detecting contradictions due to its rich lexical, semantics\nambiguity. We have created a data set of more than 12k sentences and named\nArNLI, that will be publicly available. Moreover, we have applied a new model\ninspired by Stanford contradiction detection proposed solutions on English\nlanguage. We proposed an approach to detect contradictions between pairs of\nsentences in Arabic language using contradiction vector combined with language\nmodel vector as an input to machine learning model. We analyzed results of\ndifferent traditional machine learning classifiers and compared their results\non our created data set (ArNLI) and on an automatic translation of both PHEME,\nSICK English data sets. Best results achieved using Random Forest classifier\nwith an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI respectively.",
        "pdf_link": "https://arxiv.org/pdf/2209.13953v1.pdf"
    },
    {
        "title": "YATO: Yet Another deep learning based Text analysis Open toolkit",
        "authors": [
            "Zeqiang Wang",
            "Yile Wang",
            "Jiageng Wu",
            "Zhiyang Teng",
            "Jie Yang"
        ],
        "published": "2022-09-28T07:25:04Z",
        "summary": "We introduce YATO, an open-source, easy-to-use toolkit for text analysis with\ndeep learning. Different from existing heavily engineered toolkits and\nplatforms, YATO is lightweight and user-friendly for researchers from\ncross-disciplinary areas. Designed in a hierarchical structure, YATO supports\nfree combinations of three types of widely used features including 1)\ntraditional neural networks (CNN, RNN, etc.); 2) pre-trained language models\n(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a\nsimple configurable file. Benefiting from the advantages of flexibility and\nease of use, YATO can facilitate fast reproduction and refinement of\nstate-of-the-art NLP models, and promote the cross-disciplinary applications of\nNLP techniques. The code, examples, and documentation are publicly available at\nhttps://github.com/jiesutd/YATO. A demo video is also available at\nhttps://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.",
        "pdf_link": "https://arxiv.org/pdf/2209.13877v4.pdf"
    },
    {
        "title": "Streaming Video Temporal Action Segmentation In Real Time",
        "authors": [
            "Wujun Wen",
            "Yunheng Li",
            "Zhuben Dong",
            "Lin Feng",
            "Wanxiao Yang",
            "Shenlan Liu"
        ],
        "published": "2022-09-28T03:27:37Z",
        "summary": "Temporal action segmentation (TAS) is a critical step toward long-term video\nunderstanding. Recent studies follow a pattern that builds models based on\nfeatures instead of raw video picture information. However, we claim those\nmodels are trained complicatedly and limit application scenarios. It is hard\nfor them to segment human actions of video in real time because they must work\nafter the full video features are extracted. As the real-time action\nsegmentation task is different from TAS task, we define it as streaming video\nreal-time temporal action segmentation (SVTAS) task. In this paper, we propose\na real-time end-to-end multi-modality model for SVTAS task. More specifically,\nunder the circumstances that we cannot get any future information, we segment\nthe current human action of streaming video chunk in real time. Furthermore,\nthe model we propose combines the last steaming video chunk feature extracted\nby language model with the current image feature extracted by image model to\nimprove the quantity of real-time temporal action segmentation. To the best of\nour knowledge, it is the first multi-modality real-time temporal action\nsegmentation model. Under the same evaluation criteria as full video temporal\naction segmentation, our model segments human action in real time with less\nthan 40% of state-of-the-art model computation and achieves 90% of the accuracy\nof the full video state-of-the-art model.",
        "pdf_link": "https://arxiv.org/pdf/2209.13808v3.pdf"
    },
    {
        "title": "How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI",
        "authors": [
            "Kaiping Chen",
            "Anqi Shao",
            "Jirayu Burapacheep",
            "Yixuan Li"
        ],
        "published": "2022-09-27T18:44:41Z",
        "summary": "Autoregressive language models, which use deep learning to produce human-like\ntexts, have become increasingly widespread. Such models are powering popular\nvirtual assistants in areas like smart health, finance, and autonomous driving.\nWhile the parameters of these large language models are improving, concerns\npersist that these models might not work equally for all subgroups in society.\nDespite growing discussions of AI fairness across disciplines, there lacks\nsystemic metrics to assess what equity means in dialogue systems and how to\nengage different populations in the assessment loop. Grounded in theories of\ndeliberative democracy and science and technology studies, this paper proposes\nan analytical framework for unpacking the meaning of equity in human-AI\ndialogues. Using this framework, we conducted an auditing study to examine how\nGPT-3 responded to different sub-populations on crucial science and social\ntopics: climate change and the Black Lives Matter (BLM) movement. Our corpus\nconsists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals\nwho vary in gender, race and ethnicity, education level, English as a first\nlanguage, and opinions toward the issues. We found a substantively worse user\nexperience with GPT-3 among the opinion and the education minority\nsubpopulations; however, these two groups achieved the largest knowledge gain,\nchanging attitudes toward supporting BLM and climate change efforts after the\nchat. We traced these user experience divides to conversational differences and\nfound that GPT-3 used more negative expressions when it responded to the\neducation and opinion minority groups, compared to its responses to the\nmajority groups. We discuss the implications of our findings for a deliberative\nconversational AI system that centralizes diversity, equity, and inclusion.",
        "pdf_link": "https://arxiv.org/pdf/2209.13627v2.pdf"
    },
    {
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models",
        "authors": [
            "Xiuying Wei",
            "Yunchen Zhang",
            "Xiangguo Zhang",
            "Ruihao Gong",
            "Shanghang Zhang",
            "Qi Zhang",
            "Fengwei Yu",
            "Xianglong Liu"
        ],
        "published": "2022-09-27T12:05:59Z",
        "summary": "Transformer architecture has become the fundamental element of the widespread\nnatural language processing~(NLP) models. With the trends of large NLP models,\nthe increasing memory and computation costs hinder their efficient deployment\non resource-limited devices. Therefore, transformer quantization attracts wide\nresearch interest. Recent work recognizes that structured outliers are the\ncritical bottleneck for quantization performance. However, their proposed\nmethods increase the computation overhead and still leave the outliers there.\nTo fundamentally address this problem, this paper delves into the inherent\ninducement and importance of the outliers. We discover that $\\boldsymbol\n\\gamma$ in LayerNorm (LN) acts as a sinful amplifier for the outliers, and the\nimportance of outliers varies greatly where some outliers provided by a few\ntokens cover a large area but can be clipped sharply without negative impacts.\nMotivated by these findings, we propose an outlier suppression framework\nincluding two components: Gamma Migration and Token-Wise Clipping. The Gamma\nMigration migrates the outlier amplifier to subsequent modules in an equivalent\ntransformation, contributing to a more quantization-friendly model without any\nextra burden. The Token-Wise Clipping takes advantage of the large variance of\ntoken range and designs a token-wise coarse-to-fine pipeline, obtaining a\nclipping range with minimal final quantization loss in an efficient way. This\nframework effectively suppresses the outliers and can be used in a\nplug-and-play mode. Extensive experiments prove that our framework surpasses\nthe existing works and, for the first time, pushes the 6-bit post-training BERT\nquantization to the full-precision (FP) level. Our code is available at\nhttps://github.com/wimh966/outlier_suppression.",
        "pdf_link": "https://arxiv.org/pdf/2209.13325v3.pdf"
    },
    {
        "title": "A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing",
        "authors": [
            "Pranav Shetty",
            "Arunkumar Chitteth Rajan",
            "Christopher Kuenneth",
            "Sonkakshi Gupta",
            "Lakshmi Prerana Panchumarti",
            "Lauren Holm",
            "Chao Zhang",
            "Rampi Ramprasad"
        ],
        "published": "2022-09-27T03:47:03Z",
        "summary": "The ever-increasing number of materials science articles makes it hard to\ninfer chemistry-structure-property relations from published literature. We used\nnatural language processing (NLP) methods to automatically extract material\nproperty data from the abstracts of polymer literature. As a component of our\npipeline, we trained MaterialsBERT, a language model, using 2.4 million\nmaterials science abstracts, which outperforms other baseline models in three\nout of five named entity recognition datasets when used as the encoder for\ntext. Using this pipeline, we obtained ~300,000 material property records from\n~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse\nrange of applications such as fuel cells, supercapacitors, and polymer solar\ncells to recover non-trivial insights. The data extracted through our pipeline\nis made available through a web platform at https://polymerscholar.org which\ncan be used to locate material property data recorded in abstracts\nconveniently. This work demonstrates the feasibility of an automatic pipeline\nthat starts from published literature and ends with a complete set of extracted\nmaterial property information.",
        "pdf_link": "https://arxiv.org/pdf/2209.13136v1.pdf"
    },
    {
        "title": "Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors",
        "authors": [
            "Vignav Ramesh",
            "Nathan Andrew Chi",
            "Pranav Rajpurkar"
        ],
        "published": "2022-09-27T00:44:41Z",
        "summary": "Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.",
        "pdf_link": "https://arxiv.org/pdf/2210.06340v2.pdf"
    },
    {
        "title": "Towards Simple and Efficient Task-Adaptive Pre-training for Text Classification",
        "authors": [
            "Arnav Ladkat",
            "Aamir Miyajiwala",
            "Samiksha Jagadale",
            "Rekha Kulkarni",
            "Raviraj Joshi"
        ],
        "published": "2022-09-26T18:29:12Z",
        "summary": "Language models are pre-trained using large corpora of generic data like book\ncorpus, common crawl and Wikipedia, which is essential for the model to\nunderstand the linguistic characteristics of the language. New studies suggest\nusing Domain Adaptive Pre-training (DAPT) and Task-Adaptive Pre-training (TAPT)\nas an intermediate step before the final finetuning task. This step helps cover\nthe target domain vocabulary and improves the model performance on the\ndownstream task. In this work, we study the impact of training only the\nembedding layer on the model's performance during TAPT and task-specific\nfinetuning. Based on our study, we propose a simple approach to make the\nintermediate step of TAPT for BERT-based models more efficient by performing\nselective pre-training of BERT layers. We show that training only the BERT\nembedding layer during TAPT is sufficient to adapt to the vocabulary of the\ntarget domain and achieve comparable performance. Our approach is\ncomputationally efficient, with 78\\% fewer parameters trained during TAPT. The\nproposed embedding layer finetuning approach can also be an efficient domain\nadaptation technique.",
        "pdf_link": "https://arxiv.org/pdf/2209.12943v1.pdf"
    },
    {
        "title": "Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour",
        "authors": [
            "Fangyu Liu",
            "Julian Martin Eisenschlos",
            "Jeremy R. Cole",
            "Nigel Collier"
        ],
        "published": "2022-09-26T15:45:23Z",
        "summary": "Language models (LMs) trained on raw texts have no direct access to the\nphysical world. Gordon and Van Durme (2013) point out that LMs can thus suffer\nfrom reporting bias: texts rarely report on common facts, instead focusing on\nthe unusual aspects of a situation. If LMs are only trained on text corpora and\nnaively memorise local co-occurrence statistics, they thus naturally would\nlearn a biased view of the physical world. While prior studies have repeatedly\nverified that LMs of smaller scales (e.g., RoBERTa, GPT-2) amplify reporting\nbias, it remains unknown whether such trends continue when models are scaled\nup. We investigate reporting bias from the perspective of colour in larger\nlanguage models (LLMs) such as PaLM and GPT-3. Specifically, we query LLMs for\nthe typical colour of objects, which is one simple type of perceptually\ngrounded physical common sense. Surprisingly, we find that LLMs significantly\noutperform smaller LMs in determining an object's typical colour and more\nclosely track human judgments, instead of overfitting to surface patterns\nstored in texts. This suggests that very large models of language alone are\nable to overcome certain types of reporting bias that are characterized by\nlocal co-occurrences.",
        "pdf_link": "https://arxiv.org/pdf/2209.12786v1.pdf"
    },
    {
        "title": "Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts",
        "authors": [
            "Joel Jang",
            "Seonghyeon Ye",
            "Minjoon Seo"
        ],
        "published": "2022-09-26T14:05:10Z",
        "summary": "Previous work has shown that there exists a scaling law between the size of\nLanguage Models (LMs) and their zero-shot performance on different downstream\nNLP tasks. In this work, we show that this phenomenon does not hold when\nevaluating large LMs on tasks with negated prompts, but instead shows an\ninverse scaling law. We evaluate 9 different tasks with negated prompts on (1)\npretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further\npretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with\nfew-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all\nLM types perform worse on negated prompts as they scale and show a huge\nperformance gap between the human performance when comparing the average score\non both original and negated prompts. By highlighting a critical limitation of\nexisting LMs and methods, we urge the community to develop new approaches of\ndeveloping LMs that actually follow the given instructions. We provide the code\nand the datasets to explore negated prompts at\nhttps://github.com/joeljang/negated-prompts-for-llms",
        "pdf_link": "https://arxiv.org/pdf/2209.12711v1.pdf"
    },
    {
        "title": "Entailment Semantics Can Be Extracted from an Ideal Language Model",
        "authors": [
            "William Merrill",
            "Alex Warstadt",
            "Tal Linzen"
        ],
        "published": "2022-09-26T04:16:02Z",
        "summary": "Language models are often trained on text alone, without additional\ngrounding. There is debate as to how much of natural language semantics can be\ninferred from such a procedure. We prove that entailment judgments between\nsentences can be extracted from an ideal language model that has perfectly\nlearned its target distribution, assuming the training sentences are generated\nby Gricean agents, i.e., agents who follow fundamental principles of\ncommunication from the linguistic theory of pragmatics. We also show entailment\njudgments can be decoded from the predictions of a language model trained on\nsuch Gricean data. Our results reveal a pathway for understanding the semantic\ninformation encoded in unlabeled linguistic data and a potential framework for\nextracting semantics from language models.",
        "pdf_link": "https://arxiv.org/pdf/2209.12407v3.pdf"
    },
    {
        "title": "News Summarization and Evaluation in the Era of GPT-3",
        "authors": [
            "Tanya Goyal",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "published": "2022-09-26T01:04:52Z",
        "summary": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.",
        "pdf_link": "https://arxiv.org/pdf/2209.12356v2.pdf"
    },
    {
        "title": "Paraphrasing Is All You Need for Novel Object Captioning",
        "authors": [
            "Cheng-Fu Yang",
            "Yao-Hung Hubert Tsai",
            "Wan-Cyuan Fan",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency",
            "Yu-Chiang Frank Wang"
        ],
        "published": "2022-09-25T22:56:04Z",
        "summary": "Novel object captioning (NOC) aims to describe images containing objects\nwithout observing their ground truth captions during training. Due to the\nabsence of caption annotation, captioning models cannot be directly optimized\nvia sequence-to-sequence training or CIDEr optimization. As a result, we\npresent Paraphrasing-to-Captioning (P2C), a two-stage learning framework for\nNOC, which would heuristically optimize the output captions via paraphrasing.\nWith P2C, the captioning model first learns paraphrasing from a language model\npre-trained on text-only corpus, allowing expansion of the word bank for\nimproving linguistic fluency. To further enforce the output caption\nsufficiently describing the visual content of the input image, we perform\nself-paraphrasing for the captioning model with fidelity and adequacy\nobjectives introduced. Since no ground truth captions are available for novel\nobject images during training, our P2C leverages cross-modality (image-text)\nassociation modules to ensure the above caption characteristics can be properly\npreserved. In the experiments, we not only show that our P2C achieves\nstate-of-the-art performances on nocaps and COCO Caption datasets, we also\nverify the effectiveness and flexibility of our learning framework by replacing\nlanguage and cross-modality association models for NOC. Implementation details\nand code are available in the supplementary materials.",
        "pdf_link": "https://arxiv.org/pdf/2209.12343v1.pdf"
    },
    {
        "title": "WinoDict: Probing language models for in-context word acquisition",
        "authors": [
            "Julian Martin Eisenschlos",
            "Jeremy R. Cole",
            "Fangyu Liu",
            "William W. Cohen"
        ],
        "published": "2022-09-25T05:30:13Z",
        "summary": "We introduce a new in-context learning paradigm to measure Large Language\nModels' (LLMs) ability to learn novel words during inference. In particular, we\nrewrite Winograd-style co-reference resolution problems by replacing the key\nconcept word with a synthetic but plausible word that the model must understand\nto complete the task. Solving this task requires the model to make use of the\ndictionary definition of the new word given in the prompt. This benchmark\naddresses word acquisition, one important aspect of the diachronic degradation\nknown to afflict LLMs. As LLMs are frozen in time at the moment they are\ntrained, they are normally unable to reflect the way language changes over\ntime. We show that the accuracy of LLMs compared to the original Winograd tasks\ndecreases radically in our benchmark, thus identifying a limitation of current\nmodels and providing a benchmark to measure future improvements in LLMs ability\nto do in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2209.12153v1.pdf"
    },
    {
        "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
        "authors": [
            "Gabriel Simmons"
        ],
        "published": "2022-09-24T23:55:53Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating fluent text, as well as tendencies to reproduce undesirable social\nbiases. This study investigates whether LLMs reproduce the moral biases\nassociated with political groups in the United States, an instance of a broader\ncapability herein termed moral mimicry. This hypothesis is explored in the\nGPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral\nFoundations Theory, it is shown that these LLMs are indeed moral mimics. When\nprompted with a liberal or conservative political identity, the models generate\ntext reflecting corresponding moral biases. This study also explores the\nrelationship between moral mimicry and model size, and similarity between human\nand LLM moral word use.",
        "pdf_link": "https://arxiv.org/pdf/2209.12106v2.pdf"
    },
    {
        "title": "Can Transformer Models Effectively Detect Software Aspects in StackOverflow Discussion?",
        "authors": [
            "Nibir Chandra Mandal",
            "Tashreef Muhammad",
            "G. M. Shahariar"
        ],
        "published": "2022-09-24T18:28:14Z",
        "summary": "Dozens of new tools and technologies are being incorporated to help\ndevelopers, which is becoming a source of consternation as they struggle to\nchoose one over the others. For example, there are at least ten frameworks\navailable to developers for developing web applications, posing a conundrum in\nselecting the best one that meets their needs. As a result, developers are\ncontinuously searching for all of the benefits and drawbacks of each API,\nframework, tool, and so on. One of the typical approaches is to examine all of\nthe features through official documentation and discussion. This approach is\ntime-consuming, often makes it difficult to determine which aspects are the\nmost important to a particular developer and whether a particular aspect is\nimportant to the community at large. In this paper, we have used a benchmark\nAPI aspects dataset (Opiner) collected from StackOverflow posts and observed\nhow Transformer models (BERT, RoBERTa, DistilBERT, and XLNet) perform in\ndetecting software aspects in textual developer discussion with respect to the\nbaseline Support Vector Machine (SVM) model. Through extensive experimentation,\nwe have found that transformer models improve the performance of baseline SVM\nfor most of the aspects, i.e., `Performance', `Security', `Usability',\n`Documentation', `Bug', `Legal', `OnlySentiment', and `Others'. However, the\nmodels fail to apprehend some of the aspects (e.g., `Community' and\n`Potability') and their performance varies depending on the aspects. Also,\nlarger architectures like XLNet are ineffective in interpreting software\naspects compared to smaller architectures like DistilBERT.",
        "pdf_link": "https://arxiv.org/pdf/2209.12065v1.pdf"
    },
    {
        "title": "Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports",
        "authors": [
            "Gosse Minnema",
            "Sara Gemelli",
            "Chiara Zanchi",
            "Tommaso Caselli",
            "Malvina Nissim"
        ],
        "published": "2022-09-24T15:14:03Z",
        "summary": "Different linguistic expressions can conceptualize the same event from\ndifferent viewpoints by emphasizing certain participants over others. Here, we\ninvestigate a case where this has social consequences: how do linguistic\nexpressions of gender-based violence (GBV) influence who we perceive as\nresponsible? We build on previous psycholinguistic research in this area and\nconduct a large-scale perception survey of GBV descriptions automatically\nextracted from a corpus of Italian newspapers. We then train regression models\nthat predict the salience of GBV participants with respect to different\ndimensions of perceived responsibility. Our best model (fine-tuned BERT) shows\nsolid overall performance, with large differences between dimensions and\nparticipants: salient _focus_ is more predictable than salient _blame_, and\nperpetrators' salience is more predictable than victims' salience. Experiments\nwith ridge regression models using different representations show that features\nbased on linguistic theory similarly to word-based features. Overall, we show\nthat different linguistic choices do trigger different perceptions of\nresponsibility, and that such perceptions can be modelled automatically. This\nwork can be a core instrument to raise awareness of the consequences of\ndifferent perspectivizations in the general public and in news producers alike.",
        "pdf_link": "https://arxiv.org/pdf/2209.12030v1.pdf"
    },
    {
        "title": "Learning Chess With Language Models and Transformers",
        "authors": [
            "Michael DeLeo",
            "Erhan Guven"
        ],
        "published": "2022-09-24T01:22:59Z",
        "summary": "Representing a board game and its positions by text-based notation enables\nthe possibility of NLP applications. Language models, can help gain insight\ninto a variety of interesting problems such as unsupervised learning rules of a\ngame, detecting player behavior patterns, player attribution, and ultimately\nlearning the game to beat state of the art. In this study, we applied BERT\nmodels, first to the simple Nim game to analyze its performance in the presence\nof noise in a setup of a few-shot learning architecture. We analyzed the model\nperformance via three virtual players, namely Nim Guru, Random player, and\nQ-learner. In the second part, we applied the game learning language model to\nthe chess game, and a large set of grandmaster games with exhaustive\nencyclopedia openings. Finally, we have shown that model practically learns the\nrules of the chess game and can survive games against Stockfish at a category-A\nrating level.",
        "pdf_link": "https://arxiv.org/pdf/2209.11902v1.pdf"
    },
    {
        "title": "Whodunit? Learning to Contrast for Authorship Attribution",
        "authors": [
            "Bo Ai",
            "Yuchen Wang",
            "Yugin Tan",
            "Samson Tan"
        ],
        "published": "2022-09-23T23:45:08Z",
        "summary": "Authorship attribution is the task of identifying the author of a given text.\nThe key is finding representations that can differentiate between authors.\nExisting approaches typically use manually designed features that capture a\ndataset's content and style, but these approaches are dataset-dependent and\nyield inconsistent performance across corpora. In this work, we propose\n\\textit{learning} author-specific representations by fine-tuning pre-trained\ngeneric language representations with a contrastive objective (Contra-X). We\nshow that Contra-X learns representations that form highly separable clusters\nfor different authors. It advances the state-of-the-art on multiple human and\nmachine authorship attribution benchmarks, enabling improvements of up to 6.8%\nover cross-entropy fine-tuning. However, we find that Contra-X improves overall\naccuracy at the cost of sacrificing performance for some authors. Resolving\nthis tension will be an important direction for future work. To the best of our\nknowledge, we are the first to integrate contrastive learning with pre-trained\nlanguage model fine-tuning for authorship attribution.",
        "pdf_link": "https://arxiv.org/pdf/2209.11887v2.pdf"
    },
    {
        "title": "Augmenting Interpretable Models with LLMs during Training",
        "authors": [
            "Chandan Singh",
            "Armin Askari",
            "Rich Caruana",
            "Jianfeng Gao"
        ],
        "published": "2022-09-23T18:36:01Z",
        "summary": "Recent large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their proliferation into\nhigh-stakes domains (e.g. medicine) and compute-limited settings has created a\nburgeoning need for interpretability and efficiency. We address this need by\nproposing Augmented Interpretable Models (Aug-imodels), a framework for\nleveraging the knowledge learned by LLMs to build extremely efficient and\ninterpretable models. Aug-imodels use LLMs during fitting but not during\ninference, allowing complete transparency and often a speed/memory improvement\nof greater than 1,000x for inference compared to LLMs. We explore two\ninstantiations of Aug-imodels in natural-language processing: (i) Aug-GAM,\nwhich augments a generalized additive model with decoupled embeddings from an\nLLM and (ii) Aug-Tree, which augments a decision tree with LLM feature\nexpansions. Across a variety of text-classification datasets, both outperform\ntheir non-augmented counterparts. Aug-GAM can even outperform much larger\nmodels (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer\nparameters and being fully transparent. We further explore Aug-imodels in a\nnatural-language fMRI study, where they generate interesting interpretations\nfrom scientific data. All code for using Aug-imodels and reproducing results is\nmade available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2209.11799v3.pdf"
    },
    {
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
        "authors": [
            "Zhuyun Dai",
            "Vincent Y. Zhao",
            "Ji Ma",
            "Yi Luan",
            "Jianmo Ni",
            "Jing Lu",
            "Anton Bakalov",
            "Kelvin Guu",
            "Keith B. Hall",
            "Ming-Wei Chang"
        ],
        "published": "2022-09-23T17:59:06Z",
        "summary": "Much recent research on information retrieval has focused on how to transfer\nfrom one task (typically with abundant supervised data) to various other tasks\nwhere supervision is limited, with the implicit assumption that it is possible\nto generalize from one task to all the rest. However, this overlooks the fact\nthat there are many diverse and unique retrieval tasks, each targeting\ndifferent search intents, queries, and search domains. In this paper, we\nsuggest to work on Few-shot Dense Retrieval, a setting where each task comes\nwith a short description and a few examples. To amplify the power of a few\nexamples, we propose Prompt-base Query Generation for Retriever (Promptagator),\nwhich leverages large language models (LLM) as a few-shot query generator, and\ncreates task-specific retrievers based on the generated data. Powered by LLM's\ngeneralization ability, Promptagator makes it possible to create task-specific\nend-to-end retrievers solely based on a few examples {without} using Natural\nQuestions or MS MARCO to train %question generators or dual encoders.\nSurprisingly, LLM prompting with no more than 8 examples allows dual encoders\nto outperform heavily engineered models trained on MS MARCO like ColBERT v2 by\nmore than 1.2 nDCG on average on 11 retrieval sets. Further training\nstandard-size re-rankers using the same generated data yields another 5.0 point\nnDCG improvement. Our studies determine that query generation can be far more\neffective than previously observed, especially when a small amount of\ntask-specific knowledge is given.",
        "pdf_link": "https://arxiv.org/pdf/2209.11755v1.pdf"
    },
    {
        "title": "Variational Open-Domain Question Answering",
        "authors": [
            "Valentin Li\u00e9vin",
            "Andreas Geert Motzfeldt",
            "Ida Riis Jensen",
            "Ole Winther"
        ],
        "published": "2022-09-23T10:25:59Z",
        "summary": "Retrieval-augmented models have proven to be effective in natural language\nprocessing tasks, yet there remains a lack of research on their optimization\nusing variational inference. We introduce the Variational Open-Domain (VOD)\nframework for end-to-end training and evaluation of retrieval-augmented models,\nfocusing on open-domain question answering and language modelling. The VOD\nobjective, a self-normalized estimate of the R\\'enyi variational bound,\napproximates the task marginal likelihood and is evaluated under samples drawn\nfrom an auxiliary sampling distribution (cached retriever and/or approximate\nposterior). It remains tractable, even for retriever distributions defined on\nlarge corpora. We demonstrate VOD's versatility by training reader-retriever\nBERT-sized models on multiple-choice medical exam questions. On the MedMCQA\ndataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using\n2.500$\\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model\nscored 62.9% on the MedMCQA and 55.0% on the MedQA-USMLE. Last, we show the\neffectiveness of our learned retriever component in the context of medical\nsemantic search.",
        "pdf_link": "https://arxiv.org/pdf/2210.06345v2.pdf"
    },
    {
        "title": "IDEA: Interactive DoublE Attentions from Label Embedding for Text Classification",
        "authors": [
            "Ziyuan Wang",
            "Hailiang Huang",
            "Songqiao Han"
        ],
        "published": "2022-09-23T04:50:47Z",
        "summary": "Current text classification methods typically encode the text merely into\nembedding before a naive or complicated classifier, which ignores the\nsuggestive information contained in the label text. As a matter of fact, humans\nclassify documents primarily based on the semantic meaning of the\nsubcategories. We propose a novel model structure via siamese BERT and\ninteractive double attentions named IDEA ( Interactive DoublE Attentions) to\ncapture the information exchange of text and label names. Interactive double\nattentions enable the model to exploit the inter-class and intra-class\ninformation from coarse to fine, which involves distinguishing among all labels\nand matching the semantical subclasses of ground truth labels. Our proposed\nmethod outperforms the state-of-the-art methods using label texts significantly\nwith more stable results.",
        "pdf_link": "https://arxiv.org/pdf/2209.11407v1.pdf"
    },
    {
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
        "authors": [
            "Ishika Singh",
            "Valts Blukis",
            "Arsalan Mousavian",
            "Ankit Goyal",
            "Danfei Xu",
            "Jonathan Tremblay",
            "Dieter Fox",
            "Jesse Thomason",
            "Animesh Garg"
        ],
        "published": "2022-09-22T20:29:49Z",
        "summary": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2209.11302v1.pdf"
    },
    {
        "title": "Optimization of FPGA-based CNN Accelerators Using Metaheuristics",
        "authors": [
            "Sadiq M. Sait",
            "Aiman El-Maleh",
            "Mohammad Altakrouri",
            "Ahmad Shawahna"
        ],
        "published": "2022-09-22T18:57:49Z",
        "summary": "In recent years, convolutional neural networks (CNNs) have demonstrated their\nability to solve problems in many fields and with accuracy that was not\npossible before. However, this comes with extensive computational requirements,\nwhich made general CPUs unable to deliver the desired real-time performance. At\nthe same time, FPGAs have seen a surge in interest for accelerating CNN\ninference. This is due to their ability to create custom designs with different\nlevels of parallelism. Furthermore, FPGAs provide better performance per watt\ncompared to GPUs. The current trend in FPGA-based CNN accelerators is to\nimplement multiple convolutional layer processors (CLPs), each of which is\ntailored for a subset of layers. However, the growing complexity of CNN\narchitectures makes optimizing the resources available on the target FPGA\ndevice to deliver optimal performance more challenging. In this paper, we\npresent a CNN accelerator and an accompanying automated design methodology that\nemploys metaheuristics for partitioning available FPGA resources to design a\nMulti-CLP accelerator. Specifically, the proposed design tool adopts simulated\nannealing (SA) and tabu search (TS) algorithms to find the number of CLPs\nrequired and their respective configurations to achieve optimal performance on\na given target FPGA device. Here, the focus is on the key specifications and\nhardware resources, including digital signal processors, block RAMs, and\noff-chip memory bandwidth. Experimental results and comparisons using four\nwell-known benchmark CNNs are presented demonstrating that the proposed\nacceleration framework is both encouraging and promising. The SA-/TS-based\nMulti-CLP achieves 1.31x - 2.37x higher throughput than the state-of-the-art\nSingle-/Multi-CLP approaches in accelerating AlexNet, SqueezeNet 1.1, VGGNet,\nand GoogLeNet architectures on the Xilinx VC707 and VC709 FPGA boards.",
        "pdf_link": "https://arxiv.org/pdf/2209.11272v1.pdf"
    },
    {
        "title": "A Case Report On The \"A.I. Locked-In Problem\": social concerns with modern NLP",
        "authors": [
            "Yoshija Walter"
        ],
        "published": "2022-09-22T16:39:35Z",
        "summary": "Modern NLP models are becoming better conversational agents than their\npredecessors. Recurrent Neural Networks (RNNs) and especially Long-Short Term\nMemory (LSTM) features allow the agent to better store and use information\nabout semantic content, a trend that has become even more pronounced with the\nTransformer Models. Large Language Models (LLMs) such as GPT-3 by OpenAI have\nbecome known to be able to construct and follow a narrative, which enables the\nsystem to adopt personas on the go, adapt them and play along in conversational\nstories. However, practical experimentation with GPT-3 shows that there is a\nrecurring problem with these modern NLP systems, namely that they can \"get\nstuck\" in the narrative so that further conversations, prompt executions or\ncommands become futile. This is here referred to as the \"Locked-In Problem\" and\nis exemplified with an experimental case report, followed by practical and\nsocial concerns that are accompanied with this problem.",
        "pdf_link": "https://arxiv.org/pdf/2209.12687v1.pdf"
    },
    {
        "title": "Prompting for a conversation: How to control a dialog model?",
        "authors": [
            "Josef Valvoda",
            "Yimai Fang",
            "David Vandyke"
        ],
        "published": "2022-09-22T14:59:55Z",
        "summary": "Dialog modelling faces a difficult trade-off. Models are trained on a large\namount of text, yet their responses need to be limited to a desired scope and\nstyle of a dialog agent. Because the datasets used to achieve the former\ncontain language that is not compatible with the latter, pre-trained dialog\nmodels are fine-tuned on smaller curated datasets. However, the fine-tuning\nprocess robs them of the ability to produce diverse responses, eventually\nreducing them to dull conversation partners. In this paper we investigate if\nprompting can mitigate the above trade-off. Specifically, we experiment with\nconditioning the prompt on the query, rather than training a single prompt for\nall queries. By following the intuition that freezing the pre-trained language\nmodel will conserve its expressivity, we find that compared to fine-tuning,\nprompting can achieve a higher BLEU score and substantially improve the\ndiversity and novelty of the responses.",
        "pdf_link": "https://arxiv.org/pdf/2209.11068v1.pdf"
    },
    {
        "title": "MonoByte: A Pool of Monolingual Byte-level Language Models",
        "authors": [
            "Hugo Abonizio",
            "Leandro Rodrigues de Souza",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-09-22T14:32:48Z",
        "summary": "The zero-shot cross-lingual ability of models pretrained on multilingual and\neven monolingual corpora has spurred many hypotheses to explain this intriguing\nempirical result. However, due to the costs of pretraining, most research uses\npublic models whose pretraining methodology, such as the choice of\ntokenization, corpus size, and computational budget, might differ drastically.\nWhen researchers pretrain their own models, they often do so under a\nconstrained budget, and the resulting models might underperform significantly\ncompared to SOTA models. These experimental differences led to various\ninconsistent conclusions about the nature of the cross-lingual ability of these\nmodels. To help further research on the topic, we released 10 monolingual\nbyte-level models rigorously pretrained under the same configuration with a\nlarge compute budget (equivalent to 420 days on a V100) and corpora that are 4\ntimes larger than the original BERT's. Because they are tokenizer-free, the\nproblem of unseen token embeddings is eliminated, thus allowing researchers to\ntry a wider range of cross-lingual experiments in languages with different\nscripts. Additionally, we release two models pretrained on non-natural language\ntexts that can be used in sanity-check experiments. Experiments on QA and NLI\ntasks show that our monolingual models achieve competitive performance to the\nmultilingual one, and hence can be served to strengthen our understanding of\ncross-lingual transferability in language models.",
        "pdf_link": "https://arxiv.org/pdf/2209.11035v2.pdf"
    },
    {
        "title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation",
        "authors": [
            "Xingdi Yuan",
            "Tong Wang",
            "Yen-Hsiang Wang",
            "Emery Fine",
            "Rania Abdelghani",
            "Pauline Lucas",
            "H\u00e9l\u00e8ne Sauz\u00e9on",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2022-09-22T13:33:48Z",
        "summary": "Large Language Models (LLMs) have in recent years demonstrated impressive\nprowess in natural language generation. A common practice to improve generation\ndiversity is to sample multiple outputs from the model. However, there lacks a\nsimple and robust way of selecting the best output from these stochastic\nsamples. As a case study framed in the context of question generation, we\npropose two prompt-based approaches to selecting high-quality questions from a\nset of LLM-generated candidates. Our method works under the constraints of 1) a\nblack-box (non-modifiable) question generation model and 2) lack of access to\nhuman-annotated references -- both of which are realistic limitations for\nreal-world deployment of LLMs. With automatic as well as human evaluations, we\nempirically demonstrate that our approach can effectively select questions of\nhigher qualities than greedy generation.",
        "pdf_link": "https://arxiv.org/pdf/2209.11000v1.pdf"
    },
    {
        "title": "Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines",
        "authors": [
            "Anmol Bansal",
            "Arjun Choudhry",
            "Anubhav Sharma",
            "Seba Susan"
        ],
        "published": "2022-09-22T12:36:40Z",
        "summary": "Covid-19 has spread across the world and several vaccines have been developed\nto counter its surge. To identify the correct sentiments associated with the\nvaccines from social media posts, we fine-tune various state-of-the-art\npre-trained transformer models on tweets associated with Covid-19 vaccines.\nSpecifically, we use the recently introduced state-of-the-art pre-trained\ntransformer models RoBERTa, XLNet and BERT, and the domain-specific transformer\nmodels CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further\nexplore the option of text augmentation by oversampling using Language Model\nbased Oversampling Technique (LMOTE) to improve the accuracies of these models,\nspecifically, for small sample datasets where there is an imbalanced class\ndistribution among the positive, negative and neutral sentiment classes. Our\nresults summarize our findings on the suitability of text oversampling for\nimbalanced small sample datasets that are used to fine-tune state-of-the-art\npre-trained transformer models, and the utility of domain-specific transformer\nmodels for the classification task.",
        "pdf_link": "https://arxiv.org/pdf/2209.10966v2.pdf"
    },
    {
        "title": "Developing, Evaluating and Scaling Learning Agents in Multi-Agent Environments",
        "authors": [
            "Ian Gemp",
            "Thomas Anthony",
            "Yoram Bachrach",
            "Avishkar Bhoopchand",
            "Kalesha Bullard",
            "Jerome Connor",
            "Vibhavari Dasagi",
            "Bart De Vylder",
            "Edgar Duenez-Guzman",
            "Romuald Elie",
            "Richard Everett",
            "Daniel Hennes",
            "Edward Hughes",
            "Mina Khan",
            "Marc Lanctot",
            "Kate Larson",
            "Guy Lever",
            "Siqi Liu",
            "Luke Marris",
            "Kevin R. McKee",
            "Paul Muller",
            "Julien Perolat",
            "Florian Strub",
            "Andrea Tacchetti",
            "Eugene Tarassov",
            "Zhe Wang",
            "Karl Tuyls"
        ],
        "published": "2022-09-22T12:28:29Z",
        "summary": "The Game Theory & Multi-Agent team at DeepMind studies several aspects of\nmulti-agent learning ranging from computing approximations to fundamental\nconcepts in game theory to simulating social dilemmas in rich spatial\nenvironments and training 3-d humanoids in difficult team coordination tasks. A\nsignature aim of our group is to use the resources and expertise made available\nto us at DeepMind in deep reinforcement learning to explore multi-agent systems\nin complex environments and use these benchmarks to advance our understanding.\nHere, we summarise the recent work of our team and present a taxonomy that we\nfeel highlights many important open challenges in multi-agent research.",
        "pdf_link": "https://arxiv.org/pdf/2209.10958v1.pdf"
    },
    {
        "title": "Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model",
        "authors": [
            "Qiao Cheng",
            "Jin Huang",
            "Yitao Duan"
        ],
        "published": "2022-09-22T09:19:08Z",
        "summary": "This paper introduces a new data augmentation method for neural machine\ntranslation that can enforce stronger semantic consistency both within and\nacross languages. Our method is based on Conditional Masked Language Model\n(CMLM) which is bi-directional and can be conditional on both left and right\ncontext, as well as the label. We demonstrate that CMLM is a good technique for\ngenerating context-dependent word distributions. In particular, we show that\nCMLM is capable of enforcing semantic consistency by conditioning on both\nsource and target during substitution. In addition, to enhance diversity, we\nincorporate the idea of soft word substitution for data augmentation which\nreplaces a word with a probabilistic distribution over the vocabulary.\nExperiments on four translation datasets of different scales show that the\noverall solution results in more realistic data augmentation and better\ntranslation quality. Our approach consistently achieves the best performance in\ncomparison with strong and recent works and yields improvements of up to 1.90\nBLEU points over the baseline.",
        "pdf_link": "https://arxiv.org/pdf/2209.10875v1.pdf"
    },
    {
        "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation",
        "authors": [
            "Seongmin Hong",
            "Seungjae Moon",
            "Junsoo Kim",
            "Sungjae Lee",
            "Minsub Kim",
            "Dongsoo Lee",
            "Joo-Young Kim"
        ],
        "published": "2022-09-22T05:59:59Z",
        "summary": "Transformer is a deep learning language model widely used for natural\nlanguage processing (NLP) services in datacenters. Among transformer models,\nGenerative Pre-trained Transformer (GPT) has achieved remarkable performance in\ntext generation, or natural language generation (NLG), which needs the\nprocessing of a large input context in the summarization stage, followed by the\ngeneration stage that produces a single word at a time. The conventional\nplatforms such as GPU are specialized for the parallel processing of large\ninputs in the summarization stage, but their performance significantly degrades\nin the generation stage due to its sequential characteristic. Therefore, an\nefficient hardware platform is required to address the high latency caused by\nthe sequential characteristic of text generation.\n  In this paper, we present DFX, a multi-FPGA acceleration appliance that\nexecutes GPT-2 model inference end-to-end with low latency and high throughput\nin both summarization and generation stages. DFX uses model parallelism and\noptimized dataflow that is model-and-hardware-aware for fast simultaneous\nworkload execution among devices. Its compute cores operate on custom\ninstructions and provide GPT-2 operations end-to-end. We implement the proposed\nhardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the\nchannels of the high bandwidth memory (HBM) and the maximum number of compute\nresources for high hardware efficiency. DFX achieves 5.58x speedup and 3.99x\nenergy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is\nalso 8.21x more cost-effective than the GPU appliance, suggesting that it is a\npromising solution for text generation workloads in cloud datacenters.",
        "pdf_link": "https://arxiv.org/pdf/2209.10797v1.pdf"
    },
    {
        "title": "Deep Learning Based Page Creation for Improving E-Commerce Organic Search Traffic",
        "authors": [
            "Cheng Jie",
            "Da Xu",
            "Zigeng Wang",
            "Wei Shen"
        ],
        "published": "2022-09-22T05:36:44Z",
        "summary": "Organic search comprises a large portion of the total traffic for e-commerce\ncompanies. One approach to expand company's exposure on organic search channel\nlies on creating landing pages having broader coverage on customer intentions.\nIn this paper, we present a transformer language model based organic channel\npage management system aiming at increasing prominence of the company's overall\nclicks on the channel. Our system successfully handles the creation and\ndeployment process of millions of new landing pages. We show and discuss the\nreal-world performances of state-of-the-art language representation learning\nmethod, and reveal how we find them as the production-optimal solutions.",
        "pdf_link": "https://arxiv.org/pdf/2209.10792v2.pdf"
    },
    {
        "title": "AIR-JPMC@SMM4H'22: Classifying Self-Reported Intimate Partner Violence in Tweets with Multiple BERT-based Models",
        "authors": [
            "Alec Candidato",
            "Akshat Gupta",
            "Xiaomo Liu",
            "Sameena Shah"
        ],
        "published": "2022-09-22T03:43:25Z",
        "summary": "This paper presents our submission for the SMM4H 2022-Shared Task on the\nclassification of self-reported intimate partner violence on Twitter (in\nEnglish). The goal of this task was to accurately determine if the contents of\na given tweet demonstrated someone reporting their own experience with intimate\npartner violence. The submitted system is an ensemble of five RoBERTa models\neach weighted by their respective F1-scores on the validation data-set. This\nsystem performed 13% better than the baseline and was the best performing\nsystem overall for this shared task.",
        "pdf_link": "https://arxiv.org/pdf/2209.10763v1.pdf"
    },
    {
        "title": "Representing Affect Information in Word Embeddings",
        "authors": [
            "Yuhan Zhang",
            "Wenqi Chen",
            "Ruihan Zhang",
            "Xiajie Zhang"
        ],
        "published": "2022-09-21T18:16:33Z",
        "summary": "A growing body of research in natural language processing (NLP) and natural\nlanguage understanding (NLU) is investigating human-like knowledge learned or\nencoded in the word embeddings from large language models. This is a step\ntowards understanding what knowledge language models capture that resembles\nhuman understanding of language and communication. Here, we investigated\nwhether and how the affect meaning of a word (i.e., valence, arousal,\ndominance) is encoded in word embeddings pre-trained in large neural networks.\nWe used the human-labeled dataset as the ground truth and performed various\ncorrelational and classification tests on four types of word embeddings. The\nembeddings varied in being static or contextualized, and how much affect\nspecific information was prioritized during the pre-training and fine-tuning\nphase. Our analyses show that word embedding from the vanilla BERT model did\nnot saliently encode the affect information of English words. Only when the\nBERT model was fine-tuned on emotion-related tasks or contained extra\ncontextualized information from emotion-rich contexts could the corresponding\nembedding encode more relevant affect information.",
        "pdf_link": "https://arxiv.org/pdf/2209.10583v1.pdf"
    },
    {
        "title": "Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT",
        "authors": [
            "Karim Lasri",
            "Olga Seminck",
            "Alessandro Lenci",
            "Thierry Poibeau"
        ],
        "published": "2022-09-21T17:57:23Z",
        "summary": "Both humans and neural language models are able to perform subject-verb\nnumber agreement (SVA). In principle, semantics shouldn't interfere with this\ntask, which only requires syntactic knowledge. In this work we test whether\nmeaning interferes with this type of agreement in English in syntactic\nstructures of various complexities. To do so, we generate both semantically\nwell-formed and nonsensical items. We compare the performance of BERT-base to\nthat of humans, obtained with a psycholinguistic online crowdsourcing\nexperiment. We find that BERT and humans are both sensitive to our semantic\nmanipulation: They fail more often when presented with nonsensical items,\nespecially when their syntactic structure features an attractor (a noun phrase\nbetween the subject and the verb that has not the same number as the subject).\nWe also find that the effect of meaningfulness on SVA errors is stronger for\nBERT than for humans, showing higher lexical sensitivity of the former on this\ntask.",
        "pdf_link": "https://arxiv.org/pdf/2209.10538v1.pdf"
    },
    {
        "title": "Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers",
        "authors": [
            "Ruisi Zhang",
            "Seira Hidano",
            "Farinaz Koushanfar"
        ],
        "published": "2022-09-21T17:05:12Z",
        "summary": "Text classification has become widely used in various natural language\nprocessing applications like sentiment analysis. Current applications often use\nlarge transformer-based language models to classify input texts. However, there\nis a lack of systematic study on how much private information can be inverted\nwhen publishing models. In this paper, we formulate \\emph{Text Revealer} -- the\nfirst model inversion attack for text reconstruction against text\nclassification with transformers. Our attacks faithfully reconstruct private\ntexts included in training data with access to the target model. We leverage an\nexternal dataset and GPT-2 to generate the target domain-like fluent text, and\nthen perturb its hidden state optimally with the feedback from the target\nmodel. Our extensive experiments demonstrate that our attacks are effective for\ndatasets with different text lengths and can reconstruct private texts with\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2209.10505v1.pdf"
    },
    {
        "title": "SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese",
        "authors": [
            "Luan Thanh Nguyen",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "published": "2022-09-21T16:33:46Z",
        "summary": "Text classification is a typical natural language processing or computational\nlinguistics task with various interesting applications. As the number of users\non social media platforms increases, data acceleration promotes emerging\nstudies on Social Media Text Classification (SMTC) or social media text mining\non these valuable resources. In contrast to English, Vietnamese, one of the\nlow-resource languages, is still not concentrated on and exploited thoroughly.\nInspired by the success of the GLUE, we introduce the Social Media Text\nClassification Evaluation (SMTCE) benchmark, as a collection of datasets and\nmodels across a diverse set of SMTC tasks. With the proposed benchmark, we\nimplement and analyze the effectiveness of a variety of multilingual BERT-based\nmodels (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models\n(PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark.\nMonolingual models outperform multilingual models and achieve state-of-the-art\nresults on all text classification tasks. It provides an objective assessment\nof multilingual and monolingual BERT-based models on the benchmark, which will\nbenefit future studies about BERTology in the Vietnamese language.",
        "pdf_link": "https://arxiv.org/pdf/2209.10482v1.pdf"
    },
    {
        "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
        "authors": [
            "Hui Su",
            "Xiao Zhou",
            "Houjin Yu",
            "Xiaoyu Shen",
            "Yuwen Chen",
            "Zilin Zhu",
            "Yang Yu",
            "Jie Zhou"
        ],
        "published": "2022-09-21T14:05:30Z",
        "summary": "Large Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \"reading\" a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to 25 times larger. WeLM also exhibits strong\ncapabilities in multi-lingual and code-switching understanding, outperforming\nexisting multilingual language models pre-trained on 30 languages. Furthermore,\nWe collected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which can\nbe promising directions for future research. Our models can be applied from\nhttps://welm.weixin.qq.com/docs/api/.",
        "pdf_link": "https://arxiv.org/pdf/2209.10372v5.pdf"
    },
    {
        "title": "Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling",
        "authors": [
            "Thiemo Wambsganss",
            "Vinitra Swamy",
            "Roman Rietsche",
            "Tanja K\u00e4ser"
        ],
        "published": "2022-09-21T13:08:16Z",
        "summary": "Natural Language Processing (NLP) has become increasingly utilized to provide\nadaptivity in educational applications. However, recent research has\nhighlighted a variety of biases in pre-trained language models. While existing\nstudies investigate bias in different domains, they are limited in addressing\nfine-grained analysis on educational and multilingual corpora. In this work, we\nanalyze bias across text and through multiple architectures on a corpus of\n9,165 German peer-reviews collected from university students over five years.\nNotably, our corpus includes labels such as helpfulness, quality, and critical\naspect ratings from the peer-review recipient as well as demographic\nattributes. We conduct a Word Embedding Association Test (WEAT) analysis on (1)\nour collected corpus in connection with the clustered labels, (2) the most\ncommon pre-trained German language models (T5, BERT, and GPT-2) and GloVe\nembeddings, and (3) the language models after fine-tuning on our collected\ndata-set. In contrast to our initial expectations, we found that our collected\ncorpus does not reveal many biases in the co-occurrence analysis or in the\nGloVe embeddings. However, the pre-trained German language models find\nsubstantial conceptual, racial, and gender bias and have significant changes in\nbias across conceptual and racial axes during fine-tuning on the peer-review\ndata. With our research, we aim to contribute to the fourth UN sustainability\ngoal (quality education) with a novel dataset, an understanding of biases in\nnatural language education data, and the potential harms of not counteracting\nbiases in language models for educational tasks.",
        "pdf_link": "https://arxiv.org/pdf/2209.10335v2.pdf"
    },
    {
        "title": "T5QL: Taming language models for SQL generation",
        "authors": [
            "Samuel Arcadinho",
            "David Apar\u00edcio",
            "Hugo Veiga",
            "Ant\u00f3nio Alegria"
        ],
        "published": "2022-09-21T10:43:13Z",
        "summary": "Automatic SQL generation has been an active research area, aiming at\nstreamlining the access to databases by writing natural language with the given\nintent instead of writing SQL. Current SOTA methods for semantic parsing depend\non LLMs to achieve high predictive accuracy on benchmark datasets. This reduces\ntheir applicability, since LLMs requires expensive GPUs. Furthermore, SOTA\nmethods are ungrounded and thus not guaranteed to always generate valid SQL.\nHere we propose T5QL, a new SQL generation method that improves the performance\nin benchmark datasets when using smaller LMs, namely T5-Base, by 13pp when\ncompared against SOTA methods. Additionally, T5QL is guaranteed to always\noutput valid SQL using a context-free grammar to constrain SQL generation.\nFinally, we show that dividing semantic parsing in two tasks, candidate SQLs\ngeneration and candidate re-ranking, is a promising research avenue that can\nreduce the need for large LMs.",
        "pdf_link": "https://arxiv.org/pdf/2209.10254v1.pdf"
    },
    {
        "title": "Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers",
        "authors": [
            "Adebayo Oshingbesan",
            "Courage Ekoh",
            "Germann Atakpa",
            "Yonah Byaruagaba"
        ],
        "published": "2022-09-21T04:21:27Z",
        "summary": "Text-to-text transformers have shown remarkable success in the task of\nmulti-task transfer learning, especially in natural language processing (NLP).\nHowever, while there have been several attempts to train transformers on\ndifferent domains, there is usually a clear relationship between these domains,\ne.g.,, code summarization, where the natural language summary describes the\ncode. There have been very few attempts to study how multi-task transfer\nlearning works on tasks in significantly different domains. In this project, we\ninvestigated the behavior of multi-domain, multi-task learning using\nmulti-domain text-to-text transfer transformers (MD-T5) on four tasks across\ntwo domains - Python Code and Chess. We carried out extensive experiments using\nthree popular training strategies: Bert-style joint pretraining + successive\nfinetuning, GPT-style joint pretraining + successive finetuning, and GPT-style\njoint pretraining + joint finetuning. Also, we evaluate the model on four\nmetrics - Play Score, Eval Score, BLEU Score, and Multi-Domain Learning Score\n(MDLS). These metrics measure performance across the various tasks and\nmulti-domain learning. We show that while negative knowledge transfer and\ncatastrophic forgetting are still considerable challenges for all the models,\nthe GPT-style joint pretraining + joint finetuning strategy showed the most\npromise in multi-domain, multi-task learning as it performs well across all\nfour tasks while still keeping its multi-domain knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2209.10106v1.pdf"
    },
    {
        "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
        "authors": [
            "Wenhao Yu",
            "Dan Iter",
            "Shuohang Wang",
            "Yichong Xu",
            "Mingxuan Ju",
            "Soumya Sanyal",
            "Chenguang Zhu",
            "Michael Zeng",
            "Meng Jiang"
        ],
        "published": "2022-09-21T01:30:59Z",
        "summary": "Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large amount of world or domain knowledge. A common\napproach for knowledge-intensive tasks is to employ a retrieve-then-read\npipeline that first retrieves a handful of relevant contextual documents from\nan external corpus such as Wikipedia and then predicts an answer conditioned on\nthe retrieved documents. In this paper, we present a novel perspective for\nsolving knowledge-intensive tasks by replacing document retrievers with large\nlanguage model generators. We call our method generate-then-read (GenRead),\nwhich first prompts a large language model to generate contextutal documents\nbased on a given question, and then reads the generated documents to produce\nthe final answer. Furthermore, we propose a novel clustering-based prompting\nmethod that selects distinct prompts, resulting in the generated documents that\ncover different perspectives, leading to better recall over acceptable answers.\nWe conduct extensive experiments on three different knowledge-intensive tasks,\nincluding open-domain QA, fact checking, and dialogue system. Notably, GenRead\nachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly\noutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0\nand +3.9, without retrieving any documents from any external knowledge source.\nLastly, we demonstrate the model performance can be further improved by\ncombining retrieval and generation. Our code and generated documents can be\nfound at https://github.com/wyu97/GenRead.",
        "pdf_link": "https://arxiv.org/pdf/2209.10063v3.pdf"
    },
    {
        "title": "LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging",
        "authors": [
            "Andy Rosenbaum",
            "Saleh Soltan",
            "Wael Hamza",
            "Yannick Versley",
            "Markus Boese"
        ],
        "published": "2022-09-20T17:59:08Z",
        "summary": "We present LINGUIST, a method for generating annotated data for Intent\nClassification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a\n5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a\nflexible instruction prompt. In a 10-shot novel intent setting for the SNIPS\ndataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and\nExample Extrapolation) by a wide margin, showing absolute improvement for the\ntarget intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In\nthe zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST\nout-performs a strong baseline of Machine Translation with Slot Alignment by\n+4.14 points absolute on ST F1 Score across 6 languages, while matching\nperformance on IC. Finally, we verify our results on an internal large-scale\nmultilingual dataset for conversational agent IC+ST and show significant\nimprovements over a baseline which uses Back-Translation, Paraphrasing and Slot\nCatalog Resampling. To our knowledge, we are the first to demonstrate\ninstruction fine-tuning of a large-scale seq2seq model to control the outputs\nof multilingual intent- and slot-labeled data generation.",
        "pdf_link": "https://arxiv.org/pdf/2209.09900v1.pdf"
    },
    {
        "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
        "authors": [
            "Boyuan Chen",
            "Fei Xia",
            "Brian Ichter",
            "Kanishka Rao",
            "Keerthana Gopalakrishnan",
            "Michael S. Ryoo",
            "Austin Stone",
            "Daniel Kappler"
        ],
        "published": "2022-09-20T17:29:56Z",
        "summary": "Large language models (LLMs) have unlocked new capabilities of task planning\nfrom human instructions. However, prior attempts to apply LLMs to real-world\nrobotic tasks are limited by the lack of grounding in the surrounding scene. In\nthis paper, we develop NLMap, an open-vocabulary and queryable scene\nrepresentation to address this problem. NLMap serves as a framework to gather\nand integrate contextual information into LLM planners, allowing them to see\nand query available objects in the scene before generating a\ncontext-conditioned plan. NLMap first establishes a natural language queryable\nscene representation with Visual Language models (VLMs). An LLM based object\nproposal module parses instructions and proposes involved objects to query the\nscene representation for object availability and location. An LLM planner then\nplans with such information about the scene. NLMap allows robots to operate\nwithout a fixed list of objects nor executable options, enabling real robot\noperation unachievable by previous methods. Project website:\nhttps://nlmap-saycan.github.io",
        "pdf_link": "https://arxiv.org/pdf/2209.09874v2.pdf"
    },
    {
        "title": "Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation",
        "authors": [
            "Mohammadreza Tayaranian",
            "Alireza Ghaffari",
            "Marzieh S. Tahaei",
            "Mehdi Rezagholizadeh",
            "Masoud Asgharian",
            "Vahid Partovi Nia"
        ],
        "published": "2022-09-20T16:02:28Z",
        "summary": "The large number of parameters of some prominent language models, such as\nBERT, makes their fine-tuning on downstream tasks computationally intensive and\nenergy hungry. Previously researchers were focused on lower bit-width integer\ndata types for the forward propagation of language models to save memory and\ncomputation. As for the backward propagation, however, only 16-bit\nfloating-point data type has been used for the fine-tuning of BERT. In this\nwork, we use integer arithmetic for both forward and back propagation in the\nfine-tuning of BERT. We study the effects of varying the integer bit-width on\nthe model's metric performance. Our integer fine-tuning uses integer arithmetic\nto perform forward propagation and gradient computation of linear, layer-norm,\nand embedding layers of BERT. We fine-tune BERT using our integer training\nmethod on SQuAD v1.1 and SQuAD v2., and GLUE benchmark. We demonstrate that\nmetric performance of fine-tuning 16-bit integer BERT matches both 16-bit and\n32-bit floating-point baselines. Furthermore, using the faster and more memory\nefficient 8-bit integer data type, integer fine-tuning of BERT loses an average\nof 3.1 points compared to the FP32 baseline.",
        "pdf_link": "https://arxiv.org/pdf/2209.09815v2.pdf"
    },
    {
        "title": "Relaxed Attention for Transformer Models",
        "authors": [
            "Timo Lohrenz",
            "Bj\u00f6rn M\u00f6ller",
            "Zhengyang Li",
            "Tim Fingscheidt"
        ],
        "published": "2022-09-20T14:10:28Z",
        "summary": "The powerful modeling capabilities of all-attention-based transformer\narchitectures often cause overfitting and - for natural language processing\ntasks - lead to an implicitly learned internal language model in the\nautoregressive transformer decoder complicating the integration of external\nlanguage models. In this paper, we explore relaxed attention, a simple and\neasy-to-implement smoothing of the attention weights, yielding a two-fold\nimprovement to the general transformer architecture: First, relaxed attention\nprovides regularization when applied to the self-attention layers in the\nencoder. Second, we show that it naturally supports the integration of an\nexternal language model as it suppresses the implicitly learned internal\nlanguage model by relaxing the cross attention in the decoder. We demonstrate\nthe benefit of relaxed attention across several tasks with clear improvement in\ncombination with recent benchmark approaches. Specifically, we exceed the\nformer state-of-the-art performance of 26.90% word error rate on the largest\npublic lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as\nwe achieve a top-performing BLEU score of 37.67 on the IWSLT14\n(DE$\\rightarrow$EN) machine translation task without external language models\nand virtually no additional model parameters. Code and models will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2209.09735v1.pdf"
    },
    {
        "title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics",
        "authors": [
            "Daniil Larionov",
            "Jens Gr\u00fcnwald",
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "published": "2022-09-20T10:12:07Z",
        "summary": "Efficiency is a key property to foster inclusiveness and reduce environmental\ncosts, especially in an era of LLMs. In this work, we provide a comprehensive\nevaluation of efficiency for MT evaluation metrics. Our approach involves\nreplacing computation-intensive transformers with lighter alternatives and\nemploying linear and quadratic approximations for alignment algorithms on top\nof LLM representations. We evaluate six (reference-free and reference-based)\nmetrics across three MT datasets and examine 16 lightweight transformers. In\naddition, we look into the training efficiency of metrics like COMET by\nutilizing adapters. Our results indicate that (a) TinyBERT provides the optimal\nbalance between quality and efficiency, (b) CPU speed-ups are more substantial\nthan those on GPU; (c) WMD approximations yield no efficiency gains while\nreducing quality and (d) adapters enhance training efficiency (regarding\nbackward pass speed and memory requirements) as well as, in some cases, metric\nquality. These findings can help to strike a balance between evaluation speed\nand quality, which is essential for effective NLG systems. Furthermore, our\nresearch contributes to the ongoing efforts to optimize NLG evaluation metrics\nwith minimal impact on performance. To our knowledge, ours is the most\ncomprehensive analysis of different aspects of efficiency for MT metrics\nconducted so far.",
        "pdf_link": "https://arxiv.org/pdf/2209.09593v2.pdf"
    },
    {
        "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
        "authors": [
            "Pan Lu",
            "Swaroop Mishra",
            "Tony Xia",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Song-Chun Zhu",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "published": "2022-09-20T07:04:24Z",
        "summary": "When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2209.09513v2.pdf"
    },
    {
        "title": "GAMA: Generative Adversarial Multi-Object Scene Attacks",
        "authors": [
            "Abhishek Aich",
            "Calvin-Khang Ta",
            "Akash Gupta",
            "Chengyu Song",
            "Srikanth V. Krishnamurthy",
            "M. Salman Asif",
            "Amit K. Roy-Chowdhury"
        ],
        "published": "2022-09-20T06:40:54Z",
        "summary": "The majority of methods for crafting adversarial attacks have focused on\nscenes with a single dominant object (e.g., images from ImageNet). On the other\nhand, natural scenes include multiple dominant objects that are semantically\nrelated. Thus, it is crucial to explore designing attack strategies that look\nbeyond learning on single-object scenes or attack single-object victim\nclassifiers. Due to their inherent property of strong transferability of\nperturbations to unknown models, this paper presents the first approach of\nusing generative models for adversarial attacks on multi-object scenes. In\norder to represent the relationships between different objects in the input\nscene, we leverage upon the open-sourced pre-trained vision-language model CLIP\n(Contrastive Language-Image Pre-training), with the motivation to exploit the\nencoded semantics in the language space along with the visual space. We call\nthis attack approach Generative Adversarial Multi-object scene Attacks (GAMA).\nGAMA demonstrates the utility of the CLIP model as an attacker's tool to train\nformidable perturbation generators for multi-object scenes. Using the joint\nimage-text features to train the generator, we show that GAMA can craft potent\ntransferable perturbations in order to fool victim classifiers in various\nattack settings. For example, GAMA triggers ~16% more misclassification than\nstate-of-the-art generative approaches in black-box settings where both the\nclassifier architecture and data distribution of the attacker are different\nfrom the victim. Our code is available here:\nhttps://abhishekaich27.github.io/gama.html",
        "pdf_link": "https://arxiv.org/pdf/2209.09502v2.pdf"
    },
    {
        "title": "Generalizing through Forgetting -- Domain Generalization for Symptom Event Extraction in Clinical Notes",
        "authors": [
            "Sitong Zhou",
            "Kevin Lybarger",
            "Meliha Yetisgen",
            "Mari Ostendorf"
        ],
        "published": "2022-09-20T05:53:22Z",
        "summary": "Symptom information is primarily documented in free-text clinical notes and\nis not directly accessible for downstream applications. To address this\nchallenge, information extraction approaches that can handle clinical language\nvariation across different institutions and specialties are needed. In this\npaper, we present domain generalization for symptom extraction using\npretraining and fine-tuning data that differs from the target domain in terms\nof institution and/or specialty and patient population. We extract symptom\nevents using a transformer-based joint entity and relation extraction method.\nTo reduce reliance on domain-specific features, we propose a domain\ngeneralization method that dynamically masks frequent symptoms words in the\nsource domain. Additionally, we pretrain the transformer language model (LM) on\ntask-related unlabeled texts for better representation. Our experiments\nindicate that masking and adaptive pretraining methods can significantly\nimprove performance when the source domain is more distant from the target\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2209.09485v2.pdf"
    },
    {
        "title": "Unsupervised Early Exit in DNNs with Multiple Exits",
        "authors": [
            "Hari Narayan N U",
            "Manjesh K. Hanawal",
            "Avinash Bhardwaj"
        ],
        "published": "2022-09-20T05:35:54Z",
        "summary": "Deep Neural Networks (DNNs) are generally designed as sequentially cascaded\ndifferentiable blocks/layers with a prediction module connected only to its\nlast layer. DNNs can be attached with prediction modules at multiple points\nalong the backbone where inference can stop at an intermediary stage without\npassing through all the modules. The last exit point may offer a better\nprediction error but also involves more computational resources and latency. An\nexit point that is `optimal' in terms of both prediction error and cost is\ndesirable. The optimal exit point may depend on the latent distribution of the\ntasks and may change from one task type to another. During neural inference,\nthe ground truth of instances may not be available and error rates at each exit\npoint cannot be estimated. Hence one is faced with the problem of selecting the\noptimal exit in an unsupervised setting. Prior works tackled this problem in an\noffline supervised setting assuming that enough labeled data is available to\nestimate the error rate at each exit point and tune the parameters for better\naccuracy. However, pre-trained DNNs are often deployed in new domains for which\na large amount of ground truth may not be available. We model the problem of\nexit selection as an unsupervised online learning problem and use bandit theory\nto identify the optimal exit point. Specifically, we focus on Elastic BERT, a\npre-trained multi-exit DNN to demonstrate that it `nearly' satisfies the Strong\nDominance (SD) property making it possible to learn the optimal exit in an\nonline setup without knowing the ground truth labels. We develop upper\nconfidence bound (UCB) based algorithm named UEE-UCB that provably achieves\nsub-linear regret under the SD property. Thus our method provides a means to\nadaptively learn domain-specific optimal exit points in multi-exit DNNs. We\nempirically validate our algorithm on IMDb and Yelp datasets.",
        "pdf_link": "https://arxiv.org/pdf/2209.09480v1.pdf"
    },
    {
        "title": "A Few-shot Approach to Resume Information Extraction via Prompts",
        "authors": [
            "Chengguang Gan",
            "Tatsunori Mori"
        ],
        "published": "2022-09-20T04:01:46Z",
        "summary": "Prompt learning's fine-tune performance on text classification tasks has\nattracted the NLP community. This paper applies it to resume information\nextraction, improving existing methods for this task. We created manual\ntemplates and verbalizers tailored to resume texts and compared the performance\nof Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced the\nverbalizer design for Knowledgeable Prompt-tuning, contributing to prompt\ntemplate design across NLP tasks. We present the Manual Knowledgeable\nVerbalizer (MKV), a rule for constructing verbalizers for specific\napplications. Our tests show that MKV rules yield more effective, robust\ntemplates and verbalizers than existing methods. Our MKV approach resolved\nsample imbalance, surpassing current automatic prompt methods. This study\nunderscores the value of tailored prompt learning for resume extraction,\nstressing the importance of custom-designed templates and verbalizers.",
        "pdf_link": "https://arxiv.org/pdf/2209.09450v2.pdf"
    },
    {
        "title": "Probabilistic Generative Transformer Language models for Generative Design of Molecules",
        "authors": [
            "Lai Wei",
            "Nihang Fu",
            "Yuqi Song",
            "Qian Wang",
            "Jianjun Hu"
        ],
        "published": "2022-09-20T01:51:57Z",
        "summary": "Self-supervised neural language models have recently found wide applications\nin generative design of organic molecules and protein sequences as well as\nrepresentation learning for downstream structure classification and functional\nprediction. However, most of the existing deep learning models for molecule\ndesign usually require a big dataset and have a black-box architecture, which\nmakes it difficult to interpret their design logic. Here we propose Generative\nMolecular Transformer (GMTransformer), a probabilistic neural network model for\ngenerative design of molecules. Our model is built on the blank filling\nlanguage model originally developed for text processing, which has demonstrated\nunique advantages in learning the \"molecules grammars\" with high-quality\ngeneration, interpretability, and data efficiency. Benchmarked on the MOSES\ndatasets, our models achieve high novelty and Scaf compared to other baselines.\nThe probabilistic generation steps have the potential in tinkering molecule\ndesign due to their capability of recommending how to modify existing molecules\nwith explanation, guided by the learned implicit molecule chemistry. The source\ncode and datasets can be accessed freely at\nhttps://github.com/usccolumbia/GMTransformer",
        "pdf_link": "https://arxiv.org/pdf/2209.09406v1.pdf"
    },
    {
        "title": "Will It Blend? Mixing Training Paradigms & Prompting for Argument Quality Prediction",
        "authors": [
            "Michiel van der Meer",
            "Myrthe Reuver",
            "Urja Khurana",
            "Lea Krause",
            "Selene B\u00e1ez Santamar\u00eda"
        ],
        "published": "2022-09-19T12:34:46Z",
        "summary": "This paper describes our contributions to the Shared Task of the 9th Workshop\non Argument Mining (2022). Our approach uses Large Language Models for the task\nof Argument Quality Prediction. We perform prompt engineering using GPT-3, and\nalso investigate the training paradigms multi-task learning, contrastive\nlearning, and intermediate-task training. We find that a mixed prediction setup\noutperforms single models. Prompting GPT-3 works best for predicting argument\nvalidity, and argument novelty is best estimated by a model trained using all\nthree training paradigms.",
        "pdf_link": "https://arxiv.org/pdf/2209.08966v2.pdf"
    },
    {
        "title": "Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer",
        "authors": [
            "Qiong Nan",
            "Danding Wang",
            "Yongchun Zhu",
            "Qiang Sheng",
            "Yuhui Shi",
            "Juan Cao",
            "Jintao Li"
        ],
        "published": "2022-09-19T10:21:13Z",
        "summary": "Both real and fake news in various domains, such as politics, health, and\nentertainment are spread via online social media every day, necessitating fake\nnews detection for multiple domains. Among them, fake news in specific domains\nlike politics and health has more serious potential negative impacts on the\nreal world (e.g., the infodemic led by COVID-19 misinformation). Previous\nstudies focus on multi-domain fake news detection, by equally mining and\nmodeling the correlation between domains. However, these multi-domain methods\nsuffer from a seesaw problem: the performance of some domains is often improved\nat the cost of hurting the performance of other domains, which could lead to an\nunsatisfying performance in specific domains. To address this issue, we propose\na Domain- and Instance-level Transfer Framework for Fake News Detection\n(DITFEND), which could improve the performance of specific target domains. To\ntransfer coarse-grained domain-level knowledge, we train a general model with\ndata of all domains from the meta-learning perspective. To transfer\nfine-grained instance-level knowledge and adapt the general model to a target\ndomain, we train a language model on the target domain to evaluate the\ntransferability of each data instance in source domains and re-weigh each\ninstance's contribution. Offline experiments on two datasets demonstrate the\neffectiveness of DITFEND. Online experiments show that DITFEND brings\nadditional improvements over the base models in a real-world scenario.",
        "pdf_link": "https://arxiv.org/pdf/2209.08902v2.pdf"
    },
    {
        "title": "Tree-based Text-Vision BERT for Video Search in Baidu Video Advertising",
        "authors": [
            "Tan Yu",
            "Jie Liu",
            "Yi Yang",
            "Yi Li",
            "Hongliang Fei",
            "Ping Li"
        ],
        "published": "2022-09-19T04:49:51Z",
        "summary": "The advancement of the communication technology and the popularity of the\nsmart phones foster the booming of video ads. Baidu, as one of the leading\nsearch engine companies in the world, receives billions of search queries per\nday. How to pair the video ads with the user search is the core task of Baidu\nvideo advertising. Due to the modality gap, the query-to-video retrieval is\nmuch more challenging than traditional query-to-document retrieval and\nimage-to-image search. Traditionally, the query-to-video retrieval is tackled\nby the query-to-title retrieval, which is not reliable when the quality of\ntiles are not high. With the rapid progress achieved in computer vision and\nnatural language processing in recent years, content-based search methods\nbecomes promising for the query-to-video retrieval. Benefited from pretraining\non large-scale datasets, some visionBERT methods based on cross-modal attention\nhave achieved excellent performance in many vision-language tasks not only in\nacademia but also in industry. Nevertheless, the expensive computation cost of\ncross-modal attention makes it impractical for large-scale search in industrial\napplications. In this work, we present a tree-based combo-attention network\n(TCAN) which has been recently launched in Baidu's dynamic video advertising\nplatform. It provides a practical solution to deploy the heavy cross-modal\nattention for the large-scale query-to-video search. After launching tree-based\ncombo-attention network, click-through rate gets improved by 2.29\\% and\nconversion rate get improved by 2.63\\%.",
        "pdf_link": "https://arxiv.org/pdf/2209.08759v1.pdf"
    },
    {
        "title": "Knowledge-based Analogical Reasoning in Neuro-symbolic Latent Spaces",
        "authors": [
            "Vishwa Shah",
            "Aditya Sharma",
            "Gautam Shroff",
            "Lovekesh Vig",
            "Tirtharaj Dash",
            "Ashwin Srinivasan"
        ],
        "published": "2022-09-19T04:03:20Z",
        "summary": "Analogical Reasoning problems challenge both connectionist and symbolic AI\nsystems as these entail a combination of background knowledge, reasoning and\npattern recognition. While symbolic systems ingest explicit domain knowledge\nand perform deductive reasoning, they are sensitive to noise and require inputs\nbe mapped to preset symbolic features. Connectionist systems on the other hand\ncan directly ingest rich input spaces such as images, text or speech and\nrecognize pattern even with noisy inputs. However, connectionist models\nstruggle to include explicit domain knowledge for deductive reasoning. In this\npaper, we propose a framework that combines the pattern recognition abilities\nof neural networks with symbolic reasoning and background knowledge for solving\na class of Analogical Reasoning problems where the set of attributes and\npossible relations across them are known apriori. We take inspiration from the\n'neural algorithmic reasoning' approach [DeepMind 2020] and use\nproblem-specific background knowledge by (i) learning a distributed\nrepresentation based on a symbolic model of the problem (ii) training\nneural-network transformations reflective of the relations involved in the\nproblem and finally (iii) training a neural network encoder from images to the\ndistributed representation in (i). These three elements enable us to perform\nsearch-based reasoning using neural networks as elementary functions\nmanipulating distributed representations. We test this on visual analogy\nproblems in RAVENs Progressive Matrices, and achieve accuracy competitive with\nhuman performance and, in certain cases, superior to initial end-to-end\nneural-network based approaches. While recent neural models trained at scale\nyield SOTA, our novel neuro-symbolic reasoning approach is a promising\ndirection for this problem, and is arguably more general, especially for\nproblems where domain knowledge is available.",
        "pdf_link": "https://arxiv.org/pdf/2209.08750v1.pdf"
    },
    {
        "title": "Enabling Conversational Interaction with Mobile UI using Large Language Models",
        "authors": [
            "Bryan Wang",
            "Gang Li",
            "Yang Li"
        ],
        "published": "2022-09-18T20:58:39Z",
        "summary": "Conversational agents show the promise to allow users to interact with mobile\ndevices using language. However, to perform diverse UI tasks with natural\nlanguage, developers typically need to create separate datasets and models for\neach specific task, which is expensive and effort-consuming. Recently,\npre-trained large language models (LLMs) have been shown capable of\ngeneralizing to various downstream tasks when prompted with a handful of\nexamples from the target task. This paper investigates the feasibility of\nenabling versatile conversational interactions with mobile UIs using a single\nLLM. We designed prompting techniques to adapt an LLM to mobile UIs. We\nexperimented with four important modeling tasks that address various scenarios\nin conversational interaction. Our method achieved competitive performance on\nthese challenging tasks without requiring dedicated datasets and training,\noffering a lightweight and generalizable approach to enable language-based\nmobile interaction.",
        "pdf_link": "https://arxiv.org/pdf/2209.08655v2.pdf"
    },
    {
        "title": "CodeQueries: A Dataset of Semantic Queries over Code",
        "authors": [
            "Surya Prakash Sahu",
            "Madhurima Mandal",
            "Shikhar Bharadwaj",
            "Aditya Kanade",
            "Petros Maniatis",
            "Shirish Shevade"
        ],
        "published": "2022-09-17T17:09:30Z",
        "summary": "Developers often have questions about semantic aspects of code they are\nworking on, e.g., \"Is there a class whose parent classes declare a conflicting\nattribute?\". Answering them requires understanding code semantics such as\nattributes and inheritance relation of classes. An answer to such a question\nshould identify code spans constituting the answer (e.g., the declaration of\nthe subclass) as well as supporting facts (e.g., the definitions of the\nconflicting attributes). The existing work on question-answering over code has\nconsidered yes/no questions or method-level context. We contribute a labeled\ndataset, called CodeQueries, of semantic queries over Python code. Compared to\nthe existing datasets, in CodeQueries, the queries are about code semantics,\nthe context is file level and the answers are code spans. We curate the dataset\nbased on queries supported by a widely-used static analysis tool, CodeQL, and\ninclude both positive and negative examples, and queries requiring single-hop\nand multi-hop reasoning.\n  To assess the value of our dataset, we evaluate baseline neural approaches.\nWe study a large language model (GPT3.5-Turbo) in zero-shot and few-shot\nsettings on a subset of CodeQueries. We also evaluate a BERT style model\n(CuBERT) with fine-tuning. We find that these models achieve limited success on\nCodeQueries. CodeQueries is thus a challenging dataset to test the ability of\nneural models, to understand code semantics, in the extractive\nquestion-answering setting.",
        "pdf_link": "https://arxiv.org/pdf/2209.08372v2.pdf"
    },
    {
        "title": "From Disfluency Detection to Intent Detection and Slot Filling",
        "authors": [
            "Mai Hoang Dao",
            "Thinh Hung Truong",
            "Dat Quoc Nguyen"
        ],
        "published": "2022-09-17T16:03:57Z",
        "summary": "We present the first empirical study investigating the influence of\ndisfluency detection on downstream tasks of intent detection and slot filling.\nWe perform this study for Vietnamese -- a low-resource language that has no\nprevious study as well as no public dataset available for disfluency detection.\nFirst, we extend the fluent Vietnamese intent detection and slot filling\ndataset PhoATIS by manually adding contextual disfluencies and annotating them.\nThen, we conduct experiments using strong baselines for disfluency detection\nand joint intent detection and slot filling, which are based on pre-trained\nlanguage models. We find that: (i) disfluencies produce negative effects on the\nperformances of the downstream intent detection and slot filling tasks, and\n(ii) in the disfluency context, the pre-trained multilingual language model\nXLM-R helps produce better intent detection and slot filling performances than\nthe pre-trained monolingual language model PhoBERT, and this is opposite to\nwhat generally found in the fluency context.",
        "pdf_link": "https://arxiv.org/pdf/2209.08359v1.pdf"
    },
    {
        "title": "Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models",
        "authors": [
            "Ben Prystawski",
            "Paul Thibodeau",
            "Christopher Potts",
            "Noah D. Goodman"
        ],
        "published": "2022-09-16T19:23:13Z",
        "summary": "Probabilistic models of language understanding are valuable tools for\ninvestigating human language use. However, they need to be hand-designed for a\nparticular domain. In contrast, large language models (LLMs) are trained on\ntext that spans a wide array of domains, but they lack the structure and\ninterpretability of probabilistic models. In this paper, we use\nchain-of-thought prompts to introduce structures from probabilistic models into\nLLMs. We explore this approach in the case of metaphor understanding. Our\nchain-of-thought prompts lead language models to infer latent variables and\nreason about their relationships in order to choose appropriate paraphrases for\nmetaphors. The latent variables and relationships chosen are informed by\ntheories of metaphor understanding from cognitive psychology. We apply these\nprompts to the two largest versions of GPT-3 and show that they can improve\nperformance in a paraphrase selection task.",
        "pdf_link": "https://arxiv.org/pdf/2209.08141v2.pdf"
    },
    {
        "title": "Changing the Representation: Examining Language Representation for Neural Sign Language Production",
        "authors": [
            "Harry Walsh",
            "Ben Saunders",
            "Richard Bowden"
        ],
        "published": "2022-09-16T12:45:29Z",
        "summary": "Neural Sign Language Production (SLP) aims to automatically translate from\nspoken language sentences to sign language videos. Historically the SLP task\nhas been broken into two steps; Firstly, translating from a spoken language\nsentence to a gloss sequence and secondly, producing a sign language video\ngiven a sequence of glosses. In this paper we apply Natural Language Processing\ntechniques to the first step of the SLP pipeline. We use language models such\nas BERT and Word2Vec to create better sentence level embeddings, and apply\nseveral tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to\nHamNoSys (T2H) translation, and show the advantages of using a phonetic\nrepresentation for sign language translation rather than a sign level gloss\nrepresentation. Furthermore, we use HamNoSys to extract the hand shape of a\nsign and use this as additional supervision during training, further increasing\nthe performance on T2H. Assembling best practise, we achieve a BLEU-4 score of\n26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.06312v1.pdf"
    },
    {
        "title": "The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding",
        "authors": [
            "Hao Fang",
            "Anusha Balakrishnan",
            "Harsh Jhamtani",
            "John Bufe",
            "Jean Crawford",
            "Jayant Krishnamurthy",
            "Adam Pauls",
            "Jason Eisner",
            "Jacob Andreas",
            "Dan Klein"
        ],
        "published": "2022-09-16T09:00:49Z",
        "summary": "In a real-world dialogue system, generated text must be truthful and\ninformative while remaining fluent and adhering to a prescribed style.\nSatisfying these constraints simultaneously is difficult for the two\npredominant paradigms in language generation: neural language modeling and\nrule-based generation. We describe a hybrid architecture for dialogue response\ngeneration that combines the strengths of both paradigms. The first component\nof this architecture is a rule-based content selection model defined using a\nnew formal framework called dataflow transduction, which uses declarative rules\nto transduce a dialogue agent's actions and their results (represented as\ndataflow graphs) into context-free grammars representing the space of\ncontextually acceptable responses. The second component is a constrained\ndecoding procedure that uses these grammars to constrain the output of a neural\nlanguage model, which selects fluent utterances. Our experiments show that this\nsystem outperforms both rule-based and learned approaches in human evaluations\nof fluency, relevance, and truthfulness.",
        "pdf_link": "https://arxiv.org/pdf/2209.07800v2.pdf"
    },
    {
        "title": "Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning",
        "authors": [
            "David Bertoin",
            "Adil Zouitine",
            "Mehdi Zouitine",
            "Emmanuel Rachelson"
        ],
        "published": "2022-09-16T08:28:38Z",
        "summary": "Deep reinforcement learning policies, despite their outstanding efficiency in\nsimulated visual control tasks, have shown disappointing ability to generalize\nacross disturbances in the input training images. Changes in image statistics\nor distracting background elements are pitfalls that prevent generalization and\nreal-world applicability of such control policies. We elaborate on the\nintuition that a good visual policy should be able to identify which pixels are\nimportant for its decision, and preserve this identification of important\nsources of information across images. This implies that training of a policy\nwith small generalization gap should focus on such important pixels and ignore\nthe others. This leads to the introduction of saliency-guided Q-networks\n(SGQN), a generic method for visual reinforcement learning, that is compatible\nwith any value function learning method. SGQN vastly improves the\ngeneralization capability of Soft Actor-Critic agents and outperforms existing\nstateof-the-art methods on the Deepmind Control Generalization benchmark,\nsetting a new reference in terms of training efficiency, generalization gap,\nand policy interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2209.09203v3.pdf"
    },
    {
        "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
        "authors": [
            "Aman Madaan",
            "Amir Yazdanbakhsh"
        ],
        "published": "2022-09-16T02:54:00Z",
        "summary": "The past decade has witnessed dramatic gains in natural language processing\nand an unprecedented scaling of large language models. These developments have\nbeen accelerated by the advent of few-shot techniques such as chain of thought\n(CoT) prompting. Specifically, CoT pushes the performance of large language\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\nDespite impressive results across various tasks, the reasons behind their\nsuccess have not been explored. This work uses counterfactual prompting to\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\nlarge language models. We first systematically identify and define the key\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\nconduct an exhaustive set of experiments across four different tasks, by\nquerying the model with counterfactual prompts where only one of these\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\nCODEX) reveal several surprising findings and brings into question the\nconventional wisdom around few-shot prompting. First, the presence of factual\npatterns in a prompt is practically immaterial to the success of CoT. Second,\nour results conclude that the primary role of intermediate steps may not be to\nfacilitate learning how to solve a task. The intermediate steps are rather a\nbeacon for the model to realize what symbols to replicate in the output to form\na factual answer. Further, text imbues patterns with commonsense knowledge and\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\nrelationship between text and patterns explains the success of few-shot\nprompting: text helps extract commonsense from the question to help patterns,\nand patterns enforce task understanding and direct text generation.",
        "pdf_link": "https://arxiv.org/pdf/2209.07686v2.pdf"
    },
    {
        "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter",
        "authors": [
            "Xinyang Zhang",
            "Yury Malkov",
            "Omar Florez",
            "Serim Park",
            "Brian McWilliams",
            "Jiawei Han",
            "Ahmed El-Kishky"
        ],
        "published": "2022-09-15T19:01:21Z",
        "summary": "Pre-trained language models (PLMs) are fundamental for natural language\nprocessing applications. Most existing PLMs are not tailored to the noisy\nuser-generated text on social media, and the pre-training does not factor in\nthe valuable social engagement logs available in a social network. We present\nTwHIN-BERT, a multilingual language model productionized at Twitter, trained on\nin-domain data from the popular social network. TwHIN-BERT differs from prior\npre-trained language models as it is trained with not only text-based\nself-supervision, but also with a social objective based on the rich social\nengagements within a Twitter heterogeneous information network (TwHIN). Our\nmodel is trained on 7 billion tweets covering over 100 distinct languages,\nproviding a valuable representation to model short, noisy, user-generated text.\nWe evaluate our model on various multilingual social recommendation and\nsemantic understanding tasks and demonstrate significant metric improvement\nover established pre-trained language models. We open-source TwHIN-BERT and our\ncurated hashtag prediction and social engagement benchmark datasets to the\nresearch community.",
        "pdf_link": "https://arxiv.org/pdf/2209.07562v3.pdf"
    },
    {
        "title": "Machine Reading, Fast and Slow: When Do Models \"Understand\" Language?",
        "authors": [
            "Sagnik Ray Choudhury",
            "Anna Rogers",
            "Isabelle Augenstein"
        ],
        "published": "2022-09-15T16:25:44Z",
        "summary": "Two of the most fundamental challenges in Natural Language Understanding\n(NLU) at present are: (a) how to establish whether deep learning-based models\nscore highly on NLU benchmarks for the 'right' reasons; and (b) to understand\nwhat those reasons would even be. We investigate the behavior of reading\ncomprehension models with respect to two linguistic 'skills': coreference\nresolution and comparison. We propose a definition for the reasoning steps\nexpected from a system that would be 'reading slowly', and compare that with\nthe behavior of five models of the BERT family of various sizes, observed\nthrough saliency scores and counterfactual explanations. We find that for\ncomparison (but not coreference) the systems based on larger encoders are more\nlikely to rely on the 'right' information, but even they struggle with\ngeneralization, suggesting that they still learn specific lexical patterns\nrather than the general principles of comparison.",
        "pdf_link": "https://arxiv.org/pdf/2209.07430v1.pdf"
    },
    {
        "title": "Continuous MDP Homomorphisms and Homomorphic Policy Gradient",
        "authors": [
            "Sahand Rezaei-Shoshtari",
            "Rosie Zhao",
            "Prakash Panangaden",
            "David Meger",
            "Doina Precup"
        ],
        "published": "2022-09-15T15:26:49Z",
        "summary": "Abstraction has been widely studied as a way to improve the efficiency and\ngeneralization of reinforcement learning algorithms. In this paper, we study\nabstraction in the continuous-control setting. We extend the definition of MDP\nhomomorphisms to encompass continuous actions in continuous state spaces. We\nderive a policy gradient theorem on the abstract MDP, which allows us to\nleverage approximate symmetries of the environment for policy optimization.\nBased on this theorem, we propose an actor-critic algorithm that is able to\nlearn the policy and the MDP homomorphism map simultaneously, using the lax\nbisimulation metric. We demonstrate the effectiveness of our method on\nbenchmark tasks in the DeepMind Control Suite. Our method's ability to utilize\nMDP homomorphisms for representation learning leads to improved performance\nwhen learning from pixel observations.",
        "pdf_link": "https://arxiv.org/pdf/2209.07364v1.pdf"
    },
    {
        "title": "Linear Transformations for Cross-lingual Sentiment Analysis",
        "authors": [
            "Pavel P\u0159ib\u00e1\u0148",
            "Jakub \u0160m\u00edd",
            "Adam Mi\u0161tera",
            "Pavel Kr\u00e1l"
        ],
        "published": "2022-09-15T12:27:16Z",
        "summary": "This paper deals with cross-lingual sentiment analysis in Czech, English and\nFrench languages. We perform zero-shot cross-lingual classification using five\nlinear transformations combined with LSTM and CNN based classifiers. We compare\nthe performance of the individual transformations, and in addition, we confront\nthe transformation-based approach with existing state-of-the-art BERT-like\nmodels. We show that the pre-trained embeddings from the target domain are\ncrucial to improving the cross-lingual classification results, unlike in the\nmonolingual classification, where the effect is not so distinctive.",
        "pdf_link": "https://arxiv.org/pdf/2209.07244v1.pdf"
    },
    {
        "title": "Learning to Exploit Elastic Actuators for Quadruped Locomotion",
        "authors": [
            "Antonin Raffin",
            "Daniel Seidel",
            "Jens Kober",
            "Alin Albu-Sch\u00e4ffer",
            "Jo\u00e3o Silv\u00e9rio",
            "Freek Stulp"
        ],
        "published": "2022-09-15T09:43:17Z",
        "summary": "Spring-based actuators in legged locomotion provide energy-efficiency and\nimproved performance, but increase the difficulty of controller design. While\nprevious work has focused on extensive modeling and simulation to find optimal\ncontrollers for such systems, we propose to learn model-free controllers\ndirectly on the real robot. In our approach, gaits are first synthesized by\ncentral pattern generators (CPGs), whose parameters are optimized to quickly\nobtain an open-loop controller that achieves efficient locomotion. Then, to\nmake this controller more robust and further improve the performance, we use\nreinforcement learning to close the loop, to learn corrective actions on top of\nthe CPGs. We evaluate the proposed approach on the DLR elastic quadruped bert.\nOur results in learning trotting and pronking gaits show that exploitation of\nthe spring actuator dynamics emerges naturally from optimizing for dynamic\nmotions, yielding high-performing locomotion, particularly the fastest walking\ngait recorded on bert, despite being model-free. The whole process takes no\nmore than 1.5 hours on the real robot and results in natural-looking gaits.",
        "pdf_link": "https://arxiv.org/pdf/2209.07171v3.pdf"
    },
    {
        "title": "PTab: Using the Pre-trained Language Model for Modeling Tabular Data",
        "authors": [
            "Guang Liu",
            "Jie Yang",
            "Ledell Wu"
        ],
        "published": "2022-09-15T08:58:42Z",
        "summary": "Tabular data is the foundation of the information age and has been\nextensively studied. Recent studies show that neural-based models are effective\nin learning contextual representation for tabular data. The learning of an\neffective contextual representation requires meaningful features and a large\namount of data. However, current methods often fail to properly learn a\ncontextual representation from the features without semantic information. In\naddition, it's intractable to enlarge the training set through mixed tabular\ndatasets due to the difference between datasets. To address these problems, we\npropose a novel framework PTab, using the Pre-trained language model to model\nTabular data. PTab learns a contextual representation of tabular data through a\nthree-stage processing: Modality Transformation(MT), Masked-Language\nFine-tuning(MF), and Classification Fine-tuning(CF). We initialize our model\nwith a pre-trained Model (PTM) which contains semantic information learned from\nthe large-scale language data. Consequently, contextual representation can be\nlearned effectively during the fine-tuning stages. In addition, we can\nnaturally mix the textualized tabular data to enlarge the training set to\nfurther improve representation learning. We evaluate PTab on eight popular\ntabular classification datasets. Experimental results show that our method has\nachieved a better average AUC score in supervised settings compared to the\nstate-of-the-art baselines(e.g. XGBoost), and outperforms counterpart methods\nunder semi-supervised settings. We present visualization results that show PTab\nhas well instance-based interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2209.08060v1.pdf"
    },
    {
        "title": "COOL-MC: A Comprehensive Tool for Reinforcement Learning and Model Checking",
        "authors": [
            "Dennis Gross",
            "Nils Jansen",
            "Sebastian Junges",
            "Guillermo A. Perez"
        ],
        "published": "2022-09-15T08:25:43Z",
        "summary": "This paper presents COOL-MC, a tool that integrates state-of-the-art\nreinforcement learning (RL) and model checking. Specifically, the tool builds\nupon the OpenAI gym and the probabilistic model checker Storm. COOL-MC provides\nthe following features: (1) a simulator to train RL policies in the OpenAI gym\nfor Markov decision processes (MDPs) that are defined as input for Storm, (2) a\nnew model builder for Storm, which uses callback functions to verify (neural\nnetwork) RL policies, (3) formal abstractions that relate models and policies\nspecified in OpenAI gym or Storm, and (4) algorithms to obtain bounds on the\nperformance of so-called permissive policies. We describe the components and\narchitecture of COOL-MC and demonstrate its features on multiple benchmark\nenvironments.",
        "pdf_link": "https://arxiv.org/pdf/2209.07133v1.pdf"
    },
    {
        "title": "uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers",
        "authors": [
            "Piji Li"
        ],
        "published": "2022-09-15T05:57:12Z",
        "summary": "The task of Chinese Spelling Check (CSC) is aiming to detect and correct\nspelling errors that can be found in the text. While manually annotating a\nhigh-quality dataset is expensive and time-consuming, thus the scale of the\ntraining dataset is usually very small (e.g., SIGHAN15 only contains 2339\nsamples for training), therefore supervised-learning based models usually\nsuffer the data sparsity limitation and over-fitting issue, especially in the\nera of big language models. In this paper, we are dedicated to investigating\nthe \\textbf{unsupervised} paradigm to address the CSC problem and we propose a\nframework named \\textbf{uChecker} to conduct unsupervised spelling error\ndetection and correction. Masked pretrained language models such as BERT are\nintroduced as the backbone model considering their powerful language diagnosis\ncapability. Benefiting from the various and flexible MASKing operations, we\npropose a Confusionset-guided masking strategy to fine-train the masked\nlanguage model to further improve the performance of unsupervised detection and\ncorrection. Experimental results on standard datasets demonstrate the\neffectiveness of our proposed model uChecker in terms of character-level and\nsentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling\nerror detection and correction respectively.",
        "pdf_link": "https://arxiv.org/pdf/2209.07068v1.pdf"
    },
    {
        "title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach",
        "authors": [
            "Yue Yu",
            "Rongzhi Zhang",
            "Ran Xu",
            "Jieyu Zhang",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2022-09-15T01:51:22Z",
        "summary": "Large Language Models have demonstrated remarkable few-shot performance, but\nthe performance can be sensitive to the selection of few-shot instances. We\npropose PATRON, a new method that uses prompt-based uncertainty estimation for\ndata selection for pre-trained language model fine-tuning under cold-start\nscenarios, i.e., no initial labeled data are available. In PATRON, we design\n(1) a prompt-based uncertainty propagation approach to estimate the importance\nof data points and (2) a partition-then-rewrite (PTR) strategy to promote\nsample diversity when querying for annotations. Experiments on six text\nclassification datasets show that PATRON outperforms the strongest cold-start\ndata selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON\nachieves 91.0% and 92.1% of the fully supervised performance based on vanilla\nfine-tuning and prompt-based learning respectively. Our implementation of\nPATRON is available at \\url{https://github.com/yueyu1030/Patron}.",
        "pdf_link": "https://arxiv.org/pdf/2209.06995v2.pdf"
    },
    {
        "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
        "authors": [
            "Lisa P. Argyle",
            "Ethan C. Busby",
            "Nancy Fulda",
            "Joshua Gubler",
            "Christopher Rytting",
            "David Wingate"
        ],
        "published": "2022-09-14T19:53:32Z",
        "summary": "We propose and explore the possibility that language models can be studied as\neffective proxies for specific human sub-populations in social science\nresearch. Practical and research applications of artificial intelligence tools\nhave sometimes been limited by problematic biases (such as racism or sexism),\nwhich are often treated as uniform properties of the models. We show that the\n\"algorithmic bias\" within one such tool -- the GPT-3 language model -- is\ninstead both fine-grained and demographically correlated, meaning that proper\nconditioning will cause it to accurately emulate response distributions from a\nwide variety of human subgroups. We term this property \"algorithmic fidelity\"\nand explore its extent in GPT-3. We create \"silicon samples\" by conditioning\nthe model on thousands of socio-demographic backstories from real human\nparticipants in multiple large surveys conducted in the United States. We then\ncompare the silicon and human samples to demonstrate that the information\ncontained in GPT-3 goes far beyond surface similarity. It is nuanced,\nmultifaceted, and reflects the complex interplay between ideas, attitudes, and\nsocio-cultural context that characterize human attitudes. We suggest that\nlanguage models with sufficient algorithmic fidelity thus constitute a novel\nand powerful tool to advance understanding of humans and society across a\nvariety of disciplines.",
        "pdf_link": "https://arxiv.org/pdf/2209.06899v1.pdf"
    },
    {
        "title": "On the State of the Art in Authorship Attribution and Authorship Verification",
        "authors": [
            "Jacob Tyo",
            "Bhuwan Dhingra",
            "Zachary C. Lipton"
        ],
        "published": "2022-09-14T18:32:26Z",
        "summary": "Despite decades of research on authorship attribution (AA) and authorship\nverification (AV), inconsistent dataset splits/filtering and mismatched\nevaluation methods make it difficult to assess the state of the art. In this\npaper, we present a survey of the fields, resolve points of confusion,\nintroduce Valla that standardizes and benchmarks AA/AV datasets and metrics,\nprovide a large-scale empirical evaluation, and provide apples-to-apples\ncomparisons between existing methods. We evaluate eight promising methods on\nfifteen datasets (including distribution-shifted challenge sets) and introduce\na new large-scale dataset based on texts archived by Project Gutenberg.\nSurprisingly, we find that a traditional Ngram-based model performs best on 5\n(of 7) AA tasks, achieving an average macro-accuracy of $76.50\\%$ (compared to\n$66.71\\%$ for a BERT-based model). However, on the two AA datasets with the\ngreatest number of words per author, as well as on the AV datasets, BERT-based\nmodels perform best. While AV methods are easily applied to AA, they are seldom\nincluded as baselines in AA papers. We show that through the application of\nhard-negative mining, AV methods are competitive alternatives to AA methods.\nValla and all experiment code can be found here:\nhttps://github.com/JacobTyo/Valla",
        "pdf_link": "https://arxiv.org/pdf/2209.06869v2.pdf"
    },
    {
        "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "authors": [
            "Xi Chen",
            "Xiao Wang",
            "Soravit Changpinyo",
            "AJ Piergiovanni",
            "Piotr Padlewski",
            "Daniel Salz",
            "Sebastian Goodman",
            "Adam Grycner",
            "Basil Mustafa",
            "Lucas Beyer",
            "Alexander Kolesnikov",
            "Joan Puigcerver",
            "Nan Ding",
            "Keran Rong",
            "Hassan Akbari",
            "Gaurav Mishra",
            "Linting Xue",
            "Ashish Thapliyal",
            "James Bradbury",
            "Weicheng Kuo",
            "Mojtaba Seyedhosseini",
            "Chao Jia",
            "Burcu Karagol Ayan",
            "Carlos Riquelme",
            "Andreas Steiner",
            "Anelia Angelova",
            "Xiaohua Zhai",
            "Neil Houlsby",
            "Radu Soricut"
        ],
        "published": "2022-09-14T17:24:07Z",
        "summary": "Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.",
        "pdf_link": "https://arxiv.org/pdf/2209.06794v4.pdf"
    },
    {
        "title": "Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing",
        "authors": [
            "Hunter Osterhoudt",
            "Courtney E. Schneider",
            "Haneef A Mohammad",
            "Minmei Shih",
            "Alexandra E. Harper",
            "Leming Zhou",
            "Elizabeth R Skidmore",
            "Yanshan Wang"
        ],
        "published": "2022-09-14T15:33:30Z",
        "summary": "Strategy training is a multidisciplinary rehabilitation approach that teaches\nskills to reduce disability among those with cognitive impairments following a\nstroke. Strategy training has been shown in randomized, controlled clinical\ntrials to be a more feasible and efficacious intervention for promoting\nindependence than traditional rehabilitation approaches. A standardized\nfidelity assessment is used to measure adherence to treatment principles by\nexamining guided and directed verbal cues in video recordings of rehabilitation\nsessions. Although the fidelity assessment for detecting guided and directed\nverbal cues is valid and feasible for single-site studies, it can become labor\nintensive, time consuming, and expensive in large, multi-site pragmatic trials.\nTo address this challenge to widespread strategy training implementation, we\nleveraged natural language processing (NLP) techniques to automate the strategy\ntraining fidelity assessment, i.e., to automatically identify guided and\ndirected verbal cues from video recordings of rehabilitation sessions. We\ndeveloped a rule-based NLP algorithm, a long-short term memory (LSTM) model,\nand a bidirectional encoder representation from transformers (BERT) model for\nthis task. The best performance was achieved by the BERT model with a 0.8075\nF1-score. This BERT model was verified on an external validation dataset\ncollected from a separate major regional health system and achieved an F1 score\nof 0.8259, which shows that the BERT model generalizes well. The findings from\nthis study hold widespread promise in psychology and rehabilitation\nintervention research and practice.",
        "pdf_link": "https://arxiv.org/pdf/2209.06727v2.pdf"
    },
    {
        "title": "Toward Improving Health Literacy in Patient Education Materials with Neural Machine Translation Models",
        "authors": [
            "David Oniani",
            "Sreekanth Sreekumar",
            "Renuk DeAlmeida",
            "Dinuk DeAlmeida",
            "Vivian Hui",
            "Young Ji Lee",
            "Yiye Zhang",
            "Leming Zhou",
            "Yanshan Wang"
        ],
        "published": "2022-09-14T15:30:54Z",
        "summary": "Health literacy is the central focus of Healthy People 2030, the fifth\niteration of the U.S. national goals and objectives. People with low health\nliteracy usually have trouble understanding health information, following\npost-visit instructions, and using prescriptions, which results in worse health\noutcomes and serious health disparities. In this study, we propose to leverage\nnatural language processing techniques to improve health literacy in patient\neducation materials by automatically translating illiterate languages in a\ngiven sentence. We scraped patient education materials from four online health\ninformation websites: MedlinePlus.gov, Drugs.com, Mayoclinic.org and\nReddit.com. We trained and tested the state-of-the-art neural machine\ntranslation (NMT) models on a silver standard training dataset and a gold\nstandard testing dataset, respectively. The experimental results showed that\nthe Bidirectional Long Short-Term Memory (BiLSTM) NMT model outperformed\nBidirectional Encoder Representations from Transformers (BERT)-based NMT\nmodels. We also verified the effectiveness of NMT models in translating health\nilliterate languages by comparing the ratio of health illiterate language in\nthe sentence. The proposed NMT models were able to identify the correct\ncomplicated words and simplify into layman language while at the same time the\nmodels suffer from sentence completeness, fluency, readability, and have\ndifficulty in translating certain medical terms.",
        "pdf_link": "https://arxiv.org/pdf/2209.06723v1.pdf"
    },
    {
        "title": "Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?",
        "authors": [
            "Jiawen Wu",
            "Xinyu Zhang",
            "Yutao Zhu",
            "Zheng Liu",
            "Zikai Guo",
            "Zhaoye Fei",
            "Ruofei Lai",
            "Yongkang Wu",
            "Zhao Cao",
            "Zhicheng Dou"
        ],
        "published": "2022-09-14T12:03:31Z",
        "summary": "Recent years have witnessed great progress on applying pre-trained language\nmodels, e.g., BERT, to information retrieval (IR) tasks. Hyperlinks, which are\ncommonly used in Web pages, have been leveraged for designing pre-training\nobjectives. For example, anchor texts of the hyperlinks have been used for\nsimulating queries, thus constructing tremendous query-document pairs for\npre-training. However, as a bridge across two web pages, the potential of\nhyperlinks has not been fully explored. In this work, we focus on modeling the\nrelationship between two documents that are connected by hyperlinks and\ndesigning a new pre-training objective for ad-hoc retrieval. Specifically, we\ncategorize the relationships between documents into four groups: no link,\nunidirectional link, symmetric link, and the most relevant symmetric link. By\ncomparing two documents sampled from adjacent groups, the model can gradually\nimprove its capability of capturing matching signals. We propose a progressive\nhyperlink predication ({PHP}) framework to explore the utilization of\nhyperlinks in pre-training. Experimental results on two large-scale ad-hoc\nretrieval datasets and six question-answering datasets demonstrate its\nsuperiority over existing pre-training methods.",
        "pdf_link": "https://arxiv.org/pdf/2209.06583v1.pdf"
    },
    {
        "title": "How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation",
        "authors": [
            "Julius Steen",
            "Katja Markert"
        ],
        "published": "2022-09-14T09:42:19Z",
        "summary": "Automatically evaluating the coherence of summaries is of great significance\nboth to enable cost-efficient summarizer evaluation and as a tool for improving\ncoherence by selecting high-scoring candidate summaries. While many different\napproaches have been suggested to model summary coherence, they are often\nevaluated using disparate datasets and metrics. This makes it difficult to\nunderstand their relative performance and identify ways forward towards better\nsummary coherence modelling. In this work, we conduct a large-scale\ninvestigation of various methods for summary coherence modelling on an even\nplaying field. Additionally, we introduce two novel analysis measures,\nintra-system correlation and bias matrices, that help identify biases in\ncoherence measures and provide robustness against system-level confounders.\nWhile none of the currently available automatic coherence measures are able to\nassign reliable coherence scores to system summaries across all evaluation\nmetrics, large-scale language models fine-tuned on self-supervised tasks show\npromising results, as long as fine-tuning takes into account that they need to\ngeneralize across different summary lengths.",
        "pdf_link": "https://arxiv.org/pdf/2209.06517v2.pdf"
    },
    {
        "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
        "authors": [
            "Jiawei Liu",
            "Yangyang Kang",
            "Di Tang",
            "Kaisong Song",
            "Changlong Sun",
            "Xiaofeng Wang",
            "Wei Lu",
            "Xiaozhong Liu"
        ],
        "published": "2022-09-14T09:10:07Z",
        "summary": "Neural text ranking models have witnessed significant advancement and are\nincreasingly being deployed in practice. Unfortunately, they also inherit\nadversarial vulnerabilities of general neural models, which have been detected\nbut remain underexplored by prior studies. Moreover, the inherit adversarial\nvulnerabilities might be leveraged by blackhat SEO to defeat better-protected\nsearch engines. In this study, we propose an imitation adversarial attack on\nblack-box neural passage ranking models. We first show that the target passage\nranking model can be transparentized and imitated by enumerating critical\nqueries/candidates and then train a ranking imitation model. Leveraging the\nranking imitation model, we can elaborately manipulate the ranking results and\ntransfer the manipulation attack to the target ranking model. For this purpose,\nwe propose an innovative gradient-based attack method, empowered by the\npairwise objective function, to generate adversarial triggers, which causes\npremeditated disorderliness with very few tokens. To equip the trigger\ncamouflages, we add the next sentence prediction loss and the language model\nfluency constraint to the objective function. Experimental results on passage\nranking demonstrate the effectiveness of the ranking imitation attack model and\nadversarial triggers against various SOTA neural ranking models. Furthermore,\nvarious mitigation analyses and human evaluation show the effectiveness of\ncamouflages when facing potential mitigation approaches. To motivate other\nscholars to further investigate this novel and important problem, we make the\nexperiment data and code publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2209.06506v2.pdf"
    },
    {
        "title": "BERT-based Ensemble Approaches for Hate Speech Detection",
        "authors": [
            "Khouloud Mnassri",
            "Praboda Rajapaksha",
            "Reza Farahbakhsh",
            "Noel Crespi"
        ],
        "published": "2022-09-14T09:08:24Z",
        "summary": "With the freedom of communication provided in online social media, hate\nspeech has increasingly generated. This leads to cyber conflicts affecting\nsocial life at the individual and national levels. As a result, hateful content\nclassification is becoming increasingly demanded for filtering hate content\nbefore being sent to the social networks. This paper focuses on classifying\nhate speech in social media using multiple deep models that are implemented by\nintegrating recent transformer-based language models such as BERT, and neural\nnetworks. To improve the classification performances, we evaluated with several\nensemble techniques, including soft voting, maximum value, hard voting and\nstacking. We used three publicly available Twitter datasets (Davidson,\nHatEval2019, OLID) that are generated to identify offensive languages. We fused\nall these datasets to generate a single dataset (DHO dataset), which is more\nbalanced across different labels, to perform multi-label classification. Our\nexperiments have been held on Davidson dataset and the DHO corpora. The later\ngave the best overall results, especially F1 macro score, even it required more\nresources (time execution and memory). The experiments have shown good results\nespecially the ensemble models, where stacking gave F1 score of 97% on Davidson\ndataset and aggregating ensembles 77% on the DHO dataset.",
        "pdf_link": "https://arxiv.org/pdf/2209.06505v2.pdf"
    },
    {
        "title": "PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization",
        "authors": [
            "Shreyas Fadnavis",
            "Amit Dhurandhar",
            "Raquel Norel",
            "Jenna M Reinen",
            "Carla Agurto",
            "Erica Secchettin",
            "Vittorio Schweiger",
            "Giovanni Perini",
            "Guillermo Cecchi"
        ],
        "published": "2022-09-14T06:08:13Z",
        "summary": "Chronic pain is a pervasive disorder which is often very disabling and is\nassociated with comorbidities such as depression and anxiety. Neuropathic Pain\n(NP) is a common sub-type which is often caused due to nerve damage and has a\nknown pathophysiology. Another common sub-type is Fibromyalgia (FM) which is\ndescribed as musculoskeletal, diffuse pain that is widespread through the body.\nThe pathophysiology of FM is poorly understood, making it very hard to\ndiagnose. Standard medications and treatments for FM and NP differ from one\nanother and if misdiagnosed it can cause an increase in symptom severity. To\novercome this difficulty, we propose a novel framework, PainPoints, which\naccurately detects the sub-type of pain and generates clinical notes via\nsummarizing the patient interviews. Specifically, PainPoints makes use of large\nlanguage models to perform sentence-level classification of the text obtained\nfrom interviews of FM and NP patients with a reliable AUC of 0.83. Using a\nsufficiency-based interpretability approach, we explain how the fine-tuned\nmodel accurately picks up on the nuances that patients use to describe their\npain. Finally, we generate summaries of these interviews via expert\ninterventions by introducing a novel facet-based approach. PainPoints thus\nenables practitioners to add/drop facets and generate a custom summary based on\nthe notion of \"facet-coverage\" which is also introduced in this work.",
        "pdf_link": "https://arxiv.org/pdf/2209.09814v1.pdf"
    },
    {
        "title": "CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification",
        "authors": [
            "Charaf Eddine Benarab",
            "Shenglin Gui"
        ],
        "published": "2022-09-13T23:23:08Z",
        "summary": "BERT achieves remarkable results in text classification tasks, it is yet not\nfully exploited, since only the last layer is used as a representation output\nfor downstream classifiers. The most recent studies on the nature of linguistic\nfeatures learned by BERT, suggest that different layers focus on different\nkinds of linguistic features. We propose a CNN-Enhanced Transformer-Encoder\nmodel which is trained on top of fixed BERT $[CLS]$ representations from all\nlayers, employing Convolutional Neural Networks to generate QKV feature maps\ninside the Transformer-Encoder, instead of linear projections of the input into\nthe embedding space. CNN-Trans-Enc is relatively small as a downstream\nclassifier and doesn't require any fine-tuning of BERT, as it ensures an\noptimal use of the $[CLS]$ representations from all layers, leveraging\ndifferent linguistic features with more meaningful, and generalizable QKV\nrepresentations of the input. Using BERT with CNN-Trans-Enc keeps $98.9\\%$ and\n$94.8\\%$ of current state-of-the-art performance on the IMDB and SST-5 datasets\nrespectably, while obtaining new state-of-the-art on YELP-5 with $82.23$\n($8.9\\%$ improvement), and on Amazon-Polarity with $0.98\\%$ ($0.2\\%$\nimprovement) (K-fold Cross Validation on a 1M sample subset from both\ndatasets). On the AG news dataset CNN-Trans-Enc achieves $99.94\\%$ of the\ncurrent state-of-the-art, and achieves a new top performance with an average\naccuracy of $99.51\\%$ on DBPedia-14.\n  Index terms: Text Classification, Natural Language Processing, Convolutional\nNeural Networks, Transformers, BERT",
        "pdf_link": "https://arxiv.org/pdf/2209.06344v1.pdf"
    },
    {
        "title": "Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest",
        "authors": [
            "Jack Hessel",
            "Ana Marasovi\u0107",
            "Jena D. Hwang",
            "Lillian Lee",
            "Jeff Da",
            "Rowan Zellers",
            "Robert Mankoff",
            "Yejin Choi"
        ],
        "published": "2022-09-13T20:54:00Z",
        "summary": "Large neural networks can now generate jokes, but do they really \"understand\"\nhumor? We challenge AI models with three tasks derived from the New Yorker\nCartoon Caption Contest: matching a joke to a cartoon, identifying a winning\ncaption, and explaining why a winning caption is funny. These tasks encapsulate\nprogressively more sophisticated aspects of \"understanding\" a cartoon; key\nelements are the complex, often surprising relationships between images and\ncaptions and the frequent inclusion of indirect and playful allusions to human\nexperience and culture. We investigate both multimodal and language-only\nmodels: the former are challenged with the cartoon images directly, while the\nlatter are given multifaceted descriptions of the visual scene to simulate\nhuman-level visual understanding. We find that both types of models struggle at\nall three tasks. For example, our best multimodal models fall 30 accuracy\npoints behind human performance on the matching task, and, even when provided\nground-truth visual scene descriptors, human-authored explanations are\npreferred head-to-head over the best machine-authored ones (few-shot GPT-4) in\nmore than 2/3 of cases. We release models, code, leaderboard, and corpus, which\nincludes newly-gathered annotations describing the image's locations/entities,\nwhat's unusual in the scene, and an explanation of the joke.",
        "pdf_link": "https://arxiv.org/pdf/2209.06293v2.pdf"
    },
    {
        "title": "Exploring Code Style Transfer with Neural Networks",
        "authors": [
            "Karl Munson",
            "Anish Savla",
            "Chih-Kai Ting",
            "Serenity Wade",
            "Kiran Kate",
            "Kavitha Srinivas"
        ],
        "published": "2022-09-13T19:34:42Z",
        "summary": "Style is a significant component of natural language text, reflecting a\nchange in the tone of text while keeping the underlying information the same.\nEven though programming languages have strict syntax rules, they also have\nstyle. Code can be written with the same functionality but using different\nlanguage features. However, programming style is difficult to quantify, and\nthus as part of this work, we define style attributes, specifically for Python.\nTo build a definition of style, we utilized hierarchical clustering to capture\na style definition without needing to specify transformations. In addition to\ndefining style, we explore the capability of a pre-trained code language model\nto capture information about code style. To do this, we fine-tuned pre-trained\ncode-language models and evaluated their performance in code style transfer\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2209.06273v1.pdf"
    },
    {
        "title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-gram Language Models",
        "authors": [
            "Mohammed Rakib",
            "Md. Ismail Hossain",
            "Nabeel Mohammed",
            "Fuad Rahman"
        ],
        "published": "2022-09-13T17:59:21Z",
        "summary": "Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.",
        "pdf_link": "https://arxiv.org/pdf/2209.12650v1.pdf"
    },
    {
        "title": "Improving Language Model Prompting in Support of Semi-autonomous Task Learning",
        "authors": [
            "James R. Kirk",
            "Robert E. Wray",
            "Peter Lindes",
            "John E. Laird"
        ],
        "published": "2022-09-13T15:36:01Z",
        "summary": "Language models (LLMs) offer potential as a source of knowledge for agents\nthat need to acquire new task competencies within a performance environment. We\ndescribe efforts toward a novel agent capability that can construct cues (or\n\"prompts\") that result in useful LLM responses for an agent learning a new\ntask. Importantly, responses must not only be \"reasonable\" (a measure used\ncommonly in research on knowledge extraction from LLMs) but also specific to\nthe agent's task context and in a form that the agent can interpret given its\nnative language capacities. We summarize a series of empirical investigations\nof prompting strategies and evaluate responses against the goals of targeted\nand actionable responses for task learning. Our results demonstrate that\nactionable task knowledge can be obtained from LLMs in support of online agent\ntask learning.",
        "pdf_link": "https://arxiv.org/pdf/2209.07636v2.pdf"
    },
    {
        "title": "Don't Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling",
        "authors": [
            "Dongsuk Oh",
            "Yejin Kim",
            "Hodong Lee",
            "H. Howie Huang",
            "Heuiseok Lim"
        ],
        "published": "2022-09-13T13:09:49Z",
        "summary": "Recent pre-trained language models (PLMs) achieved great success on many\nnatural language processing tasks through learning linguistic features and\ncontextualized sentence representation. Since attributes captured in stacked\nlayers of PLMs are not clearly identified, straightforward approaches such as\nembedding the last layer are commonly preferred to derive sentence\nrepresentations from PLMs. This paper introduces the attention-based pooling\nstrategy, which enables the model to preserve layer-wise signals captured in\neach layer and learn digested linguistic features for downstream tasks. The\ncontrastive learning objective can adapt the layer-wise attention pooling to\nboth unsupervised and supervised manners. It results in regularizing the\nanisotropic space of pre-trained embeddings and being more uniform. We evaluate\nour model on standard semantic textual similarity (STS) and semantic search\ntasks. As a result, our method improved the performance of the base contrastive\nlearned BERT_base and variants.",
        "pdf_link": "https://arxiv.org/pdf/2209.05972v1.pdf"
    },
    {
        "title": "SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus",
        "authors": [
            "Yufeng Zhao",
            "Haiying Che"
        ],
        "published": "2022-09-13T05:49:10Z",
        "summary": "BERT is a widely used pre-trained model in natural language processing.\nHowever, since BERT is quadratic to the text length, the BERT model is\ndifficult to be used directly on the long-text corpus. In some fields, the\ncollected text data may be quite long, such as in the health care field.\nTherefore, to apply the pre-trained language knowledge of BERT to long text, in\nthis paper, imitating the skimming-intensive reading method used by humans when\nreading a long paragraph, the Skimming-Intensive Model (SkIn) is proposed. It\ncan dynamically select the critical information in the text so that the\nsentence input into the BERT-Base model is significantly shortened, which can\neffectively save the cost of the classification algorithm. Experiments show\nthat the SkIn method has achieved superior accuracy than the baselines on\nlong-text classification datasets in the medical field, while its time and\nspace requirements increase linearly with the text length, alleviating the time\nand space overflow problem of basic BERT on long-text data.",
        "pdf_link": "https://arxiv.org/pdf/2209.05741v2.pdf"
    },
    {
        "title": "Robin: A Novel Online Suicidal Text Corpus of Substantial Breadth and Scale",
        "authors": [
            "Daniel DiPietro",
            "Vivek Hazari",
            "Soroush Vosoughi"
        ],
        "published": "2022-09-13T03:32:47Z",
        "summary": "Suicide is a major public health crisis. With more than 20,000,000 suicide\nattempts each year, the early detection of suicidal intent has the potential to\nsave hundreds of thousands of lives. Traditional mental health screening\nmethods are time-consuming, costly, and often inaccessible to disadvantaged\npopulations; online detection of suicidal intent using machine learning offers\na viable alternative. Here we present Robin, the largest non-keyword generated\nsuicidal corpus to date, consisting of over 1.1 million online forum postings.\nIn addition to its unprecedented size, Robin is specially constructed to\ninclude various categories of suicidal text, such as suicide bereavement and\nflippant references, better enabling models trained on Robin to learn the\nsubtle nuances of text expressing suicidal ideation. Experimental results\nachieve state-of-the-art performance for the classification of suicidal text,\nboth with traditional methods like logistic regression (F1=0.85), as well as\nwith large-scale pre-trained language models like BERT (F1=0.92). Finally, we\nrelease the Robin dataset publicly as a machine learning resource with the\npotential to drive the next generation of suicidal sentiment research.",
        "pdf_link": "https://arxiv.org/pdf/2209.05707v1.pdf"
    },
    {
        "title": "Unified State Representation Learning under Data Augmentation",
        "authors": [
            "Taylor Hearn",
            "Sravan Jayanthi",
            "Sehoon Ha"
        ],
        "published": "2022-09-12T15:10:28Z",
        "summary": "The capacity for rapid domain adaptation is important to increasing the\napplicability of reinforcement learning (RL) to real world problems.\nGeneralization of RL agents is critical to success in the real world, yet\nzero-shot policy transfer is a challenging problem since even minor visual\nchanges could make the trained agent completely fail in the new task. We\npropose USRA: Unified State Representation Learning under Data Augmentation, a\nrepresentation learning framework that learns a latent unified state\nrepresentation by performing data augmentations on its observations to improve\nits ability to generalize to unseen target domains. We showcase the success of\nour approach on the DeepMind Control Generalization Benchmark for the Walker\nenvironment and find that USRA achieves higher sample efficiency and 14.3%\nbetter domain adaptation performance compared to the best baseline results.",
        "pdf_link": "https://arxiv.org/pdf/2209.05302v1.pdf"
    },
    {
        "title": "DECK: Behavioral Tests to Improve Interpretability and Generalizability of BERT Models Detecting Depression from Text",
        "authors": [
            "Jekaterina Novikova",
            "Ksenia Shkaruta"
        ],
        "published": "2022-09-12T14:39:46Z",
        "summary": "Models that accurately detect depression from text are important tools for\naddressing the post-pandemic mental health crisis. BERT-based classifiers'\npromising performance and the off-the-shelf availability make them great\ncandidates for this task. However, these models are known to suffer from\nperformance inconsistencies and poor generalization. In this paper, we\nintroduce the DECK (DEpression ChecKlist), depression-specific model\nbehavioural tests that allow better interpretability and improve\ngeneralizability of BERT classifiers in depression domain. We create 23 tests\nto evaluate BERT, RoBERTa and ALBERT depression classifiers on three datasets,\ntwo Twitter-based and one clinical interview-based. Our evaluation shows that\nthese models: 1) are robust to certain gender-sensitive variations in text; 2)\nrely on the important depressive language marker of the increased use of first\nperson pronouns; 3) fail to detect some other depression symptoms like suicidal\nideation. We also demonstrate that DECK tests can be used to incorporate\nsymptom-specific information in the training data and consistently improve\ngeneralizability of all three BERT models, with an out-of-distribution F1-score\nincrease of up to 53.93%.",
        "pdf_link": "https://arxiv.org/pdf/2209.05286v1.pdf"
    },
    {
        "title": "A new hazard event classification model via deep learning and multifractal",
        "authors": [
            "Zhenhua Wang",
            "Bin Wang",
            "Ming Ren",
            "Dong Gao"
        ],
        "published": "2022-09-12T14:13:13Z",
        "summary": "Hazard and operability analysis (HAZOP) is the paradigm of industrial safety\nthat can reveal the hazards of process from its node deviations, consequences,\ncauses, measures and suggestions, and such hazards can be considered as hazard\nevents (HaE). The classification research on HaE has much irreplaceable\npragmatic values. In this paper, we present a novel deep learning model termed\nDLF through multifractal to explore HaE classification where the motivation is\nthat HaE can be naturally regarded as a kind of time series. Specifically,\nfirst HaE is vectorized to get HaE time series by employing BERT. Then, a new\nmultifractal analysis method termed HmF-DFA is proposed to win HaE fractal\nseries by analyzing HaE time series. Finally, a new hierarchical gating neural\nnetwork (HGNN) is designed to process HaE fractal series to accomplish the\nclassification of HaE from three aspects: severity, possibility and risk. We\ntake HAZOP reports of 18 processes as cases, and launch the experiments on this\nbasis. Results demonstrate that compared with other classifiers, DLF classifier\nperforms better under metrics of precision, recall and F1-score, especially for\nthe severity aspect. Also, HmF-DFA and HGNN effectively promote HaE\nclassification. Our HaE classification system can serve application incentives\nto experts, engineers, employees, and other enterprises. We hope our research\ncan contribute added support to the daily practice in industrial safety.",
        "pdf_link": "https://arxiv.org/pdf/2209.05263v2.pdf"
    },
    {
        "title": "Open-Domain Dialog Evaluation using Follow-Ups Likelihood",
        "authors": [
            "Maxime De Bruyn",
            "Ehsan Lotfi",
            "Jeska Buhmann",
            "Walter Daelemans"
        ],
        "published": "2022-09-12T12:22:31Z",
        "summary": "Automatic evaluation of open-domain dialogs remains an unsolved problem.\nMoreover, existing methods do not correlate strongly with human annotations.\nThis paper presents a new automated evaluation method using follow-ups: we\nmeasure the probability that a language model will continue the conversation\nwith a fixed set of follow-ups (e.g., not really relevant here, what are you\ntrying to say). When compared against twelve existing methods, our new\nevaluation achieves the highest correlation with human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2209.05185v1.pdf"
    },
    {
        "title": "Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset",
        "authors": [
            "H. A. Z. Sameen Shahgir",
            "Khondker Salman Sayeed",
            "Tanjeem Azwad Zaman"
        ],
        "published": "2022-09-11T15:05:42Z",
        "summary": "Speech is inherently continuous, where discrete words, phonemes and other\nunits are not clearly segmented, and so speech recognition has been an active\nresearch problem for decades. In this work we have fine-tuned wav2vec 2.0 to\nrecognize and transcribe Bengali speech -- training it on the Bengali Common\nVoice Speech Dataset. After training for 71 epochs, on a training set\nconsisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of\n0.2524 on a validation set of size 7,747. Using a 5-gram language model, the\nLevenshtein Distance was 2.6446 on a test set of size 7,747. Then the training\nset and validation set were combined, shuffled and split into 85-15 ratio.\nTraining for 7 more epochs on this combined dataset yielded an improved\nLevenshtein Distance of 2.60753 on the test set. Our model was the best\nperforming one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,\nwhich was 1.1049 units lower than other competing submissions.",
        "pdf_link": "https://arxiv.org/pdf/2209.06581v1.pdf"
    },
    {
        "title": "Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models",
        "authors": [
            "David K. Yi",
            "James V. Bruno",
            "Jiayu Han",
            "Peter Zukerman",
            "Shane Steinert-Threlkeld"
        ],
        "published": "2022-09-11T08:04:40Z",
        "summary": "We investigate the extent to which verb alternation classes, as described by\nLevin (1993), are encoded in the embeddings of Large Pre-trained Language\nModels (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively\nconstructed diagnostic classifiers for word and sentence-level prediction\ntasks. We follow and expand upon the experiments of Kann et al. (2019), which\naim to probe whether static embeddings encode frame-selectional properties of\nverbs. At both the word and sentence level, we find that contextual embeddings\nfrom PLMs not only outperform non-contextual embeddings, but achieve\nastonishingly high accuracies on tasks across most alternation classes.\nAdditionally, we find evidence that the middle-to-upper layers of PLMs achieve\nbetter performance on average than the lower layers across all probing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2209.04811v1.pdf"
    },
    {
        "title": "OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue",
        "authors": [
            "Zhi Chen",
            "Yuncong Liu",
            "Lu Chen",
            "Su Zhu",
            "Mengyue Wu",
            "Kai Yu"
        ],
        "published": "2022-09-10T04:38:27Z",
        "summary": "This paper presents an ontology-aware pretrained language model (OPAL) for\nend-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,\ntask-oriented dialogue models fulfill at least two task-specific modules:\ndialogue state tracker (DST) and response generator (RG). The dialogue state\nconsists of the domain-slot-value triples, which are regarded as the user's\nconstraints to search the domain-related databases. The large-scale\ntask-oriented dialogue data with the annotated structured dialogue state\nusually are inaccessible. It prevents the development of the pretrained\nlanguage model for the task-oriented dialogue. We propose a simple yet\neffective pretraining method to alleviate this problem, which consists of two\npretraining phases. The first phase is to pretrain on large-scale contextual\ntext data, where the structured information of the text is extracted by the\ninformation extracting tool. To bridge the gap between the pretraining method\nand downstream tasks, we design two pretraining tasks: ontology-like triple\nrecovery and next-text generation, which simulates the DST and RG,\nrespectively. The second phase is to fine-tune the pretrained model on the TOD\ndata. The experimental results show that our proposed method achieves an\nexciting boost and get competitive performance even without any TOD data on\nCamRest676 and MultiWOZ benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2209.04595v1.pdf"
    },
    {
        "title": "Yes, DLGM! A novel hierarchical model for hazard classification",
        "authors": [
            "Zhenhua Wang",
            "Ming Ren",
            "Dong Gao",
            "Bin Wang"
        ],
        "published": "2022-09-10T02:45:59Z",
        "summary": "Hazards can be exposed by HAZOP as text information, and studying their\nclassification is of great significance to the development of industrial\ninformatics, which is conducive to safety early warning, decision support,\npolicy evaluation, etc. However, there is no research on this important field\nat present. In this paper, we propose a novel model termed DLGM via deep\nlearning for hazard classification. Specifically, first, we leverage BERT to\nvectorize the hazard and treat it as a type of time series (HTS). Secondly, we\nbuild a grey model FSGM(1, 1) to model it, and get the grey guidance in the\nsense of the structural parameters. Finally, we design a hierarchical-feature\nfusion neural network (HFFNN) to investigate the HTS with grey guidance (HTSGG)\nfrom three themes, where, HFFNN is a hierarchical structure with four types of\nmodules: two feature encoders, a gating mechanism, and a deepening mechanism.\nWe take 18 industrial processes as application cases and launch a series of\nexperiments. The experimental results prove that DLGM has promising aptitudes\nfor hazard classification and that FSGM(1, 1) and HFFNN are effective. We hope\nour research can contribute added value and support to the daily practice in\nindustrial safety.",
        "pdf_link": "https://arxiv.org/pdf/2209.04576v1.pdf"
    },
    {
        "title": "Trigger Warnings: Bootstrapping a Violence Detector for FanFiction",
        "authors": [
            "Magdalena Wolska",
            "Christopher Schr\u00f6der",
            "Ole Borchardt",
            "Benno Stein",
            "Martin Potthast"
        ],
        "published": "2022-09-09T17:27:03Z",
        "summary": "We present the first dataset and evaluation results on a newly defined\ncomputational task of trigger warning assignment. Labeled corpus data has been\ncompiled from narrative works hosted on Archive of Our Own (AO3), a well-known\nfanfiction site. In this paper, we focus on the most frequently assigned\ntrigger type--violence--and define a document-level binary classification task\nof whether or not to assign a violence trigger warning to a fanfiction,\nexploiting warning labels provided by AO3 authors. SVM and BERT models trained\nin four evaluation setups on the corpora we compiled yield $F_1$ results\nranging from 0.585 to 0.798, proving the violence trigger warning assignment to\nbe a doable, however, non-trivial task.",
        "pdf_link": "https://arxiv.org/pdf/2209.04409v1.pdf"
    },
    {
        "title": "T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition",
        "authors": [
            "Asahi Ushio",
            "Jose Camacho-Collados"
        ],
        "published": "2022-09-09T15:00:38Z",
        "summary": "Language model (LM) pretraining has led to consistent improvements in many\nNLP downstream tasks, including named entity recognition (NER). In this paper,\nwe present T-NER (Transformer-based Named Entity Recognition), a Python library\nfor NER LM finetuning. In addition to its practical utility, T-NER facilitates\nthe study and investigation of the cross-domain and cross-lingual\ngeneralization ability of LMs finetuned on NER. Our library also provides a web\napp where users can get model predictions interactively for arbitrary text,\nwhich facilitates qualitative model evaluation for non-expert programmers. We\nshow the potential of the library by compiling nine public NER datasets into a\nunified format and evaluating the cross-domain and cross-lingual performance\nacross the datasets. The results from our initial experiments show that\nin-domain performance is generally competitive across datasets. However,\ncross-domain generalization is challenging even with a large pretrained LM,\nwhich has nevertheless capacity to learn domain-specific features if fine-tuned\non a combined dataset. To facilitate future research, we also release all our\nLM checkpoints via the Hugging Face model hub.",
        "pdf_link": "https://arxiv.org/pdf/2209.12616v1.pdf"
    },
    {
        "title": "Automatic Readability Assessment of German Sentences with Transformer Ensembles",
        "authors": [
            "Patrick Gustav Blaneck",
            "Tobias Bornheim",
            "Niklas Grieger",
            "Stephan Bialonski"
        ],
        "published": "2022-09-09T13:47:55Z",
        "summary": "Reliable methods for automatic readability assessment have the potential to\nimpact a variety of fields, ranging from machine translation to self-informed\nlearning. Recently, large language models for the German language (such as\nGBERT and GPT-2-Wechsel) have become available, allowing to develop Deep\nLearning based approaches that promise to further improve automatic readability\nassessment. In this contribution, we studied the ability of ensembles of\nfine-tuned GBERT and GPT-2-Wechsel models to reliably predict the readability\nof German sentences. We combined these models with linguistic features and\ninvestigated the dependence of prediction performance on ensemble size and\ncomposition. Mixed ensembles of GBERT and GPT-2-Wechsel performed better than\nensembles of the same size consisting of only GBERT or GPT-2-Wechsel models.\nOur models were evaluated in the GermEval 2022 Shared Task on Text Complexity\nAssessment on data of German sentences. On out-of-sample data, our best\nensemble achieved a root mean squared error of 0.435.",
        "pdf_link": "https://arxiv.org/pdf/2209.04299v1.pdf"
    },
    {
        "title": "EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography",
        "authors": [
            "Rand Muhtaseb",
            "Mohammad Yaqub"
        ],
        "published": "2022-09-09T11:01:59Z",
        "summary": "Learning spatiotemporal features is an important task for efficient video\nunderstanding especially in medical images such as echocardiograms.\nConvolutional neural networks (CNNs) and more recent vision transformers (ViTs)\nare the most commonly used methods with limitations per each. CNNs are good at\ncapturing local context but fail to learn global information across video\nframes. On the other hand, vision transformers can incorporate global details\nand long sequences but are computationally expensive and typically require more\ndata to train. In this paper, we propose a method that addresses the\nlimitations we typically face when training on medical video data such as\nechocardiographic scans. The algorithm we propose (EchoCoTr) utilizes the\nstrength of vision transformers and CNNs to tackle the problem of estimating\nthe left ventricular ejection fraction (LVEF) on ultrasound videos. We\ndemonstrate how the proposed method outperforms state-of-the-art work to-date\non the EchoNet-Dynamic dataset with MAE of 3.95 and $R^2$ of 0.82. These\nresults show noticeable improvement compared to all published research. In\naddition, we show extensive ablations and comparisons with several algorithms,\nincluding ViT and BERT. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/EchoCoTr.",
        "pdf_link": "https://arxiv.org/pdf/2209.04242v1.pdf"
    },
    {
        "title": "MaxMatch-Dropout: Subword Regularization for WordPiece",
        "authors": [
            "Tatsuya Hiraoka"
        ],
        "published": "2022-09-09T05:41:26Z",
        "summary": "We present a subword regularization method for WordPiece, which uses a\nmaximum matching algorithm for tokenization. The proposed method,\nMaxMatch-Dropout, randomly drops words in a search using the maximum matching\nalgorithm. It realizes finetuning with subword regularization for popular\npretrained language models such as BERT-base. The experimental results\ndemonstrate that MaxMatch-Dropout improves the performance of text\nclassification and machine translation tasks as well as other subword\nregularization methods. Moreover, we provide a comparative analysis of subword\nregularization methods: subword regularization with SentencePiece (Unigram),\nBPE-Dropout, and MaxMatch-Dropout.",
        "pdf_link": "https://arxiv.org/pdf/2209.04126v1.pdf"
    },
    {
        "title": "PoxVerifi: An Information Verification System to Combat Monkeypox Misinformation",
        "authors": [
            "Akaash Kolluri",
            "Kami Vinton",
            "Dhiraj Murthy"
        ],
        "published": "2022-09-09T02:50:47Z",
        "summary": "Following recent outbreaks, monkeypox-related misinformation continues to\nrapidly spread online. This negatively impacts response strategies and\ndisproportionately harms LGBTQ+ communities in the short-term, and ultimately\nundermines the overall effectiveness of public health responses. In an attempt\nto combat monkeypox-related misinformation, we present PoxVerifi, an\nopen-source, extensible tool that provides a comprehensive approach to\nassessing the accuracy of monkeypox related claims. Leveraging information from\nexisting fact checking sources and published World Health Organization (WHO)\ninformation, we created an open-sourced corpus of 225 rated monkeypox claims.\nAdditionally, we trained an open-sourced BERT-based machine learning model for\nspecifically classifying monkeypox information, which achieved 96%\ncross-validation accuracy. PoxVerifi is a Google Chrome browser extension\ndesigned to empower users to navigate through monkeypox-related misinformation.\nSpecifically, PoxVerifi provides users with a comprehensive toolkit to assess\nthe veracity of headlines on any webpage across the Internet without having to\nvisit an external site. Users can view an automated accuracy review from our\ntrained machine learning model, a user-generated accuracy review based on\ncommunity-member votes, and have the ability to see similar, vetted, claims.\nBesides PoxVerifi's comprehensive approach to claim-testing, our platform\nprovides an efficient and accessible method to crowdsource accuracy ratings on\nmonkeypox related-claims, which can be aggregated to create new labeled\nmisinformation datasets.",
        "pdf_link": "https://arxiv.org/pdf/2209.09300v1.pdf"
    },
    {
        "title": "Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM",
        "authors": [
            "Hayato Futami",
            "Hirofumi Inaguma",
            "Sei Ueno",
            "Masato Mimura",
            "Shinsuke Sakai",
            "Tatsuya Kawahara"
        ],
        "published": "2022-09-08T23:42:37Z",
        "summary": "Connectionist temporal classification (CTC) -based models are attractive in\nautomatic speech recognition (ASR) because of their non-autoregressive nature.\nTo take advantage of text-only data, language model (LM) integration approaches\nsuch as rescoring and shallow fusion have been widely used for CTC. However,\nthey lose CTC's non-autoregressive nature because of the need for beam search,\nwhich slows down the inference speed. In this study, we propose an error\ncorrection method with phone-conditioned masked LM (PC-MLM). In the proposed\nmethod, less confident word tokens in a greedy decoded output from CTC are\nmasked. PC-MLM then predicts these masked word tokens given unmasked words and\nphones supplementally predicted from CTC. We further extend it to Deletable\nPC-MLM in order to address insertion errors. Since both CTC and PC-MLM are\nnon-autoregressive models, the method enables fast LM integration. Experimental\nevaluations on the Corpus of Spontaneous Japanese (CSJ) and TED-LIUM2 in domain\nadaptation setting shows that our proposed method outperformed rescoring and\nshallow fusion in terms of inference speed, and also in terms of recognition\naccuracy on CSJ.",
        "pdf_link": "https://arxiv.org/pdf/2209.04062v1.pdf"
    },
    {
        "title": "Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages",
        "authors": [
            "Li Miao",
            "Jian Wu",
            "Piyush Behre",
            "Shuangyu Chang",
            "Sarangarajan Parthasarathy"
        ],
        "published": "2022-09-08T21:40:41Z",
        "summary": "It is challenging to train and deploy Transformer LMs for hybrid speech\nrecognition 2nd pass re-ranking in low-resource languages due to (1) data\nscarcity in low-resource languages, (2) expensive computing costs for training\nand refreshing 100+ monolingual models, and (3) hosting inefficiency\nconsidering sparse traffic. In this study, we present a new way to group\nmultiple low-resource locales together and optimize the performance of\nMultilingual Transformer LMs in ASR. Our Locale-group Multilingual Transformer\nLMs outperform traditional multilingual LMs along with reducing maintenance\ncosts and operating expenses. Further, for low-resource but high-traffic\nlocales where deploying monolingual models is feasible, we show that\nfine-tuning our locale-group multilingual LMs produces better monolingual LM\ncandidates than baseline monolingual LMs.",
        "pdf_link": "https://arxiv.org/pdf/2209.04041v1.pdf"
    },
    {
        "title": "IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-Signal Triplets via Pre-trained Autoregressive Language Model",
        "authors": [
            "Martin Fajcik",
            "Muskaan Singh",
            "Juan Zuluaga-Gomez",
            "Esa\u00fa Villatoro-Tello",
            "Sergio Burdisso",
            "Petr Motlicek",
            "Pavel Smrz"
        ],
        "published": "2022-09-08T15:54:25Z",
        "summary": "In this paper, we describe our shared task submissions for Subtask 2 in\nCASE-2022, Event Causality Identification with Casual News Corpus. The\nchallenge focused on the automatic detection of all cause-effect-signal spans\npresent in the sentence from news-media. We detect cause-effect-signal spans in\na sentence using T5 -- a pre-trained autoregressive language model. We\niteratively identify all cause-effect-signal span triplets, always conditioning\nthe prediction of the next triplet on the previously predicted ones. To predict\nthe triplet itself, we consider different causal relationships such as\ncause$\\rightarrow$effect$\\rightarrow$signal. Each triplet component is\ngenerated via a language model conditioned on the sentence, the previous parts\nof the current triplet, and previously predicted triplets. Despite training on\nan extremely small dataset of 160 samples, our approach achieved competitive\nperformance, being placed second in the competition. Furthermore, we show that\nassuming either cause$\\rightarrow$effect or effect$\\rightarrow$cause order\nachieves similar results.",
        "pdf_link": "https://arxiv.org/pdf/2209.03891v2.pdf"
    },
    {
        "title": "Pre-Training a Graph Recurrent Network for Language Representation",
        "authors": [
            "Yile Wang",
            "Linyi Yang",
            "Zhiyang Teng",
            "Ming Zhou",
            "Yue Zhang"
        ],
        "published": "2022-09-08T14:12:15Z",
        "summary": "Transformer-based pre-trained models have gained much advance in recent\nyears, becoming one of the most important backbones in natural language\nprocessing. Recent work shows that the attention mechanism inside Transformer\nmay not be necessary, both convolutional neural networks and multi-layer\nperceptron based models have also been investigated as Transformer\nalternatives. In this paper, we consider a graph recurrent network for language\nmodel pre-training, which builds a graph structure for each sequence with local\ntoken-level communications, together with a sentence-level representation\ndecoupled from other tokens. The original model performs well in\ndomain-specific text classification under supervised training, however, its\npotential in learning transfer knowledge by self-supervised way has not been\nfully exploited. We fill this gap by optimizing the architecture and verifying\nits effectiveness in more general language understanding tasks, for both\nEnglish and Chinese languages. As for model efficiency, instead of the\nquadratic complexity in Transformer-based models, our model has linear\ncomplexity and performs more efficiently during inference. Moreover, we find\nthat our model can generate more diverse outputs with less contextualized\nfeature redundancy than existing attention-based models.",
        "pdf_link": "https://arxiv.org/pdf/2209.03834v2.pdf"
    },
    {
        "title": "Multi-Granularity Prediction for Scene Text Recognition",
        "authors": [
            "Peng Wang",
            "Cheng Da",
            "Cong Yao"
        ],
        "published": "2022-09-08T06:43:59Z",
        "summary": "Scene text recognition (STR) has been an active research topic in computer\nvision for years. To tackle this challenging problem, numerous innovative\nmethods have been successively proposed and incorporating linguistic knowledge\ninto STR models has recently become a prominent trend. In this work, we first\ndraw inspiration from the recent progress in Vision Transformer (ViT) to\nconstruct a conceptually simple yet powerful vision STR model, which is built\nupon ViT and outperforms previous state-of-the-art models for scene text\nrecognition, including both pure vision models and language-augmented methods.\nTo integrate linguistic knowledge, we further propose a Multi-Granularity\nPrediction strategy to inject information from the language modality into the\nmodel in an implicit way, i.e. , subword representations (BPE and WordPiece)\nwidely-used in NLP are introduced into the output space, in addition to the\nconventional character level representation, while no independent language\nmodel (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push\nthe performance envelop of STR to an even higher level. Specifically, it\nachieves an average recognition accuracy of 93.35% on standard benchmarks. Code\nis available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR.",
        "pdf_link": "https://arxiv.org/pdf/2209.03592v2.pdf"
    },
    {
        "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
        "authors": [
            "Wai Man Si",
            "Michael Backes",
            "Jeremy Blackburn",
            "Emiliano De Cristofaro",
            "Gianluca Stringhini",
            "Savvas Zannettou",
            "Yang Zhang"
        ],
        "published": "2022-09-07T20:45:41Z",
        "summary": "Chatbots are used in many applications, e.g., automated agents, smart home\nassistants, interactive characters in online games, etc. Therefore, it is\ncrucial to ensure they do not behave in undesired manners, providing offensive\nor toxic responses to users. This is not a trivial task as state-of-the-art\nchatbot models are trained on large, public datasets openly collected from the\nInternet. This paper presents a first-of-its-kind, large-scale measurement of\ntoxicity in chatbots. We show that publicly available chatbots are prone to\nproviding toxic responses when fed toxic queries. Even more worryingly, some\nnon-toxic queries can trigger toxic responses too. We then set out to design\nand experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to\ngenerate non-toxic queries that make chatbots respond in a toxic manner. Our\nextensive experimental evaluation demonstrates that our attack is effective\nagainst public chatbot models and outperforms manually-crafted malicious\nqueries proposed by previous work. We also evaluate three defense mechanisms\nagainst ToxicBuddy, showing that they either reduce the attack performance at\nthe cost of affecting the chatbot's utility or are only effective at mitigating\na portion of the attack. This highlights the need for more research from the\ncomputer security and online safety communities to ensure that chatbot models\ndo not hurt their users. Overall, we are confident that ToxicBuddy can be used\nas an auditing tool and that our work will pave the way toward designing more\neffective defenses for chatbot safety.",
        "pdf_link": "https://arxiv.org/pdf/2209.03463v2.pdf"
    },
    {
        "title": "AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles",
        "authors": [
            "Beatrice Portelli",
            "Simone Scaboro",
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Giuseppe Serra"
        ],
        "published": "2022-09-07T20:17:15Z",
        "summary": "This paper describes the models developed by the AILAB-Udine team for the\nSMM4H 22 Shared Task. We explored the limits of Transformer based models on\ntext classification, entity extraction and entity normalization, tackling Tasks\n1, 2, 5, 6 and 10. The main take-aways we got from participating in different\ntasks are: the overwhelming positive effects of combining different\narchitectures when using ensemble learning, and the great potential of\ngenerative models for term normalization.",
        "pdf_link": "https://arxiv.org/pdf/2209.03452v1.pdf"
    },
    {
        "title": "Blessing of Class Diversity in Pre-training",
        "authors": [
            "Yulai Zhao",
            "Jianshu Chen",
            "Simon S. Du"
        ],
        "published": "2022-09-07T20:10:12Z",
        "summary": "This paper presents a new statistical analysis aiming to explain the recent\nsuperior achievements of the pre-training techniques in natural language\nprocessing (NLP). We prove that when the classes of the pre-training task\n(e.g., different words in the masked language model task) are sufficiently\ndiverse, in the sense that the least singular value of the last linear layer in\npre-training (denoted as $\\tilde{\\nu}$) is large, then pre-training can\nsignificantly improve the sample efficiency of downstream tasks. Specially, we\nshow the transfer learning excess risk enjoys an $O\\left(\\frac{1}{\\tilde{\\nu}\n\\sqrt{n}}\\right)$ rate, in contrast to the $O\\left(\\frac{1}{\\sqrt{m}}\\right)$\nrate in the standard supervised learning. Here, $n$ is the number of\npre-training data and $m$ is the number of data in the downstream task, and\ntypically $n \\gg m$. Our proof relies on a vector-form Rademacher complexity\nchain rule for disassembling composite function classes and a modified\nself-concordance condition. These techniques can be of independent interest.",
        "pdf_link": "https://arxiv.org/pdf/2209.03447v3.pdf"
    },
    {
        "title": "What does a platypus look like? Generating customized prompts for zero-shot image classification",
        "authors": [
            "Sarah Pratt",
            "Ian Covert",
            "Rosanne Liu",
            "Ali Farhadi"
        ],
        "published": "2022-09-07T17:27:08Z",
        "summary": "Open-vocabulary models are a promising new paradigm for image classification.\nUnlike traditional classification models, open-vocabulary models classify among\nany arbitrary set of categories specified with natural language during\ninference. This natural language, called \"prompts\", typically consists of a set\nof hand-written templates (e.g., \"a photo of a {}\") which are completed with\neach of the category names. This work introduces a simple method to generate\nhigher accuracy prompts, without relying on any explicit knowledge of the task\ndomain and with far fewer hand-constructed sentences. To achieve this, we\ncombine open-vocabulary models with large language models (LLMs) to create\nCustomized Prompts via Language models (CuPL, pronounced \"couple\"). In\nparticular, we leverage the knowledge contained in LLMs in order to generate\nmany descriptive sentences that contain important discriminating\ncharacteristics of the image categories. This allows the model to place a\ngreater importance on these regions in the image when making predictions. We\nfind that this straightforward and general approach improves accuracy on a\nrange of zero-shot image classification benchmarks, including over one\npercentage point gain on ImageNet. Finally, this simple baseline requires no\nadditional training and remains completely zero-shot. Code available at\nhttps://github.com/sarahpratt/CuPL.",
        "pdf_link": "https://arxiv.org/pdf/2209.03320v3.pdf"
    },
    {
        "title": "Distilling Deep RL Models Into Interpretable Neuro-Fuzzy Systems",
        "authors": [
            "Arne Gevaert",
            "Jonathan Peck",
            "Yvan Saeys"
        ],
        "published": "2022-09-07T14:43:14Z",
        "summary": "Deep Reinforcement Learning uses a deep neural network to encode a policy,\nwhich achieves very good performance in a wide range of applications but is\nwidely regarded as a black box model. A more interpretable alternative to deep\nnetworks is given by neuro-fuzzy controllers. Unfortunately, neuro-fuzzy\ncontrollers often need a large number of rules to solve relatively simple\ntasks, making them difficult to interpret. In this work, we present an\nalgorithm to distill the policy from a deep Q-network into a compact\nneuro-fuzzy controller. This allows us to train compact neuro-fuzzy controllers\nthrough distillation to solve tasks that they are unable to solve directly,\ncombining the flexibility of deep reinforcement learning and the\ninterpretability of compact rule bases. We demonstrate the algorithm on three\nwell-known environments from OpenAI Gym, where we nearly match the performance\nof a DQN agent using only 2 to 6 fuzzy rules.",
        "pdf_link": "https://arxiv.org/pdf/2209.03357v1.pdf"
    },
    {
        "title": "AudioLM: a Language Modeling Approach to Audio Generation",
        "authors": [
            "Zal\u00e1n Borsos",
            "Rapha\u00ebl Marinier",
            "Damien Vincent",
            "Eugene Kharitonov",
            "Olivier Pietquin",
            "Matt Sharifi",
            "Dominik Roblek",
            "Olivier Teboul",
            "David Grangier",
            "Marco Tagliasacchi",
            "Neil Zeghidour"
        ],
        "published": "2022-09-07T13:40:08Z",
        "summary": "We introduce AudioLM, a framework for high-quality audio generation with\nlong-term consistency. AudioLM maps the input audio to a sequence of discrete\ntokens and casts audio generation as a language modeling task in this\nrepresentation space. We show how existing audio tokenizers provide different\ntrade-offs between reconstruction quality and long-term structure, and we\npropose a hybrid tokenization scheme to achieve both objectives. Namely, we\nleverage the discretized activations of a masked language model pre-trained on\naudio to capture long-term structure and the discrete codes produced by a\nneural audio codec to achieve high-quality synthesis. By training on large\ncorpora of raw audio waveforms, AudioLM learns to generate natural and coherent\ncontinuations given short prompts. When trained on speech, and without any\ntranscript or annotation, AudioLM generates syntactically and semantically\nplausible speech continuations while also maintaining speaker identity and\nprosody for unseen speakers. Furthermore, we demonstrate how our approach\nextends beyond speech by generating coherent piano music continuations, despite\nbeing trained without any symbolic representation of music.",
        "pdf_link": "https://arxiv.org/pdf/2209.03143v2.pdf"
    },
    {
        "title": "DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention",
        "authors": [
            "Shunsuke Kitada",
            "Yuki Iwazaki",
            "Riku Togashi",
            "Hitoshi Iyatomi"
        ],
        "published": "2022-09-07T13:25:09Z",
        "summary": "There is increasing interest in the use of multimodal data in various web\napplications, such as digital advertising and e-commerce. Typical methods for\nextracting important information from multimodal data rely on a mid-fusion\narchitecture that combines the feature representations from multiple encoders.\nHowever, as the number of modalities increases, several potential problems with\nthe mid-fusion model structure arise, such as an increase in the dimensionality\nof the concatenated multimodal features and missing modalities. To address\nthese problems, we propose a new concept that considers multimodal inputs as a\nset of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our\nset-aware concept consists of three components that capture the relationships\namong multiple modalities: (a) a BERT-based encoder to handle the inter- and\nintra-order of elements in the sequences, (b) intra-modality residual attention\n(IntraMRA) to capture the importance of the elements in a modality, and (c)\ninter-modality residual attention (InterMRA) to enhance the importance of\nelements with modality-level granularity further. Our concept exhibits\nperformance that is comparable to or better than the previous set-aware models.\nFurthermore, we demonstrate that the visualization of the learned InterMRA and\nIntraMRA weights can provide an interpretation of the prediction results.",
        "pdf_link": "https://arxiv.org/pdf/2209.03126v2.pdf"
    },
    {
        "title": "The Ethical Need for Watermarks in Machine-Generated Language",
        "authors": [
            "Alexei Grinbaum",
            "Laurynas Adomaitis"
        ],
        "published": "2022-09-07T13:09:44Z",
        "summary": "Watermarks should be introduced in the natural language outputs of AI systems\nin order to maintain the distinction between human and machine-generated text.\nThe ethical imperative to not blur this distinction arises from the asemantic\nnature of large language models and from human projections of emotional and\ncognitive states on machines, possibly leading to manipulation, spreading\nfalsehoods or emotional distress. Enforcing this distinction requires\nunintrusive, yet easily accessible marks of the machine origin. We propose to\nimplement a code based on equidistant letter sequences. While no such code\nexists in human-written texts, its appearance in machine-generated ones would\nprove helpful for ethical reasons.",
        "pdf_link": "https://arxiv.org/pdf/2209.03118v1.pdf"
    },
    {
        "title": "Non-Standard Vietnamese Word Detection and Normalization for Text-to-Speech",
        "authors": [
            "Huu-Tien Dang",
            "Thi-Hai-Yen Vuong",
            "Xuan-Hieu Phan"
        ],
        "published": "2022-09-07T07:34:05Z",
        "summary": "Converting written texts into their spoken forms is an essential problem in\nany text-to-speech (TTS) systems. However, building an effective text\nnormalization solution for a real-world TTS system face two main challenges:\n(1) the semantic ambiguity of non-standard words (NSWs), e.g., numbers, dates,\nranges, scores, abbreviations, and (2) transforming NSWs into pronounceable\nsyllables, such as URL, email address, hashtag, and contact name. In this\npaper, we propose a new two-phase normalization approach to deal with these\nchallenges. First, a model-based tagger is designed to detect NSWs. Then,\ndepending on NSW types, a rule-based normalizer expands those NSWs into their\nfinal verbal forms. We conducted three empirical experiments for NSW detection\nusing Conditional Random Fields (CRFs), BiLSTM-CNN-CRF, and BERT-BiGRU-CRF\nmodels on a manually annotated dataset including 5819 sentences extracted from\nVietnamese news articles. In the second phase, we propose a forward\nlexicon-based maximum matching algorithm to split down the hashtag, email, URL,\nand contact name. The experimental results of the tagging phase show that the\naverage F1 scores of the BiLSTM-CNN-CRF and CRF models are above 90.00%,\nreaching the highest F1 of 95.00% with the BERT-BiGRU-CRF model. Overall, our\napproach has low sentence error rates, at 8.15% with CRF and 7.11% with\nBiLSTM-CNN-CRF taggers, and only 6.67% with BERT-BiGRU-CRF tagger.",
        "pdf_link": "https://arxiv.org/pdf/2209.02971v1.pdf"
    },
    {
        "title": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform",
        "authors": [
            "Z. Jiang",
            "G. Song"
        ],
        "published": "2022-09-07T06:33:57Z",
        "summary": "With the development of industry, drones are appearing in various field. In\nrecent years, deep reinforcement learning has made impressive gains in games,\nand we are committed to applying deep reinforcement learning algorithms to the\nfield of robotics, moving reinforcement learning algorithms from game scenarios\nto real-world application scenarios. We are inspired by the LunarLander of\nOpenAI Gym, we decided to make a bold attempt in the field of reinforcement\nlearning to control drones. At present, there is still a lack of work applying\nreinforcement learning algorithms to robot control, the physical simulation\nplatform related to robot control is only suitable for the verification of\nclassical algorithms, and is not suitable for accessing reinforcement learning\nalgorithms for the training. In this paper, we will face this problem, bridging\nthe gap between physical simulation platforms and intelligent agent, connecting\nintelligent agents to a physical simulation platform, allowing agents to learn\nand complete drone flight tasks in a simulator that approximates the real\nworld. We proposed a reinforcement learning framework based on Gazebo that is a\nkind of physical simulation platform (ROS-RL), and used three continuous action\nspace reinforcement learning algorithms in the framework to dealing with the\nproblem of autonomous landing of drones. Experiments show the effectiveness of\nthe algorithm, the task of autonomous landing of drones based on reinforcement\nlearning achieved full success.",
        "pdf_link": "https://arxiv.org/pdf/2209.02954v1.pdf"
    },
    {
        "title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio",
        "authors": [
            "Xinjian Li",
            "Florian Metze",
            "David R Mortensen",
            "Alan W Black",
            "Shinji Watanabe"
        ],
        "published": "2022-09-06T22:48:29Z",
        "summary": "Most recent speech recognition models rely on large supervised datasets,\nwhich are unavailable for many low-resource languages. In this work, we present\na speech recognition pipeline that does not require any audio for the target\nlanguage. The only assumption is that we have access to raw text datasets or a\nset of n-gram statistics. Our speech pipeline consists of three components:\nacoustic, pronunciation, and language models. Unlike the standard pipeline, our\nacoustic and pronunciation models use multilingual models without any\nsupervision. The language model is built using n-gram statistics or the raw\ntext dataset. We build speech recognition for 1909 languages by combining it\nwith Crubadan: a large endangered languages n-gram database. Furthermore, we\ntest our approach on 129 languages across two datasets: Common Voice and CMU\nWilderness dataset. We achieve 50% CER and 74% WER on the Wilderness dataset\nwith Crubadan statistics only and improve them to 45% CER and 69% WER when\nusing 10000 raw text utterances.",
        "pdf_link": "https://arxiv.org/pdf/2209.02842v1.pdf"
    },
    {
        "title": "Depression Symptoms Modelling from Social Media Text: A Semi-supervised Learning Approach",
        "authors": [
            "Nawshad Farruque",
            "Randy Goebel",
            "Sudhakar Sivapalan",
            "Osmar Zaiane"
        ],
        "published": "2022-09-06T18:41:57Z",
        "summary": "A fundamental component of user-level social media language based clinical\ndepression modelling is depression symptoms detection (DSD). Unfortunately,\nthere does not exist any DSD dataset that reflects both the clinical insights\nand the distribution of depression symptoms from the samples of self-disclosed\ndepressed population. In our work, we describe a Semi-supervised Learning (SSL)\nframework which uses an initial supervised learning model that leverages 1) a\nstate-of-the-art large mental health forum text pre-trained language model\nfurther fine-tuned on a clinician annotated DSD dataset, 2) a Zero-Shot\nlearning model for DSD, and couples them together to harvest depression\nsymptoms related samples from our large self-curated Depression Tweets\nRepository (DTR). Our clinician annotated dataset is the largest of its kind.\nFurthermore, DTR is created from the samples of tweets in self-disclosed\ndepressed users Twitter timeline from two datasets, including one of the\nlargest benchmark datasets for user-level depression detection from Twitter.\nThis further helps preserve the depression symptoms distribution of\nself-disclosed Twitter users tweets. Subsequently, we iteratively retrain our\ninitial DSD model with the harvested data. We discuss the stopping criteria and\nlimitations of this SSL process, and elaborate the underlying constructs which\nplay a vital role in the overall SSL process. We show that we can produce a\nfinal dataset which is the largest of its kind. Furthermore, a DSD and a\nDepression Post Detection (DPD) model trained on it achieves significantly\nbetter accuracy than their initial version.",
        "pdf_link": "https://arxiv.org/pdf/2209.02765v3.pdf"
    },
    {
        "title": "Project proposal: A modular reinforcement learning based automated theorem prover",
        "authors": [
            "Boris Shminke"
        ],
        "published": "2022-09-06T15:12:53Z",
        "summary": "We propose to build a reinforcement learning prover of independent\ncomponents: a deductive system (an environment), the proof state representation\n(how an agent sees the environment), and an agent training algorithm. To that\npurpose, we contribute an additional Vampire-based environment to\n$\\texttt{gym-saturation}$ package of OpenAI Gym environments for saturation\nprovers. We demonstrate a prototype of using $\\texttt{gym-saturation}$ together\nwith a popular reinforcement learning framework (Ray $\\texttt{RLlib}$).\nFinally, we discuss our plans for completing this work in progress to a\ncompetitive automated theorem prover.",
        "pdf_link": "https://arxiv.org/pdf/2209.02562v1.pdf"
    },
    {
        "title": "\"Mama Always Had a Way of Explaining Things So I Could Understand'': A Dialogue Corpus for Learning to Construct Explanations",
        "authors": [
            "Henning Wachsmuth",
            "Milad Alshomary"
        ],
        "published": "2022-09-06T14:00:22Z",
        "summary": "As AI is more and more pervasive in everyday life, humans have an increasing\ndemand to understand its behavior and decisions. Most research on explainable\nAI builds on the premise that there is one ideal explanation to be found. In\nfact, however, everyday explanations are co-constructed in a dialogue between\nthe person explaining (the explainer) and the specific person being explained\nto (the explainee). In this paper, we introduce a first corpus of dialogical\nexplanations to enable NLP research on how humans explain as well as on how AI\ncan learn to imitate this process. The corpus consists of 65 transcribed\nEnglish dialogues from the Wired video series \\emph{5 Levels}, explaining 13\ntopics to five explainees of different proficiency. All 1550 dialogue turns\nhave been manually labeled by five independent professionals for the topic\ndiscussed as well as for the dialogue act and the explanation move performed.\nWe analyze linguistic patterns of explainers and explainees, and we explore\ndifferences across proficiency levels. BERT-based baseline results indicate\nthat sequence information helps predicting topics, acts, and moves effectively",
        "pdf_link": "https://arxiv.org/pdf/2209.02508v1.pdf"
    },
    {
        "title": "Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors",
        "authors": [
            "Xi Wang",
            "Gen Li",
            "Yen-Ling Kuo",
            "Muhammed Kocabas",
            "Emre Aksan",
            "Otmar Hilliges"
        ],
        "published": "2022-09-06T13:32:55Z",
        "summary": "We present a method for inferring diverse 3D models of human-object\ninteractions from images. Reasoning about how humans interact with objects in\ncomplex scenes from a single 2D image is a challenging task given ambiguities\narising from the loss of information through projection. In addition, modeling\n3D interactions requires the generalization ability towards diverse object\ncategories and interaction types. We propose an action-conditioned modeling of\ninteractions that allows us to infer diverse 3D arrangements of humans and\nobjects without supervision on contact regions or 3D scene geometry. Our method\nextracts high-level commonsense knowledge from large language models (such as\nGPT-3), and applies them to perform 3D reasoning of human-object interactions.\nOur key insight is priors extracted from large language models can help in\nreasoning about human-object contacts from textural prompts only. We\nquantitatively evaluate the inferred 3D models on a large human-object\ninteraction dataset and show how our method leads to better 3D reconstructions.\nWe further qualitatively evaluate the effectiveness of our method on real\nimages and demonstrate its generalizability towards interaction types and\nobject categories.",
        "pdf_link": "https://arxiv.org/pdf/2209.02485v1.pdf"
    },
    {
        "title": "Layer or Representation Space: What makes BERT-based Evaluation Metrics Robust?",
        "authors": [
            "Doan Nam Long Vu",
            "Nafise Sadat Moosavi",
            "Steffen Eger"
        ],
        "published": "2022-09-06T09:10:54Z",
        "summary": "The evaluation of recent embedding-based evaluation metrics for text\ngeneration is primarily based on measuring their correlation with human\nevaluations on standard benchmarks. However, these benchmarks are mostly from\nsimilar domains to those used for pretraining word embeddings. This raises\nconcerns about the (lack of) generalization of embedding-based metrics to new\nand noisy domains that contain a different vocabulary than the pretraining\ndata. In this paper, we examine the robustness of BERTScore, one of the most\npopular embedding-based metrics for text generation. We show that (a) an\nembedding-based metric that has the highest correlation with human evaluations\non a standard benchmark can have the lowest correlation if the amount of input\nnoise or unknown tokens increases, (b) taking embeddings from the first layer\nof pretrained models improves the robustness of all metrics, and (c) the\nhighest robustness is achieved when using character-level embeddings, instead\nof token-based embeddings, from the first layer of the pretrained model.",
        "pdf_link": "https://arxiv.org/pdf/2209.02317v2.pdf"
    },
    {
        "title": "Reference Resolution and Context Change in Multimodal Situated Dialogue for Exploring Data Visualizations",
        "authors": [
            "Abhinav Kumar",
            "Barbara Di Eugenio",
            "Abari Bhattacharya",
            "Jillian Aurisano",
            "Andrew Johnson"
        ],
        "published": "2022-09-06T04:43:28Z",
        "summary": "Reference resolution, which aims to identify entities being referred to by a\nspeaker, is more complex in real world settings: new referents may be created\nby processes the agents engage in and/or be salient only because they belong to\nthe shared physical setting. Our focus is on resolving references to\nvisualizations on a large screen display in multimodal dialogue; crucially,\nreference resolution is directly involved in the process of creating new\nvisualizations. We describe our annotations for user references to\nvisualizations appearing on a large screen via language and hand gesture and\nalso new entity establishment, which results from executing the user request to\ncreate a new visualization. We also describe our reference resolution pipeline\nwhich relies on an information-state architecture to maintain dialogue context.\nWe report results on detecting and resolving references, effectiveness of\ncontextual information on the model, and under-specified requests for creating\nvisualizations. We also experiment with conventional CRF and deep learning /\ntransformer models (BiLSTM-CRF and BERT-CRF) for tagging references in user\nutterance text. Our results show that transfer learning significantly boost\nperformance of the deep learning methods, although CRF still out-performs them,\nsuggesting that conventional methods may generalize better for low resource\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2209.02215v1.pdf"
    },
    {
        "title": "Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection in Aerial Images",
        "authors": [
            "Zhanchao Huang",
            "Wei Li",
            "Xiang-Gen Xia",
            "Hao Wang",
            "Ran Tao"
        ],
        "published": "2022-09-06T03:42:18Z",
        "summary": "Arbitrary-oriented object detection (AOOD) has been widely applied to locate\nand classify objects with diverse orientations in remote sensing images.\nHowever, the inconsistent features for the localization and classification\ntasks in AOOD models may lead to ambiguity and low-quality object predictions,\nwhich constrains the detection performance. In this article, an AOOD method\ncalled task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv\nadaptively samples task-wise features from respective sensitive regions and\nmaps these features together in alignment to guide a dynamic label assignment\nfor better predictions. Specifically, sampling positions of the localization\nconvolution in TS-Conv are supervised by the oriented bounding box (OBB)\nprediction associated with spatial coordinates, while sampling positions and\nconvolutional kernel of the classification convolution are designed to be\nadaptively adjusted according to different orientations for improving the\norientation robustness of features. Furthermore, a dynamic\ntask-consistent-aware label assignment (DTLA) strategy is developed to select\noptimal candidate positions and assign labels dynamically according to ranked\ntask-aware scores obtained from TS-Conv. Extensive experiments on several\npublic datasets covering multiple scenes, multimodal images, and multiple\ncategories of objects demonstrate the effectiveness, scalability, and superior\nperformance of the proposed TS-Conv.",
        "pdf_link": "https://arxiv.org/pdf/2209.02200v3.pdf"
    },
    {
        "title": "Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",
        "authors": [
            "Hezekiah J. Branch",
            "Jonathan Rodriguez Cefalu",
            "Jeremy McHugh",
            "Leyla Hujer",
            "Aditya Bahl",
            "Daniel del Castillo Iglesias",
            "Ron Heichman",
            "Ramesh Darwishi"
        ],
        "published": "2022-09-05T20:29:17Z",
        "summary": "Recent advances in the development of large language models have resulted in\npublic access to state-of-the-art pre-trained language models (PLMs), including\nGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder\nRepresentations from Transformers (BERT). However, evaluations of PLMs, in\npractice, have shown their susceptibility to adversarial attacks during the\ntraining and fine-tuning stages of development. Such attacks can result in\nerroneous outputs, model-generated hate speech, and the exposure of users'\nsensitive information. While existing research has focused on adversarial\nattacks during either the training or the fine-tuning of PLMs, there is a\ndeficit of information on attacks made between these two development phases. In\nthis work, we highlight a major security vulnerability in the public release of\nGPT-3 and further investigate this vulnerability in other state-of-the-art\nPLMs. We restrict our work to pre-trained models that have not undergone\nfine-tuning. Further, we underscore token distance-minimized perturbations as\nan effective adversarial approach, bypassing both supervised and unsupervised\nquality measures. Following this approach, we observe a significant decrease in\ntext classification quality when evaluating for semantic similarity.",
        "pdf_link": "https://arxiv.org/pdf/2209.02128v1.pdf"
    },
    {
        "title": "Distilling the Knowledge of BERT for CTC-based ASR",
        "authors": [
            "Hayato Futami",
            "Hirofumi Inaguma",
            "Masato Mimura",
            "Shinsuke Sakai",
            "Tatsuya Kawahara"
        ],
        "published": "2022-09-05T16:08:35Z",
        "summary": "Connectionist temporal classification (CTC) -based models are attractive\nbecause of their fast inference in automatic speech recognition (ASR). Language\nmodel (LM) integration approaches such as shallow fusion and rescoring can\nimprove the recognition accuracy of CTC-based ASR by taking advantage of the\nknowledge in text corpora. However, they significantly slow down the inference\nof CTC. In this study, we propose to distill the knowledge of BERT for\nCTC-based ASR, extending our previous study for attention-based ASR. CTC-based\nASR learns the knowledge of BERT during training and does not use BERT during\ntesting, which maintains the fast inference of CTC. Different from\nattention-based models, CTC-based models make frame-level predictions, so they\nneed to be aligned with token-level predictions of BERT for distillation. We\npropose to obtain alignments by calculating the most plausible CTC paths.\nExperimental evaluations on the Corpus of Spontaneous Japanese (CSJ) and\nTED-LIUM2 show that our method improves the performance of CTC-based ASR\nwithout the cost of inference speed.",
        "pdf_link": "https://arxiv.org/pdf/2209.02030v1.pdf"
    },
    {
        "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
        "authors": [
            "Hongjin Su",
            "Jungo Kasai",
            "Chen Henry Wu",
            "Weijia Shi",
            "Tianlu Wang",
            "Jiayi Xin",
            "Rui Zhang",
            "Mari Ostendorf",
            "Luke Zettlemoyer",
            "Noah A. Smith",
            "Tao Yu"
        ],
        "published": "2022-09-05T14:01:15Z",
        "summary": "Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%\nrelative gain under an annotation budget of 18/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https://github.com/HKUNLP/icl-selective-annotation.",
        "pdf_link": "https://arxiv.org/pdf/2209.01975v1.pdf"
    },
    {
        "title": "ChemBERTa-2: Towards Chemical Foundation Models",
        "authors": [
            "Walid Ahmad",
            "Elana Simon",
            "Seyone Chithrananda",
            "Gabriel Grand",
            "Bharath Ramsundar"
        ],
        "published": "2022-09-05T00:31:12Z",
        "summary": "Large pretrained models such as GPT-3 have had tremendous impact on modern\nnatural language processing by leveraging self-supervised learning to learn\nsalient representations that can be used to readily finetune on a wide variety\nof downstream tasks. We investigate the possibility of transferring such\nadvances to molecular machine learning by building a chemical foundation model,\nChemBERTa-2, using the language of SMILES. While labeled data for molecular\nprediction tasks is typically scarce, libraries of SMILES strings are readily\navailable. In this work, we build upon ChemBERTa by optimizing the pretraining\nprocess. We compare multi-task and self-supervised pretraining by varying\nhyperparameters and pretraining dataset size, up to 77M compounds from PubChem.\nTo our knowledge, the 77M set constitutes one of the largest datasets used for\nmolecular pretraining to date. We find that with these pretraining\nimprovements, we are competitive with existing state-of-the-art architectures\non the MoleculeNet benchmark suite. We analyze the degree to which improvements\nin pretraining translate to improvement on downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2209.01712v1.pdf"
    },
    {
        "title": "Every picture tells a story: Image-grounded controllable stylistic story generation",
        "authors": [
            "Holy Lovenia",
            "Bryan Wilie",
            "Romain Barraud",
            "Samuel Cahyawijaya",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2022-09-04T15:07:53Z",
        "summary": "Generating a short story out of an image is arduous. Unlike image captioning,\nstory generation from an image poses multiple challenges: preserving the story\ncoherence, appropriately assessing the quality of the story, steering the\ngenerated story into a certain style, and addressing the scarcity of\nimage-story pair reference datasets limiting supervision during training. In\nthis work, we introduce Plug-and-Play Story Teller (PPST) and improve\nimage-to-story generation by: 1) alleviating the data scarcity problem by\nincorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a\nfluent image-to-text generation with minimal supervision, and 2) enabling a\nmore style-relevant generation by incorporating stylistic adapters to control\nthe story generation. We conduct image-to-story generation experiments with\nnon-styled, romance-styled, and action-styled PPST approaches and compare our\ngenerated stories with those of previous work over three aspects, i.e., story\ncoherence, image-story relevance, and style fitness, using both automatic and\nhuman evaluation. The results show that PPST improves story coherence and has\nbetter image-story relevance, but has yet to be adequately stylistic.",
        "pdf_link": "https://arxiv.org/pdf/2209.01638v2.pdf"
    },
    {
        "title": "Generalization in Neural Networks: A Broad Survey",
        "authors": [
            "Chris Rohlfs"
        ],
        "published": "2022-09-04T12:48:30Z",
        "summary": "This paper reviews concepts, modeling approaches, and recent findings along a\nspectrum of different levels of abstraction of neural network models including\ngeneralization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks,\n(5) Modalities, and (6) Scopes. Results on (1) sample generalization show that,\nin the case of ImageNet, nearly all the recent improvements reduced training\nerror while overfitting stayed flat; with nearly all the training error\neliminated, future progress will require a focus on reducing overfitting.\nPerspectives from statistics highlight how (2) distribution generalization can\nbe viewed alternately as a change in sample weights or a change in the\ninput-output relationship; thus, techniques that have been successful in domain\ngeneralization have the potential to be applied to difficult forms of sample or\ndistribution generalization. Transfer learning approaches to (3) domain\ngeneralization are summarized, as are recent advances and the wealth of domain\nadaptation benchmark datasets available. Recent breakthroughs surveyed in (4)\ntask generalization include few-shot meta-learning approaches and the BERT NLP\nengine, and recent (5) modality generalization studies are discussed that\nintegrate image and text data and that apply a biologically-inspired network\nacross olfactory, visual, and auditory modalities. Recent (6) scope\ngeneralization results are reviewed that embed knowledge graphs into deep NLP\napproaches. Additionally, concepts from neuroscience are discussed on the\nmodular architecture of brains and the steps by which dopamine-driven\nconditioning leads to abstract thinking.",
        "pdf_link": "https://arxiv.org/pdf/2209.01610v2.pdf"
    },
    {
        "title": "Selective Text Augmentation with Word Roles for Low-Resource Text Classification",
        "authors": [
            "Biyang Guo",
            "Songqiao Han",
            "Hailiang Huang"
        ],
        "published": "2022-09-04T08:13:11Z",
        "summary": "Data augmentation techniques are widely used in text classification tasks to\nimprove the performance of classifiers, especially in low-resource scenarios.\nMost previous methods conduct text augmentation without considering the\ndifferent functionalities of the words in the text, which may generate\nunsatisfactory samples. Different words may play different roles in text\nclassification, which inspires us to strategically select the proper roles for\ntext augmentation. In this work, we first identify the relationships between\nthe words in a text and the text category from the perspectives of statistical\ncorrelation and semantic similarity and then utilize them to divide the words\ninto four roles -- Gold, Venture, Bonus, and Trivial words, which have\ndifferent functionalities for text classification. Based on these word roles,\nwe present a new augmentation technique called STA (Selective Text\nAugmentation) where different text-editing operations are selectively applied\nto words with specific roles. STA can generate diverse and relatively clean\nsamples, while preserving the original core semantics, and is also quite simple\nto implement. Extensive experiments on 5 benchmark low-resource text\nclassification datasets illustrate that augmented samples produced by STA\nsuccessfully boost the performance of classification models which significantly\noutperforms previous non-selective methods, including two large language\nmodel-based techniques. Cross-dataset experiments further indicate that STA can\nhelp the classifiers generalize better to other datasets than previous methods.",
        "pdf_link": "https://arxiv.org/pdf/2209.01560v1.pdf"
    },
    {
        "title": "Do Large Language Models know what humans know?",
        "authors": [
            "Sean Trott",
            "Cameron Jones",
            "Tyler Chang",
            "James Michaelov",
            "Benjamin Bergen"
        ],
        "published": "2022-09-04T01:29:53Z",
        "summary": "Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers' mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others' beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior -- despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.",
        "pdf_link": "https://arxiv.org/pdf/2209.01515v3.pdf"
    },
    {
        "title": "Neural Approaches to Multilingual Information Retrieval",
        "authors": [
            "Dawn Lawrie",
            "Eugene Yang",
            "Douglas W. Oard",
            "James Mayfield"
        ],
        "published": "2022-09-03T06:02:52Z",
        "summary": "Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.",
        "pdf_link": "https://arxiv.org/pdf/2209.01335v2.pdf"
    },
    {
        "title": "TransPolymer: a Transformer-based language model for polymer property predictions",
        "authors": [
            "Changwen Xu",
            "Yuyang Wang",
            "Amir Barati Farimani"
        ],
        "published": "2022-09-03T01:29:59Z",
        "summary": "Accurate and efficient prediction of polymer properties is of great\nsignificance in polymer design. Conventionally, expensive and time-consuming\nexperiments or simulations are required to evaluate polymer functions.\nRecently, Transformer models, equipped with self-attention mechanisms, have\nexhibited superior performance in natural language processing. However, such\nmethods have not been investigated in polymer sciences. Herein, we report\nTransPolymer, a Transformer-based language model for polymer property\nprediction. Our proposed polymer tokenizer with chemical awareness enables\nlearning representations from polymer sequences. Rigorous experiments on ten\npolymer property prediction benchmarks demonstrate the superior performance of\nTransPolymer. Moreover, we show that TransPolymer benefits from pretraining on\nlarge unlabeled dataset via Masked Language Modeling. Experimental results\nfurther manifest the important role of self-attention in modeling polymer\nsequences. We highlight this model as a promising computational tool for\npromoting rational polymer design and understanding structure-property\nrelationships from a data science view.",
        "pdf_link": "https://arxiv.org/pdf/2209.01307v4.pdf"
    },
    {
        "title": "Elaboration-Generating Commonsense Question Answering at Scale",
        "authors": [
            "Wenya Wang",
            "Vivek Srikumar",
            "Hanna Hajishirzi",
            "Noah A. Smith"
        ],
        "published": "2022-09-02T18:32:09Z",
        "summary": "In question answering requiring common sense, language models (e.g., GPT-3)\nhave been used to generate text expressing background knowledge that helps\nimprove performance. Yet the cost of working with such models is very high; in\nthis work, we finetune smaller language models to generate useful intermediate\ncontext, referred to here as elaborations. Our framework alternates between\nupdating two language models -- an elaboration generator and an answer\npredictor -- allowing each to influence the other. Using less than 0.5% of the\nparameters of GPT-3, our model outperforms alternatives with similar sizes and\ncloses the gap on GPT-3 on four commonsense question answering benchmarks.\nHuman evaluations show that the quality of the generated elaborations is high.",
        "pdf_link": "https://arxiv.org/pdf/2209.01232v2.pdf"
    },
    {
        "title": "Petals: Collaborative Inference and Fine-tuning of Large Models",
        "authors": [
            "Alexander Borzunov",
            "Dmitry Baranchuk",
            "Tim Dettmers",
            "Max Ryabinin",
            "Younes Belkada",
            "Artem Chumachenko",
            "Pavel Samygin",
            "Colin Raffel"
        ],
        "published": "2022-09-02T17:38:03Z",
        "summary": "Many NLP tasks benefit from using large language models (LLMs) that often\nhave more than 100 billion parameters. With the release of BLOOM-176B and\nOPT-175B, everyone can download pretrained models of this scale. Still, using\nthese models requires high-end hardware unavailable to many researchers. In\nsome cases, LLMs can be used more affordably via RAM offloading or hosted APIs.\nHowever, these techniques have innate limitations: offloading is too slow for\ninteractive inference, while APIs are not flexible enough for research that\nrequires access to weights, attention or logits. In this work, we propose\nPetals - a system for inference and fine-tuning of large models collaboratively\nby joining the resources of multiple parties. We demonstrate that this strategy\noutperforms offloading for very large models, running inference of BLOOM-176B\non consumer GPUs with $\\approx$ 1 step per second, which is enough for many\ninteractive LLM applications. Unlike most inference APIs, Petals also natively\nexposes hidden states of served models, allowing to train and share custom\nmodel extensions based on efficient fine-tuning methods.",
        "pdf_link": "https://arxiv.org/pdf/2209.01188v2.pdf"
    },
    {
        "title": "GReS: Graphical Cross-domain Recommendation for Supply Chain Platform",
        "authors": [
            "Zhiwen Jing",
            "Ziliang Zhao",
            "Yang Feng",
            "Xiaochen Ma",
            "Nan Wu",
            "Shengqiao Kang",
            "Cheng Yang",
            "Yujia Zhang",
            "Hao Guo"
        ],
        "published": "2022-09-02T12:58:03Z",
        "summary": "Supply Chain Platforms (SCPs) provide downstream industries with numerous raw\nmaterials. Compared with traditional e-commerce platforms, data in SCPs is more\nsparse due to limited user interests. To tackle the data sparsity problem, one\ncan apply Cross-Domain Recommendation (CDR) which improves the recommendation\nperformance of the target domain with the source domain information. However,\napplying CDR to SCPs directly ignores the hierarchical structure of commodities\nin SCPs, which reduce the recommendation performance. To leverage this feature,\nin this paper, we take the catering platform as an example and propose GReS, a\ngraphical cross-domain recommendation model. The model first constructs a\ntree-shaped graph to represent the hierarchy of different nodes of dishes and\ningredients, and then applies our proposed Tree2vec method combining GCN and\nBERT models to embed the graph for recommendations. Experimental results on a\ncommercial dataset show that GReS significantly outperforms state-of-the-art\nmethods in Cross-Domain Recommendation for Supply Chain Platforms.",
        "pdf_link": "https://arxiv.org/pdf/2209.01031v1.pdf"
    },
    {
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
        "authors": [
            "Simeng Han",
            "Hailey Schoelkopf",
            "Yilun Zhao",
            "Zhenting Qi",
            "Martin Riddell",
            "Luke Benson",
            "Lucy Sun",
            "Ekaterina Zubova",
            "Yujie Qiao",
            "Matthew Burtell",
            "David Peng",
            "Jonathan Fan",
            "Yixin Liu",
            "Brian Wong",
            "Malcolm Sailor",
            "Ansong Ni",
            "Linyong Nan",
            "Jungo Kasai",
            "Tao Yu",
            "Rui Zhang",
            "Shafiq Joty",
            "Alexander R. Fabbri",
            "Wojciech Kryscinski",
            "Xi Victoria Lin",
            "Caiming Xiong",
            "Dragomir Radev"
        ],
        "published": "2022-09-02T06:50:11Z",
        "summary": "We present FOLIO, a human-annotated, open-domain, and logically complex and\ndiverse dataset for reasoning in natural language (NL), equipped with first\norder logic (FOL) annotations. FOLIO consists of 1,435 examples (unique\nconclusions), each paired with one of 487 sets of premises which serve as rules\nto be used to deductively reason for the validity of each conclusion. The\nlogical correctness of premises and conclusions is ensured by their parallel\nFOL annotations, which are automatically verified by our FOL inference engine.\nIn addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically\nconstitute a new NL-FOL translation dataset using FOL as the logical form. Our\nexperiments on FOLIO systematically evaluate the FOL reasoning ability of\nsupervised fine-tuning on medium-sized language models (BERT, RoBERTa) and\nfew-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For\nNL-FOL translation, we experiment with GPT-3 and Codex. Our results show that\none of the most capable Large Language Model (LLM) publicly available, GPT-3\ndavinci, achieves only slightly better than random results with few-shot\nprompting on a subset of FOLIO, and the model is especially bad at predicting\nthe correct truth values for False and Unknown conclusions. Our dataset and\ncode are available at https://github.com/Yale-LILY/FOLIO.",
        "pdf_link": "https://arxiv.org/pdf/2209.00840v1.pdf"
    },
    {
        "title": "In conversation with Artificial Intelligence: aligning language models with human values",
        "authors": [
            "Atoosa Kasirzadeh",
            "Iason Gabriel"
        ],
        "published": "2022-09-01T21:16:47Z",
        "summary": "Large-scale language technologies are increasingly used in various forms of\ncommunication with humans across different contexts. One particular use case\nfor these technologies is conversational agents, which output natural language\ntext in response to prompts and queries. This mode of engagement raises a\nnumber of social and ethical questions. For example, what does it mean to align\nconversational agents with human norms or values? Which norms or values should\nthey be aligned with? And how can this be accomplished? In this paper, we\npropose a number of steps that help answer these questions. We start by\ndeveloping a philosophical analysis of the building blocks of linguistic\ncommunication between conversational agents and human interlocutors. We then\nuse this analysis to identify and formulate ideal norms of conversation that\ncan govern successful linguistic communication between humans and\nconversational agents. Furthermore, we explore how these norms can be used to\nalign conversational agents with human values across a range of different\ndiscursive domains. We conclude by discussing the practical implications of our\nproposal for the design of conversational agents that are aligned with these\nnorms and values.",
        "pdf_link": "https://arxiv.org/pdf/2209.00731v2.pdf"
    },
    {
        "title": "Unsupervised Simplification of Legal Texts",
        "authors": [
            "Mert Cemri",
            "Tolga \u00c7ukur",
            "Aykut Ko\u00e7"
        ],
        "published": "2022-09-01T15:58:12Z",
        "summary": "The processing of legal texts has been developing as an emerging field in\nnatural language processing (NLP). Legal texts contain unique jargon and\ncomplex linguistic attributes in vocabulary, semantics, syntax, and morphology.\nTherefore, the development of text simplification (TS) methods specific to the\nlegal domain is of paramount importance for facilitating comprehension of legal\ntext by ordinary people and providing inputs to high-level models for\nmainstream legal NLP applications. While a recent study proposed a rule-based\nTS method for legal text, learning-based TS in the legal domain has not been\nconsidered previously. Here we introduce an unsupervised simplification method\nfor legal texts (USLT). USLT performs domain-specific TS by replacing complex\nwords and splitting long sentences. To this end, USLT detects complex words in\na sentence, generates candidates via a masked-transformer model, and selects a\ncandidate for substitution based on a rank score. Afterward, USLT recursively\ndecomposes long sentences into a hierarchy of shorter core and context\nsentences while preserving semantic meaning. We demonstrate that USLT\noutperforms state-of-the-art domain-general TS methods in text simplicity while\nkeeping the semantics intact.",
        "pdf_link": "https://arxiv.org/pdf/2209.00557v1.pdf"
    },
    {
        "title": "Generating Coherent Drum Accompaniment With Fills And Improvisations",
        "authors": [
            "Rishabh Dahale",
            "Vaibhav Talwadker",
            "Preeti Rao",
            "Prateek Verma"
        ],
        "published": "2022-09-01T08:31:26Z",
        "summary": "Creating a complex work of art like music necessitates profound creativity.\nWith recent advancements in deep learning and powerful models such as\ntransformers, there has been huge progress in automatic music generation. In an\naccompaniment generation context, creating a coherent drum pattern with\napposite fills and improvisations at proper locations in a song is a\nchallenging task even for an experienced drummer. Drum beats tend to follow a\nrepetitive pattern through stanzas with fills or improvisation at section\nboundaries. In this work, we tackle the task of drum pattern generation\nconditioned on the accompanying music played by four melodic instruments:\nPiano, Guitar, Bass, and Strings. We use the transformer sequence to sequence\nmodel to generate a basic drum pattern conditioned on the melodic accompaniment\nto find that improvisation is largely absent, attributed possibly to its\nexpectedly relatively low representation in the training data. We propose a\nnovelty function to capture the extent of improvisation in a bar relative to\nits neighbors. We train a model to predict improvisation locations from the\nmelodic accompaniment tracks. Finally, we use a novel BERT-inspired in-filling\narchitecture, to learn the structure of both the drums and melody to in-fill\nelements of improvised music.",
        "pdf_link": "https://arxiv.org/pdf/2209.00291v1.pdf"
    },
    {
        "title": "Enhancing Semantic Understanding with Self-supervised Methods for Abstractive Dialogue Summarization",
        "authors": [
            "Hyunjae Lee",
            "Jaewoong Yun",
            "Hyunjin Choi",
            "Seongho Joe",
            "Youngjune L. Gwon"
        ],
        "published": "2022-09-01T07:51:46Z",
        "summary": "Contextualized word embeddings can lead to state-of-the-art performances in\nnatural language understanding. Recently, a pre-trained deep contextualized\ntext encoder such as BERT has shown its potential in improving natural language\ntasks including abstractive summarization. Existing approaches in dialogue\nsummarization focus on incorporating a large language model into summarization\ntask trained on large-scale corpora consisting of news articles rather than\ndialogues of multiple speakers. In this paper, we introduce self-supervised\nmethods to compensate shortcomings to train a dialogue summarization model. Our\nprinciple is to detect incoherent information flows using pretext dialogue text\nto enhance BERT's ability to contextualize the dialogue text representations.\nWe build and fine-tune an abstractive dialogue summarization model on a shared\nencoder-decoder architecture using the enhanced BERT. We empirically evaluate\nour abstractive dialogue summarizer with the SAMSum corpus, a recently\nintroduced dataset with abstractive dialogue summaries. All of our methods have\ncontributed improvements to abstractive summary measured in ROUGE scores.\nThrough an extensive ablation study, we also present a sensitivity analysis to\ncritical model hyperparameters, probabilities of switching utterances and\nmasking interlocutors.",
        "pdf_link": "https://arxiv.org/pdf/2209.00278v1.pdf"
    },
    {
        "title": "Isotropic Representation Can Improve Dense Retrieval",
        "authors": [
            "Euna Jung",
            "Jungwon Park",
            "Jaekeol Choi",
            "Sungyoon Kim",
            "Wonjong Rhee"
        ],
        "published": "2022-09-01T04:29:38Z",
        "summary": "The recent advancement in language representation modeling has broadly\naffected the design of dense retrieval models. In particular, many of the\nhigh-performing dense retrieval models evaluate representations of query and\ndocument using BERT, and subsequently apply a cosine-similarity based scoring\nto determine the relevance. BERT representations, however, are known to follow\nan anisotropic distribution of a narrow cone shape and such an anisotropic\ndistribution can be undesirable for the cosine-similarity based scoring. In\nthis work, we first show that BERT-based DR also follows an anisotropic\ndistribution. To cope with the problem, we introduce unsupervised\npost-processing methods of Normalizing Flow and whitening, and develop\ntoken-wise method in addition to the sequence-wise method for applying the\npost-processing methods to the representations of dense retrieval models. We\nshow that the proposed methods can effectively enhance the representations to\nbe isotropic, then we perform experiments with ColBERT and RepBERT to show that\nthe performance (NDCG at 10) of document re-ranking can be improved by\n5.17\\%$\\sim$8.09\\% for ColBERT and 6.88\\%$\\sim$22.81\\% for RepBERT. To examine\nthe potential of isotropic representation for improving the robustness of DR\nmodels, we investigate out-of-distribution tasks where the test dataset differs\nfrom the training dataset. The results show that isotropic representation can\nachieve a generally improved performance. For instance, when training dataset\nis MS-MARCO and test dataset is Robust04, isotropy post-processing can improve\nthe baseline performance by up to 24.98\\%. Furthermore, we show that an\nisotropic model trained with an out-of-distribution dataset can even outperform\na baseline model trained with the in-distribution dataset.",
        "pdf_link": "https://arxiv.org/pdf/2209.00218v2.pdf"
    },
    {
        "title": "The Fellowship of the Authors: Disambiguating Names from Social Network Context",
        "authors": [
            "Ryan Muther",
            "David Smith"
        ],
        "published": "2022-08-31T21:51:55Z",
        "summary": "Most NLP approaches to entity linking and coreference resolution focus on\nretrieving similar mentions using sparse or dense text representations. The\ncommon \"Wikification\" task, for instance, retrieves candidate Wikipedia\narticles for each entity mention. For many domains, such as bibliographic\ncitations, authority lists with extensive textual descriptions for each entity\nare lacking and ambiguous named entities mostly occur in the context of other\nnamed entities. Unlike prior work, therefore, we seek to leverage the\ninformation that can be gained from looking at association networks of\nindividuals derived from textual evidence in order to disambiguate names. We\ncombine BERT-based mention representations with a variety of graph induction\nstrategies and experiment with supervised and unsupervised cluster inference\nmethods. We experiment with data consisting of lists of names from two domains:\nbibliographic citations from CrossRef and chains of transmission (isnads) from\nclassical Arabic histories. We find that in-domain language model pretraining\ncan significantly improve mention representations, especially for larger\ncorpora, and that the availability of bibliographic information, such as\npublication venue or title, can also increase performance on this task. We also\npresent a novel supervised cluster inference model which gives competitive\nperformance for little computational effort, making it ideal for situations\nwhere individuals must be identified without relying on an exhaustive authority\nlist.",
        "pdf_link": "https://arxiv.org/pdf/2209.00133v1.pdf"
    },
    {
        "title": "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks",
        "authors": [
            "David Oniani",
            "Sonish Sivarajkumar",
            "Yanshan Wang"
        ],
        "published": "2022-08-31T15:36:27Z",
        "summary": "Clinical Natural Language Processing (NLP) has become an emerging technology\nin healthcare that leverages a large amount of free-text data in electronic\nhealth records (EHRs) to improve patient care, support clinical decisions, and\nfacilitate clinical and translational science research. Recently, deep learning\nhas achieved state-of-the-art performance in many clinical NLP tasks. However,\ntraining deep learning models usually requires large annotated datasets, which\nare normally not publicly available and can be time-consuming to build in\nclinical domains. Working with smaller annotated datasets is typical in\nclinical NLP and therefore, ensuring that deep learning models perform well is\ncrucial for the models to be used in real-world applications. A widely adopted\napproach is fine-tuning existing Pre-trained Language Models (PLMs), but these\nattempts fall short when the training dataset contains only a few annotated\nsamples. Few-Shot Learning (FSL) has recently been investigated to tackle this\nproblem. Siamese Neural Network (SNN) has been widely utilized as an FSL\napproach in computer vision, but has not been studied well in NLP. Furthermore,\nthe literature on its applications in clinical domains is scarce. In this\npaper, we propose two SNN-based FSL approaches for clinical NLP, including\nPre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We\nevaluated the proposed approaches on two clinical tasks, namely clinical text\nclassification and clinical named entity recognition. We tested three few-shot\nsettings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP\ntasks were benchmarked using three PLMs, including BERT,BioBERT, and\nBioClinicalBERT. The experimental results verified the effectiveness of the\nproposed SNN-based FSL approaches in both NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.14923v2.pdf"
    },
    {
        "title": "Cluster-based Sampling in Hindsight Experience Replay for Robotic Tasks (Student Abstract)",
        "authors": [
            "Taeyoung Kim",
            "Dongsoo Har"
        ],
        "published": "2022-08-31T09:45:30Z",
        "summary": "In multi-goal reinforcement learning with a sparse binary reward, training\nagents is particularly challenging, due to a lack of successful experiences. To\nsolve this problem, hindsight experience replay (HER) generates successful\nexperiences even from unsuccessful ones. However, generating successful\nexperiences from uniformly sampled ones is not an efficient process. In this\npaper, the impact of exploiting the property of achieved goals in generating\nsuccessful experiences is investigated and a novel cluster-based sampling\nstrategy is proposed. The proposed sampling strategy groups episodes with\ndifferent achieved goals by using a cluster model and samples experiences in\nthe manner of HER to create the training batch. The proposed method is\nvalidated by experiments with three robotic control tasks of the OpenAI Gym.\nThe results of experiments demonstrate that the proposed method is\nsubstantially sample efficient and achieves better performance than baseline\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2208.14741v4.pdf"
    },
    {
        "title": "Continuous QA Learning with Structured Prompts",
        "authors": [
            "Yinhe Zheng"
        ],
        "published": "2022-08-31T02:38:16Z",
        "summary": "QA models with lifelong learning (LL) abilities are important for practical\nQA applications, and architecture-based LL methods are reported to be an\neffective implementation for these models. However, it is non-trivial to extend\nprevious approaches to QA tasks since they either require access to task\nidentities in the testing phase or do not explicitly model samples from unseen\ntasks. In this paper, we propose Diana: a dynamic architecture-based lifelong\nQA model that tries to learn a sequence of QA tasks with a prompt enhanced\nlanguage model. Four types of hierarchically organized prompts are used in\nDiana to capture QA knowledge from different granularities. Specifically, we\ndedicate task-level prompts to capture task-specific knowledge to retain high\nLL performances and maintain instance-level prompts to learn knowledge shared\nacross different input samples to improve the model's generalization\nperformance. Moreover, we dedicate separate prompts to explicitly model unseen\ntasks and introduce a set of prompt key vectors to facilitate knowledge sharing\nbetween tasks. Extensive experiments demonstrate that Diana outperforms\nstate-of-the-art lifelong QA models, especially in handling unseen tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.14602v3.pdf"
    },
    {
        "title": "A Prescriptive Learning Analytics Framework: Beyond Predictive Modelling and onto Explainable AI with Prescriptive Analytics and ChatGPT",
        "authors": [
            "Teo Susnjak"
        ],
        "published": "2022-08-31T00:57:17Z",
        "summary": "A significant body of recent research in the field of Learning Analytics has\nfocused on leveraging machine learning approaches for predicting at-risk\nstudents in order to initiate timely interventions and thereby elevate\nretention and completion rates. The overarching feature of the majority of\nthese research studies has been on the science of prediction only. The\ncomponent of predictive analytics concerned with interpreting the internals of\nthe models and explaining their predictions for individual cases to\nstakeholders has largely been neglected. Additionally, works that attempt to\nemploy data-driven prescriptive analytics to automatically generate\nevidence-based remedial advice for at-risk learners are in their infancy.\n  eXplainable AI is a field that has recently emerged providing cutting-edge\ntools which support transparent predictive analytics and techniques for\ngenerating tailored advice for at-risk students. This study proposes a novel\nframework that unifies both transparent machine learning as well as techniques\nfor enabling prescriptive analytics, while integrating the latest advances in\nlarge language models. This work practically demonstrates the proposed\nframework using predictive models for identifying at-risk learners of programme\nnon-completion. The study then further demonstrates how predictive modelling\ncan be augmented with prescriptive analytics on two case studies in order to\ngenerate human-readable prescriptive feedback for those who are at risk using\nChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2208.14582v2.pdf"
    },
    {
        "title": "To Adapt or to Fine-tune: A Case Study on Abstractive Summarization",
        "authors": [
            "Zheng Zhao",
            "Pinzhen Chen"
        ],
        "published": "2022-08-30T22:48:28Z",
        "summary": "Recent advances in the field of abstractive summarization leverage\npre-trained language models rather than train a model from scratch. However,\nsuch models are sluggish to train and accompanied by a massive overhead.\nResearchers have proposed a few lightweight alternatives such as smaller\nadapters to mitigate the drawbacks. Nonetheless, it remains uncertain whether\nusing adapters benefits the task of summarization, in terms of improved\nefficiency without an unpleasant sacrifice in performance. In this work, we\ncarry out multifaceted investigations on fine-tuning and adapters for\nsummarization tasks with varying complexity: language, domain, and task\ntransfer. In our experiments, fine-tuning a pre-trained language model\ngenerally attains a better performance than using adapters; the performance gap\npositively correlates with the amount of training data used. Notably, adapters\nexceed fine-tuning under extremely low-resource conditions. We further provide\ninsights on multilinguality, model convergence, and robustness, hoping to shed\nlight on the pragmatic choice of fine-tuning or adapters in abstractive\nsummarization.",
        "pdf_link": "https://arxiv.org/pdf/2208.14559v1.pdf"
    },
    {
        "title": "Faithful Reasoning Using Large Language Models",
        "authors": [
            "Antonia Creswell",
            "Murray Shanahan"
        ],
        "published": "2022-08-30T13:44:41Z",
        "summary": "Although contemporary large language models (LMs) demonstrate impressive\nquestion-answering capabilities, their answers are typically the product of a\nsingle call to the model. This entails an unwelcome degree of opacity and\ncompromises performance, especially on problems that are inherently multi-step.\nTo address these limitations, we show how LMs can be made to perform faithful\nmulti-step reasoning via a process whose causal structure mirrors the\nunderlying logical structure of the problem. Our approach works by chaining\ntogether reasoning steps, where each step results from calls to two fine-tuned\nLMs, one for selection and one for inference, to produce a valid reasoning\ntrace. Our method carries out a beam search through the space of reasoning\ntraces to improve reasoning quality. We demonstrate the effectiveness of our\nmodel on multi-step logical deduction and scientific question-answering,\nshowing that it outperforms baselines on final answer accuracy, and generates\nhumanly interpretable reasoning traces whose validity can be checked by the\nuser.",
        "pdf_link": "https://arxiv.org/pdf/2208.14271v1.pdf"
    },
    {
        "title": "Efficient and Interpretable Neural Models for Entity Tracking",
        "authors": [
            "Shubham Toshniwal"
        ],
        "published": "2022-08-30T13:25:27Z",
        "summary": "What would it take for a natural language model to understand a novel, such\nas The Lord of the Rings? Among other things, such a model must be able to: (a)\nidentify and record new characters (entities) and their attributes as they are\nintroduced in the text, and (b) identify subsequent references to the\ncharacters previously introduced and update their attributes. This problem of\nentity tracking is essential for language understanding, and thus, useful for a\nwide array of downstream applications in NLP such as question-answering,\nsummarization.\n  In this thesis, we focus on two key problems in relation to facilitating the\nuse of entity tracking models: (i) scaling entity tracking models to long\ndocuments, such as a novel, and (ii) integrating entity tracking into language\nmodels. Applying language technologies to long documents has garnered interest\nrecently, but computational constraints are a significant bottleneck in scaling\nup current methods. In this thesis, we argue that computationally efficient\nentity tracking models can be developed by representing entities with rich,\nfixed-dimensional vector representations derived from pretrained language\nmodels, and by exploiting the ephemeral nature of entities. We also argue for\nthe integration of entity tracking into language models as it will allow for:\n(i) wider application given the current ubiquitous use of pretrained language\nmodels in NLP applications, and (ii) easier adoption since it is much easier to\nswap in a new pretrained language model than to integrate a separate standalone\nentity tracking model.",
        "pdf_link": "https://arxiv.org/pdf/2208.14252v1.pdf"
    },
    {
        "title": "Expressions Causing Differences in Emotion Recognition in Social Networking Service Documents",
        "authors": [
            "Tsubasa Nakagawa",
            "Shunsuke Kitada",
            "Hitoshi Iyatomi"
        ],
        "published": "2022-08-30T13:17:32Z",
        "summary": "It is often difficult to correctly infer a writer's emotion from text\nexchanged online, and differences in recognition between writers and readers\ncan be problematic. In this paper, we propose a new framework for detecting\nsentences that create differences in emotion recognition between the writer and\nthe reader and for detecting the kinds of expressions that cause such\ndifferences. The proposed framework consists of a bidirectional encoder\nrepresentations from transformers (BERT)-based detector that detects sentences\ncausing differences in emotion recognition and an analysis that acquires\nexpressions that characteristically appear in such sentences. The detector,\nbased on a Japanese SNS-document dataset with emotion labels annotated by both\nthe writer and three readers of the social networking service (SNS) documents,\ndetected \"hidden-anger sentences\" with AUC = 0.772; these sentences gave rise\nto differences in the recognition of anger. Because SNS documents contain many\nsentences whose meaning is extremely difficult to interpret, by analyzing the\nsentences detected by this detector, we obtained several expressions that\nappear characteristically in hidden-anger sentences. The detected sentences and\nexpressions do not convey anger explicitly, and it is difficult to infer the\nwriter's anger, but if the implicit anger is pointed out, it becomes possible\nto guess why the writer is angry. Put into practical use, this framework would\nlikely have the ability to mitigate problems based on misunderstandings.",
        "pdf_link": "https://arxiv.org/pdf/2208.14244v2.pdf"
    },
    {
        "title": "Transformers with Learnable Activation Functions",
        "authors": [
            "Haishuo Fang",
            "Ji-Ung Lee",
            "Nafise Sadat Moosavi",
            "Iryna Gurevych"
        ],
        "published": "2022-08-30T09:47:31Z",
        "summary": "Activation functions can have a significant impact on reducing the\ntopological complexity of input data and therefore improve the performance of\nthe model. Selecting a suitable activation function is an essential step in\nneural model design. However, the choice of activation function is seldom\ndiscussed or explored in Transformer-based language models. Their activation\nfunctions are chosen beforehand and then remain fixed from pre-training to\nfine-tuning. As a result, the inductive biases they imposed on models cannot be\nadjusted during this long life cycle. Moreover, subsequently developed models\n(e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use\nthe same activation function without justification. In this paper, we\ninvestigate the effectiveness of using Rational Activation Function (RAF), a\nlearnable activation function, in the Transformer architecture. In contrast to\nconventional, predefined activation functions, RAFs can adaptively learn\noptimal activation functions during training according to input data. Our\nexperiments show the RAF-based Transformer (RAFT) achieves a lower validation\nperplexity than a vanilla BERT with the GELU function. We further evaluate RAFT\non downstream tasks in low- and full-data settings. Our results show that RAFT\noutperforms the counterpart model across the majority of tasks and settings.\nFor instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71\npoints on average in low-data scenario (where 100 training examples are\navailable) and by 2.05 points on SQuAD in full-data setting. Analysis of the\nshapes of learned RAFs further unveils that they substantially vary between\ndifferent layers of the pre-trained model and mostly look very different from\nconventional activation functions. RAFT opens a new research direction for\nanalyzing and interpreting pre-trained models according to the learned\nactivation functions.",
        "pdf_link": "https://arxiv.org/pdf/2208.14111v3.pdf"
    },
    {
        "title": "SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance",
        "authors": [
            "Li Lyna Zhang",
            "Youkow Homma",
            "Yujing Wang",
            "Min Wu",
            "Mao Yang",
            "Ruofei Zhang",
            "Ting Cao",
            "Wei Shen"
        ],
        "published": "2022-08-30T03:05:56Z",
        "summary": "Ad relevance modeling plays a critical role in online advertising systems\nincluding Microsoft Bing. To leverage powerful transformers like BERT in this\nlow-latency setting, many existing approaches perform ad-side computations\noffline. While efficient, these approaches are unable to serve cold start ads,\nresulting in poor relevance predictions for such ads. This work aims to design\na new, low-latency BERT via structured pruning to empower real-time online\ninference for cold start ads relevance on a CPU platform. Our challenge is that\nprevious methods typically prune all layers of the transformer to a high,\nuniform sparsity, thereby producing models which cannot achieve satisfactory\ninference speed with an acceptable accuracy.\n  In this paper, we propose SwiftPruner - an efficient framework that leverages\nevolution-based search to automatically find the best-performing layer-wise\nsparse BERT model under the desired latency constraint. Different from existing\nevolution algorithms that conduct random mutations, we propose a reinforced\nmutator with a latency-aware multi-objective reward to conduct better mutations\nfor efficiently searching the large space of layer-wise sparse models.\nExtensive experiments demonstrate that our method consistently achieves higher\nROC AUC and lower latency than the uniform sparse baseline and state-of-the-art\nsearch methods. Remarkably, under our latency requirement of 1900us on CPU,\nSwiftPruner achieves a 0.86% higher AUC than the state-of-the-art uniform\nsparse baseline for BERT-Mini on a large scale real-world dataset. Online A/B\ntesting shows that our model also achieves a significant 11.7% cut in the ratio\nof defective cold start ads with satisfactory real-time serving latency.",
        "pdf_link": "https://arxiv.org/pdf/2209.00625v1.pdf"
    },
    {
        "title": "Personal Attribute Prediction from Conversations",
        "authors": [
            "Yinan Liu",
            "Hu Chen",
            "Wei Shen"
        ],
        "published": "2022-08-29T15:21:53Z",
        "summary": "Personal knowledge bases (PKBs) are critical to many applications, such as\nWeb-based chatbots and personalized recommendation. Conversations containing\nrich personal knowledge can be regarded as a main source to populate the PKB.\nGiven a user, a user attribute, and user utterances from a conversational\nsystem, we aim to predict the personal attribute value for the user, which is\nhelpful for the enrichment of PKBs. However, there are three issues existing in\nprevious studies: (1) manually labeled utterances are required for model\ntraining; (2) personal attribute knowledge embedded in both utterances and\nexternal resources is underutilized; (3) the performance on predicting some\ndifficult personal attributes is unsatisfactory. In this paper, we propose a\nframework DSCGN based on the pre-trained language model with a noise-robust\nloss function to predict personal attributes from conversations without\nrequiring any labeled utterances. We yield two categories of supervision, i.e.,\ndocument-level supervision via a distant supervision strategy and\ncontextualized word-level supervision via a label guessing method, by mining\nthe personal attribute knowledge embedded in both unlabeled utterances and\nexternal resources to fine-tune the language model. Extensive experiments over\ntwo real-world data sets (i.e., a profession data set and a hobby data set)\nshow our framework obtains the best performance compared with all the twelve\nbaselines in terms of nDCG and MRR.",
        "pdf_link": "https://arxiv.org/pdf/2209.09619v1.pdf"
    },
    {
        "title": "LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems",
        "authors": [
            "Bj\u00f6rn Deiseroth",
            "Patrick Schramowski",
            "Hikaru Shindo",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "published": "2022-08-29T11:40:36Z",
        "summary": "Text-to-image models have recently achieved remarkable success with seemingly\naccurate samples in photo-realistic quality. However as state-of-the-art\nlanguage models still struggle evaluating precise statements consistently, so\ndo language model based image generation processes. In this work we showcase\nproblems of state-of-the-art text-to-image models like DALL-E with generating\naccurate samples from statements related to the draw bench benchmark.\nFurthermore we show that CLIP is not able to rerank those generated samples\nconsistently. To this end we propose LogicRank, a neuro-symbolic reasoning\nframework that can result in a more accurate ranking-system for such\nprecision-demanding settings. LogicRank integrates smoothly into the generation\nprocess of text-to-image models and moreover can be used to further fine-tune\ntowards a more logical precise model.",
        "pdf_link": "https://arxiv.org/pdf/2208.13518v1.pdf"
    },
    {
        "title": "Multi-dimensional Racism Classification during COVID-19: Stigmatization, Offensiveness, Blame, and Exclusion",
        "authors": [
            "Xin Pei",
            "Deval Mehta"
        ],
        "published": "2022-08-29T00:38:56Z",
        "summary": "Transcending the binary categorization of racist texts, our study takes cues\nfrom social science theories to develop a multi-dimensional model for racism\ndetection, namely stigmatization, offensiveness, blame, and exclusion. With the\naid of BERT and topic modeling, this categorical detection enables insights\ninto the underlying subtlety of racist discussion on digital platforms during\nCOVID-19. Our study contributes to enriching the scholarly discussion on\ndeviant racist behaviours on social media. First, a stage-wise analysis is\napplied to capture the dynamics of the topic changes across the early stages of\nCOVID-19 which transformed from a domestic epidemic to an international public\nhealth emergency and later to a global pandemic. Furthermore, mapping this\ntrend enables a more accurate prediction of public opinion evolvement\nconcerning racism in the offline world, and meanwhile, the enactment of\nspecified intervention strategies to combat the upsurge of racism during the\nglobal public health crisis like COVID-19. In addition, this interdisciplinary\nresearch also points out a direction for future studies on social network\nanalysis and mining. Integration of social science perspectives into the\ndevelopment of computational methods provides insights into more accurate data\ndetection and analytics.",
        "pdf_link": "https://arxiv.org/pdf/2208.13318v1.pdf"
    },
    {
        "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
        "authors": [
            "Kaizhi Zheng",
            "Kaiwen Zhou",
            "Jing Gu",
            "Yue Fan",
            "Jialu Wang",
            "Zonglin Di",
            "Xuehai He",
            "Xin Eric Wang"
        ],
        "published": "2022-08-28T18:30:46Z",
        "summary": "Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge.",
        "pdf_link": "https://arxiv.org/pdf/2208.13266v3.pdf"
    },
    {
        "title": "Target Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization",
        "authors": [
            "Dongmei Wang",
            "Xiong Xiao",
            "Naoyuki Kanda",
            "Takuya Yoshioka",
            "Jian Wu"
        ],
        "published": "2022-08-27T21:11:45Z",
        "summary": "This paper describes a speaker diarization model based on target speaker\nvoice activity detection (TS-VAD) using transformers. To overcome the original\nTS-VAD model's drawback of being unable to handle an arbitrary number of\nspeakers, we investigate model architectures that use input tensors with\nvariable-length time and speaker dimensions. Transformer layers are applied to\nthe speaker axis to make the model output insensitive to the order of the\nspeaker profiles provided to the TS-VAD model. Time-wise sequential layers are\ninterspersed between these speaker-wise transformer layers to allow the\ntemporal and cross-speaker correlations of the input speech signal to be\ncaptured. We also extend a diarization model based on end-to-end neural\ndiarization with encoder-decoder based attractors (EEND-EDA) by replacing its\ndot-product-based speaker detection layer with the transformer-based TS-VAD.\nExperimental results on VoxConverse show that using the transformers for the\ncross-speaker modeling reduces the diarization error rate (DER) of TS-VAD by\n11.3%, achieving a new state-of-the-art (SOTA) DER of 4.57%. Also, our extended\nEEND-EDA reduces DER by 6.9% on the CALLHOME dataset relative to the original\nEEND-EDA with a similar model size, achieving a new SOTA DER of 11.18% under a\nwidely used training data setting.",
        "pdf_link": "https://arxiv.org/pdf/2208.13085v3.pdf"
    },
    {
        "title": "On Unsupervised Training of Link Grammar Based Language Models",
        "authors": [
            "Nikolay Mikhaylovskiy"
        ],
        "published": "2022-08-27T14:07:24Z",
        "summary": "In this short note we explore what is needed for the unsupervised training of\ngraph language models based on link grammars. First, we introduce the\nter-mination tags formalism required to build a language model based on a link\ngrammar formalism of Sleator and Temperley [21] and discuss the influence of\ncontext on the unsupervised learning of link grammars. Second, we pro-pose a\nstatistical link grammar formalism, allowing for statistical language\ngeneration. Third, based on the above formalism, we show that the classical\ndissertation of Yuret [25] on discovery of linguistic relations using lexical\nat-traction ignores contextual properties of the language, and thus the\napproach to unsupervised language learning relying just on bigrams is flawed.\nThis correlates well with the unimpressive results in unsupervised training of\ngraph language models based on bigram approach of Yuret.",
        "pdf_link": "https://arxiv.org/pdf/2208.13021v1.pdf"
    },
    {
        "title": "Task-specific Pre-training and Prompt Decomposition for Knowledge Graph Population with Language Models",
        "authors": [
            "Tianyi Li",
            "Wenyu Huang",
            "Nikos Papasarantopoulos",
            "Pavlos Vougiouklis",
            "Jeff Z. Pan"
        ],
        "published": "2022-08-26T09:56:27Z",
        "summary": "We present a system for knowledge graph population with Language Models,\nevaluated on the Knowledge Base Construction from Pre-trained Language Models\n(LM-KBC) challenge at ISWC 2022. Our system involves task-specific pre-training\nto improve LM representation of the masked object tokens, prompt decomposition\nfor progressive generation of candidate objects, among other methods for\nhigher-quality retrieval. Our system is the winner of track 1 of the LM-KBC\nchallenge, based on BERT LM; it achieves 55.0% F-1 score on the hidden test set\nof the challenge.",
        "pdf_link": "https://arxiv.org/pdf/2208.12539v2.pdf"
    },
    {
        "title": "Extracting Biomedical Factual Knowledge Using Pretrained Language Model and Electronic Health Record Context",
        "authors": [
            "Zonghai Yao",
            "Yi Cao",
            "Zhichao Yang",
            "Vijeta Deshpande",
            "Hong Yu"
        ],
        "published": "2022-08-26T00:01:26Z",
        "summary": "Language Models (LMs) have performed well on biomedical natural language\nprocessing applications. In this study, we conducted some experiments to use\nprompt methods to extract knowledge from LMs as new knowledge Bases (LMs as\nKBs). However, prompting can only be used as a low bound for knowledge\nextraction, and perform particularly poorly on biomedical domain KBs. In order\nto make LMs as KBs more in line with the actual application scenarios of the\nbiomedical domain, we specifically add EHR notes as context to the prompt to\nimprove the low bound in the biomedical domain. We design and validate a series\nof experiments for our Dynamic-Context-BioLAMA task. Our experiments show that\nthe knowledge possessed by those language models can distinguish the correct\nknowledge from the noise knowledge in the EHR notes, and such distinguishing\nability can also be used as a new metric to evaluate the amount of knowledge\npossessed by the model.",
        "pdf_link": "https://arxiv.org/pdf/2209.07859v2.pdf"
    },
    {
        "title": "Training a T5 Using Lab-sized Resources",
        "authors": [
            "Manuel R. Ciosici",
            "Leon Derczynski"
        ],
        "published": "2022-08-25T13:55:16Z",
        "summary": "Training large neural language models on large datasets is resource- and\ntime-intensive. These requirements create a barrier to entry, where those with\nfewer resources cannot build competitive models. This paper presents various\ntechniques for making it possible to (a) train a large language model using\nresources that a modest research lab might have, and (b) train it in a\nreasonable amount of time. We provide concrete recommendations for\npractitioners, which we illustrate with a case study: a T5 model for Danish,\nthe first for this language.",
        "pdf_link": "https://arxiv.org/pdf/2208.12097v1.pdf"
    },
    {
        "title": "On Reality and the Limits of Language Data: Aligning LLMs with Human Norms",
        "authors": [
            "Nigel H. Collier",
            "Fangyu Liu",
            "Ehsan Shareghi"
        ],
        "published": "2022-08-25T10:21:23Z",
        "summary": "Recent advancements in Large Language Models (LLMs) harness linguistic\nassociations in vast natural language data for practical applications. However,\ntheir ability to understand the physical world using only language data remains\na question. After reviewing existing protocols, we explore this question using\na novel and tightly controlled reasoning test (ART) and compare human norms\nagainst versions of GPT-3. Our findings highlight the categories of\ncommon-sense relations models that could learn directly from data and areas of\nweakness. GPT-3 offers evidence for verbal reasoning on a par with human\nsubjects for several relations including Synonymy, Antonymy, and Default\ninheritance, Without reinforcement learning from human judgements, it appears\nGPT-3 performs at the lower end of the reference interval for Has-part and\nContained-in. Weaknesses were observed also in affordance characteristics\nthrough Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs\nwith symbolic world grounding is a promising direction to address associative\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2208.11981v2.pdf"
    },
    {
        "title": "Shortcut Learning of Large Language Models in Natural Language Understanding",
        "authors": [
            "Mengnan Du",
            "Fengxiang He",
            "Na Zou",
            "Dacheng Tao",
            "Xia Hu"
        ],
        "published": "2022-08-25T03:51:39Z",
        "summary": "Large language models (LLMs) have achieved state-of-the-art performance on a\nseries of natural language understanding tasks. However, these LLMs might rely\non dataset bias and artifacts as shortcuts for prediction. This has\nsignificantly affected their generalizability and adversarial robustness. In\nthis paper, we provide a review of recent developments that address the\nshortcut learning and robustness challenge of LLMs. We first introduce the\nconcepts of shortcut learning of language models. We then introduce methods to\nidentify shortcut learning behavior in language models, characterize the\nreasons for shortcut learning, as well as introduce mitigation solutions.\nFinally, we discuss key research challenges and potential research directions\nin order to advance the field of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2208.11857v2.pdf"
    },
    {
        "title": "Addressing Token Uniformity in Transformers via Singular Value Transformation",
        "authors": [
            "Hanqi Yan",
            "Lin Gui",
            "Wenjie Li",
            "Yulan He"
        ],
        "published": "2022-08-24T22:44:09Z",
        "summary": "Token uniformity is commonly observed in transformer-based models, in which\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer. In this paper,\nwe propose to use the distribution of singular values of outputs of each\ntransformer layer to characterise the phenomenon of token uniformity and\nempirically illustrate that a less skewed singular value distribution can\nalleviate the `token uniformity' problem. Base on our observations, we define\nseveral desirable properties of singular value distributions and propose a\nnovel transformation function for updating the singular values. We show that\napart from alleviating token uniformity, the transformation function should\npreserve the local neighbourhood structure in the original embedding space. Our\nproposed singular value transformation function is applied to a range of\ntransformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT,\nand improved performance is observed in semantic textual similarity evaluation\nand a range of GLUE tasks. Our source code is available at\nhttps://github.com/hanqi-qi/tokenUni.git.",
        "pdf_link": "https://arxiv.org/pdf/2208.11790v2.pdf"
    },
    {
        "title": "Learning from Unlabeled 3D Environments for Vision-and-Language Navigation",
        "authors": [
            "Shizhe Chen",
            "Pierre-Louis Guhur",
            "Makarand Tapaswi",
            "Cordelia Schmid",
            "Ivan Laptev"
        ],
        "published": "2022-08-24T21:50:20Z",
        "summary": "In vision-and-language navigation (VLN), an embodied agent is required to\nnavigate in realistic 3D environments following natural language instructions.\nOne major bottleneck for existing VLN approaches is the lack of sufficient\ntraining data, resulting in unsatisfactory generalization to unseen\nenvironments. While VLN data is typically collected manually, such an approach\nis expensive and prevents scalability. In this work, we address the data\nscarcity issue by proposing to automatically create a large-scale VLN dataset\nfrom 900 unlabeled 3D buildings from HM3D. We generate a navigation graph for\neach building and transfer object predictions from 2D to generate pseudo 3D\nobject labels by cross-view consistency. We then fine-tune a pretrained\nlanguage model using pseudo object labels as prompts to alleviate the\ncross-modal gap in instruction generation. Our resulting HM3D-AutoVLN dataset\nis an order of magnitude larger than existing VLN datasets in terms of\nnavigation environments and instructions. We experimentally demonstrate that\nHM3D-AutoVLN significantly increases the generalization ability of resulting\nVLN models. On the SPL metric, our approach improves over state of the art by\n7.1% and 8.1% on the unseen validation splits of REVERIE and SOON datasets\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2208.11781v1.pdf"
    },
    {
        "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages",
        "authors": [
            "Tahir Javed",
            "Kaushal Santosh Bhogale",
            "Abhigyan Raman",
            "Anoop Kunchukuttan",
            "Pratyush Kumar",
            "Mitesh M. Khapra"
        ],
        "published": "2022-08-24T20:14:52Z",
        "summary": "A cornerstone in AI research has been the creation and adoption of\nstandardized training and test datasets to earmark the progress of\nstate-of-the-art models. A particularly successful example is the GLUE dataset\nfor training and evaluating Natural Language Understanding (NLU) models for\nEnglish. The large body of research around self-supervised BERT-based language\nmodels revolved around performance improvements on NLU tasks in GLUE. To\nevaluate language models in other languages, several language-specific GLUE\ndatasets were created. The area of speech language understanding (SLU) has\nfollowed a similar trajectory. The success of large self-supervised models such\nas wav2vec2 enable creation of speech models with relatively easy to access\nunlabelled data. These models can then be evaluated on SLU tasks, such as the\nSUPERB benchmark. In this work, we extend this to Indic languages by releasing\nthe IndicSUPERB benchmark. Specifically, we make the following three\ncontributions. (i) We collect Kathbath containing 1,684 hours of labelled\nspeech data across 12 Indian languages from 1,218 contributors located in 203\ndistricts in India. (ii) Using Kathbath, we create benchmarks across 6 speech\ntasks: Automatic Speech Recognition, Speaker Verification, Speaker\nIdentification (mono/multi), Language Identification, Query By Example, and\nKeyword Spotting for 12 languages. (iii) On the released benchmarks, we train\nand evaluate different self-supervised models alongside a commonly used\nbaseline FBANK. We show that language-specific fine-tuned models are more\naccurate than baseline on most of the tasks, including a large gap of 76\\% for\nthe Language Identification task. However, for speaker identification,\nself-supervised models trained on large datasets demonstrate an advantage. We\nhope IndicSUPERB contributes to the progress of developing speech language\nunderstanding models for Indian languages.",
        "pdf_link": "https://arxiv.org/pdf/2208.11761v2.pdf"
    },
    {
        "title": "Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model",
        "authors": [
            "Yixiao Zhang",
            "Junyan Jiang",
            "Gus Xia",
            "Simon Dixon"
        ],
        "published": "2022-08-24T17:07:37Z",
        "summary": "Lyric interpretations can help people understand songs and their lyrics\nquickly, and can also make it easier to manage, retrieve and discover songs\nefficiently from the growing mass of music archives. In this paper we propose\nBART-fusion, a novel model for generating lyric interpretations from lyrics and\nmusic audio that combines a large-scale pre-trained language model with an\naudio encoder. We employ a cross-modal attention module to incorporate the\naudio representation into the lyrics representation to help the pre-trained\nlanguage model understand the song from an audio perspective, while preserving\nthe language model's original generative performance. We also release the Song\nInterpretation Dataset, a new large-scale dataset for training and evaluating\nour model. Experimental results show that the additional audio information\nhelps our model to understand words and music better, and to generate precise\nand fluent interpretations. An additional experiment on cross-modal music\nretrieval shows that interpretations generated by BART-fusion can also help\npeople retrieve music more accurately than with the original BART.",
        "pdf_link": "https://arxiv.org/pdf/2208.11671v1.pdf"
    },
    {
        "title": "PEER: A Collaborative Language Model",
        "authors": [
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Zhengbao Jiang",
            "Fabio Petroni",
            "Patrick Lewis",
            "Gautier Izacard",
            "Qingfei You",
            "Christoforos Nalmpantis",
            "Edouard Grave",
            "Sebastian Riedel"
        ],
        "published": "2022-08-24T16:56:47Z",
        "summary": "Textual content is often the output of a collaborative writing process: We\nstart with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today's language models are trained to generate only\nthe final result. As a consequence, they lack several abilities crucial for\ncollaborative writing: They are unable to update existing texts, difficult to\ncontrol and incapable of verbally planning or explaining their actions. To\naddress these shortcomings, we introduce PEER, a collaborative language model\nthat is trained to imitate the entire writing process itself: PEER can write\ndrafts, add suggestions, propose edits and provide explanations for its\nactions. Crucially, we train multiple instances of PEER able to infill various\nparts of the writing process, enabling the use of self-training techniques for\nincreasing the quality, amount and diversity of training data. This unlocks\nPEER's full potential by making it applicable in domains for which no edit\nhistories are available and improving its ability to follow instructions, to\nwrite useful comments, and to explain its actions. We show that PEER achieves\nstrong performance across various domains and editing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.11663v1.pdf"
    },
    {
        "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
        "authors": [
            "Harshit Joshi",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Ivan Radicek",
            "Gust Verbruggen"
        ],
        "published": "2022-08-24T16:25:58Z",
        "summary": "Most programmers make mistakes when writing code. Some of these mistakes are\nsmall and require few edits to the original program -- a class of errors\nrecently termed last mile mistakes. These errors break the flow for experienced\ndevelopers and can stump novice programmers. Existing automated repair\ntechniques targeting this class of errors are language-specific and do not\neasily carry over to new languages. Transferring symbolic approaches requires\nsubstantial engineering and neural approaches require data and retraining. We\nintroduce RING, a multilingual repair engine powered by a large language model\ntrained on code (LLMC) such as Codex. Such a multilingual engine enables a\nflipped model for programming assistance, one where the programmer writes code\nand the AI assistance suggests fixes, compared to traditional code suggestion\ntechnology. Taking inspiration from the way programmers manually fix bugs, we\nshow that a prompt-based strategy that conceptualizes repair as localization,\ntransformation, and candidate ranking, can successfully repair programs in\nmultiple languages with minimal effort. We present the first results for such a\nmultilingual repair engine by evaluating on 6 different languages and comparing\nperformance to language-specific repair engines. We show that RING can\noutperform language-specific repair engines for three of these languages.",
        "pdf_link": "https://arxiv.org/pdf/2208.11640v3.pdf"
    },
    {
        "title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval",
        "authors": [
            "Zhengyang Tang",
            "Benyou Wang",
            "Ting Yao"
        ],
        "published": "2022-08-24T12:55:00Z",
        "summary": "Deep prompt tuning (DPT) has gained great success in most natural language\nprocessing~(NLP) tasks. However, it is not well-investigated in dense retrieval\nwhere fine-tuning~(FT) still dominates. When deploying multiple retrieval tasks\nusing the same backbone model~(e.g., RoBERTa), FT-based methods are unfriendly\nin terms of deployment cost: each new retrieval model needs to repeatedly\ndeploy the backbone model without reuse. To reduce the deployment cost in such\na scenario, this work investigates applying DPT in dense retrieval. The\nchallenge is that directly applying DPT in dense retrieval largely\nunderperforms FT methods. To compensate for the performance drop, we propose\ntwo model-agnostic and task-agnostic strategies for DPT-based retrievers,\nnamely retrieval-oriented intermediate pretraining and unified negative mining,\nas a general approach that could be compatible with any pre-trained language\nmodel and retrieval task. The experimental results show that the proposed\nmethod (called DPTDR) outperforms previous state-of-the-art models on both\nMS-MARCO and Natural Questions. We also conduct ablation studies to examine the\neffectiveness of each strategy in DPTDR. We believe this work facilitates the\nindustry, as it saves enormous efforts and costs of deployment and increases\nthe utility of computing resources. Our code is available at\nhttps://github.com/tangzhy/DPTDR.",
        "pdf_link": "https://arxiv.org/pdf/2208.11503v1.pdf"
    },
    {
        "title": "Ontology-Driven Self-Supervision for Adverse Childhood Experiences Identification Using Social Media Datasets",
        "authors": [
            "Jinge Wu",
            "Rowena Smith",
            "Honghan Wu"
        ],
        "published": "2022-08-24T12:23:01Z",
        "summary": "Adverse Childhood Experiences (ACEs) are defined as a collection of highly\nstressful, and potentially traumatic, events or circumstances that occur\nthroughout childhood and/or adolescence. They have been shown to be associated\nwith increased risks of mental health diseases or other abnormal behaviours in\nlater lives. However, the identification of ACEs from textual data with Natural\nLanguage Processing (NLP) is challenging because (a) there are no NLP ready ACE\nontologies; (b) there are few resources available for machine learning,\nnecessitating the data annotation from clinical experts; (c) costly annotations\nby domain experts and large number of documents for supporting large machine\nlearning models. In this paper, we present an ontology-driven self-supervised\napproach (derive concept embeddings using an auto-encoder from baseline NLP\nresults) for producing a publicly available resource that would support\nlarge-scale machine learning (e.g., training transformer based large language\nmodels) on social media corpus. This resource as well as the proposed approach\nare aimed to facilitate the community in training transferable NLP models for\neffectively surfacing ACEs in low-resource scenarios like NLP on clinical notes\nwithin Electronic Health Records. The resource including a list of ACE ontology\nterms, ACE concept embeddings and the NLP annotated corpus is available at\nhttps://github.com/knowlab/ACE-NLP.",
        "pdf_link": "https://arxiv.org/pdf/2208.11701v1.pdf"
    },
    {
        "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models",
        "authors": [
            "Mirelle Bueno",
            "Carlos Gemmell",
            "Jeffrey Dalton",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-08-24T11:25:27Z",
        "summary": "The ability to extrapolate, i.e., to make predictions on sequences that are\nlonger than those presented as training examples, is a challenging problem for\ncurrent deep learning models. Recent work shows that this limitation persists\nin state-of-the-art Transformer-based models. Most solutions to this problem\nuse specific architectures or training methods that do not generalize to other\ntasks. We demonstrate that large language models can succeed in extrapolation\nwithout modifying their architecture or training procedure. Our experimental\nresults show that generating step-by-step rationales and introducing marker\ntokens are both required for effective extrapolation. First, we induce a\nlanguage model to produce step-by-step rationales before outputting the answer\nto effectively communicate the task to the model. However, as sequences become\nlonger, we find that current models struggle to keep track of token positions.\nTo address this issue, we interleave output tokens with markup tokens that act\nas explicit positional and counting symbols. Our findings show how these two\ncomplementary approaches enable remarkable sequence extrapolation and highlight\na limitation of current architectures to effectively generalize without\nexplicit surface form guidance. Code available at\nhttps://github.com/MirelleB/induced-rationales-markup-tokens",
        "pdf_link": "https://arxiv.org/pdf/2208.11445v3.pdf"
    },
    {
        "title": "Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration",
        "authors": [
            "Zijian Gao",
            "YiYing Li",
            "Kele Xu",
            "Yuanzhao Zhai",
            "Dawei Feng",
            "Bo Ding",
            "XinJun Mao",
            "Huaimin Wang"
        ],
        "published": "2022-08-24T07:56:12Z",
        "summary": "The sparsity of extrinsic rewards poses a serious challenge for reinforcement\nlearning (RL). Currently, many efforts have been made on curiosity which can\nprovide a representative intrinsic reward for effective exploration. However,\nthe challenge is still far from being solved. In this paper, we present a novel\ncuriosity for RL, named DyMeCu, which stands for Dynamic Memory-based\nCuriosity. Inspired by human curiosity and information theory, DyMeCu consists\nof a dynamic memory and dual online learners. The curiosity arouses if\nmemorized information can not deal with the current state, and the information\ngap between dual learners can be formulated as the intrinsic reward for agents,\nand then such state information can be consolidated into the dynamic memory.\nCompared with previous curiosity methods, DyMeCu can better mimic human\ncuriosity with dynamic memory, and the memory module can be dynamically grown\nbased on a bootstrap paradigm with dual learners. On multiple benchmarks\nincluding DeepMind Control Suite and Atari Suite, large-scale empirical\nexperiments are conducted and the results demonstrate that DyMeCu outperforms\ncompetitive curiosity-based methods with or without extrinsic rewards. We will\nrelease the code to enhance reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2208.11349v2.pdf"
    },
    {
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
        "authors": [
            "Deep Ganguli",
            "Liane Lovitt",
            "Jackson Kernion",
            "Amanda Askell",
            "Yuntao Bai",
            "Saurav Kadavath",
            "Ben Mann",
            "Ethan Perez",
            "Nicholas Schiefer",
            "Kamal Ndousse",
            "Andy Jones",
            "Sam Bowman",
            "Anna Chen",
            "Tom Conerly",
            "Nova DasSarma",
            "Dawn Drain",
            "Nelson Elhage",
            "Sheer El-Showk",
            "Stanislav Fort",
            "Zac Hatfield-Dodds",
            "Tom Henighan",
            "Danny Hernandez",
            "Tristan Hume",
            "Josh Jacobson",
            "Scott Johnston",
            "Shauna Kravec",
            "Catherine Olsson",
            "Sam Ringer",
            "Eli Tran-Johnson",
            "Dario Amodei",
            "Tom Brown",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Chris Olah",
            "Jared Kaplan",
            "Jack Clark"
        ],
        "published": "2022-08-23T23:37:14Z",
        "summary": "We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.",
        "pdf_link": "https://arxiv.org/pdf/2209.07858v2.pdf"
    },
    {
        "title": "Prompting as Probing: Using Language Models for Knowledge Base Construction",
        "authors": [
            "Dimitrios Alivanistos",
            "Selene B\u00e1ez Santamar\u00eda",
            "Michael Cochez",
            "Jan-Christoph Kalo",
            "Emile van Krieken",
            "Thiviyan Thanapalasingam"
        ],
        "published": "2022-08-23T16:03:50Z",
        "summary": "Language Models (LMs) have proven to be useful in various downstream\napplications, such as summarisation, translation, question answering and text\nclassification. LMs are becoming increasingly important tools in Artificial\nIntelligence, because of the vast quantity of information they can store. In\nthis work, we present ProP (Prompting as Probing), which utilizes GPT-3, a\nlarge Language Model originally proposed by OpenAI in 2020, to perform the task\nof Knowledge Base Construction (KBC). ProP implements a multi-step approach\nthat combines a variety of prompting techniques to achieve this. Our results\nshow that manual prompt curation is essential, that the LM must be encouraged\nto give answer sets of variable lengths, in particular including empty answer\nsets, that true/false questions are a useful device to increase precision on\nsuggestions generated by the LM, that the size of the LM is a crucial factor,\nand that a dictionary of entity aliases improves the LM score. Our evaluation\nstudy indicates that these proposed techniques can substantially enhance the\nquality of the final predictions: ProP won track 2 of the LM-KBC competition,\noutperforming the baseline by 36.4 percentage points. Our implementation is\navailable on https://github.com/HEmile/iswc-challenge.",
        "pdf_link": "https://arxiv.org/pdf/2208.11057v3.pdf"
    },
    {
        "title": "Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning",
        "authors": [
            "Letian Peng",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "published": "2022-08-23T14:42:14Z",
        "summary": "Commonsense reasoning is an appealing topic in natural language processing\n(NLP) as it plays a fundamental role in supporting the human-like actions of\nNLP systems. With large-scale language models as the backbone, unsupervised\npre-training on numerous corpora shows the potential to capture commonsense\nknowledge. Current pre-trained language model (PLM)-based reasoning follows the\ntraditional practice using perplexity metric. However, commonsense reasoning is\nmore than existing probability evaluation, which is biased by word frequency.\nThis paper reconsiders the nature of commonsense reasoning and proposes a novel\ncommonsense reasoning metric, Non-Replacement Confidence (NRC). In detail, it\nworks on PLMs according to the Replaced Token Detection (RTD) pre-training\nobjective in ELECTRA, in which the corruption detection objective reflects the\nconfidence on contextual integrity that is more relevant to commonsense\nreasoning than existing probability. Our proposed novel method boosts zero-shot\nperformance on two commonsense reasoning benchmark datasets and further seven\ncommonsense question-answering datasets. Our analysis shows that pre-endowed\ncommonsense knowledge, especially for RTD-based PLMs, is essential in\ndownstream reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2208.11007v1.pdf"
    },
    {
        "title": "CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations",
        "authors": [
            "Borun Chen",
            "Hongyin Tang",
            "Jiahao Bu",
            "Kai Zhang",
            "Jingang Wang",
            "Qifan Wang",
            "Hai-Tao Zheng",
            "Wei Wu",
            "Liqian Yu"
        ],
        "published": "2022-08-23T09:52:34Z",
        "summary": "Pre-trained Language Models (PLMs) have achieved remarkable performance gains\nacross numerous downstream tasks in natural language understanding. Various\nChinese PLMs have been successively proposed for learning better Chinese\nlanguage representation. However, most current models use Chinese characters as\ninputs and are not able to encode semantic information contained in Chinese\nwords. While recent pre-trained models incorporate both words and characters\nsimultaneously, they usually suffer from deficient semantic interactions and\nfail to capture the semantic relation between words and characters. To address\nthe above issues, we propose a simple yet effective PLM CLOWER, which adopts\nthe Contrastive Learning Over Word and charactER representations. In\nparticular, CLOWER implicitly encodes the coarse-grained information (i.e.,\nwords) into the fine-grained representations (i.e., characters) through\ncontrastive learning on multi-grained information. CLOWER is of great value in\nrealistic scenarios since it can be easily incorporated into any existing\nfine-grained based PLMs without modifying the production pipelines.Extensive\nexperiments conducted on a range of downstream tasks demonstrate the superior\nperformance of CLOWER over several state-of-the-art baselines.",
        "pdf_link": "https://arxiv.org/pdf/2208.10844v2.pdf"
    },
    {
        "title": "Multimodal Crop Type Classification Fusing Multi-Spectral Satellite Time Series with Farmers Crop Rotations and Local Crop Distribution",
        "authors": [
            "Valentin Barriere",
            "Martin Claverie"
        ],
        "published": "2022-08-23T09:41:09Z",
        "summary": "Accurate, detailed, and timely crop type mapping is a very valuable\ninformation for the institutions in order to create more accurate policies\naccording to the needs of the citizens. In the last decade, the amount of\navailable data dramatically increased, whether it can come from Remote Sensing\n(using Copernicus Sentinel-2 data) or directly from the farmers (providing\nin-situ crop information throughout the years and information on crop\nrotation). Nevertheless, the majority of the studies are restricted to the use\nof one modality (Remote Sensing data or crop rotation) and never fuse the Earth\nObservation data with domain knowledge like crop rotations. Moreover, when they\nuse Earth Observation data they are mainly restrained to one year of data, not\ntaking into account the past years. In this context, we propose to tackle a\nland use and crop type classification task using three data types, by using a\nHierarchical Deep Learning algorithm modeling the crop rotations like a\nlanguage model, the satellite signals like a speech signal and using the crop\ndistribution as additional context vector. We obtained very promising results\ncompared to classical approaches with significant performances, increasing the\nAccuracy by 5.1 points in a 28-class setting (.948), and the micro-F1 by 9.6\npoints in a 10-class setting (.887) using only a set of crop of interests\nselected by an expert. We finally proposed a data-augmentation technique to\nallow the model to classify the crop before the end of the season, which works\nsurprisingly well in a multimodal setting.",
        "pdf_link": "https://arxiv.org/pdf/2208.10838v1.pdf"
    },
    {
        "title": "GenTUS: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers",
        "authors": [
            "Hsien-Chin Lin",
            "Christian Geishauser",
            "Shutong Feng",
            "Nurul Lubis",
            "Carel van Niekerk",
            "Michael Heck",
            "Milica Ga\u0161i\u0107"
        ],
        "published": "2022-08-23T09:01:17Z",
        "summary": "User simulators (USs) are commonly used to train task-oriented dialogue\nsystems (DSs) via reinforcement learning. The interactions often take place on\nsemantic level for efficiency, but there is still a gap from semantic actions\nto natural language, which causes a mismatch between training and deployment\nenvironment. Incorporating a natural language generation (NLG) module with USs\nduring training can partly deal with this problem. However, since the policy\nand NLG of USs are optimised separately, these simulated user utterances may\nnot be natural enough in a given context. In this work, we propose a generative\ntransformer-based user simulator (GenTUS). GenTUS consists of an\nencoder-decoder structure, which means it can optimise both the user policy and\nnatural language generation jointly. GenTUS generates both semantic actions and\nnatural language utterances, preserving interpretability and enhancing language\nvariation. In addition, by representing the inputs and outputs as word\nsequences and by using a large pre-trained language model we can achieve\ngeneralisability in feature representation. We evaluate GenTUS with automatic\nmetrics and human evaluation. Our results show that GenTUS generates more\nnatural language and is able to transfer to an unseen ontology in a zero-shot\nfashion. In addition, its behaviour can be further shaped with reinforcement\nlearning opening the door to training specialised user simulators.",
        "pdf_link": "https://arxiv.org/pdf/2208.10817v1.pdf"
    },
    {
        "title": "Learning Better Masking for Better Language Model Pre-training",
        "authors": [
            "Dongjie Yang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022-08-23T08:27:52Z",
        "summary": "Masked Language Modeling (MLM) has been widely used as the denoising\nobjective in pre-training language models (PrLMs). Existing PrLMs commonly\nadopt a Random-Token Masking strategy where a fixed masking ratio is applied\nand different contents are masked by an equal probability throughout the entire\ntraining. However, the model may receive complicated impact from pre-training\nstatus, which changes accordingly as training time goes on. In this paper, we\nshow that such time-invariant MLM settings on masking ratio and masked content\nare unlikely to deliver an optimal outcome, which motivates us to explore the\ninfluence of time-variant MLM settings. We propose two scheduled masking\napproaches that adaptively tune the masking ratio and masked content in\ndifferent training stages, which improves the pre-training efficiency and\neffectiveness verified on the downstream tasks. Our work is a pioneer study on\ntime-variant masking strategy on ratio and content and gives a better\nunderstanding of how masking ratio and masked content influence the MLM\npre-training.",
        "pdf_link": "https://arxiv.org/pdf/2208.10806v3.pdf"
    },
    {
        "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
        "authors": [
            "Xiaohang Tang",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2022-08-23T05:12:27Z",
        "summary": "Dynamic contextualised word embeddings (DCWEs) represent the temporal\nsemantic variations of words. We propose a method for learning DCWEs by\ntime-adapting a pretrained Masked Language Model (MLM) using time-sensitive\ntemplates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively\nat two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised\nmethod to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and\n(b) \\emph{anchor} terms that are associated with a specific pivot term in each\nindividual snapshot. We then generate prompts by filling manually compiled\ntemplates using the extracted pivot and anchor terms. Moreover, we propose an\nautomatic method to learn time-sensitive templates from $C_1$ and $C_2$,\nwithout requiring any human supervision. Next, we use the generated prompts to\nadapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple\nexperiments show that our proposed method reduces the perplexity of test\nsentences in $C_2$, outperforming the current state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2208.10734v3.pdf"
    },
    {
        "title": "K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment",
        "authors": [
            "Jean Lee",
            "Taejun Lim",
            "Heejun Lee",
            "Bogeun Jo",
            "Yangsok Kim",
            "Heegeun Yoon",
            "Soyeon Caren Han"
        ],
        "published": "2022-08-23T02:10:53Z",
        "summary": "Online hate speech detection has become an important issue due to the growth\nof online content, but resources in languages other than English are extremely\nlimited. We introduce K-MHaS, a new multi-label dataset for hate speech\ndetection that effectively handles Korean language patterns. The dataset\nconsists of 109k utterances from news comments and provides a multi-label\nclassification using 1 to 4 labels, and handles subjectivity and\nintersectionality. We evaluate strong baseline experiments on K-MHaS using\nKorean-BERT-based language models with six different metrics. KR-BERT with a\nsub-character tokenizer outperforms others, recognizing decomposed characters\nin each hate speech class.",
        "pdf_link": "https://arxiv.org/pdf/2208.10684v3.pdf"
    },
    {
        "title": "Interpreting Embedding Spaces by Conceptualization",
        "authors": [
            "Adi Simhi",
            "Shaul Markovitch"
        ],
        "published": "2022-08-22T15:32:17Z",
        "summary": "One of the main methods for computational interpretation of a text is mapping\nit into a vector in some embedding space. Such vectors can then be used for a\nvariety of textual processing tasks. Recently, most embedding spaces are a\nproduct of training large language models (LLMs). One major drawback of this\ntype of representation is their incomprehensibility to humans. Understanding\nthe embedding space is crucial for several important needs, including the need\nto debug the embedding method and compare it to alternatives, and the need to\ndetect biases hidden in the model. In this paper, we present a novel method of\nunderstanding embeddings by transforming a latent embedding space into a\ncomprehensible conceptual space. We present an algorithm for deriving a\nconceptual space with dynamic on-demand granularity. We devise a new evaluation\nmethod, using either human rater or LLM-based raters, to show that the\nconceptualized vectors indeed represent the semantics of the original latent\nones. We show the use of our method for various tasks, including comparing the\nsemantics of alternative models and tracing the layers of the LLM. The code is\navailable online\nhttps://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.",
        "pdf_link": "https://arxiv.org/pdf/2209.00445v3.pdf"
    },
    {
        "title": "Selection Collider Bias in Large Language Models",
        "authors": [
            "Emily McMilin"
        ],
        "published": "2022-08-22T05:38:15Z",
        "summary": "In this paper we motivate the causal mechanisms behind sample selection\ninduced collider bias (selection collider bias) that can cause Large Language\nModels (LLMs) to learn unconditional dependence between entities that are\nunconditionally independent in the real world. We show that selection collider\nbias can become amplified in underspecified learning tasks, and although\ndifficult to overcome, we describe a method to exploit the resulting spurious\ncorrelations for determination of when a model may be uncertain about its\nprediction. We demonstrate an uncertainty metric that matches human uncertainty\nin tasks with gender pronoun underspecification on an extended version of the\nWinogender Schemas evaluation set, and we provide an online demo where users\ncan apply our uncertainty metric to their own texts and models.",
        "pdf_link": "https://arxiv.org/pdf/2208.10063v2.pdf"
    },
    {
        "title": "GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization",
        "authors": [
            "Qianqian Xie",
            "Jimin Huang",
            "Tulika Saha",
            "Sophia Ananiadou"
        ],
        "published": "2022-08-21T23:09:29Z",
        "summary": "Recently, neural topic models (NTMs) have been incorporated into pre-trained\nlanguage models (PLMs), to capture the global semantic information for text\nsummarization. However, in these methods, there remain limitations in the way\nthey capture and integrate the global semantic information. In this paper, we\npropose a novel model, the graph contrastive topic enhanced language model\n(GRETEL), that incorporates the graph contrastive topic model with the\npre-trained language model, to fully leverage both the global and local\ncontextual semantics for long document extractive summarization. To better\ncapture and incorporate the global semantic information into PLMs, the graph\ncontrastive topic model integrates the hierarchical transformer encoder and the\ngraph contrastive learning to fuse the semantic information from the global\ndocument context and the gold summary. To this end, GRETEL encourages the model\nto efficiently extract salient sentences that are topically related to the gold\nsummary, rather than redundant sentences that cover sub-optimal topics.\nExperimental results on both general domain and biomedical datasets demonstrate\nthat our proposed method outperforms SOTA methods.",
        "pdf_link": "https://arxiv.org/pdf/2208.09982v1.pdf"
    },
    {
        "title": "A Syntax Aware BERT for Identifying Well-Formed Queries in a Curriculum Framework",
        "authors": [
            "Avinash Madasu",
            "Anvesh Rao Vijjini"
        ],
        "published": "2022-08-21T15:35:33Z",
        "summary": "A well formed query is defined as a query which is formulated in the manner\nof an inquiry, and with correct interrogatives, spelling and grammar. While\nidentifying well formed queries is an important task, few works have attempted\nto address it. In this paper we propose transformer based language model -\nBidirectional Encoder Representations from Transformers (BERT) to this task. We\nfurther imbibe BERT with parts-of-speech information inspired from earlier\nworks. Furthermore, we also train the model in multiple curriculum settings for\nimprovement in performance. Curriculum Learning over the task is experimented\nwith Baby Steps and One Pass techniques. Proposed architecture performs\nexceedingly well on the task. The best approach achieves accuracy of 83.93%,\noutperforming previous state-of-the-art at 75.0% and reaching close to the\napproximate human upper bound of 88.4%.",
        "pdf_link": "https://arxiv.org/pdf/2208.09912v1.pdf"
    },
    {
        "title": "CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive Learning for linguistic, visual, acoustic Representations",
        "authors": [
            "Junghun Kim",
            "Jihie Kim"
        ],
        "published": "2022-08-21T08:21:43Z",
        "summary": "Multimodal sentiment analysis has become an increasingly popular research\narea as the demand for multimodal online content is growing. For multimodal\nsentiment analysis, words can have different meanings depending on the\nlinguistic context and non-verbal information, so it is crucial to understand\nthe meaning of the words accordingly. In addition, the word meanings should be\ninterpreted within the whole utterance context that includes nonverbal\ninformation. In this paper, we present a Context-driven Modality Shifting BERT\nwith Contrastive Learning for linguistic, visual, acoustic Representations\n(CMSBERT-CLR), which incorporates the whole context's non-verbal and verbal\ninformation and aligns modalities more effectively through contrastive\nlearning. First, we introduce a Context-driven Modality Shifting (CMS) to\nincorporate the non-verbal and verbal information within the whole context of\nthe sentence utterance. Then, for improving the alignment of different\nmodalities within a common embedding space, we apply contrastive learning.\nFurthermore, we use an exponential moving average parameter and label smoothing\nas optimization strategies, which can make the convergence of the network more\nstable and increase the flexibility of the alignment. In our experiments, we\ndemonstrate that our approach achieves state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2209.07424v1.pdf"
    },
    {
        "title": "I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning",
        "authors": [
            "Yang Liu",
            "Zequn Sun",
            "Guangyao Li",
            "Wei Hu"
        ],
        "published": "2022-08-21T07:34:37Z",
        "summary": "Knowledge graph (KG) embedding seeks to learn vector representations for\nentities and relations. Conventional models reason over graph structures, but\nthey suffer from the issues of graph incompleteness and long-tail entities.\nRecent studies have used pre-trained language models to learn embeddings based\non the textual information of entities and relations, but they cannot take\nadvantage of graph structures. In the paper, we show empirically that these two\nkinds of features are complementary for KG embedding. To this end, we propose\nCoLE, a Co-distillation Learning method for KG Embedding that exploits the\ncomplementarity of graph structures and text information. Its graph embedding\nmodel employs Transformer to reconstruct the representation of an entity from\nits neighborhood subgraph. Its text embedding model uses a pre-trained language\nmodel to generate entity representations from the soft prompts of their names,\ndescriptions, and relational neighbors. To let the two model promote each\nother, we propose co-distillation learning that allows them to distill\nselective knowledge from each other's prediction logits. In our co-distillation\nlearning, each model serves as both a teacher and a student. Experiments on\nbenchmark datasets demonstrate that the two models outperform their related\nbaselines, and the ensemble method CoLE with co-distillation learning advances\nthe state-of-the-art of KG embedding.",
        "pdf_link": "https://arxiv.org/pdf/2208.09828v3.pdf"
    },
    {
        "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
        "authors": [
            "Pengcheng He",
            "Baolin Peng",
            "Liyang Lu",
            "Song Wang",
            "Jie Mei",
            "Yang Liu",
            "Ruochen Xu",
            "Hany Hassan Awadalla",
            "Yu Shi",
            "Chenguang Zhu",
            "Wayne Xiong",
            "Michael Zeng",
            "Jianfeng Gao",
            "Xuedong Huang"
        ],
        "published": "2022-08-21T01:00:54Z",
        "summary": "This paper presents Z-Code++, a new pre-trained language model optimized for\nabstractive text summarization. The model extends the state of the art\nencoder-decoder model using three techniques. First, we use a two-phase\npre-training process to improve model's performance on low-resource\nsummarization tasks. The model is first pre-trained using text corpora for\nlanguage understanding, and then is continually pre-trained on summarization\ncorpora for grounded text generation. Second, we replace self-attention layers\nin the encoder with disentangled attention layers, where each word is\nrepresented using two vectors that encode its content and position,\nrespectively. Third, we use fusion-in-encoder, a simple yet effective method of\nencoding long sequences in a hierarchical manner. Z-Code++ creates new state of\nthe art on 9 out of 13 text summarization tasks across 5 languages. Our model\nis parameter-efficient in that it outperforms the 600x larger PaLM-540B on\nXSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and\nfew-shot settings, our model substantially outperforms the competing models.",
        "pdf_link": "https://arxiv.org/pdf/2208.09770v2.pdf"
    },
    {
        "title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
        "authors": [
            "Chowdhury Rafeed Rahman",
            "MD. Hasibur Rahman",
            "Samiha Zakir",
            "Mohammad Rafsan",
            "Mohammed Eunus Ali"
        ],
        "published": "2022-08-20T15:21:35Z",
        "summary": "Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker",
        "pdf_link": "https://arxiv.org/pdf/2208.09709v2.pdf"
    },
    {
        "title": "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks",
        "authors": [
            "Rajiv Movva",
            "Jinhao Lei",
            "Shayne Longpre",
            "Ajay Gupta",
            "Chris DuBois"
        ],
        "published": "2022-08-20T14:01:56Z",
        "summary": "Quantization, knowledge distillation, and magnitude pruning are among the\nmost popular methods for neural network compression in NLP. Independently,\nthese methods reduce model size and can accelerate inference, but their\nrelative benefit and combinatorial interactions have not been rigorously\nstudied. For each of the eight possible subsets of these techniques, we compare\naccuracy vs. model size tradeoffs across six BERT architecture sizes and eight\nGLUE tasks. We find that quantization and distillation consistently provide\ngreater benefit than pruning. Surprisingly, except for the pair of pruning and\nquantization, using multiple methods together rarely yields diminishing\nreturns. Instead, we observe complementary and super-multiplicative reductions\nto model size. Our work quantitatively demonstrates that combining compression\nmethods can synergistically reduce model size, and that practitioners should\nprioritize (1) quantization, (2) knowledge distillation, and (3) pruning to\nmaximize accuracy vs. model size tradeoffs.",
        "pdf_link": "https://arxiv.org/pdf/2208.09684v1.pdf"
    },
    {
        "title": "SPOT: Knowledge-Enhanced Language Representations for Information Extraction",
        "authors": [
            "Jiacheng Li",
            "Yannis Katsis",
            "Tyler Baldwin",
            "Ho-Cheol Kim",
            "Andrew Bartko",
            "Julian McAuley",
            "Chun-Nan Hsu"
        ],
        "published": "2022-08-20T07:32:25Z",
        "summary": "Knowledge-enhanced pre-trained models for language representation have been\nshown to be more effective in knowledge base construction tasks (i.e.,~relation\nextraction) than language models such as BERT. These knowledge-enhanced\nlanguage models incorporate knowledge into pre-training to generate\nrepresentations of entities or relationships. However, existing methods\ntypically represent each entity with a separate embedding. As a result, these\nmethods struggle to represent out-of-vocabulary entities and a large amount of\nparameters, on top of their underlying token models (i.e.,~the transformer),\nmust be used and the number of entities that can be handled is limited in\npractice due to memory constraints. Moreover, existing models still struggle to\nrepresent entities and relationships simultaneously. To address these problems,\nwe propose a new pre-trained model that learns representations of both entities\nand relationships from token spans and span pairs in the text respectively. By\nencoding spans efficiently with span modules, our model can represent both\nentities and their relationships but requires fewer parameters than existing\nmodels. We pre-trained our model with the knowledge graph extracted from\nWikipedia and test it on a broad range of supervised and unsupervised\ninformation extraction tasks. Results show that our model learns better\nrepresentations for both entities and relationships than baselines, while in\nsupervised settings, fine-tuning our model outperforms RoBERTa consistently and\nachieves competitive results on information extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.09625v2.pdf"
    },
    {
        "title": "Pretrained Language Encoders are Natural Tagging Frameworks for Aspect Sentiment Triplet Extraction",
        "authors": [
            "Yanjie Gou",
            "Yinjie Lei",
            "Lingqiao Liu",
            "Yong Dai",
            "Chunxu Shen",
            "Yongqi Tong"
        ],
        "published": "2022-08-20T06:40:45Z",
        "summary": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract the spans of\naspect, opinion, and their sentiment relations as sentiment triplets. Existing\nworks usually formulate the span detection as a 1D token tagging problem, and\nmodel the sentiment recognition with a 2D tagging matrix of token pairs.\nMoreover, by leveraging the token representation of Pretrained Language\nEncoders (PLEs) like BERT, they can achieve better performance. However, they\nsimply leverage PLEs as feature extractors to build their modules but never\nhave a deep look at what specific knowledge does PLEs contain. In this paper,\nwe argue that instead of further designing modules to capture the inductive\nbias of ASTE, PLEs themselves contain \"enough\" features for 1D and 2D tagging:\n(1) The token representation contains the contextualized meaning of token\nitself, so this level feature carries necessary information for 1D tagging. (2)\nThe attention matrix of different PLE layers can further capture multi-level\nlinguistic knowledge existing in token pairs, which benefits 2D tagging. (3)\nFurthermore, with simple transformations, these two features can also be easily\nconverted to the 2D tagging matrix and 1D tagging sequence, respectively. That\nwill further boost the tagging results. By doing so, PLEs can be natural\ntagging frameworks and achieve a new state of the art, which is verified by\nextensive experiments and deep analyses.",
        "pdf_link": "https://arxiv.org/pdf/2208.09617v1.pdf"
    },
    {
        "title": "Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection",
        "authors": [
            "Souvik Das",
            "Sougata Saha",
            "Rohini K. Srihari"
        ],
        "published": "2022-08-20T04:13:27Z",
        "summary": "Personalized response selection systems are generally grounded on persona.\nHowever, there exists a co-relation between persona and empathy, which is not\nexplored well in these systems. Also, faithfulness to the conversation context\nplunges when a contradictory or an off-topic response is selected. This paper\nattempts to address these issues by proposing a suite of fusion strategies that\ncapture the interaction between persona, emotion, and entailment information of\nthe utterances. Ablation studies on the Persona-Chat dataset show that\nincorporating emotion and entailment improves the accuracy of response\nselection. We combine our fusion strategies and concept-flow encoding to train\na BERT-based model which outperforms the previous methods by margins larger\nthan 2.3 % on original personas and 1.9 % on revised personas in terms of\nhits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the\nPersona-Chat dataset.",
        "pdf_link": "https://arxiv.org/pdf/2208.09601v2.pdf"
    },
    {
        "title": "Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks",
        "authors": [
            "James R. Kirk",
            "Robert E. Wray",
            "Peter Lindes",
            "John E. Laird"
        ],
        "published": "2022-08-19T21:53:15Z",
        "summary": "Autonomous agents are able to draw on a wide variety of potential sources of\ntask knowledge; however current approaches invariably focus on only one or two.\nHere we investigate the challenges and impact of exploiting diverse knowledge\nsources to learn online, in one-shot, new tasks for a simulated office mobile\nrobot. The resulting agent, developed in the Soar cognitive architecture, uses\nthe following sources of domain and task knowledge: interaction with the\nenvironment, task execution and search knowledge, human natural language\ninstruction, and responses retrieved from a large language model (GPT-3). We\nexplore the distinct contributions of these knowledge sources and evaluate the\nperformance of different combinations in terms of learning correct task\nknowledge and human workload. Results show that an agent's online integration\nof diverse knowledge sources improves one-shot task learning overall, reducing\nhuman feedback needed for rapid and reliable task learning.",
        "pdf_link": "https://arxiv.org/pdf/2208.09554v3.pdf"
    },
    {
        "title": "Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes",
        "authors": [
            "Can Zheng",
            "Yanshan Wang",
            "Xiaowei Jia"
        ],
        "published": "2022-08-19T16:34:41Z",
        "summary": "Semantic textual similarity (STS) in the clinical domain helps improve\ndiagnostic efficiency and produce concise texts for downstream data mining\ntasks. However, given the high degree of domain knowledge involved in clinic\ntext, it remains challenging for general language models to infer implicit\nmedical relationships behind clinical sentences and output similarities\ncorrectly. In this paper, we present a graph-augmented cyclic learning\nframework for similarity estimation in the clinical domain. The framework can\nbe conveniently implemented on a state-of-art backbone language model, and\nimprove its performance by leveraging domain knowledge through co-training with\nan auxiliary graph convolution network (GCN) based network. We report the\nsuccess of introducing domain knowledge in GCN and the co-training framework by\nimproving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2208.09437v1.pdf"
    },
    {
        "title": "Nonlinear Optical Data Transformer for Machine Learning",
        "authors": [
            "Mustafa Yildirim",
            "Ilker Oguz",
            "Fabian Kaufmann",
            "Marc Reig Escale",
            "Rachel Grange",
            "Demetri Psaltis",
            "Christophe Moser"
        ],
        "published": "2022-08-19T15:28:48Z",
        "summary": "Modern machine learning models use an ever-increasing number of parameters to\ntrain (175 billion parameters for GPT-3) with large datasets to obtain better\nperformance. Bigger is better has been the norm. Optical computing has been\nreawakened as a potential solution to large-scale computing through optical\naccelerators that carry out linear operations while reducing electrical power.\nHowever, to achieve efficient computing with light, creating and controlling\nnonlinearity optically rather than electronically remains a challenge. This\nstudy explores a reservoir computing (RC) approach whereby a 14 mm long\nfew-mode waveguide in LiNbO3 on insulator is used as a complex nonlinear\noptical processor. A dataset is encoded digitally on the spectrum of a\nfemtosecond pulse which is then launched in the waveguide. The output spectrum\ndepends nonlinearly on the input. We experimentally show that a simple digital\nlinear classifier with 784 parameters using the output spectrum from the\nwaveguide as input increased the classification accuracy of several databases\ncompared to non-transformed data, approximately 10$\\%$. In comparison, a deep\ndigital neural network (NN) with 40000 parameters was necessary to achieve the\nsame accuracy. Reducing the number of parameters by a factor of $\\sim$50\nillustrates that a compact optical RC approach can perform on par with a deep\ndigital NN.",
        "pdf_link": "https://arxiv.org/pdf/2208.09398v1.pdf"
    },
    {
        "title": "Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed Pricing",
        "authors": [
            "Po-Yi Liu",
            "Chi-Hua Wang",
            "Henghsiu Tsai"
        ],
        "published": "2022-08-19T14:37:37Z",
        "summary": "This paper presents a novel non-stationary dynamic pricing algorithm design,\nwhere pricing agents face incomplete demand information and market environment\nshifts. The agents run price experiments to learn about each product's demand\ncurve and the profit-maximizing price, while being aware of market environment\nshifts to avoid high opportunity costs from offering sub-optimal prices. The\nproposed ACIDP extends information-directed sampling (IDS) algorithms from\nstatistical machine learning to include microeconomic choice theory, with a\nnovel pricing strategy auditing procedure to escape sub-optimal pricing after\nmarket environment shift. The proposed ACIDP outperforms competing bandit\nalgorithms including Upper Confidence Bound (UCB) and Thompson sampling (TS) in\na series of market environment shifts.",
        "pdf_link": "https://arxiv.org/pdf/2208.09372v3.pdf"
    },
    {
        "title": "UniCausal: Unified Benchmark and Repository for Causal Text Mining",
        "authors": [
            "Fiona Anting Tan",
            "Xinyu Zuo",
            "See-Kiong Ng"
        ],
        "published": "2022-08-19T06:14:05Z",
        "summary": "Current causal text mining datasets vary in objectives, data coverage, and\nannotation schemes. These inconsistent efforts prevent modeling capabilities\nand fair comparisons of model performance. Furthermore, few datasets include\ncause-effect span annotations, which are needed for end-to-end causal relation\nextraction. To address these issues, we propose UniCausal, a unified benchmark\nfor causal text mining across three tasks: (I) Causal Sequence Classification,\n(II) Cause-Effect Span Detection and (III) Causal Pair Classification. We\nconsolidated and aligned annotations of six high quality, mainly\nhuman-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165\nexamples for each task respectively. Since the definition of causality can be\nsubjective, our framework was designed to allow researchers to work on some or\nall datasets and tasks. To create an initial benchmark, we fine-tuned BERT\npre-trained language models to each task, achieving 70.10% Binary F1, 52.42%\nMacro F1, and 84.68% Binary F1 scores respectively.",
        "pdf_link": "https://arxiv.org/pdf/2208.09163v2.pdf"
    },
    {
        "title": "A Risk-Sensitive Approach to Policy Optimization",
        "authors": [
            "Jared Markowitz",
            "Ryan W. Gardner",
            "Ashley Llorens",
            "Raman Arora",
            "I-Jeng Wang"
        ],
        "published": "2022-08-19T00:55:05Z",
        "summary": "Standard deep reinforcement learning (DRL) aims to maximize expected reward,\nconsidering collected experiences equally in formulating a policy. This differs\nfrom human decision-making, where gains and losses are valued differently and\noutlying outcomes are given increased consideration. It also fails to\ncapitalize on opportunities to improve safety and/or performance through the\nincorporation of distributional context. Several approaches to distributional\nDRL have been investigated, with one popular strategy being to evaluate the\nprojected distribution of returns for possible actions. We propose a more\ndirect approach whereby risk-sensitive objectives, specified in terms of the\ncumulative distribution function (CDF) of the distribution of full-episode\nrewards, are optimized. This approach allows for outcomes to be weighed based\non relative quality, can be used for both continuous and discrete action\nspaces, and may naturally be applied in both constrained and unconstrained\nsettings. We show how to compute an asymptotically consistent estimate of the\npolicy gradient for a broad class of risk-sensitive objectives via sampling,\nsubsequently incorporating variance reduction and regularization measures to\nfacilitate effective on-policy learning. We then demonstrate that the use of\nmoderately \"pessimistic\" risk profiles, which emphasize scenarios where the\nagent performs poorly, leads to enhanced exploration and a continual focus on\naddressing deficiencies. We test the approach using different risk profiles in\nsix OpenAI Safety Gym environments, comparing to state of the art on-policy\nmethods. Without cost constraints, we find that pessimistic risk profiles can\nbe used to reduce cost while improving total reward accumulation. With cost\nconstraints, they are seen to provide higher positive rewards than risk-neutral\napproaches at the prescribed allowable cost.",
        "pdf_link": "https://arxiv.org/pdf/2208.09106v2.pdf"
    },
    {
        "title": "MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing",
        "authors": [
            "Unggi Lee",
            "Yonghyun Park",
            "Yujin Kim",
            "Seongyune Choi",
            "Hyeoncheol Kim"
        ],
        "published": "2022-08-19T00:43:47Z",
        "summary": "Knowledge tracing (KT) is a field of study that predicts the future\nperformance of students based on prior performance datasets collected from\neducational applications such as intelligent tutoring systems, learning\nmanagement systems, and online courses. Some previous studies on KT have\nconcentrated only on the interpretability of the model, whereas others have\nfocused on enhancing the performance. Models that consider both\ninterpretability and the performance improvement have been insufficient.\nMoreover, models that focus on performance improvements have not shown an\noverwhelming performance compared with existing models. In this study, we\npropose MonaCoBERT, which achieves the best performance on most benchmark\ndatasets and has significant interpretability. MonaCoBERT uses a BERT-based\narchitecture with monotonic convolutional multihead attention, which reflects\nforgetting behavior of the students and increases the representation power of\nthe model. We can also increase the performance and interpretability using a\nclassical test-theory-based (CTT-based) embedding strategy that considers the\ndifficulty of the question. To determine why MonaCoBERT achieved the best\nperformance and interpret the results quantitatively, we conducted ablation\nstudies and additional analyses using Grad-CAM, UMAP, and various visualization\ntechniques. The analysis results demonstrate that both attention components\ncomplement one another and that CTT-based embedding represents information on\nboth global and local difficulties. We also demonstrate that our model\nrepresents the relationship between concepts.",
        "pdf_link": "https://arxiv.org/pdf/2208.12615v2.pdf"
    },
    {
        "title": "MARTI-4: new model of human brain, considering neocortex and basal ganglia -- learns to play Atari game by reinforcement learning on a single CPU",
        "authors": [
            "Igor Pivovarov",
            "Sergey Shumsky"
        ],
        "published": "2022-08-18T20:23:49Z",
        "summary": "We present Deep Control - new ML architecture of cortico-striatal brain\ncircuits, which use whole cortical column as a structural element, instead of a\nsinge neuron. Based on this architecture, we present MARTI - new model of human\nbrain, considering neocortex and basal ganglia. This model is de-signed to\nimplement expedient behavior and is capable to learn and achieve goals in\nunknown environments. We introduce a novel surprise feeling mechanism, that\nsignificantly improves reinforcement learning process through inner rewards. We\nuse OpenAI Gym environment to demonstrate MARTI learning on a single CPU just\nin several hours.",
        "pdf_link": "https://arxiv.org/pdf/2209.02387v1.pdf"
    },
    {
        "title": "VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media",
        "authors": [
            "Georgios Chochlakis",
            "Tejas Srinivasan",
            "Jesse Thomason",
            "Shrikanth Narayanan"
        ],
        "published": "2022-08-18T18:51:13Z",
        "summary": "We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight and\nnovelty of VAuLT is to propagate the output representations of a large language\nmodel (LM) like BERT to the language input of ViLT. We show that joint training\nof the LM and ViLT can yield relative improvements up to 20% over ViLT and\nachieve state-of-the-art or comparable performance on VL tasks involving richer\nlanguage inputs and affective constructs, such as for Target-Oriented Sentiment\nClassification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification\nin MVSA-Single and MVSA-Multiple. Our code is available at\nhttps://github.com/gchochla/VAuLT.",
        "pdf_link": "https://arxiv.org/pdf/2208.09021v3.pdf"
    },
    {
        "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
        "authors": [
            "Gati Aher",
            "Rosa I. Arriaga",
            "Adam Tauman Kalai"
        ],
        "published": "2022-08-18T17:54:49Z",
        "summary": "We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model's simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.",
        "pdf_link": "https://arxiv.org/pdf/2208.10264v5.pdf"
    },
    {
        "title": "MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation",
        "authors": [
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "published": "2022-08-18T04:28:20Z",
        "summary": "Building dialogue generation systems in a zero-shot scenario remains a huge\nchallenge, since the typical zero-shot approaches in dialogue generation rely\nheavily on large-scale pre-trained language generation models such as GPT-3 and\nT5. The research on zero-shot dialogue generation without cumbersome language\nmodels is limited due to lacking corresponding parallel dialogue corpora. In\nthis paper, we propose a simple but effective Multilingual learning framework\nfor Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively\ntransfer knowledge from an English corpus with large-scale training samples to\na non-English corpus with zero samples. Besides, MulZDG can be viewed as a\nmultilingual data augmentation method to improve the performance of the\nresource-rich language. First, we construct multilingual code-switching\ndialogue datasets via translation utterances randomly selected from monolingual\nEnglish datasets. Then we employ MulZDG to train a unified multilingual\ndialogue model based on the code-switching datasets. The MulZDG can conduct\nimplicit semantic alignment between different languages. Experiments on\nDailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve\ncompetitive performance under zero-shot case compared to training with\nsufficient examples but also greatly improve the performance of the source\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2208.08629v1.pdf"
    },
    {
        "title": "Extracting Medication Changes in Clinical Narratives using Pre-trained Language Models",
        "authors": [
            "Giridhar Kaushik Ramachandran",
            "Kevin Lybarger",
            "Yaya Liu",
            "Diwakar Mahajan",
            "Jennifer J. Liang",
            "Ching-Huei Tsou",
            "Meliha Yetisgen",
            "\u00d6zlem Uzuner"
        ],
        "published": "2022-08-17T17:22:48Z",
        "summary": "An accurate and detailed account of patient medications, including medication\nchanges within the patient timeline, is essential for healthcare providers to\nprovide appropriate patient care. Healthcare providers or the patients\nthemselves may initiate changes to patient medication. Medication changes take\nmany forms, including prescribed medication and associated dosage modification.\nThese changes provide information about the overall health of the patient and\nthe rationale that led to the current care. Future care can then build on the\nresulting state of the patient. This work explores the automatic extraction of\nmedication change information from free-text clinical notes. The Contextual\nMedication Event Dataset (CMED) is a corpus of clinical notes with annotations\nthat characterize medication changes through multiple change-related\nattributes, including the type of change (start, stop, increase, etc.),\ninitiator of the change, temporality, change likelihood, and negation. Using\nCMED, we identify medication mentions in clinical text and propose three novel\nhigh-performing BERT-based systems that resolve the annotated medication change\ncharacteristics. We demonstrate that our proposed systems improve medication\nchange classification performance over the initial work exploring CMED.",
        "pdf_link": "https://arxiv.org/pdf/2208.08417v2.pdf"
    },
    {
        "title": "Neural Embeddings for Text",
        "authors": [
            "Oleg Vasilyev",
            "John Bohannon"
        ],
        "published": "2022-08-17T16:26:13Z",
        "summary": "We propose a new kind of embedding for natural language text that deeply\nrepresents semantic meaning. Standard text embeddings use the outputs from\nhidden layers of a pretrained language model. In our method, we let a language\nmodel learn from the text and then literally pick its brain, taking the actual\nweights of the model's neurons to generate a vector. We call this\nrepresentation of the text a neural embedding. We confirm the ability of this\nrepresentation to reflect semantics of the text by an analysis of its behavior\non several datasets, and by a comparison of neural embedding with state of the\nart sentence embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2208.08386v2.pdf"
    },
    {
        "title": "A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting",
        "authors": [
            "Pradyumna Tambwekar",
            "Lakshita Dodeja",
            "Nathan Vaska",
            "Wei Xu",
            "Matthew Gombolay"
        ],
        "published": "2022-08-17T16:11:07Z",
        "summary": "Many real-world tasks involve a mixed-initiative setup, wherein humans and AI\nsystems collaboratively perform a task. While significant work has been\nconducted towards enabling humans to specify, through language, exactly how an\nagent should complete a task (i.e., low-level specification), prior work lacks\non interpreting the high-level strategic intent of the human commanders.\nParsing strategic intent from language will allow autonomous systems to\nindependently operate according to the user's plan without frequent guidance or\ninstruction. In this paper, we build a computational interface capable of\ntranslating unstructured language strategies into actionable intent in the form\nof goals and constraints. Leveraging a game environment, we collect a dataset\nof over 1000 examples, mapping language strategies to the corresponding goals\nand constraints, and show that our model, trained on this dataset,\nsignificantly outperforms human interpreters in inferring strategic intent\n(i.e., goals and constraints) from language (p < 0.05). Furthermore, we show\nthat our model (125M parameters) significantly outperforms ChatGPT for this\ntask (p < 0.05) in a low-data setting.",
        "pdf_link": "https://arxiv.org/pdf/2208.08374v2.pdf"
    },
    {
        "title": "Ask Question First for Enhancing Lifelong Language Learning",
        "authors": [
            "Han Wang",
            "Ruiliu Fu",
            "Xuejun Zhang",
            "Jun Zhou",
            "Qingwei Zhao"
        ],
        "published": "2022-08-17T15:58:33Z",
        "summary": "Lifelong language learning aims to stream learning NLP tasks while retaining\nknowledge of previous tasks. Previous works based on the language model and\nfollowing data-free constraint approaches have explored formatting all data as\n\"begin token (\\textit{B}) + context (\\textit{C}) + question (\\textit{Q}) +\nanswer (\\textit{A})\" for different tasks. However, they still suffer from\ncatastrophic forgetting and are exacerbated when the previous task's pseudo\ndata is insufficient for the following reasons: (1) The model has difficulty\ngenerating task-corresponding pseudo data, and (2) \\textit{A} is prone to error\nwhen \\textit{A} and \\textit{C} are separated by \\textit{Q} because the\ninformation of the \\textit{C} is diminished before generating \\textit{A}.\nTherefore, we propose the Ask Question First and Replay Question (AQF-RQ),\nincluding a novel data format \"\\textit{BQCA}\" and a new training task to train\npseudo questions of previous tasks. Experimental results demonstrate that\nAQF-RQ makes it easier for the model to generate more pseudo data that match\ncorresponding tasks, and is more robust to both sufficient and insufficient\npseudo-data when the task boundary is both clear and unclear. AQF-RQ can\nachieve only 0.36\\% lower performance than multi-task learning.",
        "pdf_link": "https://arxiv.org/pdf/2208.08367v2.pdf"
    },
    {
        "title": "Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model",
        "authors": [
            "Yinghui Xing",
            "Qirui Wu",
            "De Cheng",
            "Shizhou Zhang",
            "Guoqiang Liang",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "published": "2022-08-17T15:06:36Z",
        "summary": "With the emergence of large pre-trained vison-language model like CLIP,\ntransferable representations can be adapted to a wide range of downstream tasks\nvia prompt tuning. Prompt tuning tries to probe the beneficial information for\ndownstream tasks from the general knowledge stored in the pre-trained model. A\nrecently proposed method named Context Optimization (CoOp) introduces a set of\nlearnable vectors as text prompt from the language side. However, tuning the\ntext prompt alone can only adjust the synthesized \"classifier\", while the\ncomputed visual features of the image encoder can not be affected , thus\nleading to sub-optimal solutions. In this paper, we propose a novel\nDual-modality Prompt Tuning (DPT) paradigm through learning text and visual\nprompts simultaneously. To make the final image feature concentrate more on the\ntarget visual concept, a Class-Aware Visual Prompt Tuning (CAVPT) scheme is\nfurther proposed in our DPT, where the class-aware visual prompt is generated\ndynamically by performing the cross attention between text prompts features and\nimage patch token embeddings to encode both the downstream task-related\ninformation and visual instance information. Extensive experimental results on\n11 datasets demonstrate the effectiveness and generalization ability of the\nproposed method. Our code is available in https://github.com/fanrena/DPT.",
        "pdf_link": "https://arxiv.org/pdf/2208.08340v4.pdf"
    },
    {
        "title": "Quality Diversity Evolutionary Learning of Decision Trees",
        "authors": [
            "Andrea Ferigo",
            "Leonardo Lucio Custode",
            "Giovanni Iacca"
        ],
        "published": "2022-08-17T13:57:32Z",
        "summary": "Addressing the need for explainable Machine Learning has emerged as one of\nthe most important research directions in modern Artificial Intelligence (AI).\nWhile the current dominant paradigm in the field is based on black-box models,\ntypically in the form of (deep) neural networks, these models lack direct\ninterpretability for human users, i.e., their outcomes (and, even more so,\ntheir inner working) are opaque and hard to understand. This is hindering the\nadoption of AI in safety-critical applications, where high interests are at\nstake. In these applications, explainable by design models, such as decision\ntrees, may be more suitable, as they provide interpretability. Recent works\nhave proposed the hybridization of decision trees and Reinforcement Learning,\nto combine the advantages of the two approaches. So far, however, these works\nhave focused on the optimization of those hybrid models. Here, we apply\nMAP-Elites for diversifying hybrid models over a feature space that captures\nboth the model complexity and its behavioral variability. We apply our method\non two well-known control problems from the OpenAI Gym library, on which we\ndiscuss the \"illumination\" patterns projected by MAP-Elites, comparing its\nresults against existing similar approaches.",
        "pdf_link": "https://arxiv.org/pdf/2208.12758v1.pdf"
    },
    {
        "title": "HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models",
        "authors": [
            "Swaroop Mishra",
            "Elnaz Nouri"
        ],
        "published": "2022-08-17T11:20:41Z",
        "summary": "Controlling the text generated by language models and customizing the content\nhas been a long-standing challenge. Existing prompting techniques proposed in\npursuit of providing control are task-specific and lack generality; this\nprovides overwhelming choices for non-expert users to find a suitable method\nfor their task. The effort associated with those techniques, such as in writing\nexamples, explanations, instructions, etc. further limits their adoption among\nnon-expert users. In this paper, we propose a simple prompting strategy HELP ME\nTHINK where we encourage GPT3 to help non-expert users by asking a set of\nrelevant questions and leveraging user answers to execute the task. We\ndemonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.\nSpecifically, we focus on tasks that are hard for average humans and require\nsignificant thinking to perform. We hope our work will encourage the\ndevelopment of unconventional ways to harness the power of large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2208.08232v2.pdf"
    },
    {
        "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation",
        "authors": [
            "Federico Cassano",
            "John Gouwar",
            "Daniel Nguyen",
            "Sydney Nguyen",
            "Luna Phipps-Costin",
            "Donald Pinckney",
            "Ming-Ho Yee",
            "Yangtian Zi",
            "Carolyn Jane Anderson",
            "Molly Q Feldman",
            "Arjun Guha",
            "Michael Greenberg",
            "Abhinav Jangda"
        ],
        "published": "2022-08-17T11:16:52Z",
        "summary": "Large language models have demonstrated the ability to generate both natural\nlanguage and programming language text. Such models open up the possibility of\nmulti-language code generation: could code generation models generalize\nknowledge from one language to another? Although contemporary code generation\nmodels can generate semantically correct Python code, little is known about\ntheir abilities with other languages. We propose MultiPL-E, a system for\ntranslating unit test-driven code generation benchmarks to new languages. We\ncreate the first massively multilingual code generation benchmark by using\nMultiPL-E to translate two popular Python code generation benchmarks to 18\nadditional programming languages.\n  We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18\nlanguages that encompass a range of programming paradigms and popularity. Using\nthese new parallel benchmarks, we evaluate the multi-language performance of\nthree state-of-the-art code generation models: Codex, CodeGen, and InCoder. We\nfind that Codex matches or even exceeds its performance on Python for several\nother languages. The range of programming languages represented in MultiPL-E\nallow us to explore the impact of language frequency and language features on\nmodel performance. Finally, the MultiPL-E approach of compiling code generation\nbenchmarks to new programming languages is both scalable and extensible, making\nit straightforward to evaluate new models, benchmarks, and languages.",
        "pdf_link": "https://arxiv.org/pdf/2208.08227v4.pdf"
    },
    {
        "title": "Visual Comparison of Language Model Adaptation",
        "authors": [
            "Rita Sevastjanova",
            "Eren Cakmak",
            "Shauli Ravfogel",
            "Ryan Cotterell",
            "Mennatallah El-Assady"
        ],
        "published": "2022-08-17T09:25:28Z",
        "summary": "Neural language models are widely used; however, their model parameters often\nneed to be adapted to the specific domains and tasks of an application, which\nis time- and resource-consuming. Thus, adapters have recently been introduced\nas a lightweight alternative for model adaptation. They consist of a small set\nof task-specific parameters with a reduced training time and simple parameter\ncomposition. The simplicity of adapter training and composition comes along\nwith new challenges, such as maintaining an overview of adapter properties and\neffectively comparing their produced embedding spaces. To help developers\novercome these challenges, we provide a twofold contribution. First, in close\ncollaboration with NLP researchers, we conducted a requirement analysis for an\napproach supporting adapter evaluation and detected, among others, the need for\nboth intrinsic (i.e., embedding similarity-based) and extrinsic (i.e.,\nprediction-based) explanation methods. Second, motivated by the gathered\nrequirements, we designed a flexible visual analytics workspace that enables\nthe comparison of adapter properties. In this paper, we discuss several design\niterations and alternatives for interactive, comparative visual explanation\nmethods. Our comparative visualizations show the differences in the adapted\nembedding vectors and prediction outcomes for diverse human-interpretable\nconcepts (e.g., person names, human qualities). We evaluate our workspace\nthrough case studies and show that, for instance, an adapter trained on the\nlanguage debiasing task according to context-0 (decontextualized) embeddings\nintroduces a new type of bias where words (even gender-independent words such\nas countries) become more similar to female than male pronouns. We demonstrate\nthat these are artifacts of context-0 embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2208.08176v1.pdf"
    },
    {
        "title": "ASTRO: An AST-Assisted Approach for Generalizable Neural Clone Detection",
        "authors": [
            "Yifan Zhang",
            "Junwen Yang",
            "Haoyu Dong",
            "Qingchen Wang",
            "Huajie Shao",
            "Kevin Leach",
            "Yu Huang"
        ],
        "published": "2022-08-17T04:50:51Z",
        "summary": "Neural clone detection has attracted the attention of software engineering\nresearchers and practitioners. However, most neural clone detection methods do\nnot generalize beyond the scope of clones that appear in the training dataset.\nThis results in poor model performance, especially in terms of model recall. In\nthis paper, we present an Abstract Syntax Tree (AST) assisted approach for\ngeneralizable neural clone detection, or ASTRO, a framework for finding clones\nin codebases reflecting industry practices. We present three main components:\n(1) an AST-inspired representation for source code that leverages program\nstructure and semantics, (2) a global graph representation that captures the\ncontext of an AST among a corpus of programs, and (3) a graph embedding for\nprograms that, in combination with extant large-scale language models, improves\nstate-of-the-art code clone detection. Our experimental results show that ASTRO\nimproves state-of-the-art neural clone detection approaches in both recall and\nF-1 scores.",
        "pdf_link": "https://arxiv.org/pdf/2208.08067v1.pdf"
    },
    {
        "title": "Transformer Encoder for Social Science",
        "authors": [
            "Haosen Ge",
            "In Young Park",
            "Xuancheng Qian",
            "Grace Zeng"
        ],
        "published": "2022-08-17T01:01:25Z",
        "summary": "High-quality text data has become an important data source for social\nscientists. We have witnessed the success of pretrained deep neural network\nmodels, such as BERT and RoBERTa, in recent social science research. In this\npaper, we propose a compact pretrained deep neural network, Transformer Encoder\nfor Social Science (TESS), explicitly designed to tackle text processing tasks\nin social science research. Using two validation tests, we demonstrate that\nTESS outperforms BERT and RoBERTa by 16.7% on average when the number of\ntraining samples is limited (<1,000 training instances). The results display\nthe superiority of TESS over BERT and RoBERTa on social science text processing\ntasks. Lastly, we discuss the limitation of our model and present advice for\nfuture researchers.",
        "pdf_link": "https://arxiv.org/pdf/2208.08005v1.pdf"
    },
    {
        "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
        "authors": [
            "Hendrik Strobelt",
            "Albert Webson",
            "Victor Sanh",
            "Benjamin Hoover",
            "Johanna Beyer",
            "Hanspeter Pfister",
            "Alexander M. Rush"
        ],
        "published": "2022-08-16T17:17:53Z",
        "summary": "State-of-the-art neural language models can now be used to solve ad-hoc\nlanguage tasks through zero-shot prompting without the need for supervised\ntraining. This approach has gained popularity in recent years, and researchers\nhave demonstrated prompts that achieve strong accuracy on specific NLP tasks.\nHowever, finding a prompt for new tasks requires experimentation. Different\nprompt templates with different wording choices lead to significant accuracy\ndifferences. PromptIDE allows users to experiment with prompt variations,\nvisualize prompt performance, and iteratively optimize prompts. We developed a\nworkflow that allows users to first focus on model feedback using small data\nbefore moving on to a large data regime that allows empirical grounding of\npromising prompts using quantitative measures of the task. The tool then allows\neasy deployment of the newly created ad-hoc models. We demonstrate the utility\nof PromptIDE (demo at http://prompt.vizhub.ai) and our workflow using several\nreal-world use cases.",
        "pdf_link": "https://arxiv.org/pdf/2208.07852v1.pdf"
    },
    {
        "title": "BERT(s) to Detect Multiword Expressions",
        "authors": [
            "Damith Premasiri",
            "Tharindu Ranasinghe"
        ],
        "published": "2022-08-16T16:32:23Z",
        "summary": "Multiword expressions (MWEs) present groups of words in which the meaning of\nthe whole is not derived from the meaning of its parts. The task of processing\nMWEs is crucial in many natural language processing (NLP) applications,\nincluding machine translation and terminology extraction. Therefore, detecting\nMWEs is a popular research theme. In this paper, we explore state-of-the-art\nneural transformers in the task of detecting MWEs.We empirically evaluate\nseveral transformer models in the dataset for SemEval-2016 Task 10: Detecting\nMinimal Semantic Units and their Meanings (DiMSUM). We show that transformer\nmodels outperform the previous neural models based on long short-term memory\n(LSTM). The code and pre-trained model will be made freely available to the\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2208.07832v1.pdf"
    },
    {
        "title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
        "authors": [
            "Nolan Wagener",
            "Andrey Kolobov",
            "Felipe Vieira Frujeri",
            "Ricky Loynd",
            "Ching-An Cheng",
            "Matthew Hausknecht"
        ],
        "published": "2022-08-15T17:57:33Z",
        "summary": "Simulated humanoids are an appealing research domain due to their physical\ncapabilities. Nonetheless, they are also challenging to control, as a policy\nmust drive an unstable, discontinuous, and high-dimensional physical system.\nOne widely studied approach is to utilize motion capture (MoCap) data to teach\nthe humanoid agent low-level skills (e.g., standing, walking, and running) that\ncan then be re-used to synthesize high-level behaviors. However, even with\nMoCap data, controlling simulated humanoids remains very hard, as MoCap data\noffers only kinematic information. Finding physical control inputs to realize\nthe demonstrated motions requires computationally intensive methods like\nreinforcement learning. Thus, despite the publicly available MoCap data, its\nutility has been limited to institutions with large-scale compute. In this\nwork, we dramatically lower the barrier for productive research on this topic\nby training and releasing high-quality agents that can track over three hours\nof MoCap data for a simulated humanoid in the dm_control physics-based\nenvironment. We release MoCapAct (Motion Capture with Actions), a dataset of\nthese expert agents and their rollouts, which contain proprioceptive\nobservations and actions. We demonstrate the utility of MoCapAct by using it to\ntrain a single hierarchical policy capable of tracking the entire MoCap dataset\nwithin dm_control and show the learned low-level component can be re-used to\nefficiently learn downstream high-level tasks. Finally, we use MoCapAct to\ntrain an autoregressive GPT model and show that it can control a simulated\nhumanoid to perform natural motion completion given a motion prompt.\n  Videos of the results and links to the code and dataset are available at\nhttps://microsoft.github.io/MoCapAct.",
        "pdf_link": "https://arxiv.org/pdf/2208.07363v3.pdf"
    },
    {
        "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
        "authors": [
            "Tim Dettmers",
            "Mike Lewis",
            "Younes Belkada",
            "Luke Zettlemoyer"
        ],
        "published": "2022-08-15T17:08:50Z",
        "summary": "Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.",
        "pdf_link": "https://arxiv.org/pdf/2208.07339v2.pdf"
    },
    {
        "title": "MENLI: Robust Evaluation Metrics from Natural Language Inference",
        "authors": [
            "Yanran Chen",
            "Steffen Eger"
        ],
        "published": "2022-08-15T16:30:14Z",
        "summary": "Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).",
        "pdf_link": "https://arxiv.org/pdf/2208.07316v5.pdf"
    },
    {
        "title": "Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection",
        "authors": [
            "Daniele Comi",
            "Dimitrios Christofidellis",
            "Pier Francesco Piazza",
            "Matteo Manica"
        ],
        "published": "2022-08-15T09:27:34Z",
        "summary": "Intent discovery is a crucial task in natural language processing, and it is\nincreasingly relevant for various of industrial applications. Identifying\nnovel, unseen intents from user inputs remains one of the biggest challenges in\nthis field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for\nmultilingual intent discovery relying on a Transformer architecture, fine-tuned\nwith Adapters. We train the model for Natural Language Inference (NLI) and\nlater perform unknown intent classification in a zero-shot setting for multiple\nlanguages. In our evaluation, we first analyze the quality of the model after\nadaptive fine-tuning on known classes. Secondly, we evaluate its performance in\ncasting intent classification as an NLI task. Lastly, we test the zero-shot\nperformance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters\ncan effectively perform intent discovery by generating semantically similar\nintents, if not equal, to the ground-truth ones. Our experiments show how\nZero-Shot-BERT-Adapters outperforms various baselines in two zero-shot\nsettings: known intent classification and unseen intent discovery. The proposed\npipeline holds the potential for broad application in customer care. It enables\nautomated dynamic triage using a lightweight model that can be easily deployed\nand scaled in various business scenarios, unlike large language models.\nZero-Shot-BERT-Adapters represents an innovative multi-language approach for\nintent discovery, enabling the online generation of novel intents. A Python\npackage implementing the pipeline and the new datasets we compiled are\navailable at the following link:\nhttps://github.com/GT4SD/zero-shot-bert-adapters.",
        "pdf_link": "https://arxiv.org/pdf/2208.07084v3.pdf"
    },
    {
        "title": "Syntax-driven Data Augmentation for Named Entity Recognition",
        "authors": [
            "Arie Pratama Sutiono",
            "Gus Hahn-Powell"
        ],
        "published": "2022-08-15T01:24:55Z",
        "summary": "In low resource settings, data augmentation strategies are commonly leveraged\nto improve performance. Numerous approaches have attempted document-level\naugmentation (e.g., text classification), but few studies have explored\ntoken-level augmentation. Performed naively, data augmentation can produce\nsemantically incongruent and ungrammatical examples. In this work, we compare\nsimple masked language model replacement and an augmentation method using\nconstituency tree mutations to improve the performance of named entity\nrecognition in low-resource settings with the aim of preserving linguistic\ncohesion of the augmented sentences.",
        "pdf_link": "https://arxiv.org/pdf/2208.06957v2.pdf"
    },
    {
        "title": "Continuous Active Learning Using Pretrained Transformers",
        "authors": [
            "Nima Sadri",
            "Gordon V. Cormack"
        ],
        "published": "2022-08-15T01:09:19Z",
        "summary": "Pre-trained and fine-tuned transformer models like BERT and T5 have improved\nthe state of the art in ad-hoc retrieval and question-answering, but not as yet\nin high-recall information retrieval, where the objective is to retrieve\nsubstantially all relevant documents. We investigate whether the use of\ntransformer-based models for reranking and/or featurization can improve the\nBaseline Model Implementation of the TREC Total Recall Track, which represents\nthe current state of the art for high-recall information retrieval. We also\nintroduce CALBERT, a model that can be used to continuously fine-tune a\nBERT-based model based on relevance feedback.",
        "pdf_link": "https://arxiv.org/pdf/2208.06955v1.pdf"
    },
    {
        "title": "Targeted Honeyword Generation with Language Models",
        "authors": [
            "Fangyi Yu",
            "Miguel Vargas Martin"
        ],
        "published": "2022-08-15T00:06:29Z",
        "summary": "Honeywords are fictitious passwords inserted into databases in order to\nidentify password breaches. The major difficulty is how to produce honeywords\nthat are difficult to distinguish from real passwords. Although the generation\nof honeywords has been widely investigated in the past, the majority of\nexisting research assumes attackers have no knowledge of the users. These\nhoneyword generating techniques (HGTs) may utterly fail if attackers exploit\nusers' personally identifiable information (PII) and the real passwords include\nusers' PII. In this paper, we propose to build a more secure and trustworthy\nauthentication system that employs off-the-shelf pre-trained language models\nwhich require no further training on real passwords to produce honeywords while\nretaining the PII of the associated real password, therefore significantly\nraising the bar for attackers.\n  We conducted a pilot experiment in which individuals are asked to distinguish\nbetween authentic passwords and honeywords when the username is provided for\nGPT-3 and a tweaking technique. Results show that it is extremely difficult to\ndistinguish the real passwords from the artifical ones for both techniques. We\nspeculate that a larger sample size could reveal a significant difference\nbetween the two HGT techniques, favouring our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2208.06946v2.pdf"
    },
    {
        "title": "Teacher Guided Training: An Efficient Framework for Knowledge Transfer",
        "authors": [
            "Manzil Zaheer",
            "Ankit Singh Rawat",
            "Seungyeon Kim",
            "Chong You",
            "Himanshu Jain",
            "Andreas Veit",
            "Rob Fergus",
            "Sanjiv Kumar"
        ],
        "published": "2022-08-14T10:33:58Z",
        "summary": "The remarkable performance gains realized by large pretrained models, e.g.,\nGPT-3, hinge on the massive amounts of data they are exposed to during\ntraining. Analogously, distilling such large models to compact models for\nefficient deployment also necessitates a large amount of (labeled or unlabeled)\ntraining data. In this paper, we propose the teacher-guided training (TGT)\nframework for training a high-quality compact model that leverages the\nknowledge acquired by pretrained generative models, while obviating the need to\ngo through a large volume of data. TGT exploits the fact that the teacher has\nacquired a good representation of the underlying data domain, which typically\ncorresponds to a much lower dimensional manifold than the input space.\nFurthermore, we can use the teacher to explore input space more efficiently\nthrough sampling or gradient-based methods; thus, making TGT especially\nattractive for limited data or long-tail settings. We formally capture this\nbenefit of proposed data-domain exploration in our generalization bounds. We\nfind that TGT can improve accuracy on several image classification benchmarks\nas well as a range of text classification and retrieval tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.06825v1.pdf"
    },
    {
        "title": "Text Difficulty Study: Do machines behave the same as humans regarding text difficulty?",
        "authors": [
            "Bowen Chen",
            "Xiao Ding",
            "Li Du",
            "Qin Bing",
            "Ting Liu"
        ],
        "published": "2022-08-14T06:12:08Z",
        "summary": "Given a task, human learns from easy to hard, whereas the model learns\nrandomly. Undeniably, difficulty insensitive learning leads to great success in\nNLP, but little attention has been paid to the effect of text difficulty in\nNLP. In this research, we propose the Human Learning Matching Index (HLM Index)\nto investigate the effect of text difficulty. Experiment results show: (1) LSTM\nhas more human-like learning behavior than BERT. (2) UID-SuperLinear gives the\nbest evaluation of text difficulty among four text difficulty criteria. (3)\nAmong nine tasks, some tasks' performance is related to text difficulty,\nwhereas some are not. (4) Model trained on easy data performs best in easy and\nmedium data, whereas trains on a hard level only perform well on hard data. (5)\nTraining the model from easy to hard leads to fast convergence.",
        "pdf_link": "https://arxiv.org/pdf/2208.14509v1.pdf"
    },
    {
        "title": "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models",
        "authors": [
            "Xingyu Xie",
            "Pan Zhou",
            "Huan Li",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "published": "2022-08-13T16:04:39Z",
        "summary": "In deep learning, different kinds of deep networks typically need different\noptimizers, which have to be chosen after multiple trials, making the training\nprocess inefficient. To relieve this issue and consistently improve the model\ntraining speed across deep networks, we propose the ADAptive Nesterov momentum\nalgorithm, Adan for short. Adan first reformulates the vanilla Nesterov\nacceleration to develop a new Nesterov momentum estimation (NME) method, which\navoids the extra overhead of computing gradient at the extrapolation point.\nThen Adan adopts NME to estimate the gradient's first- and second-order moments\nin adaptive gradient algorithms for convergence acceleration. Besides, we prove\nthat Adan finds an $\\epsilon$-approximate first-order stationary point within\n$O(\\epsilon^{-3.5})$ stochastic gradient complexity on the non-convex\nstochastic problems (e.g., deep learning problems), matching the best-known\nlower bound. Extensive experimental results show that Adan consistently\nsurpasses the corresponding SoTA optimizers on vision, language, and RL tasks\nand sets new SoTAs for many popular networks and frameworks, e.g., ResNet,\nConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More\nsurprisingly, Adan can use half of the training cost (epochs) of SoTA\noptimizers to achieve higher or comparable performance on ViT, GPT-2, MAE,\ne.t.c., and also shows great tolerance to a large range of minibatch size,\ne.g., from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and\nhas been used in multiple popular deep learning frameworks or projects.",
        "pdf_link": "https://arxiv.org/pdf/2208.06677v4.pdf"
    },
    {
        "title": "Cloud-Based Real-Time Molecular Screening Platform with MolFormer",
        "authors": [
            "Brian Belgodere",
            "Vijil Chenthamarakshan",
            "Payel Das",
            "Pierre Dognin",
            "Toby Kurien",
            "Igor Melnyk",
            "Youssef Mroueh",
            "Inkit Padhi",
            "Mattia Rigotti",
            "Jarret Ross",
            "Yair Schiff",
            "Richard A. Young"
        ],
        "published": "2022-08-13T14:43:19Z",
        "summary": "With the prospect of automating a number of chemical tasks with high\nfidelity, chemical language processing models are emerging at a rapid speed.\nHere, we present a cloud-based real-time platform that allows users to\nvirtually screen molecules of interest. For this purpose, molecular embeddings\ninferred from a recently proposed large chemical language model, named\nMolFormer, are leveraged. The platform currently supports three tasks: nearest\nneighbor retrieval, chemical space visualization, and property prediction.\nBased on the functionalities of this platform and results obtained, we believe\nthat such a platform can play a pivotal role in automating chemistry and\nchemical engineering research, as well as assist in drug discovery and material\ndesign tasks. A demo of our platform is provided at\n\\url{www.ibm.biz/molecular_demo}.",
        "pdf_link": "https://arxiv.org/pdf/2208.06665v1.pdf"
    },
    {
        "title": "Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification",
        "authors": [
            "Emadeldeen Eldele",
            "Mohamed Ragab",
            "Zhenghua Chen",
            "Min Wu",
            "Chee-Keong Kwoh",
            "Xiaoli Li",
            "Cuntai Guan"
        ],
        "published": "2022-08-13T10:22:12Z",
        "summary": "Learning time-series representations when only unlabeled data or few labeled\nsamples are available can be a challenging task. Recently, contrastive\nself-supervised learning has shown great improvement in extracting useful\nrepresentations from unlabeled data via contrasting different augmented views\nof data. In this work, we propose a novel Time-Series representation learning\nframework via Temporal and Contextual Contrasting (TS-TCC) that learns\nrepresentations from unlabeled data with contrastive learning. Specifically, we\npropose time-series-specific weak and strong augmentations and use their views\nto learn robust temporal relations in the proposed temporal contrasting module,\nbesides learning discriminative representations by our proposed contextual\ncontrasting module. Additionally, we conduct a systematic study of time-series\ndata augmentation selection, which is a key part of contrastive learning. We\nalso extend TS-TCC to the semi-supervised learning settings and propose a\nClass-Aware TS-TCC (CA-TCC) that benefits from the available few labeled data\nto further improve representations learned by TS-TCC. Specifically, we leverage\nthe robust pseudo labels produced by TS-TCC to realize a class-aware\ncontrastive loss. Extensive experiments show that the linear evaluation of the\nfeatures learned by our proposed framework performs comparably with the fully\nsupervised training. Additionally, our framework shows high efficiency in the\nfew labeled data and transfer learning scenarios. The code is publicly\navailable at \\url{https://github.com/emadeldeen24/CA-TCC}.",
        "pdf_link": "https://arxiv.org/pdf/2208.06616v3.pdf"
    },
    {
        "title": "Interpreting BERT-based Text Similarity via Activation and Saliency Maps",
        "authors": [
            "Itzik Malkiel",
            "Dvir Ginzburg",
            "Oren Barkan",
            "Avi Caciularu",
            "Jonathan Weill",
            "Noam Koenigstein"
        ],
        "published": "2022-08-13T10:06:24Z",
        "summary": "Recently, there has been growing interest in the ability of Transformer-based\nmodels to produce meaningful embeddings of text with several applications, such\nas text similarity. Despite significant progress in the field, the explanations\nfor similarity predictions remain challenging, especially in unsupervised\nsettings. In this work, we present an unsupervised technique for explaining\nparagraph similarities inferred by pre-trained BERT models. By looking at a\npair of paragraphs, our technique identifies important words that dictate each\nparagraph's semantics, matches between the words in both paragraphs, and\nretrieves the most important pairs that explain the similarity between the two.\nThe method, which has been assessed by extensive human evaluations and\ndemonstrated on datasets comprising long and complex paragraphs, has shown\ngreat promise, providing accurate interpretations that correlate better with\nhuman perceptions.",
        "pdf_link": "https://arxiv.org/pdf/2208.06612v1.pdf"
    },
    {
        "title": "MetricBERT: Text Representation Learning via Self-Supervised Triplet Training",
        "authors": [
            "Itzik Malkiel",
            "Dvir Ginzburg",
            "Oren Barkan",
            "Avi Caciularu",
            "Yoni Weill",
            "Noam Koenigstein"
        ],
        "published": "2022-08-13T09:52:58Z",
        "summary": "We present MetricBERT, a BERT-based model that learns to embed text under a\nwell-defined similarity metric while simultaneously adhering to the\n``traditional'' masked-language task. We focus on downstream tasks of learning\nsimilarities for recommendations where we show that MetricBERT outperforms\nstate-of-the-art alternatives, sometimes by a substantial margin. We conduct\nextensive evaluations of our method and its different variants, showing that\nour training objective is highly beneficial over a traditional contrastive\nloss, a standard cosine similarity objective, and six other baselines. As an\nadditional contribution, we publish a dataset of video games descriptions along\nwith a test set of similarity annotations crafted by a domain expert.",
        "pdf_link": "https://arxiv.org/pdf/2208.06610v1.pdf"
    },
    {
        "title": "LM-CORE: Language Models with Contextually Relevant External Knowledge",
        "authors": [
            "Jivat Neet Kaur",
            "Sumit Bhatia",
            "Milan Aggarwal",
            "Rachit Bansal",
            "Balaji Krishnamurthy"
        ],
        "published": "2022-08-12T18:59:37Z",
        "summary": "Large transformer-based pre-trained language models have achieved impressive\nperformance on a variety of knowledge-intensive tasks and can capture factual\nknowledge in their parameters. We argue that storing large amounts of knowledge\nin the model parameters is sub-optimal given the ever-growing amounts of\nknowledge and resource requirements. We posit that a more efficient alternative\nis to provide explicit access to contextually relevant structured knowledge to\nthe model and train it to use that knowledge. We present LM-CORE -- a general\nframework to achieve this -- that allows \\textit{decoupling} of the language\nmodel training from the external knowledge source and allows the latter to be\nupdated without affecting the already trained model. Experimental results show\nthat LM-CORE, having access to external knowledge, achieves significant and\nrobust outperformance over state-of-the-art knowledge-enhanced language models\non knowledge probing tasks; can effectively handle knowledge updates; and\nperforms well on two downstream tasks. We also present a thorough error\nanalysis highlighting the successes and failures of LM-CORE.",
        "pdf_link": "https://arxiv.org/pdf/2208.06458v1.pdf"
    },
    {
        "title": "What is it like to program with artificial intelligence?",
        "authors": [
            "Advait Sarkar",
            "Andrew D. Gordon",
            "Carina Negreanu",
            "Christian Poelitz",
            "Sruti Srinivasa Ragavan",
            "Ben Zorn"
        ],
        "published": "2022-08-12T10:48:46Z",
        "summary": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can\ngenerate code to solve a variety of problems expressed in natural language.\nThis technology has already been commercialised in at least one widely-used\nprogramming editor extension: GitHub Copilot.\n  In this paper, we explore how programming with large language models\n(LLM-assisted programming) is similar to, and differs from, prior\nconceptualisations of programmer assistance. We draw upon publicly available\nexperience reports of LLM-assisted programming, as well as prior usability and\ndesign studies. We find that while LLM-assisted programming shares some\nproperties of compilation, pair programming, and programming via search and\nreuse, there are fundamental differences both in the technical possibilities as\nwell as the practical experience. Thus, LLM-assisted programming ought to be\nviewed as a new way of programming with its own distinct properties and\nchallenges.\n  Finally, we draw upon observations from a user study in which non-expert end\nuser programmers use LLM-assisted tools for solving data tasks in spreadsheets.\nWe discuss the issues that might arise, and open research challenges, in\napplying large language models to end-user programming, particularly with users\nwho have little or no programming expertise.",
        "pdf_link": "https://arxiv.org/pdf/2208.06213v2.pdf"
    },
    {
        "title": "A Twitter-Driven Deep Learning Mechanism for the Determination of Vehicle Hijacking Spots in Cities",
        "authors": [
            "Taahir Aiyoob Patel",
            "Clement N. Nyirenda"
        ],
        "published": "2022-08-11T21:56:34Z",
        "summary": "Vehicle hijacking is one of the leading crimes in many cities. For instance,\nin South Africa, drivers must constantly remain vigilant on the road in order\nto ensure that they do not become hijacking victims. This work is aimed at\ndeveloping a map depicting hijacking spots in a city by using Twitter data.\nTweets, which include the keyword \"hijacking\", are obtained in a designated\ncity of Cape Town, in this work. In order to extract relevant tweets, these\ntweets are analyzed by using the following machine learning techniques: 1) a\nMulti-layer Feed-forward Neural Network (MLFNN); 2) Convolutional Neural\nNetwork; and Bidirectional Encoder Representations from Transformers (BERT).\nThrough training and testing, CNN achieved an accuracy of 99.66%, while MLFNN\nand BERT achieve accuracies of 98.99% and 73.99% respectively. In terms of\nRecall, Precision and F1-score, CNN also achieved the best results. Therefore,\nCNN was used for the identification of relevant tweets. The relevant reports\nthat it generates are visually presented on a points map of the City of Cape\nTown. This work used a small dataset of 426 tweets. In future, the use of\nevolutionary computation will be explored for purposes of optimizing the deep\nlearning models. A mobile application is under development to make this\ninformation usable by the general public.",
        "pdf_link": "https://arxiv.org/pdf/2208.10280v1.pdf"
    },
    {
        "title": "Bayesian Soft Actor-Critic: A Directed Acyclic Strategy Graph Based Deep Reinforcement Learning",
        "authors": [
            "Qin Yang",
            "Ramviyas Parasuraman"
        ],
        "published": "2022-08-11T20:36:23Z",
        "summary": "Adopting reasonable strategies is challenging but crucial for an intelligent\nagent with limited resources working in hazardous, unstructured, and dynamic\nenvironments to improve the system's utility, decrease the overall cost, and\nincrease mission success probability. This paper proposes a novel directed\nacyclic strategy graph decomposition approach based on Bayesian chaining to\nseparate an intricate policy into several simple sub-policies and organize\ntheir relationships as Bayesian strategy networks (BSN). We integrate this\napproach into the state-of-the-art DRL method -- soft actor-critic (SAC), and\nbuild the corresponding Bayesian soft actor-critic (BSAC) model by organizing\nseveral sub-policies as a joint policy. We compare our method against the\nstate-of-the-art deep reinforcement learning algorithms on the standard\ncontinuous control benchmarks in the OpenAI Gym environment. The results\ndemonstrate that the promising potential of the BSAC method significantly\nimproves training efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2208.06033v2.pdf"
    },
    {
        "title": "New drugs and stock market: how to predict pharma market reaction to clinical trial announcements",
        "authors": [
            "Semen Budennyy",
            "Alexey Kazakov",
            "Elizaveta Kovtun",
            "Leonid Zhukov"
        ],
        "published": "2022-08-11T20:20:21Z",
        "summary": "Pharmaceutical companies operate in a strictly regulated and highly risky\nenvironment in which a single slip can lead to serious financial implications.\nAccordingly, the announcements of clinical trial results tend to determine the\nfuture course of events, hence being closely monitored by the public. In this\nwork, we provide statistical evidence for the result promulgation influence on\nthe public pharma market value. Whereas most works focus on retrospective\nimpact analysis, the present research aims to predict the numerical values of\nannouncement-induced changes in stock prices. For this purpose, we develop a\npipeline that includes a BERT-based model for extracting sentiment polarity of\nannouncements, a Temporal Fusion Transformer for forecasting the expected\nreturn, a graph convolution network for capturing event relationships, and\ngradient boosting for predicting the price change. The challenge of the problem\nlies in inherently different patterns of responses to positive and negative\nannouncements, reflected in a stronger and more pronounced reaction to the\nnegative news. Moreover, such phenomenon as the drop in stocks after the\npositive announcements affirms the counterintuitiveness of the price behavior.\nImportantly, we discover two crucial factors that should be considered while\nworking within a predictive framework. The first factor is the drug portfolio\nsize of the company, indicating the greater susceptibility to an announcement\nin the case of small drug diversification. The second one is the network effect\nof the events related to the same company or nosology. All findings and\ninsights are gained on the basis of one of the biggest FDA (the Food and Drug\nAdministration) announcement datasets, consisting of 5436 clinical trial\nannouncements from 681 companies over the last five years.",
        "pdf_link": "https://arxiv.org/pdf/2208.07248v2.pdf"
    },
    {
        "title": "Interactive Code Generation via Test-Driven User-Intent Formalization",
        "authors": [
            "Shuvendu K. Lahiri",
            "Sarah Fakhoury",
            "Aaditya Naik",
            "Georgios Sakkas",
            "Saikat Chakraborty",
            "Madanlal Musuvathi",
            "Piali Choudhury",
            "Curtis von Veh",
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Jianfeng Gao"
        ],
        "published": "2022-08-11T17:41:08Z",
        "summary": "Large language models (LLMs) have shown great potential in automating\nsignificant aspects of coding by producing natural code from informal natural\nlanguage (NL) intent. However, when interacting with LLMs, users have no\nguarantees that the code suggestions produced correctly satisfy the intent they\nprovided. In fact, it is hard to define a notion of correctness since natural\nlanguage can be ambiguous and lacks a formal semantics.\n  In this paper, we propose the workflow of {\\it interactive test-driven code\ngeneration}, which leverages lightweight user feedback to (a) formalize the\nuser intent using generated tests that can be useful for debugging, and (b)\nproduce an improved set of code suggestions by pruning and ranking candidate\ncode suggestions. We describe a language-agnostic abstract algorithm and a\nconcrete implementation TiCoder. We perform an automated evaluation of TiCoder\non the \\emph{MBPP} and \\emph{HumanEval} code generation benchmarks. Our results\nare promising with using the OpenAI Codex LLM: our best algorithm improves the\n\\passk{1} code generation accuracy (in absolute percentages) between $22.49\\%$\nto $37.71\\%$ for MBPP and between $24.79\\%$ to $53.98\\%$ for HumanEval using\nbetween 1 to 5 simulated user queries.",
        "pdf_link": "https://arxiv.org/pdf/2208.05950v2.pdf"
    },
    {
        "title": "A Model of Anaphoric Ambiguities using Sheaf Theoretic Quantum-like Contextuality and BERT",
        "authors": [
            "Kin Ian Lo",
            "Mehrnoosh Sadrzadeh",
            "Shane Mansfield"
        ],
        "published": "2022-08-11T09:31:15Z",
        "summary": "Ambiguities of natural language do not preclude us from using it and context\nhelps in getting ideas across. They, nonetheless, pose a key challenge to the\ndevelopment of competent machines to understand natural language and use it as\nhumans do. Contextuality is an unparalleled phenomenon in quantum mechanics,\nwhere different mathematical formalisms have been put forwards to understand\nand reason about it. In this paper, we construct a schema for anaphoric\nambiguities that exhibits quantum-like contextuality. We use a recently\ndeveloped criterion of sheaf-theoretic contextuality that is applicable to\nsignalling models. We then take advantage of the neural word embedding engine\nBERT to instantiate the schema to natural language examples and extract\nprobability distributions for the instances. As a result, plenty of\nsheaf-contextual examples were discovered in the natural language corpora BERT\nutilises. Our hope is that these examples will pave the way for future research\nand for finding ways to extend applications of quantum computing to natural\nlanguage processing.",
        "pdf_link": "https://arxiv.org/pdf/2208.05720v1.pdf"
    },
    {
        "title": "Re-creation of Creations: A New Paradigm for Lyric-to-Melody Generation",
        "authors": [
            "Ang Lv",
            "Xu Tan",
            "Tao Qin",
            "Tie-Yan Liu",
            "Rui Yan"
        ],
        "published": "2022-08-11T08:44:47Z",
        "summary": "Lyric-to-melody generation is an important task in songwriting, and is also\nquite challenging due to its unique characteristics: the generated melodies\nshould not only follow good musical patterns, but also align with features in\nlyrics such as rhythms and structures. These characteristics cannot be well\nhandled by neural generation models that learn lyric-to-melody mapping in an\nend-to-end way, due to several issues: (1) lack of aligned lyric-melody\ntraining data to sufficiently learn lyric-melody feature alignment; (2) lack of\ncontrollability in generation to better and explicitly align the lyric-melody\nfeatures. In this paper, we propose Re-creation of Creations (ROC), a new\nparadigm for lyric-to-melody generation. ROC generates melodies according to\ngiven lyrics and also conditions on user-designated chord progression. It\naddresses the above issues through a generation-retrieval pipeline.\nSpecifically, our paradigm has two stages: (1) creation stage, where a huge\namount of music fragments generated by a neural melody language model are\nindexed in a database through several key features (e.g., chords, tonality,\nrhythm, and structural information); (2) re-creation stage, where melodies are\nre-created by retrieving music fragments from the database according to the key\nfeatures from lyrics and concatenating best music fragments based on\ncomposition guidelines and melody language model scores. ROC has several\nadvantages: (1) It only needs unpaired melody data to train melody language\nmodel, instead of paired lyric-melody data in previous models. (2) It achieves\ngood lyric-melody feature alignment in lyric-to-melody generation. Tested by\nEnglish and Chinese lyrics, ROC outperforms previous neural based\nlyric-to-melody generation models on both objective and subjective metrics.",
        "pdf_link": "https://arxiv.org/pdf/2208.05697v4.pdf"
    },
    {
        "title": "Searching for chromate replacements using natural language processing and machine learning algorithms",
        "authors": [
            "Shujing Zhao",
            "Nick Birbilis"
        ],
        "published": "2022-08-11T07:21:18Z",
        "summary": "The past few years has seen the application of machine learning utilised in\nthe exploration of new materials. As in many fields of research - the vast\nmajority of knowledge is published as text, which poses challenges in either a\nconsolidated or statistical analysis across studies and reports. Such\nchallenges include the inability to extract quantitative information, and in\naccessing the breadth of non-numerical information. To address this issue, the\napplication of natural language processing (NLP) has been explored in several\nstudies to date. In NLP, assignment of high-dimensional vectors, known as\nembeddings, to passages of text preserves the syntactic and semantic\nrelationship between words. Embeddings rely on machine learning algorithms and\nin the present work, we have employed the Word2Vec model, previously explored\nby others, and the BERT model - applying them towards a unique challenge in\nmaterials engineering. That challenge is the search for chromate replacements\nin the field of corrosion protection. From a database of over 80 million\nrecords, a down-selection of 5990 papers focused on the topic of corrosion\nprotection were examined using NLP. This study demonstrates it is possible to\nextract knowledge from the automated interpretation of the scientific\nliterature and achieve expert human level insights.",
        "pdf_link": "https://arxiv.org/pdf/2208.05672v1.pdf"
    },
    {
        "title": "Reducing Retraining by Recycling Parameter-Efficient Prompts",
        "authors": [
            "Brian Lester",
            "Joshua Yurtsever",
            "Siamak Shakeri",
            "Noah Constant"
        ],
        "published": "2022-08-10T22:10:53Z",
        "summary": "Parameter-efficient methods are able to use a single frozen pre-trained large\nlanguage model (LLM) to perform many tasks by learning task-specific soft\nprompts that modulate model behavior when concatenated to the input text.\nHowever, these learned prompts are tightly coupled to a given frozen model --\nif the model is updated, corresponding new prompts need to be obtained. In this\nwork, we propose and investigate several approaches to \"Prompt Recycling'\"\nwhere a prompt trained on a source model is transformed to work with the new\ntarget model. Our methods do not rely on supervised pairs of prompts,\ntask-specific data, or training updates with the target model, which would be\njust as costly as re-tuning prompts with the target model from scratch. We show\nthat recycling between models is possible (our best settings are able to\nsuccessfully recycle $88.9\\%$ of prompts, producing a prompt that out-performs\nbaselines), but significant performance headroom remains, requiring improved\nrecycling techniques.",
        "pdf_link": "https://arxiv.org/pdf/2208.05577v1.pdf"
    },
    {
        "title": "CoditT5: Pretraining for Source Code and Natural Language Editing",
        "authors": [
            "Jiyang Zhang",
            "Sheena Panthaplackel",
            "Pengyu Nie",
            "Junyi Jessy Li",
            "Milos Gligoric"
        ],
        "published": "2022-08-10T16:59:40Z",
        "summary": "Pretrained language models have been shown to be effective in many\nsoftware-related generation tasks; however, they are not well-suited for\nediting tasks as they are not designed to reason about edits. To address this,\nwe propose a novel pretraining objective which explicitly models edits and use\nit to build CoditT5, a large language model for software-related editing tasks\nthat is pretrained on large amounts of source code and natural language\ncomments. We fine-tune it on various downstream editing tasks, including\ncomment updating, bug fixing, and automated code review. By outperforming\nstandard generation-based models, we demonstrate the generalizability of our\napproach and its suitability for editing tasks. We also show how a standard\ngeneration model and our edit-based model can complement one another through\nsimple reranking strategies, with which we achieve state-of-the-art performance\nfor the three downstream editing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2208.05446v2.pdf"
    },
    {
        "title": "Generative Action Description Prompts for Skeleton-based Action Recognition",
        "authors": [
            "Wangmeng Xiang",
            "Chao Li",
            "Yuxuan Zhou",
            "Biao Wang",
            "Lei Zhang"
        ],
        "published": "2022-08-10T12:55:56Z",
        "summary": "Skeleton-based action recognition has recently received considerable\nattention. Current approaches to skeleton-based action recognition are\ntypically formulated as one-hot classification tasks and do not fully exploit\nthe semantic relations between actions. For example, \"make victory sign\" and\n\"thumb up\" are two actions of hand gestures, whose major difference lies in the\nmovement of hands. This information is agnostic from the categorical one-hot\nencoding of action classes but could be unveiled from the action description.\nTherefore, utilizing action description in training could potentially benefit\nrepresentation learning. In this work, we propose a Generative\nAction-description Prompts (GAP) approach for skeleton-based action\nrecognition. More specifically, we employ a pre-trained large-scale language\nmodel as the knowledge engine to automatically generate text descriptions for\nbody parts movements of actions, and propose a multi-modal training scheme by\nutilizing the text encoder to generate feature vectors for different body parts\nand supervise the skeleton encoder for action representation learning.\nExperiments show that our proposed GAP method achieves noticeable improvements\nover various baseline models without extra computation cost at inference. GAP\nachieves new state-of-the-arts on popular skeleton-based action recognition\nbenchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is\navailable at https://github.com/MartinXM/GAP.",
        "pdf_link": "https://arxiv.org/pdf/2208.05318v2.pdf"
    },
    {
        "title": "Controlling Perceived Emotion in Symbolic Music Generation with Monte Carlo Tree Search",
        "authors": [
            "Lucas N. Ferreira",
            "Lili Mou",
            "Jim Whitehead",
            "Levi H. S. Lelis"
        ],
        "published": "2022-08-10T05:49:37Z",
        "summary": "This paper presents a new approach for controlling emotion in symbolic music\ngeneration with Monte Carlo Tree Search. We use Monte Carlo Tree Search as a\ndecoding mechanism to steer the probability distribution learned by a language\nmodel towards a given emotion. At every step of the decoding process, we use\nPredictor Upper Confidence for Trees (PUCT) to search for sequences that\nmaximize the average values of emotion and quality as given by an emotion\nclassifier and a discriminator, respectively. We use a language model as PUCT's\npolicy and a combination of the emotion classifier and the discriminator as its\nvalue function. To decode the next token in a piece of music, we sample from\nthe distribution of node visits created during the search. We evaluate the\nquality of the generated samples with respect to human-composed pieces using a\nset of objective metrics computed directly from the generated samples. We also\nperform a user study to evaluate how human subjects perceive the generated\nsamples' quality and emotion. We compare PUCT against Stochastic Bi-Objective\nBeam Search (SBBS) and Conditional Sampling (CS). Results suggest that PUCT\noutperforms SBBS and CS in almost all metrics of music quality and emotion.",
        "pdf_link": "https://arxiv.org/pdf/2208.05162v4.pdf"
    },
    {
        "title": "Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology",
        "authors": [
            "Sangjoon Park",
            "Eun Sun Lee",
            "Kyung Sook Shin",
            "Jeong Eun Lee",
            "Jong Chul Ye"
        ],
        "published": "2022-08-10T04:35:58Z",
        "summary": "Oversight AI is an emerging concept in radiology where the AI forms a\nsymbiosis with radiologists by continuously supporting radiologists in their\ndecision-making. Recent advances in vision-language models sheds a light on the\nlong-standing problems of the oversight AI by the understanding both visual and\ntextual concepts and their semantic correspondences. However, there have been\nlimited successes in the application of vision-language models in the medical\ndomain, as the current vision-language models and learning strategies for\nphotographic images and captions call for the web-scale data corpus of image\nand text pairs which was not often feasible in the medical domain. To address\nthis, here we present a model dubbed Medical Cross-attention Vision-Language\nmodel (Medical X-VL), leveraging the key components to be tailored for the\nmedical domain. Our medical X-VL model is based on the following components:\nself-supervised uni-modal models in medical domain and fusion encoder to bridge\nthem, momentum distillation, sentence-wise contrastive learning for medical\nreports, and the sentence similarity-adjusted hard negative mining. We\nexperimentally demonstrated that our model enables various zero-shot tasks for\noversight AI, ranging from the zero-shot classification to zero-shot error\ncorrection. Our model outperformed the current state-of-the-art models in two\ndifferent medical image database, suggesting the novel clinical usage of our\noversight AI model for monitoring human errors. Our method was especially\nsuccessful in the data-limited setting, which is frequently encountered in the\nclinics, suggesting the potential widespread applicability in medical domain.",
        "pdf_link": "https://arxiv.org/pdf/2208.05140v4.pdf"
    },
    {
        "title": "Increasing Students' Engagement to Reminder Emails Through Multi-Armed Bandits",
        "authors": [
            "Fernando J. Yanez",
            "Angela Zavaleta-Bernuy",
            "Ziwen Han",
            "Michael Liut",
            "Anna Rafferty",
            "Joseph Jay Williams"
        ],
        "published": "2022-08-10T00:30:52Z",
        "summary": "Conducting randomized experiments in education settings raises the question\nof how we can use machine learning techniques to improve educational\ninterventions. Using Multi-Armed Bandits (MAB) algorithms like Thompson\nSampling (TS) in adaptive experiments can increase students' chances of\nobtaining better outcomes by increasing the probability of assignment to the\nmost optimal condition (arm), even before an intervention completes. This is an\nadvantage over traditional A/B testing, which may allocate an equal number of\nstudents to both optimal and non-optimal conditions. The problem is the\nexploration-exploitation trade-off. Even though adaptive policies aim to\ncollect enough information to allocate more students to better arms reliably,\npast work shows that this may not be enough exploration to draw reliable\nconclusions about whether arms differ. Hence, it is of interest to provide\nadditional uniform random (UR) exploration throughout the experiment. This\npaper shows a real-world adaptive experiment on how students engage with\ninstructors' weekly email reminders to build their time management habits. Our\nmetric of interest is open email rates which tracks the arms represented by\ndifferent subject lines. These are delivered following different allocation\nalgorithms: UR, TS, and what we identified as TS{\\dag} - which combines both TS\nand UR rewards to update its priors. We highlight problems with these adaptive\nalgorithms - such as possible exploitation of an arm when there is no\nsignificant difference - and address their causes and consequences. Future\ndirections includes studying situations where the early choice of the optimal\narm is not ideal and how adaptive algorithms can address them.",
        "pdf_link": "https://arxiv.org/pdf/2208.05090v1.pdf"
    },
    {
        "title": "Thai Wav2Vec2.0 with CommonVoice V8",
        "authors": [
            "Wannaphong Phatthiyaphaibun",
            "Chompakorn Chaksangchaichot",
            "Peerat Limkonchotiwat",
            "Ekapol Chuangsuwanich",
            "Sarana Nutanong"
        ],
        "published": "2022-08-09T14:21:48Z",
        "summary": "Recently, Automatic Speech Recognition (ASR), a system that converts audio\ninto text, has caught a lot of attention in the machine learning community.\nThus, a lot of publicly available models were released in HuggingFace. However,\nmost of these ASR models are available in English; only a minority of the\nmodels are available in Thai. Additionally, most of the Thai ASR models are\nclosed-sourced, and the performance of existing open-sourced models lacks\nrobustness. To address this problem, we train a new ASR model on a pre-trained\nXLSR-Wav2Vec model with the Thai CommonVoice corpus V8 and train a trigram\nlanguage model to boost the performance of our ASR model. We hope that our\nmodels will be beneficial to individuals and the ASR community in Thailand.",
        "pdf_link": "https://arxiv.org/pdf/2208.04799v1.pdf"
    },
    {
        "title": "E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes",
        "authors": [
            "Tu Anh Dinh",
            "Jeroen den Boef",
            "Joran Cornelisse",
            "Paul Groth"
        ],
        "published": "2022-08-09T09:05:10Z",
        "summary": "Node classification utilizing text-based node attributes has many real-world\napplications, ranging from prediction of paper topics in academic citation\ngraphs to classification of user characteristics in social media networks.\nState-of-the-art node classification frameworks, such as GIANT, use a two-stage\npipeline: first embedding the text attributes of graph nodes then feeding the\nresulting embeddings into a node classification model. In this paper, we\neliminate these two stages and develop an end-to-end node classification model\nthat builds upon GIANT, called End-to-End-GIANT (E2EG). The tandem utilization\nof a main and an auxiliary classification objectives in our approach results in\na more robust model, enabling the BERT backbone to be switched out for a\ndistilled encoder with a 25% - 40% reduction in the number of parameters.\nMoreover, the model's end-to-end nature increases ease of use, as it avoids the\nneed of chaining multiple models for node classification. Compared to a\nGIANT+MLP baseline on the ogbn-arxiv and ogbn-products datasets, E2EG obtains\nslightly better accuracy in the transductive setting (+0.5%), while reducing\nmodel training time by up to 40%. Our model is also applicable in the inductive\nsetting, outperforming GIANT+MLP by up to +2.23%.",
        "pdf_link": "https://arxiv.org/pdf/2208.04609v2.pdf"
    },
    {
        "title": "Emotion Detection From Tweets Using a BERT and SVM Ensemble Model",
        "authors": [
            "Ionu\u0163-Alexandru Albu",
            "Stelian Sp\u00eenu"
        ],
        "published": "2022-08-09T05:32:29Z",
        "summary": "Automatic identification of emotions expressed in Twitter data has a wide\nrange of applications. We create a well-balanced dataset by adding a neutral\nclass to a benchmark dataset consisting of four emotions: fear, sadness, joy,\nand anger. On this extended dataset, we investigate the use of Support Vector\nMachine (SVM) and Bidirectional Encoder Representations from Transformers\n(BERT) for emotion recognition. We propose a novel ensemble model by combining\nthe two BERT and SVM models. Experiments show that the proposed model achieves\na state-of-the-art accuracy of 0.91 on emotion recognition in tweets.",
        "pdf_link": "https://arxiv.org/pdf/2208.04547v1.pdf"
    },
    {
        "title": "A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction",
        "authors": [
            "Weimin Lyu",
            "Xinyu Dong",
            "Rachel Wong",
            "Songzhu Zheng",
            "Kayley Abell-Hart",
            "Fusheng Wang",
            "Chao Chen"
        ],
        "published": "2022-08-09T03:49:52Z",
        "summary": "Deep-learning-based clinical decision support using structured electronic\nhealth records (EHR) has been an active research area for predicting risks of\nmortality and diseases. Meanwhile, large amounts of narrative clinical notes\nprovide complementary information, but are often not integrated into predictive\nmodels. In this paper, we provide a novel multimodal transformer to fuse\nclinical notes and structured EHR data for better prediction of in-hospital\nmortality. To improve interpretability, we propose an integrated gradients (IG)\nmethod to select important words in clinical notes and discover the critical\nstructured EHR features with Shapley values. These important words and clinical\nfeatures are visualized to assist with interpretation of the prediction\noutcomes. We also investigate the significance of domain adaptive pretraining\nand task adaptive fine-tuning on the Clinical BERT, which is used to learn the\nrepresentations of clinical notes. Experiments demonstrated that our model\noutperforms other methods (AUCPR: 0.538, AUCROC: 0.877, F1:0.490).",
        "pdf_link": "https://arxiv.org/pdf/2208.10240v2.pdf"
    },
    {
        "title": "Exploring Hate Speech Detection with HateXplain and BERT",
        "authors": [
            "Arvind Subramaniam",
            "Aryan Mehra",
            "Sayani Kundu"
        ],
        "published": "2022-08-09T01:32:44Z",
        "summary": "Hate Speech takes many forms to target communities with derogatory comments,\nand takes humanity a step back in societal progress. HateXplain is a recently\npublished and first dataset to use annotated spans in the form of rationales,\nalong with speech classification categories and targeted communities to make\nthe classification more humanlike, explainable, accurate and less biased. We\ntune BERT to perform this task in the form of rationales and class prediction,\nand compare our performance on different metrics spanning across accuracy,\nexplainability and bias. Our novelty is threefold. Firstly, we experiment with\nthe amalgamated rationale class loss with different importance values.\nSecondly, we experiment extensively with the ground truth attention values for\nthe rationales. With the introduction of conservative and lenient attentions,\nwe compare performance of the model on HateXplain and test our hypothesis.\nThirdly, in order to improve the unintended bias in our models, we use masking\nof the target community words and note the improvement in bias and\nexplainability metrics. Overall, we are successful in achieving model\nexplanability, bias removal and several incremental improvements on the\noriginal BERT implementation.",
        "pdf_link": "https://arxiv.org/pdf/2208.04489v1.pdf"
    },
    {
        "title": "Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts",
        "authors": [
            "Babak Hemmatian",
            "Lav R. Varshney"
        ],
        "published": "2022-08-08T20:59:16Z",
        "summary": "Recent work demonstrates a bias in the GPT-3 model towards generating violent\ntext completions when prompted about Muslims, compared with Christians and\nHindus. Two pre-registered replication attempts, one exact and one approximate,\nfound only the weakest bias in the more recent Instruct Series version of\nGPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent\ncompletions were observed. Additional pre-registered experiments, however,\nshowed that using common names associated with the religions in prompts yields\na highly significant increase in violent completions, also revealing a stronger\nsecond-order bias against Muslims. Names of Muslim celebrities from non-violent\ndomains resulted in relatively fewer violent completions, suggesting that\naccess to individualized information can steer the model away from using\nstereotypes. Nonetheless, content analysis revealed religion-specific violent\nthemes containing highly offensive ideas regardless of prompt format. Our\nresults show the need for additional debiasing of large language models to\naddress higher-order schemas and associations.",
        "pdf_link": "https://arxiv.org/pdf/2208.04417v2.pdf"
    },
    {
        "title": "When can I Speak? Predicting initiation points for spoken dialogue agents",
        "authors": [
            "Siyan Li",
            "Ashwin Paranjape",
            "Christopher D. Manning"
        ],
        "published": "2022-08-07T20:58:52Z",
        "summary": "Current spoken dialogue systems initiate their turns after a long period of\nsilence (700-1000ms), which leads to little real-time feedback, sluggish\nresponses, and an overall stilted conversational flow. Humans typically respond\nwithin 200ms and successfully predicting initiation points in advance would\nallow spoken dialogue agents to do the same. In this work, we predict the\nlead-time to initiation using prosodic features from a pre-trained speech\nrepresentation model (wav2vec 1.0) operating on user audio and word features\nfrom a pre-trained language model (GPT-2) operating on incremental\ntranscriptions. To evaluate errors, we propose two metrics w.r.t. predicted and\ntrue lead times. We train and evaluate the models on the Switchboard Corpus and\nfind that our method outperforms features from prior work on both metrics and\nvastly outperforms the common approach of waiting for 700ms of silence.",
        "pdf_link": "https://arxiv.org/pdf/2208.03812v1.pdf"
    },
    {
        "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
        "authors": [
            "Margaret Li",
            "Suchin Gururangan",
            "Tim Dettmers",
            "Mike Lewis",
            "Tim Althoff",
            "Noah A. Smith",
            "Luke Zettlemoyer"
        ],
        "published": "2022-08-05T17:46:38Z",
        "summary": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for\nembarrassingly parallel training of large language models (LLMs). We show it is\npossible to independently train subparts of a new class of LLMs on different\nsubsets of the data, eliminating the massive multi-node synchronization\ncurrently required to train LLMs. BTM learns a set of independent expert LMs\n(ELMs), each specialized to a different textual domain, such as scientific or\nlegal text. These ELMs can be added and removed to update data coverage,\nensembled to generalize to new domains, or averaged to collapse back to a\nsingle LM for efficient inference. New ELMs are learned by branching from\n(mixtures of) ELMs in the current set, further training the parameters on data\nfor the new domain, and then merging the resulting model back into the set for\nfuture use. Experiments show that BTM improves in- and out-of-domain\nperplexities as compared to GPT-style Transformer LMs, when controlling for\ntraining cost. Through extensive analysis, we show that these results are\nrobust to different ELM initialization schemes, but require expert domain\nspecialization; LM ensembles with random data splits do not perform well. We\nalso present a study of scaling BTM into a new corpus of 64 domains (192B\nwhitespace-separated tokens in total); the resulting LM (22.4B total\nparameters) performs as well as a Transformer LM trained with 2.5 times more\ncompute. These gains grow with the number of domains, suggesting more\naggressive parallelism could be used to efficiently train larger models in\nfuture work.",
        "pdf_link": "https://arxiv.org/pdf/2208.03306v1.pdf"
    },
    {
        "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
        "authors": [
            "Gautier Izacard",
            "Patrick Lewis",
            "Maria Lomeli",
            "Lucas Hosseini",
            "Fabio Petroni",
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Armand Joulin",
            "Sebastian Riedel",
            "Edouard Grave"
        ],
        "published": "2022-08-05T17:39:22Z",
        "summary": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2208.03299v3.pdf"
    },
    {
        "title": "Towards No.1 in CLUE Semantic Matching Challenge: Pre-trained Language Model Erlangshen with Propensity-Corrected Loss",
        "authors": [
            "Junjie Wang",
            "Yuxiang Zhang",
            "Ping Yang",
            "Ruyi Gan"
        ],
        "published": "2022-08-05T02:52:29Z",
        "summary": "This report describes a pre-trained language model Erlangshen with\npropensity-corrected loss, the No.1 in CLUE Semantic Matching Challenge. In the\npre-training stage, we construct a dynamic masking strategy based on knowledge\nin Masked Language Modeling (MLM) with whole word masking. Furthermore, by\nobserving the specific structure of the dataset, the pre-trained Erlangshen\napplies propensity-corrected loss (PCL) in the fine-tuning phase. Overall, we\nachieve 72.54 points in F1 Score and 78.90 points in Accuracy on the test set.\nOur code is publicly available at:\nhttps://github.com/IDEA-CCNL/Fengshenbang-LM/tree/hf-ds/fengshen/examples/clue_sim.",
        "pdf_link": "https://arxiv.org/pdf/2208.02959v1.pdf"
    },
    {
        "title": "Meaning without reference in large language models",
        "authors": [
            "Steven T. Piantadosi",
            "Felix Hill"
        ],
        "published": "2022-08-05T02:48:26Z",
        "summary": "The widespread success of large language models (LLMs) has been met with\nskepticism that they possess anything like human concepts or meanings. Contrary\nto claims that LLMs possess no meaning whatsoever, we argue that they likely\ncapture important aspects of meaning, and moreover work in a way that\napproximates a compelling account of human cognition in which meaning arises\nfrom conceptual role. Because conceptual role is defined by the relationships\nbetween internal representational states, meaning cannot be determined from a\nmodel's architecture, training data, or objective function, but only by\nexamination of how its internal states relate to each other. This approach may\nclarify why and how LLMs are so successful and suggest how they can be made\nmore human-like.",
        "pdf_link": "https://arxiv.org/pdf/2208.02957v2.pdf"
    },
    {
        "title": "LATTE: LAnguage Trajectory TransformEr",
        "authors": [
            "Arthur Bucker",
            "Luis Figueredo",
            "Sami Haddadin",
            "Ashish Kapoor",
            "Shuang Ma",
            "Sai Vemprala",
            "Rogerio Bonatti"
        ],
        "published": "2022-08-04T22:43:21Z",
        "summary": "Natural language is one of the most intuitive ways to express human intent.\nHowever, translating instructions and commands towards robotic motion\ngeneration and deployment in the real world is far from being an easy task. The\nchallenge of combining a robot's inherent low-level geometric and kinodynamic\nconstraints with a human's high-level semantic instructions traditionally is\nsolved using task-specific solutions with little generalizability between\nhardware platforms, often with the use of static sets of target actions and\ncommands. This work instead proposes a flexible language-based framework that\nallows a user to modify generic robotic trajectories. Our method leverages\npre-trained language models (BERT and CLIP) to encode the user's intent and\ntarget objects directly from a free-form text input and scene images, fuses\ngeometrical features generated by a transformer encoder network, and finally\noutputs trajectories using a transformer decoder, without the need of priors\nrelated to the task or robot information. We significantly extend our own\nprevious work presented in Bucker et al. by expanding the trajectory\nparametrization space to 3D and velocity as opposed to just XY movements. In\naddition, we now train the model to use actual images of the objects in the\nscene for context (as opposed to textual descriptions), and we evaluate the\nsystem in a diverse set of scenarios beyond manipulation, such as aerial and\nlegged robots. Our simulated and real-life experiments demonstrate that our\ntransformer model can successfully follow human intent, modifying the shape and\nspeed of trajectories within multiple environments. Codebase available at:\nhttps://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git",
        "pdf_link": "https://arxiv.org/pdf/2208.02918v3.pdf"
    },
    {
        "title": "Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models",
        "authors": [
            "Vil\u00e9m Zouhar",
            "Marius Mosbach",
            "Dietrich Klakow"
        ],
        "published": "2022-08-04T02:13:03Z",
        "summary": "Although masked language models are highly performant and widely adopted by\nNLP practitioners, they can not be easily used for autoregressive language\nmodelling (next word prediction and sequence probability estimation). We\npresent an LSTM-based autoregressive language model which uses prefix\nembeddings (from a pretrained masked language model) via fusion (e.g.\nconcatenation) to obtain a richer context representation for language\nmodelling. We find that fusion helps reliably in lowering the perplexity (16.74\n$\\rightarrow$ 15.80), which is even preserved after a transfer to a dataset\nfrom a different domain than the training data. We also evaluate the\nbest-performing fusion model by correlating its next word surprisal estimates\nwith human reading times. Contradicting our expectation, and despite the\nimprovement in perplexity overall, the correlation remains the same as for the\nbaseline model. Lastly, while we focus on language models pre-trained on text\nas the sources for the fusion, our approach can be possibly extended to fuse\nany information represented as a fixed-size vector into an auto-regressive\nlanguage model. These include e.g. sentence external information retrieved for\na knowledge base or representations of multi-modal encoders.",
        "pdf_link": "https://arxiv.org/pdf/2208.02402v2.pdf"
    },
    {
        "title": "A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis",
        "authors": [
            "Qibing Bai",
            "Tom Ko",
            "Yu Zhang"
        ],
        "published": "2022-08-03T16:21:08Z",
        "summary": "In human speech, the attitude of a speaker cannot be fully expressed only by\nthe textual content. It has to come along with the intonation. Declarative\nquestions are commonly used in daily Cantonese conversations, and they are\nusually uttered with rising intonation. Vanilla neural text-to-speech (TTS)\nsystems are not capable of synthesizing rising intonation for these sentences\ndue to the loss of semantic information. Though it has become more common to\ncomplement the systems with extra language models, their performance in\nmodeling rising intonation is not well studied. In this paper, we propose to\ncomplement the Cantonese TTS model with a BERT-based statement/question\nclassifier. We design different training strategies and compare their\nperformance. We conduct our experiments on a Cantonese corpus named CanTTS.\nEmpirical results show that the separate training approach obtains the best\ngeneralization performance and feasibility.",
        "pdf_link": "https://arxiv.org/pdf/2208.02189v1.pdf"
    },
    {
        "title": "KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports",
        "authors": [
            "Lars Hillebrand",
            "Tobias Deu\u00dfer",
            "Tim Dilmaghani",
            "Bernd Kliem",
            "R\u00fcdiger Loitz",
            "Christian Bauckhage",
            "Rafet Sifa"
        ],
        "published": "2022-08-03T15:21:28Z",
        "summary": "We present KPI-BERT, a system which employs novel methods of named entity\nrecognition (NER) and relation extraction (RE) to extract and link key\nperformance indicators (KPIs), e.g. \"revenue\" or \"interest expenses\", of\ncompanies from real-world German financial documents. Specifically, we\nintroduce an end-to-end trainable architecture that is based on Bidirectional\nEncoder Representations from Transformers (BERT) combining a recurrent neural\nnetwork (RNN) with conditional label masking to sequentially tag entities\nbefore it classifies their relations. Our model also introduces a learnable\nRNN-based pooling mechanism and incorporates domain expert knowledge by\nexplicitly filtering impossible relations. We achieve a substantially higher\nprediction performance on a new practical dataset of German financial reports,\noutperforming several strong baselines including a competing state-of-the-art\nspan-based entity tagging approach.",
        "pdf_link": "https://arxiv.org/pdf/2208.02140v1.pdf"
    },
    {
        "title": "Efficient Fine-Tuning of Compressed Language Models with Learners",
        "authors": [
            "Danilo Vucetic",
            "Mohammadreza Tayaranian",
            "Maryam Ziaeefard",
            "James J. Clark",
            "Brett H. Meyer",
            "Warren J. Gross"
        ],
        "published": "2022-08-03T13:42:30Z",
        "summary": "Fine-tuning BERT-based models is resource-intensive in memory, computation,\nand time. While many prior works aim to improve inference efficiency via\ncompression techniques, e.g., pruning, these works do not explicitly address\nthe computational challenges of training to downstream tasks. We introduce\nLearner modules and priming, novel methods for fine-tuning that exploit the\noverparameterization of pre-trained language models to gain benefits in\nconvergence speed and resource utilization. Learner modules navigate the double\nbind of 1) training efficiently by fine-tuning a subset of parameters, and 2)\ntraining effectively by ensuring quick convergence and high metric scores. Our\nresults on DistilBERT demonstrate that learners perform on par with or surpass\nthe baselines. Learners train 7x fewer parameters than state-of-the-art methods\non GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower\nresource utilization.",
        "pdf_link": "https://arxiv.org/pdf/2208.02070v1.pdf"
    },
    {
        "title": "Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language",
        "authors": [
            "Avi Shmidman",
            "Joshua Guedalia",
            "Shaltiel Shmidman",
            "Cheyn Shmuel Shmidman",
            "Eli Handel",
            "Moshe Koppel"
        ],
        "published": "2022-08-03T06:59:04Z",
        "summary": "We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed\nBerel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist\nfor processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on\nmodern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms\nof its lexicographical, morphological, syntactic and orthographic norms. We\ndemonstrate the superiority of Berel on Rabbinic texts via a challenge set of\nHebrew homographs. We release the new model and homograph challenge set for\nunrestricted use.",
        "pdf_link": "https://arxiv.org/pdf/2208.01875v1.pdf"
    },
    {
        "title": "VQ-T: RNN Transducers using Vector-Quantized Prediction Network States",
        "authors": [
            "Jiatong Shi",
            "George Saon",
            "David Haws",
            "Shinji Watanabe",
            "Brian Kingsbury"
        ],
        "published": "2022-08-03T02:45:52Z",
        "summary": "Beam search, which is the dominant ASR decoding algorithm for end-to-end\nmodels, generates tree-structured hypotheses. However, recent studies have\nshown that decoding with hypothesis merging can achieve a more efficient search\nwith comparable or better performance. But, the full context in recurrent\nnetworks is not compatible with hypothesis merging. We propose to use\nvector-quantized long short-term memory units (VQ-LSTM) in the prediction\nnetwork of RNN transducers. By training the discrete representation jointly\nwith the ASR network, hypotheses can be actively merged for lattice generation.\nOur experiments on the Switchboard corpus show that the proposed VQ RNN\ntransducers improve ASR performance over transducers with regular prediction\nnetworks while also producing denser lattices with a very low oracle word error\nrate (WER) for the same beam size. Additional language model rescoring\nexperiments also demonstrate the effectiveness of the proposed lattice\ngeneration scheme.",
        "pdf_link": "https://arxiv.org/pdf/2208.01818v1.pdf"
    },
    {
        "title": "Debiasing Gender Bias in Information Retrieval Models",
        "authors": [
            "Dhanasekar Sundararaman",
            "Vivek Subramanian"
        ],
        "published": "2022-08-02T21:12:05Z",
        "summary": "Biases in culture, gender, ethnicity, etc. have existed for decades and have\naffected many areas of human social interaction. These biases have been shown\nto impact machine learning (ML) models, and for natural language processing\n(NLP), this can have severe consequences for downstream tasks. Mitigating\ngender bias in information retrieval (IR) is important to avoid propagating\nstereotypes. In this work, we employ a dataset consisting of two components:\n(1) relevance of a document to a query and (2) \"gender\" of a document, in which\npronouns are replaced by male, female, and neutral conjugations. We\ndefinitively show that pre-trained models for IR do not perform well in\nzero-shot retrieval tasks when full fine-tuning of a large pre-trained BERT\nencoder is performed and that lightweight fine-tuning performed with adapter\nnetworks improves zero-shot retrieval performance almost by 20% over baseline.\nWe also illustrate that pre-trained models have gender biases that result in\nretrieved articles tending to be more often male than female. We overcome this\nby introducing a debiasing technique that penalizes the model when it prefers\nmales over females, resulting in an effective model that retrieves articles in\na balanced fashion across genders.",
        "pdf_link": "https://arxiv.org/pdf/2208.01755v3.pdf"
    },
    {
        "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
        "authors": [
            "Saleh Soltan",
            "Shankar Ananthakrishnan",
            "Jack FitzGerald",
            "Rahul Gupta",
            "Wael Hamza",
            "Haidar Khan",
            "Charith Peris",
            "Stephen Rawls",
            "Andy Rosenbaum",
            "Anna Rumshisky",
            "Chandana Satya Prakash",
            "Mukund Sridhar",
            "Fabian Triefenbach",
            "Apurv Verma",
            "Gokhan Tur",
            "Prem Natarajan"
        ],
        "published": "2022-08-02T13:30:07Z",
        "summary": "In this work, we demonstrate that multilingual large-scale\nsequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising\nand Causal Language Modeling (CLM) tasks, are more efficient few-shot learners\nthan decoder-only models on various tasks. In particular, we train a 20 billion\nparameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B)\nand show that it achieves state-of-the-art (SOTA) performance on 1-shot\nsummarization tasks, outperforming a much larger 540B PaLM decoder model.\nAlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for\nlow-resource languages, across almost all language pairs supported by the model\n(Arabic, English, French, German, Hindi, Italian, Japanese, Marathi,\nPortuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in\nzero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2\ndatasets and provides SOTA performance on multilingual tasks such as XNLI,\nXCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case\nfor seq2seq models as a powerful alternative to decoder-only models for\nLarge-scale Language Model (LLM) training.",
        "pdf_link": "https://arxiv.org/pdf/2208.01448v2.pdf"
    },
    {
        "title": "BERT4Loc: BERT for Location -- POI Recommender System",
        "authors": [
            "Syed Raza Bashir",
            "Shaina Raza",
            "Vojislav Misic"
        ],
        "published": "2022-08-02T11:46:59Z",
        "summary": "Recommending points of interest (POIs) is a challenging task that requires\nextracting comprehensive location data from location-based social media\nplatforms. To provide effective location-based recommendations, it's important\nto analyze users' historical behavior and preferences. In this study, we\npresent a sophisticated location-aware recommendation system that uses\nBidirectional Encoder Representations from Transformers (BERT) to offer\npersonalized location-based suggestions. Our model combines location\ninformation and user preferences to provide more relevant recommendations\ncompared to models that predict the next POI in a sequence. Our experiments on\ntwo benchmark dataset show that our BERT-based model outperforms various\nstate-of-the-art sequential models. Moreover, we see the effectiveness of the\nproposed model for quality through additional experiments.",
        "pdf_link": "https://arxiv.org/pdf/2208.01375v2.pdf"
    },
    {
        "title": "A Comparative Study on COVID-19 Fake News Detection Using Different Transformer Based Models",
        "authors": [
            "Sajib Kumar Saha Joy",
            "Dibyo Fabian Dofadar",
            "Riyo Hayat Khan",
            "Md. Sabbir Ahmed",
            "Rafeed Rahman"
        ],
        "published": "2022-08-02T10:50:16Z",
        "summary": "The rapid advancement of social networks and the convenience of internet\navailability have accelerated the rampant spread of false news and rumors on\nsocial media sites. Amid the COVID 19 epidemic, this misleading information has\naggravated the situation by putting peoples mental and physical lives in\ndanger. To limit the spread of such inaccuracies, identifying the fake news\nfrom online platforms could be the first and foremost step. In this research,\nthe authors have conducted a comparative analysis by implementing five\ntransformer based models such as BERT, BERT without LSTM, ALBERT, RoBERTa, and\na Hybrid of BERT & ALBERT in order to detect the fraudulent news of COVID 19\nfrom the internet. COVID 19 Fake News Dataset has been used for training and\ntesting the models. Among all these models, the RoBERTa model has performed\nbetter than other models by obtaining an F1 score of 0.98 in both real and fake\nclasses.",
        "pdf_link": "https://arxiv.org/pdf/2208.01355v1.pdf"
    },
    {
        "title": "Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories",
        "authors": [
            "Gizem Sogancioglu",
            "Fabian Mijsters",
            "Amar van Uden",
            "Jelle Peperzak"
        ],
        "published": "2022-08-02T10:02:21Z",
        "summary": "Clinical word embeddings are extensively used in various Bio-NLP problems as\na state-of-the-art feature vector representation. Although they are quite\nsuccessful at the semantic representation of words, due to the dataset - which\npotentially carries statistical and societal bias - on which they are trained,\nthey might exhibit gender stereotypes. This study analyses gender bias of\nclinical embeddings on three medical categories: mental disorders, sexually\ntransmitted diseases, and personality traits. To this extent, we analyze two\ndifferent pre-trained embeddings namely (contextualized) clinical-BERT and\n(non-contextualized) BioWordVec. We show that both embeddings are biased\ntowards sensitive gender groups but BioWordVec exhibits a higher bias than\nclinical-BERT for all three categories. Moreover, our analyses show that\nclinical embeddings carry a high degree of bias for some medical terms and\ndiseases which is conflicting with medical literature. Having such an\nill-founded relationship might cause harm in downstream applications that use\nclinical embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2208.01341v2.pdf"
    },
    {
        "title": "Automatic Classification of Bug Reports Based on Multiple Text Information and Reports' Intention",
        "authors": [
            "Fanqi Meng",
            "Xuesong Wang",
            "Jingdong Wang",
            "Peifang Wang"
        ],
        "published": "2022-08-02T06:44:51Z",
        "summary": "With the rapid growth of software scale and complexity, a large number of bug\nreports are submitted to the bug tracking system. In order to speed up defect\nrepair, these reports need to be accurately classified so that they can be sent\nto the appropriate developers. However, the existing classification methods\nonly use the text information of the bug report, which leads to their low\nperformance. To solve the above problems, this paper proposes a new automatic\nclassification method for bug reports. The innovation is that when categorizing\nbug reports, in addition to using the text information of the report, the\nintention of the report (i.e. suggestion or explanation) is also considered,\nthereby improving the performance of the classification. First, we collect bug\nreports from four ecosystems (Apache, Eclipse, Gentoo, Mozilla) and manually\nannotate them to construct an experimental data set. Then, we use Natural\nLanguage Processing technology to preprocess the data. On this basis, BERT and\nTF-IDF are used to extract the features of the intention and the multiple text\ninformation. Finally, the features are used to train the classifiers. The\nexperimental result on five classifiers (including K-Nearest Neighbor, Naive\nBayes, Logistic Regression, Support Vector Machine, and Random Forest) show\nthat our proposed method achieves better performance and its F-Measure achieves\nfrom 87.3% to 95.5%.",
        "pdf_link": "https://arxiv.org/pdf/2208.01274v1.pdf"
    },
    {
        "title": "Implicit Two-Tower Policies",
        "authors": [
            "Yunfan Zhao",
            "Qingkai Pan",
            "Krzysztof Choromanski",
            "Deepali Jain",
            "Vikas Sindhwani"
        ],
        "published": "2022-08-02T01:23:50Z",
        "summary": "We present a new class of structured reinforcement learning\npolicy-architectures, Implicit Two-Tower (ITT) policies, where the actions are\nchosen based on the attention scores of their learnable latent representations\nwith those of the input states. By explicitly disentangling action from state\nprocessing in the policy stack, we achieve two main goals: substantial\ncomputational gains and better performance. Our architectures are compatible\nwith both: discrete and continuous action spaces. By conducting tests on 15\nenvironments from OpenAI Gym and DeepMind Control Suite, we show that\nITT-architectures are particularly suited for blackbox/evolutionary\noptimization and the corresponding policy training algorithms outperform their\nvanilla unstructured implicit counterparts as well as commonly used explicit\npolicies. We complement our analysis by showing how techniques such as hashing\nand lazy tower updates, critically relying on the two-tower structure of ITTs,\ncan be applied to obtain additional computational improvements.",
        "pdf_link": "https://arxiv.org/pdf/2208.01191v2.pdf"
    },
    {
        "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
        "authors": [
            "Shivam Garg",
            "Dimitris Tsipras",
            "Percy Liang",
            "Gregory Valiant"
        ],
        "published": "2022-08-01T18:01:40Z",
        "summary": "In-context learning refers to the ability of a model to condition on a prompt\nsequence consisting of in-context examples (input-output pairs corresponding to\nsome task) along with a new query input, and generate the corresponding output.\nCrucially, in-context learning happens only at inference time without any\nparameter updates to the model. While large language models such as GPT-3\nexhibit some ability to perform in-context learning, it is unclear what the\nrelationship is between tasks on which this succeeds and what is present in the\ntraining data. To make progress towards understanding in-context learning, we\nconsider the well-defined problem of training a model to in-context learn a\nfunction class (e.g., linear functions): that is, given data derived from some\nfunctions in the class, can we train a model to in-context learn \"most\"\nfunctions from this class? We show empirically that standard Transformers can\nbe trained from scratch to perform in-context learning of linear functions --\nthat is, the trained model is able to learn unseen linear functions from\nin-context examples with performance comparable to the optimal least squares\nestimator. In fact, in-context learning is possible even under two forms of\ndistribution shift: (i) between the training data of the model and\ninference-time prompts, and (ii) between the in-context examples and the query\ninput during inference. We also show that we can train Transformers to\nin-context learn more complex function classes -- namely sparse linear\nfunctions, two-layer neural networks, and decision trees -- with performance\nthat matches or exceeds task-specific learning algorithms. Our code and models\nare available at https://github.com/dtsip/in-context-learning .",
        "pdf_link": "https://arxiv.org/pdf/2208.01066v3.pdf"
    },
    {
        "title": "giMLPs: Gate with Inhibition Mechanism in MLPs",
        "authors": [
            "Cheng Kang",
            "Jindich Prokop",
            "Lei Tong",
            "Huiyu Zhou",
            "Yong Hu",
            "Daneil Novak"
        ],
        "published": "2022-08-01T15:23:51Z",
        "summary": "This paper presents a new model architecture, gate with inhibition MLP\n(giMLP).The gate with inhibition on CycleMLP (gi-CycleMLP) can produce equal\nperformance on the ImageNet classification task, and it also improves the BERT,\nRoberta, and DeBERTaV3 models depending on two novel techniques. The first is\nthe gating MLP, where matrix multiplications between the MLP and the trunk\nAttention input in further adjust models' adaptation. The second is inhibition\nwhich inhibits or enhances the branch adjustment, and with the inhibition\nlevels increasing, it offers models more muscular features restriction. We show\nthat the giCycleMLP with a lower inhibition level can be competitive with the\noriginal CycleMLP in terms of ImageNet classification accuracy. In addition, we\nalso show through a comprehensive empirical study that these techniques\nsignificantly improve the performance of fine-tuning NLU downstream tasks. As\nfor the gate with inhibition MLPs on DeBERTa (giDeBERTa) fine-tuning, we find\nit can achieve appealing results on most parts of NLU tasks without any extra\npretraining again. We also find that with the use of Gate With Inhibition, the\nactivation function should have a short and smooth negative tail, with which\nthe unimportant features or the features that hurt models can be moderately\ninhibited. The experiments on ImageNet and twelve language downstream tasks\ndemonstrate the effectiveness of Gate With Inhibition, both for image\nclassification and for enhancing the capacity of nature language fine-tuning\nwithout any extra pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2208.00929v2.pdf"
    },
    {
        "title": "Learning from flowsheets: A generative transformer model for autocompletion of flowsheets",
        "authors": [
            "Gabriel Vogel",
            "Lukas Schulze Balhorn",
            "Artur M. Schweidtmann"
        ],
        "published": "2022-08-01T13:43:58Z",
        "summary": "We propose a novel method enabling autocompletion of chemical flowsheets.\nThis idea is inspired by the autocompletion of text. We represent flowsheets as\nstrings using the text-based SFILES 2.0 notation and learn the grammatical\nstructure of the SFILES 2.0 language and common patterns in flowsheets using a\ntransformer-based language model. We pre-train our model on synthetically\ngenerated flowsheets to learn the flowsheet language grammar. Then, we\nfine-tune our model in a transfer learning step on real flowsheet topologies.\nFinally, we use the trained model for causal language modeling to autocomplete\nflowsheets. Eventually, the proposed method can provide chemical engineers with\nrecommendations during interactive flowsheet synthesis. The results demonstrate\na high potential of this approach for future AI-assisted process synthesis.",
        "pdf_link": "https://arxiv.org/pdf/2208.00859v1.pdf"
    },
    {
        "title": "Composable Text Controls in Latent Space with ODEs",
        "authors": [
            "Guangyi Liu",
            "Zeyu Feng",
            "Yuan Gao",
            "Zichao Yang",
            "Xiaodan Liang",
            "Junwei Bao",
            "Xiaodong He",
            "Shuguang Cui",
            "Zhen Li",
            "Zhiting Hu"
        ],
        "published": "2022-08-01T06:51:45Z",
        "summary": "Real-world text applications often involve composing a wide range of text\ncontrol operations, such as editing the text w.r.t. an attribute, manipulating\nkeywords and structure, and generating new text of desired properties. Prior\nwork typically learns/finetunes a language model (LM) to perform individual or\nspecific subsets of operations. Recent research has studied combining\noperations in a plug-and-play manner, often with costly search or optimization\nin the complex sequence space. This paper proposes a new efficient approach for\ncomposable text operations in the compact latent space of text. The\nlow-dimensionality and differentiability of the text latent vector allow us to\ndevelop an efficient sampler based on ordinary differential equations (ODEs)\ngiven arbitrary plug-in operators (e.g., attribute classifiers). By connecting\npretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we\nthen decode the sampled vectors into desired text sequences. The flexible\napproach permits diverse control operators (sentiment, tense, formality,\nkeywords, etc.) acquired using any relevant data from different domains.\nExperiments show that composing those operators within our approach manages to\ngenerate or edit high-quality text, substantially improving over previous\nmethods in terms of generation quality and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2208.00638v3.pdf"
    },
    {
        "title": "Interacting with next-phrase suggestions: How suggestion systems aid and influence the cognitive processes of writing",
        "authors": [
            "Advait Bhat",
            "Saaket Agashe",
            "Niharika Mohile",
            "Parth Oberoi",
            "Ravi Jangir",
            "Anirudha Joshi"
        ],
        "published": "2022-08-01T06:49:07Z",
        "summary": "Writing with next-phrase suggestions powered by large language models is\nbecoming more pervasive by the day. However, research to understand writers'\ninteraction and decision-making processes while engaging with such systems is\nstill emerging. We conducted a qualitative study to shed light on writers'\ncognitive processes while writing with next-phrase suggestion systems. To do\nso, we recruited 14 amateur writers to write two reviews each, one without\nsuggestions and one with suggestions. Additionally, we also positively and\nnegatively biased the suggestion system to get a diverse range of instances\nwhere writers' opinions and the bias in the language model align or misalign to\nvarying degrees. We found that writers interact with next-phrase suggestions in\nvarious complex ways: Writers abstracted and extracted multiple parts of the\nsuggestions and incorporated them within their writing, even when they\ndisagreed with the suggestion as a whole; along with evaluating the suggestions\non various criteria. The suggestion system also had various effects on the\nwriting process, such as altering the writer's usual writing plans, leading to\nhigher levels of distraction etc. Based on our qualitative analysis using the\ncognitive process model of writing by Hayes as a lens, we propose a theoretical\nmodel of 'writer-suggestion interaction' for writing with GPT-2 (and causal\nlanguage models in general) for a movie review writing task, followed by\ndirections for future research and design.",
        "pdf_link": "https://arxiv.org/pdf/2208.00636v2.pdf"
    },
    {
        "title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning",
        "authors": [
            "Qianglong Chen",
            "Feng-Lin Li",
            "Guohai Xu",
            "Ming Yan",
            "Ji Zhang",
            "Yin Zhang"
        ],
        "published": "2022-08-01T06:43:19Z",
        "summary": "Although pre-trained language models (PLMs) have achieved state-of-the-art\nperformance on various natural language processing (NLP) tasks, they are shown\nto be lacking in knowledge when dealing with knowledge driven tasks. Despite\nthe many efforts made for injecting knowledge into PLMs, this problem remains\nopen. To address the challenge, we propose \\textbf{DictBERT}, a novel approach\nthat enhances PLMs with dictionary knowledge which is easier to acquire than\nknowledge graph (KG). During pre-training, we present two novel pre-training\ntasks to inject dictionary knowledge into PLMs via contrastive learning:\n\\textit{dictionary entry prediction} and \\textit{entry description\ndiscrimination}. In fine-tuning, we use the pre-trained DictBERT as a plugin\nknowledge base (KB) to retrieve implicit knowledge for identified entries in an\ninput sequence, and infuse the retrieved knowledge into the input to enhance\nits representation via a novel extra-hop attention mechanism. We evaluate our\napproach on a variety of knowledge driven and language understanding tasks,\nincluding NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE.\nExperimental results demonstrate that our model can significantly improve\ntypical PLMs: it gains a substantial improvement of 0.5\\%, 2.9\\%, 9.0\\%, 7.1\\%\nand 3.3\\% on BERT-large respectively, and is also effective on RoBERTa-large.",
        "pdf_link": "https://arxiv.org/pdf/2208.00635v1.pdf"
    },
    {
        "title": "PASTA: A Dataset for Modeling Participant States in Narratives",
        "authors": [
            "Sayontan Ghosh",
            "Mahnaz Koupaee",
            "Isabella Chen",
            "Francis Ferraro",
            "Nathanael Chambers",
            "Niranjan Balasubramanian"
        ],
        "published": "2022-07-31T01:21:48Z",
        "summary": "The events in a narrative are understood as a coherent whole via the\nunderlying states of their participants. Often, these participant states are\nnot explicitly mentioned, instead left to be inferred by the reader. A model\nthat understands narratives should likewise infer these implicit states, and\neven reason about the impact of changes to these states on the narrative. To\nfacilitate this goal, we introduce a new crowdsourced English-language,\nParticipant States dataset, PASTA. This dataset contains inferable participant\nstates; a counterfactual perturbation to each state; and the changes to the\nstory that would be necessary if the counterfactual were true. We introduce\nthree state-based reasoning tasks that test for the ability to infer when a\nstate is entailed by a story, to revise a story conditioned on a counterfactual\nstate, and to explain the most likely state change given a revised story.\nExperiments show that today's LLMs can reason about states to some degree, but\nthere is large room for improvement, especially in problems requiring access\nand ability to reason with diverse types of knowledge (e.g. physical,\nnumerical, factual).",
        "pdf_link": "https://arxiv.org/pdf/2208.00329v2.pdf"
    },
    {
        "title": "Smoothing Entailment Graphs with Language Models",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published": "2022-07-30T22:15:22Z",
        "summary": "The diversity and Zipfian frequency distribution of natural language\npredicates in corpora leads to sparsity in Entailment Graphs (EGs) built by\nOpen Relation Extraction (ORE). EGs are computationally efficient and\nexplainable models of natural language inference, but as symbolic models, they\nfail if a novel premise or hypothesis vertex is missing at test-time. We\npresent theory and methodology for overcoming such sparsity in symbolic models.\nFirst, we introduce a theory of optimal smoothing of EGs by constructing\ntransitive chains. We then demonstrate an efficient, open-domain, and\nunsupervised smoothing method using an off-the-shelf Language Model to find\napproximations of missing premise predicates. This improves recall by 25.1 and\n16.3 percentage points on two difficult directional entailment datasets, while\nraising average precision and maintaining model explainability. Further, in a\nQA task we show that EG smoothing is most useful for answering questions with\nlesser supporting text, where missing premise predicates are more costly.\nFinally, controlled experiments with WordNet confirm our theory and show that\nhypothesis smoothing is difficult, but possible in principle.",
        "pdf_link": "https://arxiv.org/pdf/2208.00318v2.pdf"
    },
    {
        "title": "A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond",
        "authors": [
            "Chaoning Zhang",
            "Chenshuang Zhang",
            "Junha Song",
            "John Seon Keun Yi",
            "Kang Zhang",
            "In So Kweon"
        ],
        "published": "2022-07-30T09:59:28Z",
        "summary": "Masked autoencoders are scalable vision learners, as the title of MAE\n\\cite{he2022masked}, which suggests that self-supervised learning (SSL) in\nvision might undertake a similar trajectory as in NLP. Specifically, generative\npretext tasks with the masked prediction (e.g., BERT) have become a de facto\nstandard SSL practice in NLP. By contrast, early attempts at generative methods\nin vision have been buried by their discriminative counterparts (like\ncontrastive learning); however, the success of mask image modeling has revived\nthe masking autoencoder (often termed denoising autoencoder in the past). As a\nmilestone to bridge the gap with BERT in NLP, masked autoencoder has attracted\nunprecedented attention for SSL in vision and beyond. This work conducts a\ncomprehensive survey of masked autoencoders to shed insight on a promising\ndirection of SSL. As the first to review SSL with masked autoencoders, this\nwork focuses on its application in vision by discussing its historical\ndevelopments, recent progress, and implications for diverse applications.",
        "pdf_link": "https://arxiv.org/pdf/2208.00173v1.pdf"
    },
    {
        "title": "SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter",
        "authors": [
            "Heng Ee Tay",
            "Mei Kuan Lim",
            "Chun Yong Chong"
        ],
        "published": "2022-07-29T08:08:15Z",
        "summary": "Conventional approaches to identify depression are not scalable, and the\npublic has limited awareness of mental health, especially in developing\ncountries. As evident by recent studies, social media has the potential to\ncomplement mental health screening on a greater scale. The vast amount of\nfirst-person narrative posts in chronological order can provide insights into\none's thoughts, feelings, behavior, or mood for some time, enabling a better\nunderstanding of depression symptoms reflected in the online space. In this\npaper, we propose SERCNN, which improves the user representation by (1)\nstacking two pretrained embeddings from different domains and (2) reintroducing\nthe embedding context to the MLP classifier. Our SERCNN shows great performance\nover state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold\ncross-validation setting. Since not all users share the same level of online\nactivity, we introduced the concept of a fixed observation window that\nquantifies the observation period in a predefined number of posts. With as\nminimal as 10 posts per user, SERCNN performed exceptionally well with an 87%\naccuracy, which is on par with the BERT model, while having 98% less in the\nnumber of parameters. Our findings open up a promising direction for detecting\ndepression on social media with a smaller number of posts for inference,\ntowards creating solutions for a cost-effective and timely intervention. We\nhope that our work can bring this research area closer to real-world adoption\nin existing clinical practice.",
        "pdf_link": "https://arxiv.org/pdf/2207.14535v2.pdf"
    },
    {
        "title": "Curriculum Learning for Data-Efficient Vision-Language Alignment",
        "authors": [
            "Tejas Srinivasan",
            "Xiang Ren",
            "Jesse Thomason"
        ],
        "published": "2022-07-29T07:45:56Z",
        "summary": "Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.",
        "pdf_link": "https://arxiv.org/pdf/2207.14525v1.pdf"
    },
    {
        "title": "Code Comment Inconsistency Detection with BERT and Longformer",
        "authors": [
            "Theo Steiner",
            "Rui Zhang"
        ],
        "published": "2022-07-29T02:43:51Z",
        "summary": "Comments, or natural language descriptions of source code, are standard\npractice among software developers. By communicating important aspects of the\ncode such as functionality and usage, comments help with software project\nmaintenance. However, when the code is modified without an accompanying\ncorrection to the comment, an inconsistency between the comment and code can\narise, which opens up the possibility for developer confusion and bugs. In this\npaper, we propose two models based on BERT (Devlin et al., 2019) and Longformer\n(Beltagy et al., 2020) to detect such inconsistencies in a natural language\ninference (NLI) context. Through an evaluation on a previously established\ncorpus of comment-method pairs both during and after code changes, we\ndemonstrate that our models outperform multiple baselines and yield comparable\nresults to the state-of-the-art models that exclude linguistic and lexical\nfeatures. We further discuss ideas for future research in using pretrained\nlanguage models for both inconsistency detection and automatic comment\nupdating.",
        "pdf_link": "https://arxiv.org/pdf/2207.14444v1.pdf"
    },
    {
        "title": "LAD: Language Models as Data for Zero-Shot Dialog",
        "authors": [
            "Shikib Mehri",
            "Yasemin Altun",
            "Maxine Eskenazi"
        ],
        "published": "2022-07-28T22:10:45Z",
        "summary": "To facilitate zero-shot generalization in taskoriented dialog, this paper\nproposes Language Models as Data (LAD). LAD is a paradigm for creating diverse\nand accurate synthetic data which conveys the necessary structural constraints\nand can be used to train a downstream neural dialog model. LAD leverages GPT-3\nto induce linguistic diversity. LAD achieves significant performance gains in\nzero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and\nnext action prediction (+11 F1). Furthermore, an interactive human evaluation\nshows that training with LAD is competitive with training on human dialogs. LAD\nis open-sourced, with the code and data available at\nhttps://github.com/Shikib/lad.",
        "pdf_link": "https://arxiv.org/pdf/2207.14393v1.pdf"
    },
    {
        "title": "Large Language Models and the Reverse Turing Test",
        "authors": [
            "Terrence Sejnowski"
        ],
        "published": "2022-07-28T21:22:47Z",
        "summary": "Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that are self-supervised and can be adapted with fine\ntuning to a wide range of natural language tasks, each of which previously\nwould have required a separate network model. This is one step closer to the\nextraordinary versatility of human language. GPT-3 and more recently LaMDA can\ncarry on dialogs with humans on many topics after minimal priming with a few\nexamples. However, there has been a wide range of reactions and debate on\nwhether these LLMs understand what they are saying or exhibit signs of\nintelligence. This high variance is exhibited in three interviews with LLMs\nreaching wildly different conclusions. A new possibility was uncovered that\ncould explain this divergence. What appears to be intelligence in LLMs may in\nfact be a mirror that reflects the intelligence of the interviewer, a\nremarkable twist that could be considered a Reverse Turing Test. If so, then by\nstudying interviews we may be learning more about the intelligence and beliefs\nof the interviewer than the intelligence of the LLMs. As LLMs become more\ncapable they may transform the way we interact with machines and how they\ninteract with each other. Increasingly, LLMs are being coupled with\nsensorimotor devices. LLMs can talk the talk, but can they walk the walk? A\nroad map for achieving artificial general autonomy is outlined with seven major\nimprovements inspired by brain systems. LLMs could be used to uncover new\ninsights into brain function by downloading brain data during natural\nbehaviors.",
        "pdf_link": "https://arxiv.org/pdf/2207.14382v9.pdf"
    },
    {
        "title": "Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions",
        "authors": [
            "Yanai Elazar",
            "Nora Kassner",
            "Shauli Ravfogel",
            "Amir Feder",
            "Abhilasha Ravichander",
            "Marius Mosbach",
            "Yonatan Belinkov",
            "Hinrich Sch\u00fctze",
            "Yoav Goldberg"
        ],
        "published": "2022-07-28T17:36:24Z",
        "summary": "Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.",
        "pdf_link": "https://arxiv.org/pdf/2207.14251v2.pdf"
    },
    {
        "title": "CrAM: A Compression-Aware Minimizer",
        "authors": [
            "Alexandra Peste",
            "Adrian Vladu",
            "Eldar Kurtic",
            "Christoph H. Lampert",
            "Dan Alistarh"
        ],
        "published": "2022-07-28T16:13:28Z",
        "summary": "Deep neural networks (DNNs) often have to be compressed, via pruning and/or\nquantization, before they can be deployed in practical settings. In this work\nwe propose a new compression-aware minimizer dubbed CrAM that modifies the\noptimization step in a principled way, in order to produce models whose local\nloss behavior is stable under compression operations such as pruning. Thus,\ndense models trained via CrAM should be compressible post-training, in a single\nstep, without significant accuracy loss. Experimental results on standard\nbenchmarks, such as residual networks for ImageNet classification and BERT\nmodels for language modelling, show that CrAM produces dense models that can be\nmore accurate than the standard SGD/Adam-based baselines, but which are stable\nunder weight pruning: specifically, we can prune models in one-shot to 70-80%\nsparsity with almost no accuracy loss, and to 90% with reasonable ($\\sim 1\\%$)\naccuracy loss, which is competitive with gradual compression methods.\nAdditionally, CrAM can produce sparse models which perform well for transfer\nlearning, and it also works for semi-structured 2:4 pruning patterns supported\nby GPU hardware. The code for reproducing the results is available at\nhttps://github.com/IST-DASLab/CrAM .",
        "pdf_link": "https://arxiv.org/pdf/2207.14200v4.pdf"
    },
    {
        "title": "Bayesian Optimization-Based Beam Alignment for MmWave MIMO Communication Systems",
        "authors": [
            "Songjie Yang",
            "Baojuan Liu",
            "Zhiqin Hong",
            "Zhongpei Zhang"
        ],
        "published": "2022-07-28T15:37:49Z",
        "summary": "Due to the very narrow beam used in millimeter wave communication (mmWave),\nbeam alignment (BA) is a critical issue. In this work, we investigate the issue\nof mmWave BA and present a novel beam alignment scheme on the basis of a\nmachine learning strategy, Bayesian optimization (BO). In this context, we\nconsider the beam alignment issue to be a black box function and then use BO to\nfind the possible optimal beam pair. During the BA procedure, this strategy\nexploits information from the measured beam pairs to predict the best beam\npair. In addition, we suggest a novel BO algorithm based on the gradient\nboosting regression tree model. The simulation results demonstrate the spectral\nefficiency performance of our proposed schemes for BA using three different\nsurrogate models. They also demonstrate that the proposed schemes can achieve\nspectral efficiency with a small overhead when compared to the orthogonal match\npursuit (OMP) algorithm and the Thompson sampling-based multi-armed bandit\n(TS-MAB) method.",
        "pdf_link": "https://arxiv.org/pdf/2207.14174v1.pdf"
    },
    {
        "title": "Entity Type Prediction Leveraging Graph Walks and Entity Descriptions",
        "authors": [
            "Russa Biswas",
            "Jan Portisch",
            "Heiko Paulheim",
            "Harald Sack",
            "Mehwish Alam"
        ],
        "published": "2022-07-28T13:56:55Z",
        "summary": "The entity type information in Knowledge Graphs (KGs) such as DBpedia,\nFreebase, etc. is often incomplete due to automated generation or human\ncuration. Entity typing is the task of assigning or inferring the semantic type\nof an entity in a KG. This paper presents \\textit{GRAND}, a novel approach for\nentity typing leveraging different graph walk strategies in RDF2vec together\nwith textual entity descriptions. RDF2vec first generates graph walks and then\nuses a language model to obtain embeddings for each node in the graph. This\nstudy shows that the walk generation strategy and the embedding model have a\nsignificant effect on the performance of the entity typing task. The proposed\napproach outperforms the baseline approaches on the benchmark datasets DBpedia\nand FIGER for entity typing in KGs for both fine-grained and coarse-grained\nclasses. The results show that the combination of order-aware RDF2vec variants\ntogether with the contextual embeddings of the textual entity descriptions\nachieve the best results.",
        "pdf_link": "https://arxiv.org/pdf/2207.14094v2.pdf"
    },
    {
        "title": "Sequence to sequence pretraining for a less-resourced Slovenian language",
        "authors": [
            "Matej Ul\u010dar",
            "Marko Robnik-\u0160ikonja"
        ],
        "published": "2022-07-28T10:08:50Z",
        "summary": "Large pretrained language models have recently conquered the area of natural\nlanguage processing. As an alternative to predominant masked language modelling\nintroduced in BERT, the T5 model has introduced a more general training\nobjective, namely sequence to sequence transformation, which includes masked\nlanguage model but more naturally fits text generation tasks such as machine\ntranslation, summarization, question answering, text simplification, dialogue\nsystems, etc. The monolingual variants of T5 models have been limited to\nwell-resourced languages, while the massively multilingual T5 model supports\n101 languages. In contrast, we trained two different sized T5-type sequence to\nsequence models for morphologically rich Slovene language with much less\nresources and analyzed their behavior on 11 tasks. Concerning classification\ntasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa\nmodel but are useful for the generative tasks.",
        "pdf_link": "https://arxiv.org/pdf/2207.13988v2.pdf"
    },
    {
        "title": "ClaSP -- Parameter-free Time Series Segmentation",
        "authors": [
            "Arik Ermshaus",
            "Patrick Sch\u00e4fer",
            "Ulf Leser"
        ],
        "published": "2022-07-28T10:05:53Z",
        "summary": "The study of natural and human-made processes often results in long sequences\nof temporally-ordered values, aka time series (TS). Such processes often\nconsist of multiple states, e.g. operating modes of a machine, such that state\nchanges in the observed processes result in changes in the distribution of\nshape of the measured values. Time series segmentation (TSS) tries to find such\nchanges in TS post-hoc to deduce changes in the data-generating process. TSS is\ntypically approached as an unsupervised learning problem aiming at the\nidentification of segments distinguishable by some statistical property.\nCurrent algorithms for TSS require domain-dependent hyper-parameters to be set\nby the user, make assumptions about the TS value distribution or the types of\ndetectable changes which limits their applicability. Common hyperparameters are\nthe measure of segment homogeneity and the number of change points, which are\nparticularly hard to tune for each data set. We present ClaSP, a novel, highly\naccurate, hyper-parameter-free and domain-agnostic method for TSS. ClaSP\nhierarchically splits a TS into two parts. A change point is determined by\ntraining a binary TS classifier for each possible split point and selecting the\none split that is best at identifying subsequences to be from either of the\npartitions. ClaSP learns its main two model-parameters from the data using two\nnovel bespoke algorithms. In our experimental evaluation using a benchmark of\n107 data sets, we show that ClaSP outperforms the state of the art in terms of\naccuracy and is fast and scalable. Furthermore, we highlight properties of\nClaSP using several real-world case studies.",
        "pdf_link": "https://arxiv.org/pdf/2207.13987v2.pdf"
    },
    {
        "title": "RangL: A Reinforcement Learning Competition Platform",
        "authors": [
            "Viktor Zobernig",
            "Richard A. Saldanha",
            "Jinke He",
            "Erica van der Sar",
            "Jasper van Doorn",
            "Jia-Chen Hua",
            "Lachlan R. Mason",
            "Aleksander Czechowski",
            "Drago Indjic",
            "Tomasz Kosmala",
            "Alessandro Zocca",
            "Sandjai Bhulai",
            "Jorge Montalvo Arvizu",
            "Claude Kl\u00f6ckl",
            "John Moriarty"
        ],
        "published": "2022-07-28T09:44:21Z",
        "summary": "The RangL project hosted by The Alan Turing Institute aims to encourage the\nwider uptake of reinforcement learning by supporting competitions relating to\nreal-world dynamic decision problems. This article describes the reusable code\nrepository developed by the RangL team and deployed for the 2022 Pathways to\nNet Zero Challenge, supported by the UK Net Zero Technology Centre. The winning\nsolutions to this particular Challenge seek to optimize the UK's energy\ntransition policy to net zero carbon emissions by 2050. The RangL repository\nincludes an OpenAI Gym reinforcement learning environment and code that\nsupports both submission to, and evaluation in, a remote instance of the open\nsource EvalAI platform as well as all winning learning agent strategies. The\nrepository is an illustrative example of RangL's capability to provide a\nreusable structure for future challenges.",
        "pdf_link": "https://arxiv.org/pdf/2208.00003v1.pdf"
    },
    {
        "title": "Knowing Where and What: Unified Word Block Pretraining for Document Understanding",
        "authors": [
            "Song Tao",
            "Zijian Wang",
            "Tiantian Fan",
            "Canjie Luo",
            "Can Huang"
        ],
        "published": "2022-07-28T09:43:06Z",
        "summary": "Due to the complex layouts of documents, it is challenging to extract\ninformation for documents. Most previous studies develop multimodal pre-trained\nmodels in a self-supervised way. In this paper, we focus on the embedding\nlearning of word blocks containing text and layout information, and propose\nUTel, a language model with Unified TExt and Layout pre-training. Specifically,\nwe propose two pre-training tasks: Surrounding Word Prediction (SWP) for the\nlayout learning, and Contrastive learning of Word Embeddings (CWE) for\nidentifying different word blocks. Moreover, we replace the commonly used 1D\nposition embedding with a 1D clipped relative position embedding. In this way,\nthe joint training of Masked Layout-Language Modeling (MLLM) and two newly\nproposed tasks enables the interaction between semantic and spatial features in\na unified way. Additionally, the proposed UTel can process arbitrary-length\nsequences by removing the 1D position embedding, while maintaining competitive\nperformance. Extensive experimental results show UTel learns better joint\nrepresentations and achieves superior performance than previous methods on\nvarious downstream tasks, though requiring no image modality. Code is available\nat \\url{https://github.com/taosong2019/UTel}.",
        "pdf_link": "https://arxiv.org/pdf/2207.13979v2.pdf"
    },
    {
        "title": "MLRIP: Pre-training a military language representation model with informative factual knowledge and professional knowledge base",
        "authors": [
            "Hui Li",
            "Xuekang Yang",
            "Xin Zhao",
            "Lin Yu",
            "Jiping Zheng",
            "Wei Sun"
        ],
        "published": "2022-07-28T07:39:30Z",
        "summary": "Incorporating prior knowledge into pre-trained language models has proven to\nbe effective for knowledge-driven NLP tasks, such as entity typing and relation\nextraction. Current pre-training procedures usually inject external knowledge\ninto models by using knowledge masking, knowledge fusion and knowledge\nreplacement. However, factual information contained in the input sentences have\nnot been fully mined, and the external knowledge for injecting have not been\nstrictly checked. As a result, the context information cannot be fully\nexploited and extra noise will be introduced or the amount of knowledge\ninjected is limited. To address these issues, we propose MLRIP, which modifies\nthe knowledge masking strategies proposed by ERNIE-Baidu, and introduce a\ntwo-stage entity replacement strategy. Extensive experiments with comprehensive\nanalyses illustrate the superiority of MLRIP over BERT-based models in military\nknowledge-driven NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2207.13929v1.pdf"
    },
    {
        "title": "SDBERT: SparseDistilBERT, a faster and smaller BERT model",
        "authors": [
            "Devaraju Vinoda",
            "Pawan Kumar Yadav"
        ],
        "published": "2022-07-28T07:34:07Z",
        "summary": "In this work we introduce a new transformer architecture called\nSparseDistilBERT (SDBERT), which is a combination of sparse attention and\nknowledge distillantion (KD). We implemented sparse attention mechanism to\nreduce quadratic dependency on input length to linear. In addition to reducing\ncomputational complexity of the model, we used knowledge distillation (KD). We\nwere able to reduce the size of BERT model by 60% while retaining 97%\nperformance and it only took 40% of time to train.",
        "pdf_link": "https://arxiv.org/pdf/2208.10246v1.pdf"
    },
    {
        "title": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative",
        "authors": [
            "Xiaomin Fang",
            "Fan Wang",
            "Lihang Liu",
            "Jingzhou He",
            "Dayong Lin",
            "Yingfei Xiang",
            "Xiaonan Zhang",
            "Hua Wu",
            "Hui Li",
            "Le Song"
        ],
        "published": "2022-07-28T07:30:33Z",
        "summary": "AI-based protein structure prediction pipelines, such as AlphaFold2, have\nachieved near-experimental accuracy. These advanced pipelines mainly rely on\nMultiple Sequence Alignments (MSAs) as inputs to learn the co-evolution\ninformation from the homologous sequences. Nonetheless, searching MSAs from\nprotein databases is time-consuming, usually taking dozens of minutes.\nConsequently, we attempt to explore the limits of fast protein structure\nprediction by using only primary sequences of proteins. HelixFold-Single is\nproposed to combine a large-scale protein language model with the superior\ngeometric learning capability of AlphaFold2. Our proposed method,\nHelixFold-Single, first pre-trains a large-scale protein language model (PLM)\nwith thousands of millions of primary sequences utilizing the self-supervised\nlearning paradigm, which will be used as an alternative to MSAs for learning\nthe co-evolution information. Then, by combining the pre-trained PLM and the\nessential components of AlphaFold2, we obtain an end-to-end differentiable\nmodel to predict the 3D coordinates of atoms from only the primary sequence.\nHelixFold-Single is validated in datasets CASP14 and CAMEO, achieving\ncompetitive accuracy with the MSA-based methods on the targets with large\nhomologous families. Furthermore, HelixFold-Single consumes much less time than\nthe mainstream pipelines for protein structure prediction, demonstrating its\npotential in tasks requiring many predictions. The code of HelixFold-Single is\navailable at\nhttps://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single,\nand we also provide stable web services on\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.",
        "pdf_link": "https://arxiv.org/pdf/2207.13921v3.pdf"
    },
    {
        "title": "Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms",
        "authors": [
            "Baturay Saglam",
            "Dogan C. Cicek",
            "Furkan B. Mutlu",
            "Suleyman S. Kozat"
        ],
        "published": "2022-07-27T11:10:50Z",
        "summary": "Learning in high dimensional continuous tasks is challenging, mainly when the\nexperience replay memory is very limited. We introduce a simple yet effective\nexperience sharing mechanism for deterministic policies in continuous action\ndomains for the future off-policy deep reinforcement learning applications in\nwhich the allocated memory for the experience replay buffer is limited. To\novercome the extrapolation error induced by learning from other agents'\nexperiences, we facilitate our algorithm with a novel off-policy correction\ntechnique without any action probability estimates. We test the effectiveness\nof our method in challenging OpenAI Gym continuous control tasks and conclude\nthat it can achieve a safe experience sharing across multiple agents and\nexhibits a robust performance when the replay memory is strictly limited.",
        "pdf_link": "https://arxiv.org/pdf/2207.13453v1.pdf"
    },
    {
        "title": "RealTime QA: What's the Answer Right Now?",
        "authors": [
            "Jungo Kasai",
            "Keisuke Sakaguchi",
            "Yoichi Takahashi",
            "Ronan Le Bras",
            "Akari Asai",
            "Xinyan Yu",
            "Dragomir Radev",
            "Noah A. Smith",
            "Yejin Choi",
            "Kentaro Inui"
        ],
        "published": "2022-07-27T07:26:01Z",
        "summary": "We introduce REALTIME QA, a dynamic question answering (QA) platform that\nannounces questions and evaluates systems on a regular basis (weekly in this\nversion). REALTIME QA inquires about the current world, and QA systems need to\nanswer questions about novel events or information. It therefore challenges\nstatic, conventional assumptions in open-domain QA datasets and pursues\ninstantaneous applications. We build strong baseline models upon large\npretrained language models, including GPT-3 and T5. Our benchmark is an ongoing\neffort, and this paper presents real-time evaluation results over the past\nyear. Our experimental results show that GPT-3 can often properly update its\ngeneration results, based on newly-retrieved documents, highlighting the\nimportance of up-to-date information retrieval. Nonetheless, we find that GPT-3\ntends to return outdated answers when retrieved documents do not provide\nsufficient information to find an answer. This suggests an important avenue for\nfuture research: can an open-domain QA system identify such unanswerable cases\nand communicate with the user or even the retrieval module to modify the\nretrieval results? We hope that REALTIME QA will spur progress in instantaneous\napplications of question answering and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2207.13332v2.pdf"
    },
    {
        "title": "Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation",
        "authors": [
            "Jingjie Yi",
            "Deqing Yang",
            "Siyu Yuan",
            "Caiyan Cao",
            "Zhiyao Zhang",
            "Yanghua Xiao"
        ],
        "published": "2022-07-27T02:34:05Z",
        "summary": "Emotion recognition in conversation (ERC) aims to detect the emotion for each\nutterance in a given conversation. The newly proposed ERC models have leveraged\npre-trained language models (PLMs) with the paradigm of pre-training and\nfine-tuning to obtain good performance. However, these models seldom exploit\nPLMs' advantages thoroughly, and perform poorly for the conversations lacking\nexplicit emotional expressions. In order to fully leverage the latent knowledge\nrelated to the emotional expressions in utterances, we propose a novel ERC\nmodel CISPER with the new paradigm of prompt and language model (LM) tuning.\nSpecifically, CISPER is equipped with the prompt blending the contextual\ninformation and commonsense related to the interlocutor's utterances, to\nachieve ERC more effectively. Our extensive experiments demonstrate CISPER's\nsuperior performance over the state-of-the-art ERC models, and the\neffectiveness of leveraging these two kinds of significant prompt information\nfor performance gains. To reproduce our experimental results conveniently,\nCISPER's sourcecode and the datasets have been shared at\nhttps://github.com/DeqingYang/CISPER.",
        "pdf_link": "https://arxiv.org/pdf/2207.13254v1.pdf"
    },
    {
        "title": "SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation",
        "authors": [
            "Artem Ploujnikov",
            "Mirco Ravanelli"
        ],
        "published": "2022-07-27T01:14:59Z",
        "summary": "End-to-end speech synthesis models directly convert the input characters into\nan audio representation (e.g., spectrograms). Despite their impressive\nperformance, such models have difficulty disambiguating the pronunciations of\nidentically spelled words. To mitigate this issue, a separate\nGrapheme-to-Phoneme (G2P) model can be employed to convert the characters into\nphonemes before synthesizing the audio. This paper proposes SoundChoice, a\nnovel G2P architecture that processes entire sentences rather than operating at\nthe word level. The proposed architecture takes advantage of a weighted\nhomograph loss (that improves disambiguation), exploits curriculum learning\n(that gradually switches from word-level to sentence-level G2P), and integrates\nword embeddings from BERT (for further performance improvement). Moreover, the\nmodel inherits the best practices in speech recognition, including multi-task\nlearning with Connectionist Temporal Classification (CTC) and beam search with\nan embedded language model. As a result, SoundChoice achieves a Phoneme Error\nRate (PER) of 2.65% on whole-sentence transcription using data from LibriSpeech\nand Wikipedia. Index Terms grapheme-to-phoneme, speech synthesis,\ntext-tospeech, phonetics, pronunciation, disambiguation.",
        "pdf_link": "https://arxiv.org/pdf/2207.13703v1.pdf"
    },
    {
        "title": "Boosting Point-BERT by Multi-choice Tokens",
        "authors": [
            "Kexue Fu",
            "Mingzhi Yuan",
            "Manning Wang"
        ],
        "published": "2022-07-27T00:34:33Z",
        "summary": "Masked language modeling (MLM) has become one of the most successful\nself-supervised pre-training task. Inspired by its success, Point-BERT, as a\npioneer work in point cloud, proposed masked point modeling (MPM) to pre-train\npoint transformer on large scale unanotated dataset. Despite its great\nperformance, we find the inherent difference between language and point cloud\ntends to cause ambiguous tokenization for point cloud. For point cloud, there\ndoesn't exist a gold standard for point cloud tokenization. Point-BERT use a\ndiscrete Variational AutoEncoder (dVAE) as tokenizer, but it might generate\ndifferent token ids for semantically-similar patches and generate the same\ntoken ids for semantically-dissimilar patches. To tackle above problem, we\npropose our McP-BERT, a pre-training framework with multi-choice tokens.\nSpecifically, we ease the previous single-choice constraint on patch token ids\nin Point-BERT, and provide multi-choice token ids for each patch as\nsupervision. Moreover, we utilitze the high-level semantics learned by\ntransformer to further refine our supervision signals. Extensive experiments on\npoint cloud classification, few-shot classification and part segmentation tasks\ndemonstrate the superiority of our method, e.g., the pre-trained transformer\nachieves 94.1% accuracy on ModelNet40, 84.28% accuracy on the hardest setting\nof ScanObjectNN and new state-of-the-art performance on few-shot learning. We\nalso demonstrate that our method not only improves the performance of\nPoint-BERT on all downstream tasks, but also incurs almost no extra\ncomputational overhead. The code will be released in\nhttps://github.com/fukexue/McP-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2207.13226v2.pdf"
    },
    {
        "title": "When BERT Fails -- The Limits of EHR Classification",
        "authors": [
            "Augusto Garcia-Agundez",
            "Carsten Eickhoff"
        ],
        "published": "2022-07-26T17:18:24Z",
        "summary": "Transformers are powerful text representation learners, useful for all kinds\nof clinical decision support tasks. Although they outperform baselines on\nreadmission prediction, they are not infallible. Here, we look into one such\nfailure case, and report patterns that lead to inferior predictive performance.",
        "pdf_link": "https://arxiv.org/pdf/2208.10245v1.pdf"
    },
    {
        "title": "Learning structures of the French clinical language:development and validation of word embedding models using 21 million clinical reports from electronic health records",
        "authors": [
            "Basile Dura",
            "Charline Jean",
            "Xavier Tannier",
            "Alice Calliger",
            "Romain Bey",
            "Antoine Neuraz",
            "R\u00e9mi Flicoteaux"
        ],
        "published": "2022-07-26T14:46:34Z",
        "summary": "Background\n  Clinical studies using real-world data may benefit from exploiting clinical\nreports, a particularly rich albeit unstructured medium. To that end, natural\nlanguage processing can extract relevant information. Methods based on transfer\nlearning using pre-trained language models have achieved state-of-the-art\nresults in most NLP applications; however, publicly available models lack\nexposure to speciality-languages, especially in the medical field.\n  Objective\n  We aimed to evaluate the impact of adapting a language model to French\nclinical reports on downstream medical NLP tasks.\n  Methods\n  We leveraged a corpus of 21M clinical reports collected from August 2017 to\nJuly 2021 at the Greater Paris University Hospitals (APHP) to produce two\nCamemBERT architectures on speciality language: one retrained from scratch and\nthe other using CamemBERT as its initialisation. We used two French annotated\nmedical datasets to compare our language models to the original CamemBERT\nnetwork, evaluating the statistical significance of improvement with the\nWilcoxon test.\n  Results\n  Our models pretrained on clinical reports increased the average F1-score on\nAPMed (an APHP-specific task) by 3 percentage points to 91%, a statistically\nsignificant improvement. They also achieved performance comparable to the\noriginal CamemBERT on QUAERO. These results hold true for the fine-tuned and\nfrom-scratch versions alike, starting from very few pre-training samples.\n  Conclusions\n  We confirm previous literature showing that adapting generalist pre-train\nlanguage models such as CamenBERT on speciality corpora improves their\nperformance for downstream clinical NLP tasks. Our results suggest that\nretraining from scratch does not induce a statistically significant performance\ngain compared to fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2207.12940v1.pdf"
    },
    {
        "title": "Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases",
        "authors": [
            "S\u0142awomir Dadas"
        ],
        "published": "2022-07-26T09:08:56Z",
        "summary": "Sentence embeddings are commonly used in text clustering and semantic\nretrieval tasks. State-of-the-art sentence representation methods are based on\nartificial neural networks fine-tuned on large collections of manually labeled\nsentence pairs. Sufficient amount of annotated data is available for\nhigh-resource languages such as English or Chinese. In less popular languages,\nmultilingual models have to be used, which offer lower performance. In this\npublication, we address this problem by proposing a method for training\neffective language-specific sentence encoders without manually labeled data.\nOur approach is to automatically construct a dataset of paraphrase pairs from\nsentence-aligned bilingual text corpora. We then use the collected data to\nfine-tune a Transformer language model with an additional recurrent pooling\nlayer. Our sentence encoder can be trained in less than a day on a single\ngraphics card, achieving high performance on a diverse set of sentence-level\ntasks. We evaluate our method on eight linguistic tasks in Polish, comparing it\nwith the best available multilingual sentence encoders.",
        "pdf_link": "https://arxiv.org/pdf/2207.12759v1.pdf"
    },
    {
        "title": "Bundle MCR: Towards Conversational Bundle Recommendation",
        "authors": [
            "Zhankui He",
            "Handong Zhao",
            "Tong Yu",
            "Sungchul Kim",
            "Fan Du",
            "Julian McAuley"
        ],
        "published": "2022-07-26T03:28:42Z",
        "summary": "Bundle recommender systems recommend sets of items (e.g., pants, shirt, and\nshoes) to users, but they often suffer from two issues: significant interaction\nsparsity and a large output space. In this work, we extend multi-round\nconversational recommendation (MCR) to alleviate these issues. MCR, which uses\na conversational paradigm to elicit user interests by asking user preferences\non tags (e.g., categories or attributes) and handling user feedback across\nmultiple rounds, is an emerging recommendation setting to acquire user feedback\nand narrow down the output space, but has not been explored in the context of\nbundle recommendation. In this work, we propose a novel recommendation task\nnamed Bundle MCR. We first propose a new framework to formulate Bundle MCR as\nMarkov Decision Processes (MDPs) with multiple agents, for user modeling,\nconsultation and feedback handling in bundle contexts. Under this framework, we\npropose a model architecture, called Bundle Bert (Bunt) to (1) recommend items,\n(2) post questions and (3) manage conversations based on bundle-aware\nconversation states. Moreover, to train Bunt effectively, we propose a\ntwo-stage training strategy. In an offline pre-training stage, Bunt is trained\nusing multiple cloze tasks to mimic bundle interactions in conversations. Then\nin an online fine-tuning stage, Bunt agents are enhanced by user interactions.\nOur experiments on multiple offline datasets as well as the human evaluation\nshow the value of extending MCR frameworks to bundle settings and the\neffectiveness of our Bunt design.",
        "pdf_link": "https://arxiv.org/pdf/2207.12628v1.pdf"
    },
    {
        "title": "Modelling non-reinforced preferences using selective attention",
        "authors": [
            "Noor Sajid",
            "Panagiotis Tigas",
            "Zafeirios Fountas",
            "Qinghai Guo",
            "Alexey Zakharov",
            "Lancelot Da Costa"
        ],
        "published": "2022-07-25T22:01:32Z",
        "summary": "How can artificial agents learn non-reinforced preferences to continuously\nadapt their behaviour to a changing environment? We decompose this question\ninto two challenges: ($i$) encoding diverse memories and ($ii$) selectively\nattending to these for preference formation. Our proposed\n\\emph{no}n-\\emph{re}inforced preference learning mechanism using selective\nattention, \\textsc{Nore}, addresses both by leveraging the agent's world model\nto collect a diverse set of experiences which are interleaved with imagined\nroll-outs to encode memories. These memories are selectively attended to, using\nattention and gating blocks, to update agent's preferences. We validate\n\\textsc{Nore} in a modified OpenAI Gym FrozenLake environment (without any\nexternal signal) with and without volatility under a fixed model of the\nenvironment -- and compare its behaviour to \\textsc{Pepper}, a Hebbian\npreference learning mechanism. We demonstrate that \\textsc{Nore} provides a\nstraightforward framework to induce exploratory preferences in the absence of\nexternal signals.",
        "pdf_link": "https://arxiv.org/pdf/2207.13699v1.pdf"
    },
    {
        "title": "A Hazard Analysis Framework for Code Synthesis Large Language Models",
        "authors": [
            "Heidy Khlaaf",
            "Pamela Mishkin",
            "Joshua Achiam",
            "Gretchen Krueger",
            "Miles Brundage"
        ],
        "published": "2022-07-25T20:44:40Z",
        "summary": "Codex, a large language model (LLM) trained on a variety of codebases,\nexceeds the previous state of the art in its capacity to synthesize and\ngenerate code. Although Codex provides a plethora of benefits, models that may\ngenerate code on such scale have significant limitations, alignment problems,\nthe potential to be misused, and the possibility to increase the rate of\nprogress in technical fields that may themselves have destabilizing impacts or\nhave misuse potential. Yet such safety impacts are not yet known or remain to\nbe explored. In this paper, we outline a hazard analysis framework constructed\nat OpenAI to uncover hazards or safety risks that the deployment of models like\nCodex may impose technically, socially, politically, and economically. The\nanalysis is informed by a novel evaluation framework that determines the\ncapacity of advanced code generation techniques against the complexity and\nexpressivity of specification prompts, and their capability to understand and\nexecute them relative to human ability.",
        "pdf_link": "https://arxiv.org/pdf/2207.14157v1.pdf"
    },
    {
        "title": "Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment",
        "authors": [
            "Yiwen Shi",
            "Jing Wang",
            "Ping Ren",
            "Taha ValizadehAslani",
            "Yi Zhang",
            "Meng Hu",
            "Hualou Liang"
        ],
        "published": "2022-07-25T17:43:36Z",
        "summary": "Product-specific guidances (PSGs) recommended by the United States Food and\nDrug Administration (FDA) are instrumental to promote and guide generic drug\nproduct development. To assess a PSG, the FDA assessor needs to take extensive\ntime and effort to manually retrieve supportive drug information of absorption,\ndistribution, metabolism, and excretion (ADME) from the reference listed drug\nlabeling. In this work, we leveraged the state-of-the-art pre-trained language\nmodels to automatically label the ADME paragraphs in the pharmacokinetics\nsection from the FDA-approved drug labeling to facilitate PSG assessment. We\napplied a transfer learning approach by fine-tuning the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model to develop\na novel application of ADME semantic labeling, which can automatically retrieve\nADME paragraphs from drug labeling instead of manual work. We demonstrated that\nfine-tuning the pre-trained BERT model can outperform the conventional machine\nlearning techniques, achieving up to 11.6% absolute F1 improvement. To our\nknowledge, we were the first to successfully apply BERT to solve the ADME\nsemantic labeling task. We further assessed the relative contribution of\npre-training and fine-tuning to the overall performance of the BERT model in\nthe ADME semantic labeling task using a series of analysis methods such as\nattention similarity and layer-based ablations. Our analysis revealed that the\ninformation learned via fine-tuning is focused on task-specific knowledge in\nthe top layers of the BERT, whereas the benefit from the pre-trained BERT model\nis from the bottom layers.",
        "pdf_link": "https://arxiv.org/pdf/2207.12376v1.pdf"
    },
    {
        "title": "Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?",
        "authors": [
            "Pietro Bongini",
            "Federico Becattini",
            "Alberto Del Bimbo"
        ],
        "published": "2022-07-25T12:12:46Z",
        "summary": "The use of Deep Learning and Computer Vision in the Cultural Heritage domain\nis becoming highly relevant in the last few years with lots of applications\nabout audio smart guides, interactive museums and augmented reality. All these\ntechnologies require lots of data to work effectively and be useful for the\nuser. In the context of artworks, such data is annotated by experts in an\nexpensive and time consuming process. In particular, for each artwork, an image\nof the artwork and a description sheet have to be collected in order to perform\ncommon tasks like Visual Question Answering. In this paper we propose a method\nfor Visual Question Answering that allows to generate at runtime a description\nsheet that can be used for answering both visual and contextual questions about\nthe artwork, avoiding completely the image and the annotation process. For this\npurpose, we investigate on the use of GPT-3 for generating descriptions for\nartworks analyzing the quality of generated descriptions through captioning\nmetrics. Finally we evaluate the performance for Visual Question Answering and\ncaptioning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2207.12101v2.pdf"
    },
    {
        "title": "UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu",
        "authors": [
            "Maaz Amjad",
            "Grigori Sidorov",
            "Alisa Zhila",
            "Alexander Gelbukh",
            "Paolo Rosso"
        ],
        "published": "2022-07-25T03:46:51Z",
        "summary": "This paper gives the overview of the first shared task at FIRE 2020 on fake\nnews detection in the Urdu language. This is a binary classification task in\nwhich the goal is to identify fake news using a dataset composed of 900\nannotated news articles for training and 400 news articles for testing. The\ndataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz,\n(iv) Technology, and (v) Business. 42 teams from 6 different countries (India,\nChina, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams\nsubmitted their experimental results. The participants used various machine\nlearning methods ranging from feature-based traditional machine learning to\nneural network techniques. The best performing system achieved an F-score value\nof 0.90, showing that the BERT-based approach outperforms other machine\nlearning classifiers.",
        "pdf_link": "https://arxiv.org/pdf/2207.12406v1.pdf"
    },
    {
        "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020",
        "authors": [
            "Maaz Amjad",
            "Grigori Sidorov",
            "Alisa Zhila",
            "Alexander Gelbukh",
            "Paolo Rosso"
        ],
        "published": "2022-07-25T03:41:32Z",
        "summary": "This overview paper describes the first shared task on fake news detection in\nUrdu language. The task was posed as a binary classification task, in which the\ngoal is to differentiate between real and fake news. We provided a dataset\ndivided into 900 annotated news articles for training and 400 news articles for\ntesting. The dataset contained news in five domains: (i) Health, (ii) Sports,\n(iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different\ncountries (India, China, Egypt, Germany, Pakistan, and the UK) registered for\nthe task. 9 teams submitted their experimental results. The participants used\nvarious machine learning methods ranging from feature-based traditional machine\nlearning to neural networks techniques. The best performing system achieved an\nF-score value of 0.90, showing that the BERT-based approach outperforms other\nmachine learning techniques",
        "pdf_link": "https://arxiv.org/pdf/2207.11893v1.pdf"
    },
    {
        "title": "A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach",
        "authors": [
            "Praneeth Nemani",
            "Satyanarayana Vollala"
        ],
        "published": "2022-07-24T11:06:56Z",
        "summary": "Semantic similarity analysis and modeling is a fundamentally acclaimed task\nin many pioneering applications of natural language processing today. Owing to\nthe sensation of sequential pattern recognition, many neural networks like RNNs\nand LSTMs have achieved satisfactory results in semantic similarity modeling.\nHowever, these solutions are considered inefficient due to their inability to\nprocess information in a non-sequential manner, thus leading to the improper\nextraction of context. Transformers function as the state-of-the-art\narchitecture due to their advantages like non-sequential data processing and\nself-attention. In this paper, we perform semantic similarity analysis and\nmodeling on the U.S Patent Phrase to Phrase Matching Dataset using both\ntraditional and transformer-based techniques. We experiment upon four different\nvariants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by\nperforming K-Fold Cross-Validation. The experimental results demonstrate our\nmethodology's enhanced performance compared to traditional techniques, with an\naverage Pearson correlation score of 0.79.",
        "pdf_link": "https://arxiv.org/pdf/2207.11716v3.pdf"
    },
    {
        "title": "Improving Mandarin Speech Recogntion with Block-augmented Transformer",
        "authors": [
            "Xiaoming Ren",
            "Huifeng Zhu",
            "Liuwei Wei",
            "Minghui Wu",
            "Jie Hao"
        ],
        "published": "2022-07-24T09:23:04Z",
        "summary": "Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.29\\% without using a language model and 4.05\\% with an external language\nmodel on the testset.",
        "pdf_link": "https://arxiv.org/pdf/2207.11697v5.pdf"
    },
    {
        "title": "A Transformer-based Neural Language Model that Synthesizes Brain Activation Maps from Free-Form Text Queries",
        "authors": [
            "Gia H. Ngo",
            "Minh Nguyen",
            "Nancy F. Chen",
            "Mert R. Sabuncu"
        ],
        "published": "2022-07-24T09:15:03Z",
        "summary": "Neuroimaging studies are often limited by the number of subjects and\ncognitive processes that can be feasibly interrogated. However, a rapidly\ngrowing number of neuroscientific studies have collectively accumulated an\nextensive wealth of results. Digesting this growing literature and obtaining\nnovel insights remains to be a major challenge, since existing meta-analytic\ntools are constrained to keyword queries. In this paper, we present Text2Brain,\nan easy to use tool for synthesizing brain activation maps from open-ended text\nqueries. Text2Brain was built on a transformer-based neural network language\nmodel and a coordinate-based meta-analysis of neuroimaging studies. Text2Brain\ncombines a transformer-based text encoder and a 3D image generator, and was\ntrained on variable-length text snippets and their corresponding activation\nmaps sampled from 13,000 published studies. In our experiments, we demonstrate\nthat Text2Brain can synthesize meaningful neural activation patterns from\nvarious free-form textual descriptions. Text2Brain is available at\nhttps://braininterpreter.com as a web-based tool for efficiently searching\nthrough the vast neuroimaging literature and generating new hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2208.00840v1.pdf"
    },
    {
        "title": "Robots Enact Malignant Stereotypes",
        "authors": [
            "Andrew Hundt",
            "William Agnew",
            "Vicky Zeng",
            "Severin Kacianka",
            "Matthew Gombolay"
        ],
        "published": "2022-07-23T18:08:12Z",
        "summary": "Stereotypes, bias, and discrimination have been extensively documented in\nMachine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural\nLanguage Processing (NLP) [6], or both, in the case of large image and caption\nmodels such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias\nmanifests in robots that physically and autonomously act within the world. We\naudit one of several recently published CLIP-powered robotic manipulation\nmethods, presenting it with objects that have pictures of human faces on the\nsurface which vary across race and gender, alongside task descriptions that\ncontain terms associated with common stereotypes. Our experiments definitively\nshow robots acting out toxic stereotypes with respect to gender, race, and\nscientifically-discredited physiognomy, at scale. Furthermore, the audited\nmethods are less likely to recognize Women and People of Color. Our\ninterdisciplinary sociotechnical analysis synthesizes across fields and\napplications such as Science Technology and Society (STS), Critical Studies,\nHistory, Safety, Robotics, and AI. We find that robots powered by large\ndatasets and Dissolution Models (sometimes called \"foundation models\", e.g.\nCLIP) that contain humans risk physically amplifying malignant stereotypes in\ngeneral; and that merely correcting disparities will be insufficient for the\ncomplexity and scale of the problem. Instead, we recommend that robot learning\nmethods that physically manifest stereotypes or other harmful outcomes be\npaused, reworked, or even wound down when appropriate, until outcomes can be\nproven safe, effective, and just. Finally, we discuss comprehensive policy\nchanges and the potential of new interdisciplinary research on topics like\nIdentity Safety Assessment Frameworks and Design Justice to better understand\nand address these harms.",
        "pdf_link": "https://arxiv.org/pdf/2207.11569v1.pdf"
    },
    {
        "title": "Better Reasoning Behind Classification Predictions with BERT for Fake News Detection",
        "authors": [
            "Daesoo Lee"
        ],
        "published": "2022-07-23T17:54:48Z",
        "summary": "Fake news detection has become a major task to solve as there has been an\nincreasing number of fake news on the internet in recent years. Although many\nclassification models have been proposed based on statistical learning methods\nshowing good results, reasoning behind the classification performances may not\nbe enough. In the self-supervised learning studies, it has been highlighted\nthat a quality of representation (embedding) space matters and directly affects\na downstream task performance. In this study, a quality of the representation\nspace is analyzed visually and analytically in terms of linear separability for\ndifferent classes on a real and fake news dataset. To further add\ninterpretability to a classification model, a modification of Class Activation\nMapping (CAM) is proposed. The modified CAM provides a CAM score for each word\ntoken, where the CAM score on a word token denotes a level of focus on that\nword token to make the prediction. Finally, it is shown that the naive BERT\nmodel topped with a learnable linear layer is enough to achieve robust\nperformance while being compatible with CAM.",
        "pdf_link": "https://arxiv.org/pdf/2207.11562v1.pdf"
    },
    {
        "title": "Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter",
        "authors": [
            "Dilara Dogan",
            "Bahadir Altun",
            "Muhammed Said Zengin",
            "Mucahid Kutlu",
            "Tamer Elsayed"
        ],
        "published": "2022-07-23T11:55:18Z",
        "summary": "The recent advances in natural language processing have yielded many exciting\ndevelopments in text analysis and language understanding models; however, these\nmodels can also be used to track people, bringing severe privacy concerns. In\nthis work, we investigate what individuals can do to avoid being detected by\nthose models while using social media platforms. We ground our investigation in\ntwo exposure-risky tasks, stance detection and geotagging. We explore a variety\nof simple techniques for modifying text, such as inserting typos in salient\nwords, paraphrasing, and adding dummy social media posts. Our experiments show\nthat the performance of BERT-based models fined tuned for stance detection\ndecreases significantly due to typos, but it is not affected by paraphrasing.\nMoreover, we find that typos have minimal impact on state-of-the-art geotagging\nmodels due to their increased reliance on social networks; however, we show\nthat users can deceive those models by interacting with different users,\nreducing their performance by almost 50%.",
        "pdf_link": "https://arxiv.org/pdf/2207.11500v1.pdf"
    },
    {
        "title": "Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations",
        "authors": [
            "Qian Yang",
            "Yunxin Li",
            "Baotian Hu",
            "Lin Ma",
            "Yuxing Ding",
            "Min Zhang"
        ],
        "published": "2022-07-23T03:19:50Z",
        "summary": "Visual Entailment with natural language explanations aims to infer the\nrelationship between a text-image pair and generate a sentence to explain the\ndecision-making process. Previous methods rely mainly on a pre-trained\nvision-language model to perform the relation inference and a language model to\ngenerate the corresponding explanation. However, the pre-trained\nvision-language models mainly build token-level alignment between text and\nimage yet ignore the high-level semantic alignment between the phrases (chunks)\nand visual contents, which is critical for vision-language reasoning. Moreover,\nthe explanation generator based only on the encoded joint representation does\nnot explicitly consider the critical decision-making points of relation\ninference. Thus the generated explanations are less faithful to visual-language\nreasoning. To mitigate these problems, we propose a unified Chunk-aware\nAlignment and Lexical Constraint based method, dubbed as CALeC. It contains a\nChunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical\nConstraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence\nstructure inherent in language and various image regions to build chunk-aware\nsemantic alignment. Relation inferrer uses an attention-based reasoning network\nto incorporate the token-level and chunk-level vision-language representations.\nLeCG utilizes lexical constraints to expressly incorporate the words or chunks\nfocused by the relation inferrer into explanation generation, improving the\nfaithfulness and informativeness of the explanations. We conduct extensive\nexperiments on three datasets, and experimental results indicate that CALeC\nsignificantly outperforms other competitor models on inference accuracy and\nquality of generated explanations.",
        "pdf_link": "https://arxiv.org/pdf/2207.11401v2.pdf"
    },
    {
        "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling",
        "authors": [
            "Fenia Christopoulou",
            "Gerasimos Lampouras",
            "Milan Gritta",
            "Guchun Zhang",
            "Yinpeng Guo",
            "Zhongqi Li",
            "Qi Zhang",
            "Meng Xiao",
            "Bo Shen",
            "Lin Li",
            "Hao Yu",
            "Li Yan",
            "Pingyi Zhou",
            "Xin Wang",
            "Yuchi Ma",
            "Ignacio Iacobacci",
            "Yasheng Wang",
            "Guangtai Liang",
            "Jiansheng Wei",
            "Xin Jiang",
            "Qianxiang Wang",
            "Qun Liu"
        ],
        "published": "2022-07-22T18:08:16Z",
        "summary": "We present PanGu-Coder, a pretrained decoder-only language model adopting the\nPanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of\nprogramming language solutions given a natural language problem description. We\ntrain PanGu-Coder using a two-stage strategy: the first stage employs Causal\nLanguage Modelling (CLM) to pre-train on raw programming language data, while\nthe second stage uses a combination of Causal Language Modelling and Masked\nLanguage Modelling (MLM) training objectives that focus on the downstream task\nof text-to-code generation and train on loosely curated pairs of natural\nlanguage program definitions and code functions. Finally, we discuss\nPanGu-Coder-FT, which is fine-tuned on a combination of competitive programming\nproblems and code with continuous integration tests. We evaluate PanGu-Coder\nwith a focus on whether it generates functionally correct programs and\ndemonstrate that it achieves equivalent or better performance than similarly\nsized models, such as CodeX, while attending a smaller context window and\ntraining on less data.",
        "pdf_link": "https://arxiv.org/pdf/2207.11280v1.pdf"
    },
    {
        "title": "Zero-Shot Video Captioning with Evolving Pseudo-Tokens",
        "authors": [
            "Yoad Tewel",
            "Yoav Shalev",
            "Roy Nadler",
            "Idan Schwartz",
            "Lior Wolf"
        ],
        "published": "2022-07-22T14:19:31Z",
        "summary": "We introduce a zero-shot video captioning method that employs two frozen\nnetworks: the GPT-2 language model and the CLIP image-text matching model. The\nmatching score is used to steer the language model toward generating a sentence\nthat has a high average matching score to a subset of the video frames. Unlike\nzero-shot image captioning methods, our work considers the entire sentence at\nonce. This is achieved by optimizing, during the generation process, part of\nthe prompt from scratch, by modifying the representation of all other tokens in\nthe prompt, and by repeating the process iteratively, gradually improving the\nspecificity and comprehensiveness of the generated sentence. Our experiments\nshow that the generated captions are coherent and display a broad range of\nreal-world knowledge. Our code is available at:\nhttps://github.com/YoadTew/zero-shot-video-to-text",
        "pdf_link": "https://arxiv.org/pdf/2207.11100v2.pdf"
    },
    {
        "title": "BigIssue: A Realistic Bug Localization Benchmark",
        "authors": [
            "Paul Kassianik",
            "Erik Nijkamp",
            "Bo Pang",
            "Yingbo Zhou",
            "Caiming Xiong"
        ],
        "published": "2022-07-21T20:17:53Z",
        "summary": "As machine learning tools progress, the inevitable question arises: How can\nmachine learning help us write better code? With significant progress being\nachieved in natural language processing with models like GPT-3 and Bert, the\napplications of natural language processing techniques to code are starting to\nbe explored. Most of the research has been focused on automatic program repair\n(APR), and while the results on synthetic or highly filtered datasets are\npromising, such models are hard to apply in real-world scenarios because of\ninadequate bug localization. We propose BigIssue: a benchmark for realistic bug\nlocalization. The goal of the benchmark is two-fold. We provide (1) a general\nbenchmark with a diversity of real and synthetic Java bugs and (2) a motivation\nto improve bug localization capabilities of models through attention to the\nfull repository context. With the introduction of BigIssue, we hope to advance\nthe state of the art in bug localization, in turn improving APR performance and\nincreasing its applicability to the modern development cycle.",
        "pdf_link": "https://arxiv.org/pdf/2207.10739v2.pdf"
    },
    {
        "title": "Efficient model compression with Random Operation Access Specific Tile (ROAST) hashing",
        "authors": [
            "Aditya Desai",
            "Keren Zhou",
            "Anshumali Shrivastava"
        ],
        "published": "2022-07-21T18:31:17Z",
        "summary": "Advancements in deep learning are often associated with increasing model\nsizes. The model size dramatically affects the deployment cost and latency of\ndeep models. For instance, models like BERT cannot be deployed on edge devices\nand mobiles due to their sheer size. As a result, most advances in Deep\nLearning are yet to reach the edge. Model compression has sought much-deserved\nattention in literature across natural language processing, vision, and\nrecommendation domains. This paper proposes a model-agnostic, cache-friendly\nmodel compression approach: Random Operation Access Specific Tile (ROAST)\nhashing. ROAST collapses the parameters by clubbing them through a lightweight\nmapping. Notably, while clubbing these parameters, ROAST utilizes cache\nhierarchies by aligning the memory access pattern with the parameter access\npattern. ROAST is up to $\\sim 25 \\times$ faster to train and $\\sim 50 \\times$\nfaster to infer than the popular parameter sharing method HashedNet.\nAdditionally, ROAST introduces global weight sharing, which is empirically and\ntheoretically superior to local weight sharing in HashedNet, and can be of\nindependent interest in itself. With ROAST, we present the first compressed\nBERT, which is $100\\times - 1000\\times$ smaller but does not result in quality\ndegradation. These compression levels on universal architecture like\ntransformers are promising for the future of SOTA model deployment on\nresource-constrained devices like mobile and edge devices",
        "pdf_link": "https://arxiv.org/pdf/2207.10702v1.pdf"
    },
    {
        "title": "Leveraging Natural Supervision for Language Representation Learning and Generation",
        "authors": [
            "Mingda Chen"
        ],
        "published": "2022-07-21T17:26:03Z",
        "summary": "Recent breakthroughs in Natural Language Processing (NLP) have been driven by\nlanguage models trained on a massive amount of plain text. While powerful,\nderiving supervision from textual resources is still an open question. For\nexample, language model pretraining often neglects the rich, freely-available\nstructures in textual data. In this thesis, we describe three lines of work\nthat seek to improve the training and evaluation of neural models using\nnaturally-occurring supervision.\n  We first investigate self-supervised training losses to help enhance the\nperformance of pretrained language models for various NLP tasks. Specifically,\nwe alter the sentence prediction loss to make it better suited to other\npretraining losses and more challenging to solve. We design an intermediate\nfinetuning step that uses self-supervised training to promote models' ability\nin cross-task generalization.\n  Then we describe methods to leverage the structures in Wikipedia and\nparaphrases. In particular, we propose training losses to exploit hyperlinks,\narticle structures, and article category graphs for entity-, discourse-,\nentailment-related knowledge. We propose a framework that uses paraphrase pairs\nto disentangle semantics and syntax in sentence representations. We extend the\nframework for a novel generation task that controls the syntax of output text\nwith a sentential exemplar.\n  Lastly, we discuss our work on tailoring textual resources for establishing\nchallenging evaluation tasks. We introduce three datasets by defining novel\ntasks using various fan-contributed websites, including a long-form\ndata-to-text generation dataset, a screenplay summarization dataset, and a\nlong-form story generation dataset. These datasets have unique characteristics\noffering challenges to future work in their respective task settings.",
        "pdf_link": "https://arxiv.org/pdf/2207.10617v1.pdf"
    },
    {
        "title": "Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration",
        "authors": [
            "Haotian Bai",
            "Ruimao Zhang",
            "Jiong Wang",
            "Xiang Wan"
        ],
        "published": "2022-07-21T12:37:15Z",
        "summary": "Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Recent studies leverage the advantage\nof self-attention in visual Transformer for long-range dependency to re-active\nsemantic regions, aiming to avoid partial activation in traditional class\nactivation mapping (CAM). However, the long-range modeling in Transformer\nneglects the inherent spatial coherence of the object, and it usually diffuses\nthe semantic-aware regions far from the object boundary, making localization\nresults significantly larger or far smaller. To address such an issue, we\nintroduce a simple yet effective Spatial Calibration Module (SCM) for accurate\nWSOL, incorporating semantic similarities of patch tokens and their spatial\nrelationships into a unified diffusion model. Specifically, we introduce a\nlearnable parameter to dynamically adjust the semantic correlations and spatial\ncontext intensities for effective information propagation. In practice, SCM is\ndesigned as an external module of Transformer, and can be removed during\ninference to reduce the computation cost. The object-sensitive localization\nability is implicitly embedded into the Transformer encoder through\noptimization in the training phase. It enables the generated attention maps to\ncapture the sharper object boundaries and filter the object-irrelevant\nbackground area. Extensive experimental results demonstrate the effectiveness\nof the proposed method, which significantly outperforms its counterpart TS-CAM\non both CUB-200 and ImageNet-1K benchmarks. The code is available at\nhttps://github.com/164140757/SCM.",
        "pdf_link": "https://arxiv.org/pdf/2207.10447v2.pdf"
    },
    {
        "title": "Language Model Cascades",
        "authors": [
            "David Dohan",
            "Winnie Xu",
            "Aitor Lewkowycz",
            "Jacob Austin",
            "David Bieber",
            "Raphael Gontijo Lopes",
            "Yuhuai Wu",
            "Henryk Michalewski",
            "Rif A. Saurous",
            "Jascha Sohl-dickstein",
            "Kevin Murphy",
            "Charles Sutton"
        ],
        "published": "2022-07-21T07:35:18Z",
        "summary": "Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.",
        "pdf_link": "https://arxiv.org/pdf/2207.10342v2.pdf"
    },
    {
        "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
        "authors": [
            "Oskar van der Wal",
            "Jaap Jumelet",
            "Katrin Schulz",
            "Willem Zuidema"
        ],
        "published": "2022-07-21T00:59:04Z",
        "summary": "Detecting and mitigating harmful biases in modern language models are widely\nrecognized as crucial, open problems. In this paper, we take a step back and\ninvestigate how language models come to be biased in the first place. We use a\nrelatively small language model, using the LSTM architecture trained on an\nEnglish Wikipedia corpus. With full access to the data and to the model\nparameters as they change during every step while training, we can map in\ndetail how the representation of gender develops, what patterns in the dataset\ndrive this, and how the model's internal state relates to the bias in a\ndownstream task (semantic textual similarity). We find that the representation\nof gender is dynamic and identify different phases during training.\nFurthermore, we show that gender information is represented increasingly\nlocally in the input embeddings of the model and that, as a consequence,\ndebiasing these can be effective in reducing the downstream bias. Monitoring\nthe training dynamics, allows us to detect an asymmetry in how the female and\nmale gender are represented in the input embeddings. This is important, as it\nmay cause naive mitigation strategies to introduce new undesirable biases. We\ndiscuss the relevance of the findings for mitigation strategies more generally\nand the prospects of generalizing our methods to larger language models, the\nTransformer architecture, other languages and other undesirable biases.",
        "pdf_link": "https://arxiv.org/pdf/2207.10245v1.pdf"
    },
    {
        "title": "Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores",
        "authors": [
            "Zeju Li",
            "Konstantinos Kamnitsas",
            "Mobarakol Islam",
            "Chen Chen",
            "Ben Glocker"
        ],
        "published": "2022-07-20T15:04:32Z",
        "summary": "Machine learning models are typically deployed in a test setting that differs\nfrom the training setting, potentially leading to decreased model performance\nbecause of domain shift. If we could estimate the performance that a\npre-trained model would achieve on data from a specific deployment setting, for\nexample a certain clinic, we could judge whether the model could safely be\ndeployed or if its performance degrades unacceptably on the specific data.\nExisting approaches estimate this based on the confidence of predictions made\non unlabeled test data from the deployment's domain. We find existing methods\nstruggle with data that present class imbalance, because the methods used to\ncalibrate confidence do not account for bias induced by class imbalance,\nconsequently failing to estimate class-wise accuracy. Here, we introduce\nclass-wise calibration within the framework of performance estimation for\nimbalanced datasets. Specifically, we derive class-specific modifications of\nstate-of-the-art confidence-based model evaluation methods including\ntemperature scaling (TS), difference of confidences (DoC), and average\nthresholded confidence (ATC). We also extend the methods to estimate Dice\nsimilarity coefficient (DSC) in image segmentation. We conduct experiments on\nfour tasks and find the proposed modifications consistently improve the\nestimation accuracy for imbalanced datasets. Our methods improve accuracy\nestimation by 18\\% in classification under natural domain shifts, and double\nthe estimation accuracy on segmentation tasks, when compared with prior\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2207.09957v1.pdf"
    },
    {
        "title": "Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition",
        "authors": [
            "Huabin Liu",
            "Weixian Lv",
            "John See",
            "Weiyao Lin"
        ],
        "published": "2022-07-20T09:04:12Z",
        "summary": "A primary challenge faced in few-shot action recognition is inadequate video\ndata for training. To address this issue, current methods in this field mainly\nfocus on devising algorithms at the feature level while little attention is\npaid to processing input video data. Moreover, existing frame sampling\nstrategies may omit critical action information in temporal and spatial\ndimensions, which further impacts video utilization efficiency. In this paper,\nwe propose a novel video frame sampler for few-shot action recognition to\naddress this issue, where task-specific spatial-temporal frame sampling is\nachieved via a temporal selector (TS) and a spatial amplifier (SA).\nSpecifically, our sampler first scans the whole video at a small computational\ncost to obtain a global perception of video frames. The TS plays its role in\nselecting top-T frames that contribute most significantly and subsequently. The\nSA emphasizes the discriminative information of each frame by amplifying\ncritical regions with the guidance of saliency maps. We further adopt\ntask-adaptive learning to dynamically adjust the sampling strategy according to\nthe episode task at hand. Both the implementations of TS and SA are\ndifferentiable for end-to-end optimization, facilitating seamless integration\nof our proposed sampler with most few-shot action recognition methods.\nExtensive experiments show a significant boost in the performances on various\nbenchmarks including long-term videos.The code is available at\nhttps://github.com/R00Kie-Liu/Sampler",
        "pdf_link": "https://arxiv.org/pdf/2207.09759v3.pdf"
    },
    {
        "title": "Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis",
        "authors": [
            "Elliot Dang",
            "Zheyuan Hu",
            "Tong Li"
        ],
        "published": "2022-07-19T21:04:31Z",
        "summary": "Collaborative Filtering(CF) recommender is a crucial application in the\nonline market and ecommerce. However, CF recommender has been proven to suffer\nfrom persistent problems related to sparsity of the user rating that will\nfurther lead to a cold-start issue. Existing methods address the data sparsity\nissue by applying token-level sentiment analysis that translate text review\ninto sentiment scores as a complement of the user rating. In this paper, we\nattempt to optimize the sentiment analysis with advanced NLP models including\nBERT and RoBERTa, and experiment on whether the CF recommender has been further\nenhanced. We build the recommenders on the Amazon US Reviews dataset, and tune\nthe pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as\nwell as the new prompt-based learning paradigm. Experimental result shows that\nthe recommender enhanced with the sentiment ratings predicted by the fine-tuned\nRoBERTa has the best performance, and achieved 30.7% overall gain by comparing\nMAP, NDCG and precision at K to the baseline recommender. Prompt-based learning\nparadigm, although superior to traditional fine-tune paradigm in pure sentiment\nanalysis, fail to further improve the CF recommender.",
        "pdf_link": "https://arxiv.org/pdf/2207.12883v1.pdf"
    },
    {
        "title": "Revealing Secrets From Pre-trained Models",
        "authors": [
            "Mujahid Al Rafi",
            "Yuan Feng",
            "Hyeran Jeon"
        ],
        "published": "2022-07-19T20:19:03Z",
        "summary": "With the growing burden of training deep learning models with large data\nsets, transfer-learning has been widely adopted in many emerging deep learning\nalgorithms. Transformer models such as BERT are the main player in natural\nlanguage processing and use transfer-learning as a de facto standard training\nmethod. A few big data companies release pre-trained models that are trained\nwith a few popular datasets with which end users and researchers fine-tune the\nmodel with their own datasets. Transfer-learning significantly reduces the time\nand effort of training models. However, it comes at the cost of security\nconcerns. In this paper, we show a new observation that pre-trained models and\nfine-tuned models have significantly high similarities in weight values. Also,\nwe demonstrate that there exist vendor-specific computing patterns even for the\nsame models. With these new findings, we propose a new model extraction attack\nthat reveals the model architecture and the pre-trained model used by the\nblack-box victim model with vendor-specific computing patterns and then\nestimates the entire model weights based on the weight value similarities\nbetween the fine-tuned model and pre-trained model. We also show that the\nweight similarity can be leveraged for increasing the model extraction\nfeasibility through a novel weight extraction pruning.",
        "pdf_link": "https://arxiv.org/pdf/2207.09539v1.pdf"
    },
    {
        "title": "Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices",
        "authors": [
            "Mingbin Xu",
            "Congzheng Song",
            "Ye Tian",
            "Neha Agrawal",
            "Filip Granqvist",
            "Rogier van Dalen",
            "Xiao Zhang",
            "Arturo Argueta",
            "Shiyi Han",
            "Yaqiao Deng",
            "Leo Liu",
            "Anmol Walia",
            "Alex Jin"
        ],
        "published": "2022-07-18T23:53:17Z",
        "summary": "Federated Learning (FL) is a technique to train models using data distributed\nacross devices. Differential Privacy (DP) provides a formal privacy guarantee\nfor sensitive data. Our goal is to train a large neural network language model\n(NNLM) on compute-constrained devices while preserving privacy using FL and DP.\nHowever, the DP-noise introduced to the model increases as the model size\ngrows, which often prevents convergence. We propose Partial Embedding Updates\n(PEU), a novel technique to decrease noise by decreasing payload size.\nFurthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive\nEstimation (NCE) to reduce the memory demands of large models on\ncompute-constrained devices. This combination of techniques makes it possible\nto train large-vocabulary language models while preserving accuracy and\nprivacy.",
        "pdf_link": "https://arxiv.org/pdf/2207.08988v1.pdf"
    },
    {
        "title": "Selection Bias Induced Spurious Correlations in Large Language Models",
        "authors": [
            "Emily McMilin"
        ],
        "published": "2022-07-18T23:43:52Z",
        "summary": "In this work we show how large language models (LLMs) can learn statistical\ndependencies between otherwise unconditionally independent variables due to\ndataset selection bias. To demonstrate the effect, we developed a masked gender\ntask that can be applied to BERT-family models to reveal spurious correlations\nbetween predicted gender pronouns and a variety of seemingly gender-neutral\nvariables like date and location, on pre-trained (unmodified) BERT and RoBERTa\nlarge models. Finally, we provide an online demo, inviting readers to\nexperiment further.",
        "pdf_link": "https://arxiv.org/pdf/2207.08982v1.pdf"
    },
    {
        "title": "Word Play for Playing Othello (Reverses)",
        "authors": [
            "Samantha E. Miller Noever",
            "David Noever"
        ],
        "published": "2022-07-18T17:13:32Z",
        "summary": "Language models like OpenAI's Generative Pre-Trained Transformers (GPT-2/3)\ncapture the long-term correlations needed to generate text in a variety of\ndomains (such as language translators) and recently in gameplay (chess, Go, and\ncheckers). The present research applies both the larger (GPT-3) and smaller\n(GPT-2) language models to explore the complex strategies for the game of\nOthello (or Reverses). Given the game rules for rapid reversals of fortune, the\nlanguage model not only represents a candidate predictor of the next move based\non previous game moves but also avoids sparse rewards in gameplay. The language\nmodel automatically captures or emulates championship-level strategies. The\nfine-tuned GPT-2 model generates Othello games ranging from 13-71% completion,\nwhile the larger GPT-3 model reaches 41% of a complete game. Like previous work\nwith chess and Go, these language models offer a novel way to generate\nplausible game archives, particularly for comparing opening moves across a\nlarger sample than humanly possible to explore. A primary contribution of these\nmodels magnifies (by two-fold) the previous record for player archives (120,000\nhuman games over 45 years from 1977-2022), thus supplying the research\ncommunity with more diverse and original strategies for sampling with other\nreinforcement learning techniques.",
        "pdf_link": "https://arxiv.org/pdf/2207.08766v1.pdf"
    },
    {
        "title": "Label2Label: A Language Modeling Framework for Multi-Attribute Learning",
        "authors": [
            "Wanhua Li",
            "Zhexuan Cao",
            "Jianjiang Feng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "published": "2022-07-18T15:12:33Z",
        "summary": "Objects are usually associated with multiple attributes, and these attributes\noften exhibit high correlations. Modeling complex relationships between\nattributes poses a great challenge for multi-attribute learning. This paper\nproposes a simple yet generic framework named Label2Label to exploit the\ncomplex attribute correlations. Label2Label is the first attempt for\nmulti-attribute prediction from the perspective of language modeling.\nSpecifically, it treats each attribute label as a \"word\" describing the sample.\nAs each sample is annotated with multiple attribute labels, these \"words\" will\nnaturally form an unordered but meaningful \"sentence\", which depicts the\nsemantic information of the corresponding sample. Inspired by the remarkable\nsuccess of pre-training language models in NLP, Label2Label introduces an\nimage-conditioned masked language model, which randomly masks some of the\n\"word\" tokens from the label \"sentence\" and aims to recover them based on the\nmasked \"sentence\" and the context conveyed by image features. Our intuition is\nthat the instance-wise attribute relations are well grasped if the neural net\ncan infer the missing attributes based on the context and the remaining\nattribute hints. Label2Label is conceptually simple and empirically powerful.\nWithout incorporating task-specific prior knowledge and highly specialized\nnetwork designs, our approach achieves state-of-the-art results on three\ndifferent multi-attribute learning tasks, compared to highly customized\ndomain-specific methods. Code is available at\nhttps://github.com/Li-Wanhua/Label2Label.",
        "pdf_link": "https://arxiv.org/pdf/2207.08677v1.pdf"
    },
    {
        "title": "Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks",
        "authors": [
            "Julie Jiang",
            "Xiang Ren",
            "Emilio Ferrara"
        ],
        "published": "2022-07-18T02:18:20Z",
        "summary": "Estimating the political leanings of social media users is a challenging and\never more pressing problem given the increase in social media consumption. We\nintroduce Retweet-BERT, a simple and scalable model to estimate the political\nleanings of Twitter users. Retweet-BERT leverages the retweet network structure\nand the language used in users' profile descriptions. Our assumptions stem from\npatterns of networks and linguistics homophily among people who share similar\nideologies. Retweet-BERT demonstrates competitive performance against other\nstate-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter\ndatasets (a COVID-19 dataset and a 2020 United States presidential elections\ndataset). We also perform manual validation to validate the performance of\nRetweet-BERT on users not in the training data. Finally, in a case study of\nCOVID-19, we illustrate the presence of political echo chambers on Twitter and\nshow that it exists primarily among right-leaning users. Our code is\nopen-sourced and our data is publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2207.08349v4.pdf"
    },
    {
        "title": "Towards the Human Global Context: Does the Vision-Language Model Really Judge Like a Human Being?",
        "authors": [
            "Sangmyeong Woh",
            "Jaemin Lee",
            "Ho Joong Kim",
            "Jinsuk Lee"
        ],
        "published": "2022-07-18T01:01:43Z",
        "summary": "As computer vision and NLP make progress, Vision-Language(VL) is becoming an\nimportant area of research. Despite the importance, evaluation metrics of the\nresearch domain is still at a preliminary stage of development. In this paper,\nwe propose a quantitative metric \"Equivariance Score\" and evaluation dataset\n\"Human Puzzle\" to assess whether a VL model is understanding an image like a\nhuman. We observed that the VL model does not interpret the overall context of\nan input image but instead shows biases toward a specific object or shape that\nforms the local context. We aim to quantitatively measure a model's performance\nin understanding context. To verify the current existing VL model's capability,\nwe sliced the original input image into pieces and randomly placed them,\ndistorting the global context of the image. Our paper discusses each VL model's\nlevel of interpretation on global context and addresses how the structural\ncharacteristics influenced the results.",
        "pdf_link": "https://arxiv.org/pdf/2207.08333v1.pdf"
    },
    {
        "title": "Technology and Consciousness",
        "authors": [
            "John Rushby",
            "Daniel Sanchez"
        ],
        "published": "2022-07-17T23:23:01Z",
        "summary": "We report on a series of eight workshops held in the summer of 2017 on the\ntopic \"technology and consciousness.\" The workshops covered many subjects but\nthe overall goal was to assess the possibility of machine consciousness, and\nits potential implications. In the body of the report, we summarize most of the\nbasic themes that were discussed: the structure and function of the brain,\ntheories of consciousness, explicit attempts to construct conscious machines,\ndetection and measurement of consciousness, possible emergence of a conscious\ntechnology, methods for control of such a technology and ethical considerations\nthat might be owed to it. An appendix outlines the topics of each workshop and\nprovides abstracts of the talks delivered.\n  Update: Although this report was published in 2018 and the workshops it is\nbased on were held in 2017, recent events suggest that it is worth bringing\nforward. In particular, in the Spring of 2022, a Google engineer claimed that\nLaMDA, one of their \"large language models\" is sentient or even conscious. This\nprovoked a flurry of commentary in both the scientific and popular press, some\nof it interesting and insightful, but almost all of it ignorant of the prior\nconsideration given to these topics and the history of research into machine\nconsciousness. Thus, we are making a lightly refreshed version of this report\navailable in the hope that it will provide useful background to the current\ndebate and will enable more informed commentary. Although this material is five\nyears old, its technical points remain valid and up to date, but we have\n\"refreshed\" it by adding a few footnotes highlighting recent developments.",
        "pdf_link": "https://arxiv.org/pdf/2209.03956v1.pdf"
    },
    {
        "title": "An Overview of Distant Supervision for Relation Extraction with a Focus on Denoising and Pre-training Methods",
        "authors": [
            "William Hogan"
        ],
        "published": "2022-07-17T21:02:04Z",
        "summary": "Relation Extraction (RE) is a foundational task of natural language\nprocessing. RE seeks to transform raw, unstructured text into structured\nknowledge by identifying relational information between entity pairs found in\ntext. RE has numerous uses, such as knowledge graph completion, text\nsummarization, question-answering, and search querying. The history of RE\nmethods can be roughly organized into four phases: pattern-based RE,\nstatistical-based RE, neural-based RE, and large language model-based RE. This\nsurvey begins with an overview of a few exemplary works in the earlier phases\nof RE, highlighting limitations and shortcomings to contextualize progress.\nNext, we review popular benchmarks and critically examine metrics used to\nassess RE performance. We then discuss distant supervision, a paradigm that has\nshaped the development of modern RE methods. Lastly, we review recent RE works\nfocusing on denoising and pre-training methods.",
        "pdf_link": "https://arxiv.org/pdf/2207.08286v1.pdf"
    },
    {
        "title": "Representation Learning of Image Schema",
        "authors": [
            "Fajrian Yunus",
            "Chlo\u00e9 Clavel",
            "Catherine Pelachaud"
        ],
        "published": "2022-07-17T18:42:37Z",
        "summary": "Image schema is a recurrent pattern of reasoning where one entity is mapped\ninto another. Image schema is similar to conceptual metaphor and is also\nrelated to metaphoric gesture. Our main goal is to generate metaphoric gestures\nfor an Embodied Conversational Agent.\n  We propose a technique to learn the vector representation of image schemas.\nAs far as we are aware of, this is the first work which addresses that problem.\nOur technique uses Ravenet et al's algorithm which we use to compute the image\nschemas from the text input and also BERT and SenseBERT which we use as the\nbase word embedding technique to calculate the final vector representation of\nthe image schema. Our representation learning technique works by clustering:\nword embedding vectors which belong to the same image schema should be\nrelatively closer to each other, and thus form a cluster.\n  With the image schemas representable as vectors, it also becomes possible to\nhave a notion that some image schemas are closer or more similar to each other\nthan to the others because the distance between the vectors is a proxy of the\ndissimilarity between the corresponding image schemas. Therefore, after\nobtaining the vector representation of the image schemas, we calculate the\ndistances between those vectors. Based on these, we create visualizations to\nillustrate the relative distances between the different image schemas.",
        "pdf_link": "https://arxiv.org/pdf/2207.08256v1.pdf"
    },
    {
        "title": "A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets",
        "authors": [
            "Seyhmus Yilmaz",
            "Sultan Zavrak"
        ],
        "published": "2022-07-17T17:12:16Z",
        "summary": "In this study, we aimed to address the growing concern of trolling behavior\non social media by developing and evaluating a set of model architectures for\nthe automatic detection of troll tweets. Utilizing deep learning techniques and\npre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated\nthe performance of each architecture using metrics such as classification\naccuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo\nembedding methods performed better than the GloVe method, likely due to their\nability to provide contextualized word embeddings that better capture the\nnuances and subtleties of language use in online social media. Additionally, we\nfound that CNN and GRU encoders performed similarly in terms of F1 score and\nAUC, suggesting their effectiveness in extracting relevant information from\ninput text. The best-performing method was found to be an ELMo-based\narchitecture that employed a GRU classifier, with an AUC score of 0.929. This\nresearch highlights the importance of utilizing contextualized word embeddings\nand appropriate encoder methods in the task of troll tweet detection, which can\nassist social-based systems in improving their performance in identifying and\naddressing trolling behavior on their platforms.",
        "pdf_link": "https://arxiv.org/pdf/2207.08230v4.pdf"
    },
    {
        "title": "Natural language processing for clusterization of genes according to their functions",
        "authors": [
            "Vladislav Dordiuk",
            "Ekaterina Demicheva",
            "Fernando Polanco Espino",
            "Konstantin Ushenin"
        ],
        "published": "2022-07-17T12:59:34Z",
        "summary": "There are hundreds of methods for analysis of data obtained in\nmRNA-sequencing. The most of them are focused on small number of genes. In this\nstudy, we propose an approach that reduces the analysis of several thousand\ngenes to analysis of several clusters. The list of genes is enriched with\ninformation from open databases. Then, the descriptions are encoded as vectors\nusing the pretrained language model (BERT) and some text processing approaches.\nThe encoded gene function pass through the dimensionality reduction and\nclusterization. Aiming to find the most efficient pipeline, 180 cases of\npipeline with different methods in the major pipeline steps were analyzed. The\nperformance was evaluated with clusterization indexes and expert review of the\nresults.",
        "pdf_link": "https://arxiv.org/pdf/2207.08162v1.pdf"
    },
    {
        "title": "Can large language models reason about medical questions?",
        "authors": [
            "Valentin Li\u00e9vin",
            "Christoffer Egeberg Hother",
            "Andreas Geert Motzfeldt",
            "Ole Winther"
        ],
        "published": "2022-07-17T11:24:44Z",
        "summary": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2207.08143v4.pdf"
    },
    {
        "title": "ELECTRA is a Zero-Shot Learner, Too",
        "authors": [
            "Shiwen Ni",
            "Hung-Yu Kao"
        ],
        "published": "2022-07-17T11:20:58Z",
        "summary": "Recently, for few-shot or even zero-shot learning, the new paradigm\n\"pre-train, prompt, and predict\" has achieved remarkable achievements compared\nwith the \"pre-train, fine-tune\" paradigm. After the success of prompt-based\nGPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)\nprompt learning methods became popular and widely used. However, another\nefficient pre-trained discriminative model, ELECTRA, has probably been\nneglected. In this paper, we attempt to accomplish several NLP tasks in the\nzero-shot scenario using a novel our proposed replaced token detection\n(RTD)-based prompt learning method. Experimental results show that ELECTRA\nmodel based on RTD-prompt learning achieves surprisingly state-of-the-art\nzero-shot performance. Numerically, compared to MLM-RoBERTa-large and\nMLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%\nimprovement on all 15 tasks. Especially on the SST-2 task, our\nRTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training\ndata. Overall, compared to the pre-trained masked language models, the\npre-trained replaced token detection model performs better in zero-shot\nlearning. The source code is available at:\nhttps://github.com/nishiwen1214/RTD-ELECTRA.",
        "pdf_link": "https://arxiv.org/pdf/2207.08141v2.pdf"
    },
    {
        "title": "Aspect-specific Context Modeling for Aspect-based Sentiment Analysis",
        "authors": [
            "Fang Ma",
            "Chen Zhang",
            "Bo Zhang",
            "Dawei Song"
        ],
        "published": "2022-07-17T07:22:19Z",
        "summary": "Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity\n(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous\nwork in ABSA mostly relies on rather complicated aspect-specific feature\ninduction. Recently, pretrained language models (PLMs), e.g., BERT, have been\nused as context modeling layers to simplify the feature induction structures\nand achieve state-of-the-art performance. However, such PLM-based context\nmodeling can be not that aspect-specific. Therefore, a key question is left\nunder-explored: how the aspect-specific context can be better modeled through\nPLMs? To answer the question, we attempt to enhance aspect-specific context\nmodeling with PLM in a non-intrusive manner. We propose three aspect-specific\ninput transformations, namely aspect companion, aspect prompt, and aspect\nmarker. Informed by these transformations, non-intrusive aspect-specific PLMs\ncan be achieved to promote the PLM to pay more attention to the aspect-specific\ncontext in a sentence. Additionally, we craft an adversarial benchmark for ABSA\n(advABSA) to see how aspect-specific modeling can impact model robustness.\nExtensive experimental results on standard and adversarial benchmarks for SC\nand OE demonstrate the effectiveness and robustness of the proposed method,\nyielding new state-of-the-art performance on OE and competitive performance on\nSC.",
        "pdf_link": "https://arxiv.org/pdf/2207.08099v1.pdf"
    },
    {
        "title": "Automatic Context Pattern Generation for Entity Set Expansion",
        "authors": [
            "Yinghui Li",
            "Shulin Huang",
            "Xinwei Zhang",
            "Qingyu Zhou",
            "Yangning Li",
            "Ruiyang Liu",
            "Yunbo Cao",
            "Hai-Tao Zheng",
            "Ying Shen"
        ],
        "published": "2022-07-17T06:50:35Z",
        "summary": "Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.",
        "pdf_link": "https://arxiv.org/pdf/2207.08087v4.pdf"
    },
    {
        "title": "Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model",
        "authors": [
            "Xiaolin Chen",
            "Xuemeng Song",
            "Liqiang Jing",
            "Shuo Li",
            "Linmei Hu",
            "Liqiang Nie"
        ],
        "published": "2022-07-16T13:02:54Z",
        "summary": "Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.",
        "pdf_link": "https://arxiv.org/pdf/2207.07934v1.pdf"
    },
    {
        "title": "Clover: Towards A Unified Video-Language Alignment and Fusion Model",
        "authors": [
            "Jingjia Huang",
            "Yinan Li",
            "Jiashi Feng",
            "Xinglong Wu",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published": "2022-07-16T09:38:52Z",
        "summary": "Building a universal Video-Language model for solving various video\nunderstanding tasks (\\emph{e.g.}, text-video retrieval, video question\nanswering) is an open challenge to the machine learning field. Towards this\ngoal, most recent works build the model by stacking uni-modal and cross-modal\nfeature encoders and train it with pair-wise contrastive pre-text tasks. Though\noffering attractive generality, the resulted models have to compromise between\nefficiency and performance. They mostly adopt different architectures to deal\nwith different downstream tasks. We find this is because the pair-wise training\ncannot well \\emph{align} and \\emph{fuse} features from different modalities. We\nthen introduce \\textbf{Clover}\\textemdash a Correlated Video-Language\npre-training method\\textemdash towards a universal Video-Language model for\nsolving multiple video understanding tasks with neither performance nor\nefficiency compromise. It improves cross-modal feature alignment and fusion via\na novel tri-modal alignment pre-training task. Additionally, we propose to\nenhance the tri-modal alignment via incorporating learning from semantic masked\nsamples and a new pair-wise ranking loss. Clover establishes new\nstate-of-the-arts on multiple downstream tasks, including three retrieval tasks\nfor both zero-shot and fine-tuning settings, and eight video question answering\ntasks. Codes and pre-trained models will be released at\n\\url{https://github.com/LeeYN-43/Clover}.",
        "pdf_link": "https://arxiv.org/pdf/2207.07885v3.pdf"
    },
    {
        "title": "A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language",
        "authors": [
            "Michael Desmond",
            "Evelyn Duesterwald",
            "Vatche Isahagian",
            "Vinod Muthusamy"
        ],
        "published": "2022-07-15T19:17:55Z",
        "summary": "Most business process automation is still developed using traditional\nautomation technologies such as workflow engines. These systems provide domain\nspecific languages that require both business knowledge and programming skills\nto effectively use. As such, business users often lack adequate programming\nskills to fully leverage these code oriented environments. We propose a\nparadigm for the construction of business automations using natural language.\nThe approach applies a large language model to translate business rules and\nautomations described in natural language, into a domain specific language\ninterpretable by a business rule engine. We compare the performance of various\nlanguage model configurations, across various target domains, and explore the\nuse of constrained decoding to ensure syntactically correct generation of\noutput.",
        "pdf_link": "https://arxiv.org/pdf/2207.10648v1.pdf"
    },
    {
        "title": "POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging",
        "authors": [
            "Shishir G. Patil",
            "Paras Jain",
            "Prabal Dutta",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "published": "2022-07-15T18:36:29Z",
        "summary": "Fine-tuning models on edge devices like mobile phones would enable\nprivacy-preserving personalization over sensitive data. However, edge training\nhas historically been limited to relatively small models with simple\narchitectures because training is both memory and energy intensive. We present\nPOET, an algorithm to enable training large neural networks on memory-scarce\nbattery-operated edge devices. POET jointly optimizes the integrated search\nsearch spaces of rematerialization and paging, two algorithms to reduce the\nmemory consumption of backpropagation. Given a memory budget and a run-time\nconstraint, we formulate a mixed-integer linear program (MILP) for\nenergy-optimal training. Our approach enables training significantly larger\nmodels on embedded devices while reducing energy consumption while not\nmodifying mathematical correctness of backpropagation. We demonstrate that it\nis possible to fine-tune both ResNet-18 and BERT within the memory constraints\nof a Cortex-M class embedded device while outperforming current edge training\nmethods in energy efficiency. POET is an open-source project available at\nhttps://github.com/ShishirPatil/poet",
        "pdf_link": "https://arxiv.org/pdf/2207.07697v1.pdf"
    },
    {
        "title": "Position Prediction as an Effective Pretraining Strategy",
        "authors": [
            "Shuangfei Zhai",
            "Navdeep Jaitly",
            "Jason Ramapuram",
            "Dan Busbridge",
            "Tatiana Likhomanenko",
            "Joseph Yitan Cheng",
            "Walter Talbott",
            "Chen Huang",
            "Hanlin Goh",
            "Joshua Susskind"
        ],
        "published": "2022-07-15T17:10:48Z",
        "summary": "Transformers have gained increasing popularity in a wide range of\napplications, including Natural Language Processing (NLP), Computer Vision and\nSpeech Recognition, because of their powerful representational capacity.\nHowever, harnessing this representational capacity effectively requires a large\namount of data, strong regularization, or both, to mitigate overfitting.\nRecently, the power of the Transformer has been unlocked by self-supervised\npretraining strategies based on masked autoencoders which rely on\nreconstructing masked inputs, directly, or contrastively from unmasked content.\nThis pretraining strategy which has been used in BERT models in NLP, Wav2Vec\nmodels in Speech and, recently, in MAE models in Vision, forces the model to\nlearn about relationships between the content in different parts of the input\nusing autoencoding related objectives. In this paper, we propose a novel, but\nsurprisingly simple alternative to content reconstruction~-- that of predicting\nlocations from content, without providing positional information for it. Doing\nso requires the Transformer to understand the positional relationships between\ndifferent parts of the input, from their content alone. This amounts to an\nefficient implementation where the pretext task is a classification problem\namong all possible positions for each input token. We experiment on both Vision\nand Speech benchmarks, where our approach brings improvements over strong\nsupervised training baselines and is comparable to modern\nunsupervised/self-supervised pretraining methods. Our method also enables\nTransformers trained without position embeddings to outperform ones trained\nwith full position information.",
        "pdf_link": "https://arxiv.org/pdf/2207.07611v1.pdf"
    },
    {
        "title": "Learning Flexible Translation between Robot Actions and Language Descriptions",
        "authors": [
            "Ozan \u00d6zdemir",
            "Matthias Kerzel",
            "Cornelius Weber",
            "Jae Hee Lee",
            "Stefan Wermter"
        ],
        "published": "2022-07-15T12:37:05Z",
        "summary": "Handling various robot action-language translation tasks flexibly is an\nessential requirement for natural interaction between a robot and a human.\nPrevious approaches require change in the configuration of the model\narchitecture per task during inference, which undermines the premise of\nmulti-task learning. In this work, we propose the paired gated autoencoders\n(PGAE) for flexible translation between robot actions and language descriptions\nin a tabletop object manipulation scenario. We train our model in an end-to-end\nfashion by pairing each action with appropriate descriptions that contain a\nsignal informing about the translation direction. During inference, our model\ncan flexibly translate from action to language and vice versa according to the\ngiven language signal. Moreover, with the option to use a pretrained language\nmodel as the language encoder, our model has the potential to recognise unseen\nnatural language input. Another capability of our model is that it can\nrecognise and imitate actions of another agent by utilising robot\ndemonstrations. The experiment results highlight the flexible bidirectional\ntranslation capabilities of our approach alongside with the ability to\ngeneralise to the actions of the opposite-sitting agent.",
        "pdf_link": "https://arxiv.org/pdf/2207.07437v2.pdf"
    },
    {
        "title": "Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text",
        "authors": [
            "Prerona Tarannum",
            "Firoj Alam",
            "Md. Arid Hasan",
            "Sheak Rashed Haider Noori"
        ],
        "published": "2022-07-15T06:21:35Z",
        "summary": "The wide use of social media and digital technologies facilitates sharing\nvarious news and information about events and activities. Despite sharing\npositive information misleading and false information is also spreading on\nsocial media. There have been efforts in identifying such misleading\ninformation both manually by human experts and automatic tools. Manual effort\ndoes not scale well due to the high volume of information, containing factual\nclaims, are appearing online. Therefore, automatically identifying check-worthy\nclaims can be very useful for human experts. In this study, we describe our\nparticipation in Subtask-1A: Check-worthiness of tweets (English, Dutch and\nSpanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing\nsteps and applied different models to identify whether a given text is worthy\nof fact checking or not. We use the oversampling technique to balance the\ndataset and applied SVM and Random Forest (RF) with TF-IDF representations. We\nalso used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models\nfor the experiments. We used BERT-m for the official submissions and our\nsystems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,\nrespectively. In further experiments, our evaluation shows that transformer\nmodels (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and\nEnglish languages where a different scenario is observed for Spanish.",
        "pdf_link": "https://arxiv.org/pdf/2207.07308v1.pdf"
    },
    {
        "title": "Bootstrapped Masked Autoencoders for Vision BERT Pretraining",
        "authors": [
            "Xiaoyi Dong",
            "Jianmin Bao",
            "Ting Zhang",
            "Dongdong Chen",
            "Weiming Zhang",
            "Lu Yuan",
            "Dong Chen",
            "Fang Wen",
            "Nenghai Yu"
        ],
        "published": "2022-07-14T17:59:58Z",
        "summary": "We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.",
        "pdf_link": "https://arxiv.org/pdf/2207.07116v1.pdf"
    },
    {
        "title": "Confident Adaptive Language Modeling",
        "authors": [
            "Tal Schuster",
            "Adam Fisch",
            "Jai Gupta",
            "Mostafa Dehghani",
            "Dara Bahri",
            "Vinh Q. Tran",
            "Yi Tay",
            "Donald Metzler"
        ],
        "published": "2022-07-14T17:00:19Z",
        "summary": "Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.07061v2.pdf"
    },
    {
        "title": "Language models show human-like content effects on reasoning tasks",
        "authors": [
            "Ishita Dasgupta",
            "Andrew K. Lampinen",
            "Stephanie C. Y. Chan",
            "Hannah R. Sheahan",
            "Antonia Creswell",
            "Dharshan Kumaran",
            "James L. McClelland",
            "Felix Hill"
        ],
        "published": "2022-07-14T16:51:09Z",
        "summary": "Abstract reasoning is a key ability for an intelligent system. Large language\nmodels (LMs) achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect. For example, human reasoning is affected by our real-world knowledge\nand beliefs, and shows notable \"content effects\"; humans reason more reliably\nwhen the semantic content of a problem supports the correct logical inferences.\nThese content-entangled reasoning patterns play a central role in debates about\nthe fundamental nature of human intelligence. Here, we investigate whether\nlanguage models $\\unicode{x2014}$ whose prior expectations capture some aspects\nof human knowledge $\\unicode{x2014}$ similarly mix content into their answers\nto logical problems. We explored this question across three logical reasoning\ntasks: natural language inference, judging the logical validity of syllogisms,\nand the Wason selection task. We evaluate state of the art large language\nmodels, as well as humans, and find that the language models reflect many of\nthe same patterns observed in humans across these tasks $\\unicode{x2014}$ like\nhumans, models answer more accurately when the semantic content of a task\nsupports the logical inferences. These parallels are reflected both in answer\npatterns, and in lower-level features like the relationship between model\nanswer distributions and human response times. Our findings have implications\nfor understanding both these cognitive effects in humans, and the factors that\ncontribute to language model performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.07051v3.pdf"
    },
    {
        "title": "Language Modelling with Pixels",
        "authors": [
            "Phillip Rust",
            "Jonas F. Lotz",
            "Emanuele Bugliarello",
            "Elizabeth Salesky",
            "Miryam de Lhoneux",
            "Desmond Elliott"
        ],
        "published": "2022-07-14T15:20:36Z",
        "summary": "Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust than BERT to orthographic\nattacks and linguistic code-switching, further confirming the benefits of\nmodelling language with pixels.",
        "pdf_link": "https://arxiv.org/pdf/2207.06991v2.pdf"
    },
    {
        "title": "Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language",
        "authors": [
            "Rita Sevastjanova",
            "Mennatallah El-Assady"
        ],
        "published": "2022-07-14T13:26:03Z",
        "summary": "Language models learn and represent language differently than humans; they\nlearn the form and not the meaning. Thus, to assess the success of language\nmodel explainability, we need to consider the impact of its divergence from a\nuser's mental model of language. In this position paper, we argue that in order\nto avoid harmful rationalization and achieve truthful understanding of language\nmodels, explanation processes must satisfy three main conditions: (1)\nexplanations have to truthfully represent the model behavior, i.e., have a high\nfidelity; (2) explanations must be complete, as missing information distorts\nthe truth; and (3) explanations have to take the user's mental model into\naccount, progressively verifying a person's knowledge and adapting their\nunderstanding. We introduce a decision tree model to showcase potential reasons\nwhy current explanations fail to reach their objectives. We further emphasize\nthe need for human-centered design to explain the model from multiple\nperspectives, progressively adapting explanations to changing user\nexpectations.",
        "pdf_link": "https://arxiv.org/pdf/2207.06897v1.pdf"
    },
    {
        "title": "Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages",
        "authors": [
            "Amit Pandey",
            "Swayatta Daw",
            "Narendra Babu Unnam",
            "Vikram Pudi"
        ],
        "published": "2022-07-14T13:00:41Z",
        "summary": "We leverage pre-trained language models to solve the task of complex NER for\ntwo low-resource languages: Chinese and Spanish. We use the technique of Whole\nWord Masking(WWM) to boost the performance of masked language modeling\nobjective on large and unsupervised corpora. We experiment with multiple neural\nnetwork architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on\ntop of a fine-tuned BERT layer. All our models outperform the baseline by a\nsignificant margin and our best performing model obtains a competitive position\non the evaluation leaderboard for the blind test set.",
        "pdf_link": "https://arxiv.org/pdf/2207.06882v1.pdf"
    },
    {
        "title": "Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model",
        "authors": [
            "Chris van der Lee",
            "Thiago Castro Ferreira",
            "Chris Emmery",
            "Travis Wiltshire",
            "Emiel Krahmer"
        ],
        "published": "2022-07-14T11:53:04Z",
        "summary": "This study discusses the effect of semi-supervised learning in combination\nwith pretrained language models for data-to-text generation. It is not known\nwhether semi-supervised learning is still helpful when a large-scale language\nmodel is also supplemented. This study aims to answer this question by\ncomparing a data-to-text system only supplemented with a language model, to two\ndata-to-text systems that are additionally enriched by a data augmentation or a\npseudo-labeling semi-supervised learning approach.\n  Results show that semi-supervised learning results in higher scores on\ndiversity metrics. In terms of output quality, extending the training set of a\ndata-to-text system with a language model using the pseudo-labeling approach\ndid increase text quality scores, but the data augmentation approach yielded\nsimilar scores to the system without training set extension. These results\nindicate that semi-supervised learning approaches can bolster output quality\nand diversity, even when a language model is also present.",
        "pdf_link": "https://arxiv.org/pdf/2207.06839v1.pdf"
    },
    {
        "title": "BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling",
        "authors": [
            "Javier de la Rosa",
            "Eduardo G. Ponferrada",
            "Paulo Villegas",
            "Pablo Gonzalez de Prado Salas",
            "Manu Romero",
            "Mar\u0131a Grandury"
        ],
        "published": "2022-07-14T10:48:42Z",
        "summary": "The pre-training of large language models usually requires massive amounts of\nresources, both in terms of computation and data. Frequently used web sources\nsuch as Common Crawl might contain enough noise to make this pre-training\nsub-optimal. In this work, we experiment with different sampling methods from\nthe Spanish version of mC4, and present a novel data-centric technique which we\nname $\\textit{perplexity sampling}$ that enables the pre-training of language\nmodels in roughly half the amount of steps and using one fifth of the data. The\nresulting models are comparable to the current state-of-the-art, and even\nachieve better results for certain tasks. Our work is proof of the versatility\nof Transformers, and paves the way for small teams to train their models on a\nlimited budget. Our models are available at this\n$\\href{https://huggingface.co/bertin-project}{URL}$.",
        "pdf_link": "https://arxiv.org/pdf/2207.06814v1.pdf"
    },
    {
        "title": "TRIE++: Towards End-to-End Information Extraction from Visually Rich Documents",
        "authors": [
            "Zhanzhan Cheng",
            "Peng Zhang",
            "Can Li",
            "Qiao Liang",
            "Yunlu Xu",
            "Pengfei Li",
            "Shiliang Pu",
            "Yi Niu",
            "Fei Wu"
        ],
        "published": "2022-07-14T08:52:07Z",
        "summary": "Recently, automatically extracting information from visually rich documents\n(e.g., tickets and resumes) has become a hot and vital research topic due to\nits widespread commercial value. Most existing methods divide this task into\ntwo subparts: the text reading part for obtaining the plain text from the\noriginal document images and the information extraction part for extracting key\ncontents. These methods mainly focus on improving the second, while neglecting\nthat the two parts are highly correlated. This paper proposes a unified\nend-to-end information extraction framework from visually rich documents, where\ntext reading and information extraction can reinforce each other via a\nwell-designed multi-modal context block. Specifically, the text reading part\nprovides multi-modal features like visual, textual and layout features. The\nmulti-modal context block is developed to fuse the generated multi-modal\nfeatures and even the prior knowledge from the pre-trained language model for\nbetter semantic representation. The information extraction part is responsible\nfor generating key contents with the fused context features. The framework can\nbe trained in an end-to-end trainable manner, achieving global optimization.\nWhat is more, we define and group visually rich documents into four categories\nacross two dimensions, the layout and text type. For each document category, we\nprovide or recommend the corresponding benchmarks, experimental settings and\nstrong baselines for remedying the problem that this research area lacks the\nuniform evaluation standard. Extensive experiments on four kinds of benchmarks\n(from fixed layout to variable layout, from full-structured text to\nsemi-unstructured text) are reported, demonstrating the proposed method's\neffectiveness. Data, source code and models are available.",
        "pdf_link": "https://arxiv.org/pdf/2207.06744v1.pdf"
    },
    {
        "title": "Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration",
        "authors": [
            "Zhenyu Zhang",
            "Bowen Yu",
            "Haiyang Yu",
            "Tingwen Liu",
            "Cheng Fu",
            "Jingyang Li",
            "Chengguang Tang",
            "Jian Sun",
            "Yongbin Li"
        ],
        "published": "2022-07-14T07:59:45Z",
        "summary": "Building document-grounded dialogue systems have received growing interest as\ndocuments convey a wealth of human knowledge and commonly exist in enterprises.\nWherein, how to comprehend and retrieve information from documents is a\nchallenging research problem. Previous work ignores the visual property of\ndocuments and treats them as plain text, resulting in incomplete modality. In\nthis paper, we propose a Layout-aware document-level Information Extraction\ndataset, LIE, to facilitate the study of extracting both structural and\nsemantic knowledge from visually rich documents (VRDs), so as to generate\naccurate responses in dialogue systems. LIE contains 62k annotations of three\nextraction tasks from 4,061 pages in product and official documents, becoming\nthe largest VRD-based information extraction dataset to the best of our\nknowledge. We also develop benchmark methods that extend the token-based\nlanguage model to consider layout features like humans. Empirical results show\nthat layout is critical for VRD-based extraction, and system demonstration also\nverifies that the extracted knowledge can help locate the answers that users\ncare about.",
        "pdf_link": "https://arxiv.org/pdf/2207.06717v1.pdf"
    },
    {
        "title": "Overview of Abusive and Threatening Language Detection in Urdu at FIRE 2021",
        "authors": [
            "Maaz Amjad",
            "Alisa Zhila",
            "Grigori Sidorov",
            "Andrey Labunets",
            "Sabur Butta",
            "Hamza Imam Amjad",
            "Oxana Vitman",
            "Alexander Gelbukh"
        ],
        "published": "2022-07-14T07:38:13Z",
        "summary": "With the growth of social media platform influence, the effect of their\nmisuse becomes more and more impactful. The importance of automatic detection\nof threatening and abusive language can not be overestimated. However, most of\nthe existing studies and state-of-the-art methods focus on English as the\ntarget language, with limited work on low- and medium-resource languages. In\nthis paper, we present two shared tasks of abusive and threatening language\ndetection for the Urdu language which has more than 170 million speakers\nworldwide. Both are posed as binary classification tasks where participating\nsystems are required to classify tweets in Urdu into two classes, namely: (i)\nAbusive and Non-Abusive for the first task, and (ii) Threatening and\nNon-Threatening for the second. We present two manually annotated datasets\ncontaining tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening\nand Non-Threatening. The abusive dataset contains 2400 annotated tweets in the\ntrain part and 1100 annotated tweets in the test part. The threatening dataset\ncontains 6000 annotated tweets in the train part and 3950 annotated tweets in\nthe test part. We also provide logistic regression and BERT-based baseline\nclassifiers for both tasks. In this shared task, 21 teams from six countries\nregistered for participation (India, Pakistan, China, Malaysia, United Arab\nEmirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is\nAbusive Language Detection and 9 teams submitted their runs for Subtask B,\nwhich is Threatening Language detection, and seven teams submitted their\ntechnical reports. The best performing system achieved an F1-score value of\n0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based\ntransformer model showed the best performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.06710v1.pdf"
    },
    {
        "title": "Combing for Credentials: Active Pattern Extraction from Smart Reply",
        "authors": [
            "Bargav Jayaraman",
            "Esha Ghosh",
            "Melissa Chase",
            "Sambuddha Roy",
            "Wei Dai",
            "David Evans"
        ],
        "published": "2022-07-14T05:03:56Z",
        "summary": "Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, are\noften fine-tuned to achieve state-of-the-art performance on a downstream task.\nOne natural example is the ``Smart Reply'' application where a pre-trained\nmodel is tuned to provide suggested responses for a given query message. Since\nthe tuning data is often sensitive data such as emails or chat transcripts, it\nis important to understand and mitigate the risk that the model leaks its\ntuning data. We investigate potential information leakage vulnerabilities in a\ntypical Smart Reply pipeline. We consider a realistic setting where the\nadversary can only interact with the underlying model through a front-end\ninterface that constrains what types of queries can be sent to the model.\nPrevious attacks do not work in these settings, but require the ability to send\nunconstrained queries directly to the model. Even when there are no constraints\non the queries, previous attacks typically require thousands, or even millions,\nof queries to extract useful information, while our attacks can extract\nsensitive data in just a handful of queries. We introduce a new type of active\nextraction attack that exploits canonical patterns in text containing sensitive\ndata. We show experimentally that it is possible for an adversary to extract\nsensitive user information present in the training data, even in realistic\nsettings where all interactions with the model must go through a front-end that\nlimits the types of queries. We explore potential mitigation strategies and\ndemonstrate empirically how differential privacy appears to be a reasonably\neffective defense mechanism to such pattern extraction attacks.",
        "pdf_link": "https://arxiv.org/pdf/2207.10802v3.pdf"
    },
    {
        "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America",
        "authors": [
            "Laura Alonso Alemany",
            "Luciana Benotti",
            "Hern\u00e1n Maina",
            "Luc\u00eda Gonz\u00e1lez",
            "Mariela Rajngewerc",
            "Lautaro Mart\u00ednez",
            "Jorge S\u00e1nchez",
            "Mauro Schilman",
            "Guido Ivetta",
            "Alexia Halvorsen",
            "Amanda Mata Rojo",
            "Mat\u00edas Bordone",
            "Beatriz Busaniche"
        ],
        "published": "2022-07-14T01:07:55Z",
        "summary": "Automated decision-making systems, especially those based on natural language\nprocessing, are pervasive in our lives. They are not only behind the internet\nsearch engines we use daily, but also take more critical roles: selecting\ncandidates for a job, determining suspects of a crime, diagnosing autism and\nmore. Such automated systems make errors, which may be harmful in many ways, be\nit because of the severity of the consequences (as in health issues) or because\nof the sheer number of people they affect. When errors made by an automated\nsystem affect a population more than others, we call the system\n\\textit{biased}.\n  Most modern natural language technologies are based on artifacts obtained\nfrom enormous volumes of text using machine learning, namely language models\nand word embeddings. Since they are created by applying subsymbolic machine\nlearning, mostly artificial neural networks, they are opaque and practically\nuninterpretable by direct inspection, thus making it very difficult to audit\nthem.\n  In this paper, we present a methodology that spells out how social\nscientists, domain experts, and machine learning experts can collaboratively\nexplore biases and harmful stereotypes in word embeddings and large language\nmodels. Our methodology is based on the following principles:\n  * focus on the linguistic manifestations of discrimination on word embeddings\nand language models, not on the mathematical properties of the models * reduce\nthe technical barrier for discrimination experts%, be it social scientists,\ndomain experts or other * characterize through a qualitative exploratory\nprocess in addition to a metric-based approach * address mitigation as part of\nthe training process, not as an afterthought",
        "pdf_link": "https://arxiv.org/pdf/2207.06591v3.pdf"
    },
    {
        "title": "Re2G: Retrieve, Rerank, Generate",
        "authors": [
            "Michael Glass",
            "Gaetano Rossiello",
            "Md Faisal Mahbub Chowdhury",
            "Ankita Rajaram Naik",
            "Pengshan Cai",
            "Alfio Gliozzo"
        ],
        "published": "2022-07-13T15:51:40Z",
        "summary": "As demonstrated by GPT-3 and T5, transformers grow in capability as parameter\nspaces become larger and larger. However, for tasks that require a large amount\nof knowledge, non-parametric memory allows models to grow dramatically with a\nsub-linear increase in computational cost and GPU memory requirements. Recent\nmodels such as RAG and REALM have introduced retrieval into conditional\ngeneration. These models incorporate neural initial retrieval from a corpus of\npassages. We build on this line of research, proposing Re2G, which combines\nboth neural initial retrieval and reranking into a BART-based\nsequence-to-sequence generation. Our reranking approach also permits merging\nretrieval results from sources with incomparable scores, enabling an ensemble\nof BM25 and neural initial retrieval. To train our system end-to-end, we\nintroduce a novel variation of knowledge distillation to train the initial\nretrieval, reranker, and generation using only ground truth on the target\nsequence output. We find large gains in four diverse tasks: zero-shot slot\nfilling, question answering, fact-checking, and dialog, with relative gains of\n9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make\nour code available as open source at\nhttps://github.com/IBM/kgi-slot-filling/tree/re2g.",
        "pdf_link": "https://arxiv.org/pdf/2207.06300v1.pdf"
    },
    {
        "title": "A Transfer Learning Based Model for Text Readability Assessment in German",
        "authors": [
            "Salar Mohtaj",
            "Babak Naderi",
            "Sebastian M\u00f6ller",
            "Faraz Maschhur",
            "Chuyang Wu",
            "Max Reinhard"
        ],
        "published": "2022-07-13T15:15:44Z",
        "summary": "Text readability assessment has a wide range of applications for different\ntarget people, from language learners to people with disabilities. The fast\npace of textual content production on the web makes it impossible to measure\ntext complexity without the benefit of machine learning and natural language\nprocessing techniques. Although various research addressed the readability\nassessment of English text in recent years, there is still room for improvement\nof the models for other languages. In this paper, we proposed a new model for\ntext complexity assessment for German text based on transfer learning. Our\nresults show that the model outperforms more classical solutions based on\nlinguistic features extraction from input text. The best model is based on the\nBERT pre-trained language model achieved the Root Mean Square Error (RMSE) of\n0.483.",
        "pdf_link": "https://arxiv.org/pdf/2207.06265v2.pdf"
    },
    {
        "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
        "authors": [
            "Yookyung Shin",
            "Younggun Lee",
            "Suhee Jo",
            "Yeongtae Hwang",
            "Taesu Kim"
        ],
        "published": "2022-07-13T07:05:44Z",
        "summary": "Expressive text-to-speech has shown improved performance in recent years.\nHowever, the style control of synthetic speech is often restricted to discrete\nemotion categories and requires training data recorded by the target speaker in\nthe target style. In many practical situations, users may not have reference\nspeech recorded in target emotion but still be interested in controlling speech\nstyle just by typing text description of desired emotional style. In this\npaper, we propose a text-based interface for emotional style control and\ncross-speaker style transfer in multi-speaker TTS. We propose the bi-modal\nstyle encoder which models the semantic relationship between text description\nembedding and speech style embedding with a pretrained language model. To\nfurther improve cross-speaker style transfer on disjoint, multi-style datasets,\nwe propose the novel style loss. The experimental results show that our model\ncan generate high-quality expressive speech even in unseen style.",
        "pdf_link": "https://arxiv.org/pdf/2207.06000v1.pdf"
    },
    {
        "title": "DocPrompting: Generating Code by Retrieving the Docs",
        "authors": [
            "Shuyan Zhou",
            "Uri Alon",
            "Frank F. Xu",
            "Zhiruo Wang",
            "Zhengbao Jiang",
            "Graham Neubig"
        ],
        "published": "2022-07-13T06:47:51Z",
        "summary": "Publicly available source-code libraries are continuously growing and\nchanging. This makes it impossible for models of code to keep current with all\navailable APIs by simply training these models on existing code repositories.\nThus, existing models inherently cannot generalize to using unseen functions\nand libraries, because these would never appear in the training data. In\ncontrast, when human programmers use functions and libraries for the first\ntime, they frequently refer to textual resources such as code manuals and\ndocumentation, to explore and understand the available functionality. Inspired\nby this observation, we introduce DocPrompting: a natural-language-to-code\ngeneration approach that explicitly leverages documentation by (1) retrieving\nthe relevant documentation pieces given an NL intent, and (2) generating code\nbased on the NL intent and the retrieved documentation. DocPrompting is\ngeneral: it can be applied to any programming language and is agnostic to the\nunderlying neural model. We demonstrate that DocPrompting consistently improves\nNL-to-code models: DocPrompting improves strong base models such as CodeT5 by\n2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in\nexecution-based evaluation on the popular Python CoNaLa benchmark; on a new\nBash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to\nabsolute 6.9% exact match.",
        "pdf_link": "https://arxiv.org/pdf/2207.05987v3.pdf"
    },
    {
        "title": "Developing a Component Comment Extractor from Product Reviews on E-Commerce Sites",
        "authors": [
            "Shogo Anda",
            "Masato Kikuchi",
            "Tadachika Ozono"
        ],
        "published": "2022-07-13T06:25:55Z",
        "summary": "Consumers often read product reviews to inform their buying decision, as some\nconsumers want to know a specific component of a product. However, because\ntypical sentences on product reviews contain various details, users must\nidentify sentences about components they want to know amongst the many reviews.\nTherefore, we aimed to develop a system that identifies and collects component\nand aspect information of products in sentences. Our BERT-based classifiers\nassign labels referring to components and aspects to sentences in reviews and\nextract sentences with comments on specific components and aspects. We\ndetermined proper labels based for the words identified through pattern\nmatching from product reviews to create the training data. Because we could not\nuse the words as labels, we carefully created labels covering the meanings of\nthe words. However, the training data was imbalanced on component and aspect\npairs. We introduced a data augmentation method using WordNet to reduce the\nbias. Our evaluation demonstrates that the system can determine labels for road\nbikes using pattern matching, covering more than 88\\% of the indicators of\ncomponents and aspects on e-commerce sites. Moreover, our data augmentation\nmethod can improve the-F1-measure on insufficient data from 0.66 to 0.76.",
        "pdf_link": "https://arxiv.org/pdf/2207.05979v1.pdf"
    },
    {
        "title": "Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models",
        "authors": [
            "Wenbiao Li",
            "Rui Sun",
            "Yunfang Wu"
        ],
        "published": "2022-07-13T02:28:08Z",
        "summary": "Most of the Chinese pre-trained models adopt characters as basic units for\ndownstream tasks. However, these models ignore the information carried by words\nand thus lead to the loss of some important semantics. In this paper, we\npropose a new method to exploit word structure and integrate lexical semantics\ninto character representations of pre-trained models. Specifically, we project\na word's embedding into its internal characters' embeddings according to the\nsimilarity weight. To strengthen the word boundary information, we mix the\nrepresentations of the internal characters within a word. After that, we apply\na word-to-character alignment attention mechanism to emphasize important\ncharacters by masking unimportant ones. Moreover, in order to reduce the error\npropagation caused by word segmentation, we present an ensemble approach to\ncombine segmentation results given by different tokenizers. The experimental\nresults show that our approach achieves superior performance over the basic\npre-trained models BERT, BERT-wwm and ERNIE on different Chinese NLP tasks:\nsentiment classification, sentence pair matching, natural language inference\nand machine reading comprehension. We make further analysis to prove the\neffectiveness of each component of our model.",
        "pdf_link": "https://arxiv.org/pdf/2207.05928v1.pdf"
    },
    {
        "title": "A Novel DeBERTa-based Model for Financial Question Answering Task",
        "authors": [
            "Yanbo J. Wang",
            "Yuming Li",
            "Hui Qin",
            "Yuhang Guan",
            "Sheng Chen"
        ],
        "published": "2022-07-12T22:34:39Z",
        "summary": "As a rising star in the field of natural language processing, question\nanswering systems (Q&A Systems) are widely used in all walks of life. Compared\nwith other scenarios, the applicationin financial scenario has strong\nrequirements in the traceability and interpretability of the Q&A systems. In\naddition, since the demand for artificial intelligence technology has gradually\nshifted from the initial computational intelligence to cognitive intelligence,\nthis research mainly focuses on the financial numerical reasoning dataset -\nFinQA. In the shared task, the objective is to generate the reasoning program\nand the final answer according to the given financial report containing text\nand tables. We use the method based on DeBERTa pre-trained language model, with\nadditional optimization methods including multi-model fusion, training set\ncombination on this basis. We finally obtain an execution accuracy of 68.99 and\na program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.",
        "pdf_link": "https://arxiv.org/pdf/2207.05875v1.pdf"
    },
    {
        "title": "Learning Bellman Complete Representations for Offline Policy Evaluation",
        "authors": [
            "Jonathan D. Chang",
            "Kaiwen Wang",
            "Nathan Kallus",
            "Wen Sun"
        ],
        "published": "2022-07-12T21:02:02Z",
        "summary": "We study representation learning for Offline Reinforcement Learning (RL),\nfocusing on the important task of Offline Policy Evaluation (OPE). Recent work\nshows that, in contrast to supervised learning, realizability of the Q-function\nis not enough for learning it. Two sufficient conditions for sample-efficient\nOPE are Bellman completeness and coverage. Prior work often assumes that\nrepresentations satisfying these conditions are given, with results being\nmostly theoretical in nature. In this work, we propose BCRL, which directly\nlearns from data an approximately linear Bellman complete representation with\ngood coverage. With this learned representation, we perform OPE using Least\nSquare Policy Evaluation (LSPE) with linear functions in our learned\nrepresentation. We present an end-to-end theoretical analysis, showing that our\ntwo-stage algorithm enjoys polynomial sample complexity provided some\nrepresentation in the rich class considered is linear Bellman complete.\nEmpirically, we extensively evaluate our algorithm on challenging, image-based\ncontinuous control tasks from the Deepmind Control Suite. We show our\nrepresentation enables better OPE compared to previous representation learning\nmethods developed for off-policy RL (e.g., CURL, SPR). BCRL achieve competitive\nOPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats\nFQE when evaluating beyond the initial state distribution. Our ablations show\nthat both linear Bellman complete and coverage components of our method are\ncrucial.",
        "pdf_link": "https://arxiv.org/pdf/2207.05837v1.pdf"
    },
    {
        "title": "How Do Multilingual Encoders Learn Cross-lingual Representation?",
        "authors": [
            "Shijie Wu"
        ],
        "published": "2022-07-12T17:57:05Z",
        "summary": "NLP systems typically require support for more than one language. As\ndifferent languages have different amounts of supervision, cross-lingual\ntransfer benefits languages with little to no training data by transferring\nfrom other languages. From an engineering perspective, multilingual NLP\nbenefits development and maintenance by serving multiple languages with a\nsingle system. Both cross-lingual transfer and multilingual NLP rely on\ncross-lingual representations serving as the foundation. As BERT revolutionized\nrepresentation learning and NLP, it also revolutionized cross-lingual\nrepresentations and cross-lingual transfer. Multilingual BERT was released as a\nreplacement for single-language BERT, trained with Wikipedia data in 104\nlanguages.\n  Surprisingly, without any explicit cross-lingual signal, multilingual BERT\nlearns cross-lingual representations in addition to representations for\nindividual languages. This thesis first shows such surprising cross-lingual\neffectiveness compared against prior art on various tasks. Naturally, it raises\na set of questions, most notably how do these multilingual encoders learn\ncross-lingual representations. In exploring these questions, this thesis will\nanalyze the behavior of multilingual models in a variety of settings on high\nand low resource languages. We also look at how to inject different\ncross-lingual signals into multilingual encoders, and the optimization behavior\nof cross-lingual transfer with these models. Together, they provide a better\nunderstanding of multilingual encoders on cross-lingual transfer. Our findings\nwill lead us to suggested improvements to multilingual encoders and\ncross-lingual transfer.",
        "pdf_link": "https://arxiv.org/pdf/2207.05737v1.pdf"
    },
    {
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
        "authors": [
            "Wenlong Huang",
            "Fei Xia",
            "Ted Xiao",
            "Harris Chan",
            "Jacky Liang",
            "Pete Florence",
            "Andy Zeng",
            "Jonathan Tompson",
            "Igor Mordatch",
            "Yevgen Chebotar",
            "Pierre Sermanet",
            "Noah Brown",
            "Tomas Jackson",
            "Linda Luu",
            "Sergey Levine",
            "Karol Hausman",
            "Brian Ichter"
        ],
        "published": "2022-07-12T15:20:48Z",
        "summary": "Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.",
        "pdf_link": "https://arxiv.org/pdf/2207.05608v1.pdf"
    },
    {
        "title": "Ego-motion Estimation Based on Fusion of Images and Events",
        "authors": [
            "Liren Yang"
        ],
        "published": "2022-07-12T15:10:28Z",
        "summary": "Event camera is a novel bio-inspired vision sensor that outputs event stream.\nIn this paper, we propose a novel data fusion algorithm called EAS to fuse\nconventional intensity images with the event stream. The fusion result is\napplied to some ego-motion estimation frameworks, and is evaluated on a public\ndataset acquired in dim scenes. In our 3-DoF rotation estimation framework, EAS\nachieves the highest estimation accuracy among intensity images and\nrepresentations of events including event slice, TS and SITS. Compared with\noriginal images, EAS reduces the average APE by 69%, benefiting from the\ninclusion of more features for tracking. The result shows that our algorithm\neffectively leverages the high dynamic range of event cameras to improve the\nperformance of the ego-motion estimation framework based on optical flow\ntracking in difficult illumination conditions.",
        "pdf_link": "https://arxiv.org/pdf/2207.05588v1.pdf"
    },
    {
        "title": "Using Paraphrases to Study Properties of Contextual Embeddings",
        "authors": [
            "Laura Burdick",
            "Jonathan K. Kummerfeld",
            "Rada Mihalcea"
        ],
        "published": "2022-07-12T14:22:05Z",
        "summary": "We use paraphrases as a unique source of data to analyze contextualized\nembeddings, with a particular focus on BERT. Because paraphrases naturally\nencode consistent word and phrase semantics, they provide a unique lens for\ninvestigating properties of embeddings. Using the Paraphrase Database's\nalignments, we study words within paraphrases as well as phrase\nrepresentations. We find that contextual embeddings effectively handle\npolysemous words, but give synonyms surprisingly different representations in\nmany cases. We confirm previous findings that BERT is sensitive to word order,\nbut find slightly different patterns than prior work in terms of the level of\ncontextualization across BERT's layers.",
        "pdf_link": "https://arxiv.org/pdf/2207.05553v1.pdf"
    },
    {
        "title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021",
        "authors": [
            "Maaz Amjad",
            "Sabur Butt",
            "Hamza Imam Amjad",
            "Alisa Zhila",
            "Grigori Sidorov",
            "Alexander Gelbukh"
        ],
        "published": "2022-07-11T18:58:36Z",
        "summary": "Automatic detection of fake news is a highly important task in the\ncontemporary world. This study reports the 2nd shared task called\nUrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the\nshared task is to motivate the community to come up with efficient methods for\nsolving this vital problem, particularly for the Urdu language. The task is\nposed as a binary classification problem to label a given news article as a\nreal or a fake news article. The organizers provide a dataset comprising news\nin five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and\n(v) Business, split into training and testing sets. The training set contains\n1300 annotated news articles -- 750 real news, 550 fake news, while the testing\nset contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7\ndifferent countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)\nregistered to participate in the UrduFake@FIRE2021 shared task. Out of those,\n18 teams submitted their experimental results, and 11 of those submitted their\ntechnical reports, which is substantially higher compared to the UrduFake\nshared task in 2020 when only 6 teams submitted their technical reports. The\ntechnical reports submitted by the participants demonstrated different data\nrepresentation techniques ranging from count-based BoW features to word vector\nembeddings as well as the use of numerous machine learning algorithms ranging\nfrom traditional SVM to various neural network architectures including\nTransformers such as BERT and RoBERTa. In this year's competition, the best\nperforming system obtained an F1-macro score of 0.679, which is lower than the\npast year's best result of 0.907 F1-macro. Admittedly, while training sets from\nthe past and the current years overlap to a large extent, the testing set\nprovided this year is completely different.",
        "pdf_link": "https://arxiv.org/pdf/2207.05133v1.pdf"
    },
    {
        "title": "Exploring Length Generalization in Large Language Models",
        "authors": [
            "Cem Anil",
            "Yuhuai Wu",
            "Anders Andreassen",
            "Aitor Lewkowycz",
            "Vedant Misra",
            "Vinay Ramasesh",
            "Ambrose Slone",
            "Guy Gur-Ari",
            "Ethan Dyer",
            "Behnam Neyshabur"
        ],
        "published": "2022-07-11T14:24:38Z",
        "summary": "The ability to extrapolate from short problem instances to longer ones is an\nimportant form of out-of-distribution generalization in reasoning tasks, and is\ncrucial when learning from datasets where longer problem instances are rare.\nThese include theorem proving, solving quantitative mathematics problems, and\nreading/summarizing novels. In this paper, we run careful empirical studies\nexploring the length generalization capabilities of transformer-based language\nmodels. We first establish that naively finetuning transformers on length\ngeneralization tasks shows significant generalization deficiencies independent\nof model scale. We then show that combining pretrained large language models'\nin-context learning abilities with scratchpad prompting (asking the model to\noutput solution steps before producing an answer) results in a dramatic\nimprovement in length generalization. We run careful failure analyses on each\nof the learning modalities and identify common sources of mistakes that\nhighlight opportunities in equipping language models with the ability to\ngeneralize to longer problems.",
        "pdf_link": "https://arxiv.org/pdf/2207.04901v2.pdf"
    },
    {
        "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
        "authors": [
            "Zihan Zhao",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2022-07-11T08:20:53Z",
        "summary": "The research and applications of multimodal emotion recognition have become\nincreasingly popular recently. However, multimodal emotion recognition faces\nthe challenge of lack of data. To solve this problem, we propose to use\ntransfer learning which leverages state-of-the-art pre-trained models including\nwav2vec 2.0 and BERT for this task. Multi-level fusion approaches including\ncoattention-based early fusion and late fusion with the models trained on both\nembeddings are explored. Also, a multi-granularity framework which extracts not\nonly frame-level speech embeddings but also segment-level embeddings including\nphone, syllable and word-level speech embeddings is proposed to further boost\nthe performance. By combining our coattention-based early fusion model and late\nfusion model with the multi-granularity feature extraction framework, we obtain\nresult that outperforms best baseline approaches by 1.3% unweighted accuracy\n(UA) on the IEMOCAP dataset.",
        "pdf_link": "https://arxiv.org/pdf/2207.04697v2.pdf"
    },
    {
        "title": "Learning Large-scale Universal User Representation with Sparse Mixture of Experts",
        "authors": [
            "Caigao Jiang",
            "Siqiao Xue",
            "James Zhang",
            "Lingyue Liu",
            "Zhibo Zhu",
            "Hongyan Hao"
        ],
        "published": "2022-07-11T06:19:03Z",
        "summary": "Learning user sequence behaviour embedding is very sophisticated and\nchallenging due to the complicated feature interactions over time and high\ndimensions of user features. Recent emerging foundation models, e.g., BERT and\nits variants, encourage a large body of researchers to investigate in this\nfield. However, unlike natural language processing (NLP) tasks, the parameters\nof user behaviour model come mostly from user embedding layer, which makes most\nexisting works fail in training a universal user embedding of large scale.\nFurthermore, user representations are learned from multiple downstream tasks,\nand the past research work do not address the seesaw phenomenon. In this paper,\nwe propose SUPERMOE, a generic framework to obtain high quality user\nrepresentation from multiple tasks. Specifically, the user behaviour sequences\nare encoded by MoE transformer, and we can thus increase the model capacity to\nbillions of parameters, or even to trillions of parameters. In order to deal\nwith seesaw phenomenon when learning across multiple tasks, we design a new\nloss function with task indicators. We perform extensive offline experiments on\npublic datasets and online experiments on private real-world business\nscenarios. Our approach achieves the best performance over state-of-the-art\nmodels, and the results demonstrate the effectiveness of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2207.04648v1.pdf"
    },
    {
        "title": "Myers-Briggs personality classification from social media text using pre-trained language models",
        "authors": [
            "Vitor Garcia dos Santos",
            "Ivandr\u00e9 Paraboni"
        ],
        "published": "2022-07-10T14:38:09Z",
        "summary": "In Natural Language Processing, the use of pre-trained language models has\nbeen shown to obtain state-of-the-art results in many downstream tasks such as\nsentiment analysis, author identification and others. In this work, we address\nthe use of these methods for personality classification from text. Focusing on\nthe Myers-Briggs (MBTI) personality model, we describe a series of experiments\nin which the well-known Bidirectional Encoder Representations from Transformers\n(BERT) model is fine-tuned to perform MBTI classification. Our main findings\nsuggest that the current approach significantly outperforms well-known text\nclassification models based on bag-of-words and static word embeddings alike\nacross multiple evaluation scenarios, and generally outperforms previous work\nin the field.",
        "pdf_link": "https://arxiv.org/pdf/2207.04476v1.pdf"
    },
    {
        "title": "Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP",
        "authors": [
            "Teemu P\u00f6yh\u00f6nen",
            "Mika H\u00e4m\u00e4l\u00e4inen",
            "Khalid Alnajjar"
        ],
        "published": "2022-07-10T12:38:02Z",
        "summary": "Role-playing games (RPGs) have a considerable amount of text in video game\ndialogues. Quite often this text is semi-annotated by the game developers. In\nthis paper, we extract a multilingual dataset of persuasive dialogue from\nseveral RPGs. We show the viability of this data in building a persuasion\ndetection system using a natural language processing (NLP) model called BERT.\nWe believe that video games have a lot of unused potential as a datasource for\na variety of NLP tasks. The code and data described in this paper are available\non Zenodo.",
        "pdf_link": "https://arxiv.org/pdf/2207.04453v1.pdf"
    },
    {
        "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",
        "authors": [
            "Dhruv Shah",
            "Blazej Osinski",
            "Brian Ichter",
            "Sergey Levine"
        ],
        "published": "2022-07-10T10:41:50Z",
        "summary": "Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav",
        "pdf_link": "https://arxiv.org/pdf/2207.04429v2.pdf"
    },
    {
        "title": "Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis",
        "authors": [
            "Taha Belkhouja",
            "Yan Yan",
            "Janardhan Rao Doppa"
        ],
        "published": "2022-07-09T17:21:03Z",
        "summary": "Despite the success of deep neural networks (DNNs) for real-world\napplications over time-series data such as mobile health, little is known about\nhow to train robust DNNs for time-series domain due to its unique\ncharacteristics compared to images and text data. In this paper, we propose a\nnovel algorithmic framework referred as RObust Training for Time-Series (RO-TS)\nto create robust DNNs for time-series classification tasks. Specifically, we\nformulate a min-max optimization problem over the model parameters by\nexplicitly reasoning about the robustness criteria in terms of additive\nperturbations to time-series inputs measured by the global alignment kernel\n(GAK) based distance. We also show the generality and advantages of our\nformulation using the summation structure over time-series alignments by\nrelating both GAK and dynamic time warping (DTW). This problem is an instance\nof a family of compositional min-max optimization problems, which are\nchallenging and open with unclear theoretical guarantee. We propose a\nprincipled stochastic compositional alternating gradient descent ascent\n(SCAGDA) algorithm for this family of optimization problems. Unlike traditional\nmethods for time-series that require approximate computation of distance\nmeasures, SCAGDA approximates the GAK based distance on-the-fly using a moving\naverage approach. We theoretically analyze the convergence rate of SCAGDA and\nprovide strong theoretical support for the estimation of GAK based distance.\nOur experiments on real-world benchmarks demonstrate that RO-TS creates more\nrobust DNNs when compared to adversarial training using prior methods that rely\non data augmentation or new definitions of loss functions. We also demonstrate\nthe importance of GAK for time-series data over the Euclidean distance. The\nsource code of RO-TS algorithms is available at\nhttps://github.com/tahabelkhouja/Robust-Training-for-Time-Series",
        "pdf_link": "https://arxiv.org/pdf/2207.04305v2.pdf"
    },
    {
        "title": "Few-shot training LLMs for project-specific code-summarization",
        "authors": [
            "Toufique Ahmed",
            "Premkumar Devanbu"
        ],
        "published": "2022-07-09T09:57:11Z",
        "summary": "Very large language models (LLMs), such as GPT-3 and Codex have achieved\nstate-of-the-art performance on several natural-language tasks, and show great\npromise also for code. A particularly exciting aspect of LLMs is their knack\nfor few-shot and zero-shot learning: they can learn to perform a task with very\nfew examples. Few-shotting has particular synergies in software engineering,\nwhere there are a lot of phenomena (identifier names, APIs, terminology, coding\npatterns) that are known to be highly project-specific. However,\nproject-specific data can be quite limited, especially early in the history of\na project; thus the few-shot learning capacity of LLMs might be very relevant.\nIn this paper, we investigate the use few-shot training with the very large GPT\n(Generative Pre-trained Transformer) Codex model, and find evidence suggesting\nthat one can significantly surpass state-of-the-art models for\ncode-summarization, leveraging project-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2207.04237v2.pdf"
    },
    {
        "title": "Internal Language Model Estimation based Language Model Fusion for Cross-Domain Code-Switching Speech Recognition",
        "authors": [
            "Yizhou Peng",
            "Yufei Liu",
            "Jicheng Zhang",
            "Haihua Xu",
            "Yi He",
            "Hao Huang",
            "Eng Siong Chng"
        ],
        "published": "2022-07-09T02:08:54Z",
        "summary": "Internal Language Model Estimation (ILME) based language model (LM) fusion\nhas been shown significantly improved recognition results over conventional\nshallow fusion in both intra-domain and cross-domain speech recognition tasks.\nIn this paper, we attempt to apply our ILME method to cross-domain\ncode-switching speech recognition (CSSR) work. Specifically, our curiosity\ncomes from several aspects. First, we are curious about how effective the\nILME-based LM fusion is for both intra-domain and cross-domain CSSR tasks. We\nverify this with or without merging two code-switching domains. More\nimportantly, we train an end-to-end (E2E) speech recognition model by means of\nmerging two monolingual data sets and observe the efficacy of the proposed\nILME-based LM fusion for CSSR. Experimental results on SEAME that is from\nSoutheast Asian and another Chinese Mainland CS data set demonstrate the\neffectiveness of the proposed ILME-based LM fusion method.",
        "pdf_link": "https://arxiv.org/pdf/2207.04176v1.pdf"
    },
    {
        "title": "ABB-BERT: A BERT model for disambiguating abbreviations and contractions",
        "authors": [
            "Prateek Kacker",
            "Andi Cupallari",
            "Aswin Gridhar Subramanian",
            "Nimit Jain"
        ],
        "published": "2022-07-08T16:54:57Z",
        "summary": "Abbreviations and contractions are commonly found in text across different\ndomains. For example, doctors' notes contain many contractions that can be\npersonalized based on their choices. Existing spelling correction models are\nnot suitable to handle expansions because of many reductions of characters in\nwords. In this work, we propose ABB-BERT, a BERT-based model, which deals with\nan ambiguous language containing abbreviations and contractions. ABB-BERT can\nrank them from thousands of options and is designed for scale. It is trained on\nWikipedia text, and the algorithm allows it to be fine-tuned with little\ncompute to get better performance for a domain or person. We are publicly\nreleasing the training dataset for abbreviations and contractions derived from\nWikipedia.",
        "pdf_link": "https://arxiv.org/pdf/2207.04008v1.pdf"
    },
    {
        "title": "Hidden Schema Networks",
        "authors": [
            "Rams\u00e9s J. S\u00e1nchez",
            "Lukas Conrads",
            "Pascal Welke",
            "Kostadin Cvejoski",
            "C\u00e9sar Ojeda"
        ],
        "published": "2022-07-08T09:26:19Z",
        "summary": "Large, pretrained language models infer powerful representations that encode\nrich semantic and syntactic content, albeit implicitly. In this work we\nintroduce a novel neural language model that enforces, via inductive biases,\nexplicit relational structures which allow for compositionality onto the output\nrepresentations of pretrained language models. Specifically, the model encodes\nsentences into sequences of symbols (composed representations), which\ncorrespond to the nodes visited by biased random walkers on a global latent\ngraph, and infers the posterior distribution of the latter. We first\ndemonstrate that the model is able to uncover ground-truth graphs from\nartificially generated datasets of random token sequences. Next, we leverage\npretrained BERT and GPT-2 language models as encoder and decoder, respectively,\nto infer networks of symbols (schemata) from natural language datasets. Our\nexperiments show that (i) the inferred symbols can be interpreted as encoding\ndifferent aspects of language, as e.g. topics or sentiments, and that (ii)\nGPT-like models can effectively be conditioned on symbolic representations.\nFinally, we explore training autoregressive, random walk ``reasoning\" models on\nschema networks inferred from commonsense knowledge databases, and using the\nsampled paths to enhance the performance of pretrained language models on\ncommonsense If-Then reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2207.03777v2.pdf"
    },
    {
        "title": "VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning",
        "authors": [
            "Matteo Bettini",
            "Ryan Kortvelesy",
            "Jan Blumenkamp",
            "Amanda Prorok"
        ],
        "published": "2022-07-07T18:48:58Z",
        "summary": "While many multi-robot coordination problems can be solved optimally by exact\nalgorithms, solutions are often not scalable in the number of robots.\nMulti-Agent Reinforcement Learning (MARL) is gaining increasing attention in\nthe robotics community as a promising solution to tackle such problems.\nNevertheless, we still lack the tools that allow us to quickly and efficiently\nfind solutions to large-scale collective learning tasks. In this work, we\nintroduce the Vectorized Multi-Agent Simulator (VMAS). VMAS is an open-source\nframework designed for efficient MARL benchmarking. It is comprised of a\nvectorized 2D physics engine written in PyTorch and a set of twelve challenging\nmulti-robot scenarios. Additional scenarios can be implemented through a simple\nand modular interface. We demonstrate how vectorization enables parallel\nsimulation on accelerated hardware without added complexity. When comparing\nVMAS to OpenAI MPE, we show how MPE's execution time increases linearly in the\nnumber of simulations while VMAS is able to execute 30,000 parallel simulations\nin under 10s, proving more than 100x faster. Using VMAS's RLlib interface, we\nbenchmark our multi-robot scenarios using various Proximal Policy Optimization\n(PPO)-based MARL algorithms. VMAS's scenarios prove challenging in orthogonal\nways for state-of-the-art MARL algorithms. The VMAS framework is available at\nhttps://github.com/proroklab/VectorizedMultiAgentSimulator. A video of VMAS\nscenarios and experiments is available at https://youtu.be/aaDRYfiesAY.",
        "pdf_link": "https://arxiv.org/pdf/2207.03530v2.pdf"
    },
    {
        "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
        "authors": [
            "Zejiang Hou",
            "Julian Salazar",
            "George Polovets"
        ],
        "published": "2022-07-07T18:00:22Z",
        "summary": "Large pretrained language models (PLMs) are often domain- or task-adapted via\nfine-tuning or prompting. Finetuning requires modifying all of the parameters\nand having enough data to avoid overfitting while prompting requires no\ntraining and few examples but limits performance. Instead, we prepare PLMs for\ndata- and parameter-efficient adaptation by learning to learn the difference\nbetween general and adapted PLMs. This difference is expressed in terms of\nmodel weights and sublayer structure through our proposed dynamic low-rank\nreparameterization and learned architecture controller. Experiments on few-shot\ndialogue completion, low-resource abstractive summarization, and multi-domain\nlanguage modeling show improvements in adaptation time and performance over\ndirect finetuning or preparation via domain-adaptive pretraining. Ablations\nshow our task-adaptive reparameterization (TARP) and model search (TAMS)\ncomponents individually improve on other parameter-efficient transfer like\nadapters and structure-learning methods like learned sparsification.",
        "pdf_link": "https://arxiv.org/pdf/2207.03509v1.pdf"
    },
    {
        "title": "Training Transformers Together",
        "authors": [
            "Alexander Borzunov",
            "Max Ryabinin",
            "Tim Dettmers",
            "Quentin Lhoest",
            "Lucile Saulnier",
            "Michael Diskin",
            "Yacine Jernite",
            "Thomas Wolf"
        ],
        "published": "2022-07-07T17:59:16Z",
        "summary": "The infrastructure necessary for training state-of-the-art models is becoming\noverly expensive, which makes training such models affordable only to large\ncorporations and institutions. Recent work proposes several methods for\ntraining such models collaboratively, i.e., by pooling together hardware from\nmany independent parties and training a shared model over the Internet. In this\ndemonstration, we collaboratively trained a text-to-image transformer similar\nto OpenAI DALL-E. We invited the viewers to join the ongoing training run,\nshowing them instructions on how to contribute using the available hardware. We\nexplained how to address the engineering challenges associated with such a\ntraining run (slow communication, limited memory, uneven performance between\ndevices, and security concerns) and discussed how the viewers can set up\ncollaborative training runs themselves. Finally, we show that the resulting\nmodel generates images of reasonable quality on a number of prompts.",
        "pdf_link": "https://arxiv.org/pdf/2207.03481v1.pdf"
    },
    {
        "title": "AsNER -- Annotated Dataset and Baseline for Assamese Named Entity recognition",
        "authors": [
            "Dhrubajyoti Pathak",
            "Sukumar Nandi",
            "Priyankoo Sarmah"
        ],
        "published": "2022-07-07T16:45:55Z",
        "summary": "We present the AsNER, a named entity annotation dataset for low resource\nAssamese language with a baseline Assamese NER model. The dataset contains\nabout 99k tokens comprised of text from the speech of the Prime Minister of\nIndia and Assamese play. It also contains person names, location names and\naddresses. The proposed NER dataset is likely to be a significant resource for\ndeep neural based Assamese language processing. We benchmark the dataset by\ntraining NER models and evaluating using state-of-the-art architectures for\nsupervised named entity recognition (NER) such as Fasttext, BERT, XLM-R, FLAIR,\nMuRIL etc. We implement several baseline approaches with state-of-the-art\nsequence tagging Bi-LSTM-CRF architecture. The highest F1-score among all\nbaselines achieves an accuracy of 80.69% when using MuRIL as a word embedding\nmethod. The annotated dataset and the top performing model are made publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2207.03422v1.pdf"
    },
    {
        "title": "Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps",
        "authors": [
            "Alexandre Pasquiou",
            "Yair Lakretz",
            "John Hale",
            "Bertrand Thirion",
            "Christophe Pallier"
        ],
        "published": "2022-07-07T15:37:17Z",
        "summary": "Neural Language Models (NLMs) have made tremendous advances during the last\nyears, achieving impressive performance on various linguistic tasks.\nCapitalizing on this, studies in neuroscience have started to use NLMs to study\nneural activity in the human brain during language processing. However, many\nquestions remain unanswered regarding which factors determine the ability of a\nneural language model to capture brain activity (aka its 'brain score'). Here,\nwe make first steps in this direction and examine the impact of test loss,\ntraining corpus and model architecture (comparing GloVe, LSTM, GPT-2 and BERT),\non the prediction of functional Magnetic Resonance Imaging timecourses of\nparticipants listening to an audiobook. We find that (1) untrained versions of\neach model already explain significant amount of signal in the brain by\ncapturing similarity in brain responses across identical words, with the\nuntrained LSTM outperforming the transformerbased models, being less impacted\nby the effect of context; (2) that training NLP models improves brain scores in\nthe same brain regions irrespective of the model's architecture; (3) that\nPerplexity (test loss) is not a good predictor of brain score; (4) that\ntraining data have a strong influence on the outcome and, notably, that\noff-the-shelf models may lack statistical power to detect brain activations.\nOverall, we outline the impact of modeltraining choices, and suggest good\npractices for future studies aiming at explaining the human language system\nusing neural language models.",
        "pdf_link": "https://arxiv.org/pdf/2207.03380v1.pdf"
    },
    {
        "title": "Predicting Opinion Dynamics via Sociologically-Informed Neural Networks",
        "authors": [
            "Maya Okawa",
            "Tomoharu Iwata"
        ],
        "published": "2022-07-07T05:55:47Z",
        "summary": "Opinion formation and propagation are crucial phenomena in social networks\nand have been extensively studied across several disciplines. Traditionally,\ntheoretical models of opinion dynamics have been proposed to describe the\ninteractions between individuals (i.e., social interaction) and their impact on\nthe evolution of collective opinions. Although these models can incorporate\nsociological and psychological knowledge on the mechanisms of social\ninteraction, they demand extensive calibration with real data to make reliable\npredictions, requiring much time and effort. Recently, the widespread use of\nsocial media platforms provides new paradigms to learn deep learning models\nfrom a large volume of social media data. However, these methods ignore any\nscientific knowledge about the mechanism of social interaction. In this work,\nwe present the first hybrid method called Sociologically-Informed Neural\nNetwork (SINN), which integrates theoretical models and social media data by\ntransporting the concepts of physics-informed neural networks (PINNs) from\nnatural science (i.e., physics) into social science (i.e., sociology and social\npsychology). In particular, we recast theoretical models as ordinary\ndifferential equations (ODEs). Then we train a neural network that\nsimultaneously approximates the data and conforms to the ODEs that represent\nthe social scientific knowledge. In addition, we extend PINNs by integrating\nmatrix factorization and a language model to incorporate rich side information\n(e.g., user profiles) and structural knowledge (e.g., cluster structure of the\nsocial interaction network). Moreover, we develop an end-to-end training\nprocedure for SINN, which involves Gumbel-Softmax approximation to include\nstochastic mechanisms of social interaction. Extensive experiments on\nreal-world and synthetic datasets show SINN outperforms six baseline methods in\npredicting opinion dynamics.",
        "pdf_link": "https://arxiv.org/pdf/2207.03990v1.pdf"
    },
    {
        "title": "A Large Scale Search Dataset for Unbiased Learning to Rank",
        "authors": [
            "Lixin Zou",
            "Haitao Mao",
            "Xiaokai Chu",
            "Jiliang Tang",
            "Wenwen Ye",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2022-07-07T02:37:25Z",
        "summary": "The unbiased learning to rank (ULTR) problem has been greatly advanced by\nrecent deep learning techniques and well-designed debias algorithms. However,\npromising results on the existing benchmark datasets may not be extended to the\npractical scenario due to the following disadvantages observed from those\npopular benchmark datasets: (1) outdated semantic feature extraction where\nstate-of-the-art large scale pre-trained language models like BERT cannot be\nexploited due to the missing of the original text;(2) incomplete display\nfeatures for in-depth study of ULTR, e.g., missing the displayed abstract of\ndocuments for analyzing the click necessary bias; (3) lacking real-world user\nfeedback, leading to the prevalence of synthetic datasets in the empirical\nstudy. To overcome the above disadvantages, we introduce the Baidu-ULTR\ndataset. It involves randomly sampled 1.2 billion searching sessions and 7,008\nexpert annotated queries, which is orders of magnitude larger than the existing\nones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained\nlanguage model for easy usage; (2) sufficient display information such as\nposition, displayed height, and displayed abstract, enabling the comprehensive\nstudy of different biases with advanced techniques such as causal discovery and\nmeta-learning; and (3) rich user feedback on search result pages (SERPs) like\ndwelling time, allowing for user engagement optimization and promoting the\nexploration of multi-task learning in ULTR. In this paper, we present the\ndesign principle of Baidu-ULTR and the performance of benchmark ULTR algorithms\non this new data resource, favoring the exploration of ranking for long-tail\nqueries and pre-training tasks for ranking. The Baidu-ULTR dataset and\ncorresponding baseline implementation are available at\nhttps://github.com/ChuXiaokai/baidu_ultr_dataset.",
        "pdf_link": "https://arxiv.org/pdf/2207.03051v2.pdf"
    },
    {
        "title": "Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis",
        "authors": [
            "Tracy Qian",
            "Andy Xie",
            "Camille Bruckmann"
        ],
        "published": "2022-07-07T01:38:07Z",
        "summary": "The explosion in novel NLP word embedding and deep learning techniques has\ninduced significant endeavors into potential applications. One of these\ndirections is in the financial sector. Although there is a lot of work done in\nstate-of-the-art models like GPT and BERT, there are relatively few works on\nhow well these methods perform through fine-tuning after being pre-trained, as\nwell as info on how sensitive their parameters are. We investigate the\nperformance and sensitivity of transferred neural architectures from\npre-trained GPT-2 and BERT models. We test the fine-tuning performance based on\nfreezing transformer layers, batch size, and learning rate. We find the\nparameters of BERT are hypersensitive to stochasticity in fine-tuning and that\nGPT-2 is more stable in such practice. It is also clear that the earlier layers\nof GPT-2 and BERT contain essential word pattern information that should be\nmaintained.",
        "pdf_link": "https://arxiv.org/pdf/2207.03037v1.pdf"
    },
    {
        "title": "The Role of Complex NLP in Transformers for Text Ranking?",
        "authors": [
            "David Rau",
            "Jaap Kamps"
        ],
        "published": "2022-07-06T08:54:18Z",
        "summary": "Even though term-based methods such as BM25 provide strong baselines in\nranking, under certain conditions they are dominated by large pre-trained\nmasked language models (MLMs) such as BERT. To date, the source of their\neffectiveness remains unclear. Is it their ability to truly understand the\nmeaning through modeling syntactic aspects? We answer this by manipulating the\ninput order and position information in a way that destroys the natural\nsequence order of query and passage and shows that the model still achieves\ncomparable performance. Overall, our results highlight that syntactic aspects\ndo not play a critical role in the effectiveness of re-ranking with BERT. We\npoint to other mechanisms such as query-passage cross-attention and richer\nembeddings that capture word meanings based on aggregated context regardless of\nthe word order for being the main attributions for its superior performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.02522v1.pdf"
    },
    {
        "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
        "authors": [
            "Przemyslaw Joniak",
            "Akiko Aizawa"
        ],
        "published": "2022-07-06T06:20:35Z",
        "summary": "Language model debiasing has emerged as an important field of study in the\nNLP community. Numerous debiasing techniques were proposed, but bias ablation\nremains an unaddressed issue. We demonstrate a novel framework for inspecting\nbias in pre-trained transformer-based language models via movement pruning.\nGiven a model and a debiasing objective, our framework finds a subset of the\nmodel containing less bias than the original model. We implement our framework\nby pruning the model while fine-tuning it on the debiasing objective. Optimized\nare only the pruning scores - parameters coupled with the model's weights that\nact as gates. We experiment with pruning attention heads, an important building\nblock of transformers: we prune square blocks, as well as establish a new way\nof pruning the entire heads. Lastly, we demonstrate the usage of our framework\nusing gender bias, and based on our findings, we propose an improvement to an\nexisting debiasing method. Additionally, we re-discover a bias-performance\ntrade-off: the better the model performs, the more bias it contains.",
        "pdf_link": "https://arxiv.org/pdf/2207.02463v1.pdf"
    },
    {
        "title": "Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa",
        "authors": [
            "Tianyu Zhao",
            "Junping Du",
            "Zhe Xue",
            "Ang Li",
            "Zeli Guan"
        ],
        "published": "2022-07-06T03:50:31Z",
        "summary": "Text sentiment analysis, also known as opinion mining, is research on the\ncalculation of people's views, evaluations, attitude and emotions expressed by\nentities. Text sentiment analysis can be divided into text-level sentiment\nanalysis, sen-tence-level sentiment analysis and aspect-level sentiment\nanalysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the\nfield of sentiment analysis, which aims to predict the polarity of aspects. The\nresearch of pre-training neural model has significantly improved the\nperformance of many natural language processing tasks. In recent years, pre\ntraining model (PTM) has been applied in ABSA. Therefore, there has been a\nquestion, which is whether PTMs contain sufficient syntactic information for\nABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced\nBERT with disentangled attention) to solve Aspect-Based Sentiment Analysis\nproblem. DeBERTa is a kind of neural language model based on transformer, which\nuses self-supervised learning to pre-train on a large number of original text\ncorpora. Based on the Local Context Focus (LCF) mechanism, by integrating\nDeBERTa model, we purpose a multi-task learning model for aspect-based\nsentiment analysis. The experiments result on the most commonly used the laptop\nand restaurant datasets of SemEval-2014 and the ACL twitter dataset show that\nLCF mechanism with DeBERTa has significant improvement.",
        "pdf_link": "https://arxiv.org/pdf/2207.02424v2.pdf"
    },
    {
        "title": "Text Enriched Sparse Hyperbolic Graph Convolutional Networks",
        "authors": [
            "Nurendra Choudhary",
            "Nikhil Rao",
            "Karthik Subbian",
            "Chandan K. Reddy"
        ],
        "published": "2022-07-06T00:23:35Z",
        "summary": "Heterogeneous networks, which connect informative nodes containing text with\ndifferent edge types, are routinely used to store and process information in\nvarious real-world applications. Graph Neural Networks (GNNs) and their\nhyperbolic variants provide a promising approach to encode such networks in a\nlow-dimensional latent space through neighborhood aggregation and hierarchical\nfeature extraction, respectively. However, these approaches typically ignore\nmetapath structures and the available semantic information. Furthermore, these\napproaches are sensitive to the noise present in the training data. To tackle\nthese limitations, in this paper, we propose Text Enriched Sparse Hyperbolic\nGraph Convolution Network (TESH-GCN) to capture the graph's metapath structures\nusing semantic signals and further improve prediction in large heterogeneous\ngraphs. In TESH-GCN, we extract semantic node information, which successively\nacts as a connection signal to extract relevant nodes' local neighborhood and\ngraph-level metapath features from the sparse adjacency tensor in a\nreformulated hyperbolic graph convolution layer. These extracted features in\nconjunction with semantic features from the language model (for robustness) are\nused for the final downstream task. Experiments on various heterogeneous graph\ndatasets show that our model outperforms the current state-of-the-art\napproaches by a large margin on the task of link prediction. We also report a\nreduction in both the training time and model parameters compared to the\nexisting hyperbolic approaches through a reformulated hyperbolic graph\nconvolution. Furthermore, we illustrate the robustness of our model by\nexperimenting with different levels of simulated noise in both the graph\nstructure and text, and also, present a mechanism to explain TESH-GCN's\nprediction by analyzing the extracted metapaths.",
        "pdf_link": "https://arxiv.org/pdf/2207.02368v2.pdf"
    },
    {
        "title": "Machine Learning Model Sizes and the Parameter Gap",
        "authors": [
            "Pablo Villalobos",
            "Jaime Sevilla",
            "Tamay Besiroglu",
            "Lennart Heim",
            "Anson Ho",
            "Marius Hobbhahn"
        ],
        "published": "2022-07-05T20:55:38Z",
        "summary": "We study trends in model size of notable machine learning systems over time\nusing a curated dataset. From 1950 to 2018, model size in language models\nincreased steadily by seven orders of magnitude. The trend then accelerated,\nwith model size increasing by another five orders of magnitude in just 4 years\nfrom 2018 to 2022. Vision models grew at a more constant pace, totaling 7\norders of magnitude of growth between 1950 and 2022.\n  We also identify that, since 2020, there have been many language models below\n20B parameters, many models above 70B parameters, but a scarcity of models in\nthe 20-70B parameter range. We refer to that scarcity as the parameter gap.\n  We provide some stylized facts about the parameter gap and propose a few\nhypotheses to explain it. The explanations we favor are: (a) increasing model\nsize beyond 20B parameters requires adopting different parallelism techniques,\nwhich makes mid-sized models less cost-effective, (b) GPT-3 was one order of\nmagnitude larger than previous language models, and researchers afterwards\nprimarily experimented with bigger models to outperform it. While these\ndynamics likely exist, and we believe they play some role in generating the\ngap, we don't have high confidence that there are no other, more important\ndynamics at play.",
        "pdf_link": "https://arxiv.org/pdf/2207.02852v1.pdf"
    },
    {
        "title": "Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia",
        "authors": [
            "Samee Ibraheem",
            "Gaoyue Zhou",
            "John DeNero"
        ],
        "published": "2022-07-05T18:29:27Z",
        "summary": "While neural networks demonstrate a remarkable ability to model linguistic\ncontent, capturing contextual information related to a speaker's conversational\nrole is an open area of research. In this work, we analyze the effect of\nspeaker role on language use through the game of Mafia, in which participants\nare assigned either an honest or a deceptive role. In addition to building a\nframework to collect a dataset of Mafia game records, we demonstrate that there\nare differences in the language produced by players with different roles. We\nconfirm that classification models are able to rank deceptive players as more\nsuspicious than honest ones based only on their use of language. Furthermore,\nwe show that training models on two auxiliary tasks outperforms a standard\nBERT-based text classification approach. We also present methods for using our\ntrained models to identify features that distinguish between player roles,\nwhich could be used to assist players during the Mafia game.",
        "pdf_link": "https://arxiv.org/pdf/2207.02253v1.pdf"
    },
    {
        "title": "An Empirical Study of Implicit Regularization in Deep Offline RL",
        "authors": [
            "Caglar Gulcehre",
            "Srivatsan Srinivasan",
            "Jakub Sygnowski",
            "Georg Ostrovski",
            "Mehrdad Farajtabar",
            "Matt Hoffman",
            "Razvan Pascanu",
            "Arnaud Doucet"
        ],
        "published": "2022-07-05T15:07:31Z",
        "summary": "Deep neural networks are the most commonly used function approximators in\noffline reinforcement learning. Prior works have shown that neural nets trained\nwith TD-learning and gradient descent can exhibit implicit regularization that\ncan be characterized by under-parameterization of these networks. Specifically,\nthe rank of the penultimate feature layer, also called \\textit{effective rank},\nhas been observed to drastically collapse during the training. In turn, this\ncollapse has been argued to reduce the model's ability to further adapt in\nlater stages of learning, leading to the diminished final performance. Such an\nassociation between the effective rank and performance makes effective rank\ncompelling for offline RL, primarily for offline policy evaluation. In this\nwork, we conduct a careful empirical study on the relation between effective\nrank and performance on three offline RL datasets : bsuite, Atari, and DeepMind\nlab. We observe that a direct association exists only in restricted settings\nand disappears in the more extensive hyperparameter sweeps. Also, we\nempirically identify three phases of learning that explain the impact of\nimplicit regularization on the learning dynamics and found that bootstrapping\nalone is insufficient to explain the collapse of the effective rank. Further,\nwe show that several other factors could confound the relationship between\neffective rank and performance and conclude that studying this association\nunder simplistic assumptions could be highly misleading.",
        "pdf_link": "https://arxiv.org/pdf/2207.02099v2.pdf"
    },
    {
        "title": "Resource Allocation in Multicore Elastic Optical Networks: A Deep Reinforcement Learning Approach",
        "authors": [
            "Juan Pinto-R\u00edos",
            "Felipe Calder\u00f3n",
            "Ariel Leiva",
            "Gabriel Hermosilla",
            "Alejandra Beghelli",
            "Danilo B\u00f3rquez-Paredes",
            "Astrid Lozada",
            "Nicol\u00e1s Jara",
            "Ricardo Olivares",
            "Gabriel Saavedra"
        ],
        "published": "2022-07-05T14:24:21Z",
        "summary": "A deep reinforcement learning approach is applied, for the first time, to\nsolve the routing, modulation, spectrum and core allocation (RMSCA) problem in\ndynamic multicore fiber elastic optical networks (MCF-EONs). To do so, a new\nenvironment - compatible with OpenAI's Gym - was designed and implemented to\nemulate the operation of MCF-EONs. The new environment processes the agent\nactions (selection of route, core and spectrum slot) by considering the network\nstate and physical-layer-related aspects. The latter includes the available\nmodulation formats and their reach and the inter-core crosstalk (XT), an\nMCF-related impairment. If the resulting quality of the signal is acceptable,\nthe environment allocates the resources selected by the agent. After processing\nthe agent's action, the environment is configured to give the agent a numerical\nreward and information about the new network state. The blocking performance of\nfour different agents was compared through simulation to 3 baseline heuristics\nused in MCF-EONs. Results obtained for the NSFNet and COST239 network\ntopologies show that the best-performing agent achieves, on average, up to a\nfour-times decrease in blocking probability concerning the best-performing\nbaseline heuristic methods.",
        "pdf_link": "https://arxiv.org/pdf/2207.02074v1.pdf"
    },
    {
        "title": "MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering",
        "authors": [
            "Zhucheng Tu",
            "Sarguna Janani Padmanabhan"
        ],
        "published": "2022-07-05T10:27:17Z",
        "summary": "We describe our two-stage system for the Multilingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using a multilingual language model with entity representations in\npretraining, sparse retrieval signals to help dense retrieval, and\nFusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA\nand 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we\nobtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of\n31.61. We improve over the official baseline by over 4 F1 points on both the\ndevelopment and test sets.",
        "pdf_link": "https://arxiv.org/pdf/2207.01940v3.pdf"
    },
    {
        "title": "Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic",
        "authors": [
            "V\u00e9steinn Sn\u00e6bjarnarson",
            "Hafsteinn Einarsson"
        ],
        "published": "2022-07-05T09:52:34Z",
        "summary": "It can be challenging to build effective open question answering (open QA)\nsystems for languages other than English, mainly due to a lack of labeled data\nfor training. We present a data efficient method to bootstrap such a system for\nlanguages other than English. Our approach requires only limited QA resources\nin the given language, along with machine-translated data, and at least a\nbilingual language model. To evaluate our approach, we build such a system for\nthe Icelandic language and evaluate performance over trivia style datasets. The\ncorpora used for training are English in origin but machine translated into\nIcelandic. We train a bilingual Icelandic/English language model to embed\nEnglish context and Icelandic questions following methodology introduced with\nDensePhrases (Lee et al., 2021). The resulting system is an open domain\ncross-lingual QA system between Icelandic and English. Finally, the system is\nadapted for Icelandic only open QA, demonstrating how it is possible to\nefficiently create an open QA system with limited access to curated datasets in\nthe language of interest.",
        "pdf_link": "https://arxiv.org/pdf/2207.01918v1.pdf"
    },
    {
        "title": "Betti numbers of attention graphs is all you really need",
        "authors": [
            "Laida Kushnareva",
            "Dmitri Piontkovski",
            "Irina Piontkovskaya"
        ],
        "published": "2022-07-05T09:10:47Z",
        "summary": "We apply methods of topological analysis to the attention graphs, calculated\non the attention heads of the BERT model ( arXiv:1810.04805v2 ). Our research\nshows that the classifier built upon basic persistent topological features\n(namely, Betti numbers) of the trained neural network can achieve\nclassification results on par with the conventional classification method. We\nshow the relevance of such topological text representation on three text\nclassification benchmarks. For the best of our knowledge, it is the first\nattempt to analyze the topology of an attention-based neural network, widely\nused for Natural Language Processing.",
        "pdf_link": "https://arxiv.org/pdf/2207.01903v1.pdf"
    },
    {
        "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
        "authors": [
            "Valentin Pelloin",
            "Franck Dary",
            "Nicolas Herve",
            "Benoit Favre",
            "Nathalie Camelin",
            "Antoine Laurent",
            "Laurent Besacier"
        ],
        "published": "2022-07-05T08:47:51Z",
        "summary": "We aim at improving spoken language modeling (LM) using very large amount of\nautomatically transcribed speech. We leverage the INA (French National\nAudiovisual Institute) collection and obtain 19GB of text after applying ASR on\n350,000 hours of diverse TV shows. From this, spoken language models are\ntrained either by fine-tuning an existing LM (FlauBERT) or through training a\nLM from scratch. New models (FlauBERT-Oral) are shared with the community and\nevaluated for 3 downstream tasks: spoken language understanding, classification\nof TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can\nbe beneficial compared to its initial FlauBERT version demonstrating that,\ndespite its inherent noisy nature, ASR-generated text can be used to build\nspoken language models.",
        "pdf_link": "https://arxiv.org/pdf/2207.01893v1.pdf"
    },
    {
        "title": "Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer",
        "authors": [
            "Sunan He",
            "Taian Guo",
            "Tao Dai",
            "Ruizhi Qiao",
            "Bo Ren",
            "Shu-Tao Xia"
        ],
        "published": "2022-07-05T08:32:18Z",
        "summary": "Real-world recognition system often encounters the challenge of unseen\nlabels. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL)\nfocuses on transferring knowledge by a pre-trained textual label embedding\n(e.g., GloVe). However, such methods only exploit single-modal knowledge from a\nlanguage model, while ignoring the rich semantic information inherent in\nimage-text pairs. Instead, recently developed open-vocabulary (OV) based\nmethods succeed in exploiting such information of image-text pairs in object\ndetection, and achieve impressive performance. Inspired by the success of\nOV-based methods, we propose a novel open-vocabulary framework, named\nmulti-modal knowledge transfer (MKT), for multi-label classification.\nSpecifically, our method exploits multi-modal knowledge of image-text pairs\nbased on a vision and language pre-training (VLP) model. To facilitate\ntransferring the image-text matching ability of VLP model, knowledge\ndistillation is employed to guarantee the consistency of image and label\nembeddings, along with prompt tuning to further update the label embeddings. To\nfurther enable the recognition of multiple objects, a simple but effective\ntwo-stream module is developed to capture both local and global features.\nExtensive experimental results show that our method significantly outperforms\nstate-of-the-art methods on public benchmark datasets. The source code is\navailable at https://github.com/sunanhe/MKT.",
        "pdf_link": "https://arxiv.org/pdf/2207.01887v2.pdf"
    },
    {
        "title": "BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",
        "authors": [
            "Brooke Stephenson",
            "Laurent Besacier",
            "Laurent Girin",
            "Thomas Hueber"
        ],
        "published": "2022-07-04T20:43:41Z",
        "summary": "Several recent studies have tested the use of transformer language model\nrepresentations to infer prosodic features for text-to-speech synthesis (TTS).\nWhile these studies have explored prosody in general, in this work, we look\nspecifically at the prediction of contrastive focus on personal pronouns. This\nis a particularly challenging task as it often requires semantic, discursive\nand/or pragmatic knowledge to predict correctly. We collect a corpus of\nutterances containing contrastive focus and we evaluate the accuracy of a BERT\nmodel, finetuned to predict quantized acoustic prominence features, on these\nsamples. We also investigate how past utterances can provide relevant\ninformation for this prediction. Furthermore, we evaluate the controllability\nof pronoun prominence in a TTS model conditioned on acoustic prominence\nfeatures.",
        "pdf_link": "https://arxiv.org/pdf/2207.01718v1.pdf"
    },
    {
        "title": "A Cascade Model for Argument Mining in Japanese Political Discussions: the QA Lab-PoliInfo-3 Case Study",
        "authors": [
            "Ramon Ruiz-Dolz"
        ],
        "published": "2022-07-04T18:49:18Z",
        "summary": "The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of\na combination of classification and information retrieval sub-tasks. For the\nargument classification (AC), the team achieved its best performing results\nwith a five-class BERT-based cascade model complemented with some handcrafted\nrules. The rules were used to determine if the expression was monetary or not.\nThen, each monetary expression was classified as a premise or as a conclusion\nin the first level of the cascade model. Finally, each premise was classified\ninto the three premise classes, and each conclusion into the two conclusion\nclasses. For the information retrieval (i.e., relation ID detection or RID),\nour best results were achieved by a combination of a BERT-based binary\nclassifier, and the cosine similarity of pairs consisting of the monetary\nexpression and budget dense embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2207.01672v1.pdf"
    },
    {
        "title": "Using contextual sentence analysis models to recognize ESG concepts",
        "authors": [
            "Elvys Linhares Pontes",
            "Mohamed Benjannet",
            "Jose G. Moreno",
            "Antoine Doucet"
        ],
        "published": "2022-07-04T13:33:21Z",
        "summary": "This paper summarizes the joint participation of the Trading Central Labs and\nthe L3i laboratory of the University of La Rochelle on both sub-tasks of the\nShared Task FinSim-4 evaluation campaign. The first sub-task aims to enrich the\n'Fortia ESG taxonomy' with new lexicon entries while the second one aims to\nclassify sentences to either 'sustainable' or 'unsustainable' with respect to\nESG (Environment, Social and Governance) related factors. For the first\nsub-task, we proposed a model based on pre-trained Sentence-BERT models to\nproject sentences and concepts in a common space in order to better represent\nESG concepts. The official task results show that our system yields a\nsignificant performance improvement compared to the baseline and outperforms\nall other submissions on the first sub-task. For the second sub-task, we\ncombine the RoBERTa model with a feed-forward multi-layer perceptron in order\nto extract the context of sentences and classify them. Our model achieved high\naccuracy scores (over 92%) and was ranked among the top 5 systems.",
        "pdf_link": "https://arxiv.org/pdf/2207.01402v1.pdf"
    },
    {
        "title": "Egocentric Video-Language Pretraining @ Ego4D Challenge 2022",
        "authors": [
            "Kevin Qinghong Lin",
            "Alex Jinpeng Wang",
            "Mattia Soldan",
            "Michael Wray",
            "Rui Yan",
            "Eric Zhongcong Xu",
            "Difei Gao",
            "Rongcheng Tu",
            "Wenzhe Zhao",
            "Weijie Kong",
            "Chengfei Cai",
            "Hongfa Wang",
            "Dima Damen",
            "Bernard Ghanem",
            "Wei Liu",
            "Mike Zheng Shou"
        ],
        "published": "2022-07-04T12:47:16Z",
        "summary": "In this report, we propose a video-language pretraining (VLP) based solution\n\\cite{kevin2022egovlp} for four Ego4D challenge tasks, including Natural\nLanguage Query (NLQ), Moment Query (MQ), Object State Change Classification\n(OSCC), and PNR Localization (PNR). Especially, we exploit the recently\nreleased Ego4D dataset \\cite{grauman2021ego4d} to pioneer Egocentric VLP from\npretraining dataset, pretraining objective, and development set. Based on the\nabove three designs, we develop a pretrained video-language model that is able\nto transfer its egocentric video-text representation or video-only\nrepresentation to several video downstream tasks. Our Egocentric VLP achieves\n10.46R@1&IoU @0.3 on NLQ, 10.33 mAP on MQ, 74% Acc on OSCC, 0.67 sec error on\nPNR. The code is available at https://github.com/showlab/EgoVLP.",
        "pdf_link": "https://arxiv.org/pdf/2207.01622v2.pdf"
    },
    {
        "title": "Egocentric Video-Language Pretraining @ EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022",
        "authors": [
            "Kevin Qinghong Lin",
            "Alex Jinpeng Wang",
            "Rui Yan",
            "Eric Zhongcong Xu",
            "Rongcheng Tu",
            "Yanru Zhu",
            "Wenzhe Zhao",
            "Weijie Kong",
            "Chengfei Cai",
            "Hongfa Wang",
            "Wei Liu",
            "Mike Zheng Shou"
        ],
        "published": "2022-07-04T11:32:48Z",
        "summary": "In this report, we propose a video-language pretraining (VLP) based solution\n\\cite{kevin2022egovlp} for the EPIC-KITCHENS-100 Multi-Instance Retrieval (MIR)\nchallenge. Especially, we exploit the recently released Ego4D dataset\n\\cite{grauman2021ego4d} to pioneer Egocentric VLP from pretraining dataset,\npretraining objective, and development set. Based on the above three designs,\nwe develop a pretrained video-language model that is able to transfer its\negocentric video-text representation to MIR benchmark. Furthermore, we devise\nan adaptive multi-instance max-margin loss to effectively fine-tune the model\nand equip the dual-softmax technique for reliable inference. Our best single\nmodel obtains strong performance on the challenge test set with 47.39% mAP and\n61.44% nDCG. The code is available at https://github.com/showlab/EgoVLP.",
        "pdf_link": "https://arxiv.org/pdf/2207.01334v2.pdf"
    },
    {
        "title": "Revisiting Classifier: Transferring Vision-Language Models for Video Recognition",
        "authors": [
            "Wenhao Wu",
            "Zhun Sun",
            "Wanli Ouyang"
        ],
        "published": "2022-07-04T10:00:47Z",
        "summary": "Transferring knowledge from task-agnostic pre-trained deep models for\ndownstream tasks is an important topic in computer vision research. Along with\nthe growth of computational capacity, we now have open-source vision-language\npre-trained models in large scales of the model architecture and amount of\ndata. In this study, we focus on transferring knowledge for video\nclassification tasks. Conventional methods randomly initialize the linear\nclassifier head for vision classification, but they leave the usage of the text\nencoder for downstream visual recognition tasks undiscovered. In this paper, we\nrevise the role of the linear classifier and replace the classifier with the\ndifferent knowledge from pre-trained model. We utilize the well-pretrained\nlanguage model to generate good semantic target for efficient transferring\nlearning. The empirical study shows that our method improves both the\nperformance and the training speed of video classification, with a negligible\nchange in the model. Our simple yet effective tuning paradigm achieves\nstate-of-the-art performance and efficient training on various video\nrecognition scenarios, i.e., zero-shot, few-shot, general recognition. In\nparticular, our paradigm achieves the state-of-the-art accuracy of 87.8% on\nKinetics-400, and also surpasses previous methods by 20~50% absolute top-1\naccuracy under zero-shot, few-shot settings on five popular video datasets.\nCode and models can be found at https://github.com/whwu95/Text4Vis .",
        "pdf_link": "https://arxiv.org/pdf/2207.01297v4.pdf"
    },
    {
        "title": "Stabilizing Off-Policy Deep Reinforcement Learning from Pixels",
        "authors": [
            "Edoardo Cetin",
            "Philip J. Ball",
            "Steve Roberts",
            "Oya Celiktutan"
        ],
        "published": "2022-07-03T08:52:40Z",
        "summary": "Off-policy reinforcement learning (RL) from pixel observations is notoriously\nunstable. As a result, many successful algorithms must combine different\ndomain-specific practices and auxiliary losses to learn meaningful behaviors in\ncomplex environments. In this work, we provide novel analysis demonstrating\nthat these instabilities arise from performing temporal-difference learning\nwith a convolutional encoder and low-magnitude rewards. We show that this new\nvisual deadly triad causes unstable training and premature convergence to\ndegenerate solutions, a phenomenon we name catastrophic self-overfitting. Based\non our analysis, we propose A-LIX, a method providing adaptive regularization\nto the encoder's gradients that explicitly prevents the occurrence of\ncatastrophic self-overfitting using a dual objective. By applying A-LIX, we\nsignificantly outperform the prior state-of-the-art on the DeepMind Control and\nAtari 100k benchmarks without any data augmentation or auxiliary losses.",
        "pdf_link": "https://arxiv.org/pdf/2207.00986v1.pdf"
    },
    {
        "title": "Generating Repetitions with Appropriate Repeated Words",
        "authors": [
            "Toshiki Kawamoto",
            "Hidetaka Kamigaito",
            "Kotaro Funakoshi",
            "Manabu Okumura"
        ],
        "published": "2022-07-03T01:21:49Z",
        "summary": "A repetition is a response that repeats words in the previous speaker's\nutterance in a dialogue. Repetitions are essential in communication to build\ntrust with others, as investigated in linguistic studies. In this work, we\nfocus on repetition generation. To the best of our knowledge, this is the first\nneural approach to address repetition generation. We propose Weighted Label\nSmoothing, a smoothing method for explicitly learning which words to repeat\nduring fine-tuning, and a repetition scoring method that can output more\nappropriate repetitions during decoding. We conducted automatic and human\nevaluations involving applying these methods to the pre-trained language model\nT5 for generating repetitions. The experimental results indicate that our\nmethods outperformed baselines in both evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2207.00929v1.pdf"
    },
    {
        "title": "A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking",
        "authors": [
            "Eleftherios Kapelonis",
            "Efthymios Georgiou",
            "Alexandros Potamianos"
        ],
        "published": "2022-07-02T13:27:59Z",
        "summary": "Task-oriented dialogue systems often employ a Dialogue State Tracker (DST) to\nsuccessfully complete conversations. Recent state-of-the-art DST\nimplementations rely on schemata of diverse services to improve model\nrobustness and handle zero-shot generalization to new domains [1], however such\nmethods [2, 3] typically require multiple large scale transformer models and\nlong input sequences to perform well. We propose a single multi-task BERT-based\nmodel that jointly solves the three DST tasks of intent prediction, requested\nslot prediction and slot filling. Moreover, we propose an efficient and\nparsimonious encoding of the dialogue history and service schemata that is\nshown to further improve performance. Evaluation on the SGD dataset shows that\nour approach outperforms the baseline SGP-DST by a large margin and performs\nwell compared to the state-of-the-art, while being significantly more\ncomputationally efficient. Extensive ablation studies are performed to examine\nthe contributing factors to the success of our model.",
        "pdf_link": "https://arxiv.org/pdf/2207.00828v1.pdf"
    },
    {
        "title": "FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales",
        "authors": [
            "Aaron Chan",
            "Shaoliang Nie",
            "Liang Tan",
            "Xiaochang Peng",
            "Hamed Firooz",
            "Maziar Sanjabi",
            "Xiang Ren"
        ],
        "published": "2022-07-02T09:25:29Z",
        "summary": "Following how humans communicate, free-text rationales aim to use natural\nlanguage to explain neural language model (LM) behavior. However, free-text\nrationales' unconstrained nature makes them prone to hallucination, so it is\nimportant to have metrics for free-text rationale quality. Existing free-text\nrationale metrics measure how consistent the rationale is with the LM's\npredicted label, but there is no protocol for assessing such metrics'\nreliability. Thus, we propose FRAME, a framework for evaluating rationale-label\nconsistency (RLC) metrics for free-text rationales. FRAME is based on three\naxioms: (1) good metrics should yield highest scores for reference rationales,\nwhich maximize RLC by construction; (2) good metrics should be appropriately\nsensitive to semantic perturbation of rationales; and (3) good metrics should\nbe robust to variation in the LM's task performance. Across three text\nclassification datasets, we show that existing RLC metrics cannot satisfy all\nthree FRAME axioms, since they are implemented via model pretraining which\nmuddles the metric's signal. Then, we introduce a non-pretraining RLC metric\nthat greatly outperforms baselines on (1) and (3), while performing\ncompetitively on (2). Finally, we discuss the limitations of using RLC to\nevaluate free-text rationales.",
        "pdf_link": "https://arxiv.org/pdf/2207.00779v2.pdf"
    },
    {
        "title": "GUIM -- General User and Item Embedding with Mixture of Representation in E-commerce",
        "authors": [
            "Chao Yang",
            "Ru He",
            "Fangquan Lin",
            "Suoyuan Song",
            "Jingqiao Zhang",
            "Cheng Yang"
        ],
        "published": "2022-07-02T06:27:54Z",
        "summary": "Our goal is to build general representation (embedding) for each user and\neach product item across Alibaba's businesses, including Taobao and Tmall which\nare among the world's biggest e-commerce websites. The representation of users\nand items has been playing a critical role in various downstream applications,\nincluding recommendation system, search, marketing, demand forecasting and so\non. Inspired from the BERT model in natural language processing (NLP) domain,\nwe propose a GUIM (General User Item embedding with Mixture of representation)\nmodel to achieve the goal with massive, structured, multi-modal data including\nthe interactions among hundreds of millions of users and items. We utilize\nmixture of representation (MoR) as a novel representation form to model the\ndiverse interests of each user. In addition, we use the InfoNCE from\ncontrastive learning to avoid intractable computational costs due to the\nnumerous size of item (token) vocabulary. Finally, we propose a set of\nrepresentative downstream tasks to serve as a standard benchmark to evaluate\nthe quality of the learned user and/or item embeddings, analogous to the GLUE\nbenchmark in NLP domain. Our experimental results in these downstream tasks\nclearly show the comparative value of embeddings learned from our GUIM model.",
        "pdf_link": "https://arxiv.org/pdf/2207.00750v1.pdf"
    },
    {
        "title": "UserLibri: A Dataset for ASR Personalization Using Only Text",
        "authors": [
            "Theresa Breiner",
            "Swaroop Ramaswamy",
            "Ehsan Variani",
            "Shefali Garg",
            "Rajiv Mathews",
            "Khe Chai Sim",
            "Kilol Gupta",
            "Mingqing Chen",
            "Lara McConnaughey"
        ],
        "published": "2022-07-02T01:03:01Z",
        "summary": "Personalization of speech models on mobile devices (on-device\npersonalization) is an active area of research, but more often than not, mobile\ndevices have more text-only data than paired audio-text data. We explore\ntraining a personalized language model on text-only data, used during inference\nto improve speech recognition performance for that user. We experiment on a\nuser-clustered LibriSpeech corpus, supplemented with personalized text-only\ndata for each user from Project Gutenberg. We release this User-Specific\nLibriSpeech (UserLibri) dataset to aid future personalization research.\nLibriSpeech audio-transcript pairs are grouped into 55 users from the\ntest-clean dataset and 52 users from test-other. We are able to lower the\naverage word error rate per user across both sets in streaming and nonstreaming\nmodels, including an improvement of 2.5 for the harder set of test-other users\nwhen streaming.",
        "pdf_link": "https://arxiv.org/pdf/2207.00706v1.pdf"
    },
    {
        "title": "A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese",
        "authors": [
            "Song Zhang",
            "Ken Zheng",
            "Xiaoxu Zhu",
            "Baoxiang Li"
        ],
        "published": "2022-07-01T09:16:29Z",
        "summary": "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese\nMandarin text-to-speech (TTS) system, and the core of G2P conversion is to\nsolve the problem of polyphone disambiguation, which is to pick up the correct\npronunciation for several candidates for a Chinese polyphonic character. In\nthis paper, we propose a Chinese polyphone BERT model to predict the\npronunciations of Chinese polyphonic characters. Firstly, we create 741 new\nChinese monophonic characters from 354 source Chinese polyphonic characters by\npronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained\nChinese BERT with 741 new Chinese monophonic characters and adding a\ncorresponding embedding layer for new tokens, which is initialized by the\nembeddings of source Chinese polyphonic characters. In this way, we can turn\nthe polyphone disambiguation task into a pre-training task of the Chinese\npolyphone BERT. Experimental results demonstrate the effectiveness of the\nproposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%)\nimprovement of average accuracy compared with the BERT-based classifier model,\nwhich is the prior state-of-the-art in polyphone disambiguation.",
        "pdf_link": "https://arxiv.org/pdf/2207.12089v1.pdf"
    },
    {
        "title": "Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset",
        "authors": [
            "Peter Henderson",
            "Mark S. Krass",
            "Lucia Zheng",
            "Neel Guha",
            "Christopher D. Manning",
            "Dan Jurafsky",
            "Daniel E. Ho"
        ],
        "published": "2022-07-01T06:25:15Z",
        "summary": "One concern with the rise of large language models lies with their potential\nfor significant harm, particularly from pretraining on biased, obscene,\ncopyrighted, and private information. Emerging ethical approaches have\nattempted to filter pretraining material, but such approaches have been ad hoc\nand failed to take context into account. We offer an approach to filtering\ngrounded in law, which has directly addressed the tradeoffs in filtering\nmaterial. First, we gather and make available the Pile of Law, a 256GB (and\ngrowing) dataset of open-source English-language legal and administrative data,\ncovering court opinions, contracts, administrative rules, and legislative\nrecords. Pretraining on the Pile of Law may help with legal tasks that have the\npromise to improve access to justice. Second, we distill the legal norms that\ngovernments have developed to constrain the inclusion of toxic or private\ncontent into actionable lessons for researchers and discuss how our dataset\nreflects these norms. Third, we show how the Pile of Law offers researchers the\nopportunity to learn such filtering rules directly from the data, providing an\nexciting new research direction in model-based processing.",
        "pdf_link": "https://arxiv.org/pdf/2207.00220v2.pdf"
    },
    {
        "title": "When Does Differentially Private Learning Not Suffer in High Dimensions?",
        "authors": [
            "Xuechen Li",
            "Daogao Liu",
            "Tatsunori Hashimoto",
            "Huseyin A. Inan",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Abhradeep Guha Thakurta"
        ],
        "published": "2022-07-01T02:36:51Z",
        "summary": "Large pretrained models can be privately fine-tuned to achieve performance\napproaching that of non-private models. A common theme in these results is the\nsurprising observation that high-dimensional models can achieve favorable\nprivacy-utility trade-offs. This seemingly contradicts known results on the\nmodel-size dependence of differentially private convex learning and raises the\nfollowing research question: When does the performance of differentially\nprivate learning not degrade with increasing model size? We identify that the\nmagnitudes of gradients projected onto subspaces is a key factor that\ndetermines performance. To precisely characterize this for private convex\nlearning, we introduce a condition on the objective that we term\n\\emph{restricted Lipschitz continuity} and derive improved bounds for the\nexcess empirical and population risks that are dimension-independent under\nadditional conditions. We empirically show that in private fine-tuning of large\nlanguage models, gradients obtained during fine-tuning are mostly controlled by\na few principal components. This behavior is similar to conditions under which\nwe obtain dimension-independent bounds in convex settings. Our theoretical and\nempirical results together provide a possible explanation for recent successes\nin large-scale private fine-tuning. Code to reproduce our results can be found\nat\n\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.",
        "pdf_link": "https://arxiv.org/pdf/2207.00160v4.pdf"
    }
]