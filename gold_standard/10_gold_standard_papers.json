[
    {
        "title": "Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation",
        "authors": [
            "Fei Huang",
            "Pei Ke",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Non-AutoRegressive (NAR) text generation models have drawn much attention because of their significantly faster decoding speed and good generation quality in machine translation. However, in a wider range of text generation tasks, existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models. In this paper, we propose Pre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task to promote prediction consistency in NAR generation. Experiments on five text generation tasks show that our PreDAT remarkably outperforms existing pre-trained NAR models (+4.2 score on average) and even achieves better results than pre-trained autoregressive baselines in n-gram-based metrics, along with 17 times speedup in throughput. Further analysis shows that PreDAT benefits from the unbiased prediction order that alleviates the error accumulation problem in autoregressive generation, which provides new insights into the advantages of NAR generation.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.53.pdf"
    },
    {
        "title": "Time-and-Space-Efficient Weighted Deduction",
        "authors": [
            "Jason Eisner"
        ],
        "published": "2023",
        "summary": "Many NLP algorithms have been described in terms of deduction systems. Unweighted deduction allows a generic forward-chaining execution strategy. For weighted deduction, however, efficient execution should propagate the weight of each item only after it has converged. This means visiting the items in topologically sorted order (as in dynamic programming). Toposorting is fast on a materialized graph; unfortunately, materializing the graph would take extra space. Is there a generic weighted deduction strategy which, for every acyclic deduction system and every input, uses only a constant factor more time and space than generic unweighted deduction? After reviewing past strategies, we answer this question in the affirmative by combining ideas of Goodman (1999) and Kahn (1962). We also give an extension to cyclic deduction systems, based on Tarjan (1972).",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.54.pdf"
    },
    {
        "title": "Conditional Generation with a Question-Answering Blueprint",
        "authors": [
            "Shashi Narayan",
            "Joshua Maynez",
            "Reinald Kim Amplayo",
            "Kuzman Ganchev",
            "Annie Louis",
            "Fantine Huot",
            "Anders Sandholm",
            "Dipanjan Das",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded. We propose a new conceptualization of text plans as a sequence of question-answer (QA) pairs and enhance existing datasets (e.g., for summarization) with a QA blueprint operating as a proxy for content selection (i.e., what to say) and planning (i.e., in what order). We obtain blueprints automatically by exploiting state-of-the-art question generation technology and convert input-output pairs into input-blueprint-output tuples. We develop Transformer-based models, each varying in how they incorporate the blueprint in the generated output (e.g., as a global plan or iteratively). Evaluation across metrics and datasets demonstrates that blueprint models are more factual than alternatives which do not resort to planning and allow tighter control of the generation output.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.55.pdf"
    },
    {
        "title": "Collective Human Opinions in Semantic Textual Similarity",
        "authors": [
            "Yuxia Wang",
            "Shimin Tao",
            "Ning Xie",
            "Hao Yang",
            "Timothy Baldwin",
            "Karin Verspoor"
        ],
        "published": "2023",
        "summary": "Despite the subjective nature of semantic textual similarity (STS) and pervasive disagreements in STS annotation, existing benchmarks have used averaged human ratings as gold standard. Averaging masks the true distribution of human opinions on examples of low agreement, and prevents models from capturing the semantic vagueness that the individual ratings represent. In this work, we introduce USTS, the first Uncertainty-aware STS dataset with ∼15,000 Chinese sentence pairs and 150,000 labels, to study collective human opinions in STS. Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgments adequately. We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.56.pdf"
    },
    {
        "title": "Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design",
        "authors": [
            "Valentina Pyatkin",
            "Frances Yung",
            "Merel C. J. Scholman",
            "Reut Tsarfaty",
            "Ido Dagan",
            "Vera Demberg"
        ],
        "published": "2023",
        "summary": "Disagreement in natural language annotation has mostly been studied from a perspective of biases introduced by the annotators and the annotation frameworks. Here, we propose to analyze another source of bias—task design bias, which has a particularly strong impact on crowdsourced linguistic annotations where natural language is used to elicit the interpretation of lay annotators. For this purpose we look at implicit discourse relation annotation, a task that has repeatedly been shown to be difficult due to the relations’ ambiguity. We compare the annotations of 1,200 discourse relations obtained using two distinct annotation tasks and quantify the biases of both methods across four different domains. Both methods are natural language annotation tasks designed for crowdsourcing. We show that the task design can push annotators towards certain relations and that some discourse relation senses can be better elicited with one or the other annotation approach. We also conclude that this type of bias should be taken into account when training and testing models.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.57.pdf"
    },
    {
        "title": "Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off",
        "authors": [
            "Yuchen Lian",
            "Arianna Bisazza",
            "Tessa Verhoef"
        ],
        "published": "2023",
        "summary": "Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account, focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step towards the investigation of language universals with neural learners.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.58.pdf"
    },
    {
        "title": "A Cross-Linguistic Pressure for Uniform Information Density in Word Order",
        "authors": [
            "Thomas Hikaru Clark",
            "Clara Meister",
            "Tiago Pimentel",
            "Michael Hahn",
            "Ryan Cotterell",
            "Richard Futrell",
            "Roger Levy"
        ],
        "published": "2023",
        "summary": "While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.59.pdf"
    },
    {
        "title": "Cross-functional Analysis of Generalization in Behavioral Learning",
        "authors": [
            "Pedro Henrique Luz de Araujo",
            "Benjamin Roth"
        ],
        "published": "2023",
        "summary": "In behavioral testing, system functionalities underrepresented in the standard evaluation setting (with a held-out test set) are validated through controlled input-output pairs. Optimizing performance on the behavioral tests during training (behavioral learning) would improve coverage of phenomena not sufficiently represented in the i.i.d. data and could lead to seemingly more robust models. However, there is the risk that the model narrowly captures spurious correlations from the behavioral test suite, leading to overestimation and misrepresentation of model performance—one of the original pitfalls of traditional evaluation. In this work, we introduce BeLUGA, an analysis method for evaluating behavioral learning considering generalization across dimensions of different granularity levels. We optimize behavior-specific loss functions and evaluate models on several partitions of the behavioral test suite controlled to leave out specific phenomena. An aggregate score measures generalization to unseen functionalities (or overfitting). We use BeLUGA to examine three representative NLP tasks (sentiment analysis, paraphrase identification, and reading comprehension) and compare the impact of a diverse set of regularization and domain generalization methods on generalization performance.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.60.pdf"
    },
    {
        "title": "Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions",
        "authors": [
            "Zhihan Zhang",
            "Wenhao Yu",
            "Zheng Ning",
            "Mingxuan Ju",
            "Meng Jiang"
        ],
        "published": "2023",
        "summary": "Contrast consistency, the ability of a model to make consistently correct predictions in the presence of perturbations, is an essential aspect in NLP. While studied in tasks such as sentiment analysis and reading comprehension, it remains unexplored in open-domain question answering (OpenQA) due to the difficulty of collecting perturbed questions that satisfy factuality requirements. In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models. Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets. To address this issue, we introduce a simple and effective query-side contrastive loss with the aid of data augmentation to improve DPR training. Our experiments on the contrast sets demonstrate that DPR’s contrast consistency is improved without sacrificing its accuracy on the standard test sets.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.61.pdf"
    },
    {
        "title": "Compositional Zero-Shot Domain Transfer with Text-to-Text Models",
        "authors": [
            "Fangyu Liu",
            "Qianchu Liu",
            "Shruthi Bannur",
            "Fernando Pérez-García",
            "Naoto Usuyama",
            "Sheng Zhang",
            "Tristan Naumann",
            "Aditya Nori",
            "Hoifung Poon",
            "Javier Alvarez-Valle",
            "Ozan Oktay",
            "Stephanie L. Hyland"
        ],
        "published": "2023",
        "summary": "Label scarcity is a bottleneck for improving task performance in specialized domains. We propose a novel compositional transfer learning framework (DoT51) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: We simultaneously train natural language generation (NLG) for in-domain label-to-data generation, which enables data augmentation for self-finetuning and natural language understanding (NLU) for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on natural language inference, text summarization, and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current state-of-the-art in zero-shot transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.62.pdf"
    },
    {
        "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
        "authors": [
            "Xinyu Zhang",
            "Nandan Thakur",
            "Odunayo Ogundepo",
            "Ehsan Kamalloo",
            "David Alfonso-Hermelo",
            "Xiaoguang Li",
            "Qun Liu",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023",
        "summary": "MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.63.pdf"
    },
    {
        "title": "DMDD: A Large-Scale Dataset for Dataset Mentions Detection",
        "authors": [
            "Huitong Pan",
            "Qi Zhang",
            "Eduard Dragut",
            "Cornelia Caragea",
            "Longin Jan Latecki"
        ],
        "published": "2023",
        "summary": "The recognition of dataset names is a critical task for automatic information extraction in scientific literature, enabling researchers to understand and identify research opportunities. However, existing corpora for dataset mention detection are limited in size and naming diversity. In this paper, we introduce the Dataset Mentions Detection Dataset (DMDD), the largest publicly available corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219 scientific articles with over 449,000 dataset mentions weakly annotated in the format of in-text spans, and an evaluation set, which comprises 450 scientific articles manually annotated for evaluation purposes. We use DMDD to establish baseline performance for dataset mention detection and linking. By analyzing the performance of various models on DMDD, we are able to identify open problems in dataset mention detection. We invite the community to use our dataset as a challenge to develop novel dataset mention detection models.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.64.pdf"
    },
    {
        "title": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification",
        "authors": [
            "Inigo Jauregi Unanue",
            "Gholamreza Haffari",
            "Massimo Piccardi"
        ],
        "published": "2023",
        "summary": "Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/ few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic “translate-and-test” pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates “soft” translations to permit end-to-end backpropagation during fine-tuning of the pipeline. Extensive experiments have been carried out over three cross-lingual text classification datasets (XNLI, MLDoc, and MultiEURLEX), with the results showing that the proposed approach has significantly improved performance over a competitive baseline.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.65.pdf"
    },
    {
        "title": "Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",
        "authors": [
            "Jordan Meadows",
            "André Freitas"
        ],
        "published": "2023",
        "summary": "Automating discovery in mathematics and science will require sophisticated methods of information extraction and abstract reasoning, including models that can convincingly process relationships between mathematical elements and natural language, to produce problem solutions of real-world value. We analyze mathematical language processing methods across five strategic sub-areas (identifier-definition extraction, formula retrieval, natural language premise selection, math word problem solving, and informal theorem proving) from recent years, highlighting prevailing methodologies, existing limitations, overarching trends, and promising avenues for future research.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.66.pdf"
    },
    {
        "title": "Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering",
        "authors": [
            "William Dyer",
            "Charles Torres",
            "Gregory Scontras",
            "Richard Futrell"
        ],
        "published": "2023",
        "summary": "The literature on adjective ordering abounds with proposals meant to account for why certain adjectives appear before others in multi-adjective strings (e.g., the small brown box). However, these proposals have been developed and tested primarily in isolation and based on English; few researchers have looked at the combined performance of multiple factors in the determination of adjective order, and few have evaluated predictors across multiple languages. The current work approaches both of these objectives by using technologies and datasets from natural language processing to look at the combined performance of existing proposals across 32 languages. Comparing this performance with both random and idealized baselines, we show that the literature on adjective ordering has made significant meaningful progress across its many decades, but there remains quite a gap yet to be explained.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.67.pdf"
    },
    {
        "title": "Improving Multitask Retrieval by Promoting Task Specialization",
        "authors": [
            "Wenzheng Zhang",
            "Chenyan Xiong",
            "Karl Stratos",
            "Arnold Overwijk"
        ],
        "published": "2023",
        "summary": "In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model—one that is explicitly optimized for multitasking—along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.68.pdf"
    },
    {
        "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
        "authors": [
            "Elias Stengel-Eskin",
            "Benjamin Van Durme"
        ],
        "published": "2023",
        "summary": "Sequence generation models are increasingly being used to translate natural language into programs, i.e., to perform executable semantic parsing. The fact that semantic parsing aims to predict programs that can lead to executed actions in the real world motivates developing safe systems. This in turn makes measuring calibration—a central component to safety—particularly important. We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.69.pdf"
    },
    {
        "title": "Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues",
        "authors": [
            "Wentao Deng",
            "Jiahuan Pei",
            "Zhaochun Ren",
            "Zhumin Chen",
            "Pengjie Ren"
        ],
        "published": "2023",
        "summary": "Answer selection in open-domain dialogues aims to select an accurate answer from candidates. The recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5%, and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.70.pdf"
    },
    {
        "title": "Benchmarking the Generation of Fact Checking Explanations",
        "authors": [
            "Daniel Russo",
            "Serra Sinem Tekiroğlu",
            "Marco Guerini"
        ],
        "published": "2023",
        "summary": "Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of fake news produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e., news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in particular, that a claim-driven extractive step improves abstractive summarization performances. Finally, we show that although cross-dataset experiments suffer from performance degradation, a unique model trained on a combination of the two datasets is able to retain style information in an efficient manner.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.71.pdf"
    },
    {
        "title": "T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates",
        "authors": [
            "Peixin Huang",
            "Xiang Zhao",
            "Minghao Hu",
            "Zhen Tan",
            "Weidong Xiao"
        ],
        "published": "2023",
        "summary": "Named Entity Recognition (NER) has so far evolved from the traditional flat NER to overlapped and discontinuous NER. They have mostly been solved separately, with only several exceptions that concurrently tackle three tasks with a single model. The current best-performing method formalizes the unified NER as word-word relation classification, which barely focuses on mention content learning and fails to detect entity mentions comprising a single word. In this paper, we propose a two-stage span-based framework with templates, namely, T2-NER, to resolve the unified NER task. The first stage is to extract entity spans, where flat and overlapped entities can be recognized. The second stage is to classify over all entity span pairs, where discontinuous entities can be recognized. Finally, multi-task learning is used to jointly train two stages. To improve the efficiency of span-based model, we design grouped templates and typed templates for two stages to realize batch computations. We also apply an adjacent packing strategy and a latter packing strategy to model discriminative boundary information and learn better span (pair) representation. Moreover, we introduce the syntax information to enhance our span representation. We perform extensive experiments on eight benchmark datasets for flat, overlapped, and discontinuous NER, where our model beats all the current competitive baselines, obtaining the best performance of unified NER.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.72.pdf"
    },
    {
        "title": "PASTA: A Dataset for Modeling PArticipant STAtes in Narratives",
        "authors": [
            "Sayontan Ghosh",
            "Mahnaz Koupaee",
            "Isabella Chen",
            "Francis Ferraro",
            "Nathanael Chambers",
            "Niranjan Balasubramanian"
        ],
        "published": "2023",
        "summary": "The events in a narrative are understood as a coherent whole via the underlying states of their participants. Often, these participant states are not explicitly mentioned, instead left to be inferred by the reader. A model that understands narratives should likewise infer these implicit states, and even reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. This dataset contains inferable participant states; a counterfactual perturbation to each state; and the changes to the story that would be necessary if the counterfactual were true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, to revise a story conditioned on a counterfactual state, and to explain the most likely state change given a revised story. Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.73.pdf"
    },
    {
        "title": "U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction",
        "authors": [
            "Jie Zhou",
            "Shenpo Dong",
            "Yunxin Huang",
            "Meihan Wu",
            "Haili Li",
            "Jingnan Wang",
            "Hongkui Tu",
            "Xiaodong Wang"
        ],
        "published": "2023",
        "summary": "Within Open Relation Extraction (ORE) tasks, the Zero-shot ORE method is to generalize undefined relations from predefined relations, while the Unsupervised ORE method is to extract undefined relations without the need for annotations. However, despite the possibility of overlap between predefined and undefined relations in the training data, a unified framework for both Zero-shot and Unsupervised ORE has yet to be established. To address this gap, we propose U-CORE: A Unified Deep Cluster-wise Contrastive Framework for both Zero-shot and Unsupervised ORE, by leveraging techniques from Contrastive Learning (CL) and Clustering.1 U-CORE overcomes the limitations of CL-based Zero-shot ORE methods by employing Cluster-wise CL that preserves both local smoothness as well as global semantics. Additionally, we employ a deep-cluster-based updater that optimizes the cluster center, thus enhancing the accuracy and efficiency of the model. To increase the stability of the model, we adopt Adaptive Self-paced Learning that effectively addresses the data-shifting problems. Experimental results on three well-known datasets demonstrate that U-CORE significantly improves upon existing methods by showing an average improvement of 7.35% ARI on Zero-shot ORE tasks and 15.24% ARI on Unsupervised ORE tasks.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.74.pdf"
    },
    {
        "title": "In-Context Retrieval-Augmented Language Models",
        "authors": [
            "Ori Ram",
            "Yoav Levine",
            "Itay Dalmedigos",
            "Dor Muhlgay",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2023",
        "summary": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.75.pdf"
    },
    {
        "title": "Learning to Paraphrase Sentences to Different Complexity Levels",
        "authors": [
            "Alison Chi",
            "Li-Kuang Chen",
            "Yi-Chen Chang",
            "Shu-Hui Lee",
            "Jason S. Chang"
        ],
        "published": "2023",
        "summary": "While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence-level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.76.pdf"
    },
    {
        "title": "Direct Speech Translation for Automatic Subtitling",
        "authors": [
            "Sara Papi",
            "Marco Gaido",
            "Alina Karakanta",
            "Mauro Cettolo",
            "Matteo Negri",
            "Marco Turchi"
        ],
        "published": "2023",
        "summary": "Automatic subtitling is the task of automatically translating the speech of audiovisual content into short pieces of timed text, i.e., subtitles and their corresponding timestamps. The generated subtitles need to conform to space and time requirements, while being synchronized with the speech and segmented in a way that facilitates comprehension. Given its considerable complexity, the task has so far been addressed through a pipeline of components that separately deal with transcribing, translating, and segmenting text into subtitles, as well as predicting timestamps. In this paper, we propose the first direct speech translation model for automatic subtitling that generates subtitles in the target language along with their timestamps with a single model. Our experiments on 7 language pairs show that our approach outperforms a cascade system in the same data condition, also being competitive with production tools on both in-domain and newly released out-domain benchmarks covering new scenarios.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.77.pdf"
    },
    {
        "title": "How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure",
        "authors": [
            "Michael Wilson",
            "Jackson Petty",
            "Robert Frank"
        ],
        "published": "2023",
        "summary": "Language models are typically evaluated on their success at predicting the distribution of specific words in specific contexts. Yet linguistic knowledge also encodes relationships between contexts, allowing inferences between word distributions. We investigate the degree to which pre-trained transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure. We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically organized structure of the embedding space for word embeddings. However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation with current models and points to a reason for which their training is data-intensive.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.78.pdf"
    },
    {
        "title": "Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",
        "authors": [
            "Songbo Hu",
            "Han Zhou",
            "Mete Hergul",
            "Milan Gritta",
            "Guchun Zhang",
            "Ignacio Iacobacci",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "published": "2023",
        "summary": "Creating high-quality annotated data for task-oriented dialog (ToD) is known to be notoriously difficult, and the challenges are amplified when the goal is to create equitable, culturally adapted, and large-scale ToD datasets for multiple languages. Therefore, the current datasets are still very scarce and suffer from limitations such as translation-based non-native dialogs with translation artefacts, small scale, or lack of cultural adaptation, among others. In this work, we first take stock of the current landscape of multilingual ToD datasets, offering a systematic overview of their properties and limitations. Aiming to reduce all the detected limitations, we then introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD dataset. It is large-scale and offers culturally adapted dialogs in 4 languages to enable training and evaluation of multilingual and cross-lingual ToD systems. We describe a complex bottom–up data collection process that yielded the final dataset, and offer the first sets of baseline scores across different ToD-related tasks for future reference, also highlighting its challenging nature.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.79.pdf"
    },
    {
        "title": "Can Authorship Representation Learning Capture Stylistic Features?",
        "authors": [
            "Andrew Wang",
            "Cristina Aggazzotti",
            "Rebecca Kotula",
            "Rafael Rivera Soto",
            "Marcus Bishop",
            "Nicholas Andrews"
        ],
        "published": "2023",
        "summary": "Automatically disentangling an author’s style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style. As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time. Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.80.pdf"
    },
    {
        "title": "Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing",
        "authors": [
            "Tom Sherborne",
            "Tom Hosking",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods; exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation similarity.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.81.pdf"
    }
]