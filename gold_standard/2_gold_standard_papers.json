[
    {
        "title": "Automating Behavioral Testing in Machine Translation",
        "authors": [
            "Javier Ferrando",
            "Matthias Sperber",
            "Hendra Setiawan",
            "Dominic Telaar",
            "Saša Hasan"
        ],
        "published": "2023-12-01T00:00:00Z",
        "summary": "Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.",
        "pdf_link": "https://aclanthology.org/2023.wmt-1.97"
     },
     {
        "title": "ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",
        "authors": [
            "Hamideh Ghanadian",
            "Isar Nejadgholi",
            "Hussein Al Osman"
        ],
        "published": "2023-07-01T00:00:00Z",
        "summary": "This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.",
        "pdf_link": "https://aclanthology.org/2023.wassa-1.16"
     },
     {
        "title": "GenIE: Generative Information Extraction",
        "authors": [
            "Martin Josifoski",
            "Nicola De Cao",
            "Maxime Peyrard",
            "Fabio Petroni",
            "Robert West"
        ],
        "published": "2022-07-01T00:00:00Z",
        "summary": "Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.342"
     },
     {
        "title": "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks",
        "authors": [
            "Belinda Li",
            "Jane Yu",
            "Madian Khabsa",
            "Luke Zettlemoyer",
            "Alon Halevy",
            "Jacob Andreas"
        ],
        "published": "2022-07-01T00:00:00Z",
        "summary": "When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.346"
     },
     {
        "title": "PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality",
        "authors": [
            "Prashant Kodali",
            "Tanmay Sachan",
            "Akshay Goindani",
            "Anmol Goel",
            "Naman Ahuja",
            "Manish Shrivastava",
            "Ponnurangam Kumaraguru"
        ],
        "published": "2022-07-01T00:00:00Z",
        "summary": "Code-Mixing is a phenomenon of mixing two or more languages in a speech event and is prevalent in multilingual societies. Given the low-resource nature of Code-Mixing, machine generation of code-mixed text is a prevalent approach for data augmentation. However, evaluating the quality of such machine generated code-mixed text is an open problem. In our submission to HinglishEval, a shared task collocated with INLG2022, we attempt to build models to estimate factors that impact the quality of synthetically generated code-mix text by predicting ratings for code-mix quality. The HinglishEval Shared Task consists of two sub-tasks: a) Quality rating prediction, and b) Disagreement prediction. We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models. Our approach could not beat the baseline results. However, for Subtask-A, our team ranked a close second on F1 and Cohen's Kappa Score measures and first for the Mean Squared Error measure. For Subtask-B, our approach ranked third for the F1 score and first for the Mean Squared Error measure. The code for our submission can be accessed at the link provided.",
        "pdf_link": "https://aclanthology.org/2022.inlg-genchal.4"
     },
     {
        "title": "AmbiFC: Fact-Checking Ambiguous Claims with Evidence",
        "authors": [
            "Max Glockner",
            "Ieva Staliūnaitė",
            "James Thorne",
            "Gisela Vallejo",
            "Andreas Vlachos",
            "Iryna Gurevych"
        ],
        "published": "2024-01-09T00:00:00Z",
        "summary": "Automated fact-checking systems verify claims against evidence to predict their veracity. In real-world scenarios, the retrieved evidence may not unambiguously support or refute the claim and yield conflicting but valid interpretations. Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity. To address this issue we present AmbiFC, a fact-checking dataset with 10k claims derived from real-world information needs. It contains fine-grained evidence annotations of 50k passages from 5k Wikipedia pages. We analyze the disagreements arising from ambiguity when comparing claims against evidence in AmbiFC, observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning. We develop models for predicting veracity handling this ambiguity via soft labels, and find that a pipeline that learns the label distribution for sentence-level evidence selection and veracity prediction yields the best performance. We compare models trained on different subsets of AmbiFC and show that models trained on the ambiguous instances perform better when faced with the identified linguistic phenomena.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00629"
     },
     {
        "title": "Language Varieties of Italy: Technology Challenges and Opportunities",
        "authors": [
            "Alan Ramponi"
        ],
        "published": "2024-01-09T00:00:00Z",
        "summary": "Italy is characterized by a one-of-a-kind linguistic diversity landscape in Europe, which implicitly encodes local knowledge, cultural traditions, artistic expressions, and history of its speakers. However, most local languages and dialects in Italy are at risk of disappearing within a few generations. The NLP community has recently begun to engage with endangered languages, including those of Italy. Yet, most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages. In this paper, we introduce the linguistic context of Italy and challenge the default machine-centric assumptions of NLP for Italy’s language varieties. We advocate for a shift in the paradigm from machine-centric to speaker-centric NLP, and provide recommendations and opportunities for work that prioritizes languages and their speakers over technological advances. To facilitate the process, we finally propose building a local community towards responsible, participatory efforts aimed at supporting vitality of languages and dialects of Italy.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00631"
     }, 
     {
        "title": "Benchmarking Large Language Models for News Summarization",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "Kathleen McKeown",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2024-01-31T00:00:00Z",
        "summary": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00632"
     },
     {
        "title": "mGPT: Few-Shot Learners Go Multilingual",
        "authors": [
            "Oleh Shliazhko",
            "Alena Fenogenova",
            "Maria Tikhonova",
            "Anastasia Kozlova",
            "Vladislav Mikhailov",
            "Tatiana Shavrina"
        ],
        "published": "2024-01-31T00:00:00Z",
        "summary": "This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00633"
     },
     {
        "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
        "authors": [
            "Tuan Dinh",
            "Jinman Zhao",
            "Samson Tan",
            "Renato Negrinho",
            "Leonard Lausen",
            "Sheng Zha",
            "George Karypis"
        ],
        "published": "2023-12-01T00:00:00Z",
        "summary": "Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance.",
        "pdf_link": "https://doi.org/10.48550/arXiv.2306.03438"
     },
     {
        "title": "Cultural Adaptation of Recipes",
        "authors": [
            "Yong Cao",
            "Yova Kementchedjhieva",
            "Ruixiang Cui",
            "Antonia Karamolegkou",
            "Li Zhou",
            "Megan Dare",
            "Lucia Donatelli",
            "Daniel Hershcovich"
        ],
        "published": "2024-01-31T00:00:00Z",
        "summary": "Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00634"
     },
     {
        "title": "Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis",
        "authors": [
            "Shiman Zhao",
            "Yutao Xie",
            "Wei Chen",
            "Tengjiao Wang",
            "Jiahui Yao",
            "Jiabin Zheng"
        ],
        "published": "2024-01-31T00:00:00Z",
        "summary": "Few-shot Aspect Category Sentiment Analysis (ACSA) is a crucial task for aspect-based sentiment analysis, which aims to detect sentiment polarity for a given aspect category in a sentence with limited data. However, few-shot learning methods focus on distance metrics between the query and support sets to classify queries, heavily relying on aspect distributions in the embedding space. Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications. To solve the above issues, we propose a metric-free method for few-shot ACSA, which models the associated relations among the aspects of support and query sentences by Dual Relations Propagation (DRP), addressing the passive effect of overlapping distributions. Specifically, DRP uses the dual relations (similarity and diversity) among the aspects of support and query sentences to explore intra-cluster commonality and inter-cluster uniqueness for alleviating sentiment noise and enhancing aspect features. Additionally, the dual relations are transformed from support-query to class-query to promote query inference by learning class knowledge. Experiments show that we achieve convincing performance on few-shot ACSA, especially an average improvement of 2.93% accuracy and 2.10% F1 score in the 3-way 1-shot setting.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00635"
     },
     {
        "title": "Addressing the Binning Problem in Calibration Assessment through Scalar Annotations",
        "authors": [
            "Zhengping Jiang",
            "Anqi Liu",
            "Benjamnin Van Durme"
        ],
        "published": "2024-02-16T00:00:00Z",
        "summary": "Computational linguistics models commonly target the prediction of discrete—categorical—labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment. In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00636"
     },
     {
        "title": "An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation",
        "authors": [
            "Cheng Yang",
            "Guoping Huang",
            "Mo Yu",
            "Zhirui Zhang",
            "Siheng Li",
            "Mingming Yang",
            "Shuming Shi",
            "Yujiu Yang",
            "Lemao Liu"
        ],
        "published": "2024-02-23T00:00:00Z",
        "summary": "Word-level AutoCompletion (WLAC) is a rewarding yet challenging task in Computer-aided Translation. Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label). Since the context hidden vector itself does not take the label into account and it is projected to the label through a linear classifier, the model cannot sufficiently leverage valuable information from the source sentence as verified in our experiments, which eventually hinders its overall performance. To alleviate this issue, this work proposes an energy-based model for WLAC, which enables the context hidden vector to capture crucial information from the source sentence. Unfortunately, training and inference suffer from efficiency and effectiveness challenges, therefore we employ three simple yet effective strategies to put our model into practice. Experiments on four standard benchmarks demonstrate that our reranking-based approach achieves substantial improvements (about 6.07%) over the previous state-of-the-art model. Further analyses show that each strategy of our approach contributes to the final performance.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00637"
     },
     {
        "title": "Lost in the Middle: How Language Models Use Long Contexts",
        "authors": [
            "Nelson F. Liu",
            "Kevin Lin",
            "John Hewitt",
            "Ashwin Paranjape",
            "Michele Bevilacqua",
            "Fabio Petroni",
            "Percy Liang"
        ],
        "published": "2024-02-23T00:00:00Z",
        "summary": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00638"
     },
     {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "published": "2024-02-23T00:00:00Z",
        "summary": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00639"
     },
     {
        "title": "Text Attribute Control via Closed-Loop Disentanglement",
        "authors": [
            "Lei Sha",
            "Thomas Lukasiewicz"
        ],
        "published": "2024-03-01T00:00:00Z",
        "summary": "Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00640"
     },
     {
        "title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
        "authors": [
            "Shujie Li",
            "Liang Li",
            "Ruiying Geng",
            "Min Yang",
            "Binhua Li",
            "Guanghu Yuan",
            "Wanwei He",
            "Shao Yuan",
            "Can Ma",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2024-03-08T00:00:00Z",
        "summary": "Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00641"
     },
     {
        "title": "Exploring Human-Like Translation Strategy with Large Language Models",
        "authors": [
            "Zhiwei He",
            "Tian Liang",
            "Wenxiang Jiao",
            "Zhuosheng Zhang",
            "Yujiu Yang",
            "Rui Wang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "published": "2024-03-08T00:00:00Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11 directions × 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00642"
     },
     {
        "title": "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
        "authors": [
            "Dingmin Wang",
            "Qiuyuan Huang",
            "Matthew Jackson",
            "Jianfeng Gao"
        ],
        "published": "2024-04-09T00:00:00Z",
        "summary": "An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00646"
     },
     {
        "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
        "authors": [
            "Roi Cohen",
            "Eden Biran",
            "Ori Yoran",
            "Amir Globerson",
            "Mor Geva"
        ],
        "published": "2024-04-09T00:00:00Z",
        "summary": "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00644"
     }, 
     {
        "title": "The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations",
        "authors": [
            "Aina Garí Soler",
            "Matthieu Labeau",
            "Chloé Clavel"
        ],
        "published": "2024-04-05T00:00:00Z",
        "summary": "When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.",
        "pdf_link": "https://doi.org/10.1162/tacl_a_00647"
     }
]