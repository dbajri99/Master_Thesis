[
    {
        "title": "Instruction Tuning with GPT-4",
        "authors": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "published": "2023-04-06T17:58:09Z",
        "summary": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2304.03277v1.pdf"
      },
      {
        "title": "Exploring Language Models: A Comprehensive Survey and Analysis",
        "authors": [
            "A. Singh"
        ],
        "published": "2023-01-01T00:00:00Z",
        "summary": "The domain of natural language processing (NLP) has witnessed significant advancements with the arrival of large-scale pre-trained language models to revolutionize NLP research and achieve state-of-the-art performance across various tasks. They have propelled the development of sophisticated NLP applications and deepened our understanding of artificial intelligence. However, the growing size and complexity of these models have given rise to new challenges and limitations. Concerns related to model bias, interpretability, data privacy, and environmental impact have become prominent. This paper explores the impact and potential of large language models in NLP, highlighting the advancements made and the challenges that need to be addressed.",
        "pdf_link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10369423&isnumber=10368609"
      },
      {
        "title": "Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",
        "authors": [
            "Madiha Zahrah Choksi",
            "David Goedicke"
        ],
        "published": "2023-04-06T03:09:26Z",
        "summary": "Intelligent or generative writing tools rely on large language models that\nrecognize, summarize, translate, and predict content. This position paper\nprobes the copyright interests of open data sets used to train large language\nmodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumvent\nthe copyright interests of the used data? We start by defining software\ncopyright and tracing its history. We rely on GitHub Copilot as a modern case\nstudy challenging software copyright. Our conclusion outlines obstacles that\ngenerative writing assistants create for copyright, and offers a practical road\nmap for copyright analysis for developers, software law experts, and general\nusers to consider in the context of intelligent LLM-powered writing tools.",
        "pdf_link": "https://arxiv.org/pdf/2304.02839v1.pdf"
      },
      {
        "title": "Challenges and Limitations of ChatGPT and Other Large Language Models",
        "authors": [
            "Erwin L. Rimban"
        ],
        "published": "2023-06-11T00:00:00Z",
        "summary": "This article explores the challenges and limitations of large language models, focusing on ChatGPT as a representative example. We begin by discussing the potential benefits of large language models, such as their ability to generate natural language text and assist with language-related tasks. However, we also acknowledge the concerns around these models, including their environmental impact, potential for bias, and lack of interpretability. We then delve into specific challenges faced by ChatGPT and similar models, including limitations in their understanding of context, difficulty in handling rare or out-of-vocabulary words, and their tendency to generate nonsensical or offensive text. We conclude with recommendations for future research and development, including the need for increased transparency, interpretability, and ethical considerations in the creation and deployment of large language models.",
        "pdf_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4454441"
      },
      {
        "title": "Document-Level Machine Translation with Large Language Models",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-04-05T03:49:06Z",
        "summary": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2304.02210v2.pdf"
      },
      {
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
        "authors": [
            "Zhiqiang Hu",
            "Lei Wang",
            "Yihuai Lan",
            "Wanyu Xu",
            "Ee-Peng Lim",
            "Lidong Bing",
            "Xing Xu",
            "Soujanya Poria",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-04-04T16:31:37Z",
        "summary": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.01933v3.pdf"
      },
      {
        "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
        "authors": [
            "Shamane Siriwardhana",
            "Rivindu Weerasekera",
            "Elliott Wen",
            "Tharindu Kaluarachchi",
            "Rajib Rana",
            "Suranga Nanayakkara"
        ],
        "published": "2023",
        "summary": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.1.pdf"
    },
    {
        "title": "Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement",
        "authors": [
            "Bingzhi Li",
            "Guillaume Wisniewski",
            "Benoît Crabbé"
        ],
        "published": "2023",
        "summary": "Many studies have shown that transformers are able to predict subject-verb agreement, demonstrating their ability to uncover an abstract representation of the sentence in an unsupervised way. Recently, Li et al. (2021) found that transformers were also able to predict the object-past participle agreement in French, the modeling of which in formal grammar is fundamentally different from that of subject-verb agreement and relies on a movement and an anaphora resolution. To better understand transformers’ internal working, we propose to contrast how they handle these two kinds of agreement. Using probing and counterfactual analysis methods, our experiments on French agreements show that (i) the agreement task suffers from several confounders that partially question the conclusions drawn so far and (ii) transformers handle subject-verb and object-past participle agreements in a way that is consistent with their modeling in theoretical linguistics.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.2.pdf"
    },
    {
        "title": "On the Role of Negative Precedent in Legal Outcome Prediction",
        "authors": [
            "Josef Valvoda",
            "Ryan Cotterell",
            "Simone Teufel"
        ],
        "published": "2023",
        "summary": "Every legal case sets a precedent by developing the law in one of the following two ways. It either expands its scope, in which case it sets positive precedent, or it narrows it, in which case it sets negative precedent. Legal outcome prediction, the prediction of positive outcome, is an increasingly popular task in AI. In contrast, we turn our focus to negative outcomes here, and introduce a new task of negative outcome prediction. We discover an asymmetry in existing models’ ability to predict positive and negative outcomes. Where the state-of-the-art outcome prediction model we used predicts positive outcomes at 75.06 F1, it predicts negative outcomes at only 10.09 F1, worse than a random baseline. To address this performance gap, we develop two new models inspired by the dynamics of a court process. Our first model significantly improves positive outcome prediction score to 77.15 F1 and our second model more than doubles the negative outcome prediction performance to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals that there is still much room for improvement for outcome prediction models. https://github.com/valvoda/Negative-Precedent-in-Legal-Outcome-Prediction",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.3.pdf"
    },
    {
        "title": "Meta-Learning a Cross-lingual Manifold for Semantic Parsing",
        "authors": [
            "Tom Sherborne",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "Localizing a semantic parser to support new languages requires effective cross-lingual generalization. Recent work has found success with machine-translation or zero-shot methods, although these approaches can struggle to model how native speakers ask questions. We consider how to effectively leverage minimal annotated examples in new languages for few-shot cross-lingual semantic parsing. We introduce a first-order meta-learning algorithm to train a semantic parser with maximal sample efficiency during cross-lingual transfer. Our algorithm uses high-resource languages to train the parser and simultaneously optimizes for cross-lingual generalization to lower-resource languages. Results across six languages on ATIS demonstrate that our combination of generalization steps yields accurate semantic parsers sampling ≤10% of source training data in each new language. Our approach also trains a competitive model on Spider using English with generalization to Chinese similarly sampling ≤10% of training data.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.4.pdf"
    },
    {
        "title": "OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue",
        "authors": [
            "Zhi Chen",
            "Yuncong Liu",
            "Lu Chen",
            "Su Zhu",
            "Mengyue Wu",
            "Kai Yu"
        ],
        "published": "2023",
        "summary": "This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user’s constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.5.pdf"
    },
    {
        "title": "Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation",
        "authors": [
            "Llion Jones",
            "Richard Sproat",
            "Haruko Ishikawa",
            "Alexander Gutkin"
        ],
        "published": "2023",
        "summary": "If one sees the place name Houston Mercer Dog Run in New York, how does one know how to pronounce it? Assuming one knows that Houston in New York is pronounced /ˈhaʊstən/ and not like the Texas city (/ˈhjuːstən/), then one can probably guess that /ˈhaʊstən/ is also used in the name of the dog park. We present a novel architecture that learns to use the pronunciations of neighboring names in order to guess the pronunciation of a given target feature. Applied to Japanese place names, we demonstrate the utility of the model to finding and proposing corrections for errors in Google Maps. To demonstrate the utility of this approach to structurally similar problems, we also report on an application to a totally different task: Cognate reflex prediction in comparative historical linguistics. A version of the code has been open-sourced.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.6.pdf"
    },
    {
        "title": "Locally Typical Sampling",
        "authors": [
            "Clara Meister",
            "Tiago Pimentel",
            "Gian Wiher",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Today’s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process—which allows for an information-theoretic analysis—can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.7.pdf"
    },
    {
        "title": "Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization",
        "authors": [
            "Thomas Effland",
            "Michael Collins"
        ],
        "published": "2023",
        "summary": "We present Expected Statistic Regulariza tion (ESR), a novel regularization technique that utilizes low-order multi-task structural statistics to shape model distributions for semi- supervised learning on low-resource datasets. We study ESR in the context of cross-lingual transfer for syntactic analysis (POS tagging and labeled dependency parsing) and present several classes of low-order statistic functions that bear on model behavior. Experimentally, we evaluate the proposed statistics with ESR for unsupervised transfer on 5 diverse target languages and show that all statistics, when estimated accurately, yield improvements to both POS and LAS, with the best statistic improving POS by +7.0 and LAS by +8.5 on average. We also present semi-supervised transfer and learning curve experiments that show ESR provides significant gains over strong cross-lingual-transfer-plus-fine-tuning baselines for modest amounts of label data. These results indicate that ESR is a promising and complementary approach to model-transfer approaches for cross-lingual parsing.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.8.pdf"
    },
    {
        "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
        "authors": [
            "Olga Majewska",
            "Evgeniia Razumovskaia",
            "Edoardo M. Ponti",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "published": "2023",
        "summary": "Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets—both for modular and end-to-end modeling—suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn’s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.9.pdf"
    },
    {
        "title": "Modeling Emotion Dynamics in Song Lyrics with State Space Models",
        "authors": [
            "Yingjin Song",
            "Daniel Beck"
        ],
        "published": "2023",
        "summary": "Most previous work in music emotion recognition assumes a single or a few song-level labels for the whole song. While it is known that different emotions can vary in intensity within a song, annotated data for this setup is scarce and difficult to obtain. In this work, we propose a method to predict emotion dynamics in song lyrics without song-level supervision. We frame each song as a time series and employ a State Space Model (SSM), combining a sentence-level emotion predictor with an Expectation-Maximization (EM) procedure to generate the full emotion dynamics. Our experiments show that applying our method consistently improves the performance of sentence-level baselines without requiring any annotated songs, making it ideal for limited training data scenarios. Further analysis through case studies shows the benefits of our method while also indicating the limitations and pointing to future directions.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.10.pdf"
    },
    {
        "title": "FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context",
        "authors": [
            "Amith Ananthram",
            "Olivia Winn",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "While the link between color and emotion has been widely studied, how context-based changes in color impact the intensity of perceived emotions is not well understood. In this work, we present a new multimodal dataset for exploring the emotional connotation of color as mediated by line, stroke, texture, shape, and language. Our dataset, FeelingBlue, is a collection of 19,788 4-tuples of abstract art ranked by annotators according to their evoked emotions and paired with rationales for those annotations. Using this corpus, we present a baseline for a new task: Justified Affect Transformation. Given an image I, the task is to 1) recolor I to enhance a specified emotion e and 2) provide a textual justification for the change in e. Our model is an ensemble of deep neural networks which takes I, generates an emotionally transformed color palette p conditioned on I, applies p to I, and then justifies the color transformation in text via a visual-linguistic model. Experimental results shed light on the emotional connotation of color in context, demonstrating both the promise of our approach on this challenging task and the considerable potential for future investigations enabled by our corpus.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.11.pdf"
    },
    {
        "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP",
        "authors": [
            "Jiaao Chen",
            "Derek Tam",
            "Colin Raffel",
            "Mohit Bansal",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it difficult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classification, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.12.pdf"
    },
    {
        "title": "Coreference Resolution through a seq2seq Transition-Based System",
        "authors": [
            "Bernd Bohnet",
            "Chris Alberti",
            "Michael Collins"
        ],
        "published": "2023",
        "summary": "Most recent coreference resolution systems use search algorithms over possible spans to identify mentions and resolve coreference. We instead present a coreference resolution system that uses a text-to-text (seq2seq) paradigm to predict mentions and links jointly. We implement the coreference system as a transition system and use multilingual T5 as an underlying language model. We obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score for English (a 2.3 higher F1-score than previous work [Dobrovolskii, 2021]) using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than previous work), and 74.3 F1-score for Chinese (+5.3). In addition we use the SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot setting, and supervised setting using all available training data. We obtain substantially higher zero-shot F1-scores for 3 out of 4 languages than previous approaches and significantly exceed previous supervised state-of-the-art results for all five tested languages. We provide the code and models as open source.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.13.pdf"
    },
    {
        "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications",
        "authors": [
            "Gilbert Badaro",
            "Mohammed Saeed",
            "Paolo Papotti"
        ],
        "published": "2023",
        "summary": "In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs). Given the importance of knowledge available in tabular data, recent research efforts extend LMs by developing neural representations for structured data. In this article, we present a survey that analyzes these efforts. We first abstract the different systems according to a traditional machine learning pipeline in terms of training data, input representation, model training, and supported downstream tasks. For each aspect, we characterize and compare the proposed solutions. Finally, we discuss future work directions.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.14.pdf"
    }   
]