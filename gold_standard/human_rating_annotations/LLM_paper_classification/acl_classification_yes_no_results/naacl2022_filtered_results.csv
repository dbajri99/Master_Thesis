Title,Mentions LLM Limitations
Learning Natural Language Generation with Truncated Reinforcement Learning,yes
Language Model Augmented Monotonic Attention for Simultaneous Translation,yes
Enhancing Self-Attention with Knowledge-Assisted Attention Maps,yes
Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia,no
Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation,yes
SwahBERT: Language Model of Swahili,no
Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,no
Machine-in-the-Loop Rewriting for Creative Image Captioning,no
Reframing Human-AI Collaboration for Generating Free-Text Explanations,yes
NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics,no
Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection,no
Generating Repetitions with Appropriate Repeated Words,no
Abstraction not Memory: BERT and the English Article System,no
Provably Confidential Language Modelling,yes
"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",yes
DocTime: A Document-level Temporal Dependency Graph Parser,no
Towards a Progression-Aware Autonomous Dialogue Agent,yes
Cross-Domain Detection of GPT-2-Generated Technical Text,yes
Context-Aware Abbreviation Expansion Using Large Language Models,yes
Sort by Structure: Language Model Ranking as Dependency Probing,yes
Efficient Hierarchical Domain Adaptation for Pretrained Language Models,no
SKILL: Structured Knowledge Infusion for Large Language Models,yes
MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation,yes
Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models,yes
Representation Learning for Conversational Data using Discourse Mutual Information Maximization,no
ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model,yes
TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,no
KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation,yes
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,no
Sentence-Level Resampling for Named Entity Recognition,no
Locally Aggregated Feature Attribution on Natural Language Model Understanding,no
A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony,no
What do tokens know about their characters and how do they know it?,no
Data Augmentation with Dual Training for Offensive Span Detection,yes
Learning To Retrieve Prompts for In-Context Learning,no
"Re2G: Retrieve, Rerank, Generate",no
MetaICL: Learning to Learn In Context,no
Robust Conversational Agents against Imperceptible Toxicity Triggers,yes
Modal Dependency Parsing via Language Model Priming,no
PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding,yes
Progressive Class Semantic Matching for Semi-supervised Text Classification,no
Low Resource Style Transfer via Domain Adaptive Meta Learning,no
Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation,no
Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,yes
Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,yes
Unsupervised Paraphrasability Prediction for Compound Nominalizations,no
Global Entity Disambiguation with BERT,no
Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,no
On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,no
Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation,no
Exposing the Limits of Video-Text Models through Contrast Sets,yes
When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer,no
Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,no
Knowledge Inheritance for Pre-trained Language Models,yes
WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models,no
DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings,no
A Data Cartography based MixUp for Pre-trained Language Models,no
FNet: Mixing Tokens with Fourier Transforms,no
Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling,no
"Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",yes
Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge,yes
Using Paraphrases to Study Properties of Contextual Embeddings,no
Learning to Generate Examples for Semantic Processing Tasks,no
Symbolic Knowledge Distillation: from General Language Models to Commonsense Models,yes
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,yes
A Study of the Attention Abnormality in Trojaned BERTs,yes
Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora,yes
A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,no
SkillSpan: Hard and Soft Skill Extraction from English Job Postings,no
Modeling Multi-Granularity Hierarchical Features for Relation Extraction,no
Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances,no
KALA: Knowledge-Augmented Language Model Adaptation,yes
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,yes
When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes,no
Few-Shot Semantic Parsing with Language Models Trained on Code,no
ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence,no
Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification,no
Few-shot Subgoal Planning with Language Models,no
IDPG: An Instance-Dependent Prompt Generation Method,no
Embedding Hallucination for Few-shot Language Fine-tuning,no
DEMix Layers: Disentangling Domains for Modular Language Modeling,no
Contrastive Learning for Prompt-based Few-shot Language Learners,no
CoMPM: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation,no
Template-free Prompt Tuning for Few-shot NER,no
Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training,no
You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas,yes
"MOVER: Mask, Over-generate and Rank for Hyperbole Generation",no
Regularized Training of Nearest Neighbor Language Models,no
Methods for Estimating and Improving Robustness of Language Models,yes
Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,yes
Impact of Training Instance Selection on Domain-Specific Entity Extraction using BERT,no
Building a Personalized Dialogue System with Prompt-Tuning,yes
Zuo Zhuan Ancient Chinese Dataset for Word Sense Disambiguation,no
How do people talk about images? A study on open-domain conversations with images.,no
Probe-Less Probing of BERT’s Layer-Wise Linguistic Knowledge with Masked Word Prediction,no
Multimodal large language models for inclusive collaboration learning tasks,yes
Automating Human Evaluation of Dialogue Systems,no
Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations,no
Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition,yes
Towards Open-Domain Topic Classification,no
FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction,no
Self-supervised Representation Learning for Speech Processing,no
An End-to-End Dialogue Summarization System for Sales Calls,yes
Self-supervised Product Title Rewrite for Product Listing Ads,no
Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning,yes
Knowledge extraction from aeronautical messages (NOTAMs) with self-supervised language models for aircraft pilots,yes
Lightweight Transformers for Conversational AI,no
NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension,no
