[
    {
        "title": "$Se^2$: Sequential Example Selection for In-Context Learning",
        "authors": [
            "Haoyu Liu",
            "Jianfeng Liu",
            "Shaohan Huang",
            "Yuefeng Zhan",
            "Hao Sun",
            "Weiwei Deng",
            "Furu Wei",
            "Qi Zhang"
        ],
        "published": "2024-02-21T15:35:04Z",
        "summary": "The remarkable capability of large language models (LLMs) for in-context\nlearning (ICL) needs to be activated by demonstration examples. Prior work has\nextensively explored the selection of examples for ICL, predominantly following\nthe \"select then organize\" paradigm, such approaches often neglect the internal\nrelationships between examples and exist an inconsistency between the training\nand inference. In this paper, we formulate the problem as a\n$\\textit{se}$quential $\\textit{se}$lection problem and introduce $Se^2$, a\nsequential-aware method that leverages the LLM's feedback on varying context,\naiding in capturing inter-relationships and sequential information among\nexamples, significantly enriching the contextuality and relevance of ICL\nprompts. Meanwhile, we utilize beam search to seek and construct example\nsequences, enhancing both quality and diversity. Extensive experiments across\n23 NLP tasks from 8 distinct categories illustrate that $Se^2$ markedly\nsurpasses competitive baselines and achieves 42% relative improvement over\nrandom selection. Further in-depth analysis show the effectiveness of proposed\nstrategies, highlighting $Se^2$'s exceptional stability and adaptability across\nvarious scenarios. Our code will be released to facilitate future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.13874v2.pdf"
    },
    {
        "title": "An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach",
        "authors": [
            "Mohammad Amaz Uddin",
            "Iqbal H. Sarker"
        ],
        "published": "2024-02-21T15:23:21Z",
        "summary": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.",
        "pdf_link": "https://arxiv.org/pdf/2402.13871v1.pdf"
    },
    {
        "title": "Kuaiji: the First Chinese Accounting Large Language Model",
        "authors": [
            "Jiayuan Luo",
            "Songhua Yang",
            "Xiaoling Qiu",
            "Panyu Chen",
            "Yufei Nai",
            "Wenxuan Zeng",
            "Wentao Zhang",
            "Xinke Jiang"
        ],
        "published": "2024-02-21T15:14:20Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nimpressive proficiency in comprehending and generating natural language.\nHowever, they encounter difficulties when tasked with adapting to specialized\ndomains such as accounting. To address this challenge, we introduce Kuaiji, a\ntailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned\nusing the Baichuan framework, which encompasses continuous pre-training and\nsupervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing\nlarge genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy\nand response speed. Our contributions encompass the creation of the first\nChinese accounting dataset, the establishment of Kuaiji as a leading\nopen-source Chinese accounting LLM, and the validation of its efficacy through\nreal-world accounting scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13866v2.pdf"
    },
    {
        "title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs",
        "authors": [
            "Xiaoxia Li",
            "Siyuan Liang",
            "Jiyi Zhang",
            "Han Fang",
            "Aishan Liu",
            "Ee-Chien Chang"
        ],
        "published": "2024-02-21T15:13:50Z",
        "summary": "Large Language Models (LLMs), used in creative writing, code generation, and\ntranslation, generate text based on input sequences but are vulnerable to\njailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak\nprompt methods use a combination of jailbreak templates followed by questions\nto ask to create jailbreak prompts. However, existing jailbreak prompt designs\ngenerally suffer from excessive semantic differences, resulting in an inability\nto resist defenses that use simple semantic metrics as thresholds. Jailbreak\nprompts are semantically more varied than the original questions used for\nqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach\nthat bypasses LLMs by generating jailbreak prompts that are semantically\nsimilar to the original question. We model the search for jailbreak prompts\nthat satisfy both semantic similarity and jailbreak validity as a\nmulti-objective optimization problem and employ a standardized set of genetic\nalgorithms for generating eligible prompts. Compared to the baseline\nAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%\nhigher without ONION defense and 85.2% higher with ONION defense. SMJ's better\nperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,\nSimilarity, and Outlier, also means that SMJ is resistant to defenses that use\nthose metrics as thresholds.",
        "pdf_link": "https://arxiv.org/pdf/2402.14872v2.pdf"
    },
    {
        "title": "Large Language Models are Advanced Anonymizers",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunovi\u0107",
            "Martin Vechev"
        ],
        "published": "2024-02-21T14:44:00Z",
        "summary": "Recent work in privacy research on large language models has shown that they\nachieve near human-level performance at inferring personal data from real-world\nonline texts. With consistently increasing model capabilities, existing text\nanonymization methods are currently lacking behind regulatory requirements and\nadversarial threats. This raises the question of how individuals can\neffectively protect their personal data in sharing online texts. In this work,\nwe take two steps to answer this question: We first present a new setting for\nevaluating anonymizations in the face of adversarial LLMs inferences, allowing\nfor a natural measurement of anonymization performance while remedying some of\nthe shortcomings of previous metrics. We then present our LLM-based adversarial\nanonymization framework leveraging the strong inferential capabilities of LLMs\nto inform our anonymization procedure. In our experimental evaluation, we show\non real-world and synthetic online texts how adversarial anonymization\noutperforms current industry-grade anonymizers both in terms of the resulting\nutility and privacy.",
        "pdf_link": "https://arxiv.org/pdf/2402.13846v1.pdf"
    },
    {
        "title": "LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation",
        "authors": [
            "Shutong Qiao",
            "Chen Gao",
            "Junhao Wen",
            "Wei Zhou",
            "Qun Luo",
            "Peixuan Chen",
            "Yong Li"
        ],
        "published": "2024-02-21T14:38:02Z",
        "summary": "Traditional session-based recommendation (SBR) utilizes session behavior\nsequences from anonymous users for recommendation. Although this strategy is\nhighly efficient, it sacrifices the inherent semantic information of the items,\nmaking it difficult for the model to understand the true intent of the session\nand resulting in a lack of interpretability in the recommended results.\nRecently, large language models (LLMs) have flourished across various domains,\noffering a glimpse of hope in addressing the aforementioned challenges.\nInspired by the impact of LLMs, research exploring the integration of LLMs with\nthe Recommender system (RS) has surged like mushrooms after rain. However,\nconstrained by high time and space costs, as well as the brief and anonymous\nnature of session data, the first LLM recommendation framework suitable for\nindustrial deployment has yet to emerge in the field of SBR. To address the\naforementioned challenges, we have proposed the LLM Integration Framework for\nSBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR\nadopts a two-step strategy. Firstly, we transform session data into a bimodal\nform of text and behavior. In the first step, leveraging the inferential\ncapabilities of LLMs, we conduct inference on session text data from different\nperspectives and design the component for auxiliary enhancement. In the second\nstep, the SBR model is trained on behavior data, aligning and averaging two\nmodal session representations from different perspectives. Finally, we fuse\nsession representations from different perspectives and modalities as the\nultimate session representation for recommendation. We conducted experiments on\ntwo real-world datasets, and the results demonstrate that LLM4SBR significantly\nimproves the performance of traditional SBR models and is highly lightweight\nand efficient, making it suitable for industrial deployment.",
        "pdf_link": "https://arxiv.org/pdf/2402.13840v1.pdf"
    },
    {
        "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
        "authors": [
            "Hezhao Zhang",
            "Lasana Harris",
            "Nafise Sadat Moosavi"
        ],
        "published": "2024-02-21T13:57:36Z",
        "summary": "Dehumanization, characterized as a subtle yet harmful manifestation of hate\nspeech, involves denying individuals of their human qualities and often results\nin violence against marginalized groups. Despite significant progress in\nNatural Language Processing across various domains, its application in\ndetecting dehumanizing language is limited, largely due to the scarcity of\npublicly available annotated data for this domain. This paper evaluates the\nperformance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2,\nin identifying dehumanizing language. Our findings reveal that while these\nmodels demonstrate potential, achieving a 70\\% accuracy rate in distinguishing\ndehumanizing language from broader hate speech, they also display biases. They\nare over-sensitive in classifying other forms of hate speech as dehumanization\nfor a specific subset of target groups, while more frequently failing to\nidentify clear cases of dehumanization for other target groups. Moreover,\nleveraging one of the best-performing models, we automatically annotated a\nlarger dataset for training more accessible models. However, our findings\nindicate that these models currently do not meet the high-quality data\ngeneration threshold necessary for this task.",
        "pdf_link": "https://arxiv.org/pdf/2402.13818v1.pdf"
    },
    {
        "title": "LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain",
        "authors": [
            "Emanuele Musumeci",
            "Michele Brienza",
            "Vincenzo Suriani",
            "Daniele Nardi",
            "Domenico Daniele Bloisi"
        ],
        "published": "2024-02-21T13:54:53Z",
        "summary": "In the last years' digitalization process, the creation and management of\ndocuments in various domains, particularly in Public Administration (PA), have\nbecome increasingly complex and diverse. This complexity arises from the need\nto handle a wide range of document types, often characterized by\nsemi-structured forms. Semi-structured documents present a fixed set of data\nwithout a fixed format. As a consequence, a template-based solution cannot be\nused, as understanding a document requires the extraction of the data\nstructure. The recent introduction of Large Language Models (LLMs) has enabled\nthe creation of customized text output satisfying user requests. In this work,\nwe propose a novel approach that combines the LLMs with prompt engineering and\nmulti-agent systems for generating new documents compliant with a desired\nstructure. The main contribution of this work concerns replacing the commonly\nused manual prompting with a task description generated by semantic retrieval\nfrom an LLM. The potential of this approach is demonstrated through a series of\nexperiments and case studies, showcasing its effectiveness in real-world PA\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.14871v1.pdf"
    },
    {
        "title": "CriticBench: Evaluating Large Language Models as Critic",
        "authors": [
            "Tian Lan",
            "Wenwei Zhang",
            "Chen Xu",
            "Heyan Huang",
            "Dahua Lin",
            "Kai Chen",
            "Xian-ling Mao"
        ],
        "published": "2024-02-21T12:38:59Z",
        "summary": "Critique ability are crucial in the scalable oversight and self-improvement\nof Large Language Models (LLMs). While many recent studies explore the critique\nability of LLMs to judge and refine flaws in generations, how to\ncomprehensively and reliably measure the critique abilities of LLMs is\nunder-explored. This paper introduces CriticBench, a novel benchmark designed\nto comprehensively and reliably evaluate four key critique ability dimensions\nof LLMs: feedback, comparison, refinement and meta-feedback. CriticBench\nencompasses nine diverse tasks, each assessing the LLMs' ability to critique\nresponses at varying levels of quality granularity. Our extensive evaluations\nof open-source and closed-source LLMs reveal intriguing relationships between\nthe critique ability and tasks, response qualities, and model scales. Datasets,\nresources and evaluation toolkit for CriticBench will be publicly released at\nhttps://github.com/open-compass/CriticBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.13764v3.pdf"
    },
    {
        "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2024-02-21T12:35:19Z",
        "summary": "Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency(FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarisation remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.13758v1.pdf"
    },
    {
        "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
        "authors": [
            "Yiran Ding",
            "Li Lyna Zhang",
            "Chengruidong Zhang",
            "Yuanyuan Xu",
            "Ning Shang",
            "Jiahang Xu",
            "Fan Yang",
            "Mao Yang"
        ],
        "published": "2024-02-21T12:30:33Z",
        "summary": "Large context window is a desirable feature in large language models (LLMs).\nHowever, due to high fine-tuning costs, scarcity of long texts, and\ncatastrophic values introduced by new token positions, current extended context\nwindows are limited to around 128k tokens. This paper introduces LongRoPE that,\nfor the first time, extends the context window of pre-trained LLMs to an\nimpressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k\ntraining lengths, while maintaining performance at the original short context\nwindow. This is achieved by three key innovations: (i) we identify and exploit\ntwo forms of non-uniformities in positional interpolation through an efficient\nsearch, providing a better initialization for fine-tuning and enabling an 8x\nextension in non-fine-tuning scenarios; (ii) we introduce a progressive\nextension strategy that first fine-tunes a 256k length LLM and then conducts a\nsecond positional interpolation on the fine-tuned extended LLM to achieve a\n2048k context window; (iii) we readjust LongRoPE on 8k length to recover the\nshort context window performance. Extensive experiments on LLaMA2 and Mistral\nacross various tasks demonstrate the effectiveness of our method. Models\nextended via LongRoPE retain the original architecture with minor modifications\nto the positional embedding, and can reuse most pre-existing optimizations.",
        "pdf_link": "https://arxiv.org/pdf/2402.13753v1.pdf"
    },
    {
        "title": "Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph",
        "authors": [
            "Qian Zhao",
            "Hao Qian",
            "Ziqi Liu",
            "Gong-Duo Zhang",
            "Lihong Gu"
        ],
        "published": "2024-02-21T12:22:01Z",
        "summary": "Recommendation systems are widely used in e-commerce websites and online\nplatforms to address information overload. However, existing systems primarily\nrely on historical data and user feedback, making it difficult to capture user\nintent transitions. Recently, Knowledge Base (KB)-based models are proposed to\nincorporate expert knowledge, but it struggle to adapt to new items and the\nevolving e-commerce environment. To address these challenges, we propose a\nnovel Large Language Model based Complementary Knowledge Enhanced\nRecommendation System (LLM-KERec). It introduces an entity extractor that\nextracts unified concept terms from item and user information. To provide\ncost-effective and reliable prior knowledge, entity pairs are generated based\non entity popularity and specific strategies. The large language model\ndetermines complementary relationships in each entity pair, constructing a\ncomplementary knowledge graph. Furthermore, a new complementary recall module\nand an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of\nthe ranking model using real complementary exposure-click samples. Extensive\nexperiments conducted on three industry datasets demonstrate the significant\nperformance improvement of our model compared to existing approaches.\nAdditionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm\nfor consumption by recommending complementary items. In summary, LLM-KERec\naddresses the limitations of traditional recommendation systems by\nincorporating complementary knowledge and utilizing a large language model to\ncapture user intent transitions, adapt to new items, and enhance recommendation\nefficiency in the evolving e-commerce landscape.",
        "pdf_link": "https://arxiv.org/pdf/2402.13750v1.pdf"
    },
    {
        "title": "Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction",
        "authors": [
            "Guozheng Li",
            "Wenjun Ke",
            "Peng Wang",
            "Zijie Xu",
            "Ke Ji",
            "Jiajun Liu",
            "Ziyu Shang",
            "Qiqing Luo"
        ],
        "published": "2024-02-21T12:12:16Z",
        "summary": "The in-context learning (ICL) for relational triple extraction (RTE) has\nachieved promising performance, but still encounters two key challenges: (1)\nhow to design effective prompts and (2) how to select proper demonstrations.\nExisting methods, however, fail to address these challenges appropriately. On\nthe one hand, they usually recast RTE task to text-to-text prompting formats,\nwhich is unnatural and results in a mismatch between the output format at the\npre-training time and the inference time for large language models (LLMs). On\nthe other hand, they only utilize surface natural language features and lack\nconsideration of triple semantics in sample selection. These issues are\nblocking improved performance in ICL for RTE, thus we aim to tackle prompt\ndesigning and sample selection challenges simultaneously. To this end, we\ndevise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task\ninto a table generation task to incorporate explicit structured information\ninto ICL, facilitating conversion of outputs to RTE structures. Then we propose\ninstructive in-context learning (I$^2$CL) which only selects and annotates a\nfew samples considering internal triple semantics in massive unlabeled samples.",
        "pdf_link": "https://arxiv.org/pdf/2402.13741v1.pdf"
    },
    {
        "title": "From Text to CQL: Bridging Natural Language and Corpus Search Engine",
        "authors": [
            "Luming Lu",
            "Jiyuan An",
            "Yujie Wang",
            "Liner yang",
            "Cunliang Kong",
            "Zhenghao Liu",
            "Shuo Wang",
            "Haozhe Lin",
            "Mingwei Fang",
            "Yaping Huang",
            "Erhong Yang"
        ],
        "published": "2024-02-21T12:11:28Z",
        "summary": "Natural Language Processing (NLP) technologies have revolutionized the way we\ninteract with information systems, with a significant focus on converting\nnatural language queries into formal query languages such as SQL. However, less\nemphasis has been placed on the Corpus Query Language (CQL), a critical tool\nfor linguistic research and detailed analysis within text corpora. The manual\nconstruction of CQL queries is a complex and time-intensive task that requires\na great deal of expertise, which presents a notable challenge for both\nresearchers and practitioners. This paper presents the first text-to-CQL task\nthat aims to automate the translation of natural language into CQL. We present\na comprehensive framework for this task, including a specifically curated\nlarge-scale dataset and methodologies leveraging large language models (LLMs)\nfor effective text-to-CQL task. In addition, we established advanced evaluation\nmetrics to assess the syntactic and semantic accuracy of the generated queries.\nWe created innovative LLM-based conversion approaches and detailed experiments.\nThe results demonstrate the efficacy of our methods and provide insights into\nthe complexities of text-to-CQL task.",
        "pdf_link": "https://arxiv.org/pdf/2402.13740v1.pdf"
    },
    {
        "title": "Ouroboros: Speculative Decoding with Large Model Enhanced Drafting",
        "authors": [
            "Weilin Zhao",
            "Yuxiang Huang",
            "Xu Han",
            "Chaojun Xiao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-21T11:31:28Z",
        "summary": "Drafting-then-verifying decoding methods such as speculative decoding are\nwidely adopted training-free methods to accelerate the inference of large\nlanguage models (LLMs). Instead of employing an autoregressive process to\ndecode tokens sequentially, speculative decoding initially creates drafts with\nan efficient small model. Then LLMs are required to conduct verification and\ncorrection in a non-autoregressive fashion to minimize time overhead.\nGenerating longer drafts can lead to even more significant speedups once\nverified, but also incurs substantial trial and error costs if it fails.\nSuffering from the high verification failure probability, existing decoding\nmethods cannot draft too much content for verification at one time, achieving\nsub-optimal inference acceleration. In this paper, we introduce Ouroboros,\nwhich constructs a phrase candidate pool from the verification process of LLMs\nto provide candidates for draft generation of the small model. Thereby,\nOuroboros can further improve the efficiency and effectiveness of the initial\ndrafts. The experimental results on typical text generation tasks show that\nOuroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead\ndecoding and speculative decoding, respectively. The source code of Ouroboros\nis available at https://github.com/thunlp/Ouroboros.",
        "pdf_link": "https://arxiv.org/pdf/2402.13720v1.pdf"
    },
    {
        "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
        "authors": [
            "Xinrong Zhang",
            "Yingfa Chen",
            "Shengding Hu",
            "Zihang Xu",
            "Junhao Chen",
            "Moo Khai Hao",
            "Xu Han",
            "Zhen Leng Thai",
            "Shuo Wang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-21T11:30:29Z",
        "summary": "Processing and reasoning over long contexts is crucial for many practical\napplications of Large Language Models (LLMs), such as document comprehension\nand agent construction. Despite recent strides in making LLMs process contexts\nwith more than 100K tokens, there is currently a lack of a standardized\nbenchmark to evaluate this long-context capability. Existing public benchmarks\ntypically focus on contexts around 10K tokens, limiting the assessment and\ncomparison of LLMs in processing longer contexts. In this paper, we propose\n$\\infty$Bench, the first LLM benchmark featuring an average data length\nsurpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks\nspanning diverse domains, presented in both English and Chinese. The tasks in\n$\\infty$Bench are designed to require well understanding of long dependencies\nin contexts, and make simply retrieving a limited number of passages from\ncontexts not sufficient for these tasks. In our experiments, based on\n$\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source\nLLMs tailored for processing long contexts. The results indicate that existing\nlong context LLMs still require significant advancements to effectively process\n100K+ context. We further present three intriguing analyses regarding the\nbehavior of LLMs processing long context.",
        "pdf_link": "https://arxiv.org/pdf/2402.13718v3.pdf"
    },
    {
        "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
        "authors": [
            "Xiaoyan Yu",
            "Tongxu Luo",
            "Yifan Wei",
            "Fangyu Lei",
            "Yiming Huang",
            "Hao Peng",
            "Liehuang Zhu"
        ],
        "published": "2024-02-21T11:30:20Z",
        "summary": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents\nbut encounter challenges in multi-character role-playing (MCRP) scenarios. To\naddress the issue, we present Neeko, an innovative framework designed for\nefficient multiple characters imitation. Unlike existing methods, Neeko employs\na dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to\ndiverse characters. Our framework breaks down the role-playing process into\nagent pre-training, multiple characters playing, and character incremental\nlearning, effectively handling both seen and unseen roles. This dynamic\napproach, coupled with distinct LoRA blocks for each character, enhances\nNeeko's adaptability to unique attributes, personalities, and speaking\npatterns. As a result, Neeko demonstrates superior performance in MCRP over\nmost existing methods, offering more engaging and versatile user interaction\nexperiences. Code and data are available at\nhttps://github.com/weiyifan1023/Neeko.",
        "pdf_link": "https://arxiv.org/pdf/2402.13717v2.pdf"
    },
    {
        "title": "An Evaluation of Large Language Models in Bioinformatics Research",
        "authors": [
            "Hengchuang Yin",
            "Zhonghui Gu",
            "Fanhao Wang",
            "Yiparemu Abuduhaibaier",
            "Yanqiao Zhu",
            "Xinming Tu",
            "Xian-Sheng Hua",
            "Xiao Luo",
            "Yizhou Sun"
        ],
        "published": "2024-02-21T11:27:31Z",
        "summary": "Large language models (LLMs) such as ChatGPT have gained considerable\ninterest across diverse research communities. Their notable ability for text\ncompletion and generation has inaugurated a novel paradigm for\nlanguage-interfaced problem solving. However, the potential and efficacy of\nthese models in bioinformatics remain incompletely explored. In this work, we\nstudy the performance LLMs on a wide spectrum of crucial bioinformatics tasks.\nThese tasks include the identification of potential coding regions, extraction\nof named entities for genes and proteins, detection of antimicrobial and\nanti-cancer peptides, molecular optimization, and resolution of educational\nbioinformatics problems. Our findings indicate that, given appropriate prompts,\nLLMs like GPT variants can successfully handle most of these tasks. In\naddition, we provide a thorough analysis of their limitations in the context of\ncomplicated bioinformatics tasks. In conclusion, we believe that this work can\nprovide new perspectives and motivate future research in the field of LLMs\napplications, AI for Science and bioinformatics.",
        "pdf_link": "https://arxiv.org/pdf/2402.13714v1.pdf"
    },
    {
        "title": "SaGE: Evaluating Moral Consistency in Large Language Models",
        "authors": [
            "Vamshi Krishna Bonagiri",
            "Sreeram Vennam",
            "Priyanshul Govil",
            "Ponnurangam Kumaraguru",
            "Manas Gaur"
        ],
        "published": "2024-02-21T11:23:21Z",
        "summary": "Despite recent advancements showcasing the impressive capabilities of Large\nLanguage Models (LLMs) in conversational systems, we show that even\nstate-of-the-art LLMs are morally inconsistent in their generations,\nquestioning their reliability (and trustworthiness in general). Prior works in\nLLM evaluation focus on developing ground-truth data to measure accuracy on\nspecific tasks. However, for moral scenarios that often lack universally\nagreed-upon answers, consistency in model responses becomes crucial for their\nreliability. To address this issue, we propose an information-theoretic measure\ncalled Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of\nThumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract\nprinciples learned by a model and can help explain their decision-making\nstrategies effectively. To this extent, we construct the Moral Consistency\nCorpus (MCC), containing 50K moral questions, responses to them by LLMs, and\nthe RoTs that these models followed. Furthermore, to illustrate the\ngeneralizability of SaGE, we use it to investigate LLM consistency on two\npopular datasets -- TruthfulQA and HellaSwag. Our results reveal that\ntask-accuracy and consistency are independent problems, and there is a dire\nneed to investigate these issues further.",
        "pdf_link": "https://arxiv.org/pdf/2402.13709v2.pdf"
    },
    {
        "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
        "authors": [
            "Alexander Arno Weber",
            "Klaudia Thellmann",
            "Jan Ebert",
            "Nicolas Flores-Herr",
            "Jens Lehmann",
            "Michael Fromm",
            "Mehdi Ali"
        ],
        "published": "2024-02-21T11:07:07Z",
        "summary": "The adaption of multilingual pre-trained Large Language Models (LLMs) into\neloquent and helpful assistants is essential to facilitate their use across\ndifferent language regions. In that spirit, we are the first to conduct an\nextensive study of the performance of multilingual models on parallel,\nmulti-turn instruction-tuning benchmarks across a selection of the most-spoken\nIndo-European languages. We systematically examine the effects of language and\ninstruction dataset size on a mid-sized, multilingual LLM by instruction-tuning\nit on parallel instruction-tuning datasets. Our results demonstrate that\ninstruction-tuning on parallel instead of monolingual corpora benefits\ncross-lingual instruction following capabilities by up to 4.6%. Furthermore, we\nshow that the Superficial Alignment Hypothesis does not hold in general, as the\ninvestigated multilingual 7B parameter model presents a counter-example\nrequiring large-scale instruction-tuning datasets. Finally, we conduct a human\nannotation study to understand the alignment between human-based and\nGPT-4-based evaluation within multilingual chat scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13703v1.pdf"
    },
    {
        "title": "KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection",
        "authors": [
            "Michal Spiegel",
            "Dominik Macko"
        ],
        "published": "2024-02-21T10:09:56Z",
        "summary": "SemEval-2024 Task 8 is focused on multigenerator, multidomain, and\nmultilingual black-box machine-generated text detection. Such a detection is\nimportant for preventing a potential misuse of large language models (LLMs),\nthe newest of which are very capable in generating multilingual human-like\ntexts. We have coped with this task in multiple ways, utilizing language\nidentification and parameter-efficient fine-tuning of smaller LLMs for text\nclassification. We have further used the per-language classification-threshold\ncalibration to uniquely combine fine-tuned models predictions with statistical\ndetection metrics to improve generalization of the system detection\nperformance. Our submitted method achieved competitive results, ranking at the\nfourth place, just under 1 percentage point behind the winner.",
        "pdf_link": "https://arxiv.org/pdf/2402.13671v1.pdf"
    },
    {
        "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
        "authors": [
            "Zhaorui Yang",
            "Qian Liu",
            "Tianyu Pang",
            "Han Wang",
            "Haozhe Feng",
            "Minfeng Zhu",
            "Wei Chen"
        ],
        "published": "2024-02-21T10:06:08Z",
        "summary": "The surge in Large Language Models (LLMs) has revolutionized natural language\nprocessing, but fine-tuning them for specific tasks often encounters challenges\nin balancing performance and preserving general instruction-following\nabilities. In this paper, we posit that the distribution gap between task\ndatasets and the LLMs serves as the primary underlying cause. To address the\nproblem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach\nthat bridges the distribution gap by guiding fine-tuning with a distilled\ndataset generated by the model itself to match its original distribution.\nExperimental results on the Llama-2-chat model across various benchmarks\ndemonstrate that SDFT effectively mitigates catastrophic forgetting while\nachieving comparable or superior performance on downstream tasks compared to\nthe vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain\nthe helpfulness and safety alignment of LLMs. Our code is available at\n\\url{https://github.com/sail-sg/sdft}.",
        "pdf_link": "https://arxiv.org/pdf/2402.13669v1.pdf"
    },
    {
        "title": "GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model",
        "authors": [
            "Jianghui Zhou",
            "Ya Gao",
            "Jie Liu",
            "Xuemin Zhao",
            "Zhaohua Yang",
            "Yue Wu",
            "Lirong Shi"
        ],
        "published": "2024-02-21T09:59:20Z",
        "summary": "Large language models(LLM) such as ChatGPT have substantially simplified the\ngeneration of marketing copy, yet producing content satisfying domain specific\nrequirements, such as effectively engaging customers, remains a significant\nchallenge. In this work, we introduce the Genetic Copy Optimization Framework\n(GCOF) designed to enhance both efficiency and engagememnt of marketing copy\ncreation. We conduct explicit feature engineering within the prompts of LLM.\nAdditionally, we modify the crossover operator in Genetic Algorithm (GA),\nintegrating it into the GCOF to enable automatic feature engineering. This\nintegration facilitates a self-iterative refinement of the marketing copy.\nCompared to human curated copy, Online results indicate that copy produced by\nour framework achieves an average increase in click-through rate (CTR) of over\n$50\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2402.13667v1.pdf"
    },
    {
        "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
        "authors": [
            "Da Yu",
            "Peter Kairouz",
            "Sewoong Oh",
            "Zheng Xu"
        ],
        "published": "2024-02-21T09:45:08Z",
        "summary": "Service providers of large language model (LLM) applications collect user\ninstructions in the wild and use them in further aligning LLMs with users'\nintentions. These instructions, which potentially contain sensitive\ninformation, are annotated by human workers in the process. This poses a new\nprivacy risk not addressed by the typical private optimization. To this end, we\npropose using synthetic instructions to replace real instructions in data\nannotation and model fine-tuning. Formal differential privacy is guaranteed by\ngenerating those synthetic instructions using privately fine-tuned generators.\nCrucial in achieving the desired utility is our novel filtering algorithm that\nmatches the distribution of the synthetic instructions to that of the real\nones. In both supervised fine-tuning and reinforcement learning from human\nfeedback, our extensive experiments demonstrate the high utility of the final\nset of synthetic instructions by showing comparable results to real\ninstructions. In supervised fine-tuning, models trained with private synthetic\ninstructions outperform leading open-source models such as Vicuna.",
        "pdf_link": "https://arxiv.org/pdf/2402.13659v1.pdf"
    },
    {
        "title": "Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions",
        "authors": [
            "Lei Pan",
            "Yunshi Lan",
            "Yang Li",
            "Weining Qian"
        ],
        "published": "2024-02-21T09:28:02Z",
        "summary": "Unsupervised Text Style Transfer (UTST) has emerged as a critical task within\nthe domain of Natural Language Processing (NLP), aiming to transfer one\nstylistic aspect of a sentence into another style without changing its\nsemantics, syntax, or other attributes. This task is especially challenging\ngiven the intrinsic lack of parallel text pairings. Among existing methods for\nUTST tasks, attention masking approach and Large Language Models (LLMs) are\ndeemed as two pioneering methods. However, they have shortcomings in generating\nunsmooth sentences and changing the original contents, respectively. In this\npaper, we investigate if we can combine these two methods effectively. We\npropose four ways of interactions, that are pipeline framework with tuned\norders; knowledge distillation from LLMs to attention masking model; in-context\nlearning with constructed parallel examples. We empirically show these\nmulti-way interactions can improve the baselines in certain perspective of\nstyle strength, content preservation and text fluency. Experiments also\ndemonstrate that simply conducting prompting followed by attention\nmasking-based revision can consistently surpass the other systems, including\nsupervised text style transfer systems. On Yelp-clean and Amazon-clean\ndatasets, it improves the previously best mean metric by 0.5 and 3.0 absolute\npercentages respectively, and achieves new SOTA results.",
        "pdf_link": "https://arxiv.org/pdf/2402.13647v1.pdf"
    },
    {
        "title": "UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language",
        "authors": [
            "Yufei He",
            "Bryan Hooi"
        ],
        "published": "2024-02-21T09:06:31Z",
        "summary": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\nwhen this concept is applied to graph learning, a stark contrast emerges. Graph\nlearning has predominantly focused on single-graph models, tailored to specific\ntasks or datasets, lacking the ability to transfer learned knowledge to\ndifferent domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we present our UniGraph\nframework, designed to train a graph foundation model capable of generalizing\nto unseen graphs and tasks across diverse domains. Unlike single-graph models\nthat use pre-computed node features of varying dimensions as input, our\napproach leverages Text-Attributed Graphs (TAGs) for unifying node\nrepresentations. We propose a cascaded architecture of Language Models (LMs)\nand Graph Neural Networks (GNNs) as backbone networks with a self-supervised\ntraining objective based on Masked Graph Modeling (MGM). We introduce graph\ninstruction tuning using Large Language Models (LLMs) to enable zero-shot\nprediction ability. Our comprehensive experiments across various graph learning\ntasks and domains demonstrate the model's effectiveness in self-supervised\nrepresentation learning on unseen graphs, few-shot in-context transfer, and\nzero-shot transfer, even surpassing or matching the performance of GNNs that\nhave undergone supervised training on target datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.13630v1.pdf"
    },
    {
        "title": "FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models",
        "authors": [
            "Sahil Mishra",
            "Ujjwal Sudev",
            "Tanmoy Chakraborty"
        ],
        "published": "2024-02-21T08:50:40Z",
        "summary": "Taxonomies represent an arborescence hierarchical structure that establishes\nrelationships among entities to convey knowledge within a specific domain. Each\nedge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find\nutility in various real-world applications, such as e-commerce search engines\nand recommendation systems. Consequently, there arises a necessity to enhance\nthese taxonomies over time. However, manually curating taxonomies with neoteric\ndata presents challenges due to limitations in available human resources and\nthe exponential growth of data. Therefore, it becomes imperative to develop\nautomatic taxonomy expansion methods. Traditional supervised taxonomy expansion\napproaches encounter difficulties stemming from limited resources, primarily\ndue to the small size of existing taxonomies. This scarcity of training data\noften leads to overfitting. In this paper, we propose FLAME, a novel approach\nfor taxonomy expansion in low-resource environments by harnessing the\ncapabilities of large language models that are trained on extensive real-world\nknowledge. LLMs help compensate for the scarcity of domain-specific knowledge.\nSpecifically, FLAME leverages prompting in few-shot settings to extract the\ninherent knowledge within the LLMs, ascertaining the hypernym entities within\nthe taxonomy. Furthermore, it employs reinforcement learning to fine-tune the\nlarge language models, resulting in more accurate predictions. Experiments on\nthree real-world benchmark datasets demonstrate the effectiveness of FLAME in\nreal-world scenarios, achieving a remarkable improvement of 18.5% in accuracy\nand 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate\nthe strengths and weaknesses of FLAME through an extensive case study, error\nanalysis and ablation studies on the benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.13623v1.pdf"
    },
    {
        "title": "Data-driven Discovery with Large Generative Models",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Harshit Surana",
            "Dhruv Agarwal",
            "Sanchaita Hazra",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "published": "2024-02-21T08:26:43Z",
        "summary": "With the accumulation of data at an unprecedented rate, its potential to fuel\nscientific discovery is growing exponentially. This position paper urges the\nMachine Learning (ML) community to exploit the capabilities of large generative\nmodels (LGMs) to develop automated systems for end-to-end data-driven discovery\n-- a paradigm encompassing the search and verification of hypotheses purely\nfrom a set of provided datasets, without the need for additional data\ncollection or physical experiments. We first outline several desiderata for an\nideal data-driven discovery system. Then, through DATAVOYAGER, a\nproof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of\nthese desiderata -- a feat previously unattainable -- while also highlighting\nimportant limitations in the current system that open up opportunities for\nnovel ML research. We contend that achieving accurate, reliable, and robust\nend-to-end discovery systems solely through the current capabilities of LGMs is\nchallenging. We instead advocate for fail-proof tool integration, along with\nactive user moderation through feedback mechanisms, to foster data-driven\nscientific discoveries with efficiency and reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.13610v1.pdf"
    },
    {
        "title": "CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models",
        "authors": [
            "Fuwen Luo",
            "Chi Chen",
            "Zihao Wan",
            "Zhaolu Kang",
            "Qidong Yan",
            "Yingjie Li",
            "Xiaolong Wang",
            "Siyu Wang",
            "Ziyue Wang",
            "Xiaoyue Mi",
            "Peng Li",
            "Ning Ma",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-21T08:21:12Z",
        "summary": "Multimodal large language models (MLLMs) have demonstrated promising results\nin a variety of tasks that combine vision and language. As these models become\nmore integral to research and applications, conducting comprehensive\nevaluations of their capabilities has grown increasingly important. However,\nmost existing benchmarks fail to consider that, in certain situations, images\nneed to be interpreted within a broader context. In this work, we introduce a\nnew benchmark, named as CODIS, designed to assess the ability of models to use\ncontext provided in free-form text to enhance visual comprehension. Our\nfindings indicate that MLLMs consistently fall short of human performance on\nthis benchmark. Further analysis confirms that these models struggle to\neffectively extract and utilize contextual information to improve their\nunderstanding of images. This underscores the pressing need to enhance the\nability of MLLMs to comprehend visuals in a context-dependent manner. View our\nproject website at https://thunlp-mt.github.io/CODIS.",
        "pdf_link": "https://arxiv.org/pdf/2402.13607v2.pdf"
    },
    {
        "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
        "authors": [
            "Boyang Xue",
            "Hongru Wang",
            "Weichao Wang",
            "Rui Wang",
            "Sheng Wang",
            "Zeming Liu",
            "Kam-Fai Wong"
        ],
        "published": "2024-02-21T08:20:06Z",
        "summary": "The tendency of Large Language Models to generate hallucinations and exhibit\noverconfidence in predictions raises concerns regarding their reliability.\nConfidence or uncertainty estimations indicating the extent of trustworthiness\nof a model's response are essential to developing reliable AI systems. Current\nresearch primarily focuses on LLM confidence estimations in English, remaining\na void for other widely used languages and impeding the global development of\nreliable AI applications. This paper introduces a comprehensive investigation\nof Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce\nan elaborated and expert-checked multilingual QA dataset. Second, we delve into\nthe performance of confidence estimations and examine how these confidence\nscores can enhance LLM performance through self-refinement across diverse\nlanguages. Finally, we propose a cross-lingual confidence estimation method to\nachieve more precise confidence scores. The experimental results showcase the\nperformance of various confidence estimation methods across different languages\nas well as present that our proposed cross-lingual confidence estimation\ntechnique significantly enhances confidence estimation and outperforms several\nbaseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.13606v1.pdf"
    },
    {
        "title": "KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge",
        "authors": [
            "Jiyoung Lee",
            "Minwoo Kim",
            "Seungho Kim",
            "Junghwan Kim",
            "Seunghyun Won",
            "Hwaran Lee",
            "Edward Choi"
        ],
        "published": "2024-02-21T08:12:26Z",
        "summary": "For Large Language Models (LLMs) to be effectively deployed in a specific\ncountry, they must possess an understanding of the nation's culture and basic\nknowledge. To this end, we introduce National Alignment, which measures an\nalignment between an LLM and a targeted country from two aspects: social value\nalignment and common knowledge alignment. Social value alignment evaluates how\nwell the model understands nation-specific social values, while common\nknowledge alignment examines how well the model captures basic knowledge\nrelated to the nation. We constructed KorNAT, the first benchmark that measures\nnational alignment with South Korea. For the social value dataset, we obtained\nground truth labels from a large-scale survey involving 6,174 unique Korean\nparticipants. For the common knowledge dataset, we constructed samples based on\nKorean textbooks and GED reference materials. KorNAT contains 4K and 6K\nmultiple-choice questions for social value and common knowledge, respectively.\nOur dataset creation process is meticulously designed and based on statistical\nsampling theory and was refined through multiple rounds of human review. The\nexperiment results of seven LLMs reveal that only a few models met our\nreference score, indicating a potential for further enhancement. KorNAT has\nreceived government approval after passing an assessment conducted by a\ngovernment-affiliated organization dedicated to evaluating dataset quality.\nSamples and detailed evaluation protocols of our dataset can be found in\nhttps://selectstar.ai/ko/papers-national-alignment",
        "pdf_link": "https://arxiv.org/pdf/2402.13605v4.pdf"
    },
    {
        "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving",
        "authors": [
            "Mehdi Azarafza",
            "Mojtaba Nayyeri",
            "Charles Steinmetz",
            "Steffen Staab",
            "Achim Rettberg"
        ],
        "published": "2024-02-21T08:09:05Z",
        "summary": "Large Language Models (LLMs) have garnered significant attention for their\nability to understand text and images, generate human-like text, and perform\ncomplex reasoning tasks. However, their ability to generalize this advanced\nreasoning with a combination of natural language text for decision-making in\ndynamic situations requires further exploration. In this study, we investigate\nhow well LLMs can adapt and apply a combination of arithmetic and common-sense\nreasoning, particularly in autonomous driving scenarios. We hypothesize that\nLLMs hybrid reasoning abilities can improve autonomous driving by enabling them\nto analyze detected object and sensor data, understand driving regulations and\nphysical laws, and offer additional context. This addresses complex scenarios,\nlike decisions in low visibility (due to weather conditions), where traditional\nmethods might fall short. We evaluated Large Language Models (LLMs) based on\naccuracy by comparing their answers with human-generated ground truth inside\nCARLA. The results showed that when a combination of images (detected objects)\nand sensor data is fed into the LLM, it can offer precise information for brake\nand throttle control in autonomous vehicles across various weather conditions.\nThis formulation and answers can assist in decision-making for auto-pilot\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2402.13602v3.pdf"
    },
    {
        "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
        "authors": [
            "Lin Ning",
            "Luyang Liu",
            "Jiaxing Wu",
            "Neo Wu",
            "Devora Berlowitz",
            "Sushant Prakash",
            "Bradley Green",
            "Shawn O'Banion",
            "Jun Xie"
        ],
        "published": "2024-02-21T08:03:27Z",
        "summary": "Large language models (LLMs) have revolutionized natural language processing.\nHowever, effectively incorporating complex and potentially noisy user\ninteraction data remains a challenge. To address this, we propose User-LLM, a\nnovel framework that leverages user embeddings to contextualize LLMs. These\nembeddings, distilled from diverse user interactions using self-supervised\npretraining, capture latent user preferences and their evolution over time. We\nintegrate these user embeddings with LLMs through cross-attention and\nsoft-prompting, enabling LLMs to dynamically adapt to user context. Our\ncomprehensive experiments on MovieLens, Amazon Review, and Google Local Review\ndatasets demonstrate significant performance gains across various tasks.\nNotably, our approach outperforms text-prompt-based contextualization on long\nsequence tasks and tasks that require deep user understanding while being\ncomputationally efficient. We further incorporate Perceiver layers to\nstreamline the integration between user encoders and LLMs, reducing\ncomputational demands.",
        "pdf_link": "https://arxiv.org/pdf/2402.13598v1.pdf"
    },
    {
        "title": "Knowledge Graph Enhanced Large Language Model Editing",
        "authors": [
            "Mengqi Zhang",
            "Xiaotian Ye",
            "Qiang Liu",
            "Pengjie Ren",
            "Shu Wu",
            "Zhumin Chen"
        ],
        "published": "2024-02-21T07:52:26Z",
        "summary": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.13593v1.pdf"
    },
    {
        "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
        "authors": [
            "Ziyi Guan",
            "Hantao Huang",
            "Yupeng Su",
            "Hong Huang",
            "Ngai Wong",
            "Hao Yu"
        ],
        "published": "2024-02-21T07:45:22Z",
        "summary": "Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14866v1.pdf"
    },
    {
        "title": "A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Wenhan Luo",
            "Lin Ma",
            "Yuxin Ding",
            "Min Zhang"
        ],
        "published": "2024-02-21T07:38:29Z",
        "summary": "In this paper, we propose a new setting for generating product descriptions\nfrom images, augmented by marketing keywords. It leverages the combined power\nof visual and textual information to create descriptions that are more tailored\nto the unique features of products. For this setting, previous methods utilize\nvisual and textual encoders to encode the image and keywords and employ a\nlanguage model-based decoder to generate the product description. However, the\ngenerated description is often inaccurate and generic since same-category\nproducts have similar copy-writings, and optimizing the overall framework on\nlarge-scale samples makes models concentrate on common words yet ignore the\nproduct features. To alleviate the issue, we present a simple and effective\nMultimodal In-Context Tuning approach, named ModICT, which introduces a similar\nproduct sample as the reference and utilizes the in-context learning capability\nof language models to produce the description. During training, we keep the\nvisual encoder and language model frozen, focusing on optimizing the modules\nresponsible for creating multimodal in-context references and dynamic prompts.\nThis approach preserves the language generation prowess of large language\nmodels (LLMs), facilitating a substantial increase in description diversity. To\nassess the effectiveness of ModICT across various language model scales and\ntypes, we collect data from three distinct product categories within the\nE-commerce domain. Extensive experiments demonstrate that ModICT significantly\nimproves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4%\non D-5) of generated results compared to conventional methods. Our findings\nunderscore the potential of ModICT as a valuable tool for enhancing automatic\ngeneration of product descriptions in a wide range of applications. Code is at:\nhttps://github.com/HITsz-TMG/Multimodal-In-Context-Tuning",
        "pdf_link": "https://arxiv.org/pdf/2402.13587v2.pdf"
    },
    {
        "title": "WinoViz: Probing Visual Properties of Objects Under Different States",
        "authors": [
            "Woojeong Jin",
            "Tejas Srinivasan",
            "Jesse Thomason",
            "Xiang Ren"
        ],
        "published": "2024-02-21T07:31:47Z",
        "summary": "Humans perceive and comprehend different visual properties of an object based\non specific contexts. For instance, we know that a banana turns brown ``when it\nbecomes rotten,'' whereas it appears green ``when it is unripe.'' Previous\nstudies on probing visual commonsense knowledge have primarily focused on\nexamining language models' understanding of typical properties (e.g., colors\nand shapes) of objects. We present WinoViz, a text-only evaluation dataset,\nconsisting of 1,380 examples that probe the reasoning abilities of language\nmodels regarding variant visual properties of objects under different contexts\nor states. Our task is challenging since it requires pragmatic reasoning\n(finding intended meanings) and visual knowledge reasoning. We also present\nmulti-hop data, a more challenging version of our data, which requires\nmulti-step reasoning chains to solve our task. In our experimental analysis,\nour findings are: a) Large language models such as GPT-4 demonstrate effective\nperformance, but when it comes to multi-hop data, their performance is\nsignificantly degraded. b) Large models perform well on pragmatic reasoning,\nbut visual knowledge reasoning is a bottleneck in our task. c) Vision-language\nmodels outperform their language-model counterparts. d) A model with\nmachine-generated images performs poorly in our task. This is due to the poor\nquality of the generated images.",
        "pdf_link": "https://arxiv.org/pdf/2402.13584v1.pdf"
    },
    {
        "title": "BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models",
        "authors": [
            "Xueliang Zhao",
            "Xinting Huang",
            "Tingchen Fu",
            "Qintong Li",
            "Shansan Gong",
            "Lemao Liu",
            "Wei Bi",
            "Lingpeng Kong"
        ],
        "published": "2024-02-21T07:16:29Z",
        "summary": "Multimodal reasoning stands as a pivotal capability for large vision-language\nmodels (LVLMs). The integration with Domain-Specific Languages (DSL), offering\nprecise visual representations, equips these models with the opportunity to\nexecute more accurate reasoning in complex and professional domains. However,\nthe vanilla Chain-of-Thought (CoT) prompting method faces challenges in\neffectively leveraging the unique strengths of visual and DSL representations,\nprimarily due to their differing reasoning mechanisms. Additionally, it often\nfalls short in addressing critical steps in multi-step reasoning tasks. To\nmitigate these challenges, we introduce the \\underline{B}i-Modal\n\\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed\nto maximize the potential of DSL in augmenting complex multi-modal reasoning\ntasks. This method initiates by guiding LVLMs to create separate reasoning\nchains for visual and DSL representations. Subsequently, it aligns these chains\nby addressing any inconsistencies, thus achieving a cohesive integration of\nbehaviors from different modalities. Our experiments demonstrate that BBA\nsubstantially improves the performance of GPT-4V(ision) on geometry problem\nsolving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction\n($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to\n83.52\\%$).",
        "pdf_link": "https://arxiv.org/pdf/2402.13577v1.pdf"
    },
    {
        "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Qinlin Zhao",
            "Ruochen Xu",
            "Xing Xie"
        ],
        "published": "2024-02-21T06:46:34Z",
        "summary": "Evaluation of large language models (LLMs) has raised great concerns in the\ncommunity due to the issue of data contamination. Existing work designed\nevaluation protocols using well-defined algorithms for specific tasks, which\ncannot be easily extended to diverse scenarios. Moreover, current evaluation\nbenchmarks can only provide the overall benchmark results and cannot support a\nfine-grained and multifaceted analysis of LLMs' abilities. In this paper, we\npropose meta probing agents (MPA), a general dynamic evaluation protocol\ninspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal\n2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs\nthe probing and judging agents to automatically transform an original\nevaluation problem into a new one following psychometric theory on three basic\ncognitive abilities: language understanding, problem solving, and domain\nknowledge. These basic abilities are also dynamically configurable, allowing\nmultifaceted analysis. We conducted extensive evaluations using MPA and found\nthat most LLMs achieve poorer performance, indicating room for improvement. Our\nmultifaceted analysis demonstrated the strong correlation between the basic\nabilities and an implicit Matthew effect on model size, i.e., larger models\npossess stronger correlations of the abilities. MPA can also be used as a data\naugmentation approach to enhance LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14865v1.pdf"
    },
    {
        "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
        "authors": [
            "Yunxin Li",
            "Xinyu Chen",
            "Baotian Hu",
            "Haoyuan Shi",
            "Min Zhang"
        ],
        "published": "2024-02-21T06:34:46Z",
        "summary": "Evaluating and Rethinking the current landscape of Large Multimodal Models\n(LMMs), we observe that widely-used visual-language projection approaches\n(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet\nignore the visual knowledge-dimension alignment, i.e., connecting visuals to\ntheir relevant knowledge. Visual knowledge plays a significant role in\nanalyzing, inferring, and interpreting information from visuals, helping\nimprove the accuracy of answers to knowledge-based visual questions. In this\npaper, we mainly explore improving LMMs with visual-language knowledge\nalignment, especially aimed at challenging knowledge-based visual question\nanswering (VQA). To this end, we present a Cognitive Visual-Language Mapper\n(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a\nFine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning\nstage. Specifically, we design the VKA based on the interaction between a small\nlanguage model and a visual encoder, training it on collected image-knowledge\npairs to achieve visual knowledge acquisition and projection. FKA is employed\nto distill the fine-grained visual knowledge of an image and inject it into\nLarge Language Models (LLMs). We conduct extensive experiments on\nknowledge-based VQA benchmarks and experimental results show that CVLM\nsignificantly improves the performance of LMMs on knowledge-based VQA (average\ngain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.13561v1.pdf"
    },
    {
        "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
        "authors": [
            "Deuksin Kwon",
            "Emily Weiss",
            "Tara Kulshrestha",
            "Kushal Chawla",
            "Gale M. Lucas",
            "Jonathan Gratch"
        ],
        "published": "2024-02-21T06:11:03Z",
        "summary": "A successful negotiation demands a deep comprehension of the conversation\ncontext, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as\nstrategic reasoning and effective communication, making it challenging for\nautomated systems. Given the remarkable performance of LLMs across a variety of\nNLP tasks, in this work, we aim to understand how LLMs can advance different\naspects of negotiation research, ranging from designing dialogue systems to\nproviding pedagogical feedback and scaling up data collection practices. To\nthis end, we devise a methodology to analyze the multifaceted capabilities of\nLLMs across diverse dialogue scenarios covering all the time stages of a\ntypical negotiation interaction. Our analysis adds to the increasing evidence\nfor the superiority of GPT-4 across various tasks while also providing insights\ninto specific tasks that remain difficult for LLMs. For instance, the models\ncorrelate poorly with human players when making subjective assessments about\nthe negotiation dialogues and often struggle to generate responses that are\ncontextually appropriate as well as strategically advantageous.",
        "pdf_link": "https://arxiv.org/pdf/2402.13550v1.pdf"
    },
    {
        "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning",
        "authors": [
            "Zhipeng Xu",
            "Zhenghao Liu",
            "Yibin Liu",
            "Chenyan Xiong",
            "Yukun Yan",
            "Shuo Wang",
            "Shi Yu",
            "Zhiyuan Liu",
            "Ge Yu"
        ],
        "published": "2024-02-21T06:04:53Z",
        "summary": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large\nLanguage Models (LLMs), aiding in the resolution of knowledge-intensive tasks.\nHowever, current RAG models position LLMs as passive knowledge receptors,\nthereby restricting their capacity for learning and comprehending external\nknowledge. In this paper, we present ActiveRAG, an innovative RAG framework\nthat shifts from passive knowledge acquisition to an active learning mechanism.\nThis approach utilizes the Knowledge Construction mechanism to develop a deeper\nunderstanding of external knowledge by associating it with previously acquired\nor memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism\nto incorporate the outcomes from both chains of thought and knowledge\nconstruction, thereby calibrating the intrinsic cognition of LLMs. Our\nexperimental results demonstrate that ActiveRAG surpasses previous RAG models,\nachieving a 5% improvement on question-answering datasets. All data and codes\nare available at https://github.com/OpenMatch/ActiveRAG.",
        "pdf_link": "https://arxiv.org/pdf/2402.13547v1.pdf"
    },
    {
        "title": "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs",
        "authors": [
            "Yunxin Li",
            "Xinyu Chen",
            "Baotain Hu",
            "Min Zhang"
        ],
        "published": "2024-02-21T05:56:52Z",
        "summary": "Long video understanding is a significant and ongoing challenge in the\nintersection of multimedia and artificial intelligence. Employing large\nlanguage models (LLMs) for comprehending video becomes an emerging and\npromising method. However, this approach incurs high computational costs due to\nthe extensive array of video tokens, experiences reduced visual clarity as a\nconsequence of token aggregation, and confronts challenges arising from\nirrelevant visual tokens while answering video-related questions. To alleviate\nthese issues, we present an Interactive Visual Adapter (IVA) within LLMs,\ndesigned to enhance interaction with fine-grained visual elements.\nSpecifically, we first transform long videos into temporal video tokens via\nleveraging a visual encoder alongside a pretrained causal transformer, then\nfeed them into LLMs with the video instructions. Subsequently, we integrated\nIVA, which contains a lightweight temporal frame selector and a spatial feature\ninteractor, within the internal blocks of LLMs to capture instruction-aware and\nfine-grained visual signals. Consequently, the proposed video-LLM facilitates a\ncomprehensive understanding of long video content through appropriate long\nvideo modeling and precise visual interactions. We conducted extensive\nexperiments on nine video understanding benchmarks and experimental results\nshow that our interactive visual adapter significantly improves the performance\nof video LLMs on long video QA tasks. Ablation studies further verify the\neffectiveness of IVA in long and short video understandings.",
        "pdf_link": "https://arxiv.org/pdf/2402.13546v1.pdf"
    },
    {
        "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
        "authors": [
            "Lingxi Zhang",
            "Yue Yu",
            "Kuan Wang",
            "Chao Zhang"
        ],
        "published": "2024-02-21T05:41:34Z",
        "summary": "Retrieval-augmented generation enhances large language models (LLMs) by\nincorporating relevant information from external knowledge sources. This\nenables LLMs to adapt to specific domains and mitigate hallucinations in\nknowledge-intensive tasks. However, existing retrievers are often misaligned\nwith LLMs due to their separate training processes and the black-box nature of\nLLMs. To address this challenge, we propose ARL2, a retriever learning\ntechnique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and\nscore relevant evidence, enabling learning the retriever from robust LLM\nsupervision. Furthermore, ARL2 uses an adaptive self-training strategy for\ncurating high-quality and diverse relevance data, which can effectively reduce\nthe annotation cost. Extensive experiments demonstrate the effectiveness of\nARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared\nto the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer\nlearning capabilities and strong zero-shot generalization abilities. Our code\nwill be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.",
        "pdf_link": "https://arxiv.org/pdf/2402.13542v1.pdf"
    },
    {
        "title": "Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel",
        "authors": [
            "Jordan Dotzel",
            "Bahaa Kotb",
            "James Dotzel",
            "Mohamed Abdelfattah",
            "Zhiru Zhang"
        ],
        "published": "2024-02-21T05:14:30Z",
        "summary": "Traditional methods, such as JPEG, perform image compression by operating on\nstructural information, such as pixel values or frequency content. These\nmethods are effective to bitrates around one bit per pixel (bpp) and higher at\nstandard image sizes. In contrast, text-based semantic compression directly\nstores concepts and their relationships using natural language, which has\nevolved with humans to efficiently represent these salient concepts. These\nmethods can operate at extremely low bitrates by disregarding structural\ninformation like location, size, and orientation. In this work, we use GPT-4V\nand DALL-E3 from OpenAI to explore the quality-compression frontier for image\ncompression and identify the limitations of current technology. We push\nsemantic compression as low as 100 $\\mu$bpp (up to $10,000\\times$ smaller than\nJPEG) by introducing an iterative reflection process to improve the decoded\nimage. We further hypothesize this 100 $\\mu$bpp level represents a soft limit\non semantic compression at standard image resolutions.",
        "pdf_link": "https://arxiv.org/pdf/2402.13536v1.pdf"
    },
    {
        "title": "FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing",
        "authors": [
            "Xiao-Yang Liu",
            "Jie Zhang",
            "Guoxuan Wang",
            "Weiqing Tong",
            "Anwar Walid"
        ],
        "published": "2024-02-21T05:03:17Z",
        "summary": "Large language models (LLMs) are computationally intensive. The computation\nworkload and the memory footprint grow quadratically with the dimension (layer\nwidth). Most of LLMs' parameters come from the linear layers of the transformer\nstructure and are highly redundant. These linear layers contribute more than\n80% of the computation workload and 99% of the model size. To pretrain and\nfinetune LLMs efficiently, there are three major challenges to address: 1)\nreducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)\nimproving GPU utilization when using distributed training. Prior methods, such\nas LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the\nnumber of trainable parameters and model size, respectively. However, the\nresulting model still consumes a large amount of GPU memory. In this paper, we\npresent high-performance GPU-based methods that exploit low-rank structures to\npretrain and finetune LLMs for financial applications. We replace one\nconventional linear layer of the transformer structure with two narrower linear\nlayers, which allows us to reduce the number of parameters by several orders of\nmagnitude. By quantizing the parameters into low precision (8-bit and 4-bit),\nthe memory consumption of the resulting model is further reduced. Compared with\nexisting LLMs, our methods achieve a speedup of 1.3X and a model compression\nratio of 2.64X for pretaining without accuracy drop. For finetuning, our\nmethods achieve an average accuracy increase of 6.3% and 24.0% in general tasks\nand financial tasks, respectively, and GPU memory consumption ratio of 6.3X.\nThe sizes of our models are smaller than 0.59 GB, allowing inference on a\nsmartphone.",
        "pdf_link": "https://arxiv.org/pdf/2402.13533v1.pdf"
    },
    {
        "title": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models",
        "authors": [
            "Yang Liu",
            "Meng Xu",
            "Shuo Wang",
            "Liner Yang",
            "Haoyu Wang",
            "Zhenghao Liu",
            "Cunliang Kong",
            "Yun Chen",
            "Yang Liu",
            "Maosong Sun",
            "Erhong Yang"
        ],
        "published": "2024-02-21T04:42:41Z",
        "summary": "Modern large language models (LLMs) should generally benefit individuals from\nvarious cultural backgrounds around the world. However, most recent advanced\ngenerative evaluation benchmarks tailed for LLMs mainly focus on English. To\nthis end, we introduce OMGEval, the first Open-source Multilingual Generative\ntest set that can assess the capability of LLMs in different languages. For\neach language, OMGEval provides 804 open-ended questions, covering a wide range\nof important capabilities of LLMs, such as general knowledge, logical\nreasoning, and so on. Each question is rigorously verified by human annotators.\nNotably, to sufficiently reflect the compatibility of LLMs in different\ncultural backgrounds, we perform localization for each non-English language.\nSpecifically, the current version of OMGEval includes 5 languages (i.e., Zh,\nRu, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to\nautomatically score different model outputs, which is shown closely related to\nhuman evaluation. We evaluate several representative multilingual LLMs on the\nproposed OMGEval, which we believe will provide a valuable reference for the\ncommunity to further understand and improve the multilingual capability of\nLLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.",
        "pdf_link": "https://arxiv.org/pdf/2402.13524v1.pdf"
    },
    {
        "title": "AgentScope: A Flexible yet Robust Multi-Agent Platform",
        "authors": [
            "Dawei Gao",
            "Zitao Li",
            "Weirui Kuang",
            "Xuchen Pan",
            "Daoyuan Chen",
            "Zhijian Ma",
            "Bingchen Qian",
            "Liuyi Yao",
            "Lin Zhu",
            "Chen Cheng",
            "Hongzhu Shi",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "published": "2024-02-21T04:11:28Z",
        "summary": "With the rapid advancement of Large Language Models (LLMs), significant\nprogress has been made in multi-agent applications. However, the complexities\nin coordinating agents' cooperation and LLMs' erratic performance pose notable\nchallenges in developing robust and efficient multi-agent applications. To\ntackle these challenges, we propose AgentScope, a developer-centric multi-agent\nplatform with message exchange as its core communication mechanism. Together\nwith abundant syntactic tools, built-in resources, and user-friendly\ninteractions, our communication mechanism significantly reduces the barriers to\nboth development and understanding. Towards robust and flexible multi-agent\napplication, AgentScope provides both built-in and customizable fault tolerance\nmechanisms while it is also armed with system-level supports for multi-modal\ndata generation, storage and transmission. Additionally, we design an\nactor-based distribution framework, enabling easy conversion between local and\ndistributed deployments and automatic parallel optimization without extra\neffort. With these features, AgentScope empowers developers to build\napplications that fully realize the potential of intelligent agents. We have\nreleased AgentScope at https://github.com/modelscope/agentscope, and hope\nAgentScope invites wider participation and innovation in this fast-moving\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2402.14034v1.pdf"
    },
    {
        "title": "RITFIS: Robust input testing framework for LLMs-based intelligent software",
        "authors": [
            "Mingxuan Xiao",
            "Yan Xiao",
            "Hai Dong",
            "Shunhui Ji",
            "Pengcheng Zhang"
        ],
        "published": "2024-02-21T04:00:54Z",
        "summary": "The dependence of Natural Language Processing (NLP) intelligent software on\nLarge Language Models (LLMs) is increasingly prominent, underscoring the\nnecessity for robustness testing. Current testing methods focus solely on the\nrobustness of LLM-based software to prompts. Given the complexity and diversity\nof real-world inputs, studying the robustness of LLMbased software in handling\ncomprehensive inputs (including prompts and examples) is crucial for a thorough\nunderstanding of its performance.\n  To this end, this paper introduces RITFIS, a Robust Input Testing Framework\nfor LLM-based Intelligent Software. To our knowledge, RITFIS is the first\nframework designed to assess the robustness of LLM-based intelligent software\nagainst natural language inputs. This framework, based on given threat models\nand prompts, primarily defines the testing process as a combinatorial\noptimization problem. Successful test cases are determined by a goal function,\ncreating a transformation space for the original examples through perturbation\nmeans, and employing a series of search methods to filter cases that meet both\nthe testing objectives and language constraints. RITFIS, with its modular\ndesign, offers a comprehensive method for evaluating the robustness of LLMbased\nintelligent software.\n  RITFIS adapts 17 automated testing methods, originally designed for Deep\nNeural Network (DNN)-based intelligent software, to the LLM-based software\ntesting scenario. It demonstrates the effectiveness of RITFIS in evaluating\nLLM-based intelligent software through empirical validation. However, existing\nmethods generally have limitations, especially when dealing with lengthy texts\nand structurally complex threat models. Therefore, we conducted a comprehensive\nanalysis based on five metrics and provided insightful testing method\noptimization strategies, benefiting both researchers and everyday users.",
        "pdf_link": "https://arxiv.org/pdf/2402.13518v1.pdf"
    },
    {
        "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks",
        "authors": [
            "Canaan Yung",
            "Hadi Mohaghegh Dolatabadi",
            "Sarah Erfani",
            "Christopher Leckie"
        ],
        "published": "2024-02-21T03:59:52Z",
        "summary": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence",
        "pdf_link": "https://arxiv.org/pdf/2402.13517v1.pdf"
    },
    {
        "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
        "authors": [
            "Chenyang Song",
            "Xu Han",
            "Zhengyan Zhang",
            "Shengding Hu",
            "Xiyu Shi",
            "Kuai Li",
            "Chen Chen",
            "Zhiyuan Liu",
            "Guangli Li",
            "Tao Yang",
            "Maosong Sun"
        ],
        "published": "2024-02-21T03:58:49Z",
        "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, it has been proven a\npromising paradigm to boost model inference efficiency. Nevertheless, most\nlarge language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces an effective sparsification method named \"ProSparse\" to push\nLLMs for higher activation sparsity without decreasing model performance.\nSpecifically, after substituting the activation function of LLMs with ReLU,\nProSparse adopts progressive sparsity regularization with a factor smoothly\nincreasing along sine curves in multiple stages. This can enhance activation\nsparsity and alleviate performance degradation by avoiding radical shifts in\nactivation distribution. With ProSparse, we obtain high sparsity of 89.32% and\n88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable\nperformance to their original Swish-activated versions. Our inference\nacceleration experiments further demonstrate the practical acceleration brought\nby higher activation sparsity.",
        "pdf_link": "https://arxiv.org/pdf/2402.13516v2.pdf"
    },
    {
        "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
        "authors": [
            "M. Emrullah Ildiz",
            "Yixiao Huang",
            "Yingcong Li",
            "Ankit Singh Rawat",
            "Samet Oymak"
        ],
        "published": "2024-02-21T03:51:34Z",
        "summary": "Modern language models rely on the transformer architecture and attention\nmechanism to perform language understanding and text generation. In this work,\nwe study learning a 1-layer self-attention model from a set of prompts and\nassociated output data sampled from the model. We first establish a precise\nmapping between the self-attention mechanism and Markov models: Inputting a\nprompt to the model samples the output token according to a context-conditioned\nMarkov chain (CCMC) which weights the transition matrix of a base Markov chain.\nAdditionally, incorporating positional encoding results in position-dependent\nscaling of the transition probabilities. Building on this formalism, we develop\nidentifiability/coverage conditions for the prompt distribution that guarantee\nconsistent estimation and establish sample complexity guarantees under IID\nsamples. Finally, we study the problem of learning from a single output\ntrajectory generated from an initial prompt. We characterize an intriguing\nwinner-takes-all phenomenon where the generative process implemented by\nself-attention collapses into sampling a limited subset of tokens due to its\nnon-mixing nature. This provides a mathematical explanation to the tendency of\nmodern LLMs to generate repetitive text. In summary, the equivalence to CCMC\nprovides a simple but powerful framework to study self-attention and its\nproperties.",
        "pdf_link": "https://arxiv.org/pdf/2402.13512v1.pdf"
    },
    {
        "title": "The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2024-02-21T03:21:14Z",
        "summary": "Automated lay summarisation (LS) aims to simplify complex technical documents\ninto a more accessible format to non-experts. Existing approaches using\npre-trained language models, possibly augmented with external background\nknowledge, tend to struggle with effective simplification and explanation.\nMoreover, automated methods that can effectively assess the `layness' of\ngenerated summaries are lacking. Recently, large language models (LLMs) have\ndemonstrated a remarkable capacity for text simplification, background\ninformation generation, and text evaluation. This has motivated our systematic\nexploration into using LLMs to generate and evaluate lay summaries of\nbiomedical articles. We propose a novel \\textit{Explain-then-Summarise} LS\nframework, which leverages LLMs to generate high-quality background knowledge\nto improve supervised LS. We also evaluate the performance of LLMs for\nzero-shot LS and propose two novel LLM-based LS evaluation metrics, which\nassess layness from multiple perspectives. Finally, we conduct a human\nassessment of generated lay summaries. Our experiments reveal that\nLLM-generated background information can support improved supervised LS.\nFurthermore, our novel zero-shot LS evaluation metric demonstrates a high\ndegree of alignment with human preferences. We conclude that LLMs have an\nimportant part to play in improving both the performance and evaluation of LS\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.13498v1.pdf"
    },
    {
        "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis",
        "authors": [
            "Yueqi Xie",
            "Minghong Fang",
            "Renjie Pi",
            "Neil Gong"
        ],
        "published": "2024-02-21T03:09:21Z",
        "summary": "Large Language Models (LLMs) face threats from unsafe prompts. Existing\nmethods for detecting unsafe prompts are primarily online moderation APIs or\nfinetuned LLMs. These strategies, however, often require extensive and\nresource-intensive data collection and training processes. In this study, we\npropose GradSafe, which effectively detects unsafe prompts by scrutinizing the\ngradients of safety-critical parameters in LLMs. Our methodology is grounded in\na pivotal observation: the gradients of an LLM's loss for unsafe prompts paired\nwith compliance response exhibit similar patterns on certain safety-critical\nparameters. In contrast, safe prompts lead to markedly different gradient\npatterns. Building on this observation, GradSafe analyzes the gradients from\nprompts (paired with compliance responses) to accurately detect unsafe prompts.\nWe show that GradSafe, applied to Llama-2 without further training, outperforms\nLlama Guard, despite its extensive finetuning with a large dataset, in\ndetecting unsafe prompts. This superior performance is consistent across both\nzero-shot and adaptation scenarios, as evidenced by our evaluations on the\nToxicChat and XSTest. The source code is available at\nhttps://github.com/xyq7/GradSafe.",
        "pdf_link": "https://arxiv.org/pdf/2402.13494v1.pdf"
    },
    {
        "title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models",
        "authors": [
            "Seiji Maekawa",
            "Hayate Iso",
            "Sairam Gurajada",
            "Nikita Bhutani"
        ],
        "published": "2024-02-21T03:05:50Z",
        "summary": "While large language models (LMs) demonstrate remarkable performance, they\nencounter challenges in providing accurate responses when queried for\ninformation beyond their pre-trained memorization. Although augmenting them\nwith relevant external information can mitigate these issues, failure to\nconsider the necessity of retrieval may adversely affect overall performance.\nPrevious research has primarily focused on examining how entities influence\nretrieval models and knowledge recall in LMs, leaving other aspects relatively\nunexplored. In this work, our goal is to offer a more detailed, fact-centric\nanalysis by exploring the effects of combinations of entities and relations. To\nfacilitate this, we construct a new question answering (QA) dataset called\nWiTQA (Wikipedia Triple Question Answers). This dataset includes questions\nabout entities and relations of various popularity levels, each accompanied by\na supporting passage. Our extensive experiments with diverse LMs and retrievers\nreveal when retrieval does not consistently enhance LMs from the viewpoints of\nfact-centric popularity. Confirming earlier findings, we observe that larger\nLMs excel in recalling popular facts. However, they notably encounter\ndifficulty with infrequent entity-relation pairs compared to retrievers.\nInterestingly, they can effectively retain popular relations of less common\nentities. We demonstrate the efficacy of our finer-grained metric and insights\nthrough an adaptive retrieval system that selectively employs retrieval and\nrecall based on the frequencies of entities and relations in the question.",
        "pdf_link": "https://arxiv.org/pdf/2402.13492v3.pdf"
    },
    {
        "title": "ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding",
        "authors": [
            "Shuzhang Zhong",
            "Zebin Yang",
            "Meng Li",
            "Ruihao Gong",
            "Runsheng Wang",
            "Ru Huang"
        ],
        "published": "2024-02-21T02:51:07Z",
        "summary": "Recent advancements in generative large language models (LLMs) have\nsignificantly boosted the performance in natural language processing tasks.\nHowever, their efficiency is hampered by the inherent limitations in\nautoregressive token generation. While parallel decoding with token tree\nverification, e.g., Medusa, has been proposed to improve decoding parallelism\nand efficiency, it often struggles with maintaining contextual relationships\ndue to its independent token prediction approach and incurs significant\nverification overhead, especially with large tree sizes and batch processing.\nIn this paper, we propose ProPD, an efficient LLM parallel decoding framework\nbased on dynamic token tree pruning and generation. ProPD features an advanced\nearly pruning mechanism to efficiently eliminate unpromising token sequences to\nimprove verification efficiency. Additionally, it introduces a dynamic token\ntree generation algorithm to balance the computation and parallelism of the\nverification phase in real-time and maximize the overall efficiency across\ndifferent batch sizes, sequence lengths, and tasks, etc. We verify ProPD across\na diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD\nconsistently outperforms existing decoding algorithms by 1.1-3.2x.",
        "pdf_link": "https://arxiv.org/pdf/2402.13485v1.pdf"
    },
    {
        "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
        "authors": [
            "Minju Seo",
            "Jinheon Baek",
            "James Thorne",
            "Sung Ju Hwang"
        ],
        "published": "2024-02-21T02:45:46Z",
        "summary": "Despite large successes of recent language models on diverse tasks, they\nsuffer from severe performance degeneration in low-resource settings with\nlimited training data available. Many existing works tackle this problem by\ngenerating synthetic data from the training data and then training models on\nthem, recently using Large Language Models (LLMs). However, in low-resource\nsettings, the amount of seed data samples to use for data augmentation is very\nsmall, which makes generated samples suboptimal and less diverse. To tackle\nthis challenge, we propose a novel method that augments training data by\nincorporating a wealth of examples from other datasets, along with the given\ntraining data. Specifically, we first retrieve the relevant instances from\nother datasets, such as their input-output pairs or contexts, based on their\nsimilarities with the given seed data, and then prompt LLMs to generate new\nsamples with the contextual information within and across the original and\nretrieved samples. This approach can ensure that the generated data is not only\nrelevant but also more diverse than what could be achieved using the limited\nseed data alone. We validate our proposed Retrieval-Augmented Data Augmentation\n(RADA) framework on multiple datasets under low-resource settings of training\nand test-time data augmentation scenarios, on which it outperforms existing\nLLM-powered data augmentation baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.13482v1.pdf"
    },
    {
        "title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning",
        "authors": [
            "Nathan Beck",
            "Adithya Iyer",
            "Rishabh Iyer"
        ],
        "published": "2024-02-21T01:54:58Z",
        "summary": "As supervised fine-tuning of pre-trained models within NLP applications\nincreases in popularity, larger corpora of annotated data are required,\nespecially with increasing parameter counts in large language models. Active\nlearning, which attempts to mine and annotate unlabeled instances to improve\nmodel performance maximally fast, is a common choice for reducing the\nannotation cost; however, most methods typically ignore class imbalance and\neither assume access to initial annotated data or require multiple rounds of\nactive learning selection before improving rare classes. We present STENCIL,\nwhich utilizes a set of text exemplars and the recently proposed submodular\nmutual information to select a set of weakly labeled rare-class instances that\nare then strongly labeled by an annotator. We show that STENCIL improves\noverall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on\nmultiple text classification datasets over common active learning methods\nwithin the class-imbalanced cold-start setting.",
        "pdf_link": "https://arxiv.org/pdf/2402.13468v1.pdf"
    },
    {
        "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models",
        "authors": [
            "Jianhao Yan",
            "Yun Luo",
            "Yue Zhang"
        ],
        "published": "2024-02-21T01:39:56Z",
        "summary": "The application scope of large language models (LLMs) is increasingly\nexpanding. In practical use, users might provide feedback based on the model's\noutput, hoping for a responsive model that can complete responses according to\ntheir feedback. Whether the model can appropriately respond to users' refuting\nfeedback and consistently follow through with execution has not been thoroughly\nanalyzed. In light of this, this paper proposes a comprehensive benchmark,\nRefuteBench, covering tasks such as question answering, machine translation,\nand email writing. The evaluation aims to assess whether models can positively\naccept feedback in form of refuting instructions and whether they can\nconsistently adhere to user demands throughout the conversation. We conduct\nevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit\ninclination to their internal knowledge, often failing to comply with user\nfeedback. Additionally, as the length of the conversation increases, models\ngradually forget the user's stated feedback and roll back to their own\nresponses. We further propose a recall-and-repeat prompts as a simple and\neffective way to enhance the model's responsiveness to feedback.",
        "pdf_link": "https://arxiv.org/pdf/2402.13463v2.pdf"
    },
    {
        "title": "Potential and Challenges of Model Editing for Social Debiasing",
        "authors": [
            "Jianhao Yan",
            "Futing Wang",
            "Yafu Li",
            "Yue Zhang"
        ],
        "published": "2024-02-21T01:35:26Z",
        "summary": "Large language models (LLMs) trained on vast corpora suffer from inevitable\nstereotype biases. Mitigating these biases with fine-tuning could be both\ncostly and data-hungry. Model editing methods, which focus on modifying LLMs in\na post-hoc manner, are of great potential to address debiasing. However, it\nlacks a comprehensive study that facilitates both internal and external model\nediting methods, supports various bias types, as well as understands the pros\nand cons of applying editing methods to stereotypical debiasing. To mitigate\nthis gap, we carefully formulate social debiasing into an editing problem and\nbenchmark seven existing model editing algorithms on stereotypical debiasing,\ni.e., debias editing. Our findings in three scenarios reveal both the potential\nand challenges of debias editing: (1) Existing model editing methods can\neffectively preserve knowledge and mitigate biases, while the generalization of\ndebias effect from edited sentences to semantically equivalent sentences is\nlimited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et\nal. 2022b), while internal editing methods degenerate with the number of edits.\n(3) Model editing algorithms achieve generalization towards unseen biases both\nwithin the same type and from different types. In light of these findings, we\nfurther propose two simple but effective methods to improve debias editing, and\nexperimentally show the effectiveness of the proposed methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.13462v1.pdf"
    },
    {
        "title": "Learning to Poison Large Language Models During Instruction Tuning",
        "authors": [
            "Yao Qiang",
            "Xiangyu Zhou",
            "Saleh Zare Zade",
            "Mohammad Amin Roshani",
            "Douglas Zytko",
            "Dongxiao Zhu"
        ],
        "published": "2024-02-21T01:30:03Z",
        "summary": "The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning approach to identify adversarial triggers efficiently, ensuring an\nevasion of detection by conventional defenses while maintaining content\nintegrity. Through experimental validation across various LLMs and tasks, our\nstrategy demonstrates a high success rate in compromising model outputs;\npoisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance\nDrop Rate (PDR) of around 80\\%. Our work highlights the need for stronger\ndefenses against data poisoning attack, offering insights into safeguarding\nLLMs against these more sophisticated attacks. The source code can be found on\nthis GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.",
        "pdf_link": "https://arxiv.org/pdf/2402.13459v1.pdf"
    },
    {
        "title": "LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study",
        "authors": [
            "Zihao Xu",
            "Yi Liu",
            "Gelei Deng",
            "Yuekang Li",
            "Stjepan Picek"
        ],
        "published": "2024-02-21T01:26:39Z",
        "summary": "Large Language Models (LLMS) have increasingly become central to generating\ncontent with potential societal impacts. Notably, these models have\ndemonstrated capabilities for generating content that could be deemed harmful.\nTo mitigate these risks, researchers have adopted safety training techniques to\nalign model outputs with societal values to curb the generation of malicious\ncontent. However, the phenomenon of \"jailbreaking\", where carefully crafted\nprompts elicit harmful responses from models, persists as a significant\nchallenge. This research conducts a comprehensive analysis of existing studies\non jailbreaking LLMs and their defense techniques. We meticulously investigate\nnine attack techniques and seven defense techniques applied across three\ndistinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate\nthe effectiveness of these attack and defense techniques. Our findings reveal\nthat existing white-box attacks underperform compared to universal techniques\nand that including special tokens in the input significantly affects the\nlikelihood of successful attacks. This research highlights the need to\nconcentrate on the security facets of LLMs. Additionally, we contribute to the\nfield by releasing our datasets and testing framework, aiming to foster further\nresearch into LLM security. We believe these contributions will facilitate the\nexploration of security measures within this domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.13457v1.pdf"
    },
    {
        "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
        "authors": [
            "Zexue He",
            "Leonid Karlinsky",
            "Donghyun Kim",
            "Julian McAuley",
            "Dmitry Krotov",
            "Rogerio Feris"
        ],
        "published": "2024-02-21T01:00:17Z",
        "summary": "Large Language Models (LLMs) struggle to handle long input sequences due to\nhigh memory and runtime costs. Memory-augmented models have emerged as a\npromising solution to this problem, but current methods are hindered by limited\nmemory capacity and require costly re-training to integrate with a new LLM. In\nthis work, we introduce an associative memory module which can be coupled to\nany pre-trained (frozen) attention-based LLM without re-training, enabling it\nto handle arbitrarily long input sequences. Unlike previous methods, our\nassociative memory module consolidates representations of individual tokens\ninto a non-parametric distribution model, dynamically managed by properly\nbalancing the novelty and recency of the incoming data. By retrieving\ninformation from this consolidated associative memory, the base LLM can achieve\nsignificant (up to 29.7% on Arxiv) perplexity reduction in long-context\nmodeling compared to other baselines evaluated on standard benchmarks. This\narchitecture, which we call CAMELoT (Consolidated Associative Memory Enhanced\nLong Transformer), demonstrates superior performance even with a tiny context\nwindow of 128 tokens, and also enables improved in-context learning with a much\nlarger set of demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2402.13449v1.pdf"
    },
    {
        "title": "Ranking Large Language Models without Ground Truth",
        "authors": [
            "Amit Dhurandhar",
            "Rahul Nair",
            "Moninder Singh",
            "Elizabeth Daly",
            "Karthikeyan Natesan Ramamurthy"
        ],
        "published": "2024-02-21T00:49:43Z",
        "summary": "Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.",
        "pdf_link": "https://arxiv.org/pdf/2402.14860v2.pdf"
    },
    {
        "title": "Large Language Models for Data Annotation: A Survey",
        "authors": [
            "Zhen Tan",
            "Alimohammad Beigi",
            "Song Wang",
            "Ruocheng Guo",
            "Amrita Bhattacharjee",
            "Bohan Jiang",
            "Mansooreh Karami",
            "Jundong Li",
            "Lu Cheng",
            "Huan Liu"
        ],
        "published": "2024-02-21T00:44:04Z",
        "summary": "Data annotation is the labeling or tagging of raw data with relevant\ninformation, essential for improving the efficacy of machine learning models.\nThe process, however, is labor-intensive and expensive. The emergence of\nadvanced Large Language Models (LLMs), exemplified by GPT-4, presents an\nunprecedented opportunity to revolutionize and automate the intricate process\nof data annotation. While existing surveys have extensively covered LLM\narchitecture, training, and general applications, this paper uniquely focuses\non their specific utility for data annotation. This survey contributes to three\ncore aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations,\nand Learning with LLM-generated annotations. Furthermore, the paper includes an\nin-depth taxonomy of methodologies employing LLMs for data annotation, a\ncomprehensive review of learning strategies for models incorporating\nLLM-generated annotations, and a detailed discussion on primary challenges and\nlimitations associated with using LLMs for data annotation. As a key guide,\nthis survey aims to direct researchers and practitioners in exploring the\npotential of the latest LLMs for data annotation, fostering future advancements\nin this critical domain. We provide a comprehensive papers list at\n\\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.",
        "pdf_link": "https://arxiv.org/pdf/2402.13446v1.pdf"
    },
    {
        "title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Oumaima El Khettari",
            "Mickael Rouvier",
            "Pacome Constant dit Beaufils",
            "Natalia Grabar",
            "Beatrice Daille",
            "Solen Quiniou",
            "Emmanuel Morin",
            "Pierre-Antoine Gourraud",
            "Richard Dufour"
        ],
        "published": "2024-02-20T23:54:02Z",
        "summary": "The biomedical domain has sparked a significant interest in the field of\nNatural Language Processing (NLP), which has seen substantial advancements with\npre-trained language models (PLMs). However, comparing these models has proven\nchallenging due to variations in evaluation protocols across different models.\nA fair solution is to aggregate diverse downstream tasks into a benchmark,\nallowing for the assessment of intrinsic PLMs qualities from various\nperspectives. Although still limited to few languages, this initiative has been\nundertaken in the biomedical field, notably English and Chinese. This\nlimitation hampers the evaluation of the latest French biomedical models, as\nthey are either assessed on a minimal number of tasks with non-standardized\nprotocols or evaluated using general downstream tasks. To bridge this research\ngap and account for the unique sensitivities of French, we present the\nfirst-ever publicly available French biomedical language understanding\nbenchmark called DrBenchmark. It encompasses 20 diversified tasks, including\nnamed-entity recognition, part-of-speech tagging, question-answering, semantic\ntextual similarity, and classification. We evaluate 8 state-of-the-art\npre-trained masked language models (MLMs) on general and biomedical-specific\ndata, as well as English specific MLMs to assess their cross-lingual\ncapabilities. Our experiments reveal that no single model excels across all\ntasks, while generalist models are sometimes still competitive.",
        "pdf_link": "https://arxiv.org/pdf/2402.13432v1.pdf"
    },
    {
        "title": "Explaining Relationships Among Research Papers",
        "authors": [
            "Xiangci Li",
            "Jessica Ouyang"
        ],
        "published": "2024-02-20T23:38:39Z",
        "summary": "Due to the rapid pace of research publications, keeping up to date with all\nthe latest related papers is very time-consuming, even with daily feed tools.\nThere is a need for automatically generated, short, customized literature\nreviews of sets of papers to help researchers decide what to read. While\nseveral works in the last decade have addressed the task of explaining a single\nresearch paper, usually in the context of another paper citing it, the\nrelationship among multiple papers has been ignored; prior works have focused\non generating a single citation sentence in isolation, without addressing the\nexpository and transition sentences needed to connect multiple papers in a\ncoherent story. In this work, we explore a feature-based, LLM-prompting\napproach to generate richer citation texts, as well as generating multiple\ncitations at once to capture the complex relationships among research papers.\nWe perform an expert evaluation to investigate the impact of our proposed\nfeatures on the quality of the generated paragraphs and find a strong\ncorrelation between human preference and integrative writing style, suggesting\nthat humans prefer high-level, abstract citations, with transition sentences\nbetween them to provide an overall story.",
        "pdf_link": "https://arxiv.org/pdf/2402.13426v1.pdf"
    },
    {
        "title": "The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative",
        "authors": [
            "Zhen Tan",
            "Chengshuai Zhao",
            "Raha Moraffah",
            "Yifan Li",
            "Yu Kong",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "published": "2024-02-20T23:08:21Z",
        "summary": "Due to their unprecedented ability to process and respond to various types of\ndata, Multimodal Large Language Models (MLLMs) are constantly defining the new\nboundary of Artificial General Intelligence (AGI). As these advanced generative\nmodels increasingly form collaborative networks for complex tasks, the\nintegrity and security of these systems are crucial. Our paper, ``The Wolf\nWithin'', explores a novel vulnerability in MLLM societies - the indirect\npropagation of malicious content. Unlike direct harmful output generation for\nMLLMs, our research demonstrates how a single MLLM agent can be subtly\ninfluenced to generate prompts that, in turn, induce other MLLM agents in the\nsociety to output malicious content. This subtle, yet potent method of indirect\ninfluence marks a significant escalation in the security risks associated with\nMLLMs. Our findings reveal that, with minimal or even no access to MLLMs'\nparameters, an MLLM agent, when manipulated to produce specific prompts or\ninstructions, can effectively ``infect'' other agents within a society of\nMLLMs. This infection leads to the generation and circulation of harmful\noutputs, such as dangerous instructions or misinformation, across the society.\nWe also show the transferability of these indirectly generated prompts,\nhighlighting their possibility in propagating malice through inter-agent\ncommunication. This research provides a critical insight into a new dimension\nof threat posed by MLLMs, where a single agent can act as a catalyst for\nwidespread malevolent influence. Our work underscores the urgent need for\ndeveloping robust mechanisms to detect and mitigate such covert manipulations\nwithin MLLM societies, ensuring their safe and ethical utilization in societal\napplications. Our implementation is released at\n\\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.",
        "pdf_link": "https://arxiv.org/pdf/2402.14859v1.pdf"
    },
    {
        "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
        "authors": [
            "Kewei Cheng",
            "Nesreen K. Ahmed",
            "Theodore Willke",
            "Yizhou Sun"
        ],
        "published": "2024-02-20T22:56:23Z",
        "summary": "Although Large Language Models (LLMs) excel at addressing straightforward\nreasoning tasks, they frequently struggle with difficulties when confronted by\nmore complex multi-step reasoning due to a range of factors. Firstly, natural\nlanguage often encompasses complex relationships among entities, making it\nchallenging to maintain a clear reasoning chain over longer spans. Secondly,\nthe abundance of linguistic diversity means that the same entities and\nrelationships can be expressed using different terminologies and structures,\ncomplicating the task of identifying and establishing connections between\nmultiple pieces of information. Graphs provide an effective solution to\nrepresent data rich in relational information and capture long-term\ndependencies among entities. To harness the potential of graphs, our paper\nintroduces Structure Guided Prompt, an innovative three-stage task-agnostic\nprompting framework designed to improve the multi-step reasoning capabilities\nof LLMs in a zero-shot setting. This framework explicitly converts unstructured\ntext into a graph via LLMs and instructs them to navigate this graph using\ntask-specific strategies to formulate responses. By effectively organizing\ninformation and guiding navigation, it enables LLMs to provide more accurate\nand context-aware responses. Our experiments show that this framework\nsignificantly enhances the reasoning capabilities of LLMs, enabling them to\nexcel in a broader spectrum of natural language scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13415v1.pdf"
    },
    {
        "title": "Harnessing Large Language Models as Post-hoc Correctors",
        "authors": [
            "Zhiqiang Zhong",
            "Kuangyu Zhou",
            "Davide Mottin"
        ],
        "published": "2024-02-20T22:50:41Z",
        "summary": "As Machine Learning (ML) models grow in size and demand higher-quality\ntraining data, the expenses associated with re-training and fine-tuning these\nmodels are escalating rapidly. Inspired by recent impressive achievements of\nLarge Language Models (LLMs) in different fields, this paper delves into the\nquestion: can LLMs efficiently improve an ML's performance at a minimal cost?\nWe show that, through our proposed training-free framework LlmCorr, an LLM can\nwork as a post-hoc corrector to propose corrections for the predictions of an\narbitrary ML model. In particular, we form a contextual knowledge database by\nincorporating the dataset's label information and the ML model's predictions on\nthe validation dataset. Leveraging the in-context learning capability of LLMs,\nwe ask the LLM to summarise the instances in which the ML model makes mistakes\nand the correlation between primary predictions and true labels. Following\nthis, the LLM can transfer its acquired knowledge to suggest corrections for\nthe ML model's predictions. Our experimental results on the challenging\nmolecular predictions show that LlmCorr improves the performance of a number of\nmodels by up to 39%.",
        "pdf_link": "https://arxiv.org/pdf/2402.13414v1.pdf"
    },
    {
        "title": "Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems",
        "authors": [
            "Ivan Sekuli\u0107",
            "Silvia Terragni",
            "Victor Guimar\u00e3es",
            "Nghia Khau",
            "Bruna Guedes",
            "Modestas Filipavicius",
            "Andr\u00e9 Ferreira Manso",
            "Roland Mathis"
        ],
        "published": "2024-02-20T20:57:47Z",
        "summary": "In the realm of dialogue systems, user simulation techniques have emerged as\na game-changer, redefining the evaluation and enhancement of task-oriented\ndialogue (TOD) systems. These methods are crucial for replicating real user\ninteractions, enabling applications like synthetic data augmentation, error\ndetection, and robust evaluation. However, existing approaches often rely on\nrigid rule-based methods or on annotated data. This paper introduces DAUS, a\nDomain-Aware User Simulator. Leveraging large language models, we fine-tune\nDAUS on real examples of task-oriented dialogues. Results on two relevant\nbenchmarks showcase significant improvements in terms of user goal fulfillment.\nNotably, we have observed that fine-tuning enhances the simulator's coherence\nwith user goals, effectively mitigating hallucinations -- a major source of\ninconsistencies in simulator responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.13374v1.pdf"
    },
    {
        "title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries",
        "authors": [
            "Jing Han Sun",
            "Ali Emami"
        ],
        "published": "2024-02-20T20:53:24Z",
        "summary": "While Large Language Models (LLMs) excel at the Winograd Schema Challenge\n(WSC), a coreference resolution task testing common-sense reasoning through\npronoun disambiguation, they struggle with instances that feature minor\nalterations or rewording. To address this, we introduce EvoGrad, an open-source\nplatform that harnesses a human-in-the-loop approach to create a dynamic\ndataset tailored to such altered WSC instances. Leveraging ChatGPT's\ncapabilities, we expand our task instances from 182 to 3,691, setting a new\nbenchmark for diverse common-sense reasoning datasets. Additionally, we\nintroduce the error depth metric, assessing model stability in dynamic tasks.\nOur results emphasize the challenge posed by EvoGrad: Even the best performing\nLLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,\na stark contrast to human performance of 92. 8% accuracy without perturbation\nerrors. This highlights ongoing model limitations and the value of dynamic\ndatasets in uncovering them.",
        "pdf_link": "https://arxiv.org/pdf/2402.13372v2.pdf"
    },
    {
        "title": "ChatEL: Entity Linking with Chatbots",
        "authors": [
            "Yifan Ding",
            "Qingkai Zeng",
            "Tim Weninger"
        ],
        "published": "2024-02-20T20:52:57Z",
        "summary": "Entity Linking (EL) is an essential and challenging task in natural language\nprocessing that seeks to link some text representing an entity within a\ndocument or sentence with its corresponding entry in a dictionary or knowledge\nbase. Most existing approaches focus on creating elaborate contextual models\nthat look for clues the words surrounding the entity-text to help solve the\nlinking problem. Although these fine-tuned language models tend to work, they\ncan be unwieldy, difficult to train, and do not transfer well to other domains.\nFortunately, Large Language Models (LLMs) like GPT provide a highly-advanced\nsolution to the problems inherent in EL models, but simply naive prompts to\nLLMs do not work well. In the present work, we define ChatEL, which is a\nthree-step framework to prompt LLMs to return accurate results. Overall the\nChatEL framework improves the average F1 performance across 10 datasets by more\nthan 2%. Finally, a thorough error analysis shows many instances with the\nground truth labels were actually incorrect, and the labels predicted by ChatEL\nwere actually correct. This indicates that the quantitative results presented\nin this paper may be a conservative estimate of the actual performance. All\ndata and code are available as an open-source package on GitHub at\nhttps://github.com/yifding/In_Context_EL.",
        "pdf_link": "https://arxiv.org/pdf/2402.14858v1.pdf"
    },
    {
        "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
        "authors": [
            "Yinghao Li",
            "Rampi Ramprasad",
            "Chao Zhang"
        ],
        "published": "2024-02-20T20:42:02Z",
        "summary": "Large language models (LLMs) have demonstrated impressive abilities in\ngenerating unstructured natural language according to instructions. However,\ntheir performance can be inconsistent when tasked with producing text that\nadheres to specific structured formats, which is crucial in applications like\nnamed entity recognition (NER) or relation extraction (RE). To address this\nissue, this paper introduces an efficient method, G&O, to enhance their\nstructured text generation capabilities. It breaks the generation into a\ntwo-step pipeline: initially, LLMs generate answers in natural language as\nintermediate responses. Subsequently, LLMs are asked to organize the output\ninto the desired structure, using the intermediate responses as context. G&O\neffectively separates the generation of content from the structuring process,\nreducing the pressure of completing two orthogonal tasks simultaneously. Tested\non zero-shot NER and RE, the results indicate a significant improvement in LLM\nperformance with minimal additional efforts. This straightforward and adaptable\nprompting technique can also be combined with other strategies, like\nself-consistency, to further elevate LLM capabilities in various structured\ntext generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.13364v1.pdf"
    },
    {
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "authors": [
            "Liyan Tang",
            "Igor Shalyminov",
            "Amy Wing-mei Wong",
            "Jon Burnsky",
            "Jake W. Vincent",
            "Yu'an Yang",
            "Siffi Singh",
            "Song Feng",
            "Hwanjun Song",
            "Hang Su",
            "Lijia Sun",
            "Yi Zhang",
            "Saab Mansour",
            "Kathleen McKeown"
        ],
        "published": "2024-02-20T18:58:49Z",
        "summary": "Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.",
        "pdf_link": "https://arxiv.org/pdf/2402.13249v2.pdf"
    },
    {
        "title": "Investigating Cultural Alignment of Large Language Models",
        "authors": [
            "Badr AlKhamissi",
            "Muhammad ElNokrashy",
            "Mai AlKhamissi",
            "Mona Diab"
        ],
        "published": "2024-02-20T18:47:28Z",
        "summary": "The intricate relationship between language and culture has long been a\nsubject of exploration within the realm of linguistic anthropology. Large\nLanguage Models (LLMs), promoted as repositories of collective human knowledge,\nraise a pivotal question: do these models genuinely encapsulate the diverse\nknowledge adopted by different cultures? Our study reveals that these models\ndemonstrate greater cultural alignment along two dimensions -- firstly, when\nprompted with the dominant language of a specific culture, and secondly, when\npretrained with a refined mixture of languages employed by that culture. We\nquantify cultural alignment by simulating sociological surveys, comparing model\nresponses to those of actual survey participants as references. Specifically,\nwe replicate a survey conducted in various regions of Egypt and the United\nStates through prompting LLMs with different pretraining data mixtures in both\nArabic and English with the personas of the real respondents and the survey\nquestions. Further analysis reveals that misalignment becomes more pronounced\nfor underrepresented personas and for culturally sensitive topics, such as\nthose probing social values. Finally, we introduce Anthropological Prompting, a\nnovel method leveraging anthropological reasoning to enhance cultural\nalignment. Our study emphasizes the necessity for a more balanced multilingual\npretraining dataset to better represent the diversity of human experience and\nthe plurality of different cultures with many implications on the topic of\ncross-lingual transfer.",
        "pdf_link": "https://arxiv.org/pdf/2402.13231v1.pdf"
    },
    {
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
            "Arka Pal",
            "Deep Karkhanis",
            "Samuel Dooley",
            "Manley Roberts",
            "Siddartha Naidu",
            "Colin White"
        ],
        "published": "2024-02-20T18:42:34Z",
        "summary": "Direct Preference Optimisation (DPO) is effective at significantly improving\nthe performance of large language models (LLMs) on downstream tasks such as\nreasoning, summarisation, and alignment. Using pairs of preferred and\ndispreferred data, DPO models the \\textit{relative} probability of picking one\nresponse over another. In this work, first we show theoretically that the\nstandard DPO loss can lead to a \\textit{reduction} of the model's likelihood of\nthe preferred examples, as long as the relative probability between the\npreferred and dispreferred classes increases. We then show empirically that\nthis phenomenon occurs when fine-tuning LLMs on common datasets, especially\ndatasets in which the edit distance between pairs of completions is low. Using\nthese insights, we design DPO-Positive (DPOP), a new loss function and training\nprocedure which avoids this failure mode. Surprisingly, we also find that DPOP\nsignificantly outperforms DPO across a wide variety of datasets and downstream\ntasks, including datasets with high edit distances between completions. By\nfine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which\nachieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly\n2\\% better than any other open-source model on the HuggingFace Open LLM\nLeaderboard and becomes the first open-source LLM to surpass an average\naccuracy of 80\\%.",
        "pdf_link": "https://arxiv.org/pdf/2402.13228v1.pdf"
    },
    {
        "title": "RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian",
        "authors": [
            "Adrian Cosma",
            "Bogdan Iordache",
            "Paolo Rosso"
        ],
        "published": "2024-02-20T18:32:47Z",
        "summary": "Recently, large language models (LLMs) have become increasingly powerful and\nhave become capable of solving a plethora of tasks through proper instructions\nin natural language. However, the vast majority of testing suites assume that\nthe instructions are written in English, the de facto prompting language. Code\nintelligence and problem solving still remain a difficult task, even for the\nmost advanced LLMs. Currently, there are no datasets to measure the\ngeneralization power for code-generation models in a language other than\nEnglish. In this work, we present RoCode, a competitive programming dataset,\nconsisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and\nPython and comprehensive testing suites for each problem. The purpose of RoCode\nis to provide a benchmark for evaluating the code intelligence of language\nmodels trained on Romanian / multilingual text as well as a fine-tuning set for\npretrained Romanian models. Through our results and review of related works, we\nargue for the need to develop code models for languages other than English.",
        "pdf_link": "https://arxiv.org/pdf/2402.13222v1.pdf"
    },
    {
        "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts",
        "authors": [
            "Yusu Qian",
            "Haotian Zhang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "published": "2024-02-20T18:31:27Z",
        "summary": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 850 test samples divided\ninto 6 categories, such as non-existent objects, count of objects, spatial\nrelationship, and visual confusion. We provide a comprehensive analysis of\npopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as\nLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps\nbetween GPT-4V and other models; and previous robust instruction-tuned models,\nsuch as LRV-Instruction and LLaVA-RLHF, are not effective on this new\nbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of\nany other model in our experiments ranges from 5% to 35%. We further propose a\nremedy that adds an additional paragraph to the deceptive prompts to encourage\nmodels to think twice before answering the question. Surprisingly, this simple\nmethod can even double the accuracy; however, the absolute numbers are still\ntoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark\nto stimulate further research to enhance models' resilience against deceptive\nprompts.",
        "pdf_link": "https://arxiv.org/pdf/2402.13220v1.pdf"
    },
    {
        "title": "Soft Self-Consistency Improves Language Model Agents",
        "authors": [
            "Han Wang",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "published": "2024-02-20T18:22:38Z",
        "summary": "Generations from large language models (LLMs) can be improved by sampling and\nscoring multiple solutions to select a final answer. Current \"sample and\nselect\" methods such as self-consistency (SC) rely on majority voting to score\nanswers. However, when tasks have many distinct and valid answers, selection by\nvoting requires a large number of samples. This makes SC prohibitively\nexpensive for interactive tasks that involve generating multiple actions\n(answers) sequentially. After establishing that majority voting fails to\nprovide consistent gains on such tasks, we demonstrate how to increase success\nrates by softening the scoring criterion. We introduce Soft Self-Consistency\n(Soft-SC), which replaces SC's discontinuous scoring with a continuous score\ncomputed from model likelihoods, allowing for selection even when actions are\nsparsely distributed. Soft-SC improves both performance and efficiency on\nlong-horizon interactive tasks, requiring half as many samples as SC for\ncomparable or better performance. For a fixed number of samples, Soft-SC leads\nto a 1.3% increase over SC in absolute success rate on writing bash programs, a\n6.6% increase on online shopping (WebShop), and a 4.7% increase for an\ninteractive household game (ALFWorld). Finally, we show that Soft-SC can be\napplied to both open-source and black-box models.",
        "pdf_link": "https://arxiv.org/pdf/2402.13212v1.pdf"
    },
    {
        "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation",
        "authors": [
            "Dongjin Kang",
            "Sunghwan Kim",
            "Taeyoon Kwon",
            "Seungjun Moon",
            "Hyunsouk Cho",
            "Youngjae Yu",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "published": "2024-02-20T18:21:32Z",
        "summary": "Emotional Support Conversation (ESC) is a task aimed at alleviating\nindividuals' emotional distress through daily conversation. Given its inherent\ncomplexity and non-intuitive nature, ESConv dataset incorporates support\nstrategies to facilitate the generation of appropriate responses. Recently,\ndespite the remarkable conversational ability of large language models (LLMs),\nprevious studies have suggested that they often struggle with providing useful\nemotional support. Hence, this work initially analyzes the results of LLMs on\nESConv, revealing challenges in selecting the correct strategy and a notable\npreference for a specific strategy. Motivated by these, we explore the impact\nof the inherent preference in LLMs on providing emotional support, and\nconsequently, we observe that exhibiting high preference for specific\nstrategies hinders effective emotional support, aggravating its robustness in\npredicting the appropriate strategy. Moreover, we conduct a methodological\nstudy to offer insights into the necessary approaches for LLMs to serve as\nproficient emotional supporters. Our findings emphasize that (1) low preference\nfor specific strategies hinders the progress of emotional support, (2) external\nassistance helps reduce preference bias, and (3) LLMs alone cannot become good\nemotional supporters. These insights suggest promising avenues for future\nresearch to enhance the emotional intelligence of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.13211v1.pdf"
    },
    {
        "title": "Bayesian Reward Models for LLM Alignment",
        "authors": [
            "Adam X. Yang",
            "Maxime Robeyns",
            "Thomas Coste",
            "Jun Wang",
            "Haitham Bou-Ammar",
            "Laurence Aitchison"
        ],
        "published": "2024-02-20T18:20:59Z",
        "summary": "To ensure that large language model (LLM) responses are helpful and\nnon-toxic, we usually fine-tune a reward model on human preference data. We\nthen select policy responses with high rewards (best-of-n sampling) or further\noptimize the policy to produce responses with high rewards (reinforcement\nlearning from human feedback). However, this process is vulnerable to reward\noveroptimization or hacking, in which the responses selected have high rewards\ndue to errors in the reward model rather than a genuine preference. This is\nespecially problematic as the prompt or response diverges from the training\ndata. It should be possible to mitigate these issues by training a Bayesian\nreward model, which signals higher uncertainty further from the training data\ndistribution. Therefore, we trained Bayesian reward models using Laplace-LoRA\n(Yang et al., 2024) and found that the resulting uncertainty estimates can\nsuccessfully mitigate reward overoptimization in best-of-n sampling.",
        "pdf_link": "https://arxiv.org/pdf/2402.13210v1.pdf"
    },
    {
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published": "2024-02-20T17:44:06Z",
        "summary": "While large language models (LLMs) have achieved state-of-the-art performance\non a wide range of medical question answering (QA) tasks, they still face\nchallenges with hallucinations and outdated knowledge. Retrieval-augmented\ngeneration (RAG) is a promising solution and has been widely adopted. However,\na RAG system can involve multiple flexible components, and there is a lack of\nbest practices regarding the optimal RAG setting for various medical purposes.\nTo systematically evaluate such systems, we propose the Medical Information\nRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind\nbenchmark including 7,663 questions from five medical QA datasets. Using\nMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt\ntokens on 41 combinations of different corpora, retrievers, and backbone LLMs\nthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improves\nthe accuracy of six different LLMs by up to 18% over chain-of-thought\nprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our\nresults show that the combination of various medical corpora and retrievers\nachieves the best performance. In addition, we discovered a log-linear scaling\nproperty and the \"lost-in-the-middle\" effects in medical RAG. We believe our\ncomprehensive evaluations can serve as practical guidelines for implementing\nRAG systems for medicine.",
        "pdf_link": "https://arxiv.org/pdf/2402.13178v2.pdf"
    },
    {
        "title": "Is the System Message Really Important to Jailbreaks in Large Language Models?",
        "authors": [
            "Xiaotian Zou",
            "Yongkang Chen",
            "Ke Li"
        ],
        "published": "2024-02-20T17:39:40Z",
        "summary": "The rapid evolution of Large Language Models (LLMs) has rendered them\nindispensable in modern society. While security measures are typically in place\nto align LLMs with human values prior to release, recent studies have unveiled\na concerning phenomenon named \"jailbreak.\" This term refers to the unexpected\nand potentially harmful responses generated by LLMs when prompted with\nmalicious questions. Existing research focuses on generating jailbreak prompts\nbut our study aim to answer a different question: Is the system message really\nimportant to jailbreak in LLMs? To address this question, we conducted\nexperiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak\nprompts with varying system messages: short, long, and none. We discover that\ndifferent system messages have distinct resistances to jailbreak by\nexperiments. Additionally, we explore the transferability of jailbreak across\nLLMs. This finding underscores the significant impact system messages can have\non mitigating LLMs jailbreak. To generate system messages that are more\nresistant to jailbreak prompts, we propose System Messages Evolutionary\nAlgorithms (SMEA). Through SMEA, we can get robust system messages population\nthat demonstrate up to 98.9% resistance against jailbreak prompts. Our research\nnot only bolsters LLMs security but also raises the bar for jailbreak,\nfostering advancements in this field of study.",
        "pdf_link": "https://arxiv.org/pdf/2402.14857v1.pdf"
    },
    {
        "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
        "authors": [
            "Yujun Zhou",
            "Yufei Han",
            "Haomin Zhuang",
            "Taicheng Guo",
            "Kehan Guo",
            "Zhenwen Liang",
            "Hongyan Bao",
            "Xiangliang Zhang"
        ],
        "published": "2024-02-20T17:04:06Z",
        "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities across\ndiverse applications. However, concerns regarding their security, particularly\nthe vulnerability to jailbreak attacks, persist. Drawing inspiration from\nadversarial training in deep learning and LLM agent learning processes, we\nintroduce the In-Context Adversarial Game (ICAG) for defending against\njailbreaks without the need for fine-tuning. ICAG leverages agent learning to\nconduct an adversarial game, aiming to dynamically extend knowledge to defend\nagainst jailbreaks. Unlike traditional methods that rely on static datasets,\nICAG employs an iterative process to enhance both the defense and attack\nagents. This continuous improvement process strengthens defenses against newly\ngenerated jailbreak prompts. Our empirical studies affirm ICAG's efficacy,\nwhere LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success\nrates across various attack scenarios. Moreover, ICAG demonstrates remarkable\ntransferability to other LLMs, indicating its potential as a versatile defense\nmechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.13148v1.pdf"
    },
    {
        "title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
        "authors": [
            "Ivan Rep",
            "David Duki\u0107",
            "Jan \u0160najder"
        ],
        "published": "2024-02-20T16:43:20Z",
        "summary": "While BERT produces high-quality sentence embeddings, its pre-training\ncomputational cost is a significant drawback. In contrast, ELECTRA delivers a\ncost-effective pre-training objective and downstream task performance\nimprovements, but not as performant sentence embeddings. The community tacitly\nstopped utilizing ELECTRA's sentence embeddings for semantic textual similarity\n(STS). We notice a significant drop in performance when using the ELECTRA\ndiscriminator's last layer in comparison to earlier layers. We explore this\ndrop and devise a way to repair ELECTRA's embeddings, proposing a novel\ntruncated model fine-tuning (TMFT) method. TMFT improves the Spearman\ncorrelation coefficient by over 8 points while increasing parameter efficiency\non the STS benchmark dataset. We extend our analysis to various model sizes and\nlanguages. Further, we discover the surprising efficacy of ELECTRA's generator\nmodel, which performs on par with BERT, using significantly fewer parameters\nand a substantially smaller embedding size. Finally, we observe further boosts\nby combining TMFT with a word similarity task or domain adaptive pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2402.13130v1.pdf"
    },
    {
        "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
        "authors": [
            "Xiang Li",
            "Yunshi Lan",
            "Chao Yang"
        ],
        "published": "2024-02-20T16:38:33Z",
        "summary": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.",
        "pdf_link": "https://arxiv.org/pdf/2402.13125v1.pdf"
    },
    {
        "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models",
        "authors": [
            "Yizhi LI",
            "Ge Zhang",
            "Xingwei Qu",
            "Jiali Li",
            "Zhaoqun Li",
            "Zekun Wang",
            "Hao Li",
            "Ruibin Yuan",
            "Yinghao Ma",
            "Kai Zhang",
            "Wangchunshu Zhou",
            "Yiming Liang",
            "Lei Zhang",
            "Lei Ma",
            "Jiajun Zhang",
            "Zuowen Li",
            "Stephen W. Huang",
            "Chenghua Lin",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "published": "2024-02-20T16:02:12Z",
        "summary": "The advancement of large language models (LLMs) has enhanced the ability to\ngeneralize across a wide range of unseen natural language processing (NLP)\ntasks through instruction-following. Yet, their effectiveness often diminishes\nin low-resource languages like Chinese, exacerbated by biased evaluations from\ndata leakage, casting doubt on their true generalizability to new linguistic\nterritories. In response, we introduce the Chinese Instruction-Following\nBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of\nLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000\ninput-output pairs, developed by native speakers to test complex reasoning and\nChinese cultural nuances across 20 categories. To mitigate evaluation bias, we\nrelease only half of the dataset publicly, with the remainder kept private, and\nintroduce diversified instructions to minimize score variance, totaling 45,000\ndata instances. Our evaluation of 28 selected LLMs reveals a noticeable\nperformance gap, with the best model scoring only 52.9%, highlighting the\nlimitations of LLMs in less familiar language and task contexts. This work aims\nto uncover the current limitations of LLMs in handling Chinese tasks, pushing\ntowards the development of more culturally informed and linguistically diverse\nmodels with the released data and benchmark\n(https://yizhilll.github.io/CIF-Bench/).",
        "pdf_link": "https://arxiv.org/pdf/2402.13109v1.pdf"
    },
    {
        "title": "ELAD: Explanation-Guided Large Language Models Active Distillation",
        "authors": [
            "Yifei Zhang",
            "Bo Pan",
            "Chen Ling",
            "Yuntong Hu",
            "Liang Zhao"
        ],
        "published": "2024-02-20T15:47:59Z",
        "summary": "The deployment and application of Large Language Models (LLMs) is hindered by\ntheir memory inefficiency, computational demands, and the high costs of API\ninferences. Traditional distillation methods, which transfer the capabilities\nof LLMs to smaller models, often fail to determine whether the knowledge has\nbeen sufficiently transferred, potentially resulting in high costs or\nincomplete distillation. In this paper, we propose an Explanation-Guided LLMs\nActive Distillation (ELAD) framework that employs an active learning strategy\nto optimize the balance between annotation costs and model performance. To\nimprove efficient sample selection, we introduce an explanation-guided sample\nselection method that identifies samples challenging its reasoning by\nexploiting uncertainties in explanation steps. Additionally, we present a\ncustomized LLM-annotated explanation revision technique where the teacher model\ndetects and corrects flaws in the student model's reasoning. Our experiments\nacross various reasoning datasets demonstrate that our framework significantly\nenhances the efficiency of LLM knowledge distillation.",
        "pdf_link": "https://arxiv.org/pdf/2402.13098v1.pdf"
    },
    {
        "title": "Event-level Knowledge Editing",
        "authors": [
            "Hao Peng",
            "Xiaozhi Wang",
            "Chunyang Li",
            "Kaisheng Zeng",
            "Jiangshan Duo",
            "Yixin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2024-02-20T15:36:41Z",
        "summary": "Knowledge editing aims at updating knowledge of large language models (LLMs)\nto prevent them from becoming outdated. Existing work edits LLMs at the level\nof factual knowledge triplets. However, natural knowledge updates in the real\nworld come from the occurrences of new events rather than direct changes in\nfactual triplets. In this paper, we propose a new task setting: event-level\nknowledge editing, which directly edits new events into LLMs and improves over\nconventional triplet-level editing on (1) Efficiency. A single event edit leads\nto updates in multiple entailed knowledge triplets. (2) Completeness. Beyond\nupdating factual knowledge, event-level editing also requires considering the\nevent influences and updating LLMs' knowledge about future trends. We construct\na high-quality event-level editing benchmark ELKEN, consisting of 1,515 event\nedits, 6,449 questions about factual knowledge, and 10,150 questions about\nfuture tendencies. We systematically evaluate the performance of various\nknowledge editing methods and LLMs on this benchmark. We find that ELKEN poses\nsignificant challenges to existing knowledge editing approaches. Our codes and\ndataset are publicly released to facilitate further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.13093v1.pdf"
    },
    {
        "title": "Slot-VLM: SlowFast Slots for Video-Language Modeling",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "published": "2024-02-20T15:30:09Z",
        "summary": "Video-Language Models (VLMs), powered by the advancements in Large Language\nModels (LLMs), are charting new frontiers in video understanding. A pivotal\nchallenge is the development of an efficient method to encapsulate video\ncontent into a set of representative tokens to align with LLMs. In this work,\nwe introduce Slot-VLM, a novel framework designed to generate semantically\ndecomposed video tokens, in terms of object-wise and event-wise visual\nrepresentations, to facilitate LLM inference. Particularly, we design a\nSlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense\nvideo tokens from the CLIP vision encoder to a set of representative slots. In\norder to take into account both the spatial object details and the varied\ntemporal dynamics, SF-Slots is built with a dual-branch structure. The\nSlow-Slots branch focuses on extracting object-centric slots from features at\nhigh spatial resolution but low (slow) frame sample rate, emphasizing detailed\nobject information. Conversely, Fast-Slots branch is engineered to learn\nevent-centric slots from high temporal sample rate but low spatial resolution\nfeatures. These complementary slots are combined to form the vision context,\nserving as the input to the LLM for efficient question answering. Our\nexperimental results demonstrate the effectiveness of our Slot-VLM, which\nachieves the state-of-the-art performance on video question-answering.",
        "pdf_link": "https://arxiv.org/pdf/2402.13088v1.pdf"
    },
    {
        "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
        "authors": [
            "Jie Ren",
            "Qipeng Guo",
            "Hang Yan",
            "Dongrui Liu",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2024-02-20T14:43:39Z",
        "summary": "Although large language models (LLMs) have demonstrated remarkable\nperformance, the lack of transparency in their inference logic raises concerns\nabout their trustworthiness. To gain a better understanding of LLMs, we conduct\na detailed analysis of the operations of attention heads and aim to better\nunderstand the in-context learning of LLMs. Specifically, we investigate\nwhether attention heads encode two types of relationships between tokens\npresent in natural languages: the syntactic dependency parsed from sentences\nand the relation within knowledge graphs. We find that certain attention heads\nexhibit a pattern where, when attending to head tokens, they recall tail tokens\nand increase the output logits of those tail tokens. More crucially, the\nformulation of such semantic induction heads has a close correlation with the\nemergence of the in-context learning ability of language models. The study of\nsemantic attention heads advances our understanding of the intricate operations\nof attention heads in transformers, and further provides new insights into the\nin-context learning of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.13055v1.pdf"
    },
    {
        "title": "Stable Knowledge Editing in Large Language Models",
        "authors": [
            "Zihao Wei",
            "Liang Pang",
            "Hanxing Ding",
            "Jingcheng Deng",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-20T14:36:23Z",
        "summary": "Efficient knowledge editing of large language models is crucial for replacing\nobsolete information or incorporating specialized knowledge on a large scale.\nHowever, previous methods implicitly assume that knowledge is localized and\nisolated within the model, an assumption that oversimplifies the interconnected\nnature of model knowledge. The premise of localization results in an incomplete\nknowledge editing, whereas an isolated assumption may impair both other\nknowledge and general abilities. It introduces instability to the performance\nof the knowledge editing method. To transcend these assumptions, we introduce\nStableKE, a method adopts a novel perspective based on knowledge augmentation\nrather than knowledge localization. To overcome the expense of human labeling,\nStableKE integrates two automated knowledge augmentation strategies: Semantic\nParaphrase Enhancement strategy, which diversifies knowledge descriptions to\nfacilitate the teaching of new information to the model, and Contextual\nDescription Enrichment strategy, expanding the surrounding knowledge to prevent\nthe forgetting of related information. StableKE surpasses other knowledge\nediting methods, demonstrating stability both edited knowledge and multi-hop\nknowledge, while also preserving unrelated knowledge and general abilities.\nMoreover, StableKE can edit knowledge on ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.13048v1.pdf"
    },
    {
        "title": "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries",
        "authors": [
            "Seanie Lee",
            "Jianpeng Cheng",
            "Joris Driesen",
            "Alexandru Coca",
            "Anders Johannsen"
        ],
        "published": "2024-02-20T14:31:17Z",
        "summary": "Few-shot dialogue state tracking (DST) with Large Language Models (LLM)\nrelies on an effective and efficient conversation retriever to find similar\nin-context examples for prompt learning. Previous works use raw dialogue\ncontext as search keys and queries, and a retriever is fine-tuned with\nannotated dialogues to achieve superior performance. However, the approach is\nless suited for scaling to new domains or new annotation languages, where\nfine-tuning data is unavailable. To address this problem, we handle the task of\nconversation retrieval based on text summaries of the conversations. A\nLLM-based conversation summarizer is adopted for query and key generation,\nwhich enables effective maximum inner product search. To avoid the extra\ninference cost brought by LLM-based conversation summarization, we further\ndistill a light-weight conversation encoder which produces query embeddings\nwithout decoding summaries for test conversations. We validate our retrieval\napproach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The\nexperimental results show a significant improvement over relevant baselines in\nreal few-shot DST settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.13043v3.pdf"
    },
    {
        "title": "Text-Guided Molecule Generation with Diffusion Language Model",
        "authors": [
            "Haisong Gong",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published": "2024-02-20T14:29:02Z",
        "summary": "Text-guided molecule generation is a task where molecules are generated to\nmatch specific textual descriptions. Recently, most existing SMILES-based\nmolecule generation methods rely on an autoregressive architecture. In this\nwork, we propose the Text-Guided Molecule Generation with Diffusion Language\nModel (TGM-DLM), a novel approach that leverages diffusion models to address\nthe limitations of autoregressive methods. TGM-DLM updates token embeddings\nwithin the SMILES string collectively and iteratively, using a two-phase\ndiffusion generation process. The first phase optimizes embeddings from random\nnoise, guided by the text description, while the second phase corrects invalid\nSMILES strings to form valid molecular representations. We demonstrate that\nTGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for\nadditional data resources. Our findings underscore the remarkable effectiveness\nof TGM-DLM in generating coherent and precise molecules with specific\nproperties, opening new avenues in drug discovery and related scientific\ndomains. Code will be released at: https://github.com/Deno-V/tgm-dlm.",
        "pdf_link": "https://arxiv.org/pdf/2402.13040v1.pdf"
    },
    {
        "title": "SiLLM: Large Language Models for Simultaneous Machine Translation",
        "authors": [
            "Shoutao Guo",
            "Shaolei Zhang",
            "Zhengrui Ma",
            "Min Zhang",
            "Yang Feng"
        ],
        "published": "2024-02-20T14:23:34Z",
        "summary": "Simultaneous Machine Translation (SiMT) generates translations while reading\nthe source sentence, necessitating a policy to determine the optimal timing for\nreading and generating words. Despite the remarkable performance achieved by\nLarge Language Models (LLM) across various NLP tasks, existing SiMT methods\npredominantly focus on conventional transformers, employing a single model to\nconcurrently determine the policy and generate the translations. However, given\nthe complexity of SiMT, it is challenging to effectively address both tasks\nwith a single model. Therefore, there is a need to decouple the SiMT task into\npolicy-decision and translation sub-tasks. We propose SiLLM, which delegates\nthe two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The\npolicy-decision agent is managed by a conventional SiMT model, responsible for\ndetermining the translation policy. The translation agent, leveraging the\ncapabilities of LLM, generates translation using the partial source sentence.\nThe two agents collaborate to accomplish SiMT. To facilitate the application of\ntoken-level policies determined by conventional SiMT models to LLM, we propose\na word-level policy adapted for LLM. Experiments on two datasets demonstrate\nthat, with a small amount of data for fine-tuning LLM, SiLLM attains\nstate-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.13036v1.pdf"
    },
    {
        "title": "Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models",
        "authors": [
            "Che Zhang",
            "Zhenyang Xiao",
            "Chengcheng Han",
            "Yixin Lian",
            "Yuejian Fang"
        ],
        "published": "2024-02-20T14:23:23Z",
        "summary": "Large language models (LLMs) have made significant strides in reasoning\ncapabilities, with ongoing efforts to refine their reasoning through\nself-correction. However, recent studies suggest that self-correction can be\nlimited or even counterproductive without external accurate knowledge, raising\nquestions about the limits and effectiveness of self-correction. In this paper,\nwe aim to enhance LLM's self-checking capabilities by meticulously designing\ntraining data, thereby improving the accuracy of self-correction. We conduct a\ndetailed analysis of error types in mathematical reasoning and develop a\ntailored prompt, termed \"Step CoT Check\". Then we construct a\nchecking-correction dataset for training models. After integrating the original\nCoT data and checking-correction data for training, we observe that models\ncould improve their self-checking capabilities, thereby enhancing their\nself-correction capacity and eliminating the need for external feedback or\nground truth labels to ascertain the endpoint of correction. We compare the\nperformance of models fine-tuned with the \"Step CoT Check\" prompt against those\nrefined using other promps within the context of checking-correction data. The\n\"Step CoT Check\" outperforms the other two check formats in model with lager\nparameters, providing more precise feedback thus achieving a higher rate of\ncorrectness. For reproducibility, all the datasets and codes are provided in\nhttps://github.com/bammt/Learn-to-check.",
        "pdf_link": "https://arxiv.org/pdf/2402.13035v2.pdf"
    },
    {
        "title": "SoMeLVLM: A Large Vision Language Model for Social Media Processing",
        "authors": [
            "Xinnong Zhang",
            "Haoyu Kuang",
            "Xinyi Mou",
            "Hanjia Lyu",
            "Kun Wu",
            "Siming Chen",
            "Jiebo Luo",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2024-02-20T14:02:45Z",
        "summary": "The growth of social media, characterized by its multimodal nature, has led\nto the emergence of diverse phenomena and challenges, which calls for an\neffective approach to uniformly solve automated tasks. The powerful Large\nVision Language Models make it possible to handle a variety of tasks\nsimultaneously, but even with carefully designed prompting methods, the general\ndomain models often fall short in aligning with the unique speaking style and\ncontext of social media tasks. In this paper, we introduce a Large Vision\nLanguage Model for Social Media Processing (SoMeLVLM), which is a cognitive\nframework equipped with five key capabilities including knowledge &\ncomprehension, application, analysis, evaluation, and creation. SoMeLVLM is\ndesigned to understand and generate realistic social media behavior. We have\ndeveloped a 654k multimodal social media instruction-tuning dataset to support\nour cognitive framework and fine-tune our model. Our experiments demonstrate\nthat SoMeLVLM achieves state-of-the-art performance in multiple social media\ntasks. Further analysis shows its significant advantages over baselines in\nterms of cognitive abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.13022v1.pdf"
    },
    {
        "title": "Understanding the effects of language-specific class imbalance in multilingual fine-tuning",
        "authors": [
            "Vincent Jung",
            "Lonneke van der Plas"
        ],
        "published": "2024-02-20T13:59:12Z",
        "summary": "We study the effect of one type of imbalance often present in real-life\nmultilingual classification datasets: an uneven distribution of labels across\nlanguages. We show evidence that fine-tuning a transformer-based Large Language\nModel (LLM) on a dataset with this imbalance leads to worse performance, a more\npronounced separation of languages in the latent space, and the promotion of\nuninformative features. We modify the traditional class weighing approach to\nimbalance by calculating class weights separately for each language and show\nthat this helps mitigate those detrimental effects. These results create\nawareness of the negative effects of language-specific class imbalance in\nmultilingual fine-tuning and the way in which the model learns to rely on the\nseparation of languages to perform the task.",
        "pdf_link": "https://arxiv.org/pdf/2402.13016v1.pdf"
    },
    {
        "title": "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation",
        "authors": [
            "Demin Song",
            "Honglin Guo",
            "Yunhua Zhou",
            "Shuhao Xing",
            "Yudong Wang",
            "Zifan Song",
            "Wenwei Zhang",
            "Qipeng Guo",
            "Hang Yan",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2024-02-20T13:56:38Z",
        "summary": "The programming skill is one crucial ability for Large Language Models\n(LLMs), necessitating a deep understanding of programming languages (PLs) and\ntheir correlation with natural languages (NLs). We examine the impact of\npre-training data on code-focused LLMs' performance by assessing the comment\ndensity as a measure of PL-NL alignment. Given the scarcity of code-comment\naligned data in pre-training corpora, we introduce a novel data augmentation\nmethod that generates comments for existing code, coupled with a data filtering\nstrategy that filters out code data poorly correlated with natural language. We\nconducted experiments on three code-focused LLMs and observed consistent\nimprovements in performance on two widely-used programming skill benchmarks.\nNotably, the model trained on the augmented data outperformed both the model\nused for generating comments and the model further trained on the data without\naugmentation.",
        "pdf_link": "https://arxiv.org/pdf/2402.13013v1.pdf"
    },
    {
        "title": "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
        "authors": [
            "Kexin Chen",
            "Hanqun Cao",
            "Junyou Li",
            "Yuyang Du",
            "Menghao Guo",
            "Xin Zeng",
            "Lanqing Li",
            "Jiezhong Qiu",
            "Pheng Ann Heng",
            "Guangyong Chen"
        ],
        "published": "2024-02-20T13:21:46Z",
        "summary": "Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.",
        "pdf_link": "https://arxiv.org/pdf/2402.12993v1.pdf"
    },
    {
        "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
        "authors": [
            "Martin Gubri",
            "Dennis Ulmer",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "published": "2024-02-20T13:20:39Z",
        "summary": "Large Language Model (LLM) services and models often come with legal rules on\nwho can use them and how they must use them. Assessing the compliance of the\nreleased LLMs is crucial, as these rules protect the interests of the LLM\ncontributor and prevent misuse. In this context, we describe the novel problem\nof Black-box Identity Verification (BBIV). The goal is to determine whether a\nthird-party application uses a certain LLM through its chat function. We\npropose a method called Targeted Random Adversarial Prompt (TRAP) that\nidentifies the specific LLM in use. We repurpose adversarial suffixes,\noriginally proposed for jailbreaking, to get a pre-defined answer from the\ntarget LLM, while other models give random answers. TRAP detects the target\nLLMs with over 95% true positive rate at under 0.2% false positive rate even\nafter a single interaction. TRAP remains effective even if the LLM has minor\nchanges that do not significantly alter the original function.",
        "pdf_link": "https://arxiv.org/pdf/2402.12991v1.pdf"
    },
    {
        "title": "Can GNN be Good Adapter for LLMs?",
        "authors": [
            "Xuanwen Huang",
            "Kaiqiao Han",
            "Yang Yang",
            "Dezheng Bao",
            "Quanjin Tao",
            "Ziwei Chai",
            "Qi Zhu"
        ],
        "published": "2024-02-20T13:13:13Z",
        "summary": "Recently, large language models (LLMs) have demonstrated superior\ncapabilities in understanding and zero-shot learning on textual data, promising\nsignificant advances for many text-related domains. In the graph domain,\nvarious real-world scenarios also involve textual data, where tasks and node\nfeatures can be described by text. These text-attributed graphs (TAGs) have\nbroad applications in social media, recommendation systems, etc. Thus, this\npaper explores how to utilize LLMs to model TAGs. Previous methods for TAG\nmodeling are based on million-scale LMs. When scaled up to billion-scale LLMs,\nthey face huge challenges in computational costs. Additionally, they also\nignore the zero-shot inference capabilities of LLMs. Therefore, we propose\nGraphAdapter, which uses a graph neural network (GNN) as an efficient adapter\nin collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN\nadapter introduces only a few trainable parameters and can be trained with low\ncomputation costs. The entire framework is trained using auto-regression on\nnode text (next token prediction). Once trained, GraphAdapter can be seamlessly\nfine-tuned with task-specific prompts for various downstream tasks. Through\nextensive experiments across multiple real-world TAGs, GraphAdapter based on\nLlama 2 gains an average improvement of approximately 5\\% in terms of node\nclassification. Furthermore, GraphAdapter can also adapt to other language\nmodels, including RoBERTa, GPT-2. The promising results demonstrate that GNNs\ncan serve as effective adapters for LLMs in TAG modeling.",
        "pdf_link": "https://arxiv.org/pdf/2402.12984v1.pdf"
    },
    {
        "title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
        "authors": [
            "Philipp Mondorf",
            "Barbara Plank"
        ],
        "published": "2024-02-20T12:58:14Z",
        "summary": "Deductive reasoning plays a pivotal role in the formulation of sound and\ncohesive arguments. It allows individuals to draw conclusions that logically\nfollow, given the truth value of the information provided. Recent progress in\nthe domain of large language models (LLMs) has showcased their capability in\nexecuting deductive reasoning tasks. Nonetheless, a significant portion of\nresearch primarily assesses the accuracy of LLMs in solving such tasks, often\noverlooking a deeper analysis of their reasoning behavior. In this study, we\ndraw upon principles from cognitive psychology to examine inferential\nstrategies employed by LLMs, through a detailed evaluation of their responses\nto propositional logic problems. Our findings indicate that LLMs display\nreasoning patterns akin to those observed in humans, including strategies like\n$\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover,\nour research demonstrates that the architecture and scale of the model\nsignificantly affect its preferred method of reasoning, with more advanced\nmodels tending to adopt strategies more frequently than less sophisticated\nones. Importantly, we assert that a model's accuracy, that is the correctness\nof its final conclusion, does not necessarily reflect the validity of its\nreasoning process. This distinction underscores the necessity for more nuanced\nevaluation procedures in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.14856v1.pdf"
    },
    {
        "title": "Gl\u00f3rIA -- A Generative and Open Large Language Model for Portuguese",
        "authors": [
            "Ricardo Lopes",
            "Jo\u00e3o Magalh\u00e3es",
            "David Semedo"
        ],
        "published": "2024-02-20T12:36:40Z",
        "summary": "Significant strides have been made in natural language tasks, largely\nattributed to the emergence of powerful large language models (LLMs). These\nmodels, pre-trained on extensive and diverse corpora, have become increasingly\ncapable of comprehending the intricacies of language. Despite the abundance of\nLLMs for many high-resource languages, the availability of such models remains\nlimited for European Portuguese. We introduce Gl\\'orIA, a robust European\nPortuguese decoder LLM. To pre-train Gl\\'orIA, we assembled a comprehensive\nPT-PT text corpus comprising 35 billion tokens from various sources. We present\nour pre-training methodology, followed by an assessment of the model's\neffectiveness on multiple downstream tasks. Additionally, to evaluate our\nmodels' language modeling capabilities, we introduce CALAME-PT (Context-Aware\nLAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot\nlanguage-modeling benchmark. Evaluation shows that Gl\\'orIA significantly\noutperforms existing open PT decoder models in language modeling and that it\ncan generate sound, knowledge-rich, and coherent PT-PT text. The model also\nexhibits strong potential for various downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.12969v1.pdf"
    },
    {
        "title": "Prompt Stealing Attacks Against Large Language Models",
        "authors": [
            "Zeyang Sha",
            "Yang Zhang"
        ],
        "published": "2024-02-20T12:25:26Z",
        "summary": "The increasing reliance on large language models (LLMs) such as ChatGPT in\nvarious fields emphasizes the importance of ``prompt engineering,'' a\ntechnology to improve the quality of model outputs. With companies investing\nsignificantly in expert prompt engineers and educational resources rising to\nmeet market demand, designing high-quality prompts has become an intriguing\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\nstealing attacks. Our proposed prompt stealing attack aims to steal these\nwell-designed prompts based on the generated answers. The prompt stealing\nattack contains two primary modules: the parameter extractor and the prompt\nreconstruction. The goal of the parameter extractor is to figure out the\nproperties of the original prompts. We first observe that most prompts fall\ninto one of three categories: direct prompt, role-based prompt, and in-context\nprompt. Our parameter extractor first tries to distinguish the type of prompts\nbased on the generated answers. Then, it can further predict which role or how\nmany contexts are used based on the types of prompts. Following the parameter\nextractor, the prompt reconstructor can be used to reconstruct the original\nprompts based on the generated answers and the extracted features. The final\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\nsimilar to the original prompts. Our experimental results show the remarkable\nperformance of our proposed attacks. Our proposed attacks add a new dimension\nto the study of prompt engineering and call for more attention to the security\nissues on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12959v1.pdf"
    },
    {
        "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
        "authors": [
            "Jiayi Fu",
            "Xuandong Zhao",
            "Ruihan Yang",
            "Yuansen Zhang",
            "Jiangjie Chen",
            "Yanghua Xiao"
        ],
        "published": "2024-02-20T12:05:47Z",
        "summary": "Large language models (LLMs) excellently generate human-like text, but also\nraise concerns about misuse in fake news and academic dishonesty.\nDecoding-based watermark, particularly the GumbelMax-trick-based watermark(GM\nwatermark), is a standout solution for safeguarding machine-generated texts due\nto its notable detectability. However, GM watermark encounters a major\nchallenge with generation diversity, always yielding identical outputs for the\nsame prompt, negatively impacting generation diversity and user experience. To\novercome this limitation, we propose a new type of GM watermark, the\nLogits-Addition watermark, and its three variants, specifically designed to\nenhance diversity. Among these, the GumbelSoft watermark (a softmax variant of\nthe Logits-Addition watermark) demonstrates superior performance in high\ndiversity settings, with its AUROC score outperforming those of the two\nalternative variants by 0.1 to 0.3 and surpassing other decoding-based\nwatermarking methods by a minimum of 0.1.",
        "pdf_link": "https://arxiv.org/pdf/2402.12948v2.pdf"
    },
    {
        "title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
        "authors": [
            "Penghai Zhao",
            "Xin Zhang",
            "Ming-Ming Cheng",
            "Jian Yang",
            "Xiang Li"
        ],
        "published": "2024-02-20T11:28:50Z",
        "summary": "By consolidating scattered knowledge, the literature review provides a\ncomprehensive understanding of the investigated topic. However, reading,\nconducting, or peer-reviewing review papers generally demands a significant\ninvestment of time and effort from researchers. To improve efficiency, this\npaper aims to provide a thorough review of reviews in the PAMI field from\ndiverse perspectives. First, this paper proposes several article-level,\nfield-normalized, and large language model-empowered bibliometric indicators to\nevaluate reviews. To facilitate this, a meta-data database dubbed RiPAMI, and a\ntopic dataset are constructed. Second, based on these indicators, the study\npresents comparative analyses of representative reviews, unveiling the\ncharacteristics of publications across various fields, periods, and journals.\nThe newly emerging AI-generated literature reviews are also appraised, and the\nobserved differences suggest that most AI-generated reviews still lag behind\nhuman-authored reviews in multiple aspects. Third, we briefly provide a\nsubjective evaluation of representative PAMI reviews and introduce a paper\nstructure-based typology of literature reviews. This typology may improve the\nclarity and effectiveness for scholars in reading and writing reviews, while\nalso serving as a guide for AI systems in generating well-organized reviews.\nFinally, this work offers insights into the current challenges of literature\nreviews and envisions future directions for their development.",
        "pdf_link": "https://arxiv.org/pdf/2402.12928v4.pdf"
    },
    {
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "authors": [
            "Xueyang Feng",
            "Zhi-Yuan Chen",
            "Yujia Qin",
            "Yankai Lin",
            "Xu Chen",
            "Zhiyuan Liu",
            "Ji-Rong Wen"
        ],
        "published": "2024-02-20T11:03:36Z",
        "summary": "In recent developments within the research community, the integration of\nLarge Language Models (LLMs) in creating fully autonomous agents has garnered\nsignificant interest. Despite this, LLM-based agents frequently demonstrate\nnotable shortcomings in adjusting to dynamic environments and fully grasping\nhuman needs. In this work, we introduce the problem of LLM-based human-agent\ncollaboration for complex task-solving, exploring their synergistic potential.\nIn addition, we propose a Reinforcement Learning-based Human-Agent\nCollaboration method, ReHAC. This approach includes a policy model designed to\ndetermine the most opportune stages for human intervention within the\ntask-solving process. We construct a human-agent collaboration dataset to train\nthis policy model in an offline reinforcement learning environment. Our\nvalidation tests confirm the model's effectiveness. The results demonstrate\nthat the synergistic efforts of humans and LLM-based agents significantly\nimprove performance in complex tasks, primarily through well-planned, limited\nhuman intervention. Datasets and code are available at:\nhttps://github.com/XueyangFeng/ReHAC.",
        "pdf_link": "https://arxiv.org/pdf/2402.12914v1.pdf"
    },
    {
        "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
        "authors": [
            "Chengcheng Wei",
            "Ze Chen",
            "Songtan Fang",
            "Jiarong He",
            "Max Gao"
        ],
        "published": "2024-02-20T11:01:39Z",
        "summary": "This paper mainly describes a unified system for hallucination detection of\nLLMs, which wins the second prize in the model-agnostic track of the\nSemEval-2024 Task 6, and also achieves considerable results in the model-aware\ntrack. This task aims to detect hallucination with LLMs for three different\ntext-generation tasks without labeled training data. We utilize prompt\nengineering and few-shot learning to verify the performance of different LLMs\non the validation data. Then we select the LLMs with better performance to\ngenerate high-quality weakly supervised training data, which not only satisfies\nthe consistency of different LLMs, but also satisfies the consistency of the\noptimal LLM with different sampling parameters. Furthermore, we finetune\ndifferent LLMs by using the constructed training data, and finding that a\nrelatively small LLM can achieve a competitive level of performance in\nhallucination detection, when compared to the large LLMs and the prompt-based\napproaches using GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.12913v1.pdf"
    },
    {
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
        "authors": [
            "Zhiyuan Li",
            "Hong Liu",
            "Denny Zhou",
            "Tengyu Ma"
        ],
        "published": "2024-02-20T10:11:03Z",
        "summary": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a\nchain of thought (CoT), is a highly effective method to improve the accuracy of\nlarge language models (LLMs) on arithmetics and symbolic reasoning tasks.\nHowever, the mechanism behind CoT remains unclear. This work provides a\ntheoretical understanding of the power of CoT for decoder-only transformers\nthrough the lens of expressiveness. Conceptually, CoT empowers the model with\nthe ability to perform inherently serial computation, which is otherwise\nlacking in transformers, especially when depth is low. Given input length $n$,\nprevious works have shown that constant-depth transformers with finite\nprecision $\\mathsf{poly}(n)$ embedding size can only solve problems in\n$\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper\nbound for constant-depth transformers with constant-bit precision, which can\nonly solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$.\nHowever, with $T$ steps of CoT, constant-depth transformers using constant-bit\nprecision and $O(\\log n)$ embedding size can solve any problem solvable by\nboolean circuits of size $T$. Empirically, enabling CoT dramatically improves\nthe accuracy for tasks that are hard for parallel computation, including the\ncomposition of permutation groups, iterated squaring, and circuit value\nproblems, especially for low-depth transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.12875v1.pdf"
    },
    {
        "title": "Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data",
        "authors": [
            "Dehai Min",
            "Nan Hu",
            "Rihui Jin",
            "Nuo Lin",
            "Jiaoyan Chen",
            "Yongrui Chen",
            "Yu Li",
            "Guilin Qi",
            "Yun Li",
            "Nijun Li",
            "Qianren Wang"
        ],
        "published": "2024-02-20T10:00:58Z",
        "summary": "Augmenting Large Language Models (LLMs) for Question Answering (QA) with\ndomain specific data has attracted wide attention. However, domain data often\nexists in a hybrid format, including text and semi-structured tables, posing\nchallenges for the seamless integration of information. Table-to-Text\nGeneration is a promising solution by facilitating the transformation of hybrid\ndata into a uniformly text-formatted corpus. Although this technique has been\nwidely studied by the NLP community, there is currently no comparative analysis\non how corpora generated by different table-to-text methods affect the\nperformance of QA systems. In this paper, we address this research gap in two\nsteps. First, we innovatively integrate table-to-text generation into the\nframework of enhancing LLM-based QA systems with domain hybrid data. Then, we\nutilize this framework in real-world industrial data to conduct extensive\nexperiments on two types of QA systems (DSFT and RAG frameworks) with four\nrepresentative methods: Markdown format, Template serialization, TPLM-based\nmethod, and LLM-based method. Based on the experimental results, we draw some\nempirical findings and explore the underlying reasons behind the success of\nsome methods. We hope the findings of this work will provide a valuable\nreference for the academic and industrial communities in developing robust QA\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2402.12869v2.pdf"
    },
    {
        "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models",
        "authors": [
            "Tongxu Luo",
            "Jiahe Lei",
            "Fangyu Lei",
            "Weihao Liu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-02-20T09:30:48Z",
        "summary": "Fine-tuning is often necessary to enhance the adaptability of Large Language\nModels (LLM) to downstream tasks. Nonetheless, the process of updating billions\nof parameters demands significant computational resources and training time,\nwhich poses a substantial obstacle to the widespread application of large-scale\nmodels in various scenarios. To address this issue, Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.\nHowever, current PEFT approaches that employ a limited set of global parameters\n(such as LoRA, which adds low-rank approximation matrices to all weights) face\nchallenges in flexibly combining different computational modules in downstream\ntasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider\nLoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon\nobserved in MoE, we propose the utilization of contrastive learning to\nencourage experts to learn distinct features. We conducted experiments on 11\ntasks in math reasoning and common-sense reasoning benchmarks. With the same\nnumber of parameters, our approach outperforms LoRA significantly. In math\nreasoning, MoELoRA achieved an average performance that was 4.2% higher than\nLoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on\nseveral benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.12851v1.pdf"
    },
    {
        "title": "Instruction-tuned Language Models are Better Knowledge Learners",
        "authors": [
            "Zhengbao Jiang",
            "Zhiqing Sun",
            "Weijia Shi",
            "Pedro Rodriguez",
            "Chunting Zhou",
            "Graham Neubig",
            "Xi Victoria Lin",
            "Wen-tau Yih",
            "Srinivasan Iyer"
        ],
        "published": "2024-02-20T09:20:32Z",
        "summary": "In order for large language model (LLM)-based assistants to effectively adapt\nto evolving information needs, it must be possible to update their factual\nknowledge through continued training on new data. The standard recipe for doing\nso involves continued pre-training on new documents followed by\ninstruction-tuning on question-answer (QA) pairs. However, we find that LLMs\ntrained with this recipe struggle to answer questions, even though the\nperplexity of documents is minimized. We found that QA pairs are generally\nstraightforward, while documents are more complex, weaving many factual\nstatements together in an intricate manner. Therefore, we hypothesize that it\nis beneficial to expose LLMs to QA pairs before continued pre-training on\ndocuments so that the process of encoding knowledge from complex documents\ntakes into account how this knowledge is accessed through questions. Based on\nthis, we propose pre-instruction-tuning (PIT), a method that instruction-tunes\non questions prior to training on documents. This contrasts with standard\ninstruction-tuning, which learns how to extract knowledge after training on\ndocuments. Extensive experiments and ablation studies demonstrate that PIT\nsignificantly enhances the ability of LLMs to absorb knowledge from new\ndocuments, outperforming standard instruction-tuning by 17.8%.",
        "pdf_link": "https://arxiv.org/pdf/2402.12847v1.pdf"
    },
    {
        "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
        "authors": [
            "Gyeongman Kim",
            "Doohyuk Jang",
            "Eunho Yang"
        ],
        "published": "2024-02-20T09:10:08Z",
        "summary": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets using the GPT-2 model family show that PromptKD achieves\nstate-of-the-art performance while adding only 0.0007% of the teacher's\nparameters as prompts. Further analysis suggests that distilling\nstudent-friendly knowledge alleviates exposure bias effectively throughout the\nentire training process, leading to performance enhancements.",
        "pdf_link": "https://arxiv.org/pdf/2402.12842v1.pdf"
    },
    {
        "title": "PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs",
        "authors": [
            "An Liu",
            "Zonghan Yang",
            "Zhenhe Zhang",
            "Qingyuan Hu",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "published": "2024-02-20T09:02:55Z",
        "summary": "While Large language models (LLMs) have demonstrated considerable\ncapabilities across various natural language tasks, they often fall short of\nthe performance achieved by domain-specific state-of-the-art models. One\npotential approach to enhance domain-specific capabilities of LLMs involves\nfine-tuning them using corresponding datasets. However, this method can be both\nresource and time-intensive, and not applicable to closed-source commercial\nLLMs. In this paper, we propose Preference Adaptation for Enhancing\nDomain-specific Abilities of LLMs (PANDA), a method designed to augment the\ndomain-specific capabilities of LLMs by leveraging insights from the response\npreference of expert models without requiring fine-tuning. Our experimental\nresults reveal that PANDA significantly enhances the domain-specific ability of\nLLMs on text classification and interactive decision tasks. Moreover, LLM with\nPANDA even outperforms the expert model that being learned on 4 tasks of\nScienceWorld. This finding highlights the potential of exploring tuning-free\napproaches to achieve weak-to-strong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2402.12835v1.pdf"
    },
    {
        "title": "Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model",
        "authors": [
            "Liyan Xu",
            "Zhenlin Su",
            "Mo Yu",
            "Jin Xu",
            "Jinho D. Choi",
            "Jie Zhou",
            "Fei Liu"
        ],
        "published": "2024-02-20T08:41:23Z",
        "summary": "Factual inconsistency poses a significant hurdle for the commercial\ndeployment of abstractive summarizers. Under this Large Language Model (LLM)\nera, this work focuses around two important questions: what is the best way to\nleverage LLM for factual inconsistency detection, and how could we distill a\nsmaller LLM with both high efficiency and efficacy? Three zero-shot paradigms\nare firstly proposed and evaluated across five diverse datasets: direct\ninference on the entire summary or each summary window; entity verification\nthrough question generation and answering. Experiments suggest that LLM itself\nis capable to resolve this task train-free under the proper paradigm design,\nsurpassing strong trained baselines by 2.8% on average. To further promote\npractical utility, we then propose training strategies aimed at distilling\nsmaller open-source LLM that learns to score the entire summary at once with\nhigh accuracy, which outperforms the zero-shot approaches by much larger LLM,\nserving as an effective and efficient ready-to-use scorer.",
        "pdf_link": "https://arxiv.org/pdf/2402.12821v1.pdf"
    },
    {
        "title": "Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?",
        "authors": [
            "Branislav Pecher",
            "Ivan Srba",
            "Maria Bielikova"
        ],
        "published": "2024-02-20T08:38:24Z",
        "summary": "When solving a task with limited labelled data, researchers can either use a\ngeneral large language model without further update, or use the few examples to\ntune a specialised smaller model. When enough labels are available, the\nspecialised models outperform the general ones on many NLP tasks. In this work,\nwe aim to investigate how many labelled samples are required for the\nspecialised models to achieve this superior performance, while taking the\nresults variance into consideration. Observing the behaviour of prompting,\nin-context learning, fine-tuning and instruction-tuning, identifying their\nbreak-even points when increasing number of labelled training samples across\nthree tasks of varying complexity, we find that the specialised models often\nneed only few samples ($100-1000$) to be on par or better than the general\nones. At the same time, the amount of required labelled data strongly depends\non the task complexity and results variance.",
        "pdf_link": "https://arxiv.org/pdf/2402.12819v1.pdf"
    },
    {
        "title": "SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning",
        "authors": [
            "Jinu Lee",
            "Wonseok Hwang"
        ],
        "published": "2024-02-20T08:27:05Z",
        "summary": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning\nability as in Chain-of-thought prompting, but faithful multi-step reasoning\nremains a challenge. We specifically focus on backward chaining, where the\nquery is recursively decomposed using logical rules until proven. To address\nthe limitations of current backward chaining implementations, we propose SymBa\n(Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls\nthe entire proof process and the LLM is called to generate a single reasoning\nstep only when the solver encounters a dead end. By this novel solver-LLM\nintegration, while being able to produce an interpretable, structured proof,\nSymBa achieves significant improvement in performance, proof faithfulness, and\nefficiency in diverse multi-step reasoning benchmarks (ProofWriter,\nBirds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward\nchaining baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.12806v1.pdf"
    },
    {
        "title": "Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting",
        "authors": [
            "Marco Naguib",
            "Xavier Tannier",
            "Aur\u00e9lie N\u00e9v\u00e9ol"
        ],
        "published": "2024-02-20T08:20:49Z",
        "summary": "Large Language Models are becoming the go-to solution for many natural\nlanguage processing tasks, including in specialized domains where their\nfew-shot capacities are expected to yield high performance in low-resource\nsettings. Herein, we aim to assess the performance of Large Language Models for\nfew shot clinical entity recognition in multiple languages. We evaluate named\nentity recognition in English, French and Spanish using 8 in-domain (clinical)\nand 6 out-domain gold standard corpora. We assess the performance of 10\nauto-regressive language models using prompting and 16 masked language models\nused for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot\nset-up by limiting the amount of annotated data available to 100 sentences. Our\nexperiments show that although larger prompt-based models tend to achieve\ncompetitive F-measure for named entity recognition outside the clinical domain,\nthis level of performance does not carry over to the clinical domain where\nlighter supervised taggers relying on masked language models perform better,\neven with the performance drop incurred from the few-shot set-up. In all\nexperiments, the CO2 impact of masked language models is inferior to that of\nauto-regressive models. Results are consistent over the three languages and\nsuggest that few-shot learning using Large language models is not production\nready for named entity recognition in the clinical domain. Instead, models\ncould be used for speeding-up the production of gold standard annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2402.12801v1.pdf"
    },
    {
        "title": "Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models",
        "authors": [
            "Kaiwen Wei",
            "Jingyuan Zhang",
            "Hongzhi Zhang",
            "Fuzheng Zhang",
            "Di Zhang",
            "Li Jin",
            "Yue Yu"
        ],
        "published": "2024-02-20T08:03:05Z",
        "summary": "Large Language Models (LLMs) exhibit remarkable generative capabilities,\nenabling the generation of valuable information. Despite these advancements,\nprevious research found that LLMs sometimes struggle with adhering to specific\nconstraints (e.g., in specific place or at specific time), at times even\noverlooking them, which leads to responses that are either too generic or not\nfully satisfactory. Existing approaches attempted to address this issue by\ndecomposing or rewriting input instructions, yet they fall short in adequately\nemphasizing specific constraints and in unlocking the underlying knowledge\n(e.g., programming within the context of software development). In response,\nthis paper proposes a simple yet effective method named Chain-of-Specificity\n(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the\ninput instructions, unlocks knowledge within LLMs, and refines responses.\nExperiments conducted on publicly available and self-build complex datasets\ndemonstrate that CoS outperforms existing methods in enhancing generated\ncontent especially for the specificity. Besides, as the number of specific\nconstraints increase, other baselines falter, while CoS still performs well.\nMoreover, we show that distilling responses generated by CoS effectively\nenhances the ability of smaller models to follow the constrained instructions.\nResources of this paper will be released for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.15526v1.pdf"
    },
    {
        "title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations",
        "authors": [
            "Guan-Ting Lin",
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2024-02-20T07:51:43Z",
        "summary": "In spoken dialogue, even if two current turns are the same sentence, their\nresponses might still differ when they are spoken in different styles. The\nspoken styles, containing paralinguistic and prosodic information, mark the\nmost significant difference between text and speech modality. When using\ntext-only LLMs to model spoken dialogue, text-only LLMs cannot give different\nresponses based on the speaking style of the current turn. In this paper, we\nfocus on enabling LLMs to listen to the speaking styles and respond properly.\nOur goal is to teach the LLM that \"even if the sentences are identical if they\nare spoken in different styles, their corresponding responses might be\ndifferent\". Since there is no suitable dataset for achieving this goal, we\ncollect a speech-to-speech dataset, StyleTalk, with the following desired\ncharacteristics: when two current speeches have the same content but are spoken\nin different styles, their responses will be different. To teach LLMs to\nunderstand and respond properly to the speaking styles, we propose the\nSpoken-LLM framework that can model the linguistic content and the speaking\nstyles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage\ntraining pipeline to help the Spoken-LLM better learn the speaking styles.\nBased on extensive experiments, we show that Spoken-LLM outperforms text-only\nbaselines and prior speech LLMs methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.12786v1.pdf"
    },
    {
        "title": "Me LLaMA: Foundation Large Language Models for Medical Applications",
        "authors": [
            "Qianqian Xie",
            "Qingyu Chen",
            "Aokun Chen",
            "Cheng Peng",
            "Yan Hu",
            "Fongci Lin",
            "Xueqing Peng",
            "Jimin Huang",
            "Jeffrey Zhang",
            "Vipina Keloth",
            "Xingyu Zhou",
            "Huan He",
            "Lucila Ohno-Machado",
            "Yonghui Wu",
            "Hua Xu",
            "Jiang Bian"
        ],
        "published": "2024-02-20T06:37:31Z",
        "summary": "Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\ngreat promise in many AI applications. However, their performance on medical\ntasks is suboptimal and can be improved by training on extensive\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\nthat includes foundation models - Me LLaMA 13/70B, along with their\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\ndomain-specific data suite for training and evaluation includes a large-scale,\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\nLLaMA models achieve overall better performance than existing open-source\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\nwith a slight variance of within 3%, and yet falls short when compared to\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\nour results show that Me LLaMA models outperform other open-source medical LLMs\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\nfoundation LLMs that use both biomedical and clinical data. It exhibits\nsuperior performance across both general and medical tasks compared to other\nopen-source medical LLMs, rendering it an attractive choice for medical AI\napplications. We release our models, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.",
        "pdf_link": "https://arxiv.org/pdf/2402.12749v3.pdf"
    },
    {
        "title": "An LLM Maturity Model for Reliable and Transparent Text-to-Query",
        "authors": [
            "Lei Yu",
            "Abir Ray"
        ],
        "published": "2024-02-20T06:20:09Z",
        "summary": "Recognizing the imperative to address the reliability and transparency issues\nof Large Language Models (LLM), this work proposes an LLM maturity model\ntailored for text-to-query applications. This maturity model seeks to fill the\nexisting void in evaluating LLMs in such applications by incorporating\ndimensions beyond mere correctness or accuracy. Moreover, this work introduces\na real-world use case from the law enforcement domain and showcases QueryIQ, an\nLLM-powered, domain-specific text-to-query assistant to expedite user workflows\nand reveal hidden relationship in data.",
        "pdf_link": "https://arxiv.org/pdf/2402.14855v1.pdf"
    },
    {
        "title": "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion",
        "authors": [
            "Sen Li",
            "Ruochen Wang",
            "Cho-Jui Hsieh",
            "Minhao Cheng",
            "Tianyi Zhou"
        ],
        "published": "2024-02-20T06:14:30Z",
        "summary": "Existing text-to-image models still struggle to generate images of multiple\nobjects, especially in handling their spatial positions, relative sizes,\noverlapping, and attribute bindings. In this paper, we develop a training-free\nMultimodal-LLM agent (MuLan) to address these challenges by progressive\nmulti-object generation with planning and feedback control, like a human\npainter. MuLan harnesses a large language model (LLM) to decompose a prompt to\na sequence of sub-tasks, each generating only one object conditioned on\npreviously generated objects by stable diffusion. Unlike existing LLM-grounded\nmethods, MuLan only produces a high-level plan at the beginning while the exact\nsize and location of each object are determined by an LLM and attention\nguidance upon each sub-task. Moreover, MuLan adopts a vision-language model\n(VLM) to provide feedback to the image generated in each sub-task and control\nthe diffusion model to re-generate the image if it violates the original\nprompt. Hence, each model in every step of MuLan only needs to address an easy\nsub-task it is specialized for. We collect 200 prompts containing multi-objects\nwith spatial relationships and attribute bindings from different benchmarks to\nevaluate MuLan. The results demonstrate the superiority of MuLan in generating\nmultiple objects over baselines. The code is available on\nhttps://github.com/measure-infinity/mulan-code.",
        "pdf_link": "https://arxiv.org/pdf/2402.12741v1.pdf"
    },
    {
        "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
        "authors": [
            "Michimasa Inaba",
            "Mariko Ukiyo",
            "Keiko Takamizo"
        ],
        "published": "2024-02-20T06:05:36Z",
        "summary": "Mental health care poses an increasingly serious challenge to modern\nsocieties. In this context, there has been a surge in research that utilizes\ninformation technologies to address mental health problems, including those\naiming to develop counseling dialogue systems. However, there is a need for\nmore evaluations of the performance of counseling dialogue systems that use\nlarge language models. For this study, we collected counseling dialogue data\nvia role-playing scenarios involving expert counselors, and the utterances were\nannotated with the intentions of the counselors. To determine the feasibility\nof a dialogue system in real-world counseling scenarios, third-party counselors\nevaluated the appropriateness of responses from human counselors and those\ngenerated by GPT-4 in identical contexts in role-play dialogue data. Analysis\nof the evaluation results showed that the responses generated by GPT-4 were\ncompetitive with those of human counselors.",
        "pdf_link": "https://arxiv.org/pdf/2402.12738v1.pdf"
    },
    {
        "title": "Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering",
        "authors": [
            "Junnan Dong",
            "Qinggang Zhang",
            "Huachi Zhou",
            "Daochen Zha",
            "Pai Zheng",
            "Xiao Huang"
        ],
        "published": "2024-02-20T05:32:24Z",
        "summary": "Knowledge-based visual question answering (KVQA) has been extensively studied\nto answer visual questions with external knowledge, e.g., knowledge graphs\n(KGs). While several attempts have been proposed to leverage large language\nmodels (LLMs) as an implicit knowledge source, it remains challenging since\nLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,\nimages, KGs and LLMs, cannot be readily aligned for complex scenarios. To\ntackle these, we present a novel modality-aware integration with LLMs for KVQA\n(MAIL). It carefully leverages multimodal knowledge for both image\nunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stage\nprompting strategy with LLMs to densely embody the image into a scene graph\nwith detailed visual features; (ii) We construct a coupled concept graph by\nlinking the mentioned entities with external facts. (iii) A tailored\npseudo-siamese graph medium fusion is designed for sufficient multimodal\nfusion. We utilize the shared mentioned entities in two graphs as mediums to\nbridge a tight inter-modal exchange, while maximally preserving insightful\nintra-modal learning by constraining the fusion within mediums. Extensive\nexperiments on two benchmark datasets show the superiority of MAIL with 24x\nless resources.",
        "pdf_link": "https://arxiv.org/pdf/2402.12728v2.pdf"
    },
    {
        "title": "Are Large Language Models Rational Investors?",
        "authors": [
            "Yuhang Zhou",
            "Yuchen Ni",
            "Xiang Liu",
            "Jian Zhang",
            "Sen Liu",
            "Guangnan Ye",
            "Hongfeng Chai"
        ],
        "published": "2024-02-20T04:26:08Z",
        "summary": "Large Language Models (LLMs) are progressively being adopted in financial\nanalysis to harness their extensive knowledge base for interpreting complex\nmarket data and trends. However, their application in the financial domain is\nchallenged by intrinsic biases (i.e., risk-preference bias) and a superficial\ngrasp of market intricacies, underscoring the need for a thorough assessment of\ntheir financial insight. This study introduces a novel framework, Financial\nBias Indicators (FBI), to critically evaluate the financial rationality of\nLLMs, focusing on their ability to discern and navigate the subtleties of\nfinancial information and to identify any irrational biases that might skew\nmarket analysis.\n  Our research adopts an innovative methodology to measure financial\nrationality, integrating principles of behavioral finance to scrutinize the\nbiases and decision-making patterns of LLMs. We conduct a comprehensive\nevaluation of 19 leading LLMs, considering factors such as model scale,\ntraining datasets, input strategies, etc. The findings reveal varying degrees\nof financial irrationality among the models, influenced by their design and\ntraining. Models trained specifically on financial datasets might exhibit\ngreater irrationality, and it's possible that even larger financial language\nmodels (FinLLMs) could display more biases than smaller, more generalized\nmodels. This outcomes provide profound insights into how these elements affect\nthe financial rationality of LLMs, indicating that targeted training and\nstructured input methods could improve model performance. This work enriches\nour understanding of LLMs' strengths and weaknesses in financial applications,\nlaying the groundwork for the development of more dependable and rational\nfinancial analysis tools.",
        "pdf_link": "https://arxiv.org/pdf/2402.12713v1.pdf"
    },
    {
        "title": "Thermometer: Towards Universal Calibration for Large Language Models",
        "authors": [
            "Maohao Shen",
            "Subhro Das",
            "Kristjan Greenewald",
            "Prasanna Sattigeri",
            "Gregory Wornell",
            "Soumya Ghosh"
        ],
        "published": "2024-02-20T04:13:48Z",
        "summary": "We consider the issue of calibration in large language models (LLM). Recent\nstudies have found that common interventions such as instruction tuning often\nresult in poorly calibrated LLMs. Although calibration is well-explored in\ntraditional applications, calibrating LLMs is uniquely challenging. These\nchallenges stem as much from the severe computational requirements of LLMs as\nfrom their versatility, which allows them to be applied to diverse tasks.\nAddressing these challenges, we propose THERMOMETER, a calibration approach\ntailored to LLMs. THERMOMETER learns an auxiliary model, given data from\nmultiple tasks, for calibrating a LLM. It is computationally efficient,\npreserves the accuracy of the LLM, and produces better-calibrated responses for\nnew tasks. Extensive empirical evaluations across various benchmarks\ndemonstrate the effectiveness of the proposed method.",
        "pdf_link": "https://arxiv.org/pdf/2403.08819v1.pdf"
    },
    {
        "title": "SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning",
        "authors": [
            "Hanchen Xia",
            "Feng Jiang",
            "Naihao Deng",
            "Cunxiang Wang",
            "Guojiang Zhao",
            "Rada Mihalcea",
            "Yue Zhang"
        ],
        "published": "2024-02-20T03:57:55Z",
        "summary": "Modern LLMs have become increasingly powerful, but they are still facing\nchallenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a\nframework to advance LLMs' SQL generation Capabilities through inteRActive\nreFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop\n(IC-Loop) for LLMs to interact with databases automatically, as well as\nPython-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets,\nSpider and Bird, with performance improvements of up to 5.7% compared to the\nnaive prompting method. Moreover, our method surpasses the current\nstate-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of\nour framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.14851v1.pdf"
    },
    {
        "title": "FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning",
        "authors": [
            "Xiao Li",
            "Sichen Liu",
            "Bolin Zhu",
            "Yin Zhu",
            "Yiwei Liu",
            "Gong Cheng"
        ],
        "published": "2024-02-20T03:39:49Z",
        "summary": "The application of formulas is a fundamental ability of humans when\naddressing numerical reasoning problems. However, existing numerical reasoning\ndatasets seldom explicitly indicate the formulas employed during the reasoning\nsteps. To bridge this gap, we propose a question answering dataset for\nformula-based numerical reasoning called FormulaQA, from junior high school\nphysics examinations. We further conduct evaluations on LLMs with size ranging\nfrom 7B to over 100B parameters utilizing zero-shot and few-shot\nchain-of-thoughts methods and we explored the approach of using\nretrieval-augmented LLMs when providing an external formula database. We also\nfine-tune on smaller models with size not exceeding 2B. Our empirical findings\nunderscore the significant potential for improvement in existing models when\napplied to our complex, formula-driven FormulaQA.",
        "pdf_link": "https://arxiv.org/pdf/2402.12692v2.pdf"
    },
    {
        "title": "The FinBen: An Holistic Financial Benchmark for Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Zhengyu Chen",
            "Ruoyu Xiang",
            "Xiao Zhang",
            "Yueru He",
            "Mengxi Xiao",
            "Dong Li",
            "Yongfu Dai",
            "Duanyu Feng",
            "Yijing Xu",
            "Haoqiang Kang",
            "Ziyan Kuang",
            "Chenhan Yuan",
            "Kailai Yang",
            "Zheheng Luo",
            "Tianlin Zhang",
            "Zhiwei Liu",
            "Guojun Xiong",
            "Zhiyang Deng",
            "Yuechen Jiang",
            "Zhiyuan Yao",
            "Haohang Li",
            "Yangyang Yu",
            "Gang Hu",
            "Jiajia Huang",
            "Xiao-Yang Liu",
            "Alejandro Lopez-Lira",
            "Benyou Wang",
            "Yanzhao Lai",
            "Hao Wang",
            "Min Peng",
            "Sophia Ananiadou",
            "Jimin Huang"
        ],
        "published": "2024-02-20T02:16:16Z",
        "summary": "LLMs have transformed NLP and shown promise in various fields, yet their\npotential in finance is underexplored due to a lack of thorough evaluations and\nthe complexity of financial tasks. This along with the rapid development of\nLLMs, highlights the urgent need for a systematic financial evaluation\nbenchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive\nopen-sourced evaluation benchmark, specifically designed to thoroughly assess\nthe capabilities of LLMs in the financial domain. FinBen encompasses 35\ndatasets across 23 financial tasks, organized into three spectrums of\ndifficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs'\ncognitive abilities in inductive reasoning, associative memory, quantitative\nreasoning, crystallized intelligence, and more. Our evaluation of 15\nrepresentative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals\ninsights into their strengths and limitations within the financial domain. The\nfindings indicate that GPT-4 leads in quantification, extraction, numerical\nreasoning, and stock trading, while Gemini shines in generation and\nforecasting; however, both struggle with complex extraction and forecasting,\nshowing a clear need for targeted enhancements. Instruction tuning boosts\nsimple task performance but falls short in improving complex reasoning and\nforecasting abilities. FinBen seeks to continuously evaluate LLMs in finance,\nfostering AI development with regular updates of tasks and models.",
        "pdf_link": "https://arxiv.org/pdf/2402.12659v1.pdf"
    },
    {
        "title": "CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management",
        "authors": [
            "Sinan Abdulhak",
            "Wayne Hubbard",
            "Karthik Gopalakrishnan",
            "Max Z. Li"
        ],
        "published": "2024-02-20T01:59:11Z",
        "summary": "Generative artificial intelligence (AI) and large language models (LLMs) have\ngained rapid popularity through publicly available tools such as ChatGPT. The\nadoption of LLMs for personal and professional use is fueled by the natural\ninteractions between human users and computer applications such as ChatGPT,\nalong with powerful summarization and text generation capabilities. Given the\nwidespread use of such generative AI tools, in this work we investigate how\nthese tools can be deployed in a non-safety critical, strategic traffic flow\nmanagement setting. Specifically, we train an LLM, CHATATC, based on a large\nhistorical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023\nand consisting of over 80,000 GDP implementations, revisions, and\ncancellations. We test the query and response capabilities of CHATATC,\ndocumenting successes (e.g., providing correct GDP rates, durations, and\nreason) and shortcomings (e.g,. superlative questions). We also detail the\ndesign of a graphical user interface for future users to interact and\ncollaborate with the CHATATC conversational agent.",
        "pdf_link": "https://arxiv.org/pdf/2402.14850v1.pdf"
    },
    {
        "title": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation",
        "authors": [
            "Kristian Lum",
            "Jacy Reese Anthis",
            "Chirag Nagpal",
            "Alexander D'Amour"
        ],
        "published": "2024-02-20T01:49:15Z",
        "summary": "Bias benchmarks are a popular method for studying the negative impacts of\nbias in LLMs, yet there has been little empirical investigation of whether\nthese benchmarks are actually indicative of how real world harm may manifest in\nthe real world. In this work, we study the correspondence between such\ndecontextualized \"trick tests\" and evaluations that are more grounded in\nRealistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this\ncorrelation in the context of gender-occupation bias--a popular genre of bias\nevaluation. We compare three de-contextualized evaluations adapted from the\ncurrent literature to three analogous RUTEd evaluations applied to long-form\ncontent generation. We conduct each evaluation for seven instruction-tuned\nLLMs. For the RUTEd evaluations, we conduct repeated trials of three text\ngeneration tasks: children's bedtime stories, user personas, and English\nlanguage learning exercises. We found no correspondence between trick tests and\nRUTEd evaluations. Specifically, selecting the least biased model based on the\nde-contextualized results coincides with selecting the model with the best\nperformance on RUTEd evaluations only as often as random chance. We conclude\nthat evaluations that are not based in realistic use are likely insufficient to\nmitigate and assess bias and real-world harms.",
        "pdf_link": "https://arxiv.org/pdf/2402.12649v1.pdf"
    },
    {
        "title": "Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM",
        "authors": [
            "Hejie Cui",
            "Xinyu Fang",
            "Ran Xu",
            "Xuan Kan",
            "Joyce C. Ho",
            "Carl Yang"
        ],
        "published": "2024-02-19T23:48:40Z",
        "summary": "Electronic Health Records (EHRs) have become increasingly popular to support\nclinical decision-making and healthcare in recent decades. EHRs usually contain\nheterogeneous information, such as structural data in tabular form and\nunstructured data in textual notes. Different types of information in EHRs can\ncomplement each other and provide a more complete picture of the health status\nof a patient. While there has been a lot of research on representation learning\nof structured EHR data, the fusion of different types of EHR data (multimodal\nfusion) is not well studied. This is mostly because of the complex medical\ncoding systems used and the noise and redundancy present in the written notes.\nIn this work, we propose a new framework called MINGLE, which integrates both\nstructures and semantics in EHR effectively. Our framework uses a two-level\ninfusion strategy to combine medical concept semantics and clinical note\nsemantics into hypergraph neural networks, which learn the complex interactions\nbetween different types of data to generate visit representations for\ndownstream prediction. Experiment results on two EHR datasets, the public\nMIMIC-III and private CRADLE, show that MINGLE can effectively improve\npredictive performance by 11.83% relatively, enhancing semantic integration as\nwell as multimodal fusion for structural and textual EHR data.",
        "pdf_link": "https://arxiv.org/pdf/2403.08818v1.pdf"
    },
    {
        "title": "Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation",
        "authors": [
            "Joseph Marvin Imperial",
            "Gail Forey",
            "Harish Tayyar Madabushi"
        ],
        "published": "2024-02-19T23:18:18Z",
        "summary": "Domain experts across engineering, healthcare, and education follow strict\nstandards for producing quality content such as technical manuals, medication\ninstructions, and children's reading materials. However, current works in\ncontrollable text generation have yet to explore using these standards as\nreferences for control. Towards this end, we introduce Standardize, a\nretrieval-style in-context learning-based framework to guide large language\nmodels to align with expert-defined standards. Focusing on English language\nstandards in the education domain as a use case, we consider the Common\nEuropean Framework of Reference for Languages (CEFR) and Common Core Standards\n(CCS) for the task of open-ended content generation. Our findings show that\nmodels can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4,\nrespectively, demonstrating that the use of knowledge artifacts extracted from\nstandards and integrating them in the generation process can effectively guide\nmodels to produce better standard-aligned content.",
        "pdf_link": "https://arxiv.org/pdf/2402.12593v1.pdf"
    },
    {
        "title": "Detecting misinformation through Framing Theory: the Frame Element-based Model",
        "authors": [
            "Guan Wang",
            "Rebecca Frederick",
            "Jinglong Duan",
            "William Wong",
            "Verica Rupar",
            "Weihua Li",
            "Quan Bai"
        ],
        "published": "2024-02-19T21:50:42Z",
        "summary": "In this paper, we delve into the rapidly evolving challenge of misinformation\ndetection, with a specific focus on the nuanced manipulation of narrative\nframes - an under-explored area within the AI community. The potential for\nGenerative AI models to generate misleading narratives underscores the urgency\nof this problem. Drawing from communication and framing theories, we posit that\nthe presentation or 'framing' of accurate information can dramatically alter\nits interpretation, potentially leading to misinformation. We highlight this\nissue through real-world examples, demonstrating how shifts in narrative frames\ncan transmute fact-based information into misinformation. To tackle this\nchallenge, we propose an innovative approach leveraging the power of\npre-trained Large Language Models and deep neural networks to detect\nmisinformation originating from accurate facts portrayed under different\nframes. These advanced AI techniques offer unprecedented capabilities in\nidentifying complex patterns within unstructured data critical for examining\nthe subtleties of narrative frames. The objective of this paper is to bridge a\nsignificant research gap in the AI domain, providing valuable insights and\nmethodologies for tackling framing-induced misinformation, thus contributing to\nthe advancement of responsible and trustworthy AI technologies. Several\nexperiments are intensively conducted and experimental results explicitly\ndemonstrate the various impact of elements of framing theory proving the\nrationale of applying framing theory to increase the performance in\nmisinformation detection.",
        "pdf_link": "https://arxiv.org/pdf/2402.15525v1.pdf"
    },
    {
        "title": "GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence",
        "authors": [
            "Kundan Krishna",
            "Sanjana Ramprasad",
            "Prakhar Gupta",
            "Byron C. Wallace",
            "Zachary C. Lipton",
            "Jeffrey P. Bigham"
        ],
        "published": "2024-02-19T21:45:55Z",
        "summary": "LLMs can generate factually incorrect statements even when provided access to\nreference documents. Such errors can be dangerous in high-stakes applications\n(e.g., document-grounded QA for healthcare or finance). We present GenAudit --\na tool intended to assist fact-checking LLM responses for document-grounded\ntasks. GenAudit suggests edits to the LLM response by revising or removing\nclaims that are not supported by the reference document, and also presents\nevidence from the reference for facts that do appear to have support. We train\nmodels to execute these tasks, and design an interactive interface to present\nsuggested edits and evidence to users. Comprehensive evaluation by human raters\nshows that GenAudit can detect errors in 8 different LLM outputs when\nsummarizing documents from diverse domains. To ensure that most errors are\nflagged by the system, we propose a method that can increase the error recall\nwhile minimizing impact on precision. We release our tool (GenAudit) and\nfact-checking model for public use.",
        "pdf_link": "https://arxiv.org/pdf/2402.12566v2.pdf"
    },
    {
        "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models",
        "authors": [
            "Loka Li",
            "Guangyi Chen",
            "Yusheng Su",
            "Zhenhao Chen",
            "Yixuan Zhang",
            "Eric Xing",
            "Kun Zhang"
        ],
        "published": "2024-02-19T21:38:02Z",
        "summary": "The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.",
        "pdf_link": "https://arxiv.org/pdf/2402.12563v2.pdf"
    },
    {
        "title": "TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness",
        "authors": [
            "Danna Zheng",
            "Danyang Liu",
            "Mirella Lapata",
            "Jeff Z. Pan"
        ],
        "published": "2024-02-19T21:12:14Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, prompting a surge in their practical applications. However,\nconcerns have arisen regarding the trustworthiness of LLMs outputs,\nparticularly in closed-book question-answering tasks, where non-experts may\nstruggle to identify inaccuracies due to the absence of contextual or ground\ntruth information. This paper introduces TrustScore, a framework based on the\nconcept of Behavioral Consistency, which evaluates whether an LLMs response\naligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly\nintegrate with fact-checking methods, which assesses alignment with external\nknowledge sources. The experimental results show that TrustScore achieves\nstrong correlations with human judgments, surpassing existing reference-free\nmetrics, and achieving results on par with reference-based metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.12545v1.pdf"
    },
    {
        "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
        "authors": [
            "Ruibo Chen",
            "Yihan Wu",
            "Lichang Chen",
            "Guodong Liu",
            "Qi He",
            "Tianyi Xiong",
            "Chenxi Liu",
            "Junfeng Guo",
            "Heng Huang"
        ],
        "published": "2024-02-19T20:08:48Z",
        "summary": "Data selection in instruction tuning emerges as a pivotal process for\nacquiring high-quality data and training instruction-following large language\nmodels (LLMs), but it is still a new and unexplored research area for\nvision-language models (VLMs). Existing data selection approaches on LLMs\neither rely on single unreliable scores, or use downstream tasks for selection,\nwhich is time-consuming and can lead to potential over-fitting on the chosen\nevaluation datasets. To address this challenge, we introduce a novel dataset\nselection method, Self-Filter, that utilizes the VLM itself as a filter. This\napproach is inspired by the observation that VLMs benefit from training with\nthe most challenging instructions. Self-Filter operates in two stages. In the\nfirst stage, we devise a scoring network to evaluate the difficulty of training\ninstructions, which is co-trained with the VLM. In the second stage, we use the\ntrained score net to measure the difficulty of each instruction, select the\nmost challenging samples, and penalize similar samples to encourage diversity.\nComprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can\nreach better results compared to full data settings with merely about 15%\nsamples, and can achieve superior performance against competitive baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.12501v1.pdf"
    },
    {
        "title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?",
        "authors": [
            "Nishant Balepur",
            "Abhilasha Ravichander",
            "Rachel Rudinger"
        ],
        "published": "2024-02-19T19:38:58Z",
        "summary": "Multiple-choice question answering (MCQA) is often used to evaluate large\nlanguage models (LLMs). To see if MCQA assesses LLMs as intended, we probe if\nLLMs can perform MCQA with choices-only prompts, where models must select the\ncorrect answer only from the choices. In three MCQA datasets and four LLMs,\nthis prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy\ngain. To help explain this behavior, we conduct an in-depth, black-box analysis\non memorization, choice dynamics, and question inference. Our key findings are\nthreefold. First, we find no evidence that the choices-only accuracy stems from\nmemorization alone. Second, priors over individual choices do not fully explain\nchoices-only accuracy, hinting that LLMs use the group dynamics of choices.\nThird, LLMs have some ability to infer a relevant question from choices, and\nsurprisingly can sometimes even match the original question. We hope to\nmotivate the use of stronger baselines in MCQA benchmarks, the design of robust\nMCQA datasets, and further efforts to explain LLM decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.12483v1.pdf"
    },
    {
        "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",
        "authors": [
            "Zhuoming Chen",
            "Avner May",
            "Ruslan Svirschevski",
            "Yuhsun Huang",
            "Max Ryabinin",
            "Zhihao Jia",
            "Beidi Chen"
        ],
        "published": "2024-02-19T18:58:32Z",
        "summary": "As the usage of large language models (LLMs) grows, performing efficient\ninference with these models becomes increasingly important. While speculative\ndecoding has recently emerged as a promising direction for speeding up\ninference, existing methods are limited in their ability to scale to larger\nspeculation budgets, and adapt to different hyperparameters and hardware. This\npaper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for\nspeculative decoding. To attain better scalability, Sequoia introduces a\ndynamic programming algorithm to find the optimal tree structure for the\nspeculated tokens. To achieve robust speculative performance, Sequoia uses a\nnovel sampling and verification method that outperforms prior work across\ndifferent decoding temperatures. Finally, Sequoia introduces a hardware-aware\ntree optimizer that maximizes speculative performance by automatically\nselecting the token tree size and depth for a given hardware platform.\nEvaluation shows that Sequoia improves the decoding speed of Llama2-7B,\nLlama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.73\\times$, and\n$2.27\\times$. For offloading setting on L40, Sequoia achieves as low as 0.56\ns/token for exact Llama2-70B inference latency, which is $9.96\\times$ on our\noptimized offloading system (5.6 s/token), $9.7\\times$ than\nDeepSpeed-Zero-Inference, $19.5\\times$ than Huggingface Accelerate.",
        "pdf_link": "https://arxiv.org/pdf/2402.12374v2.pdf"
    },
    {
        "title": "AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies",
        "authors": [
            "Xiao Ye",
            "Andrew Wang",
            "Jacob Choi",
            "Yining Lu",
            "Shreya Sharma",
            "Lingfeng Shen",
            "Vijay Tiyyala",
            "Nicholas Andrews",
            "Daniel Khashabi"
        ],
        "published": "2024-02-19T18:56:44Z",
        "summary": "Humans regularly engage in analogical thinking, relating personal experiences\nto current situations ($X$ is analogous to $Y$ because of $Z$). Analogical\nthinking allows humans to solve problems in creative ways, grasp difficult\nconcepts, and articulate ideas more effectively. Can language models (LMs) do\nthe same? To answer this question, we propose ANALOBENCH, a benchmark to\ndetermine analogical reasoning ability in LMs. Our benchmarking approach\nfocuses on aspects of this ability that are common among humans: (i) recalling\nrelated experiences from a large amount of information, and (ii) applying\nanalogical reasoning to complex and lengthy scenarios. We test a broad\ncollection of proprietary models (e.g., GPT family, Claude V2) and open source\nmodels such as LLaMA2. As in prior results, scaling up LMs results in some\nperformance boosts. Surprisingly, scale offers minimal gains when, (i)\nanalogies involve lengthy scenarios, or (ii) recalling relevant scenarios from\na large pool of information, a process analogous to finding a needle in a\nhaystack. We hope these observations encourage further research in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.12370v1.pdf"
    },
    {
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
        "authors": [
            "Archit Sharma",
            "Sedrick Keh",
            "Eric Mitchell",
            "Chelsea Finn",
            "Kushal Arora",
            "Thomas Kollar"
        ],
        "published": "2024-02-19T18:53:54Z",
        "summary": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for\nimproving the instruction-following abilities of powerful pre-trained language\nmodels. RLAIF first performs supervised fine-tuning (SFT) using demonstrations\nfrom a teacher model and then further fine-tunes the model with reinforcement\nlearning (RL), using feedback from a critic model. While recent popular\nopen-source models have demonstrated substantial improvements in performance\nfrom the RL step, in this paper we question whether the complexity of this RL\nstep is truly warranted for AI feedback. We show that the improvements of the\nRL step are virtually entirely due to the widespread practice of using a weaker\nteacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,\nGPT-4) used for AI feedback generation. Specifically, we show that simple\nsupervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF\npipelines. More generally, we find that the gains from RLAIF vary substantially\nacross base model families, test-time evaluation protocols, and critic models.\nFinally, we provide a mechanistic explanation for when SFT may outperform the\nfull two-step RLAIF pipeline as well as suggestions for making RLAIF maximally\nuseful in practice.",
        "pdf_link": "https://arxiv.org/pdf/2402.12366v1.pdf"
    },
    {
        "title": "DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models",
        "authors": [
            "Berkay Berabi",
            "Alexey Gronskiy",
            "Veselin Raychev",
            "Gishor Sivanrupan",
            "Victor Chibotaru",
            "Martin Vechev"
        ],
        "published": "2024-02-19T18:35:40Z",
        "summary": "The automated program repair field has attracted substantial interest over\nthe years, but despite significant research efforts, creating a system that\nworks well for complex semantic bugs such as security vulnerabilities has\nproven difficult. A promising direction to solve this challenge is by\nleveraging large language models (LLMs), which are increasingly used to solve\nvarious programming tasks. In this paper, we investigate the effectiveness of\nLLMs for solving code-repair task. We show that the task is difficult as it\nrequires the model to learn long-range code relationships, a task that\ninherently relies on extensive amounts of training data. At the same time,\ncreating a large, clean dataset for complex program bugs and their\ncorresponding fixes is non-trivial. We propose a technique to address these\nchallenges with a new approach for querying and fine-tuning LLMs. The idea is\nto use program analysis to limit the LLM's attention mechanism on the portions\nof code needed to perform the fix, drastically reducing the amount of required\ntraining data. Concretely, for training and inference, rather than feeding the\nentire program to the LLM, we reduce its code to a much shorter snippet that\ncontains the reported defect together with the necessary context - and use that\ninstead. Our evaluation shows that this code reduction approach substantially\nimproves available models such as GPT-4 using few-shot learning, as well as\nfine-tuning models. To train and evaluate our system, we created a\ncomprehensive code fixing dataset by extensively labeling 156 bug patterns\n(including 40 security rules), requiring complex interprocedural dataflow to\ndiscover. Our best system with Mixtral-8x7B can remove more than 80% of the\nreported defects while exactly matching the human fix in between 10 and 50% of\ncases, outperforming baselines based on GPT-3.5 and GPT-4, or based on\nwindow-based models like TFix.",
        "pdf_link": "https://arxiv.org/pdf/2402.13291v2.pdf"
    },
    {
        "title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
        "authors": [
            "Julien Delile",
            "Srayanta Mukherjee",
            "Anton Van Pamel",
            "Leonid Zhukov"
        ],
        "published": "2024-02-19T18:31:11Z",
        "summary": "Large language models (LLMs) are transforming the way information is\nretrieved with vast amounts of knowledge being summarized and presented via\nnatural language conversations. Yet, LLMs are prone to highlight the most\nfrequently seen pieces of information from the training set and to neglect the\nrare ones. In the field of biomedical research, latest discoveries are key to\nacademic and industrial actors and are obscured by the abundance of an\never-increasing literature corpus (the information overload problem). Surfacing\nnew associations between biomedical entities, e.g., drugs, genes, diseases,\nwith LLMs becomes a challenge of capturing the long-tail knowledge of the\nbiomedical scientific production. To overcome this challenge, Retrieval\nAugmented Generation (RAG) has been proposed to alleviate some of the\nshortcomings of LLMs by augmenting the prompts with context retrieved from\nexternal datasets. RAG methods typically select the context via maximum\nsimilarity search over text embeddings. In this study, we show that RAG methods\nleave out a significant proportion of relevant information due to clusters of\nover-represented concepts in the biomedical literature. We introduce a novel\ninformation-retrieval method that leverages a knowledge graph to downsample\nthese clusters and mitigate the information overload problem. Its retrieval\nperformance is about twice better than embedding similarity alternatives on\nboth precision and recall. Finally, we demonstrate that both embedding\nsimilarity and knowledge graph retrieval methods can be advantageously combined\ninto a hybrid model that outperforms both, enabling potential improvements to\nbiomedical question-answering models.",
        "pdf_link": "https://arxiv.org/pdf/2402.12352v1.pdf"
    },
    {
        "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
        "authors": [
            "Jinhao Duan",
            "Renming Zhang",
            "James Diffenderfer",
            "Bhavya Kailkhura",
            "Lichao Sun",
            "Elias Stengel-Eskin",
            "Mohit Bansal",
            "Tianlong Chen",
            "Kaidi Xu"
        ],
        "published": "2024-02-19T18:23:36Z",
        "summary": "As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely-recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we investigate two key problems: (1) Characterizing\ngame-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning\nevaluation. We observe that (1) LLMs have distinct behaviors regarding various\ngaming scenarios; for example, LLMs fail in complete and deterministic games\nyet they are competitive in probabilistic gaming scenarios; (2) Open-source\nLLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs,\ne.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits\nstrategic reasoning, while advanced reasoning methods such as Chain-of-Thought\n(CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are\nalso provided for a better understanding of LLMs' behavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.12348v1.pdf"
    },
    {
        "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
        "authors": [
            "Zhanhui Zhou",
            "Jie Liu",
            "Zhichen Dong",
            "Jiaheng Liu",
            "Chao Yang",
            "Wanli Ouyang",
            "Yu Qiao"
        ],
        "published": "2024-02-19T18:16:51Z",
        "summary": "Large language models (LLMs) need to undergo safety alignment to ensure safe\nconversations with humans. However, this paper introduces an inference-time\nattack method, demonstrating that safety alignment can be easily reversed to\nproduce harmful language models without additional training. Specifically, this\nreversal is achieved by contrasting the output token distribution of a\nsafety-aligned language model (e.g., Llama-2-chat) against its pre-trained\nversion (e.g., Llama-2) so that the token predictions are shifted towards the\nopposite direction of alignment. We name this method emulated disalignment (ED)\nbecause it uses pure sampling to provably emulate (or \"approximate\") the result\nof fine-tuning the pre-trained model to minimize a safety reward. Our\nexperiments with ED across three evaluation datasets and four model families\n(Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of\npre-trained models and outperforms strong baselines, achieving the highest\nharmful rate in 43 out of 48 evaluation subsets by a large margin. Eventually,\ngiven ED's need for language model output token distributions, which\nparticularly compromises open-source models, our findings highlight the\nimportance of reevaluating the practice of open-sourcing language models even\nafter safety alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.12343v3.pdf"
    },
    {
        "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
        "authors": [
            "Christian Schlarmann",
            "Naman Deep Singh",
            "Francesco Croce",
            "Matthias Hein"
        ],
        "published": "2024-02-19T18:09:48Z",
        "summary": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are\nincreasingly used for various real-world tasks. Prior work has shown that these\nmodels are highly vulnerable to adversarial attacks on the vision modality.\nThese attacks can be leveraged to spread fake information or defraud users, and\nthus pose a significant risk, which makes the robustness of large multi-modal\nfoundation models a pressing problem. The CLIP model, or one of its variants,\nis used as a frozen vision encoder in many vision-language models (VLMs), e.g.\nLLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning\nscheme to obtain a robust CLIP vision encoder, which yields robustness on all\nvision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In\nparticular, we show that stealth-attacks on users of VLMs by a malicious third\nparty providing manipulated images are no longer possible once one replaces the\noriginal CLIP model with our robust one. No retraining or fine-tuning of the\nVLM is required. The code and robust models are available at\nhttps://github.com/chs20/RobustVLM",
        "pdf_link": "https://arxiv.org/pdf/2402.12336v1.pdf"
    },
    {
        "title": "Query-Based Adversarial Prompt Generation",
        "authors": [
            "Jonathan Hayase",
            "Ema Borevkovic",
            "Nicholas Carlini",
            "Florian Tram\u00e8r",
            "Milad Nasr"
        ],
        "published": "2024-02-19T18:01:36Z",
        "summary": "Recent work has shown it is possible to construct adversarial examples that\ncause an aligned language model to emit harmful strings or perform harmful\nbehavior. Existing attacks work either in the white-box setting (with full\naccess to the model weights), or through transferability: the phenomenon that\nadversarial examples crafted on one model often remain effective on other\nmodels. We improve on prior work with a query-based attack that leverages API\naccess to a remote language model to construct adversarial examples that cause\nthe model to emit harmful strings with (much) higher probability than with\ntransfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety\nclassifier; we can cause GPT-3.5 to emit harmful strings that current transfer\nattacks fail at, and we can evade the safety classifier with nearly 100%\nprobability.",
        "pdf_link": "https://arxiv.org/pdf/2402.12329v1.pdf"
    },
    {
        "title": "LLM Agents for Psychology: A Study on Gamified Assessments",
        "authors": [
            "Qisen Yang",
            "Zekun Wang",
            "Honghui Chen",
            "Shenzhi Wang",
            "Yifan Pu",
            "Xin Gao",
            "Wenhao Huang",
            "Shiji Song",
            "Gao Huang"
        ],
        "published": "2024-02-19T18:00:30Z",
        "summary": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.",
        "pdf_link": "https://arxiv.org/pdf/2402.12326v1.pdf"
    },
    {
        "title": "Large Language Model for Mental Health: A Systematic Review",
        "authors": [
            "Zhijun Guo",
            "Alvina Lai",
            "Johan Hilge Thygesen",
            "Joseph Farrington",
            "Thomas Keen",
            "Kezhi Li"
        ],
        "published": "2024-02-19T17:58:41Z",
        "summary": "Large language models (LLMs) have received much attention and shown their\npotential in digital health, while their application in mental health is\nsubject to ongoing debate. This systematic review aims to summarize and\ncharacterize the use of LLMs in mental health by investigating the strengths\nand limitations of the latest work in LLMs and discusses the challenges and\nopportunities for early screening, digital interventions, and other clinical\napplications in mental health. Following PRISMA guidelines, we examined English\narticles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore,\npublished between 1 January 2017, and 1 September 2023, focusing on mental\nhealth and LLMs. The review analyzed 32 articles, including mental health\nanalysis using social media datasets (n=13), mental health chatbots (n=10), and\nother mental health applications (n=9). Findings reveal LLMs' effectiveness in\nmental health issue detection and the enhancement of telepsychological services\nthrough personalised healthcare. Nonetheless, risks like text inconsistencies,\nhallucinatory content, and the lack of an ethical framework raise concerns\nabout their clinical use. Despite these challenges, the advancement of LLMs\nunderscores their potential as innovative clinical tools, necessitating further\nresearch and development. The review emphasizes that LLMs should complement,\nnot replace, professional mental health services.",
        "pdf_link": "https://arxiv.org/pdf/2403.15401v1.pdf"
    },
    {
        "title": "ARKS: Active Retrieval in Knowledge Soup for Code Generation",
        "authors": [
            "Hongjin Su",
            "Shuyang Jiang",
            "Yuhang Lai",
            "Haoyuan Wu",
            "Boao Shi",
            "Che Liu",
            "Qian Liu",
            "Tao Yu"
        ],
        "published": "2024-02-19T17:37:28Z",
        "summary": "Recently the retrieval-augmented generation (RAG) paradigm has raised much\nattention for its potential in incorporating external knowledge into large\nlanguage models (LLMs) without further training. While widely explored in\nnatural language applications, its utilization in code generation remains\nunder-explored. In this paper, we introduce Active Retrieval in Knowledge Soup\n(ARKS), an advanced strategy for generalizing large language models for code.\nIn contrast to relying on a single source, we construct a knowledge soup\nintegrating web search, documentation, execution feedback, and evolved code\nsnippets. We employ an active retrieval strategy that iteratively refines the\nquery and updates the knowledge soup. To assess the performance of ARKS, we\ncompile a new benchmark comprising realistic coding problems associated with\nfrequently updated libraries and long-tail programming languages. Experimental\nresults on ChatGPT and CodeLlama demonstrate a substantial improvement in the\naverage execution accuracy of ARKS on LLMs. The analysis confirms the\neffectiveness of our proposed knowledge soup and active retrieval strategies,\noffering rich insights into the construction of effective retrieval-augmented\ncode generation (RACG) pipelines. Our model, code, and data are available at\nhttps://arks-codegen.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.12317v1.pdf"
    },
    {
        "title": "Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports",
        "authors": [
            "Felix J. Dorfner",
            "Liv J\u00fcrgensen",
            "Leonhard Donle",
            "Fares Al Mohamad",
            "Tobias R. Bodenmann",
            "Mason C. Cleveland",
            "Felix Busch",
            "Lisa C. Adams",
            "James Sato",
            "Thomas Schultz",
            "Albert E. Kim",
            "Jameson Merkow",
            "Keno K. Bressem",
            "Christopher P. Bridge"
        ],
        "published": "2024-02-19T17:23:10Z",
        "summary": "Introduction: With the rapid advances in large language models (LLMs), there\nhave been numerous new open source as well as commercial models. While recent\npublications have explored GPT-4 in its application to extracting information\nof interest from radiology reports, there has not been a real-world comparison\nof GPT-4 to different leading open-source models.\n  Materials and Methods: Two different and independent datasets were used. The\nfirst dataset consists of 540 chest x-ray reports that were created at the\nMassachusetts General Hospital between July 2019 and July 2021. The second\ndataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then\ncompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the\nopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,\nQWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately\nlabel the presence of multiple findings in x-ray text reports using different\nprompting techniques.\n  Results: On the ImaGenome dataset, the best performing open-source model was\nLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot\nprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,\nrespectively. On the institutional dataset, the best performing open-source\nmodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and\nfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and\n0.973, respectively.\n  Conclusion: In this paper, we show that while GPT-4 is superior to\nopen-source models in zero-shot report labeling, the implementation of few-shot\nprompting can bring open-source models on par with GPT-4. This shows that\nopen-source models could be a performant and privacy preserving alternative to\nGPT-4 for the task of radiology report classification.",
        "pdf_link": "https://arxiv.org/pdf/2402.12298v1.pdf"
    },
    {
        "title": "Adaptive Skeleton Graph Decoding",
        "authors": [
            "Shuowei Jin",
            "Yongji Wu",
            "Haizhong Zheng",
            "Qingzhao Zhang",
            "Matthew Lentz",
            "Z. Morley Mao",
            "Atul Prakash",
            "Feng Qian",
            "Danyang Zhuo"
        ],
        "published": "2024-02-19T16:47:04Z",
        "summary": "Large language models (LLMs) have seen significant adoption for natural\nlanguage tasks, owing their success to massive numbers of model parameters\n(e.g., 70B+); however, LLM inference incurs significant computation and memory\ncosts. Recent approaches propose parallel decoding strategies, such as\nSkeleton-of-Thought (SoT), to improve performance by breaking prompts down into\nsub-problems that can be decoded in parallel; however, they often suffer from\nreduced response quality. Our key insight is that we can request additional\ninformation, specifically dependencies and difficulty, when generating the\nsub-problems to improve both response quality and performance. In this paper,\nwe propose Skeleton Graph Decoding (SGD), which uses dependencies exposed\nbetween sub-problems to support information forwarding between dependent\nsub-problems for improved quality while exposing parallelization opportunities\nfor decoding independent sub-problems. Additionally, we leverage difficulty\nestimates for each sub-problem to select an appropriately-sized model,\nimproving performance without significantly reducing quality. Compared to\nstandard autoregressive generation and SoT, SGD achieves a 1.69x speedup while\nimproving quality by up to 51%.",
        "pdf_link": "https://arxiv.org/pdf/2402.12280v1.pdf"
    },
    {
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
        "authors": [
            "Hao Tang",
            "Darren Key",
            "Kevin Ellis"
        ],
        "published": "2024-02-19T16:39:18Z",
        "summary": "We give a model-based agent that builds a Python program representing its\nknowledge of the world based on its interactions with the environment. The\nworld model tries to explain its interactions, while also being optimistic\nabout what reward it can achieve. We do this by extending work on program\nsynthesis via LLMs. We study our agent on gridworlds, finding our approach is\nmore sample-efficient compared to deep RL, and more compute-efficient compared\nto ReAct-style agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.12275v1.pdf"
    },
    {
        "title": "Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data",
        "authors": [
            "Naihao Deng",
            "Zhenjie Sun",
            "Ruiqi He",
            "Aman Sikka",
            "Yulong Chen",
            "Lin Ma",
            "Yue Zhang",
            "Rada Mihalcea"
        ],
        "published": "2024-02-19T16:34:50Z",
        "summary": "In this paper, we investigate the effectiveness of various LLMs in\ninterpreting tabular data through different prompting strategies and data\nformats. Our analysis extends across six benchmarks for table-related tasks\nsuch as question-answering and fact-checking. We introduce for the first time\nthe assessment of LLMs' performance on image-based table representations.\nSpecifically, we compare five text-based and three image-based table\nrepresentations, demonstrating the influence of representation and prompting on\nLLM performance. Our study provides insights into the effective use of LLMs on\ntable-related tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.12424v3.pdf"
    },
    {
        "title": "High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models",
        "authors": [
            "Michela Lorandi",
            "Anya Belz"
        ],
        "published": "2024-02-19T16:29:40Z",
        "summary": "The performance of NLP methods for severely under-resourced languages cannot\ncurrently hope to match the state of the art in NLP methods for well resourced\nlanguages. We explore the extent to which pretrained large language models\n(LLMs) can bridge this gap, via the example of data-to-text generation for\nIrish, Welsh, Breton and Maltese. We test LLMs on these under-resourced\nlanguages and English, in a range of scenarios. We find that LLMs easily set\nthe state of the art for the under-resourced languages by substantial margins,\nas measured by both automatic and human evaluations. For all our languages,\nhuman evaluation shows on-a-par performance with humans for our best systems,\nbut BLEU scores collapse compared to English, casting doubt on the metric's\nsuitability for evaluating non-task-specific systems. Overall, our results\ndemonstrate the great potential of LLMs to bridge the performance gap for\nunder-resourced languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.12267v1.pdf"
    },
    {
        "title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles",
        "authors": [
            "Oleksandr Balabanov",
            "Hampus Linander"
        ],
        "published": "2024-02-19T16:26:00Z",
        "summary": "Fine-tuning large language models can improve task specific performance,\nalthough a general understanding of what the fine-tuned model has learned,\nforgotten and how to trust its predictions is still missing. We derive\nprincipled uncertainty quantification for fine-tuned LLMs with posterior\napproximations using computationally efficient low-rank adaptation ensembles.\nWe analyze three common multiple-choice datasets using low-rank adaptation\nensembles based on Mistral-7b, and draw quantitative and qualitative\nconclusions on their perceived complexity and model efficacy on the different\ntarget domains during and after fine-tuning. In particular, backed by the\nnumerical experiments, we hypothesise about signals from entropic uncertainty\nmeasures for data domains that are inherently difficult for a given\narchitecture to learn.",
        "pdf_link": "https://arxiv.org/pdf/2402.12264v1.pdf"
    },
    {
        "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
        "authors": [
            "Jonathan Zheng",
            "Alan Ritter",
            "Wei Xu"
        ],
        "published": "2024-02-19T16:19:15Z",
        "summary": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2402.12261v2.pdf"
    },
    {
        "title": "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships",
        "authors": [
            "Sebastian Koch",
            "Narunas Vaskevicius",
            "Mirco Colosi",
            "Pedro Hermosilla",
            "Timo Ropinski"
        ],
        "published": "2024-02-19T16:15:03Z",
        "summary": "Current approaches for 3D scene graph prediction rely on labeled datasets to\ntrain models for a fixed set of known object classes and relationship\ncategories. We present Open3DSG, an alternative approach to learn 3D scene\ngraph prediction in an open world without requiring labeled scene graph data.\nWe co-embed the features from a 3D scene graph prediction backbone with the\nfeature space of powerful open world 2D vision language foundation models. This\nenables us to predict 3D scene graphs from 3D point clouds in a zero-shot\nmanner by querying object classes from an open vocabulary and predicting the\ninter-object relationships from a grounded LLM with scene graph features and\nqueried object classes as context. Open3DSG is the first 3D point cloud method\nto predict not only explicit open-vocabulary object classes, but also open-set\nrelationships that are not limited to a predefined label set, making it\npossible to express rare as well as specific objects and relationships in the\npredicted 3D scene graph. Our experiments show that Open3DSG is effective at\npredicting arbitrary object classes as well as their complex inter-object\nrelationships describing spatial, supportive, semantic and comparative\nrelationships.",
        "pdf_link": "https://arxiv.org/pdf/2402.12259v2.pdf"
    },
    {
        "title": "Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition",
        "authors": [
            "Anna Martin-Boyle",
            "Aahan Tyagi",
            "Marti A. Hearst",
            "Dongyeop Kang"
        ],
        "published": "2024-02-19T16:14:04Z",
        "summary": "Numerous AI-assisted scholarly applications have been developed to aid\ndifferent stages of the research process. We present an analysis of AI-assisted\nscholarly writing generated with ScholaCite, a tool we built that is designed\nfor organizing literature and composing Related Work sections for academic\npapers. Our evaluation method focuses on the analysis of citation graphs to\nassess the structural complexity and inter-connectedness of citations in texts\nand involves a three-way comparison between (1) original human-written texts,\n(2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find\nthat GPT-4 can generate reasonable coarse-grained citation groupings to support\nhuman users in brainstorming, but fails to perform detailed synthesis of\nrelated works without human intervention. We suggest that future writing\nassistant tools should not be used to draft text independently of the human\nauthor.",
        "pdf_link": "https://arxiv.org/pdf/2402.12255v1.pdf"
    },
    {
        "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
        "authors": [
            "Mosh Levy",
            "Alon Jacoby",
            "Yoav Goldberg"
        ],
        "published": "2024-02-19T16:04:53Z",
        "summary": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that traditional\nperplexity metrics do not correlate with performance of LLMs' in long input\nreasoning tasks. We analyse our results and identify failure modes that can\nserve as useful guides for future research, potentially informing strategies to\naddress the limitations observed in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14848v1.pdf"
    },
    {
        "title": "CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation",
        "authors": [
            "Jueon Eom",
            "Seyeon Jeong",
            "Taekyoung Kwon"
        ],
        "published": "2024-02-19T15:30:40Z",
        "summary": "Fuzzing is an effective bug-finding technique but it struggles with complex\nsystems like JavaScript engines that demand precise grammatical input.\nRecently, researchers have adopted language models for context-aware mutation\nin fuzzing to address this problem. However, existing techniques are limited in\nutilizing coverage guidance for fuzzing, which is rather performed in a\nblack-box manner. This paper presents a novel technique called CovRL\n(Coverage-guided Reinforcement Learning) that combines Large Language Models\n(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,\nCovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging\nthe Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a\nweighted coverage map. This map is key in calculating the fuzzing reward, which\nis then applied to the LLM-based mutator through reinforcement learning.\nCovRL-Fuzz, through this approach, enables the generation of test cases that\nare more likely to discover new coverage areas, thus improving vulnerability\ndetection while minimizing syntax and semantic errors, all without needing\nextra post-processing. Our evaluation results indicate that CovRL-Fuzz\noutperforms the state-of-the-art fuzzers in terms of code coverage and\nbug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related\nbugs in the latest JavaScript engines, including 39 previously unknown\nvulnerabilities and 11 CVEs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12222v1.pdf"
    },
    {
        "title": "Reformatted Alignment",
        "authors": [
            "Run-Ze Fan",
            "Xuefeng Li",
            "Haoyang Zou",
            "Junlong Li",
            "Shwai He",
            "Ethan Chern",
            "Jiewen Hu",
            "Pengfei Liu"
        ],
        "published": "2024-02-19T15:21:58Z",
        "summary": "The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.",
        "pdf_link": "https://arxiv.org/pdf/2402.12219v1.pdf"
    },
    {
        "title": "Polarization of Autonomous Generative AI Agents Under Echo Chambers",
        "authors": [
            "Masaya Ohagi"
        ],
        "published": "2024-02-19T15:14:15Z",
        "summary": "Online social networks often create echo chambers where people only hear\nopinions reinforcing their beliefs. An echo chamber often generates\npolarization, leading to conflicts caused by people with radical opinions, such\nas the January 6, 2021, attack on the US Capitol. The echo chamber has been\nviewed as a human-specific problem, but this implicit assumption is becoming\nless reasonable as large language models, such as ChatGPT, acquire social\nabilities. In response to this situation, we investigated the potential for\npolarization to occur among a group of autonomous AI agents based on generative\nlanguage models in an echo chamber environment. We had AI agents discuss\nspecific topics and analyzed how the group's opinions changed as the discussion\nprogressed. As a result, we found that the group of agents based on ChatGPT\ntended to become polarized in echo chamber environments. The analysis of\nopinion transitions shows that this result is caused by ChatGPT's high prompt\nunderstanding ability to update its opinion by considering its own and\nsurrounding agents' opinions. We conducted additional experiments to\ninvestigate under what specific conditions AI agents tended to polarize. As a\nresult, we identified factors that strongly influence polarization, such as the\nagent's persona. These factors should be monitored to prevent the polarization\nof AI agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.12212v1.pdf"
    },
    {
        "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
        "authors": [
            "Yuanchi Zhang",
            "Yile Wang",
            "Zijun Liu",
            "Shuo Wang",
            "Xiaolong Wang",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-19T15:07:32Z",
        "summary": "While large language models (LLMs) have been pre-trained on multilingual\ncorpora, their performance still lags behind in most languages compared to a\nfew resource-rich languages. One common approach to mitigate this issue is to\ntranslate training data from resource-rich languages into other languages and\nthen continue training. However, using the data obtained solely relying on\ntranslation while ignoring the original capabilities of LLMs across languages\nis not always effective, which we show will limit the performance of\ncross-lingual knowledge transfer. In this work, we propose SDRRL, a method\nbased on Self-Distillation from Resource-Rich Languages that effectively\nimprove multilingual performance by leveraging the internal capabilities of\nLLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and\nSeaLLM) and source languages across various comprehension and generation tasks,\nexperimental results demonstrate that SDRRL can significantly enhance\nmultilingual capabilities while minimizing the impact on original performance\nin resource-rich languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.12204v1.pdf"
    },
    {
        "title": "Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion",
        "authors": [
            "Ziyue Wang",
            "Chi Chen",
            "Yiqi Zhu",
            "Fuwen Luo",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-19T14:59:07Z",
        "summary": "With the bloom of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs) that incorporate LLMs with pre-trained vision models have\nrecently demonstrated impressive performance across diverse vision-language\ntasks. However, they fall short to comprehend context involving multiple\nimages. A primary reason for this shortcoming is that the visual features for\neach images are encoded individually by frozen encoders before feeding into the\nLLM backbone, lacking awareness of other images and the multimodal\ninstructions. We term this issue as prior-LLM modality isolation and propose a\ntwo phase paradigm, browse-and-concentrate, to enable in-depth multimodal\ncontext fusion prior to feeding the features into LLMs. This paradigm initially\n\"browses\" through the inputs for essential insights, and then revisits the\ninputs to \"concentrate\" on crucial details, guided by these insights, to\nachieve a more comprehensive understanding of the multimodal inputs.\nAdditionally, we develop training strategies specifically to enhance the\nunderstanding of multi-image inputs. Our method markedly boosts the performance\non 7 multi-image scenarios, contributing to increments on average accuracy by\n2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.12195v1.pdf"
    },
    {
        "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
        "authors": [
            "Yuxia Wang",
            "Zenan Zhai",
            "Haonan Li",
            "Xudong Han",
            "Lizhi Lin",
            "Zhenxuan Zhang",
            "Jingru Zhao",
            "Preslav Nakov",
            "Timothy Baldwin"
        ],
        "published": "2024-02-19T14:56:18Z",
        "summary": "Many studies have demonstrated that large language models (LLMs) can produce\nharmful responses, exposing users to unexpected risks when LLMs are deployed.\nPrevious studies have proposed comprehensive taxonomies of the risks posed by\nLLMs, as well as corresponding prompts that can be used to examine the safety\nmechanisms of LLMs. However, the focus has been almost exclusively on English,\nand little has been explored for other languages. Here we aim to bridge this\ngap. We first introduce a dataset for the safety evaluation of Chinese LLMs,\nand then extend it to two other scenarios that can be used to better identify\nfalse negative and false positive examples in terms of risky prompt rejections.\nWe further present a set of fine-grained safety assessment criteria for each\nrisk type, facilitating both manual annotation and automatic evaluation in\nterms of LLM response harmfulness. Our experiments on five LLMs show that\nregion-specific risks are the prevalent type of risk, presenting the major\nissue with all Chinese LLMs we experimented with. Warning: this paper contains\nexample data that may be offensive, harmful, or biased.",
        "pdf_link": "https://arxiv.org/pdf/2402.12193v1.pdf"
    },
    {
        "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
        "authors": [
            "Grgur Kova\u010d",
            "R\u00e9my Portelas",
            "Masataka Sawayama",
            "Peter Ford Dominey",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2024-02-19T14:53:01Z",
        "summary": "The standard way to study Large Language Models (LLMs) through benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLM's highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence should be\nstudied as another dimension of LLM comparison alongside others such as\ncognitive abilities, knowledge, or model size. In this paper, we present a\ncase-study about the stability of value expression over different contexts\n(simulated conversations on different topics), and as measured using a standard\npsychology questionnaire (PVQ) and a behavioral downstream task. We consider 19\nopen-sourced LLMs from five families. Reusing methods from psychology, we study\nRank-order stability on the population (interpersonal) level, and Ipsative\nstability on the individual (intrapersonal) level. We explore two settings:\nwith and without instructing LLMs to simulate particular personalities. We\nobserve similar trends in the stability of models and model families - Mixtral,\nMistral and Qwen families being more stable than LLaMa-2 and Phi - over those\ntwo settings, two different simulated populations, and even in the downstream\nbehavioral task. When instructed to simulate particular personas, LLMs exhibit\nlow Rank-Order stability, and this stability further diminishes with\nconversation length. This highlights the need for future research directions on\nLLMs that can coherently simulate a diversity of personas, as well as how\ncontext-dependence can be studied in more thorough and efficient ways. This\npaper provides a foundational step in that direction, and, to our knowledge, it\nis the first study of value stability in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14846v1.pdf"
    },
    {
        "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
        "authors": [
            "Mingtian Zhang",
            "Shawn Lan",
            "Peter Hayes",
            "David Barber"
        ],
        "published": "2024-02-19T14:33:24Z",
        "summary": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.12177v4.pdf"
    },
    {
        "title": "BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence",
        "authors": [
            "Jiajie Jin",
            "Yutao Zhu",
            "Yujia Zhou",
            "Zhicheng Dou"
        ],
        "published": "2024-02-19T14:28:31Z",
        "summary": "Retrieval-augmented large language models (LLMs) have demonstrated efficacy\nin knowledge-intensive tasks such as open-domain QA, addressing inherent\nchallenges in knowledge update and factual inadequacy. However, inconsistencies\nbetween retrieval knowledge and the necessary knowledge for LLMs, leading to a\ndecline in LLM's answer quality. This paper introduces BIDER, an approach that\nrefines retrieval documents into Key Supporting Evidence (KSE) through\nknowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We\ntrain BIDER by learning from crafting KSE, while maximizing its output to align\nwith LLM's information acquisition preferences through reinforcement learning.\nEvaluations across five datasets show BIDER boosts LLMs' answer quality by 7%\nwhile reducing input content length in retrieval documents by 80%,\noutperforming existing methods. The proposed KSE simulation effectively equips\nLLMs with essential information for accurate question answering.",
        "pdf_link": "https://arxiv.org/pdf/2402.12174v1.pdf"
    },
    {
        "title": "Transformer-based Causal Language Models Perform Clustering",
        "authors": [
            "Xinbo Wu",
            "Lav R. Varshney"
        ],
        "published": "2024-02-19T14:02:31Z",
        "summary": "Even though large language models (LLMs) have demonstrated remarkable\ncapability in solving various natural language tasks, the capability of an LLM\nto follow human instructions is still a concern. Recent works have shown great\nimprovements in the instruction-following capability via additional training\nfor instruction-following tasks. However, the mechanisms responsible for\neffective instruction-following capabilities remain inadequately understood.\nHere, we introduce a simplified instruction-following task and use synthetic\ndatasets to analyze a Transformer-based causal language model. Our findings\nsuggest that the model learns task-specific information by clustering data\nwithin its hidden space, with this clustering process evolving dynamically\nduring learning. We also demonstrate how this phenomenon assists the model in\nhandling unseen instances, and validate our results in a more realistic\nsetting. Furthermore, we present inspired applications regarding pre-training\nand alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.12151v2.pdf"
    },
    {
        "title": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
        "authors": [
            "Tianlin Li",
            "Xiaoyu Zhang",
            "Chao Du",
            "Tianyu Pang",
            "Qian Liu",
            "Qing Guo",
            "Chao Shen",
            "Yang Liu"
        ],
        "published": "2024-02-19T14:02:22Z",
        "summary": "The widespread adoption of large language models (LLMs) underscores the\nurgent need to ensure their fairness. However, LLMs frequently present dominant\nviewpoints while ignoring alternative perspectives from minority parties,\nresulting in potential biases. We hypothesize that these fairness-violating\nbehaviors occur because LLMs express their viewpoints using a human personality\nthat represents the majority of training data. In response to this, we validate\nthat prompting LLMs with specific roles can allow LLMs to express diverse\nviewpoints. Building on this insight and observation, we develop FairThinking,\na pipeline designed to automatically generate roles that enable LLMs to\narticulate diverse perspectives for fair expressions. To evaluate FairThinking,\nwe create a dataset with a thousand items covering three fairness-related\ntopics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to\ndemonstrate its superior performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.12150v1.pdf"
    },
    {
        "title": "Purifying Large Language Models by Ensembling a Small Language Model",
        "authors": [
            "Tianlin Li",
            "Qian Liu",
            "Tianyu Pang",
            "Chao Du",
            "Qing Guo",
            "Yang Liu",
            "Min Lin"
        ],
        "published": "2024-02-19T14:00:39Z",
        "summary": "The emerging success of large language models (LLMs) heavily relies on\ncollecting abundant training data from external (untrusted) sources. Despite\nsubstantial efforts devoted to data cleaning and curation, well-constructed\nLLMs have been reported to suffer from copyright infringement, data poisoning,\nand/or privacy violations, which would impede practical deployment of LLMs. In\nthis study, we propose a simple and easily implementable method for purifying\nLLMs from the negative effects caused by uncurated data, namely, through\nensembling LLMs with benign and small language models (SLMs). Aside from\ntheoretical guarantees, we perform comprehensive experiments to empirically\nconfirm the efficacy of ensembling LLMs with SLMs, which can effectively\npreserve the performance of LLMs while mitigating issues such as copyright\ninfringement, data poisoning, and privacy violations.",
        "pdf_link": "https://arxiv.org/pdf/2402.14845v1.pdf"
    },
    {
        "title": "End-to-end multilingual fact-checking at scale",
        "authors": [
            "Vinay Setty"
        ],
        "published": "2024-02-19T14:00:35Z",
        "summary": "In this article, we describe how you can perform end-to-end fact-checking in\nover 100 languages using Factiverse AI models. We also show through an\nexperimental benchmark that fine-tuned models tailored for fact-checking tasks\noutperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.",
        "pdf_link": "https://arxiv.org/pdf/2402.12147v1.pdf"
    },
    {
        "title": "Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement",
        "authors": [
            "Zijun Liu",
            "Boqun Kou",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "published": "2024-02-19T13:57:55Z",
        "summary": "Although Large Language Models (LLMs) have demonstrated strong performance on\na wide range of tasks, they still face reliability challenges such as\nhallucination. Previous studies reveal that highly capable LLMs like GPT-4 are\neffective in judging the reliability of individual responses, while less\ncapable ones are often tuned to evaluate the relative reliability of responses\nto the same query. To enable less capable LLMs to effectively judge the\nreliability of individual responses, we propose a novel method named\n$\\textit{Meta}$ $\\textit{Ranking}$ (MR). Unlike previous methods, which assess\nthe response directly, we achieve the judgement by comparing the target\nquery-response pair with reference query-response pairs. We found its\nremarkable effectiveness in error detection for LLM responses on reasoning\ntasks, where less capable LLMs could outperform strong baselines, even without\nfine-tuning. We further demonstrate that MR can be used to enhance the\nperformance of LLMs in two practical applications: query routing and iterative\ntraining data filtering. The former achieves GPT-4-turbo comparable performance\nwith less than half the token consumption, while the latter makes the\ninstruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass\nAlpaca-13B over fewer training samples, underscoring the high potential of our\nproposed method.",
        "pdf_link": "https://arxiv.org/pdf/2402.12146v1.pdf"
    },
    {
        "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
        "authors": [
            "Junbing Yan",
            "Chengyu Wang",
            "Jun Huang",
            "Wei Zhang"
        ],
        "published": "2024-02-19T12:12:35Z",
        "summary": "Over the past few years, the abilities of large language models (LLMs) have\nreceived extensive attention, which have performed exceptionally well in\ncomplicated scenarios such as logical reasoning and symbolic inference. A\nsignificant factor contributing to this progress is the benefit of in-context\nlearning and few-shot prompting. However, the reasons behind the success of\nsuch models using contextual reasoning have not been fully explored. Do LLMs\nhave understand logical rules to draw inferences, or do they ``guess'' the\nanswers by learning a type of probabilistic mapping through context? This paper\ninvestigates the reasoning capabilities of LLMs on two logical reasoning\ndatasets by using counterfactual methods to replace context text and modify\nlogical concepts. Based on our analysis, it is found that LLMs do not truly\nunderstand logical rules; rather, in-context learning has simply enhanced the\nlikelihood of these models arriving at the correct answers. If one alters\ncertain words in the context text or changes the concepts of logical terms, the\noutputs of LLMs can be significantly disrupted, leading to counter-intuitive\nresponses. This work provides critical insights into the limitations of LLMs,\nunderscoring the need for more robust mechanisms to ensure reliable logical\nreasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12091v1.pdf"
    },
    {
        "title": "Can LLMs Compute with Reasons?",
        "authors": [
            "Harshit Sandilya",
            "Peehu Raj",
            "Jainit Sushil Bafna",
            "Srija Mukhopadhyay",
            "Shivansh Sharma",
            "Ellwil Sharma",
            "Arastu Sharma",
            "Neeta Trivedi",
            "Manish Shrivastava",
            "Rajesh Kumar"
        ],
        "published": "2024-02-19T12:04:25Z",
        "summary": "Large language models (LLMs) often struggle with complex mathematical tasks,\nprone to \"hallucinating\" incorrect answers due to their reliance on statistical\npatterns. This limitation is further amplified in average Small LangSLMs with\nlimited context and training data. To address this challenge, we propose an\n\"Inductive Learning\" approach utilizing a distributed network of SLMs. This\nnetwork leverages error-based learning and hint incorporation to refine the\nreasoning capabilities of SLMs. Our goal is to provide a framework that\nempowers SLMs to approach the level of logic-based applications achieved by\nhigh-parameter models, potentially benefiting any language model. Ultimately,\nthis novel concept paves the way for bridging the logical gap between humans\nand LLMs across various fields.",
        "pdf_link": "https://arxiv.org/pdf/2402.12080v1.pdf"
    },
    {
        "title": "LVCHAT: Facilitating Long Video Comprehension",
        "authors": [
            "Yu Wang",
            "Zeyuan Zhang",
            "Julian McAuley",
            "Zexue He"
        ],
        "published": "2024-02-19T11:59:14Z",
        "summary": "Enabling large language models (LLMs) to read videos is vital for multimodal\nLLMs. Existing works show promise on short videos whereas long video (longer\nthan e.g.~1 minute) comprehension remains challenging. The major problem lies\nin the over-compression of videos, i.e., the encoded video representations are\nnot enough to represent the whole video. To address this issue, we propose Long\nVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to\ndynamically adjust the number of embeddings in alignment with the duration of\nthe video to ensure long videos are not overly compressed into a few\nembeddings. To deal with long videos whose length is beyond videos seen during\ntraining, we propose Interleaved Frame Encoding (IFE), repeating positional\nembedding and interleaving multiple groups of videos to enable long video\ninput, avoiding performance degradation due to overly long videos. Experimental\nresults show that LVChat significantly outperforms existing methods by up to\n27\\% in accuracy on long-video QA datasets and long-video captioning\nbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.",
        "pdf_link": "https://arxiv.org/pdf/2402.12079v1.pdf"
    },
    {
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
        "authors": [
            "Sahand Sabour",
            "Siyang Liu",
            "Zheyuan Zhang",
            "June M. Liu",
            "Jinfeng Zhou",
            "Alvionna S. Sunaryo",
            "Juanzi Li",
            "Tatia M. C. Lee",
            "Rada Mihalcea",
            "Minlie Huang"
        ],
        "published": "2024-02-19T11:48:09Z",
        "summary": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data will\nbe publicly available from https://github.com/Sahandfer/EmoBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.12071v1.pdf"
    },
    {
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
        "authors": [
            "Yuxuan Yue",
            "Zhihang Yuan",
            "Haojie Duanmu",
            "Sifan Zhou",
            "Jianlong Wu",
            "Liqiang Nie"
        ],
        "published": "2024-02-19T11:33:21Z",
        "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.",
        "pdf_link": "https://arxiv.org/pdf/2402.12065v2.pdf"
    },
    {
        "title": "All Language Models Large and Small",
        "authors": [
            "Zhixun Chen",
            "Yali Du",
            "David Mguni"
        ],
        "published": "2024-02-19T11:28:20Z",
        "summary": "Many leading language models (LMs) use high-intensity computational resources\nboth during training and execution. This poses the challenge of lowering\nresource costs for deployment and faster execution of decision-making tasks\namong others. We introduce a novel plug-and-play LM framework named Language\nOptimising Network Distribution (LONDI) framework. LONDI learns to selectively\nemploy large LMs only where complex decision-making and reasoning are required\nwhile using low-resource LMs everywhere else. LONDI consists of a system of two\n(off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning\nmodule that uses switching controls to quickly learn which system states to\ncall the LLM. We then introduce a variant of LONDI that maintains budget\nconstraints on LLM calls and hence its resource usage. Theoretically, we prove\nLONDI learns the subset of system states to activate the LLM required to solve\nthe task. We then prove that LONDI converges to optimal solutions while also\npreserving budgetary constraints on LLM calls almost surely enabling it to\nsolve various tasks while significantly lowering computational costs. We test\nLONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and\ndemonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs\nwhile reducing GPU usage by up to 30%.",
        "pdf_link": "https://arxiv.org/pdf/2402.12061v1.pdf"
    },
    {
        "title": "Are LLM-based Evaluators Confusing NLG Quality Criteria?",
        "authors": [
            "Xinyu Hu",
            "Mingqi Gao",
            "Sen Hu",
            "Yang Zhang",
            "Yicheng Chen",
            "Teng Xu",
            "Xiaojun Wan"
        ],
        "published": "2024-02-19T11:19:02Z",
        "summary": "Some prior work has shown that LLMs perform well in NLG evaluation for\ndifferent tasks. However, we discover that LLMs seem to confuse different\nevaluation criteria, which reduces their reliability. For further verification,\nwe first consider avoiding issues of inconsistent conceptualization and vague\nexpression in existing NLG quality criteria themselves. So we summarize a clear\nhierarchical classification system for 11 common aspects with corresponding\ndifferent criteria from previous studies involved. Inspired by behavioral\ntesting, we elaborately design 18 types of aspect-targeted perturbation attacks\nfor fine-grained analysis of the evaluation behaviors of different LLMs. We\nalso conduct human annotations beyond the guidance of the classification system\nto validate the impact of the perturbations. Our experimental results reveal\nconfusion issues inherent in LLMs, as well as other noteworthy phenomena, and\nnecessitate further research and improvements for LLM-based evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.12055v1.pdf"
    },
    {
        "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
        "authors": [
            "Didi Zhu",
            "Zhongyi Sun",
            "Zexi Li",
            "Tao Shen",
            "Ke Yan",
            "Shouhong Ding",
            "Kun Kuang",
            "Chao Wu"
        ],
        "published": "2024-02-19T11:02:05Z",
        "summary": "Catastrophic forgetting emerges as a critical challenge when fine-tuning\nmulti-modal large language models (MLLMs), where improving performance on\nunseen tasks often leads to a significant performance drop on the original\ntasks. This paper presents a comprehensive analysis of catastrophic forgetting\nin MLLMs and introduces a post-training adjustment method called Model Tailor.\nOur method primarily preserves the pre-trained parameters while replacing a\nsmall number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\%\neffectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\%\non new tasks compared to standard fine-tuning. Specifically, we derive a sparse\nmask to identify the \"model patch\", based on a fusion strategy that integrates\nsalience and sensitivity analysis. Subsequently, a compensation mechanism is\nintroduced to \"decorate the patch\", enhancing the model's performance on both\ntarget and original tasks. Additionally, our method is adaptable to multi-task\nscenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both\nimage captioning and visual question answering tasks, our approach demonstrates\nsignificant task adaptability while preserving inherent pre-trained\ncapabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.12048v1.pdf"
    },
    {
        "title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations",
        "authors": [
            "Milan Bhan",
            "Jean-Noel Vittaut",
            "Nicolas Chesneau",
            "Marie-Jeanne Lesot"
        ],
        "published": "2024-02-19T10:47:09Z",
        "summary": "Incorporating natural language rationales in the prompt and In-Context\nLearning (ICL) has led to a significant improvement of Large Language Models\n(LLMs) performance. However, rationales currently require human-annotation or\nthe use of auxiliary proxy models to target promising samples or generate\nhigh-quality rationales. In this work, we propose Self-AMPLIFY to generate\nautomatically rationales from post hoc explanation methods applied to Small\nLanguage Models (SLMs) to improve their own performance. Self-AMPLIFY is a\n3-step method that targets samples, generates rationales and builds a final\nprompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and\ntwo datasets requiring reasoning abilities: these experiments show that\nSelf-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the\nfirst method to apply post hoc explanation methods to SLM to generate\nrationales to improve their own performance in a fully automated manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.12038v1.pdf"
    },
    {
        "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
        "authors": [
            "Nicolas Boizard",
            "Kevin El Haddad",
            "C\u00e9line Hudelot",
            "Pierre Colombo"
        ],
        "published": "2024-02-19T10:37:29Z",
        "summary": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques.",
        "pdf_link": "https://arxiv.org/pdf/2402.12030v2.pdf"
    },
    {
        "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space",
        "authors": [
            "Zongru Wu",
            "Zhuosheng Zhang",
            "Pengzhou Cheng",
            "Gongshen Liu"
        ],
        "published": "2024-02-19T10:34:48Z",
        "summary": "Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, and Llama2. The codes are\navailable at https://github.com/ZrW00/MuScleLoRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.12026v2.pdf"
    },
    {
        "title": "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?",
        "authors": [
            "Marco Gaido",
            "Sara Papi",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2024-02-19T10:34:13Z",
        "summary": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.",
        "pdf_link": "https://arxiv.org/pdf/2402.12025v1.pdf"
    },
    {
        "title": "Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought",
        "authors": [
            "Yuying Du",
            "Xueyan Tang"
        ],
        "published": "2024-02-19T10:33:29Z",
        "summary": "Smart contracts, as a key component of blockchain technology, play a crucial\nrole in ensuring the automation of transactions and adherence to protocol\nrules. However, smart contracts are susceptible to security vulnerabilities,\nwhich, if exploited, can lead to significant asset losses. This study explores\nthe potential of enhancing smart contract security audits using the GPT-4\nmodel. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark\nvulnerability library, containing 732 vulnerabilities, and compared it with\nfive other vulnerability detection tools to evaluate GPT-4's ability to\nidentify seven common types of vulnerabilities. Moreover, we assessed GPT-4's\nperformance in code parsing and vulnerability capture by simulating a\nprofessional auditor's auditing process using CoT(Chain of Thought) prompts\nbased on the audit reports of eight groups of smart contracts. We also\nevaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through\nexperimentation, we found that GPT-4 performed poorly in detecting smart\ncontract vulnerabilities, with a high Precision of 96.6%, but a low Recall of\n37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities\nduring detection. Meanwhile, it demonstrated good contract code parsing\ncapabilities, with an average comprehensive score of 6.5, capable of\nidentifying the background information and functional relationships of smart\ncontracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4\nhas significant potential application in PoC writing. These experimental\nresults indicate that GPT-4 lacks the ability to detect smart contract\nvulnerabilities effectively, but its performance in contract code parsing and\nPoC writing demonstrates its significant potential as an auxiliary tool in\nenhancing the efficiency and effectiveness of smart contract security audits.",
        "pdf_link": "https://arxiv.org/pdf/2402.12023v1.pdf"
    },
    {
        "title": "Distilling Large Language Models for Text-Attributed Graph Learning",
        "authors": [
            "Bo Pan",
            "Zheng Zhang",
            "Yifei Zhang",
            "Yuntong Hu",
            "Liang Zhao"
        ],
        "published": "2024-02-19T10:31:53Z",
        "summary": "Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.12022v1.pdf"
    },
    {
        "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
        "authors": [
            "Song Guo",
            "Fan Wu",
            "Lei Zhang",
            "Xiawu Zheng",
            "Shengchuan Zhang",
            "Fei Chao",
            "Yiyu Shi",
            "Rongrong Ji"
        ],
        "published": "2024-02-19T09:55:32Z",
        "summary": "Existing methods for fine-tuning sparse LLMs often suffer from\nresource-intensive requirements and high retraining costs. Additionally, many\nfine-tuning methods often rely on approximations or heuristic optimization\nstrategies, which may lead to suboptimal solutions. To address these issues, we\npropose an efficient and fast framework for fine-tuning sparse LLMs based on\nminimizing reconstruction error. Our approach involves sampling a small dataset\nfor calibration and utilizing backpropagation to iteratively optimize\nblock-wise reconstruction error, on a block-by-block basis, aiming for optimal\nsolutions. Extensive experiments on various benchmarks consistently demonstrate\nthe superiority of our method over other baselines. For instance, on the\nWikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a\nperplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of\n75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a\nperplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the\nfine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,\nand the entire framework can be executed on a single 16GB GPU. The source code\nis available at https://github.com/sunggo/EBFT.",
        "pdf_link": "https://arxiv.org/pdf/2402.12419v1.pdf"
    },
    {
        "title": "Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models",
        "authors": [
            "Himanshu Beniwal",
            "Kowsik Nandagopan D",
            "Mayank Singh"
        ],
        "published": "2024-02-19T09:43:03Z",
        "summary": "Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their\nability to reason about and retain temporal information remains limited. This\nhinders their application in real-world scenarios where understanding the\nsequential nature of events is crucial. This paper experiments with\nstate-of-the-art models on a novel, large-scale temporal dataset,\n\\textbf{TempUN}, to reveal significant limitations in temporal retention and\nreasoning abilities. Interestingly, closed-source models indicate knowledge\ngaps more frequently, potentially suggesting a trade-off between uncertainty\nawareness and incorrect responses. Further, exploring various fine-tuning\napproaches yielded no major performance improvements. The associated dataset\nand code are available at the following URL\n(https://github.com/lingoiitgn/TempUN).",
        "pdf_link": "https://arxiv.org/pdf/2402.11997v1.pdf"
    },
    {
        "title": "Structure Guided Large Language Model for SQL Generation",
        "authors": [
            "Qinggang Zhang",
            "Junnan Dong",
            "Hao Chen",
            "Wentao Li",
            "Feiran Huang",
            "Xiao Huang"
        ],
        "published": "2024-02-19T09:07:59Z",
        "summary": "Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.13284v2.pdf"
    },
    {
        "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs",
        "authors": [
            "Hong Chen",
            "Chengtao Lv",
            "Liang Ding",
            "Haotong Qin",
            "Xiabin Zhou",
            "Yifu Ding",
            "Xuebo Liu",
            "Min Zhang",
            "Jinyang Guo",
            "Xianglong Liu",
            "Dacheng Tao"
        ],
        "published": "2024-02-19T09:04:30Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, while the expensive memory and computation consumption\nimpede their practical deployment. Quantization emerges as one of the most\neffective methods for improving the computational efficiency of LLMs. However,\nexisting ultra-low-bit quantization always causes severe accuracy drops. In\nthis paper, we empirically relieve the micro and macro characteristics of\nultra-low bit quantization and present a novel Dual-Binarization method for\nLLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage\nof 2-bit-width and the efficiency advantage of binarization into account,\nintroducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized\nweights into two independent sets of binaries, FDB ensures the accuracy of\nrepresentations and introduces flexibility, utilizing the efficient bitwise\noperations of binarization while retaining the inherent high sparsity of\nultra-low bit quantization. For the macro-level, we find the distortion that\nexists in the prediction of LLM after quantization, which is specified as the\ndeviations related to the ambiguity of samples. We propose the Deviation-Aware\nDistillation (DAD) method, enabling the model to focus differently on various\nsamples. Comprehensive experiments show that our DB-LLM not only significantly\nsurpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization\n(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional\n20\\% reduction in computational consumption compared to the SOTA method under\nthe same bit-width. Our code will be released soon.",
        "pdf_link": "https://arxiv.org/pdf/2402.11960v1.pdf"
    },
    {
        "title": "Automatic Evaluation for Mental Health Counseling using LLMs",
        "authors": [
            "Anqi Li",
            "Yu Lu",
            "Nirui Song",
            "Shuai Zhang",
            "Lizhi Ma",
            "Zhenzhong Lan"
        ],
        "published": "2024-02-19T09:00:10Z",
        "summary": "High-quality psychological counseling is crucial for mental health worldwide,\nand timely evaluation is vital for ensuring its effectiveness. However,\nobtaining professional evaluation for each counseling session is expensive and\nchallenging. Existing methods that rely on self or third-party manual reports\nto assess the quality of counseling suffer from subjective biases and\nlimitations of time-consuming.\n  To address above challenges, this paper proposes an innovative and efficient\nautomatic approach using large language models (LLMs) to evaluate the working\nalliance in counseling conversations. We collected a comprehensive counseling\ndataset and conducted multiple third-party evaluations based on therapeutic\nrelationship theory. Our LLM-based evaluation, combined with our guidelines,\nshows high agreement with human evaluations and provides valuable insights into\ncounseling scripts. This highlights the potential of LLMs as supervisory tools\nfor psychotherapists. By integrating LLMs into the evaluation process, our\napproach offers a cost-effective and dependable means of assessing counseling\nquality, enhancing overall effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11958v1.pdf"
    },
    {
        "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation",
        "authors": [
            "Keyang Xuan",
            "Li Yi",
            "Fan Yang",
            "Ruochen Wu",
            "Yi R. Fung",
            "Heng Ji"
        ],
        "published": "2024-02-19T08:32:27Z",
        "summary": "The rise of multimodal misinformation on social platforms poses significant\nchallenges for individuals and societies. Its increased credibility and broader\nimpact compared to textual misinformation make detection complex, requiring\nrobust reasoning across diverse media types and profound knowledge for accurate\nverification. The emergence of Large Vision Language Model (LVLM) offers a\npotential solution to this problem. Leveraging their proficiency in processing\nvisual and textual information, LVLM demonstrates promising capabilities in\nrecognizing complex information and exhibiting strong reasoning skills. In this\npaper, we first investigate the potential of LVLM on multimodal misinformation\ndetection. We find that even though LVLM has a superior performance compared to\nLLMs, its profound reasoning may present limited power with a lack of evidence.\nBased on these observations, we propose LEMMA: LVLM-Enhanced Multimodal\nMisinformation Detection with External Knowledge Augmentation. LEMMA leverages\nLVLM intuition and reasoning capabilities while augmenting them with external\nknowledge to enhance the accuracy of misinformation detection. Our method\nimproves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and\nFakeddit datasets respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.11943v1.pdf"
    },
    {
        "title": "Comprehensive Cognitive LLM Agent for Smartphone GUI Automation",
        "authors": [
            "Xinbei Ma",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2024-02-19T08:29:03Z",
        "summary": "Large language models (LLMs) have shown remarkable potential as human-like\nautonomous language agents to interact with real-world environments, especially\nfor graphical user interface (GUI) automation. However, those GUI agents\nrequire comprehensive cognition ability including exhaustive perception and\nreliable action response. We propose \\underline{Co}mprehensive\n\\underline{Co}gnitive LLM \\underline{Agent}, CoCo-Agent, with two novel\napproaches, comprehensive environment perception (CEP) and conditional action\nprediction (CAP), to systematically improve the GUI automation performance.\nFirst, CEP facilitates the GUI perception through different aspects and\ngranularity, including screenshots and complementary detailed layouts for the\nvisual channel and historical actions for the textual channel. Second, CAP\ndecomposes the action prediction into sub-problems: action type prediction and\naction target conditioned on the action type. With our technical design, our\nagent achieves new state-of-the-art performance on AITW and META-GUI\nbenchmarks, showing promising abilities in realistic scenarios. Code is\navailable at https://github.com/xbmxb/AAgent.",
        "pdf_link": "https://arxiv.org/pdf/2402.11941v2.pdf"
    },
    {
        "title": "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition",
        "authors": [
            "Jian Wu",
            "Linyi Yang",
            "Manabu Okumura",
            "Yue Zhang"
        ],
        "published": "2024-02-19T08:12:30Z",
        "summary": "Although Large Language Models (LLMs) have shown strong performance in\nMulti-hop Question Answering (MHQA) tasks, their real reasoning ability remains\nexploration. Current LLM QA evaluation benchmarks have shown limitations,\nincluding 1) data contamination, the evaluation data are potentially exposed to\nLLMs during the pretraining stage; and 2) ignoration of the reasoning chain\nevaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA\nbenchmark based on the new, unprecedented knowledge by editing the\noff-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the\nreasoning chain in the form of sub-questions and intermediate answers\ncorresponding to the multi-hop questions. Specifically, based on the\nobservation, 1) LLMs show a performance gap between the original HotpotQA and\nour edited data, deeming that current MHQA benchmarks have the potential risk\nof data contamination that hard to evaluate LLMs' performance objectively and\nscientifically; 2) LLMs only get a small percentage of the right reasoning\nchain, e.g. GPT-4 only gets 36.3\\% right reasoning chain. We believe this new\nMulti-hop QA evaluation benchmark and novel evaluation methods will facilitate\nthe development of trustworthy LLM evaluation on the MHQA task.",
        "pdf_link": "https://arxiv.org/pdf/2402.11924v2.pdf"
    },
    {
        "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation",
        "authors": [
            "Aiwei Liu",
            "Haoping Bai",
            "Zhiyun Lu",
            "Xiang Kong",
            "Simon Wang",
            "Jiulong Shan",
            "Meng Cao",
            "Lijie Wen"
        ],
        "published": "2024-02-19T07:46:40Z",
        "summary": "Aligning large language models (LLMs) with human expectations without\nhuman-annotated preference data is an important problem. In this paper, we\npropose a method to evaluate the response preference by using the output\nprobabilities of response pairs under contrastive prompt pairs, which could\nachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based\non this, we propose an automatic alignment method, Direct Large Model Alignment\n(DLMA). First, we use contrastive prompt pairs to automatically generate\npreference data. Then, we continue to evaluate the generated preference data\nusing contrastive prompt pairs and calculate a self-rewarding score. Finally,\nwe use the DPO algorithm to effectively align LLMs by combining this\nself-rewarding score. In the experimental stage, our DLMA method could surpass\nthe \\texttt{RLHF} method without relying on human-annotated preference data.",
        "pdf_link": "https://arxiv.org/pdf/2402.11907v1.pdf"
    },
    {
        "title": "Learning to Edit: Aligning LLMs with Knowledge Editing",
        "authors": [
            "Yuxin Jiang",
            "Yufei Wang",
            "Chuhan Wu",
            "Wanjun Zhong",
            "Xingshan Zeng",
            "Jiahui Gao",
            "Liangyou Li",
            "Xin Jiang",
            "Lifeng Shang",
            "Ruiming Tang",
            "Qun Liu",
            "Wei Wang"
        ],
        "published": "2024-02-19T07:45:17Z",
        "summary": "Knowledge editing techniques, aiming to efficiently modify a minor proportion\nof knowledge in large language models (LLMs) without negatively impacting\nperformance across other inputs, have garnered widespread attention. However,\nexisting methods predominantly rely on memorizing the updated knowledge,\nimpeding LLMs from effectively combining the new knowledge with their inherent\nknowledge when answering questions. To this end, we propose a Learning to Edit\n(LTE) framework, focusing on teaching LLMs to apply updated knowledge into\ninput questions, inspired by the philosophy of \"Teach a man to fish.\" LTE\nfeatures a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on\na meticulously curated parallel dataset to make reliable, in-scope edits while\npreserving out-of-scope information and linguistic proficiency; and (ii) the\nInference Phase, which employs a retrieval-based mechanism for real-time and\nmass knowledge editing. By comparing our approach with seven advanced baselines\nacross four popular knowledge editing benchmarks and two LLM architectures, we\ndemonstrate LTE's superiority in knowledge editing performance, robustness in\nboth batch and sequential editing, minimal interference on general tasks, and\nrapid editing speeds. The data and code are available at\nhttps://github.com/YJiangcm/LTE.",
        "pdf_link": "https://arxiv.org/pdf/2402.11905v1.pdf"
    },
    {
        "title": "SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning",
        "authors": [
            "Yu Zhang",
            "Hui-Ling Zhen",
            "Zehua Pei",
            "Yingzhao Lian",
            "Lihao Yin",
            "Mingxuan Yuan",
            "Bei Yu"
        ],
        "published": "2024-02-19T07:38:57Z",
        "summary": "Considering the challenges faced by large language models (LLMs) on logical\nreasoning, prior efforts have sought to transform problem-solving through tool\nlearning. While progress has been made on small-scale problems, solving\nindustrial cases remains difficult due to their large scale and intricate\nexpressions. In this paper, we propose a novel solver-layer adaptation (SoLA)\nmethod, where we introduce a solver as a new layer of the LLM to differentially\nguide solutions towards satisfiability. In SoLA, LLM aims to comprehend the\nsearch space described in natural language and identify local solutions of the\nhighest quality, while the solver layer focuses solely on constraints not\nsatisfied by the initial solution. Leveraging MaxSAT as a bridge, we define\nforward and backward transfer gradients, enabling the final model to converge\nto a satisfied solution or prove unsatisfiability. The backdoor theory ensures\nthat SoLA can obtain accurate solutions within polynomial loops. We evaluate\nthe performance of SoLA on various datasets and empirically demonstrate its\nconsistent outperformance against existing symbolic solvers (including Z3 and\nKissat) and tool-learning methods in terms of efficiency in large-scale\nproblem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2402.11903v1.pdf"
    },
    {
        "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
        "authors": [
            "Tianjie Ju",
            "Yijin Chen",
            "Xinwei Yuan",
            "Zhuosheng Zhang",
            "Wei Du",
            "Yubin Zheng",
            "Gongshen Liu"
        ],
        "published": "2024-02-19T07:34:10Z",
        "summary": "Recent work has showcased the powerful capability of large language models\n(LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs\nin combining these two capabilities into reasoning through multi-hop facts has\nnot been widely explored. This paper systematically investigates the\npossibilities for LLMs to utilize shortcuts based on direct connections between\nthe initial and terminal entities of multi-hop knowledge. We first explore the\nexistence of factual shortcuts through Knowledge Neurons, revealing that: (i)\nthe strength of factual shortcuts is highly correlated with the frequency of\nco-occurrence of initial and terminal entities in the pre-training corpora;\n(ii) few-shot prompting leverage more shortcuts in answering multi-hop\nquestions compared to chain-of-thought prompting. Then, we analyze the risks\nposed by factual shortcuts from the perspective of multi-hop knowledge editing.\nAnalysis shows that approximately 20% of the failures are attributed to\nshortcuts, and the initial and terminal entities in these failure instances\nusually have higher co-occurrences in the pre-training corpus. Finally, we\npropose erasing shortcut neurons to mitigate the associated risks and find that\nthis approach significantly reduces failures in multiple-hop knowledge editing\ncaused by shortcuts.",
        "pdf_link": "https://arxiv.org/pdf/2402.11900v1.pdf"
    },
    {
        "title": "SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning",
        "authors": [
            "Zhihao Wen",
            "Jie Zhang",
            "Yuan Fang"
        ],
        "published": "2024-02-19T07:22:29Z",
        "summary": "Fine-tuning all parameters of large language models (LLMs) necessitates\nsubstantial computational power and extended time. Latest advancements in\nparameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and\nLoRA, allow for adjustments to only a minor fraction of the parameters of these\nLLMs. Concurrently, it has been noted that the issue of over-smoothing\ndiminishes the effectiveness of these Transformer-based LLMs, resulting in\nsuboptimal performances in downstream tasks. In this paper, we present SIBO,\nwhich is a SImple BOoster to enhance PEFT, by injecting an initial residual.\nSIBO is straight-forward and readily extensible to a range of state-of-the-art\nPEFT techniques to alleviate over-smoothing and enhance performance. Extensive\nexperiments on 22 benchmark datasets demonstrate that SIBO significantly\nenhances the performance of various strong baselines, achieving up to 15.7% and\n23.5% improvement over existing PEFT methods on the arithmetic and commonsense\nreasoning tasks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.11896v1.pdf"
    },
    {
        "title": "Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation",
        "authors": [
            "Jiahao Ying",
            "Yixin Cao",
            "Bo Wang",
            "Wei Tang",
            "Yizhe Yang",
            "Shuicheng Yan"
        ],
        "published": "2024-02-19T07:15:59Z",
        "summary": "Due to the expanding capabilities and pre-training data, Large Language\nModels (LLMs) are facing increasingly serious evaluation challenges. On one\nhand, the data leakage issue cause over-estimation on existing benchmarks. On\nthe other hand, periodically curating datasets manually is costly. In this\npaper, we propose to automate dataset updates for reliable and timely\nevaluation. The basic idea is to generate unseen and high-quality testing\nsamples based on existing ones to mitigate leakage issues. In specific, we\npropose two strategies with systematically verification. First, the mimicking\nstrategy employs LLMs to create new samples resembling existing ones, to the\nmaximum extent preserving the stylistic of the original dataset. Our\nexperiments demonstrate its evaluation stability across multiple instantiations\nand its effectiveness in dealing with data leakage issues in most cases.\nSecond, for the cases that mimicking dataset works poorly, we design an\nextending strategy that adjusts the difficulty of the generated samples\naccording to varying cognitive levels. This not only makes our evaluation more\nsystematic, but also, with a balanced difficulty, even discern model\ncapabilities better at fine-grained levels.",
        "pdf_link": "https://arxiv.org/pdf/2402.11894v2.pdf"
    },
    {
        "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
        "authors": [
            "Xiaowei Yuan",
            "Zhao Yang",
            "Yequan Wang",
            "Shengping Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-02-19T07:10:30Z",
        "summary": "Large language models internalize enormous parametric knowledge during\npre-training. Concurrently, realistic applications necessitate external\ncontextual knowledge to aid models on the underlying tasks. This raises a\ncrucial dilemma known as knowledge conflicts, where the contextual knowledge\nclashes with the However, existing decoding works are specialized in resolving\nknowledge conflicts and could inadvertently deteriorate performance in absence\nof conflicts. In this paper, we propose an adaptive decoding method, termed as\ncontextual information-entropy constraint decoding (COIECD), to discern whether\nthe knowledge conflicts occur and resolve them. It can improve the model's\nfaithfulness to conflicting context, and simultaneously maintain high\nperformance among non- Our experiments show that COIECD exhibits strong\nperformance and robustness over knowledge conflicts in realistic datasets. Code\nis available.",
        "pdf_link": "https://arxiv.org/pdf/2402.11893v1.pdf"
    },
    {
        "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2024-02-19T06:58:42Z",
        "summary": "With the development of instruction-tuned large language models (LLMs),\nimproving the safety of LLMs has become more critical. However, the current\napproaches for aligning the LLMs output with expected safety usually require\nsubstantial training efforts, e.g., high-quality safety data and expensive\ncomputational resources, which are costly and inefficient. To this end, we\npresent reverse prompt contrastive decoding (ROSE), a simple-yet-effective\nmethod to directly boost the safety of existing instruction-tuned LLMs without\nany additional training. The principle of ROSE is to improve the probability of\ndesired safe output via suppressing the undesired output induced by the\ncarefully-designed reverse prompts. Experiments on 6 safety and 2\ngeneral-purpose tasks show that, our ROSE not only brings consistent and\nsignificant safety improvements (up to +13.8% safety score) upon 5 types of\ninstruction-tuned LLMs, but also benefits the general-purpose ability of LLMs.\nIn-depth analyses explore the underlying mechanism of ROSE, and reveal when and\nwhere to use it.",
        "pdf_link": "https://arxiv.org/pdf/2402.11889v1.pdf"
    },
    {
        "title": "RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning",
        "authors": [
            "Congyun Jin",
            "Ming Zhang",
            "Xiaowei Ma",
            "Li Yujiao",
            "Yingbo Wang",
            "Yabo Jia",
            "Yuliang Du",
            "Tao Sun",
            "Haowen Wang",
            "Cong Fan",
            "Jinjie Gu",
            "Chenfei Chi",
            "Xiangguo Lv",
            "Fangzhou Li",
            "Wei Xue",
            "Yiran Huang"
        ],
        "published": "2024-02-19T06:57:02Z",
        "summary": "Recent advancements in Large Language Models (LLMs) and Large Multi-modal\nModels (LMMs) have shown potential in various medical applications, such as\nIntelligent Medical Diagnosis. Although impressive results have been achieved,\nwe find that existing benchmarks do not reflect the complexity of real medical\nreports and specialized in-depth reasoning capabilities. In this work, we\nintroduced RJUA-MedDQA, a comprehensive benchmark in the field of medical\nspecialization, which poses several challenges: comprehensively interpreting\nimgage content across diverse challenging layouts, possessing numerical\nreasoning ability to identify abnormal indicators and demonstrating clinical\nreasoning ability to provide statements of disease diagnosis, status and advice\nbased on medical contexts. We carefully design the data generation pipeline and\nproposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed\nat restoring textual and tabular content in medical report images. This method\nsubstantially enhances annotation efficiency, doubling the productivity of each\nannotator, and yields a 26.8% improvement in accuracy. We conduct extensive\nevaluations, including few-shot assessments of 5 LMMs which are capable of\nsolving Chinese medical QA tasks. To further investigate the limitations and\npotential of current LMMs, we conduct comparative experiments on a set of\nstrong LLMs by using image-text generated by ESRA method. We report the\nperformance of baselines and offer several observations: (1) The overall\nperformance of existing LMMs is still limited; however LMMs more robust to\nlow-quality and diverse-structured images compared to LLMs. (3) Reasoning\nacross context and image content present significant challenges. We hope this\nbenchmark helps the community make progress on these challenging tasks in\nmulti-modal medical document understanding and facilitate its application in\nhealthcare.",
        "pdf_link": "https://arxiv.org/pdf/2402.14840v1.pdf"
    },
    {
        "title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth",
        "authors": [
            "Shir Lissak",
            "Nitay Calderon",
            "Geva Shenkman",
            "Yaakov Ophir",
            "Eyal Fruchter",
            "Anat Brunstein Klomek",
            "Roi Reichart"
        ],
        "published": "2024-02-19T06:54:55Z",
        "summary": "Queer youth face increased mental health risks, such as depression, anxiety,\nand suicidal ideation. Hindered by negative stigma, they often avoid seeking\nhelp and rely on online resources, which may provide incompatible information.\nAlthough access to a supportive environment and reliable information is\ninvaluable, many queer youth worldwide have no access to such support. However,\nthis could soon change due to the rapid adoption of Large Language Models\n(LLMs) such as ChatGPT. This paper aims to comprehensively explore the\npotential of LLMs to revolutionize emotional support for queers. To this end,\nwe conduct a qualitative and quantitative analysis of LLM's interactions with\nqueer-related content. To evaluate response quality, we develop a novel\nten-question scale that is inspired by psychological standards and expert\ninput. We apply this scale to score several LLMs and human comments to posts\nwhere queer youth seek advice and share experiences. We find that LLM responses\nare supportive and inclusive, outscoring humans. However, they tend to be\ngeneric, not empathetic enough, and lack personalization, resulting in\nnonreliable and potentially harmful advice. We discuss these challenges,\ndemonstrate that a dedicated prompt can improve the performance, and propose a\nblueprint of an LLM-supporter that actively (but sensitively) seeks user\ncontext to provide personalized, empathetic, and reliable responses. Our\nannotated dataset is available for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.11886v1.pdf"
    },
    {
        "title": "NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization",
        "authors": [
            "Imjin Ahn",
            "Hansle Gwon",
            "Young-Hak Kim",
            "Tae Joon Jun",
            "Sanghyun Park"
        ],
        "published": "2024-02-19T06:43:25Z",
        "summary": "The discharge summary is a one of critical documents in the patient journey,\nencompassing all events experienced during hospitalization, including multiple\nvisits, medications, tests, surgery/procedures, and admissions/discharge.\nProviding a summary of the patient's progress is crucial, as it significantly\ninfluences future care and planning. Consequently, clinicians face the\nlaborious and resource-intensive task of manually collecting, organizing, and\ncombining all the necessary data for a discharge summary. Therefore, we propose\n\"NOTE\", which stands for \"Notable generation Of patient Text summaries through\nan Efficient approach based on direct preference optimization\". NOTE is based\non Medical Information Mart for Intensive Care- III dataset and summarizes a\nsingle hospitalization of a patient. Patient events are sequentially combined\nand used to generate a discharge summary for each hospitalization. In the\npresent circumstances, large language models' application programming\ninterfaces (LLMs' APIs) are widely available, but importing and exporting\nmedical data presents significant challenges due to privacy protection policies\nin healthcare institutions. Moreover, to ensure optimal performance, it is\nessential to implement a lightweight model for internal server or program\nwithin the hospital. Therefore, we utilized DPO and parameter efficient fine\ntuning (PEFT) techniques to apply a fine-tuning method that guarantees superior\nperformance. To demonstrate the practical application of the developed NOTE, we\nprovide a webpage-based demonstration software. In the future, we will aim to\ndeploy the software available for actual use by clinicians in hospital. NOTE\ncan be utilized to generate various summaries not only discharge summaries but\nalso throughout a patient's journey, thereby alleviating the labor-intensive\nworkload of clinicians and aiming for increased efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.11882v1.pdf"
    },
    {
        "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
        "authors": [
            "Uijeong Jang",
            "Jason D. Lee",
            "Ernest K. Ryu"
        ],
        "published": "2024-02-19T06:22:09Z",
        "summary": "Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.",
        "pdf_link": "https://arxiv.org/pdf/2402.11867v1.pdf"
    },
    {
        "title": "Modularized Networks for Few-shot Hateful Meme Detection",
        "authors": [
            "Rui Cao",
            "Roy Ka-Wei Lee",
            "Jing Jiang"
        ],
        "published": "2024-02-19T05:15:13Z",
        "summary": "In this paper, we address the challenge of detecting hateful memes in the\nlow-resource setting where only a few labeled examples are available. Our\napproach leverages the compositionality of Low-rank adaptation (LoRA), a widely\nused parameter-efficient tuning technique. We commence by fine-tuning large\nlanguage models (LLMs) with LoRA on selected tasks pertinent to hateful meme\ndetection, thereby generating a suite of LoRA modules. These modules are\ncapable of essential reasoning skills for hateful meme detection. We then use\nthe few available annotated samples to train a module composer, which assigns\nweights to the LoRA modules based on their relevance. The model's learnable\nparameters are directly proportional to the number of LoRA modules. This\nmodularized network, underpinned by LLMs and augmented with LoRA modules,\nexhibits enhanced generalization in the context of hateful meme detection. Our\nevaluation spans three datasets designed for hateful meme detection in a\nfew-shot learning context. The proposed method demonstrates superior\nperformance to traditional in-context learning, which is also more\ncomputationally intensive during inference.We then use the few available\nannotated samples to train a module composer, which assigns weights to the LoRA\nmodules based on their relevance. The model's learnable parameters are directly\nproportional to the number of LoRA modules. This modularized network,\nunderpinned by LLMs and augmented with LoRA modules, exhibits enhanced\ngeneralization in the context of hateful meme detection. Our evaluation spans\nthree datasets designed for hateful meme detection in a few-shot learning\ncontext. The proposed method demonstrates superior performance to traditional\nin-context learning, which is also more computationally intensive during\ninference.",
        "pdf_link": "https://arxiv.org/pdf/2402.11845v1.pdf"
    },
    {
        "title": "UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction",
        "authors": [
            "Yuan Yuan",
            "Jingtao Ding",
            "Jie Feng",
            "Depeng Jin",
            "Yong Li"
        ],
        "published": "2024-02-19T05:04:11Z",
        "summary": "Urban spatio-temporal prediction is crucial for informed decision-making,\nsuch as transportation management, resource optimization, and urban planning.\nAlthough pretrained foundation models for natural languages have experienced\nremarkable breakthroughs, wherein one general-purpose model can tackle multiple\ntasks across various domains, urban spatio-temporal modeling lags behind.\nExisting approaches for urban prediction are usually tailored for specific\nspatio-temporal scenarios, requiring task-specific model designs and extensive\nin-domain training data. In this work, we propose a universal model, UniST, for\nurban spatio-temporal prediction. Drawing inspiration from large language\nmodels, UniST achieves success through: (i) flexibility towards diverse\nspatio-temporal data characteristics, (ii) effective generative pre-training\nwith elaborated masking strategies to capture complex spatio-temporal\nrelationships, (iii) spatio-temporal knowledge-guided prompts that align and\nleverage intrinsic and shared knowledge across scenarios. These designs\ntogether unlock the potential of a one-for-all model for spatio-temporal\nprediction with powerful generalization capability. Extensive experiments on 15\ncities and 6 domains demonstrate the universality of UniST in advancing\nstate-of-the-art prediction performance, especially in few-shot and zero-shot\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11838v1.pdf"
    },
    {
        "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search",
        "authors": [
            "Chanwoong Yoon",
            "Gangwoo Kim",
            "Byeongguk Jeon",
            "Sungdong Kim",
            "Yohan Jo",
            "Jaewoo Kang"
        ],
        "published": "2024-02-19T04:41:31Z",
        "summary": "Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM using this dataset to\nalign it with the retrievers' preferences as feedback. The resulting model\nachieves state-of-the-art performance on two recent conversational search\nbenchmarks, significantly outperforming existing baselines, including GPT-3.5.",
        "pdf_link": "https://arxiv.org/pdf/2402.11827v1.pdf"
    },
    {
        "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
        "authors": [
            "Yanbang Wang",
            "Hejie Cui",
            "Jon Kleinberg"
        ],
        "published": "2024-02-19T04:29:45Z",
        "summary": "Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.11821v2.pdf"
    },
    {
        "title": "Head-wise Shareable Attention for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published": "2024-02-19T04:19:36Z",
        "summary": "Large Language Models (LLMs) suffer from huge number of parameters, which\nrestricts their deployment on edge devices. Weight sharing is one promising\nsolution that encourages weight reuse, effectively reducing memory usage with\nless performance drop. However, current weight sharing techniques primarily\nfocus on small-scale models like BERT and employ coarse-grained sharing rules,\ne.g., layer-wise. This becomes limiting given the prevalence of LLMs and\nsharing an entire layer or block obviously diminishes the flexibility of weight\nsharing. In this paper, we present a perspective on $\\textit{$\\textbf{head-wise\nshareable attention for large language models}$}$. We further propose two\nmemory-efficient methods that share parameters across attention heads, with a\nspecific focus on LLMs. Both of them use the same dynamic strategy to select\nthe shared weight matrices. The first method directly reuses the pre-trained\nweights without retraining, denoted as $\\textbf{DirectShare}$. The second\nmethod first post-trains with constraint on weight matrix similarity and then\nshares, denoted as $\\textbf{PostShare}$. Experimental results reveal our\nhead-wise shared models still maintain satisfactory capabilities, demonstrating\nthe feasibility of fine-grained weight sharing applied to LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11819v1.pdf"
    },
    {
        "title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?",
        "authors": [
            "Shubhashis Roy Dipta",
            "Sadat Shahriar"
        ],
        "published": "2024-02-19T04:11:34Z",
        "summary": "This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.",
        "pdf_link": "https://arxiv.org/pdf/2402.11815v2.pdf"
    },
    {
        "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema",
        "authors": [
            "Junru Lu",
            "Siyu An",
            "Min Zhang",
            "Yulan He",
            "Di Yin",
            "Xing Sun"
        ],
        "published": "2024-02-19T03:56:44Z",
        "summary": "In the quest to facilitate the deep intelligence of Large Language Models\n(LLMs) accessible in final-end user-bot interactions, the art of prompt\ncrafting emerges as a critical yet complex task for the average user. Contrast\nto previous model-oriented yet instruction-agnostic Automatic Prompt\nOptimization methodologies, yielding polished results for predefined target\nmodels while suffering rapid degradation with out-of-box models, we present\nFree-form Instruction-oriented Prompt Optimization (FIPO). This approach is\nsupported by our large-scale prompt preference dataset and employs a modular\nfine-tuning schema. The FIPO schema reimagines the optimization process into\nmanageable modules, anchored by a meta prompt that dynamically adapts content.\nThis allows for the flexible integration of the raw task instruction, the\noptional instruction response, and the optional ground truth to produce finely\noptimized task prompts. The FIPO preference dataset is meticulously constructed\nusing the optimal and suboptimal LLMs, undergoing rigorous cross-verification\nby human experts and analytical models. Applying the insights from the data\nwith Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO\nschema across five public benchmarks. Codes, data and scripts are here:\nhttps://github.com/LuJunru/FIPO_Project.",
        "pdf_link": "https://arxiv.org/pdf/2402.11811v1.pdf"
    },
    {
        "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
        "authors": [
            "Kai Wang",
            "Yuwei Xu",
            "Zhiyong Wu",
            "Siqiang Luo"
        ],
        "published": "2024-02-19T03:21:19Z",
        "summary": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts\nfrom new KGs that are not seen during training, has been widely adopted in\nvarious applications. One critical challenge of KG inductive reasoning is\nhandling low-resource scenarios with scarcity in both textual and structural\naspects. In this paper, we attempt to address this challenge with Large\nLanguage Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to\ngenerate a graph-structural prompt to enhance the pre-trained Graph Neural\nNetworks (GNNs), which brings us new methodological insights into the KG\ninductive reasoning methods, as well as high generalizability in practice. On\nthe methodological side, we introduce a novel pretraining and prompting\nframework ProLINK, designed for low-resource inductive reasoning across\narbitrary KGs without requiring additional training. On the practical side, we\nexperimentally evaluate our approach on 36 low-resource KG datasets and find\nthat ProLINK outperforms previous methods in three-shot, one-shot, and\nzero-shot reasoning tasks, exhibiting average performance improvements by 20%,\n45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong\nrobustness for various LLM promptings as well as full-shot scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11804v1.pdf"
    },
    {
        "title": "What Evidence Do Language Models Find Convincing?",
        "authors": [
            "Alexander Wan",
            "Eric Wallace",
            "Dan Klein"
        ],
        "published": "2024-02-19T02:15:34Z",
        "summary": "Retrieval-augmented language models are being increasingly tasked with\nsubjective, contentious, and conflicting queries such as \"is aspartame linked\nto cancer\". To resolve these ambiguous queries, one must search through a large\nrange of websites and consider \"which, if any, of this evidence do I find\nconvincing?\". In this work, we study how LLMs answer this question. In\nparticular, we construct ConflictingQA, a dataset that pairs controversial\nqueries with a series of real-world evidence documents that contain different\nfacts (e.g., quantitative results), argument styles (e.g., appeals to\nauthority), and answers (Yes or No). We use this dataset to perform sensitivity\nand counterfactual analyses to explore which text features most affect LLM\npredictions. Overall, we find that current models rely heavily on the relevance\nof a website to the query, while largely ignoring stylistic features that\nhumans find important such as whether a text contains scientific references or\nis written with a neutral tone. Taken together, these results highlight the\nimportance of RAG corpus quality (e.g., the need to filter misinformation), and\npossibly even a shift in how LLMs are trained to better align with human\njudgements.",
        "pdf_link": "https://arxiv.org/pdf/2402.11782v1.pdf"
    },
    {
        "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs",
        "authors": [
            "Pengrui Han",
            "Rafal Kocielnik",
            "Adhithya Saravanan",
            "Roy Jiang",
            "Or Sharir",
            "Anima Anandkumar"
        ],
        "published": "2024-02-19T01:28:48Z",
        "summary": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.",
        "pdf_link": "https://arxiv.org/pdf/2402.11764v1.pdf"
    },
    {
        "title": "Large Language Models for Stemming: Promises, Pitfalls and Failures",
        "authors": [
            "Shuai Wang",
            "Shengyao Zhuang",
            "Guido Zuccon"
        ],
        "published": "2024-02-19T01:11:44Z",
        "summary": "Text stemming is a natural language processing technique that is used to\nreduce words to their base form, also known as the root form. The use of\nstemming in IR has been shown to often improve the effectiveness of\nkeyword-matching models such as BM25. However, traditional stemming methods,\nfocusing solely on individual terms, overlook the richness of contextual\ninformation. Recognizing this gap, in this paper, we investigate the promising\nidea of using large language models (LLMs) to stem words by leveraging its\ncapability of context understanding. With this respect, we identify three\navenues, each characterised by different trade-offs in terms of computational\ncost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a\ncollection, i.e., the set of unique words that appear in the collection\n(vocabulary stemming), (2) use LLMs to stem each document separately\n(contextual stemming), and (3) use LLMs to extract from each document entities\nthat should not be stemmed, then use vocabulary stemming to stem the rest of\nthe terms (entity-based contextual stemming). Through a series of empirical\nexperiments, we compare the use of LLMs for stemming with that of traditional\nlexical stemmers such as Porter and Krovetz for English text. We find that\nwhile vocabulary stemming and contextual stemming fail to achieve higher\neffectiveness than traditional stemmers, entity-based contextual stemming can\nachieve a higher effectiveness than using Porter stemmer alone, under specific\nconditions.",
        "pdf_link": "https://arxiv.org/pdf/2402.11757v1.pdf"
    },
    {
        "title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs",
        "authors": [
            "Yavuz Faruk Bakman",
            "Duygu Nur Yaldiz",
            "Baturalp Buyukates",
            "Chenyang Tao",
            "Dimitrios Dimitriadis",
            "Salman Avestimehr"
        ],
        "published": "2024-02-19T01:04:22Z",
        "summary": "Generative Large Language Models (LLMs) are widely utilized for their\nexcellence in various tasks. However, their tendency to produce inaccurate or\nmisleading outputs poses a potential risk, particularly in high-stakes\nenvironments. Therefore, estimating the correctness of generative LLM outputs\nis an important task for enhanced reliability. Uncertainty Estimation (UE) in\ngenerative LLMs is an evolving domain, where SOTA probability-based methods\ncommonly employ length-normalized scoring. In this work, we propose\nMeaning-Aware Response Scoring (MARS) as an alternative to length-normalized\nscoring for UE methods. MARS is a novel scoring function that considers the\nsemantic contribution of each token in the generated sequence in the context of\nthe question. We demonstrate that integrating MARS into UE methods results in a\nuniversal and significant improvement in UE performance. We conduct experiments\nusing three distinct closed-book question-answering datasets across five\npopular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical\nQA dataset. Code can be found https://github.com/Ybakman/LLM_Uncertainity.",
        "pdf_link": "https://arxiv.org/pdf/2402.11756v2.pdf"
    },
    {
        "title": "SPML: A DSL for Defending Language Models Against Prompt Attacks",
        "authors": [
            "Reshabh K Sharma",
            "Vinayak Gupta",
            "Dan Grossman"
        ],
        "published": "2024-02-19T00:53:48Z",
        "summary": "Large language models (LLMs) have profoundly transformed natural language\napplications, with a growing reliance on instruction-based definitions for\ndesigning chatbots. However, post-deployment the chatbot definitions are fixed\nand are vulnerable to attacks by malicious users, emphasizing the need to\nprevent unethical applications and financial losses. Existing studies explore\nuser prompts' impact on LLM-based chatbots, yet practical methods to contain\nattacks on application-specific chatbots remain unexplored. This paper presents\nSystem Prompt Meta Language (SPML), a domain-specific language for refining\nprompts and monitoring the inputs to the LLM-based chatbots. SPML actively\nchecks attack prompts, ensuring user inputs align with chatbot definitions to\nprevent malicious execution on the LLM backbone, optimizing costs. It also\nstreamlines chatbot definition crafting with programming language capabilities,\novercoming natural language design challenges. Additionally, we introduce a\ngroundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering\nthe inaugural language and benchmark for chatbot definition evaluation.\nExperiments across datasets demonstrate SPML's proficiency in understanding\nattacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data\nand codes are publicly available at: https://prompt-compiler.github.io/SPML/.",
        "pdf_link": "https://arxiv.org/pdf/2402.11755v1.pdf"
    },
    {
        "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
        "authors": [
            "Fengqing Jiang",
            "Zhangchen Xu",
            "Luyao Niu",
            "Zhen Xiang",
            "Bhaskar Ramasubramanian",
            "Bo Li",
            "Radha Poovendran"
        ],
        "published": "2024-02-19T00:43:31Z",
        "summary": "Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11753v2.pdf"
    },
    {
        "title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts",
        "authors": [
            "Mohammad Heydari Rad",
            "Farhan Farsi",
            "Shayan Bali",
            "Romina Etezadi",
            "Mehrnoush Shamsfard"
        ],
        "published": "2024-02-19T00:40:17Z",
        "summary": "Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs\nhave been used to generate texts in different languages and for different\ntasks. Additionally, due to the participation of remarkable companies such as\nGoogle and OpenAI, LLMs are now more accessible, and people can easily use\nthem. However, an important issue is how we can detect AI-generated texts from\nhuman-written ones. In this article, we have investigated the problem of\nAI-generated text detection from two different aspects: semantics and syntax.\nFinally, we presented an AI model that can distinguish AI-generated texts from\nhuman-written ones with high accuracy on both multilingual and monolingual\ntasks using the M4 dataset. According to our results, using a semantic approach\nwould be more helpful for detection. However, there is a lot of room for\nimprovement in the syntactic approach, and it would be a good approach for\nfuture work.",
        "pdf_link": "https://arxiv.org/pdf/2402.14838v1.pdf"
    },
    {
        "title": "In-Context Learning Demonstration Selection via Influence Analysis",
        "authors": [
            "Vinay M. S.",
            "Minh-Hao Van",
            "Xintao Wu"
        ],
        "published": "2024-02-19T00:39:31Z",
        "summary": "Large Language Models (LLMs) have demonstrated their In-Context Learning\n(ICL) capabilities which provides an opportunity to perform few shot learning\nwithout any gradient update. Despite its multiple benefits, ICL generalization\nperformance is sensitive to the selected demonstrations. Selecting effective\ndemonstrations for ICL is still an open research challenge. To address this\nchallenge, we propose a demonstration selection method called InfICL which\nanalyzes influences of training samples through influence functions.\nIdentifying highly influential training samples can potentially aid in\nuplifting the ICL generalization performance. To limit the running cost of\nInfICL, we only employ the LLM to generate sample embeddings, and don't perform\nany costly fine tuning. We perform empirical study on multiple real-world\ndatasets and show merits of our InfICL against state-of-the-art baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.11750v1.pdf"
    },
    {
        "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
        "authors": [
            "Rishabh Bhardwaj",
            "Do Duc Anh",
            "Soujanya Poria"
        ],
        "published": "2024-02-19T00:18:09Z",
        "summary": "Aligned language models face a significant limitation as their fine-tuning\noften results in compromised safety. To tackle this, we propose a simple method\nRESTA that performs LLM safety realignment. RESTA stands for REstoring Safety\nthrough Task Arithmetic. At its core, it involves a simple arithmetic addition\nof a safety vector to the weights of the compromised model. We demonstrate the\neffectiveness of RESTA in both parameter-efficient and full fine-tuning,\ncovering a wide range of downstream tasks, including instruction following in\nChinese, English, and Hindi, as well as problem-solving capabilities in Code\nand Math. We also showcase the generalizability of RESTA on three existing\nsafety evaluation benchmarks and a multilingual benchmark dataset proposed as a\npart of this work, consisting of 550 harmful questions covering 11 categories,\neach with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of\nthe compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in\nparameter-efficient and full fine-tuning, respectively, while maintaining most\nof the model's performance on the task. We release the source codes at:\nhttps://github.com/declare-lab/resta.",
        "pdf_link": "https://arxiv.org/pdf/2402.11746v1.pdf"
    },
    {
        "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",
        "authors": [
            "Jiajia Wang",
            "Jimmy X. Huang",
            "Xinhui Tu",
            "Junmei Wang",
            "Angela J. Huang",
            "Md Tahmid Rahman Laskar",
            "Amran Bhuiyan"
        ],
        "published": "2024-02-18T23:22:40Z",
        "summary": "Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.",
        "pdf_link": "https://arxiv.org/pdf/2403.00784v1.pdf"
    },
    {
        "title": "Solving Data-centric Tasks using Large Language Models",
        "authors": [
            "Shraddha Barke",
            "Christian Poelitz",
            "Carina Suzana Negreanu",
            "Benjamin Zorn",
            "Jos\u00e9 Cambronero",
            "Andrew D. Gordon",
            "Vu Le",
            "Elnaz Nouri",
            "Nadia Polikarpova",
            "Advait Sarkar",
            "Brian Slininger",
            "Neil Toronto",
            "Jack Williams"
        ],
        "published": "2024-02-18T23:19:21Z",
        "summary": "Large language models (LLMs) are rapidly replacing help forums like\nStackOverflow, and are especially helpful for non-professional programmers and\nend users. These users are often interested in data-centric tasks, such as\nspreadsheet manipulation and data wrangling, which are hard to solve if the\nintent is only communicated using a natural-language description, without\nincluding the data. But how do we decide how much data and which data to\ninclude in the prompt? This paper makes two contributions towards answering\nthis question. First, we create a dataset of real-world NL-to-code tasks\nmanipulating tabular data, mined from StackOverflow posts. Second, we introduce\na cluster-then-select prompting technique, which adds the most representative\nrows from the input data to the LLM prompt. Our experiments show that LLM\nperformance is indeed sensitive to the amount of data passed in the prompt, and\nthat for tasks with a lot of syntactic variation in the input table, our\ncluster-then-select technique outperforms a random selection baseline.",
        "pdf_link": "https://arxiv.org/pdf/2402.11734v2.pdf"
    },
    {
        "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
        "authors": [
            "Kai Chen",
            "Zihao He",
            "Jun Yan",
            "Taiwei Shi",
            "Kristina Lerman"
        ],
        "published": "2024-02-18T22:36:19Z",
        "summary": "Large Language Models (LLMs) possess the potential to exert substantial\ninfluence on public perceptions and interactions with information. This raises\nconcerns about the societal impact that could arise if the ideologies within\nthese models can be easily manipulated. In this work, we investigate how\neffectively LLMs can learn and generalize ideological biases from their\ninstruction-tuning data. Our findings reveal a concerning vulnerability:\nexposure to only a small amount of ideologically driven samples significantly\nalters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to\nabsorb ideology from one topic and generalize it to even unrelated ones. The\nease with which LLMs' ideologies can be skewed underscores the risks associated\nwith intentionally poisoned training data by malicious actors or inadvertently\nintroduced biases by data annotators. It also emphasizes the imperative for\nrobust safeguards to mitigate the influence of ideological manipulations on\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11725v2.pdf"
    },
    {
        "title": "Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models",
        "authors": [
            "Paramveer S. Dhillon",
            "Somayeh Molaei",
            "Jiaqi Li",
            "Maximilian Golub",
            "Shaochun Zheng",
            "Lionel P. Robert"
        ],
        "published": "2024-02-18T22:27:42Z",
        "summary": "Advances in language modeling have paved the way for novel human-AI\nco-writing experiences. This paper explores how varying levels of scaffolding\nfrom large language models (LLMs) shape the co-writing process. Employing a\nwithin-subjects field experiment with a Latin square design, we asked\nparticipants (N=131) to respond to argumentative writing prompts under three\nrandomly sequenced conditions: no AI assistance (control), next-sentence\nsuggestions (low scaffolding), and next-paragraph suggestions (high\nscaffolding). Our findings reveal a U-shaped impact of scaffolding on writing\nquality and productivity (words/time). While low scaffolding did not\nsignificantly improve writing quality or productivity, high scaffolding led to\nsignificant improvements, especially benefiting non-regular writers and less\ntech-savvy users. No significant cognitive burden was observed while using the\nscaffolded writing tools, but a moderate decrease in text ownership and\nsatisfaction was noted. Our results have broad implications for the design of\nAI-powered writing tools, including the need for personalized scaffolding\nmechanisms.",
        "pdf_link": "https://arxiv.org/pdf/2402.11723v1.pdf"
    },
    {
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "authors": [
            "Farhad Moghimifar",
            "Yuan-Fang Li",
            "Robert Thomson",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-18T21:28:06Z",
        "summary": "Coalition negotiations are a cornerstone of parliamentary democracies,\ncharacterised by complex interactions and strategic communications among\npolitical parties. Despite its significance, the modelling of these\nnegotiations has remained unexplored with the domain of Natural Language\nProcessing (NLP), mostly due to lack of proper data. In this paper, we\nintroduce coalition negotiations as a novel NLP task, and model it as a\nnegotiation between large language model-based agents. We introduce a\nmultilingual dataset, POLCA, comprising manifestos of European political\nparties and coalition agreements over a number of elections in these countries.\nThis dataset addresses the challenge of the current scope limitations in\npolitical negotiation modelling by providing a diverse, real-world basis for\nsimulation. Additionally, we propose a hierarchical Markov decision process\ndesigned to simulate the process of coalition negotiation between political\nparties and predict the outcomes. We evaluate the performance of\nstate-of-the-art large language models (LLMs) as agents in handling coalition\nnegotiations, offering insights into their capabilities and paving the way for\nfuture advancements in political modelling.",
        "pdf_link": "https://arxiv.org/pdf/2402.11712v1.pdf"
    },
    {
        "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
        "authors": [
            "Yasaman Jafari",
            "Dheeraj Mekala",
            "Rose Yu",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2024-02-18T21:25:09Z",
        "summary": "RL-based techniques can be used to search for prompts that when fed into a\ntarget language model maximize a set of user-specified reward functions.\nHowever, in many target applications, the natural reward functions are in\ntension with one another -- for example, content preservation vs. style\nmatching in style transfer tasks. Current techniques focus on maximizing the\naverage of reward functions, which does not necessarily lead to prompts that\nachieve balance across rewards -- an issue that has been well-studied in the\nmulti-objective and robust optimization literature. In this paper, we adapt\nseveral techniques for multi-objective optimization to RL-based discrete prompt\noptimization -- two that consider volume of the Pareto reward surface, and\nanother that chooses an update direction that benefits all rewards\nsimultaneously. We conduct an empirical analysis of these methods on two NLP\ntasks: style transfer and machine translation, each using three competing\nreward functions. Our experiments demonstrate that multi-objective methods that\ndirectly optimize volume perform better and achieve a better balance of all\nrewards than those that attempt to find monotonic update directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.11711v1.pdf"
    },
    {
        "title": "GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network",
        "authors": [
            "Shuzhou Yuan",
            "Ercong Nie",
            "Michael F\u00e4rber",
            "Helmut Schmid",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2024-02-18T21:13:05Z",
        "summary": "Large Language Models (LLMs) exhibit strong In-Context Learning (ICL)\ncapabilities when prompts with demonstrations are applied to them. However,\nfine-tuning still remains crucial to further enhance their adaptability.\nPrompt-based fine-tuning proves to be an effective fine-tuning method in\nlow-data scenarios, but high demands on computing resources limit its\npracticality. We address this issue by introducing a prompt-based\nparameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into\nICL's information flow dynamics, which indicates that label words act in\nprompts as anchors for information propagation. GNNavi employs a Graph Neural\nNetwork (GNN) layer to precisely guide the aggregation and distribution of\ninformation flow during the processing of prompts by hardwiring the desired\ninformation flow into the GNN. Our experiments on text classification tasks\nwith GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning\nmethods in few-shot settings by updating just 0.2% to 0.5% of parameters. We\ncompare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and\nAdapter in terms of performance and efficiency. Our analysis reveals that\nGNNavi enhances information flow and ensures a clear aggregation process.",
        "pdf_link": "https://arxiv.org/pdf/2402.11709v1.pdf"
    },
    {
        "title": "Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable",
        "authors": [
            "Shahan Ali Memon",
            "Jevin D. West"
        ],
        "published": "2024-02-18T21:10:18Z",
        "summary": "In this commentary, we discuss the evolving nature of search engines, as they\nbegin to generate, index, and distribute content created by generative\nartificial intelligence (GenAI). Our discussion highlights challenges in the\nearly stages of GenAI integration, particularly around factual inconsistencies\nand biases. We discuss how output from GenAI carries an unwarranted sense of\ncredibility, while decreasing transparency and sourcing ability. Furthermore,\nsearch engines are already answering queries with error-laden, generated\ncontent, further blurring the provenance of information and impacting the\nintegrity of the information ecosystem. We argue how all these factors could\nreduce the reliability of search engines. Finally, we summarize some of the\nactive research directions and open questions.",
        "pdf_link": "https://arxiv.org/pdf/2402.11707v1.pdf"
    },
    {
        "title": "Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation",
        "authors": [
            "Kailun Jin",
            "Chung-Yu Wang",
            "Hung Viet Pham",
            "Hadi Hemmati"
        ],
        "published": "2024-02-18T20:48:09Z",
        "summary": "Large language models (LLMs) have demonstrated notable proficiency in code\ngeneration, with numerous prior studies showing their promising capabilities in\nvarious development scenarios. However, these studies mainly provide\nevaluations in research settings, which leaves a significant gap in\nunderstanding how effectively LLMs can support developers in real-world. To\naddress this, we conducted an empirical analysis of conversations in DevGPT, a\ndataset collected from developers' conversations with ChatGPT (captured with\nthe Share Link feature on platforms such as GitHub). Our empirical findings\nindicate that the current practice of using LLM-generated code is typically\nlimited to either demonstrating high-level concepts or providing examples in\ndocumentation, rather than to be used as production-ready code. These findings\nindicate that there is much future work needed to improve LLMs in code\ngeneration before they can be integral parts of modern software development.",
        "pdf_link": "https://arxiv.org/pdf/2402.11702v2.pdf"
    },
    {
        "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
        "authors": [
            "Shuzhou Yuan",
            "Ercong Nie",
            "Bolei Ma",
            "Michael F\u00e4rber"
        ],
        "published": "2024-02-18T20:47:10Z",
        "summary": "Large Language Models (LLMs) possess outstanding capabilities in addressing\nvarious natural language processing (NLP) tasks. However, the sheer size of\nthese models poses challenges in terms of storage, training and inference due\nto the inclusion of billions of parameters through layer stacking. While\ntraditional approaches such as model pruning or distillation offer ways for\nreducing model size, they often come at the expense of performance retention.\nIn our investigation, we systematically explore the approach of reducing the\nnumber of layers in LLMs. Surprisingly, we observe that even with fewer layers,\nLLMs maintain similar or better performance levels, particularly in\nprompt-based fine-tuning for text classification tasks. Remarkably, in certain\ncases, models with a single layer outperform their fully layered counterparts.\nThese findings offer valuable insights for future work aimed at mitigating the\nsize constraints of LLMs while preserving their performance, thereby opening\navenues for significantly more efficient use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11700v1.pdf"
    },
    {
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "authors": [
            "Zhiyang Xu",
            "Chao Feng",
            "Rulin Shao",
            "Trevor Ashby",
            "Ying Shen",
            "Di Jin",
            "Yu Cheng",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "published": "2024-02-18T19:38:44Z",
        "summary": "Despite vision-language models' (VLMs) remarkable capabilities as versatile\nvisual assistants, two substantial challenges persist within the existing VLM\nframeworks: (1) lacking task diversity in pretraining and visual instruction\ntuning, and (2) annotation error and bias in GPT-4 synthesized instruction\ntuning data. Both challenges lead to issues such as poor generalizability,\nhallucination, and catastrophic forgetting. To address these challenges, we\nconstruct Vision-Flan, the most diverse publicly available visual instruction\ntuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances\nsourced from academic datasets, and each task is accompanied by an\nexpert-written instruction. In addition, we propose a two-stage instruction\ntuning framework, in which VLMs are firstly finetuned on Vision-Flan and\nfurther tuned on GPT-4 synthesized data. We find this two-stage tuning\nframework significantly outperforms the traditional single-stage visual\ninstruction tuning framework and achieves the state-of-the-art performance\nacross a wide range of multi-modal evaluation benchmarks. Finally, we conduct\nin-depth analyses to understand visual instruction tuning and our findings\nreveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs'\ncapabilities but rather modulates the model's responses to human-preferred\nformats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can\neffectively align VLM responses with human-preference; (3) Visual instruction\ntuning mainly helps large-language models (LLMs) to understand visual features.",
        "pdf_link": "https://arxiv.org/pdf/2402.11690v1.pdf"
    },
    {
        "title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation",
        "authors": [
            "Tejpalsingh Siledar",
            "Swaroop Nath",
            "Sankara Sri Raghava Ravindra Muddu",
            "Rupasai Rangaraju",
            "Swaprava Nath",
            "Pushpak Bhattacharyya",
            "Suman Banerjee",
            "Amey Patil",
            "Sudhanshu Shekhar Singh",
            "Muthusamy Chelliah",
            "Nikesh Garera"
        ],
        "published": "2024-02-18T19:13:52Z",
        "summary": "Evaluation of opinion summaries using conventional reference-based metrics\nrarely provides a holistic evaluation and has been shown to have a relatively\nlow correlation with human judgments. Recent studies suggest using Large\nLanguage Models (LLMs) as reference-free metrics for NLG evaluation, however,\nthey remain unexplored for opinion summary evaluation. Moreover, limited\nopinion summary evaluation datasets inhibit progress. To address this, we\nrelease the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation\nof opinion summaries: fluency, coherence, relevance, faithfulness, aspect\ncoverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a\ndimension-independent prompt, and Op-Prompts, a dimension-dependent set of\nprompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt\nemerges as a good alternative for evaluating opinion summaries achieving an\naverage Spearman correlation of 0.70 with humans, outperforming all previous\napproaches. To the best of our knowledge, we are the first to investigate LLMs\nas evaluators on both closed-source and open-source models in the opinion\nsummarization domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.11683v1.pdf"
    },
    {
        "title": "A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models",
        "authors": [
            "Jaylen Jones",
            "Lingbo Mo",
            "Eric Fosler-Lussier",
            "Huan Sun"
        ],
        "published": "2024-02-18T18:56:07Z",
        "summary": "Counter narratives - informed responses to hate speech contexts designed to\nrefute hateful claims and de-escalate encounters - have emerged as an effective\nhate speech intervention strategy. While previous work has proposed automatic\ncounter narrative generation methods to aid manual interventions, the\nevaluation of these approaches remains underdeveloped. Previous automatic\nmetrics for counter narrative evaluation lack alignment with human judgment as\nthey rely on superficial reference comparisons instead of incorporating key\naspects of counter narrative quality as evaluation criteria. To address prior\nevaluation limitations, we propose a novel evaluation framework prompting LLMs\nto provide scores and feedback for generated counter narrative candidates using\n5 defined aspects derived from guidelines from counter narrative specialized\nNGOs. We found that LLM evaluators achieve strong alignment to human-annotated\nscores and feedback and outperform alternative metrics, indicating their\npotential as multi-aspect, reference-free and interpretable evaluators for\ncounter narrative evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.11676v2.pdf"
    },
    {
        "title": "Autocorrect for Estonian texts: final report from project EKTB25",
        "authors": [
            "Agnes Luhtaru",
            "Martin Vainikko",
            "Krista Liin",
            "Kais Allkivi-Metsoja",
            "Jaagup Kippar",
            "Pille Eslon",
            "Mark Fishel"
        ],
        "published": "2024-02-18T18:20:57Z",
        "summary": "The project was funded in 2021-2023 by the National Programme of Estonian\nLanguage Technology. Its main aim was to develop spelling and grammar\ncorrection tools for the Estonian language. The main challenge was the very\nsmall amount of available error correction data needed for such development. To\nmitigate this, (1) we annotated more correction data for model training and\ntesting, (2) we tested transfer-learning, i.e. retraining machine learning\nmodels created for other tasks, so as not to depend solely on correction data,\n(3) we compared the developed method and model with alternatives, including\nlarge language models. We also developed automatic evaluation, which can\ncalculate the accuracy and yield of corrections by error category, so that the\neffectiveness of different methods can be compared in detail.\n  There has been a breakthrough in large language models during the project:\nGPT4, a commercial language model with Estonian-language support, has been\ncreated. We took into account the existence of the model when adjusting plans\nand in the report we present a comparison with the ability of GPT4 to improve\nthe Estonian language text.\n  The final results show that the approach we have developed provides better\nscores than GPT4 and the result is usable but not entirely reliable yet. The\nreport also contains ideas on how GPT4 and other major language models can be\nimplemented in the future, focusing on open-source solutions.\n  All results of this project are open-data/open-source, with licenses that\nallow them to be used for purposes including commercial ones.",
        "pdf_link": "https://arxiv.org/pdf/2402.11671v1.pdf"
    },
    {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
            "Renxi Wang",
            "Haonan Li",
            "Xudong Han",
            "Yixuan Zhang",
            "Timothy Baldwin"
        ],
        "published": "2024-02-18T17:10:07Z",
        "summary": "Large language models (LLMs) have achieved success in acting as agents, which\ninteract with environments through tools like search engines. However, LLMs are\nnot optimized specifically for tool use during training or alignment, limiting\ntheir effectiveness as agents. To resolve this problem, previous work has\ncollected interaction trajectories between GPT-4 and environments, and\nfine-tuned smaller models with them. As part of this, the standard approach has\nbeen to simply discard trajectories that do not finish the task successfully,\nwhich, on the one hand, leads to a significant waste of data and resources, and\non the other hand, has the potential to limit the possible optimization paths\nduring fine-tuning. In this paper, we contend that large language models can\nlearn from failures through appropriate data cleaning and fine-tuning\nstrategies. We conduct experiments on mathematical reasoning, multi-hop\nquestion answering, and strategic question answering tasks. Experimental\nresults demonstrate that compared to solely using positive examples,\nincorporating negative examples enhances model performance by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2402.11651v1.pdf"
    },
    {
        "title": "Stealthy Attack on Large Language Model based Recommendation",
        "authors": [
            "Jinghao Zhang",
            "Yuting Liu",
            "Qiang Liu",
            "Shu Wu",
            "Guibing Guo",
            "Liang Wang"
        ],
        "published": "2024-02-18T16:51:02Z",
        "summary": "Recently, the powerful large language models (LLMs) have been instrumental in\npropelling the progress of recommender systems (RS). However, while these\nsystems have flourished, their susceptibility to security threats has been\nlargely overlooked. In this work, we reveal that the introduction of LLMs into\nrecommendation models presents new security vulnerabilities due to their\nemphasis on the textual content of items. We demonstrate that attackers can\nsignificantly boost an item's exposure by merely altering its textual content\nduring the testing phase, without requiring direct interference with the\nmodel's training process. Additionally, the attack is notably stealthy, as it\ndoes not affect the overall recommendation performance and the modifications to\nthe text are subtle, making it difficult for users and platforms to detect. Our\ncomprehensive experiments across four mainstream LLM-based recommendation\nmodels demonstrate the superior efficacy and stealthiness of our approach. Our\nwork unveils a significant security gap in LLM-based recommendation systems and\npaves the way for future research on protecting these systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.14836v1.pdf"
    },
    {
        "title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
        "authors": [
            "Lanning Wei",
            "Jun Gao",
            "Huan Zhao",
            "Quanming Yao"
        ],
        "published": "2024-02-18T16:43:21Z",
        "summary": "Graph-structured data are the commonly used and have wide application\nscenarios in the real world. For these diverse applications, the vast variety\nof learning tasks, graph domains, and complex graph learning procedures present\nchallenges for human experts when designing versatile graph learning\napproaches. Facing these challenges, large language models (LLMs) offer a\npotential solution due to the extensive knowledge and the human-like\nintelligence. This paper proposes a novel conceptual prototype for designing\nversatile graph learning methods with LLMs, with a particular focus on the\n\"where\" and \"how\" perspectives. From the \"where\" perspective, we summarize four\nkey graph learning procedures, including task definition, graph data feature\nengineering, model selection and optimization, deployment and serving. We then\nexplore the application scenarios of LLMs in these procedures across a wider\nspectrum. In the \"how\" perspective, we align the abilities of LLMs with the\nrequirements of each procedure. Finally, we point out the promising directions\nthat could better leverage the strength of LLMs towards versatile graph\nlearning methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.11641v2.pdf"
    },
    {
        "title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks",
        "authors": [
            "Yichen Wang",
            "Shangbin Feng",
            "Abe Bohan Hou",
            "Xiao Pu",
            "Chao Shen",
            "Xiaoming Liu",
            "Yulia Tsvetkov",
            "Tianxing He"
        ],
        "published": "2024-02-18T16:36:00Z",
        "summary": "The widespread use of large language models (LLMs) is increasing the demand\nfor methods that detect machine-generated text to prevent misuse. The goal of\nour study is to stress test the detectors' robustness to malicious attacks\nunder realistic scenarios. We comprehensively study the robustness of popular\nmachine-generated text detectors under attacks from diverse categories:\nediting, paraphrasing, prompting, and co-generating. Our attacks assume limited\naccess to the generator LLMs, and we compare the performance of detectors on\ndifferent attacks under different budget levels. Our experiments reveal that\nalmost none of the existing detectors remain robust under all the attacks, and\nall detectors exhibit different loopholes. Averaging all detectors, the\nperformance drops by 35% across all attacks. Further, we investigate the\nreasons behind these defects and propose initial out-of-the-box patches to\nimprove robustness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11638v1.pdf"
    },
    {
        "title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs",
        "authors": [
            "Arian Askari",
            "Roxana Petcu",
            "Chuan Meng",
            "Mohammad Aliannejadi",
            "Amin Abolghasemi",
            "Evangelos Kanoulas",
            "Suzan Verberne"
        ],
        "published": "2024-02-18T16:20:43Z",
        "summary": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11633v1.pdf"
    },
    {
        "title": "On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs",
        "authors": [
            "Hankz Hankui Zhuo",
            "Xin Chen",
            "Rong Pan"
        ],
        "published": "2024-02-18T15:53:32Z",
        "summary": "Plan synthesis aims to generate a course of actions or policies to transit\ngiven initial states to goal states, provided domain models that could be\ndesigned by experts or learnt from training data or interactions with the\nworld. Intrigued by the claims of emergent planning capabilities in large\nlanguage models (LLMs), works have been proposed to investigate the planning\neffectiveness of LLMs, without considering any utilization of off-the-shelf\nplanning techniques in LLMs. In this paper, we aim to further study the insight\nof the planning capability of LLMs by investigating the roles of LLMs in\noff-the-shelf planning frameworks. To do this, we investigate the effectiveness\nof embedding LLMs into one of the well-known planning frameworks, graph-based\nplanning, proposing a novel LLMs-based planning framework with LLMs embedded in\ntwo levels of planning graphs, i.e., mutual constraints generation level and\nconstraints solving level. We empirically exhibit the effectiveness of our\nproposed framework in various planning domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.00783v1.pdf"
    },
    {
        "title": "SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models",
        "authors": [
            "Koren Lazar",
            "Matan Vetzler",
            "Guy Uziel",
            "David Boaz",
            "Esther Goldbraich",
            "David Amid",
            "Ateret Anaby-Tavor"
        ],
        "published": "2024-02-18T15:33:24Z",
        "summary": "In the digital era, the widespread use of APIs is evident. However, scalable\nutilization of APIs poses a challenge due to structure divergence observed in\nonline API documentation. This underscores the need for automatic tools to\nfacilitate API consumption. A viable approach involves the conversion of\ndocumentation into an API Specification format. While previous attempts have\nbeen made using rule-based methods, these approaches encountered difficulties\nin generalizing across diverse documentation. In this paper we introduce\nSpeCrawler, a comprehensive system that utilizes large language models (LLMs)\nto generate OpenAPI Specifications from diverse API documentation through a\ncarefully crafted pipeline. By creating a standardized format for numerous\nAPIs, SpeCrawler aids in streamlining integration processes within API\norchestrating systems and facilitating the incorporation of tools into LLMs.\nThe paper explores SpeCrawler's methodology, supported by empirical evidence\nand case studies, demonstrating its efficacy through LLM capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.11625v1.pdf"
    },
    {
        "title": "Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection",
        "authors": [
            "Valeria Pastorino",
            "Jasivan A. Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2024-02-18T15:27:48Z",
        "summary": "This work contributes to the expanding research on the applicability of LLMs\nin social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and\nFlan-T5 models in detecting framing bias in news headlines through zero-shot,\nfew-shot, and explainable prompting methods. A key insight from our evaluation\nis the notable efficacy of explainable prompting in enhancing the reliability\nof these models, highlighting the importance of explainable settings for social\nscience research on framing bias. GPT-4, in particular, demonstrated enhanced\nperformance in few-shot scenarios when presented with a range of relevant,\nin-domain examples. FLAN-T5's poor performance indicates that smaller models\nmay require additional task-specific fine-tuning for identifying framing bias\ndetection. Our study also found that models, particularly GPT-4, often\nmisinterpret emotional language as an indicator of framing bias, underscoring\nthe challenge of distinguishing between reporting genuine emotional expression\nand intentionally use framing bias in news headlines. We further evaluated the\nmodels on two subsets of headlines where the presence or absence of framing\nbias was either clear-cut or more contested, with the results suggesting that\nthese models' can be useful in flagging potential annotation inaccuracies\nwithin existing or new datasets. Finally, the study evaluates the models in\nreal-world conditions (\"in the wild\"), moving beyond the initial dataset\nfocused on U.S. Gun Violence, assessing the models' performance on framed\nheadlines covering a broad range of topics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11621v2.pdf"
    },
    {
        "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
        "authors": [
            "Guijin Son",
            "Sangwon Baek",
            "Sangdae Nam",
            "Ilgyun Jeong",
            "Seungone Kim"
        ],
        "published": "2024-02-18T14:25:19Z",
        "summary": "Large language models (LLMs) are typically prompted to follow a single\ninstruction per inference call. In this work, we analyze whether LLMs also hold\nthe capability to handle multiple instructions simultaneously, denoted as\nMulti-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task\nInference Benchmark), a comprehensive evaluation benchmark encompassing 5,000\ninstances across 25 tasks. Each task in the MTI Bench involves 2 to 3\nsub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces\nthe total inference time by 1.46 times in average since it does not require\nmultiple inference calls. Interestingly, contrary to the expectation that LLMs\nwould perform better when tasks are divided, we find that state-of-the-art\nLLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved\nperformance with Multi-Task Inference compared to Single-Task Inference on the\nMTI Bench. We release the MTI Bench dataset and our code at this link\nhttps://github.com/guijinSON/MTI-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2402.11597v1.pdf"
    },
    {
        "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark",
        "authors": [
            "Yihua Zhang",
            "Pingzhi Li",
            "Junyuan Hong",
            "Jiaxiang Li",
            "Yimeng Zhang",
            "Wenqing Zheng",
            "Pin-Yu Chen",
            "Jason D. Lee",
            "Wotao Yin",
            "Mingyi Hong",
            "Zhangyang Wang",
            "Sijia Liu",
            "Tianlong Chen"
        ],
        "published": "2024-02-18T14:08:48Z",
        "summary": "In the evolving landscape of natural language processing (NLP), fine-tuning\npre-trained Large Language Models (LLMs) with first-order (FO) optimizers like\nSGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial\nmemory overhead from back-propagation (BP) for FO gradient computation presents\na significant challenge. Addressing this issue is crucial, especially for\napplications like on-device training where memory efficiency is paramount. This\npaper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a\nsolution for reducing memory costs during LLM fine-tuning, building on the\ninitial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work\nexpands the exploration to a wider array of ZO optimization techniques, through\na comprehensive, first-of-its-kind benchmarking study across five LLM families\n(Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five\nfine-tuning schemes. Our study unveils previously overlooked optimization\nprinciples, highlighting the importance of task alignment, the role of the\nforward gradient method, and the balance between algorithm complexity and\nfine-tuning performance. We further introduce novel enhancements to ZO\noptimization, including block-wise descent, hybrid training, and gradient\nsparsity. Our study offers a promising direction for achieving further\nmemory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at\nhttps://github.com/ZO-Bench/ZO-LLM .",
        "pdf_link": "https://arxiv.org/pdf/2402.11592v2.pdf"
    },
    {
        "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
        "authors": [
            "Peng Xu",
            "Wenqi Shao",
            "Mengzhao Chen",
            "Shitao Tang",
            "Kaipeng Zhang",
            "Peng Gao",
            "Fengwei An",
            "Yu Qiao",
            "Ping Luo"
        ],
        "published": "2024-02-18T12:44:15Z",
        "summary": "Large language models (LLMs) have demonstrated outstanding performance in\nvarious tasks, such as text summarization, text question-answering, and etc.\nWhile their performance is impressive, the computational footprint due to their\nvast number of parameters can be prohibitive. Existing solutions such as\nSparseGPT and Wanda attempt to alleviate this issue through weight pruning.\nHowever, their layer-wise approach results in significant perturbation to the\nmodel's output and requires meticulous hyperparameter tuning, such as the\npruning rate, which can adversely affect overall model performance. To address\nthis, this paper introduces a novel LLM pruning technique dubbed blockwise\nparameter-efficient sparsity allocation (BESA) by applying a blockwise\nreconstruction loss. In contrast to the typical layer-wise pruning techniques,\nBESA is characterized by two distinctive attributes: i) it targets the overall\npruning error with respect to individual transformer blocks, and ii) it\nallocates layer-specific sparsity in a differentiable manner, both of which\nensure reduced performance degradation after pruning. Our experiments show that\nBESA achieves state-of-the-art performance, efficiently pruning LLMs like\nLLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five\nhours. Code is available at\n\\href{https://github.com/OpenGVLab/LLMPrune-BESA}{here}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16880v1.pdf"
    },
    {
        "title": "Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru",
        "authors": [
            "Zining Wang",
            "Paul Reisert",
            "Eric Nichols",
            "Randy Gomez"
        ],
        "published": "2024-02-18T12:35:52Z",
        "summary": "Social robots aim to establish long-term bonds with humans through engaging\nconversation. However, traditional conversational approaches, reliant on\nscripted interactions, often fall short in maintaining engaging conversations.\nThis paper addresses this limitation by integrating large language models\n(LLMs) into social robots to achieve more dynamic and expressive conversations.\nWe introduce a fully-automated conversation system that leverages LLMs to\ngenerate robot responses with expressive behaviors, congruent with the robot's\npersonality. We incorporate robot behavior with two modalities: 1) a\ntext-to-speech (TTS) engine capable of various delivery styles, and 2) a\nlibrary of physical actions for the robot. We develop a custom,\nstate-of-the-art emotion recognition model to dynamically select the robot's\ntone of voice and utilize emojis from LLM output as cues for generating robot\nactions. A demo of our system is available here. To illuminate design and\nimplementation issues, we conduct a pilot study where volunteers chat with a\nsocial robot using our proposed system, and we analyze their feedback,\nconducting a rigorous error analysis of chat transcripts. Feedback was\noverwhelmingly positive, with participants commenting on the robot's empathy,\nhelpfulness, naturalness, and entertainment. Most negative feedback was due to\nautomatic speech recognition (ASR) errors which had limited impact on\nconversations. However, we observed a small class of errors, such as the LLM\nrepeating itself or hallucinating fictitious information and human responses,\nthat have the potential to derail conversations, raising important issues for\nLLM application.",
        "pdf_link": "https://arxiv.org/pdf/2402.11571v1.pdf"
    },
    {
        "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration",
        "authors": [
            "Jun Zhao",
            "Can Zu",
            "Hao Xu",
            "Yi Lu",
            "Wei He",
            "Yiwen Ding",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-18T11:46:52Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance in\nunderstanding language and executing complex reasoning tasks. However, LLMs\nwith long context windows have been notorious for their expensive training\ncosts and high inference latency. Even the most advanced models such as GPT-4\nand Claude2 often make mistakes when processing inputs of over $100k$ tokens, a\nphenomenon also known as \\textit{lost in the middle}. In this paper, we propose\n\\textsc{LongAgent}, a method based on multi-agent collaboration, which scales\nLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority\nin long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is\nresponsible for understanding user intent and directing team members to acquire\ninformation from documents. Due to members' hallucinations, it is non-trivial\nfor a leader to obtain accurate information from the responses of dozens to\nhundreds of members. To address this, we develop an \\textit{inter-member\ncommunication} mechanism to resolve response conflicts caused by hallucinations\nthrough information sharing. Our experimental results indicate that\n\\textsc{LongAgent} offers a promising alternative for long-text processing. The\nagent team instantiated with LLaMA-7B achieves significant improvements in\ntasks such as 128k-long text retrieval, multi-hop question answering, compared\nto GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.11550v2.pdf"
    },
    {
        "title": "KMMLU: Measuring Massive Multitask Language Understanding in Korean",
        "authors": [
            "Guijin Son",
            "Hanwool Lee",
            "Sungdong Kim",
            "Seungone Kim",
            "Niklas Muennighoff",
            "Taekyoon Choi",
            "Cheonbok Park",
            "Kang Min Yoo",
            "Stella Biderman"
        ],
        "published": "2024-02-18T11:41:07Z",
        "summary": "We propose KMMLU, a new Korean benchmark with 35,030 expert-level\nmultiple-choice questions across 45 subjects ranging from humanities to STEM.\nUnlike previous Korean benchmarks that are translated from existing English\nbenchmarks, KMMLU is collected from original Korean exams, capturing linguistic\nand cultural aspects of the Korean language. We test 26 publically available\nand proprietary LLMs, identifying significant room for improvement. The best\npublicly available model achieves 50.54% on KMMLU, far below the average human\nperformance of 62.6%. This model was primarily trained for English and Chinese,\nnot Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far\nworse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and\nHyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that\nfurther work is needed to improve Korean LLMs, and KMMLU offers the right tool\nto track this progress. We make our dataset publicly available on the Hugging\nFace Hub and integrate the benchmark into EleutherAI's Language Model\nEvaluation Harness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11548v1.pdf"
    },
    {
        "title": "ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation",
        "authors": [
            "Zihao Tang",
            "Zheqi Lv",
            "Shengyu Zhang",
            "Fei Wu",
            "Kun Kuang"
        ],
        "published": "2024-02-18T11:24:34Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized\nvarious sectors by automating routine tasks, marking a step toward the\nrealization of Artificial General Intelligence (AGI). However, they still\nstruggle to accommodate the diverse and specific needs of users and simplify\nthe utilization of AI models for the average user. In response, we propose\nModelGPT, a novel framework designed to determine and generate AI models\nspecifically tailored to the data or task descriptions provided by the user,\nleveraging the capabilities of LLMs. Given user requirements, ModelGPT is able\nto provide tailored models at most 270x faster than the previous paradigms\n(e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV,\nand Tabular datasets attest to the effectiveness of our framework in making AI\nmodels more accessible and user-friendly. Our code is available at\nhttps://github.com/IshiKura-a/ModelGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.12408v1.pdf"
    },
    {
        "title": "Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning",
        "authors": [
            "Yang Zhao",
            "Li Du",
            "Xiao Ding",
            "Kai Xiong",
            "Zhouhao Sun",
            "Jun Shi",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2024-02-18T10:36:05Z",
        "summary": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11537v2.pdf"
    },
    {
        "title": "Ploutos: Towards interpretable stock movement prediction with financial large language model",
        "authors": [
            "Hanshuang Tong",
            "Jun Li",
            "Ning Wu",
            "Ming Gong",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "published": "2024-02-18T10:28:18Z",
        "summary": "Recent advancements in large language models (LLMs) have opened new pathways\nfor many domains. However, the full potential of LLMs in financial investments\nremains largely untapped. There are two main challenges for typical deep\nlearning-based methods for quantitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for stock movement prediction.\nSecond, traditional methods lack clarity and interpretability, which impedes\ntheir application in scenarios where the justification for predictions is\nessential. To solve the above challenges, we propose Ploutos, a novel financial\nLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\ncontains multiple primary experts that can analyze different modal data, such\nas text and numbers, and provide quantitative strategies from different\nperspectives. Then PloutosGPT combines their insights and predictions and\ngenerates interpretable rationales. To generate accurate and faithful\nrationales, the training strategy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\nweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\nexperiments show our framework outperforms the state-of-the-art methods on both\nprediction accuracy and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00782v1.pdf"
    },
    {
        "title": "Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models",
        "authors": [
            "Shirley Anugrah Hayati",
            "Taehee Jung",
            "Tristan Bodding-Long",
            "Sudipta Kar",
            "Abhinav Sethy",
            "Joo-Kyung Kim",
            "Dongyeop Kang"
        ],
        "published": "2024-02-18T10:10:40Z",
        "summary": "Fine-tuning large language models (LLMs) with a collection of large and\ndiverse instructions has improved the model's generalization to different\ntasks, even for unseen tasks. However, most existing instruction datasets\ninclude only single instructions, and they struggle to follow complex\ninstructions composed of multiple subtasks (Wang et al., 2023a). In this work,\nwe propose a novel concept of compositional instructions called\nchain-of-instructions (CoI), where the output of one instruction becomes an\ninput for the next like a chain. Unlike the conventional practice of solving\nsingle instruction tasks, our proposed method encourages a model to solve each\nsubtask step by step until the final answer is reached. CoI-tuning (i.e.,\nfine-tuning with CoI instructions) improves the model's ability to handle\ninstructions composed of multiple subtasks. CoI-tuned models also outperformed\nbaseline models on multilingual summarization, demonstrating the\ngeneralizability of CoI models on unseen composite downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.11532v1.pdf"
    },
    {
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
            "Muyang He",
            "Yexin Liu",
            "Boya Wu",
            "Jianhao Yuan",
            "Yueze Wang",
            "Tiejun Huang",
            "Bo Zhao"
        ],
        "published": "2024-02-18T10:09:10Z",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated notable\ncapabilities in general visual understanding and reasoning tasks. However,\ntheir deployment is hindered by substantial computational costs in both\ntraining and inference, limiting accessibility to the broader research and user\ncommunities. A straightforward solution is to leverage smaller pre-trained\nvision and language models, which inevitably causes significant performance\ndrop. In this paper, we demonstrate the possibility to beat the scaling law and\ntrain a smaller but better MLLM by exploring more informative training data.\nSpecifically, we introduce Bunny, a family of lightweight MLLMs with flexible\nvision and language backbones for efficient multimodal learning from condensed\ntraining data. Remarkably, our Bunny-3B outperforms the state-of-the-art large\nMLLMs, especially LLaVA-v1.5-13B, on multiple benchmarks. The code, models and\ndata can be found in https://github.com/BAAI-DCAI/Bunny.",
        "pdf_link": "https://arxiv.org/pdf/2402.11530v1.pdf"
    },
    {
        "title": "Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network",
        "authors": [
            "Lin Chen",
            "Fengli Xu",
            "Nian Li",
            "Zhenyu Han",
            "Meng Wang",
            "Yong Li",
            "Pan Hui"
        ],
        "published": "2024-02-18T09:21:12Z",
        "summary": "Heterogeneous information networks (HIN) have gained increasing popularity\nfor being able to capture complex relations between nodes of diverse types.\nMeta-structure was proposed to identify important patterns of relations on HIN,\nwhich has been proven effective for extracting rich semantic information and\nfacilitating graph neural networks to learn expressive representations.\nHowever, hand-crafted meta-structures pose challenges for scaling up, which\ndraws wide research attention for developing automatic meta-structure search\nalgorithms. Previous efforts concentrate on searching for meta-structures with\ngood empirical prediction performance, overlooking explainability. Thus, they\noften produce meta-structures prone to overfitting and incomprehensible to\nhumans. To address this, we draw inspiration from the emergent reasoning\nabilities of large language models (LLMs). We propose a novel REasoning\nmeta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into\nthe evolutionary procedure. ReStruct uses a grammar translator to encode\nmeta-structures into natural language sentences, and leverages the reasoning\npower of LLMs to evaluate semantically feasible meta-structures. ReStruct also\nemploys performance-oriented evolutionary operations. These two competing\nforces jointly optimize for semantic explainability and empirical performance\nof meta-structures. We also design a differential LLM explainer that can\nproduce natural language explanations for the discovered meta-structures, and\nrefine the explanation by reasoning through the search history. Experiments on\nfive datasets demonstrate ReStruct achieve SOTA performance in node\nclassification and link recommendation tasks. Additionally, a survey study\ninvolving 73 graduate students shows that the meta-structures and natural\nlanguage explanations generated by ReStruct are substantially more\ncomprehensible.",
        "pdf_link": "https://arxiv.org/pdf/2402.11518v1.pdf"
    },
    {
        "title": "Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM",
        "authors": [
            "Zijin Hong",
            "Zheng Yuan",
            "Hao Chen",
            "Qinggang Zhang",
            "Feiran Huang",
            "Xiao Huang"
        ],
        "published": "2024-02-18T09:10:04Z",
        "summary": "Generating accurate SQL for user queries (text-to-SQL) is a long-standing\nproblem since the generation of the SQL requires comprehending the query and\ndatabase and retrieving the accurate data from the database accordingly.\nExisting models rely on the comprehensive ability of Large Language Models\n(LLMs) to generate the SQL according to the database schema. However, there is\nsome necessary knowledge that is not explicitly included in the database schema\nor has been learned by LLMs. Thus, the generated SQL of the\nknowledge-insufficient queries may be inaccurate, which negatively impacts the\nrobustness of the text-to-SQL models. To deal with this situation, we propose\nthe Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM)\nto provide helpful knowledge for all types of text-to-SQL models. Specifically,\nwe provide the detailed design of DELLM, in terms of table reading, and the\nbasic fine-tuning process. We further provide a Preference Learning via\nDatabase Feedback (PLDBF) training strategy to guide the DELLM to generate more\nhelpful knowledge for LLMs. Extensive experiments verify DELLM can enhance the\nstate-of-the-art LLMs on text-to-SQL tasks. The model structure and the\nparameter weight of DELLM are released for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.11517v2.pdf"
    },
    {
        "title": "From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings",
        "authors": [
            "Aishik Rakshit",
            "Smriti Singh",
            "Shuvam Keshari",
            "Arijit Ghosh Chowdhury",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published": "2024-02-18T08:53:41Z",
        "summary": "Embeddings play a pivotal role in the efficacy of Large Language Models. They\nare the bedrock on which these models grasp contextual relationships and foster\na more nuanced understanding of language and consequently perform remarkably on\na plethora of complex tasks that require a fundamental understanding of human\nlanguage. Given that these embeddings themselves often reflect or exhibit bias,\nit stands to reason that these models may also inadvertently learn this bias.\nIn this work, we build on the seminal previous work and propose DeepSoftDebias,\nan algorithm that uses a neural network to perform 'soft debiasing'. We\nexhaustively evaluate this algorithm across a variety of SOTA datasets,\naccuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias\noutperforms the current state-of-the-art methods at reducing bias across\ngender, race, and religion.",
        "pdf_link": "https://arxiv.org/pdf/2402.11512v2.pdf"
    },
    {
        "title": "Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources",
        "authors": [
            "Jiamu Bai",
            "Daoyuan Chen",
            "Bingchen Qian",
            "Liuyi Yao",
            "Yaliang Li"
        ],
        "published": "2024-02-18T08:32:59Z",
        "summary": "Federated Learning (FL) has recently been applied to the parameter-efficient\nfine-tuning of Large Language Models (LLMs). While promising, it raises\nsignificant challenges due to the heterogeneous resources and data\ndistributions of clients.This study introduces FlexLoRA, a simple yet effective\naggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in\ntraditional FL that restricts the potential of clients with ample resources by\ntying them to the capabilities of the least-resourced participants. FlexLoRA\nallows for dynamic adjustment of local LoRA ranks, fostering the development of\na global model imbued with broader, less task-specific knowledge. By\nsynthesizing a full-size LoRA weight from individual client contributions and\nemploying Singular Value Decomposition (SVD) for weight redistribution,\nFlexLoRA fully leverages heterogeneous client resources. Involving over 1,600\nclients performing diverse NLP tasks, our experiments validate the efficacy of\nFlexLoRA, with the federated global model achieving up to a 3.1% average\nimprovement in downstream NLP task performance. FlexLoRA's practicality is\nfurther underscored by its seamless integration with existing LoRA-based FL\nmethods and theoretical analysis, offering a path toward scalable,\nprivacy-preserving federated tuning for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11505v1.pdf"
    },
    {
        "title": "What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs",
        "authors": [
            "Eran Hirsch",
            "Guy Uziel",
            "Ateret Anaby-Tavor"
        ],
        "published": "2024-02-18T07:42:49Z",
        "summary": "Planning is a fundamental task in artificial intelligence that involves\nfinding a sequence of actions that achieve a specified goal in a given\nenvironment. Large language models (LLMs) are increasingly used for\napplications that require planning capabilities, such as web or embodied\nagents. In line with recent studies, we demonstrate through experimentation\nthat LLMs lack necessary skills required for planning. Based on these\nobservations, we advocate for the potential of a hybrid approach that combines\nLLMs with classical planning methodology. Then, we introduce SimPlan, a novel\nhybrid-method, and evaluate its performance in a new challenging setup. Our\nextensive experiments across various planning domains demonstrate that SimPlan\nsignificantly outperforms existing LLM-based planners.",
        "pdf_link": "https://arxiv.org/pdf/2402.11489v1.pdf"
    },
    {
        "title": "MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing",
        "authors": [
            "Jiaqi Li",
            "Miaozeng Du",
            "Chuanyi Zhang",
            "Yongrui Chen",
            "Nan Hu",
            "Guilin Qi",
            "Haiyun Jiang",
            "Siyuan Cheng",
            "Bozhong Tian"
        ],
        "published": "2024-02-18T07:15:03Z",
        "summary": "Multimodal knowledge editing represents a critical advancement in enhancing\nthe capabilities of Multimodal Large Language Models (MLLMs). Despite its\npotential, current benchmarks predominantly focus on coarse-grained knowledge,\nleaving the intricacies of fine-grained (FG) multimodal entity knowledge\nlargely unexplored. This gap presents a notable challenge, as FG entity\nrecognition is pivotal for the practical deployment and effectiveness of MLLMs\nin diverse real-world scenarios. To bridge this gap, we introduce MIKE, a\ncomprehensive benchmark and dataset specifically designed for the FG multimodal\nentity knowledge editing. MIKE encompasses a suite of tasks tailored to assess\ndifferent perspectives, including Vanilla Name Answering, Entity-Level Caption,\nand Complex-Scenario Recognition. In addition, a new form of knowledge editing,\nMulti-step Editing, is introduced to evaluate the editing efficiency. Through\nour extensive evaluations, we demonstrate that the current state-of-the-art\nmethods face significant challenges in tackling our proposed benchmark,\nunderscoring the complexity of FG knowledge editing in MLLMs. Our findings\nspotlight the urgent need for novel approaches in this domain, setting a clear\nagenda for future research and development efforts within the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.14835v1.pdf"
    },
    {
        "title": "ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework",
        "authors": [
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Nitish Nagesh",
            "Mahyar Abbasian",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2024-02-18T06:07:17Z",
        "summary": "The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.",
        "pdf_link": "https://arxiv.org/pdf/2403.00781v2.pdf"
    },
    {
        "title": "scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation",
        "authors": [
            "Cong Li",
            "Meng Xiao",
            "Pengfei Wang",
            "Guihai Feng",
            "Xin Li",
            "Yuanchun Zhou"
        ],
        "published": "2024-02-18T05:39:00Z",
        "summary": "Despite the inherent limitations of existing Large Language Models in\ndirectly reading and interpreting single-cell omics data, they demonstrate\nsignificant potential and flexibility as the Foundation Model. This research\nfocuses on how to train and adapt the Large Language Model with the capability\nto interpret and distinguish cell types in single-cell RNA sequencing data. Our\npreliminary research results indicate that these foundational models excel in\naccurately categorizing known cell types, demonstrating the potential of the\nLarge Language Models as effective tools for uncovering new biological\ninsights.",
        "pdf_link": "https://arxiv.org/pdf/2402.12405v1.pdf"
    },
    {
        "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
        "authors": [
            "Shiyu Ni",
            "Keping Bi",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "published": "2024-02-18T04:57:19Z",
        "summary": "Large Language Models (LLMs) have been found to have difficulty knowing they\ndo not possess certain knowledge and tend to provide specious answers in such\ncases. Retrieval Augmentation (RA) has been extensively studied to mitigate\nLLMs' hallucinations. However, due to the extra overhead and unassured quality\nof retrieval, it may not be optimal to conduct RA all the time. A\nstraightforward idea is to only conduct retrieval when LLMs are uncertain about\na question. This motivates us to enhance the LLMs' ability to perceive their\nknowledge boundaries to help RA. In this paper, we first quantitatively measure\nLLMs' such ability and confirm their overconfidence. Then, we study how LLMs'\ncertainty about a question correlates with their dependence on external\nretrieved information. We propose several methods to enhance LLMs' perception\nof knowledge boundaries and show that they are effective in reducing\noverconfidence. Additionally, equipped with these methods, LLMs can achieve\ncomparable or even better performance of RA with much fewer retrieval calls.",
        "pdf_link": "https://arxiv.org/pdf/2402.11457v1.pdf"
    },
    {
        "title": "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence",
        "authors": [
            "Sebastian Antony Joseph",
            "Lily Chen",
            "Jan Trienes",
            "Hannah Louisa G\u00f6ke",
            "Monika Coers",
            "Wei Xu",
            "Byron C Wallace",
            "Junyi Jessy Li"
        ],
        "published": "2024-02-18T04:45:01Z",
        "summary": "Plain language summarization with LLMs can be useful for improving textual\naccessibility of technical content. But how factual are these summaries in a\nhigh-stakes domain like medicine? This paper presents FactPICO, a factuality\nbenchmark for plain language summarization of medical texts describing\nrandomized controlled trials (RCTs), which are the basis of evidence-based\nmedicine and can directly inform patient treatment. FactPICO consists of 345\nplain language summaries of RCT abstracts generated from three LLMs (i.e.,\nGPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language\nrationales from experts. We assess the factuality of critical elements of RCTs\nin those summaries: Populations, Interventions, Comparators, Outcomes (PICO),\nas well as the reported findings concerning these. We also evaluate the\ncorrectness of the extra information (e.g., explanations) added by LLMs. Using\nFactPICO, we benchmark a range of existing factuality metrics, including the\nnewly devised ones based on LLMs. We find that plain language summarization of\nmedical evidence is still challenging, especially when balancing between\nsimplicity and factuality, and that existing metrics correlate poorly with\nexpert judgments on the instance level.",
        "pdf_link": "https://arxiv.org/pdf/2402.11456v1.pdf"
    },
    {
        "title": "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks",
        "authors": [
            "Hanqing Wang",
            "Bowen Ping",
            "Shuo Wang",
            "Xu Han",
            "Yun Chen",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-18T04:41:25Z",
        "summary": "LoRA employs lightweight modules to customize large language models (LLMs)\nfor each downstream task or domain, where different learned additional modules\nrepresent diverse skills. Combining existing LoRAs to address new tasks can\nenhance the reusability of learned LoRAs, particularly beneficial for tasks\nwith limited annotated data. Most prior works on LoRA combination primarily\nrely on task-level weights for each involved LoRA, making different examples\nand tokens share the same LoRA weights. However, in generative tasks, different\ntokens may necessitate diverse skills to manage. Taking the Chinese math task\nas an example, understanding the problem description may depend more on the\nChinese LoRA, while the calculation part may rely more on the math LoRA. To\nthis end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the\nimpact of different LoRAs. The weights at each step are determined by a fusion\ngate with extremely few parameters, which can be learned with only 200 training\nexamples. Experiments across six generative tasks demonstrate that our method\nconsistently outperforms baselines with task-level fusion weights. This\nunderscores the necessity of introducing dynamic fusion weights for LoRA\ncombination.",
        "pdf_link": "https://arxiv.org/pdf/2402.11455v1.pdf"
    },
    {
        "title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization",
        "authors": [
            "Zhiyu Yang",
            "Zihan Zhou",
            "Shuo Wang",
            "Xin Cong",
            "Xu Han",
            "Yukun Yan",
            "Zhenghao Liu",
            "Zhixing Tan",
            "Pengyuan Liu",
            "Dong Yu",
            "Zhiyuan Liu",
            "Xiaodong Shi",
            "Maosong Sun"
        ],
        "published": "2024-02-18T04:28:28Z",
        "summary": "Scientific data visualization plays a crucial role in research by enabling\nthe direct display of complex information and assisting researchers in\nidentifying implicit patterns. Despite its importance, the use of Large\nLanguage Models (LLMs) for scientific data visualization remains rather\nunexplored. In this study, we introduce MatPlotAgent, an efficient\nmodel-agnostic LLM agent framework designed to automate scientific data\nvisualization tasks. Leveraging the capabilities of both code LLMs and\nmulti-modal LLMs, MatPlotAgent consists of three core modules: query\nunderstanding, code generation with iterative debugging, and a visual feedback\nmechanism for error correction. To address the lack of benchmarks in this\nfield, we present MatPlotBench, a high-quality benchmark consisting of 100\nhuman-verified test cases. Additionally, we introduce a scoring approach that\nutilizes GPT-4V for automatic evaluation. Experimental results demonstrate that\nMatPlotAgent can improve the performance of various LLMs, including both\ncommercial and open-source models. Furthermore, the proposed evaluation method\nshows a strong correlation with human-annotated scores.",
        "pdf_link": "https://arxiv.org/pdf/2402.11453v3.pdf"
    },
    {
        "title": "AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition",
        "authors": [
            "Zhaorun Chen",
            "Zhuokai Zhao",
            "Zhihong Zhu",
            "Ruiqi Zhang",
            "Xiang Li",
            "Bhiksha Raj",
            "Huaxiu Yao"
        ],
        "published": "2024-02-18T04:28:16Z",
        "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmulti-step reasoning tasks, yet their reliance on extensive manual labeling to\nprovide procedural feedback remains a significant impediment. To address this\nchallenge, in this paper, we propose a novel self-supervised framework AutoPRM\nthat efficiently enhances the fine-tuning of LLMs for intricate reasoning\nchallenges. Specifically, AutoPRM first decomposes complex problems into more\nmanageable subquestions with a controllable granularity switch, then\nsequentially apply reinforcement learning to iteratively improve the\nsubquestion solver. Additionally, we propose context-guided-decoding to avoid\nreward tampering and guide the subquestion solver towards the solution of the\nholistic problem. Extensive experiments show that AutoPRM significantly\nimproves performance on mathematical and commonsense reasoning tasks over SOTA.\nMore encouragingly, AutoPRM can be easily integrated with other orthogonal\nreasoning pipelines.",
        "pdf_link": "https://arxiv.org/pdf/2402.11452v1.pdf"
    },
    {
        "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
        "authors": [
            "Yubo Ma",
            "Zhibin Gou",
            "Junheng Hao",
            "Ruochen Xu",
            "Shuohang Wang",
            "Liangming Pan",
            "Yujiu Yang",
            "Yixin Cao",
            "Aixin Sun",
            "Hany Awadalla",
            "Weizhu Chen"
        ],
        "published": "2024-02-18T04:19:44Z",
        "summary": "Scientific reasoning poses an excessive challenge for even the most advanced\nLarge Language Models (LLMs). To make this task more practical and solvable for\nLLMs, we introduce a new task setting named tool-augmented scientific\nreasoning. This setting supplements LLMs with scalable toolsets, and shifts the\nfocus from pursuing an omniscient problem solver to a proficient tool-user. To\nfacilitate the research of such setting, we construct a tool-augmented training\ncorpus named MathFunc which encompasses over 30,000 samples and roughly 6,000\ntools. Building on MathFunc, we develop SciAgent to retrieve, understand and,\nif necessary, use tools for scientific problem solving. Additionally, we craft\na benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs'\nabilities with tool assistance. Extensive experiments on SciToolBench confirm\nthe effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other\nLLMs with the same size by more than 13% in absolute accuracy. Furthermore,\nSciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.11451v2.pdf"
    },
    {
        "title": "In-Context Example Ordering Guided by Label Distributions",
        "authors": [
            "Zhichao Xu",
            "Daniel Cohen",
            "Bei Wang",
            "Vivek Srikumar"
        ],
        "published": "2024-02-18T04:08:10Z",
        "summary": "By allowing models to predict without task-specific training, in-context\nlearning (ICL) with pretrained LLMs has enormous potential in NLP. However, a\nnumber of problems persist in ICL. In particular, its performance is sensitive\nto the choice and order of in-context examples. Given the same set of\nin-context examples with different orderings, model performance may vary\nbetween near random to near state-of-the-art. In this work, we formulate\nin-context example ordering as an optimization problem. We examine three\nproblem settings that differ in the assumptions they make about what is known\nabout the task. Inspired by the idea of learning from label proportions, we\npropose two principles for in-context example ordering guided by model's\nprobability predictions. We apply our proposed principles to thirteen text\nclassification datasets and nine different autoregressive LLMs with 700M to 13B\nparameters. We demonstrate that our approach outperforms the baselines by\nimproving the classification accuracy, reducing model miscalibration, and also\nby selecting better in-context examples.",
        "pdf_link": "https://arxiv.org/pdf/2402.11447v1.pdf"
    },
    {
        "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
        "authors": [
            "Siyuan Wang",
            "Zhuohan Long",
            "Zhihao Fan",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "published": "2024-02-18T03:40:06Z",
        "summary": "This paper presents a benchmark self-evolving framework to dynamically\nevaluate rapidly advancing Large Language Models (LLMs), aiming for a more\naccurate assessment of their capabilities and limitations. We utilize a\nmulti-agent system to manipulate the context or question of original instances,\nreframing new evolving instances with high confidence that dynamically extend\nexisting benchmarks. Towards a more scalable, robust and fine-grained\nevaluation, we implement six reframing operations to construct evolving\ninstances testing LLMs against diverse queries, data noise and probing their\nproblem-solving sub-abilities. With this framework, we extend benchmark\ndatasets of four tasks. Experimental results show a general performance decline\nin most LLMs against their original results. This decline under our scalable\nand robust evaluations, alongside our fine-grained evaluation, more accurately\nreflect models' capabilities. Besides, our framework widens performance\ndiscrepancies both between different models and within the same model across\nvarious tasks, facilitating more informed model selection for specific tasks\n(Code and data are available at\nhttps://github.com/NanshineLoong/Self-Evolving-Benchmark).",
        "pdf_link": "https://arxiv.org/pdf/2402.11443v1.pdf"
    },
    {
        "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs",
        "authors": [
            "Siyuan Wang",
            "Zhongyu Wei",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2024-02-18T03:38:51Z",
        "summary": "Large language models (LLMs) have achieved impressive human-like performance\nacross various reasoning tasks. However, their mastery of underlying\ninferential rules still falls short of human capabilities. To investigate this,\nwe propose a logic scaffolding inferential rule generation framework, to\nconstruct an inferential rule base, ULogic, comprising both primitive and\ncompositional rules across five domains. Our analysis of GPT-series models over\na rule subset reveals significant gaps in LLMs' logic understanding compared to\nhuman performance, especially in compositional and structural complex rules\nwith certain bias patterns. We further distill these rules into a smaller-scale\ninference engine for flexible rule generation and enhancing downstream\nreasoning. Through a multi-judger evaluation, our inference engine proves\neffective in generating accurate, complex and abstract conclusions and\npremises, and improve various commonsense reasoning tasks. Overall, our work\nsheds light on LLMs' limitations in grasping inferential rule and suggests ways\nto enhance their logical reasoning abilities~\\footnote{Code and data are\navailable at \\url{https://github.com/SiyuanWangw/ULogic}.}.",
        "pdf_link": "https://arxiv.org/pdf/2402.11442v1.pdf"
    },
    {
        "title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration",
        "authors": [
            "Fali Wang",
            "Runxue Bao",
            "Suhang Wang",
            "Wenchao Yu",
            "Yanchi Liu",
            "Wei Cheng",
            "Haifeng Chen"
        ],
        "published": "2024-02-18T03:36:26Z",
        "summary": "Though Large Language Models (LLMs) have shown remarkable open-generation\ncapabilities across diverse domains, they struggle with knowledge-intensive\ntasks. To alleviate this issue, knowledge integration methods have been\nproposed to enhance LLMs with domain-specific knowledge graphs using external\nmodules. However, they suffer from data inefficiency as they require both known\nand unknown knowledge for fine-tuning. Thus, we study a novel problem of\nintegrating unknown knowledge into LLMs efficiently without unnecessary overlap\nof known knowledge. Injecting new knowledge poses the risk of forgetting\npreviously acquired knowledge. To tackle this, we propose a novel\nInfuser-Guided Knowledge Integration (InfuserKI) framework that utilizes\ntransformer internal states to determine whether to enhance the original LLM\noutput with additional information, thereby effectively mitigating knowledge\nforgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs\ndemonstrate that InfuserKI can effectively acquire new knowledge and outperform\nstate-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge\nforgetting.",
        "pdf_link": "https://arxiv.org/pdf/2402.11441v1.pdf"
    },
    {
        "title": "Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models",
        "authors": [
            "Wenda Xu",
            "Guanglei Zhu",
            "Xuandong Zhao",
            "Liangming Pan",
            "Lei Li",
            "William Yang Wang"
        ],
        "published": "2024-02-18T03:10:39Z",
        "summary": "Recent studies show that self-feedback improves large language models (LLMs)\non certain tasks while worsens other tasks. We discovered that such a contrary\nis due to LLM's bias towards their own output. In this paper, we formally\ndefine LLM's self-bias -- the tendency to favor its own generation -- using two\nstatistics. We analyze six LLMs on translation, constrained text generation,\nand mathematical reasoning tasks. We find that self-bias is prevalent in all\nexamined LLMs across multiple languages and tasks. Our analysis reveals that\nwhile the self-refine pipeline improves the fluency and understandability of\nmodel outputs, it further amplifies self-bias. To mitigate such biases, we\ndiscover that larger model size and external feedback with accurate assessment\ncan significantly reduce bias in the self-refine pipeline, leading to actual\nperformance improvement in downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.11436v1.pdf"
    },
    {
        "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
        "authors": [
            "Long Qian",
            "Juncheng Li",
            "Yu Wu",
            "Yaobo Ye",
            "Hao Fei",
            "Tat-Seng Chua",
            "Yueting Zhuang",
            "Siliang Tang"
        ],
        "published": "2024-02-18T03:04:38Z",
        "summary": "Large Language Models (LLMs) demonstrate remarkable proficiency in\ncomprehending and handling text-based tasks. Many efforts are being made to\ntransfer these attributes to video modality, which are termed Video-LLMs.\nHowever, existing Video-LLMs can only capture the coarse-grained semantics and\nare unable to effectively handle tasks related to comprehension or localization\nof specific video segments. In light of these challenges, we propose Momentor,\na Video-LLM capable of accomplishing fine-grained temporal understanding tasks.\nTo support the training of Momentor, we design an automatic data generation\nengine to construct Moment-10M, a large-scale video instruction dataset with\nsegment-level instruction data. We train Momentor on Moment-10M, enabling it to\nperform segment-level reasoning and localization. Zero-shot evaluations on\nseveral tasks demonstrate that Momentor excels in fine-grained temporally\ngrounded comprehension and localization.",
        "pdf_link": "https://arxiv.org/pdf/2402.11435v1.pdf"
    },
    {
        "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",
        "authors": [
            "Kang Chen",
            "Zheng Lian",
            "Haiyang Sun",
            "Bin Liu",
            "Jianhua Tao"
        ],
        "published": "2024-02-18T02:52:54Z",
        "summary": "Deception detection has attracted increasing attention due to its importance\nin many practical scenarios. Currently, data scarcity harms the development of\nthis field. On the one hand, it is costly to hire participants to simulate\ndeception scenarios. On the other hand, it is difficult to collect videos\ncontaining deceptive behaviors on the Internet. To address data scarcity, this\npaper proposes a new data collection pipeline. Specifically, we use GPT-4 to\nsimulate a role-play between a suspect and a police officer. During\ninterrogation, the suspect lies to the police officer to evade responsibility\nfor the crime, while the police officer uncovers the truth and gathers\nevidence. Compared with previous datasets, this strategy reduces data\ncollection costs, providing a promising way to increase the dataset size.\nMeanwhile, we extend the traditional deception detection task to deception\nreasoning, further providing evidence for deceptive parts. This dataset can\nalso be used to evaluate the complex reasoning capability of current large\nlanguage models and serve as a reasoning benchmark for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.11432v1.pdf"
    },
    {
        "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
        "authors": [
            "Jun Gao",
            "Huan Zhao",
            "Wei Wang",
            "Changlong Yu",
            "Ruifeng Xu"
        ],
        "published": "2024-02-18T02:41:06Z",
        "summary": "In this study, we present EventRL, a reinforcement learning approach\ndeveloped to enhance event extraction for large language models (LLMs). EventRL\nutilizes outcome supervision with specific reward functions to tackle prevalent\nchallenges in LLMs, such as instruction following and hallucination, manifested\nas the mismatch of event structure and the generation of undefined event types.\nWe evaluate EventRL against existing methods like Few-Shot Prompting (FSP)\n(based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including\nGPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL\nsignificantly outperforms these conventional approaches by improving the\nperformance in identifying and structuring events, particularly in handling\nnovel event types. The study emphasizes the critical role of reward function\nselection and demonstrates the benefits of incorporating code data for better\nevent extraction. While increasing model size leads to higher accuracy,\nmaintaining the ability to generalize is essential to avoid overfitting.",
        "pdf_link": "https://arxiv.org/pdf/2402.11430v1.pdf"
    },
    {
        "title": "Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction",
        "authors": [
            "Yinghui Li",
            "Shang Qin",
            "Jingheng Ye",
            "Shirong Ma",
            "Yangning Li",
            "Libo Qin",
            "Xuming Hu",
            "Wenhao Jiang",
            "Hai-Tao Zheng",
            "Philip S. Yu"
        ],
        "published": "2024-02-18T01:40:34Z",
        "summary": "Recently, Large Language Models (LLMs) have been widely studied by\nresearchers for their roles in various downstream NLP tasks. As a fundamental\ntask in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to\ncorrect all potential grammatical errors in the input sentences. Previous\nstudies have shown that LLMs' performance as correctors on CGEC remains\nunsatisfactory due to its challenging task focus. To promote the CGEC field to\nbetter adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task\nso that they can be better utilized and explored in CGEC. Considering the rich\ngrammatical knowledge stored in LLMs and their powerful semantic understanding\ncapabilities, we utilize LLMs as explainers to provide explanation information\nfor the CGEC small models during error correction to enhance performance. We\nalso use LLMs as evaluators to bring more reasonable CGEC evaluations, thus\nalleviating the troubles caused by the subjectivity of the CGEC task. In\nparticular, our work is also an active exploration of how LLMs and small models\nbetter collaborate in downstream tasks. Extensive experiments and detailed\nanalyses on widely used datasets verify the effectiveness of our thinking\nintuition and the proposed methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.11420v1.pdf"
    },
    {
        "title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models",
        "authors": [
            "Yifan Yang",
            "Jiajun Zhou",
            "Ngai Wong",
            "Zheng Zhang"
        ],
        "published": "2024-02-18T01:20:00Z",
        "summary": "Various parameter-efficient fine-tuning (PEFT) techniques have been proposed\nto enable computationally efficient fine-tuning while maintaining model\nperformance. However, existing PEFT methods are still limited by the growing\nnumber of trainable parameters with the rapid deployment of Large Language\nModels (LLMs). To address this challenge, we present LoRETTA, an\nultra-parameter-efficient framework that significantly reduces trainable\nparameters through tensor-train decomposition. Specifically, we propose two\nmethods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs\ntensorized adapters, offering a high-performance yet lightweight approach for\nthe fine-tuning of LLMs. The latter emphasizes fine-tuning via weight\nparameterization with a set of small tensor factors. LoRETTA achieves\ncomparable or better performance than most widely used PEFT methods with up to\n$100\\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical\nresults demonstrate that the proposed method effectively improves training\nefficiency, enjoys better multi-task learning performance, and enhances the\nanti-overfitting capability. Plug-and-play codes built upon the Huggingface\nframework and PEFT library will be released.",
        "pdf_link": "https://arxiv.org/pdf/2402.11417v1.pdf"
    },
    {
        "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning",
        "authors": [
            "Yiyang Zhou",
            "Chenhang Cui",
            "Rafael Rafailov",
            "Chelsea Finn",
            "Huaxiu Yao"
        ],
        "published": "2024-02-18T00:56:16Z",
        "summary": "Instruction-following Vision Large Language Models (VLLMs) have achieved\nsignificant progress recently on a variety of tasks. These approaches merge\nstrong pre-trained vision models and large language models (LLMs). Since these\ncomponents are trained separately, the learned representations need to be\naligned with joint training on additional image-language pairs. This procedure\nis not perfect and can cause the model to hallucinate - provide answers that do\nnot accurately reflect the image, even when the core LLM is highly factual and\nthe vision backbone has sufficiently complete representations. In this work, we\nframe the hallucination problem as an alignment issue, tackle it with\npreference tuning. Specifically, we propose POVID to generate feedback data\nwith AI models. We use ground-truth instructions as the preferred response and\na two-stage approach to generate dispreferred data. First, we prompt GPT-4V to\ninject plausible hallucinations into the correct answer. Second, we distort the\nimage to trigger the inherent hallucination behavior of the VLLM. This is an\nautomated approach, which does not rely on human data generation or require a\nperfect expert, which makes it easily scalable. Finally, both of these\ngeneration strategies are integrated into an RLHF pipeline via Direct\nPreference Optimization. In experiments across broad benchmarks, we show that\nwe can not only reduce hallucinations, but improve model performance across\nstandard benchmarks, outperforming prior approaches. Our data and code are\navailable at https://github.com/YiyangZhou/POVID.",
        "pdf_link": "https://arxiv.org/pdf/2402.11411v1.pdf"
    },
    {
        "title": "Multi-dimensional Evaluation of Empathetic Dialog Responses",
        "authors": [
            "Zhichao Xu",
            "Jiepu Jiang"
        ],
        "published": "2024-02-18T00:32:33Z",
        "summary": "Empathy is a critical element of effective and satisfactory conversational\ncommunication, yet previous studies in measuring conversational empathy mostly\nfocus on expressed communicative intents -- in which way empathy is expressed,\nignoring the fact that conversation is also a collaborative practice involving\nboth speakers and listeners. In contrast, we propose a multi-dimensional\nempathy evaluation framework that extends upon existing work to measure both\nexpressed intents from the speaker's perspective and perceived empathy from the\nlistener's perspective. Applying the proposed framework to analyzing our\ninternal customer-service dialogue shows that the two dimensions (expressed\nintent types and perceived empathy) are inter-connected, while perceived\nempathy has high correlation with the satisfactory level of dialogue sessions.\nThis proposed framework still requires subjective assessments from trained\nannotators, which can be non-trivial to collect. To scale up evaluation without\nexcessive reliance on carefully annotated data, we explore different modeling\noptions to automatically measure conversational empathy with (1) prompting\nfrozen large language models (LLMs) and (2) training language model-based\nclassifiers. Extensive experiments on both internal and external dialogue\ndatasets show that measuring conversational empathy remains a challenging task\nfor prompting frozen LLMs, reflected by less satisfying performance of GPT-4\nand Flan family models. On the other hand, our proposed instruction-finetuned\nclassifiers based on sequence-to-sequence (Seq2Seq) language models is able to\nachieve the best performance compared to prior works and competitive baselines.\nFinally, we perform comprehensive ablation studies on the performance of\nproposed instruction-finetuned classifiers and give recommendations on\npotentially adopting them as automatic conversational empathy evaluation\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11409v1.pdf"
    },
    {
        "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection",
        "authors": [
            "Min Zhang",
            "Jianfeng He",
            "Taoran Ji",
            "Chang-Tien Lu"
        ],
        "published": "2024-02-18T00:04:40Z",
        "summary": "The fairness and trustworthiness of Large Language Models (LLMs) are\nreceiving increasing attention. Implicit hate speech, which employs indirect\nlanguage to convey hateful intentions, occupies a significant portion of\npractice. However, the extent to which LLMs effectively address this issue\nremains insufficiently examined. This paper delves into the capability of LLMs\nto detect implicit hate speech (Classification Task) and express confidence in\ntheir responses (Calibration Task). Our evaluation meticulously considers\nvarious prompt patterns and mainstream uncertainty estimation methods. Our\nfindings highlight that LLMs exhibit two extremes: (1) LLMs display excessive\nsensitivity towards groups or topics that may cause fairness issues, resulting\nin misclassifying benign statements as hate speech. (2) LLMs' confidence scores\nfor each method excessively concentrate on a fixed range, remaining unchanged\nregardless of the dataset's complexity. Consequently, the calibration\nperformance is heavily reliant on primary classification accuracy. These\ndiscoveries unveil new limitations of LLMs, underscoring the need for caution\nwhen optimizing models to ensure they do not veer towards extremes. This serves\nas a reminder to carefully consider sensitivity and confidence in the pursuit\nof model fairness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11406v2.pdf"
    },
    {
        "title": "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis",
        "authors": [
            "Shaochen Xu",
            "Zihao Wu",
            "Huaqin Zhao",
            "Peng Shu",
            "Zhengliang Liu",
            "Wenxiong Liao",
            "Sheng Li",
            "Andrea Sikora",
            "Tianming Liu",
            "Xiang Li"
        ],
        "published": "2024-02-17T22:46:44Z",
        "summary": "In this study, we leverage LLM to enhance the semantic analysis and develop\nsimilarity metrics for texts, addressing the limitations of traditional\nunsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs\nsuch as GPT-4 are employed for zero-shot text identification and label\ngeneration for radiology reports, where the labels are then used as\nmeasurements for text similarity. By testing the proposed framework on the\nMIMIC data, we find that GPT-4 generated labels can significantly improve the\nsemantic similarity assessment, with scores more closely aligned with clinical\nground truth than traditional NLP metrics. Our work demonstrates the\npossibility of conducting semantic analysis of the text data using\nsemi-quantitative reasoning results by the LLMs for highly specialized domains.\nWhile the framework is implemented for radiology report similarity analysis,\nits concept can be extended to other specialized domains as well.",
        "pdf_link": "https://arxiv.org/pdf/2402.11398v2.pdf"
    },
    {
        "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness",
        "authors": [
            "Jiayi Liu",
            "Tinghan Yang",
            "Jennifer Neville"
        ],
        "published": "2024-02-17T22:37:17Z",
        "summary": "Large language models (LLMs) have become pivotal in recent research. However,\nduring the inference process, LLMs still require substantial resources. In this\npaper, we propose CliqueParcel, a method designed to improve the efficiency of\nLLMs via prompt batching. Existing strategies to optimize inference efficiency\noften compromise on output quality, leading to a discounted output problem.\nThis issue might result in reduced accuracy or outputs that are less detailed.\nCliqueParcel is our answer to this challenge. While ensuring accuracy and\nminimizing deviations from the original outputs (i.e., faithfulness), our\nmethod significantly improves efficiency during inference.\n  To lay the groundwork, we first redefine efficiency measurements by excluding\nthe reduction in running time due to shorter lengths. Then, we provide a\ncomprehensive trade-off between efficiency and faithfulness to clarify the\nnature of the 'discounted output' problem. Within the CliqueParcel framework,\nwe suggest multiple batching sub-methods and discuss the specific scenarios in\nwhich they can be applied. During evaluation, CliqueParcel is tested on eight\nwidely recognized datasets, which can be classified into three types: reading\ncomprehension, open-source question-answering, and reasoning. Our experiments\nexplore the performance of CliqueParcel, including efficiency, faithfulness,\nand the trade-off between them. This work provides novel insights into\ninference efficiency and demonstrates promising performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.14833v1.pdf"
    },
    {
        "title": "Training Language Model Agents without Modifying Language Models",
        "authors": [
            "Shaokun Zhang",
            "Jieyu Zhang",
            "Jiale Liu",
            "Linxin Song",
            "Chi Wang",
            "Ranjay Krishna",
            "Qingyun Wu"
        ],
        "published": "2024-02-17T18:31:21Z",
        "summary": "Researchers and practitioners have recently reframed powerful Large Language\nModels (LLMs) as agents, enabling them to automate complex tasks largely via\nthe use of specialized functions. To facilitate the development of LLM agents,\nwe present a novel paradigm of training LLM agents without modifying the LLM\nweights, which is particularly useful when the LLMs are difficult or\ninaccessible for modifications. Inspired by how humans continuously forge tools\nto adapt to real-world tasks, rather than change our biological structure to\nfit a static set of tools, we propose to progressively forge agent's functions\nto better solve the downstream tasks instead of modifying the LLM weights. By\ntreating the functions as learnable `agent parameters' and leveraging the\nfundamental idea of model training in artificial intelligence, we develop\nAgentOptimizer that employs the LLM to update agents' functions and devise an\nagent training algorithm with two strategies, roll-back, and early-stop, to\nstreamline the training process. With extensive experiments, we showcase that\nthe agent training paradigm could significantly improve the performance of\nrepresentative LLM agents in various downstream tasks. We also study the\nbehavior of the agent training regarding aspects like the learning curve and\ndomain transferability.",
        "pdf_link": "https://arxiv.org/pdf/2402.11359v1.pdf"
    },
    {
        "title": "Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention",
        "authors": [
            "Eunkyung Jo",
            "Yuin Jeong",
            "SoHyun Park",
            "Daniel A. Epstein",
            "Young-Ho Kim"
        ],
        "published": "2024-02-17T18:05:53Z",
        "summary": "Recent large language models (LLMs) offer the potential to support public\nhealth monitoring by facilitating health disclosure through open-ended\nconversations but rarely preserve the knowledge gained about individuals across\nrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents an\nopportunity to improve engagement and self-disclosure, but we lack an\nunderstanding of how LTM impacts people's interaction with LLM-driven chatbots\nin public health interventions. We examine the case of CareCall -- an\nLLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs\nand interviews with nine users. We found that LTM enhanced health disclosure\nand fostered positive perceptions of the chatbot by offering familiarity.\nHowever, we also observed challenges in promoting self-disclosure through LTM,\nparticularly around addressing chronic health conditions and privacy concerns.\nWe discuss considerations for LTM integration in LLM-driven chatbots for public\nhealth monitoring, including carefully deciding what topics need to be\nremembered in light of public health goals.",
        "pdf_link": "https://arxiv.org/pdf/2402.11353v1.pdf"
    },
    {
        "title": "Tasks That Language Models Don't Learn",
        "authors": [
            "Bruce W. Lee",
            "JaeHyuk Lim"
        ],
        "published": "2024-02-17T17:52:24Z",
        "summary": "We argue that there are certain properties of language that our current large\nlanguage models (LLMs) don't learn. We present an empirical investigation of\nvisual-auditory properties of language through a series of tasks, termed\nH-TEST. This benchmark highlights a fundamental gap between human linguistic\ncomprehension, which naturally integrates sensory experiences, and the\nsensory-deprived processing capabilities of LLMs. In support of our hypothesis,\n1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3.\nstronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not\ntrivially bring improvements in H-TEST performance. Therefore, we make a\nparticular connection to the philosophical case of Mary, who learns about the\nworld in a sensory-deprived environment (Jackson, 1986). Our experiments show\nthat some of the strongest proprietary LLMs stay near random chance baseline\naccuracy of 50%, highlighting the limitations of knowledge acquired in the\nabsence of sensory experience.",
        "pdf_link": "https://arxiv.org/pdf/2402.11349v1.pdf"
    },
    {
        "title": "PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models",
        "authors": [
            "Wendi Cui",
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Hao Sun",
            "Damien Lopez",
            "Kamalika Das",
            "Bradley Malin",
            "Sricharan Kumar"
        ],
        "published": "2024-02-17T17:47:10Z",
        "summary": "Crafting an ideal prompt for Large Language Models (LLMs) is a challenging\ntask that demands significant resources and expert human input. Existing work\ntreats the optimization of prompt instruction and in-context learning examples\nas distinct problems, leading to sub-optimal prompt performance. This research\naddresses this limitation by establishing a unified in-context prompt\noptimization framework, which aims to achieve joint optimization of the prompt\ninstruction and examples. However, formulating such optimization in the\ndiscrete and high-dimensional natural language space introduces challenges in\nterms of convergence and computational efficiency. To overcome these issues, we\npresent PhaseEvo, an efficient automatic prompt optimization framework that\ncombines the generative capability of LLMs with the global search proficiency\nof evolution algorithms. Our framework features a multi-phase design\nincorporating innovative LLM-based mutation operators to enhance search\nefficiency and accelerate convergence. We conduct an extensive evaluation of\nour approach across 35 benchmark tasks. The results demonstrate that PhaseEvo\nsignificantly outperforms the state-of-the-art baseline methods by a large\nmargin whilst maintaining good efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.11347v1.pdf"
    },
    {
        "title": "EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries",
        "authors": [
            "Jiateng Liu",
            "Pengfei Yu",
            "Yuji Zhang",
            "Sha Li",
            "Zixuan Zhang",
            "Heng Ji"
        ],
        "published": "2024-02-17T16:34:50Z",
        "summary": "The dynamic nature of real-world information necessitates efficient knowledge\nediting (KE) in large language models (LLMs) for knowledge updating. However,\ncurrent KE approaches, which typically operate on (subject, relation, object)\ntriples, ignore the contextual information and the relation among different\nknowledge. Such editing methods could thus encounter an uncertain editing\nboundary, leaving a lot of relevant knowledge in ambiguity: Queries that could\nbe answered pre-edit cannot be reliably answered afterward. In this work, we\nanalyze this issue by introducing a theoretical framework for KE that\nhighlights an overlooked set of knowledge that remains unchanged and aids in\nknowledge deduction during editing, which we name as the deduction anchor. We\nfurther address this issue by proposing a novel task of event-based knowledge\nediting that pairs facts with event descriptions. This task manifests not only\na closer simulation of real-world editing scenarios but also a more logically\nsound setting, implicitly defining the deduction anchor to address the issue of\nindeterminate editing boundaries. We empirically demonstrate the superiority of\nevent-based editing over the existing setting on resolving uncertainty in\nedited models, and curate a new benchmark dataset EvEdit derived from the\nCounterFact dataset. Moreover, while we observe that the event-based setting is\nsignificantly challenging for existing approaches, we propose a novel approach\nSelf-Edit that showcases stronger performance, achieving 55.6% consistency\nimprovement while maintaining the naturalness of generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.11324v1.pdf"
    },
    {
        "title": "Dissecting Human and LLM Preferences",
        "authors": [
            "Junlong Li",
            "Fan Zhou",
            "Shichao Sun",
            "Yikai Zhang",
            "Hai Zhao",
            "Pengfei Liu"
        ],
        "published": "2024-02-17T14:34:31Z",
        "summary": "As a relative quality comparison of model responses, human and Large Language\nModel (LLM) preferences serve as common alignment goals in model fine-tuning\nand criteria in evaluation. Yet, these preferences merely reflect broad\ntendencies, resulting in less explainable and controllable models with\npotential safety risks. In this work, we dissect the preferences of human and\n32 different LLMs to understand their quantitative composition, using\nannotations from real-world user-model conversations for a fine-grained,\nscenario-wise analysis. We find that humans are less sensitive to errors, favor\nresponses that support their stances, and show clear dislike when models admit\ntheir limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize\ncorrectness, clarity, and harmlessness more. Additionally, LLMs of similar\nsizes tend to exhibit similar preferences, regardless of their training\nmethods, and fine-tuning for alignment does not significantly alter the\npreferences of pretrained-only LLMs. Finally, we show that preference-based\nevaluation can be intentionally manipulated. In both training-free and\ntraining-based settings, aligning a model with the preferences of judges boosts\nscores, while injecting the least preferred properties lowers them. This\nresults in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94\non AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this\nstrategic adaptation. Interactive Demo:\nhttps://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:\nhttps://huggingface.co/datasets/GAIR/preference-dissection Code:\nhttps://github.com/GAIR-NLP/Preference-Dissection",
        "pdf_link": "https://arxiv.org/pdf/2402.11296v1.pdf"
    },
    {
        "title": "OneBit: Towards Extremely Low-bit Large Language Models",
        "authors": [
            "Yuzhuang Xu",
            "Xu Han",
            "Zonghan Yang",
            "Shuo Wang",
            "Qingfu Zhu",
            "Zhiyuan Liu",
            "Weidong Liu",
            "Wanxiang Che"
        ],
        "published": "2024-02-17T14:26:57Z",
        "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of models, which is a promising approach to reduce both storage and\ncomputational overheads of deploying highly anticipated LLMs. However, existing\nquantization methods suffer severe performance degradation when the bit-width\nis extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to\nquantize models. This paper boldly quantizes the weight matrices of LLMs to\n1-bit, paving the way for the extremely low bit-width deployment of LLMs. For\nthis target, we introduce a 1-bit quantization-aware training (QAT) framework\nnamed OneBit, including a novel 1-bit parameter representation method to better\nquantize LLMs as well as an effective parameter initialization method based on\nmatrix decomposition to improve the convergence speed of the QAT framework.\nSufficient experimental results indicate that OneBit achieves good performance\n(at least 83% of the non-quantized performance) with robust training processes\nwhen only using 1-bit weight matrices.",
        "pdf_link": "https://arxiv.org/pdf/2402.11295v1.pdf"
    },
    {
        "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
        "authors": [
            "Panagiotis Giadikiaroglou",
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Giorgos Stamou"
        ],
        "published": "2024-02-17T14:19:38Z",
        "summary": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in artificial\nintelligence, marking a significant step towards understanding their\napplicability in complex reasoning tasks. This survey leverages a unique\ntaxonomy -- dividing puzzles into rule-based and rule-less categories -- to\ncritically assess LLMs through various methodologies, including prompting\ntechniques, neuro-symbolic approaches, and fine-tuning. Through a critical\nreview of relevant datasets and benchmarks, we assess LLMs' performance,\nidentifying significant challenges in complex puzzle scenarios. Our findings\nhighlight the disparity between LLM capabilities and human-like reasoning,\nparticularly in those requiring advanced logical inference. The survey\nunderscores the necessity for novel strategies and richer datasets to advance\nLLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and\ncreative problem-solving advancements.",
        "pdf_link": "https://arxiv.org/pdf/2402.11291v1.pdf"
    },
    {
        "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
        "authors": [
            "Yixin Yang",
            "Zheng Li",
            "Qingxiu Dong",
            "Heming Xia",
            "Zhifang Sui"
        ],
        "published": "2024-02-17T13:41:44Z",
        "summary": "Understanding the deep semantics of images is essential in the era dominated\nby social media. However, current research works primarily on the superficial\ndescription of images, revealing a notable deficiency in the systematic\ninvestigation of the inherent deep semantics. In this work, we introduce\nDEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs)\ncapacities of visual deep semantics. DEEPEVAL includes human-annotated dataset\nand three progressive subtasks: fine-grained description selection, in-depth\ntitle matching, and deep semantics understanding. Utilizing DEEPEVAL, we\nevaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a\nsubstantial gap between the deep semantic comprehension capabilities of\nexisting LMMs and humans. For example, GPT-4V is 30% behind humans in\nunderstanding deep semantics, even though it achieves human-comparable\nperformance in image description. Further analysis indicates that the\nintegration of description texts during the inference process notably enhances\nLMMs' ability to perceive deep semantics. Furthermore, our dataset is divided\ninto multiple categories, and we conducted a more detailed analysis within\nthese categories.",
        "pdf_link": "https://arxiv.org/pdf/2402.11281v1.pdf"
    },
    {
        "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
        "authors": [
            "Pei Wang",
            "Yejie Wang",
            "Muxi Diao",
            "Keqing He",
            "Guanting Dong",
            "Weiran Xu"
        ],
        "published": "2024-02-17T13:37:39Z",
        "summary": "In the deployment of large language models (LLMs), accurate confidence\nestimation is critical for assessing the credibility of model predictions.\nHowever, existing methods often fail to overcome the issue of overconfidence on\nincorrect answers. In this work, we focus on improving the confidence\nestimation of large language models. Considering the fragility of\nself-awareness in language models, we introduce a Multi-Perspective Consistency\n(MPC) method. We leverage complementary insights from different perspectives\nwithin models (MPC-Internal) and across different models (MPC-Across) to\nmitigate the issue of overconfidence arising from a singular viewpoint. The\nexperimental results on eight publicly available datasets show that our MPC\nachieves state-of-the-art performance. Further analyses indicate that MPC can\nmitigate the problem of overconfidence and is effectively scalable to other\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.11279v1.pdf"
    },
    {
        "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
        "authors": [
            "Shu Yang",
            "Muhammad Asif Ali",
            "Cheng-Long Wang",
            "Lijie Hu",
            "Di Wang"
        ],
        "published": "2024-02-17T12:25:31Z",
        "summary": "Adapting large language models (LLMs) to new domains/tasks and enabling them\nto be efficient lifelong learners is a pivotal challenge. In this paper, we\npropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for\nLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the\nfine-tuning abilities of LoRA for effective life-long learning of LLMs. In\ncontrast to the conventional approaches that use factual triplets as inputs\nMoRAL relies on simple question-answer pairs, which is a more practical and\neffective strategy for robust and efficient learning. Owing to new data\nsettings, we introduce a new evaluation benchmark namely: Life Long Learning of\nLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,\nand a set of evaluation metrics for rigorous evaluation of MoRAL in open-book\nand closed-book settings. Experimental evaluation shows (i) LLMs learn fast in\nopen-book settings with up to 30.15% improvement in \"RA\" for Phi-2-2.7B\ncompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows\nhigher performance improvement for models with a greater number of parameters;\n(iii) MoRAL is robust to catastrophic forgetting offering better knowledge\nretention compared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.11260v1.pdf"
    },
    {
        "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
        "authors": [
            "Ying Mo",
            "Jian Yang",
            "Jiahao Liu",
            "Shun Zhang",
            "Jingang Wang",
            "Zhoujun Li"
        ],
        "published": "2024-02-17T11:28:08Z",
        "summary": "Recently, there has been increasing interest in exploring the capabilities of\nadvanced large language models (LLMs) in the field of information extraction\n(IE), specifically focusing on tasks related to named entity recognition (NER)\nand relation extraction (RE). Although researchers are exploring the use of\nfew-shot information extraction through in-context learning with LLMs, they\ntend to focus only on using correct or positive examples for demonstration,\nneglecting the potential value of incorporating incorrect or negative examples\ninto the learning process. In this paper, we present c-ICL, a novel few-shot\ntechnique that leverages both correct and incorrect sample constructions to\ncreate in-context learning demonstrations. This approach enhances the ability\nof LLMs to extract entities and relations by utilizing prompts that incorporate\nnot only the positive samples but also the reasoning behind them. This method\nallows for the identification and correction of potential interface errors.\nSpecifically, our proposed method taps into the inherent contextual information\nand valuable information in hard negative samples and the nearest positive\nneighbors to the test and then applies the in-context learning demonstrations\nbased on LLMs. Our experiments on various datasets indicate that c-ICL\noutperforms previous few-shot in-context learning methods, delivering\nsubstantial enhancements in performance across a broad spectrum of related\ntasks. These improvements are noteworthy, showcasing the versatility of our\napproach in miscellaneous scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11254v1.pdf"
    },
    {
        "title": "Aligning Large Language Models by On-Policy Self-Judgment",
        "authors": [
            "Sangkyu Lee",
            "Sungdong Kim",
            "Ashkan Yousefpour",
            "Minjoon Seo",
            "Kang Min Yoo",
            "Youngjae Yu"
        ],
        "published": "2024-02-17T11:25:26Z",
        "summary": "Existing approaches for aligning large language models with human preferences\nface a trade-off that requires a separate reward model (RM) for on-policy\nlearning. In this paper, we present a novel alignment framework, \\method{} that\n(1) does on-policy learning and 2) is parameter efficient, as it does not\nrequire an additional RM for evaluating the samples for on-policy learning. To\nthis end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a\nsingle model to act as both a policy and a judge. Specifically, we view the\npairwise judgment task, choosing the better response from a response pair, as a\nspecial case of the instruction-following task. The resulting model can judge\npreferences of on-the-fly responses from current policy initialized from\nitself. Experimental results show the efficacy of \\method{}, outperforming\nbaselines in preference benchmarks. We also show that the rejecting sampling by\nitself can improve performance further without an additional evaluator.",
        "pdf_link": "https://arxiv.org/pdf/2402.11253v2.pdf"
    },
    {
        "title": "LLM can Achieve Self-Regulation via Hyperparameter Aware Generation",
        "authors": [
            "Siyin Wang",
            "Shimin Li",
            "Tianxiang Sun",
            "Jinlan Fu",
            "Qinyuan Cheng",
            "Jiasheng Ye",
            "Junjie Ye",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2024-02-17T11:18:22Z",
        "summary": "In the realm of Large Language Models (LLMs), users commonly employ diverse\ndecoding strategies and adjust hyperparameters to control the generated text.\nHowever, a critical question emerges: Are LLMs conscious of the existence of\nthese decoding strategies and capable of regulating themselves? The current\ndecoding generation process often relies on empirical and heuristic manual\nadjustments to hyperparameters based on types of tasks and demands. However,\nthis process is typically cumbersome, and the decoding hyperparameters may not\nalways be optimal for each sample. To address the aforementioned challenges, we\npropose a novel text generation paradigm termed Hyperparameter Aware Generation\n(HAG). By leveraging hyperparameter-aware instruction tuning, the LLM\nautonomously determines the optimal decoding strategy and configs based on the\ninput samples, enabling self-regulation. Our approach eliminates the need for\nextensive manual tuning, offering a more autonomous, self-regulate model\nbehavior. Experimental results spanning six datasets across reasoning,\ncreativity, translation, and mathematics tasks demonstrate that\nhyperparameter-aware instruction tuning empowers the LLMs to self-regulate the\ndecoding strategy and hyperparameter. HAG extends the current paradigm in the\ntext generation process, highlighting the feasibility of endowing the LLMs with\nself-regulate decoding strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11251v1.pdf"
    },
    {
        "title": "Can Large Language Models perform Relation-based Argument Mining?",
        "authors": [
            "Deniz Gorur",
            "Antonio Rago",
            "Francesca Toni"
        ],
        "published": "2024-02-17T10:37:51Z",
        "summary": "Argument mining (AM) is the process of automatically extracting arguments,\ntheir components and/or relations amongst arguments and components from text.\nAs the number of platforms supporting online debate increases, the need for AM\nbecomes ever more urgent, especially in support of downstream tasks.\nRelation-based AM (RbAM) is a form of AM focusing on identifying agreement\n(support) and disagreement (attack) relations amongst arguments. RbAM is a\nchallenging classification task, with existing methods failing to perform\nsatisfactorily. In this paper, we show that general-purpose Large Language\nModels (LLMs), appropriately primed and prompted, can significantly outperform\nthe best performing (RoBERTa-based) baseline. Specifically, we experiment with\ntwo open-source LLMs (Llama-2 and Mistral) with ten datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.11243v1.pdf"
    },
    {
        "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
        "authors": [
            "Xiangyu Zhang",
            "Hexin Liu",
            "Kaishuai Xu",
            "Qiquan Zhang",
            "Daijiao Liu",
            "Beena Ahmed",
            "Julien Epps"
        ],
        "published": "2024-02-17T09:39:46Z",
        "summary": "Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.",
        "pdf_link": "https://arxiv.org/pdf/2402.13276v1.pdf"
    },
    {
        "title": "Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs",
        "authors": [
            "Xun Liang",
            "Hanyu Wang",
            "Shichao Song",
            "Mengting Hu",
            "Xunzhi Wang",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Bo Tang"
        ],
        "published": "2024-02-17T08:14:37Z",
        "summary": "Controlled Text Generation (CTG) aims to produce texts that exhibit specific\ndesired attributes. In this study, we introduce a pluggable CTG framework for\nLarge Language Models (LLMs) named Dynamic Attribute Graphs-based controlled\ntext generation (DATG). This framework utilizes an attribute scorer to evaluate\nthe attributes of sentences generated by LLMs and constructs dynamic attribute\ngraphs. DATG modulates the occurrence of key attribute words and key\nanti-attribute words, achieving effective attribute control without\ncompromising the original capabilities of the model. We conduct experiments\nacross four datasets in two tasks: toxicity mitigation and sentiment\ntransformation, employing five LLMs as foundational models. Our findings\nhighlight a remarkable enhancement in control accuracy, achieving a peak\nimprovement of 19.29% over baseline methods in the most favorable task across\nfour datasets. Additionally, we observe a significant decrease in perplexity,\nmarkedly improving text fluency.",
        "pdf_link": "https://arxiv.org/pdf/2402.11218v1.pdf"
    },
    {
        "title": "Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models",
        "authors": [
            "Wenxuan Wang",
            "Yihang Su",
            "Jingyuan Huan",
            "Jie Liu",
            "Wenting Chen",
            "Yudi Zhang",
            "Cheng-Yi Li",
            "Kao-Jung Chang",
            "Xiaohan Xin",
            "Linlin Shen",
            "Michael R. Lyu"
        ],
        "published": "2024-02-17T08:04:23Z",
        "summary": "The significant breakthroughs of Medical Multi-Modal Large Language Models\n(Med-MLLMs) renovate modern healthcare with robust information synthesis and\nmedical decision support. However, these models are often evaluated on\nbenchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of\nthe real-world diagnostic frameworks, which encompass diverse medical\nspecialties and involve complex clinical decisions. Moreover, these benchmarks\nare susceptible to data leakage, since Med-MLLMs are trained on large\nassemblies of publicly available data. Thus, an isolated and clinically\nrepresentative benchmark is highly desirable for credible Med-MLLMs evaluation.\nTo this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously\nand comprehensively assesses model capability in terms of: distinct medical\nspecialties (cardiovascular, gastroenterology, etc.) and different diagnostic\ncapacities (perception, disease analysis, etc.). Grounded in 3 proposed core\nprinciples, Asclepius ensures a comprehensive evaluation by encompassing 15\nmedical specialties, stratifying into 3 main categories and 8 sub-categories of\nclinical tasks, and exempting from train-validate contamination. We further\nprovide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human\nspecialists, providing insights into their competencies and limitations in\nvarious medical contexts. Our work not only advances the understanding of\nMed-MLLMs' capabilities but also sets a precedent for future evaluations and\nthe safe deployment of these models in clinical environments. We launch and\nmaintain a leaderboard for community assessment of Med-MLLM capabilities\n(https://asclepius-med.github.io/).",
        "pdf_link": "https://arxiv.org/pdf/2402.11217v1.pdf"
    },
    {
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "authors": [
            "Wenkai Yang",
            "Xiaohan Bi",
            "Yankai Lin",
            "Sishuo Chen",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2024-02-17T06:48:45Z",
        "summary": "Leveraging the rapid development of Large Language Models LLMs, LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis on the different forms of\nagent backdoor attacks. Specifically, from the perspective of the final\nattacking outcomes, the attacker can either choose to manipulate the final\noutput distribution, or only introduce malicious behavior in the intermediate\nreasoning process, while keeping the final output correct. Furthermore, the\nformer category can be divided into two subcategories based on trigger\nlocations: the backdoor trigger can be hidden either in the user query or in an\nintermediate observation returned by the external environment. We propose the\ncorresponding data poisoning mechanisms to implement the above variations of\nagent backdoor attacks on two typical agent tasks, web shopping and tool\nutilization. Extensive experiments show that LLM-based agents suffer severely\nfrom backdoor attacks, indicating an urgent need for further research on the\ndevelopment of defenses against backdoor attacks on LLM-based agents. Warning:\nThis paper may contain biased content.",
        "pdf_link": "https://arxiv.org/pdf/2402.11208v1.pdf"
    },
    {
        "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs",
        "authors": [
            "Minh-Vuong Nguyen",
            "Linhao Luo",
            "Fatemeh Shiri",
            "Dinh Phung",
            "Yuan-Fang Li",
            "Thuy-Trang Vu",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-17T05:22:56Z",
        "summary": "Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.11199v1.pdf"
    },
    {
        "title": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering",
        "authors": [
            "Pragya Srivastava",
            "Manuj Malik",
            "Vivek Gupta",
            "Tanuja Ganu",
            "Dan Roth"
        ],
        "published": "2024-02-17T05:10:18Z",
        "summary": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task.",
        "pdf_link": "https://arxiv.org/pdf/2402.11194v2.pdf"
    },
    {
        "title": "I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments",
        "authors": [
            "Xuan Ren",
            "Biao Wu",
            "Lingqiao Liu"
        ],
        "published": "2024-02-17T05:05:31Z",
        "summary": "Fine-tuning large language models (LLMs) with a small data set for particular\ntasks is a widely encountered yet complex challenge. The potential for\noverfitting on a limited number of examples can negatively impact the model's\nability to generalize and retain its original skills. Our research explores the\nimpact of the style of ground-truth responses during the fine-tuning process.\nWe found that matching the ground-truth response style with the LLM's inherent\nstyle results in better learning outcomes. Building on this insight, we\ndeveloped a method that minimally alters the LLM's pre-existing responses to\ncorrect errors, using these adjusted responses as training targets. This\ntechnique enables precise corrections in line with the model's native response\nstyle, safeguarding the model's core capabilities and thus avoid overfitting.\nOur findings show that this approach not only improves the LLM's task-specific\naccuracy but also crucially maintains its original competencies and\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11192v1.pdf"
    },
    {
        "title": "Disclosure and Mitigation of Gender Bias in LLMs",
        "authors": [
            "Xiangjue Dong",
            "Yibo Wang",
            "Philip S. Yu",
            "James Caverlee"
        ],
        "published": "2024-02-17T04:48:55Z",
        "summary": "Large Language Models (LLMs) can generate biased responses. Yet previous\ndirect probing techniques contain either gender mentions or predefined gender\nstereotypes, which are challenging to comprehensively collect. Hence, we\npropose an indirect probing framework based on conditional generation. This\napproach aims to induce LLMs to disclose their gender bias even without\nexplicit gender or stereotype mentions. We explore three distinct strategies to\ndisclose explicit and implicit gender bias in LLMs. Our experiments demonstrate\nthat all tested LLMs exhibit explicit and/or implicit gender bias, even when\ngender stereotypes are not present in the inputs. In addition, an increased\nmodel size or model alignment amplifies bias in most cases. Furthermore, we\ninvestigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,\nInstruction Guiding, and Debias Tuning. Remarkably, these methods prove\neffective even in the absence of explicit genders or stereotypes.",
        "pdf_link": "https://arxiv.org/pdf/2402.11190v1.pdf"
    },
    {
        "title": "LaCo: Large Language Model Pruning via Layer Collapse",
        "authors": [
            "Yifei Yang",
            "Zouying Cao",
            "Hai Zhao"
        ],
        "published": "2024-02-17T04:16:30Z",
        "summary": "Large language models (LLMs) based on transformer are witnessing a notable\ntrend of size expansion, which brings considerable costs to both model training\nand inference. However, existing methods such as model quantization, knowledge\ndistillation, and model pruning are constrained by various issues, including\nhardware support limitations, the need for extensive training, and alterations\nto the internal structure of the model. In this paper, we propose a concise\nlayer-wise pruning method called \\textit{Layer Collapse (LaCo)}, in which rear\nmodel layers collapse into a prior layer, enabling a rapid reduction in model\nsize while preserving the model structure. Comprehensive experiments show that\nour method maintains an average task performance of over 80\\% at pruning ratios\nof 25-30\\%, significantly outperforming existing state-of-the-art structured\npruning methods. We also conduct post-training experiments to confirm that the\nproposed pruning method effectively inherits the parameters of the original\nmodel. Finally, we discuss our motivation from the perspective of layer-wise\nsimilarity and evaluate the performance of the pruned LLMs across various\npruning ratios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11187v1.pdf"
    },
    {
        "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
        "authors": [
            "Yougang Lyu",
            "Lingyong Yan",
            "Shuaiqiang Wang",
            "Haibo Shi",
            "Dawei Yin",
            "Pengjie Ren",
            "Zhumin Chen",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "published": "2024-02-17T02:54:32Z",
        "summary": "Despite their success at many natural language processing (NLP) tasks, large\nlanguage models (LLMs) still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nexplicitly and implicitly improve the knowledge awareness of LLMs. We devise an\nexplicit knowledge-aware generation stage to train LLMs to explicitly identify\nknowledge triples in answers. We also propose an implicit knowledge-aware\ncomparison stage to train LLMs to implicitly distinguish between reliable and\nunreliable knowledge, in three aspects: completeness, factuality, and\nlogicality. Extensive experiments on both generic and medical question\nanswering (QA) datasets confirm the effectiveness of KnowTuning, through\nautomatic and human evaluations, across various sizes of LLMs. Finally, we\ndemonstrate that the improvements of KnowTuning generalize to unseen QA\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.11176v1.pdf"
    },
    {
        "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
        "authors": [
            "Yuxia Wang",
            "Jonibek Mansurov",
            "Petar Ivanov",
            "Jinyan Su",
            "Artem Shelmanov",
            "Akim Tsvigun",
            "Osama Mohanned Afzal",
            "Tarek Mahmoud",
            "Giovanni Puccetti",
            "Thomas Arnold",
            "Alham Fikri Aji",
            "Nizar Habash",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2024-02-17T02:50:33Z",
        "summary": "The advent of Large Language Models (LLMs) has brought an unprecedented surge\nin machine-generated text (MGT) across diverse channels. This raises legitimate\nconcerns about its potential misuse and societal implications. The need to\nidentify and differentiate such content from genuine human-generated text is\ncritical in combating disinformation, preserving the integrity of education and\nscientific fields, and maintaining trust in communication. In this work, we\naddress this problem by introducing a new benchmark involving multilingual,\nmulti-domain and multi-generator for MGT detection -- M4GT-Bench. It is\ncollected for three task formulations: (1) mono-lingual and multi-lingual\nbinary MGT detection; (2) multi-way detection identifies which particular model\ngenerates the text; and (3) human-machine mixed text detection, where a word\nboundary delimiting MGT from human-written content should be determined. Human\nevaluation for Task 2 shows less than random guess performance, demonstrating\nthe challenges to distinguish unique LLMs. Promising results always occur when\ntraining and test data distribute within the same domain or generators.",
        "pdf_link": "https://arxiv.org/pdf/2402.11175v1.pdf"
    },
    {
        "title": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
        "authors": [
            "Fan Huang",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "published": "2024-02-17T02:25:57Z",
        "summary": "The robustness of AI-content detection models against cultivated attacks\n(e.g., paraphrasing or word switching) remains a significant concern. This\nstudy proposes a novel token-ensemble generation strategy to challenge the\nrobustness of current AI-content detection approaches. We explore the ensemble\nattack strategy by completing the prompt with the next token generated from\nrandom candidate LLMs. We find the token-ensemble approach significantly drops\nthe performance of AI-content detection models (The code and test sets will be\nreleased). Our findings reveal that token-ensemble generation poses a vital\nchallenge to current detection models and underlines the need for advancing\ndetection technologies to counter sophisticated adversarial strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11167v1.pdf"
    },
    {
        "title": "GenDec: A robust generative Question-decomposition method for Multi-hop reasoning",
        "authors": [
            "Jian Wu",
            "Linyi Yang",
            "Yuliang Ji",
            "Wenhao Huang",
            "B\u00f6rje F. Karlsson",
            "Manabu Okumura"
        ],
        "published": "2024-02-17T02:21:44Z",
        "summary": "Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex\nquestions and find multiple relevant supporting facts. However, Existing large\nlanguage models'(LLMs) reasoning ability in multi-hop question answering\nremains exploration, which is inadequate in answering multi-hop questions.\nMoreover, it is unclear whether LLMs follow a desired reasoning chain to reach\nthe right final answer. In this paper, we propose a \\textbf{gen}erative\nquestion \\textbf{dec}omposition method (GenDec) from the perspective of\nexplainable QA by generating independent and complete sub-questions based on\nincorporating additional extracted evidence for enhancing LLMs' reasoning\nability in RAG. To demonstrate the impact, generalization, and robustness of\nGendec, we conduct two experiments, the first is combining GenDec with small QA\nsystems on paragraph retrieval and QA tasks. We secondly examine the reasoning\ncapabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5\ncombined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,\nMuSiQue, and PokeMQA datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.11166v1.pdf"
    },
    {
        "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Yang Song",
            "Chen Zhu",
            "Hengshu Zhu",
            "Ji-Rong Wen"
        ],
        "published": "2024-02-17T02:07:49Z",
        "summary": "In this paper, we aim to improve the reasoning ability of large language\nmodels (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired\nby existing methods that design the interaction strategy between LLMs and KG,\nwe propose an autonomous LLM-based agent framework, called KG-Agent, which\nenables a small LLM to actively make decisions until finishing the reasoning\nprocess over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox,\nKG-based executor, and knowledge memory, and develop an iteration mechanism\nthat autonomously selects the tool then updates the memory for reasoning over\nKG. To guarantee the effectiveness, we leverage program language to formulate\nthe multi-hop reasoning process over the KG, and synthesize a code-based\ninstruction dataset to fine-tune the base LLM. Extensive experiments\ndemonstrate that only using 10K samples for tuning LLaMA-7B can outperform\nstate-of-the-art methods using larger LLMs or more data, on both in-domain and\nout-domain datasets. Our code and data will be publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2402.11163v1.pdf"
    },
    {
        "title": "PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation",
        "authors": [
            "Zongxia Li",
            "Ishani Mondal",
            "Yijun Liang",
            "Huy Nghiem",
            "Jordan Lee Boyd-Graber"
        ],
        "published": "2024-02-17T01:56:19Z",
        "summary": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but for many of the most challenging and interesting QA examples,\ncurrent answer correctness (AC) metrics do not align with human judgments,\nparticularly verbose, free form answers from large language models (LLM). There\nare two challenges: a lack of data and that models are too big. LLM based\nscorers correlate better with humans, but this expensive task has only been\ntested on limited QA datasets. We rectify these issues by providing clear\nguidelines for evaluating machine QA adopted from human QA contests. We also\nintroduce Precise ANswer correctness Determination and Adjudication (PANDA), a\nsmall, efficient, deterministic AC classifier (812 KB) that more accurately\nevaluates answer correctness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11161v1.pdf"
    },
    {
        "title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction",
        "authors": [
            "Sizhe Zhou",
            "Yu Meng",
            "Bowen Jin",
            "Jiawei Han"
        ],
        "published": "2024-02-17T00:20:06Z",
        "summary": "Relation extraction (RE), a crucial task in NLP, aims to identify semantic\nrelationships between entities mentioned in texts. Despite significant\nadvancements in this field, existing models typically rely on extensive\nannotated data for training, which can be both costly and time-consuming to\nacquire. Moreover, these models often struggle to adapt to new or unseen\nrelationships. In contrast, few-shot learning settings, which aim to reduce\nannotation requirements, may offer incomplete and biased supervision for\nunderstanding target relation semantics, leading to degraded and unstable\nperformance. To provide the model with accurate and explicit descriptions of\nthe relations types and meanwhile minimize the annotation requirements, we\nstudy the definition only zero-shot RE setting where only relation definitions\nexpressed in natural language are used to train a RE model. Motivated by the\nstrong synthetic data generation power of LLMs, we propose a framework REPaL\nwhich consists of three stages: (1) We utilize LLMs to generate initial seed\ninstances based on relation definitions and an unlabeled corpora. (2) We\nfine-tune a bidirectional Small Language Model (SLM) using these initial seeds\nto learn the relations for the target domain. (3) We enhance pattern coverage\nand mitigate bias resulting from the limited number of initial seeds by\nincorporating feedback acquired from SLM's predictions on unlabeled corpora. To\naccomplish this, we leverage the multi-turn conversation ability of LLMs to\ngenerate new instances in follow-up dialogues. Experiments on two datasets show\nREPaL achieves better zero-shot performance with large margins over baseline\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.11142v1.pdf"
    },
    {
        "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
        "authors": [
            "Sijia Chen",
            "Baochun Li",
            "Di Niu"
        ],
        "published": "2024-02-17T00:13:36Z",
        "summary": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.",
        "pdf_link": "https://arxiv.org/pdf/2402.11140v1.pdf"
    },
    {
        "title": "Contrastive Instruction Tuning",
        "authors": [
            "Tianyi Yan",
            "Fei Wang",
            "James Y. Huang",
            "Wenxuan Zhou",
            "Fan Yin",
            "Aram Galstyan",
            "Wenpeng Yin",
            "Muhao Chen"
        ],
        "published": "2024-02-17T00:09:32Z",
        "summary": "Instruction tuning has been used as a promising approach to improve the\nperformance of large language models (LLMs) on unseen tasks. However, current\nLLMs exhibit limited robustness to unseen instructions, generating inconsistent\noutputs when the same instruction is phrased with slightly varied forms or\nlanguage styles. This behavior indicates LLMs' lack of robustness to textual\nvariations and generalizability to unseen instructions, potentially leading to\ntrustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,\nwhich maximizes the similarity between the hidden representations of\nsemantically equivalent instruction-instance pairs while minimizing the\nsimilarity between semantically different ones. To facilitate this approach, we\naugment the existing FLAN collection by paraphrasing task instructions.\nExperiments on the PromptBench benchmark show that CoIN consistently improves\nLLMs' robustness to unseen instructions with variations across character, word,\nsentence, and semantic levels by an average of +2.5% in accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.11138v1.pdf"
    },
    {
        "title": "TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks",
        "authors": [
            "Benjamin Feuer",
            "Robin Tibor Schirrmeister",
            "Valeriia Cherepanova",
            "Chinmay Hegde",
            "Frank Hutter",
            "Micah Goldblum",
            "Niv Cohen",
            "Colin White"
        ],
        "published": "2024-02-17T00:02:23Z",
        "summary": "While tabular classification has traditionally relied on from-scratch\ntraining, a recent breakthrough called prior-data fitted networks (PFNs)\nchallenges this approach. Similar to large language models, PFNs make use of\npretraining and in-context learning to achieve strong performance on new tasks\nin a single forward pass. However, current PFNs have limitations that prohibit\ntheir widespread adoption. Notably, TabPFN achieves very strong performance on\nsmall tabular datasets but is not designed to make predictions for datasets of\nsize larger than 1000. In this work, we overcome these limitations and\nsubstantially improve the performance of PFNs by developing context\noptimization techniques for PFNs. Specifically, we propose TuneTables, a novel\nprompt-tuning strategy that compresses large datasets into a smaller learned\ncontext. TuneTables scales TabPFN to be competitive with state-of-the-art\ntabular classification methods on larger datasets, while having a substantially\nlower inference time than TabPFN. Furthermore, we show that TuneTables can be\nused as an interpretability tool and can even be used to mitigate biases by\noptimizing a fairness objective.",
        "pdf_link": "https://arxiv.org/pdf/2402.11137v2.pdf"
    },
    {
        "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
        "authors": [
            "Arindam Mitra",
            "Hamed Khanpour",
            "Corby Rosset",
            "Ahmed Awadallah"
        ],
        "published": "2024-02-16T23:44:38Z",
        "summary": "Mathematical word problem-solving has long been recognized as a complex task\nfor small language models (SLMs). A recent study hypothesized that the smallest\nmodel size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34\nbillion parameters. To reach this level of performance with smaller models,\nresearcher often train SLMs to generate Python code or use tools to help avoid\ncalculation errors. Additionally, they employ ensembling, where outputs of up\nto 100 model runs are combined to arrive at a more accurate result. Result\nselection is done using consensus, majority vote or a separate a verifier model\nused in conjunction with the SLM. Ensembling provides a substantial boost in\naccuracy but at a significant cost increase with multiple calls to the model\n(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).\n  In this work, we present Orca-Math, a 7-billion-parameter SLM based on the\nMistral-7B, which achieves 86.81% on GSM8k without the need for multiple model\ncalls or the use of verifiers, code execution or any other external tools. Our\napproach has the following key elements: (1) A high quality synthetic dataset\nof 200K math problems created using a multi-agent setup where agents\ncollaborate to create the data, (2) An iterative learning techniques that\nenables the SLM to practice solving problems, receive feedback on its solutions\nand learn from preference pairs incorporating the SLM solutions and the\nfeedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves\n81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math\nachieves 86.81% pass@1. Orca-Math surpasses the performance of significantly\nlarger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It\nalso significantly outperforms other smaller models while using much smaller\ndata (hundreds of thousands vs. millions of problems).",
        "pdf_link": "https://arxiv.org/pdf/2402.14830v1.pdf"
    },
    {
        "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
        "authors": [
            "Haoyu Wang",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "published": "2024-02-16T23:28:02Z",
        "summary": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits\nin enhancing performance across knowledge-intensive scenarios. However, these\nmethods often face challenges with complex inputs and encounter difficulties\ndue to noisy knowledge retrieval, notably hindering model effectiveness. To\naddress this issue, we introduce BlendFilter, a novel approach that elevates\nretrieval-augmented LLMs by integrating query generation blending with\nknowledge filtering. BlendFilter proposes the blending process through its\nquery generation method, which integrates both external and internal knowledge\naugmentation with the original query, ensuring comprehensive information\ngathering. Additionally, our distinctive knowledge filtering module capitalizes\non the intrinsic capabilities of the LLM, effectively eliminating extraneous\ndata. We conduct extensive experiments on three open-domain question answering\nbenchmarks, and the findings clearly indicate that our innovative BlendFilter\nsurpasses state-of-the-art baselines significantly.",
        "pdf_link": "https://arxiv.org/pdf/2402.11129v1.pdf"
    },
    {
        "title": "Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models",
        "authors": [
            "Smriti Singh",
            "Shuvam Keshari",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published": "2024-02-16T23:18:19Z",
        "summary": "Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2403.14633v2.pdf"
    },
    {
        "title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models",
        "authors": [
            "Zihao Lin",
            "Mohammad Beigi",
            "Hongxuan Li",
            "Yufan Zhou",
            "Yuxiang Zhang",
            "Qifan Wang",
            "Wenpeng Yin",
            "Lifu Huang"
        ],
        "published": "2024-02-16T23:08:55Z",
        "summary": "Memory Editing (ME) has emerged as an efficient method to modify erroneous\nfacts or inject new facts into Large Language Models (LLMs). Two mainstream ME\nmethods exist: parameter-modifying ME and parameter-preserving ME (integrating\nextra modules while preserving original parameters). Regrettably, previous\nstudies on ME evaluation have two critical limitations: (i) evaluating LLMs\nwith single edit only, neglecting the need for continuous editing, and (ii)\nevaluations focusing solely on basic factual triples, overlooking broader LLM\ncapabilities like logical reasoning and reading understanding. This study\naddresses these limitations with contributions threefold: (i) We explore how ME\naffects a wide range of fundamental capabilities of LLMs under sequential\nediting. Experimental results reveal an intriguing phenomenon: Most\nparameter-modifying ME consistently degrade performance across all tasks after\na few sequential edits. In contrast, parameter-preserving ME effectively\nmaintains LLMs' fundamental capabilities but struggles to accurately recall\nedited knowledge presented in a different format. (ii) We extend our evaluation\nto different editing settings, such as layers to edit, model size, instruction\ntuning, etc. Experimental findings indicate several strategies that can\npotentially mitigate the adverse effects of ME. (iii) We further explain why\nparameter-modifying ME damages LLMs from three dimensions: parameter changes\nafter editing, language modeling capability, and the in-context learning\ncapability. Our in-depth study advocates more careful use of ME in real-world\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11122v1.pdf"
    },
    {
        "title": "When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models",
        "authors": [
            "Yinghui Li",
            "Qingyu Zhou",
            "Yuanzhen Luo",
            "Shirong Ma",
            "Yangning Li",
            "Hai-Tao Zheng",
            "Xuming Hu",
            "Philip S. Yu"
        ],
        "published": "2024-02-16T22:12:53Z",
        "summary": "Recently, Large Language Models (LLMs) have made remarkable evolutions in\nlanguage understanding and generation. Following this, various benchmarks for\nmeasuring all kinds of capabilities of LLMs have sprung up. In this paper, we\nchallenge the reasoning and understanding abilities of LLMs by proposing a\nFaLlacy Understanding Benchmark (FLUB) containing cunning questions that are\neasy for humans to understand but difficult for models to grasp. Specifically,\nthe cunning questions that FLUB focuses on mainly consist of the tricky,\nhumorous, and misleading questions collected from the real internet\nenvironment. And we design three tasks with increasing difficulty in the FLUB\nbenchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB,\nwe investigate the performance of multiple representative and advanced LLMs,\nreflecting our FLUB is challenging and worthy of more future study. Interesting\ndiscoveries and valuable insights are achieved in our extensive experiments and\ndetailed analyses. We hope that our benchmark can encourage the community to\nimprove LLMs' ability to understand fallacies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11100v1.pdf"
    },
    {
        "title": "Word Embeddings Revisited: Do LLMs Offer Something New?",
        "authors": [
            "Matthew Freestone",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2024-02-16T21:47:30Z",
        "summary": "Learning meaningful word embeddings is key to training a robust language\nmodel. The recent rise of Large Language Models (LLMs) has provided us with\nmany new word/sentence/document embedding models. Although LLMs have shown\nremarkable advancement in various NLP tasks, it is still unclear whether the\nperformance improvement is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper\nsystematically investigates this issue by comparing classical word embedding\ntechniques against LLM-based word embeddings in terms of their latent vector\nsemantics. Our results show that LLMs tend to cluster semantically related\nwords more tightly than classical models. LLMs also yield higher average\naccuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally,\nsome LLMs tend to produce word embeddings similar to SBERT, a relatively\nlighter classical model.",
        "pdf_link": "https://arxiv.org/pdf/2402.11094v2.pdf"
    },
    {
        "title": "VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models",
        "authors": [
            "Ziyi Yin",
            "Muchao Ye",
            "Tianrong Zhang",
            "Jiaqi Wang",
            "Han Liu",
            "Jinghui Chen",
            "Ting Wang",
            "Fenglong Ma"
        ],
        "published": "2024-02-16T21:17:42Z",
        "summary": "Visual Question Answering (VQA) is a fundamental task in computer vision and\nnatural language process fields. Although the ``pre-training & finetuning''\nlearning paradigm significantly improves the VQA performance, the adversarial\nrobustness of such a learning paradigm has not been explored. In this paper, we\ndelve into a new problem: using a pre-trained multimodal source model to create\nadversarial image-text pairs and then transferring them to attack the target\nVQA models. Correspondingly, we propose a novel VQAttack model, which can\niteratively generate both image and text perturbations with the designed\nmodules: the large language model (LLM)-enhanced image attack and the\ncross-modal joint attack module. At each iteration, the LLM-enhanced image\nattack module first optimizes the latent representation-based loss to generate\nfeature-level image perturbations. Then it incorporates an LLM to further\nenhance the image perturbations by optimizing the designed masked answer\nanti-recovery loss. The cross-modal joint attack module will be triggered at a\nspecific iteration, which updates the image and text perturbations\nsequentially. Notably, the text perturbation updates are based on both the\nlearned gradients in the word embedding space and word synonym-based\nsubstitution. Experimental results on two VQA datasets with five validated\nmodels demonstrate the effectiveness of the proposed VQAttack in the\ntransferable attack setting, compared with state-of-the-art baselines. This\nwork reveals a significant blind spot in the ``pre-training & fine-tuning''\nparadigm on VQA tasks. Source codes will be released.",
        "pdf_link": "https://arxiv.org/pdf/2402.11083v1.pdf"
    },
    {
        "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
        "authors": [
            "Jingwei Ni",
            "Minjing Shi",
            "Dominik Stammbach",
            "Mrinmaya Sachan",
            "Elliott Ash",
            "Markus Leippold"
        ],
        "published": "2024-02-16T20:59:57Z",
        "summary": "With the rise of generative AI, automated fact-checking methods to combat\nmisinformation are becoming more and more important. However, factual claim\ndetection, the first step in a fact-checking pipeline, suffers from two key\nissues that limit its scalability and generalizability: (1) inconsistency in\ndefinitions of the task and what a claim is, and (2) the high cost of manual\nannotation. To address (1), we review the definitions in related work and\npropose a unifying definition of factual claims that focuses on verifiability.\nTo address (2), we introduce AFaCTA (Automatic Factual Claim deTection\nAnnotator), a novel framework that assists in the annotation of factual claims\nwith the help of large language models (LLMs). AFaCTA calibrates its annotation\nconfidence with consistency along three predefined reasoning paths. Extensive\nevaluation and experiments in the domain of political speech reveal that AFaCTA\ncan efficiently assist experts in annotating factual claims and training\nhigh-quality classifiers, and can work with or without expert supervision. Our\nanalyses also result in PoliClaim, a comprehensive claim detection dataset\nspanning diverse political topics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11073v1.pdf"
    },
    {
        "title": "Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions",
        "authors": [
            "Guangya Wan",
            "Yuqi Wu",
            "Mengxuan Hu",
            "Zhixuan Chu",
            "Sheng Li"
        ],
        "published": "2024-02-16T20:48:53Z",
        "summary": "Causal discovery (CD) and Large Language Models (LLMs) represent two emerging\nfields of study with significant implications for artificial intelligence.\nDespite their distinct origins, CD focuses on uncovering cause-effect\nrelationships from data, and LLMs on processing and generating humanlike text,\nthe convergence of these domains offers novel insights and methodologies for\nunderstanding complex systems. This paper presents a comprehensive survey of\nthe integration of LLMs, such as GPT4, into CD tasks. We systematically review\nand compare existing approaches that leverage LLMs for various CD tasks and\nhighlight their innovative use of metadata and natural language to infer causal\nstructures. Our analysis reveals the strengths and potential of LLMs in both\nenhancing traditional CD methods and as an imperfect expert, alongside the\nchallenges and limitations inherent in current practices. Furthermore, we\nidentify gaps in the literature and propose future research directions aimed at\nharnessing the full potential of LLMs in causality research. To our knowledge,\nthis is the first survey to offer a unified and detailed examination of the\nsynergy between LLMs and CD, setting the stage for future advancements in the\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2402.11068v1.pdf"
    },
    {
        "title": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
        "authors": [
            "Chenkai Sun",
            "Ke Yang",
            "Revanth Gangi Reddy",
            "Yi R. Fung",
            "Hou Pong Chan",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "published": "2024-02-16T20:20:43Z",
        "summary": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for the development of methodologies capable of accurately\nand efficiently identifying user opinions and preferences. Retrieval\naugmentation emerges as an effective strategy, as it can accommodate a vast\nnumber of users without the costs from fine-tuning. Existing research, however,\nhas largely focused on enhancing the retrieval stage and devoted limited\nexploration toward optimizing the representation of the database, a crucial\naspect for tasks such as personalization. In this work, we examine the problem\nfrom a novel angle, focusing on how data can be better represented for more\nefficient retrieval in the context of LLM customization. To tackle this\nchallenge, we introduce Persona-DB, a simple yet effective framework consisting\nof a hierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the task of response forecasting, Persona-DB demonstrates\nsuperior efficiency in maintaining accuracy with a significantly reduced\nretrieval size, a critical advantage in scenarios with extensive histories or\nlimited context windows. Our experiments also indicate a marked improvement of\nover 15% under cold-start scenarios, when users have extremely sparse data.\nFurthermore, our analysis reveals the increasing importance of collaborative\nknowledge as the retrieval capacity expands.",
        "pdf_link": "https://arxiv.org/pdf/2402.11060v1.pdf"
    },
    {
        "title": "Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives",
        "authors": [
            "Runcong Zhao",
            "Qinglin Zhu",
            "Hainiu Xu",
            "Jiazheng Li",
            "Yuxiang Zhou",
            "Yulan He",
            "Lin Gui"
        ],
        "published": "2024-02-16T19:59:45Z",
        "summary": "Existing datasets for narrative understanding often fail to represent the\ncomplexity and uncertainty of relationships in real-life social scenarios. To\naddress this gap, we introduce a new benchmark, Conan, designed for extracting\nand analysing intricate character relation graphs from detective narratives.\nSpecifically, we designed hierarchical relationship categories and manually\nextracted and annotated role-oriented relationships from the perspectives of\nvarious characters, incorporating both public relationships known to most\ncharacters and secret ones known to only a few. Our experiments with advanced\nLarge Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their\nlimitations in inferencing complex relationships and handling longer\nnarratives. The combination of the Conan dataset and our pipeline strategy is\ngeared towards understanding the ability of LLMs to comprehend nuanced\nrelational dynamics in narrative contexts.",
        "pdf_link": "https://arxiv.org/pdf/2402.11051v1.pdf"
    },
    {
        "title": "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?",
        "authors": [
            "Benjamin Reichman",
            "Larry Heck"
        ],
        "published": "2024-02-16T19:28:52Z",
        "summary": "Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.",
        "pdf_link": "https://arxiv.org/pdf/2402.11035v1.pdf"
    },
    {
        "title": "PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering",
        "authors": [
            "Jannat Ara Meem",
            "Muhammad Shihab Rashid",
            "Yue Dong",
            "Vagelis Hristidis"
        ],
        "published": "2024-02-16T19:26:09Z",
        "summary": "Existing work on Temporal Question Answering (TQA) has predominantly focused\non questions anchored to specific timestamps or events (e.g. \"Who was the US\npresident in 1970?\"). Little work has studied questions whose temporal context\nis relative to the present time (e.g. \"Who was the previous US president?\"). We\nrefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses\nunique challenges: (1) large language models (LLMs) may have outdated\nknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are\nhard to reason, (3) multi-hop reasoning may be required, and (4) the gold\nanswers of benchmarks must be continuously updated. To address these\nchallenges, we introduce the PAT-Questions benchmark, which includes single and\nmulti-hop temporal questions. The answers in PAT-Questions can be automatically\nrefreshed by re-running SPARQL queries on a knowledge graph, if available. We\nevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model\n(TEMPREASON-T5) on PAT-Questions through direct prompting and\nretrieval-augmented generation (RAG). The results highlight the limitations of\nexisting solutions in PATQA and motivate the need for new methods to improve\nPATQA reasoning capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.11034v1.pdf"
    },
    {
        "title": "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter",
        "authors": [
            "Junfei Xiao",
            "Zheng Xu",
            "Alan Yuille",
            "Shen Yan",
            "Boyu Wang"
        ],
        "published": "2024-02-16T18:54:47Z",
        "summary": "This paper demonstrates that a progressively aligned language model can\neffectively bridge frozen vision encoders and large language models (LLMs).\nWhile the fundamental architecture and pre-training methods of vision encoders\nand LLMs have been extensively studied, the architecture and training strategy\nof vision-language adapters vary significantly across recent works. Our\nresearch undertakes a thorough exploration of the state-of-the-art perceiver\nresampler architecture and builds a strong baseline. However, we observe that\nthe vision-language alignment with perceiver resampler exhibits slow\nconvergence and limited scalability with a lack of direct supervision. To\naddress this issue, we propose PaLM2-VAdapter, employing a progressively\naligned language model as the vision-language adapter. Compared to the strong\nbaseline with perceiver resampler, our method empirically shows faster\nconvergence, higher performance, and stronger scalability. Extensive\nexperiments across various Visual Question Answering (VQA) and captioning tasks\non both images and videos demonstrate that our model exhibits state-of-the-art\nvisual understanding and multi-modal reasoning capabilities. Notably, our\nmethod achieves these advancements with 30~70% fewer parameters than the\nstate-of-the-art large vision-language models, marking a significant efficiency\nimprovement.",
        "pdf_link": "https://arxiv.org/pdf/2402.10896v1.pdf"
    },
    {
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
        "authors": [
            "Moritz Stephan",
            "Alexander Khazatsky",
            "Eric Mitchell",
            "Annie S Chen",
            "Sheryl Hsu",
            "Archit Sharma",
            "Chelsea Finn"
        ],
        "published": "2024-02-16T18:50:24Z",
        "summary": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
        "pdf_link": "https://arxiv.org/pdf/2402.10893v1.pdf"
    },
    {
        "title": "Proving membership in LLM pretraining data via data watermarks",
        "authors": [
            "Johnny Tian-Zheng Wei",
            "Ryan Yixiang Wang",
            "Robin Jia"
        ],
        "published": "2024-02-16T18:49:27Z",
        "summary": "Detecting whether copyright holders' works were used in LLM pretraining is\npoised to be an important problem. This work proposes using data watermarks to\nenable principled detection with only black-box model access, provided that the\nrightholder contributed multiple training documents and watermarked them before\npublic release. By applying a randomly sampled data watermark, detection can be\nframed as hypothesis testing, which provides guarantees on the false detection\nrate. We study two watermarks: one that inserts random sequences, and another\nthat randomly substitutes characters with Unicode lookalikes. We first show how\nthree aspects of watermark design -- watermark length, number of duplications,\nand interference -- affect the power of the hypothesis test. Next, we study how\na watermark's detection strength changes under model and dataset scaling: while\nincreasing the dataset size decreases the strength of the watermark, watermarks\nremain strong if the model size also increases. Finally, we view SHA hashes as\nnatural watermarks and show that we can robustly detect hashes from\nBLOOM-176B's training data, as long as they occurred at least 90 times.\nTogether, our results point towards a promising future for data watermarks in\nreal world use.",
        "pdf_link": "https://arxiv.org/pdf/2402.10892v1.pdf"
    },
    {
        "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
        "authors": [
            "Dylan Zhang",
            "Justin Wang",
            "Francois Charton"
        ],
        "published": "2024-02-16T18:47:21Z",
        "summary": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
        "pdf_link": "https://arxiv.org/pdf/2402.10891v1.pdf"
    },
    {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": [
            "Ziru Chen",
            "Michael White",
            "Raymond Mooney",
            "Ali Payani",
            "Yu Su",
            "Huan Sun"
        ],
        "published": "2024-02-16T18:45:58Z",
        "summary": "In this paper, we examine how large language models (LLMs) solve multi-step\nproblems under a language agent framework with three components: a generator, a\ndiscriminator, and a planning method. We investigate the practical utility of\ntwo advanced planning methods, iterative correction and tree search. We present\na comprehensive analysis of how discrimination accuracy affects the overall\nperformance of agents when using these two methods or a simpler method,\nre-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical\nreasoning, show that: (1) advanced planning methods demand discriminators with\nat least 90% accuracy to achieve significant improvements over re-ranking; (2)\ncurrent LLMs' discrimination abilities have not met the needs of advanced\nplanning methods to achieve such improvements; (3) with LLM-based\ndiscriminators, advanced planning methods may not adequately balance accuracy\nand efficiency. For example, compared to the other two methods, tree search is\nat least 10--20 times slower but leads to negligible performance gains, which\nhinders its real-world applications. Code and data will be released at\nhttps://github.com/OSU-NLP-Group/llm-planning-eval.",
        "pdf_link": "https://arxiv.org/pdf/2402.10890v1.pdf"
    },
    {
        "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
        "authors": [
            "Shengzhi Li",
            "Rongyu Lin",
            "Shichao Pei"
        ],
        "published": "2024-02-16T18:42:08Z",
        "summary": "In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.10884v1.pdf"
    },
    {
        "title": "Exploring Value Biases: How LLMs Deviate Towards the Ideal",
        "authors": [
            "Sarath Sivaprasad",
            "Pramod Kaushik",
            "Sahar Abdelnabi",
            "Mario Fritz"
        ],
        "published": "2024-02-16T18:28:43Z",
        "summary": "Large-Language-Models (LLMs) are deployed in a wide range of applications,\nand their response has an increasing social impact. Understanding the\nnon-deliberate(ive) mechanism of LLMs in giving responses is essential in\nexplaining their performance and discerning their biases in real-world\napplications. This is analogous to human studies, where such inadvertent\nresponses are referred to as sampling. We study this sampling of LLMs in light\nof value bias and show that the sampling of LLMs tends to favour high-value\noptions. Value bias corresponds to this shift of response from the most likely\ntowards an ideal value represented in the LLM. In fact, this effect can be\nreproduced even with new entities learnt via in-context prompting. We show that\nthis bias manifests in unexpected places and has implications on relevant\napplication scenarios, like choosing exemplars. The results show that value\nbias is strong in LLMs across different categories, similar to the results\nfound in human studies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11005v2.pdf"
    },
    {
        "title": "EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models",
        "authors": [
            "Muhammad Shihab Rashid",
            "Jannat Ara Meem",
            "Yue Dong",
            "Vagelis Hristidis"
        ],
        "published": "2024-02-16T18:03:42Z",
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance in\ntext re-ranking. This process includes queries and candidate passages in the\nprompts, utilizing pointwise, listwise, and pairwise prompting strategies. A\nlimitation of these ranking strategies with LLMs is their cost: the process can\nbecome expensive due to API charges, which are based on the number of input and\noutput tokens. We study how to maximize the re-ranking performance given a\nbudget, by navigating the vast search spaces of prompt choices, LLM APIs, and\nbudget splits. We propose a suite of budget-constrained methods to perform text\nre-ranking using a set of LLM APIs. Our most efficient method, called EcoRank,\nis a two-layered pipeline that jointly optimizes decisions regarding budget\nallocation across prompt strategies and LLM APIs. Our experimental results on\nfour popular QA and passage reranking datasets show that EcoRank outperforms\nother budget-aware supervised and unsupervised baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.10866v1.pdf"
    },
    {
        "title": "Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities",
        "authors": [
            "Mingyu Jin",
            "Hua Tang",
            "Chong Zhang",
            "Qinkai Yu",
            "Chengzhi Liu",
            "Suiyuan Zhu",
            "Yongfeng Zhang",
            "Mengnan Du"
        ],
        "published": "2024-02-16T17:15:28Z",
        "summary": "Large language models (LLMs) have been applied in many fields with rapid\ndevelopment in recent years. As a classic machine learning task, time series\nforecasting has recently received a boost from LLMs. However, there is a\nresearch gap in the LLMs' preferences in this field. In this paper, by\ncomparing LLMs with traditional models, many properties of LLMs in time series\nprediction are found. For example, our study shows that LLMs excel in\npredicting time series with clear patterns and trends but face challenges with\ndatasets lacking periodicity. We explain our findings through designing prompts\nto require LLMs to tell the period of the datasets. In addition, the input\nstrategy is investigated, and it is found that incorporating external knowledge\nand adopting natural language paraphrases positively affects the predictive\nperformance of LLMs for time series. Overall, this study contributes to insight\ninto the advantages and limitations of LLMs in time series forecasting under\ndifferent conditions.",
        "pdf_link": "https://arxiv.org/pdf/2402.10835v2.pdf"
    },
    {
        "title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model",
        "authors": [
            "Jianhao Yuan",
            "Shuyang Sun",
            "Daniel Omeiza",
            "Bo Zhao",
            "Paul Newman",
            "Lars Kunze",
            "Matthew Gadd"
        ],
        "published": "2024-02-16T16:57:18Z",
        "summary": "Robots powered by 'blackbox' models need to provide human-understandable\nexplanations which we can trust. Hence, explainability plays a critical role in\ntrustworthy autonomous decision-making to foster transparency and acceptance\namong end users, especially in complex autonomous driving. Recent advancements\nin Multi-Modal Large Language models (MLLMs) have shown promising potential in\nenhancing the explainability as a driving agent by producing control\npredictions along with natural language explanations. However, severe data\nscarcity due to expensive annotation costs and significant domain gaps between\ndifferent datasets makes the development of a robust and generalisable system\nan extremely challenging task. Moreover, the prohibitively expensive training\nrequirements of MLLM and the unsolved problem of catastrophic forgetting\nfurther limit their generalisability post-deployment. To address these\nchallenges, we present RAG-Driver, a novel retrieval-augmented multi-modal\nlarge language model that leverages in-context learning for high-performance,\nexplainable, and generalisable autonomous driving. By grounding in retrieved\nexpert demonstration, we empirically validate that RAG-Driver achieves\nstate-of-the-art performance in producing driving action explanations,\njustifications, and control signal prediction. More importantly, it exhibits\nexceptional zero-shot generalisation capabilities to unseen environments\nwithout further training endeavours.",
        "pdf_link": "https://arxiv.org/pdf/2402.10828v1.pdf"
    },
    {
        "title": "Quantifying the Persona Effect in LLM Simulations",
        "authors": [
            "Tiancheng Hu",
            "Nigel Collier"
        ],
        "published": "2024-02-16T16:35:35Z",
        "summary": "Large language models (LLMs) have shown remarkable promise in simulating\nhuman language use and behavior. In this study, we delve into the intersection\nof persona variables and the capability of LLMs to simulate different\nperspectives. We find that persona variables can explain <10\\% variance in\nannotations in existing subjective NLP datasets. Nonetheless, incorporating\nthem via prompting in LLMs provides modest improvement. Persona prompting is\nmost effective on data samples where disagreements among annotators are\nfrequent yet confined to a limited range. A linear correlation exists: the more\npersona variables influence human annotations, the better LLMs predictions are\nusing persona prompting. However, when the utility of persona variables is low\n(i.e., explaining <10\\% of human annotations), persona prompting has little\neffect. Most subjective NLP datasets fall into this category, casting doubt on\nsimulating diverse perspectives in the current NLP landscape.",
        "pdf_link": "https://arxiv.org/pdf/2402.10811v1.pdf"
    },
    {
        "title": "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond",
        "authors": [
            "Yongqi Li",
            "Wenjie Wang",
            "Leigang Qu",
            "Liqiang Nie",
            "Wenjie Li",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-16T16:31:46Z",
        "summary": "The recent advancements in generative language models have demonstrated their\nability to memorize knowledge from documents and recall knowledge to respond to\nuser queries effectively. Building upon this capability, we propose to enable\nmultimodal large language models (MLLMs) to memorize and recall images within\ntheir parameters. Given a user query for visual content, the MLLM is\nanticipated to \"recall\" the relevant image from its parameters as the response.\nAchieving this target presents notable challenges, including inbuilt visual\nmemory and visual recall schemes within MLLMs. To address these challenges, we\nintroduce a generative cross-modal retrieval framework, which assigns unique\nidentifier strings to represent images and involves two training steps:\nlearning to memorize and learning to retrieve. The first step focuses on\ntraining the MLLM to memorize the association between images and their\nrespective identifiers. The latter step teaches the MLLM to generate the\ncorresponding identifier of the target image, given the textual query input. By\nmemorizing images in MLLMs, we introduce a new paradigm to cross-modal\nretrieval, distinct from previous discriminative approaches. The experiments\ndemonstrate that the generative paradigm performs effectively and efficiently\neven with large-scale image candidate sets.",
        "pdf_link": "https://arxiv.org/pdf/2402.10805v1.pdf"
    },
    {
        "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
        "authors": [
            "Yuri Kuratov",
            "Aydar Bulatov",
            "Petr Anokhin",
            "Dmitry Sorokin",
            "Artyom Sorokin",
            "Mikhail Burtsev"
        ],
        "published": "2024-02-16T16:15:01Z",
        "summary": "This paper addresses the challenge of processing long documents using\ngenerative transformer models. To evaluate different approaches, we introduce\nBABILong, a new benchmark designed to assess model capabilities in extracting\nand processing distributed facts within extensive texts. Our evaluation, which\nincludes benchmarks for GPT-4 and RAG, reveals that common methods are\neffective only for sequences up to $10^4$ elements. In contrast, fine-tuning\nGPT-2 with recurrent memory augmentations enables it to handle tasks involving\nup to $11\\times 10^6$ elements. This achievement marks a substantial leap, as\nit is by far the longest input processed by any neural network model to date,\ndemonstrating a significant improvement in the processing capabilities for long\nsequences.",
        "pdf_link": "https://arxiv.org/pdf/2402.10790v2.pdf"
    },
    {
        "title": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge",
        "authors": [
            "Xuan Shen",
            "Zhenglun Kong",
            "Changdi Yang",
            "Zhaoyang Han",
            "Lei Lu",
            "Peiyan Dong",
            "Cheng Lyu",
            "Chih-hsiang Li",
            "Xuehang Guo",
            "Zhihao Shu",
            "Wei Niu",
            "Miriam Leeser",
            "Pu Zhao",
            "Yanzhi Wang"
        ],
        "published": "2024-02-16T16:10:38Z",
        "summary": "Despite the remarkable strides of Large Language Models (LLMs) in various\nfields, the wide applications of LLMs on edge devices are limited due to their\nmassive parameters and computations. To address this, quantization is commonly\nadopted to generate lightweight LLMs with efficient computations and fast\ninference. However, Post-Training Quantization (PTQ) methods dramatically\ndegrade in quality when quantizing weights, activations, and KV cache together\nto below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize\nmodel weights, leaving the activations untouched, which do not fully exploit\nthe potential of quantization for inference acceleration on the edge. In this\npaper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the\noptimization of lightweight LLMs to achieve inference acceleration on Edge\ndevices. We first identify that the performance drop of quantization primarily\nstems from the information distortion in quantized attention maps, demonstrated\nby the different distributions in quantized query and key of the self-attention\nmechanism. Then, the entropy and distribution guided QAT is proposed to\nmitigate the information distortion. Moreover, we design a token\nimportance-aware adaptive method to dynamically quantize the tokens with\ndifferent bit widths for further optimization and acceleration. Our extensive\nexperiments verify the substantial improvements with our framework across\nvarious datasets. Furthermore, we achieve an on-device speedup of up to 2.37x\ncompared with its FP16 counterparts across multiple edge devices, signaling a\ngroundbreaking advancement.",
        "pdf_link": "https://arxiv.org/pdf/2402.10787v1.pdf"
    },
    {
        "title": "A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models",
        "authors": [
            "Mingchen Li",
            "Chen Ling",
            "Rui Zhang",
            "Liang Zhao"
        ],
        "published": "2024-02-16T16:02:33Z",
        "summary": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets",
        "pdf_link": "https://arxiv.org/pdf/2402.10779v1.pdf"
    },
    {
        "title": "AutoGPT+P: Affordance-based Task Planning with Large Language Models",
        "authors": [
            "Timo Birr",
            "Christoph Pohl",
            "Abdelrahman Younes",
            "Tamim Asfour"
        ],
        "published": "2024-02-16T16:00:50Z",
        "summary": "Recent advances in task planning leverage Large Language Models (LLMs) to\nimprove generalizability by combining such models with classical planning\nalgorithms to address their inherent limitations in reasoning capabilities.\nHowever, these approaches face the challenge of dynamically capturing the\ninitial state of the task planning problem. To alleviate this issue, we propose\nAutoGPT+P, a system that combines an affordance-based scene representation with\na planning system. Affordances encompass the action possibilities of an agent\non the environment and objects present in it. Thus, deriving the planning\ndomain from an affordance-based scene representation allows symbolic planning\nwith arbitrary objects. AutoGPT+P leverages this representation to derive and\nexecute a plan for a task specified by the user in natural language. In\naddition to solving planning tasks under a closed-world assumption, AutoGPT+P\ncan also handle planning with incomplete information, e. g., tasks with missing\nobjects by exploring the scene, suggesting alternatives, or providing a partial\nplan. The affordance-based scene representation combines object detection with\nan automatically generated object-affordance-mapping using ChatGPT. The core\nplanning tool extends existing work by automatically correcting semantic and\nsyntactic errors. Our approach achieves a success rate of 98%, surpassing the\ncurrent 81% success rate of the current state-of-the-art LLM-based planning\nmethod SayCan on the SayCan instruction set. Furthermore, we evaluated our\napproach on our newly created dataset with 150 scenarios covering a wide range\nof complex tasks with missing objects, achieving a success rate of 79% on our\ndataset. The dataset and the code are publicly available at\nhttps://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.",
        "pdf_link": "https://arxiv.org/pdf/2402.10778v1.pdf"
    },
    {
        "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
        "authors": [
            "Ehsan Doostmohammadi",
            "Oskar Holmstr\u00f6m",
            "Marco Kuhlmann"
        ],
        "published": "2024-02-16T15:48:33Z",
        "summary": "Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we study the reliability of such methods\nacross a broad range of tasks and in a cross-lingual setting. In contrast to\nprevious findings, we observe considerable variability in correlations between\nautomatic methods and human evaluators when scores are differentiated by task\ntype. Specifically, the widely-used ROUGE-L metric strongly correlates with\nhuman judgments for short-answer English tasks but is unreliable in free-form\ngeneration tasks and cross-lingual transfer. The effectiveness of GPT-4 as an\nevaluator depends on including reference answers when prompting for\nassessments, which can lead to overly strict evaluations in free-form\ngeneration tasks. In summary, we find that, while automatic evaluation methods\ncan approximate human judgements under specific conditions, their reliability\nis highly context-dependent. Our findings enhance the understanding of how\nautomatic methods should be applied and interpreted when developing and\nevaluating instruction-tuned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.10770v1.pdf"
    },
    {
        "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
        "authors": [
            "Junjie Ye",
            "Sixian Li",
            "Guanyu Li",
            "Caishuang Huang",
            "Songyang Gao",
            "Yilong Wu",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-16T15:19:46Z",
        "summary": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent $ToolSword$, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input\nstage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and\n$harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.",
        "pdf_link": "https://arxiv.org/pdf/2402.10753v1.pdf"
    },
    {
        "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models",
        "authors": [
            "Pengcheng Jiang",
            "Jiacheng Lin",
            "Zifeng Wang",
            "Jimeng Sun",
            "Jiawei Han"
        ],
        "published": "2024-02-16T15:01:24Z",
        "summary": "The field of relation extraction (RE) is experiencing a notable shift towards\ngenerative relation extraction (GRE), leveraging the capabilities of large\nlanguage models (LLMs). However, we discovered that traditional relation\nextraction (RE) metrics like precision and recall fall short in evaluating GRE\nmethods. This shortfall arises because these metrics rely on exact matching\nwith human-annotated reference relations, while GRE methods often produce\ndiverse and semantically accurate relations that differ from the references. To\nfill this gap, we introduce GenRES for a multi-dimensional assessment in terms\nof the topic similarity, uniqueness, granularity, factualness, and completeness\nof the GRE results. With GenRES, we empirically identified that (1)\nprecision/recall fails to justify the performance of GRE methods; (2)\nhuman-annotated referential relations can be incomplete; (3) prompting LLMs\nwith a fixed set of relations or entities can cause hallucinations. Next, we\nconducted a human evaluation of GRE methods that shows GenRES is consistent\nwith human preferences for RE quality. Last, we made a comprehensive evaluation\nof fourteen leading LLMs using GenRES across document, bag, and sentence level\nRE datasets, respectively, to set the benchmark for future research in GRE",
        "pdf_link": "https://arxiv.org/pdf/2402.10744v1.pdf"
    },
    {
        "title": "Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning",
        "authors": [
            "Yinpeng Liu",
            "Jiawei Liu",
            "Xiang Shi",
            "Qikai Cheng",
            "Wei Lu"
        ],
        "published": "2024-02-16T14:55:33Z",
        "summary": "Demonstration ordering, which is an important strategy for in-context\nlearning (ICL), can significantly affects the performance of large language\nmodels (LLMs). However, most of the current approaches of ordering require\nadditional knowledge and similarity calculation. We advocate the few-shot\nin-context curriculum learning (ICCL), a simple but effective demonstration\nordering method for ICL, which implies gradually increasing the complexity of\nprompt demonstrations during the inference process. Then we design three\nexperiments to discuss the effectiveness of ICCL, the formation mechanism of\nLLM's ICCL capability, and the impact of ordering subjects. Experimental\nresults demonstrate that ICCL, developed during the instruction-tuning stage,\nis effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity\ncompared to humans in discerning the difficulty levels of demonstrations. We\nrelease our code at https://github.com/61peng/curri_learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.10738v1.pdf"
    },
    {
        "title": "Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification",
        "authors": [
            "John Dougrez-Lewis",
            "Mahmud Elahi Akhter",
            "Yulan He",
            "Maria Liakata"
        ],
        "published": "2024-02-16T14:52:05Z",
        "summary": "The reasoning capabilities of LLMs are currently hotly debated. We examine\nthe issue from the perspective of claim/rumour verification. We propose the\nfirst logical reasoning framework designed to break down any claim or rumour\npaired with evidence into the atomic reasoning steps necessary for\nverification. Based on our framework, we curate two annotated collections of\nsuch claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world\nset stemming from rumours circulating on Twitter. We use them to evaluate the\nreasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as\nChatGPT) within the context of our framework, providing a thorough analysis.\nOur results show that ChatGPT struggles in abductive reasoning, although this\ncan be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to\nZero-Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body\nof research suggesting that ChatGPT's reasoning processes are unlikely to\nmirror human-like reasoning, and that LLMs need to be more rigorously evaluated\nto distinguish between hype and actual capabilities, especially in high-stakes\nreal-world tasks such as claim verification.",
        "pdf_link": "https://arxiv.org/pdf/2402.10735v2.pdf"
    },
    {
        "title": "A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles",
        "authors": [
            "Nouar AlDahoul",
            "Talal Rahwan",
            "Yasir Zaki"
        ],
        "published": "2024-02-16T14:44:30Z",
        "summary": "A quarter of US adults regularly get their news from YouTube. Yet, despite\nthe massive political content available on the platform, to date no classifier\nhas been proposed to identify the political leaning of YouTube videos. To fill\nthis gap, we propose a novel classifier based on Bert -- a language model from\nGoogle -- to classify YouTube videos merely based on their titles into six\ncategories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We\nused a public dataset of 10 million YouTube video titles (under various\ncategories) to train and validate the proposed classifier. We compare the\nclassifier against several alternatives that we trained on the same dataset,\nrevealing that our classifier achieves the highest accuracy (75%) and the\nhighest F1 score (77%). To further validate the classification performance, we\ncollect videos from YouTube channels of numerous prominent news agencies, such\nas Fox News and New York Times, which have widely known political leanings, and\napply our classifier to their video titles. For the vast majority of cases, the\npredicted political leaning matches that of the news agency.",
        "pdf_link": "https://arxiv.org/pdf/2404.04261v1.pdf"
    },
    {
        "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference",
        "authors": [
            "Atsuki Yamaguchi",
            "Aline Villavicencio",
            "Nikolaos Aletras"
        ],
        "published": "2024-02-16T14:15:15Z",
        "summary": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\nmethods have been proposed for adapting models to a target language aiming to\nimprove downstream performance. However, the effectiveness of these methods on\nincreasing inference efficiency of generative LLMs has yet to be explored. In\nthis paper, we perform an empirical study of various cross-lingual vocabulary\nadaptation methods on five generative LLMs (including monolingual and\nmultilingual models) across four typologically-diverse languages and four\nnatural language understanding tasks. We find that cross-lingual vocabulary\nadaptation substantially contributes to LLM inference speedups of up to 271.5%.\nWe also show that adapting LLMs that have been pre-trained on more balanced\nmultilingual data results in downstream performance comparable to the original\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.10712v1.pdf"
    },
    {
        "title": "AutoSAT: Automatically Optimize SAT Solvers via Large Language Models",
        "authors": [
            "Yiwen Sun",
            "Xianyin Zhang",
            "Shiyu Huang",
            "Shaowei Cai",
            "Bing-Zhen Zhang",
            "Ke Wei"
        ],
        "published": "2024-02-16T14:04:56Z",
        "summary": "Heuristics are crucial in SAT solvers, while no heuristic rules are suitable\nfor all problem instances. Therefore, it typically requires to refine specific\nsolvers for specific problem instances. In this context, we present AutoSAT, a\nnovel framework for automatically optimizing heuristics in SAT solvers. AutoSAT\nis based on Large Large Models (LLMs) which is able to autonomously generate\ncode, conduct evaluation, then utilize the feedback to further optimize\nheuristics, thereby reducing human intervention and enhancing solver\ncapabilities. AutoSAT operates on a plug-and-play basis, eliminating the need\nfor extensive preliminary setup and model training, and fosters a Chain of\nThought collaborative process with fault-tolerance, ensuring robust heuristic\noptimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL)\nsolver demonstrates the overall superior performance of AutoSAT, especially in\nsolving some specific SAT problem instances.",
        "pdf_link": "https://arxiv.org/pdf/2402.10705v1.pdf"
    },
    {
        "title": "Multi-Cultural Commonsense Knowledge Distillation",
        "authors": [
            "Tuan-Phong Nguyen",
            "Simon Razniewski",
            "Gerhard Weikum"
        ],
        "published": "2024-02-16T13:46:38Z",
        "summary": "Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin. For extrinsic evaluation, we explore augmenting\ndialogue systems with cultural knowledge assertions. We find that adding\nknowledge from MANGO improves the overall quality, specificity, and cultural\nsensitivity of dialogue responses, as judged by human annotators. Data and code\nare available for download.",
        "pdf_link": "https://arxiv.org/pdf/2402.10689v1.pdf"
    },
    {
        "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
        "authors": [
            "Haiyan Zhao",
            "Fan Yang",
            "Himabindu Lakkaraju",
            "Mengnan Du"
        ],
        "published": "2024-02-16T13:46:06Z",
        "summary": "As large language models (LLMs) grow more powerful, concerns around potential\nharms like toxicity, unfairness, and hallucination threaten user trust.\nEnsuring beneficial alignment of LLMs with human values through model alignment\nis thus critical yet challenging, requiring a deeper understanding of LLM\nbehaviors and mechanisms. We propose opening the black box of LLMs through a\nframework of holistic interpretability encompassing complementary bottom-up and\ntop-down perspectives. The bottom-up view, enabled by mechanistic\ninterpretability, focuses on component functionalities and training dynamics.\nThe top-down view utilizes representation engineering to analyze behaviors\nthrough hidden representations. In this paper, we review the landscape around\nmechanistic interpretability and representation engineering, summarizing\napproaches, discussing limitations and applications, and outlining future\nchallenges in using these techniques to achieve ethical, honest, and reliable\nreasoning aligned with human values.",
        "pdf_link": "https://arxiv.org/pdf/2402.10688v1.pdf"
    },
    {
        "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
        "authors": [
            "Yi Lu",
            "Xin Zhou",
            "Wei He",
            "Jun Zhao",
            "Tao Ji",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-16T13:39:34Z",
        "summary": "Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .",
        "pdf_link": "https://arxiv.org/pdf/2402.10685v2.pdf"
    },
    {
        "title": "German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data",
        "authors": [
            "Lars Kl\u00f6ser",
            "Mika Beele",
            "Jan-Niklas Schagen",
            "Bodo Kraft"
        ],
        "published": "2024-02-16T13:28:44Z",
        "summary": "This study pioneers the use of synthetically generated data for training\ngenerative models in document-level text simplification of German texts. We\ndemonstrate the effectiveness of our approach with real-world online texts.\nAddressing the challenge of data scarcity in language simplification, we\ncrawled professionally simplified German texts and synthesized a corpus using\nGPT-4. We finetune Large Language Models with up to 13 billion parameters on\nthis data and evaluate their performance. This paper employs various\nmethodologies for evaluation and demonstrates the limitations of currently used\nrule-based metrics. Both automatic and manual evaluations reveal that our\nmodels can significantly simplify real-world online texts, indicating the\npotential of synthetic data in improving text simplification.",
        "pdf_link": "https://arxiv.org/pdf/2402.10675v1.pdf"
    },
    {
        "title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm",
        "authors": [
            "Yuanzhen Xie",
            "Xinzhou Jin",
            "Tao Xie",
            "MingXiong Lin",
            "Liang Chen",
            "Chenyun Yu",
            "Lei Cheng",
            "ChengXiang Zhuo",
            "Bo Hu",
            "Zang Li"
        ],
        "published": "2024-02-16T13:24:05Z",
        "summary": "In-context learning of large-language models (LLMs) has achieved remarkable\nsuccess in the field of natural language processing, while extensive case\nstudies reveal that the single-step chain-of-thought prompting approach faces\nchallenges such as attention diffusion and inadequate performance in complex\ntasks like text-to-SQL. To improve the contextual learning capabilities of LLMs\nin text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the\nattention and problem-solving scope of LLMs through decomposition.\nSpecifically, the information determination module for eliminating redundant\ninformation and the brand-new prompt structure based on problem classification\ngreatly enhance the model's attention. Additionally, the inclusion of\nself-correcting and active learning modules greatly expands the problem-solving\nscope of LLMs, hence improving the upper limit of LLM-based approaches.\nExtensive experiments conducted on three datasets demonstrate that our approach\noutperforms other methods by a significant margin. About 2-3 percentage point\nimprovements compared to the existing baseline on the Spider Dev and\nSpider-Realistic datasets and new SOTA results on the Spider Test dataset are\nachieved. Our code is available on GitHub:\n\\url{https://github.com/FlyingFeather/DEA-SQL}.",
        "pdf_link": "https://arxiv.org/pdf/2402.10671v1.pdf"
    },
    {
        "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
        "authors": [
            "Yuxuan Kuang",
            "Hai Lin",
            "Meng Jiang"
        ],
        "published": "2024-02-16T13:21:33Z",
        "summary": "Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.10670v2.pdf"
    },
    {
        "title": "Humans or LLMs as the Judge? A Study on Judgement Biases",
        "authors": [
            "Guiming Hardy Chen",
            "Shunian Chen",
            "Ziche Liu",
            "Feng Jiang",
            "Benyou Wang"
        ],
        "published": "2024-02-16T13:21:06Z",
        "summary": "Adopting human and large language models (LLM) as judges (\\textit{a.k.a}\nhuman- and LLM-as-a-judge) for evaluating the performance of existing LLMs has\nrecently gained attention. Nonetheless, this approach concurrently introduces\npotential biases from human and LLM judges, questioning the reliability of the\nevaluation results. In this paper, we propose a novel framework for\ninvestigating 5 types of biases for LLM and human judges. We curate a dataset\nwith 142 samples referring to the revised Bloom's Taxonomy and conduct\nthousands of human and LLM evaluations. Results show that human and LLM judges\nare vulnerable to perturbations to various degrees, and that even the most\ncutting-edge judges possess considerable biases. We further exploit their\nweakness and conduct attacks on LLM judges. We hope that our work can notify\nthe community of the vulnerability of human- and LLM-as-a-judge against\nperturbations, as well as the urgency of developing robust evaluation systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.10669v2.pdf"
    },
    {
        "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
        "authors": [
            "Dingzirui Wang",
            "Longxu Dou",
            "Xuanliang Zhang",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "published": "2024-02-16T13:13:18Z",
        "summary": "Currently, the in-context learning method based on large language models\n(LLMs) has become the mainstream of text-to-SQL research. Previous works have\ndiscussed how to select demonstrations related to the user question from a\nhuman-labeled demonstration pool. However, human labeling suffers from the\nlimitations of insufficient diversity and high labeling overhead. Therefore, in\nthis paper, we discuss how to measure and improve the diversity of the\ndemonstrations for text-to-SQL. We present a metric to measure the diversity of\nthe demonstrations and analyze the insufficient of the existing labeled data by\nexperiments. Based on the above discovery, we propose fusing iteratively for\ndemonstrations (Fused) to build a high-diversity demonstration pool through\nhuman-free multiple-iteration synthesis, improving diversity and lowering label\ncost. Our method achieves an average improvement of 3.2% and 5.0% with and\nwithout human labeling on several mainstream datasets, which proves the\neffectiveness of Fused.",
        "pdf_link": "https://arxiv.org/pdf/2402.10663v1.pdf"
    },
    {
        "title": "Network Formation and Dynamics Among Multi-LLMs",
        "authors": [
            "Marios Papachristou",
            "Yuan Yuan"
        ],
        "published": "2024-02-16T13:10:14Z",
        "summary": "Social networks shape opinions, behaviors, and information dissemination in\nhuman societies. As large language models (LLMs) increasingly integrate into\nsocial and professional environments, understanding their behavior within the\ncontext of social interactions and networks becomes essential. Our study\nanalyzes LLMs' network formation behavior to examine whether the dynamics of\nmultiple LLMs are similar to or different from human social dynamics. We\nobserve that LLMs exhibit key social network principles, including preferential\nattachment, triadic closure, homophily, community structure, and the\nsmall-world phenomenon, when asked about their preferences in network\nformation. We also investigate LLMs' decision-making based on real-world\nnetworks, revealing that triadic closure and homophily have a stronger\ninfluence than preferential attachment and that LLMs perform well in network\nformation predictions. Overall, our study opens up new possibilities for using\nLLMs in network science research and helps develop socially aware LLMs by\nshedding light on their network formation behaviors and exploring their impacts\non social dynamics.",
        "pdf_link": "https://arxiv.org/pdf/2402.10659v2.pdf"
    },
    {
        "title": "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes",
        "authors": [
            "Dingzirui Wang",
            "Longxu Dou",
            "Xuanliang Zhang",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "published": "2024-02-16T13:02:11Z",
        "summary": "Numerical reasoning is an essential ability for NLP systems to handle numeric\ninformation. Recent research indicates that fine-tuning a small-scale model to\nlearn generating reasoning processes alongside answers can significantly\nenhance performance. However, current methods have the limitation that most\nmethods generate reasoning processes with large language models (LLMs), which\nare \"unreliable\" since such processes could contain information unrelated to\nthe answer. To address this limitation, we introduce Enhancing NumeriCal\nreasOning with Reliable procEsses (Encore), which derives the reliable\nreasoning process by decomposing the answer formula, ensuring which fully\nsupports the answer. Nevertheless, models could lack enough data to learn the\nreasoning process generation adequately, since our method generates only one\nsingle reasoning process for one formula. To overcome this difficulty, we\npresent a series of pre-training tasks to help models learn the reasoning\nprocess generation with synthesized data. The experiments show that Encore\nyields improvement on all five experimental datasets with an average of 1.8%,\nproving the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2402.10654v1.pdf"
    },
    {
        "title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation",
        "authors": [
            "Zhaowei Wang",
            "Wei Fan",
            "Qing Zong",
            "Hongming Zhang",
            "Sehyun Choi",
            "Tianqing Fang",
            "Xin Liu",
            "Yangqiu Song",
            "Ginny Y. Wong",
            "Simon See"
        ],
        "published": "2024-02-16T12:47:11Z",
        "summary": "Abstraction ability is crucial in human intelligence, which can also benefit\nvarious tasks in NLP study. Existing work shows that LLMs are deficient in\nabstract ability, and how to improve it remains unexplored. In this work, we\ndesign the framework AbsInstruct to enhance LLMs' abstraction ability through\ninstruction tuning. The framework builds instructions with in-depth\nexplanations to assist LLMs in capturing the underlying rationale of\nabstraction. Meanwhile, we introduce a plausibility estimator to select\ninstructions that are more consistent with the abstraction knowledge of LLMs to\nbe aligned. Then, our framework combines abstraction instructions with\ngeneral-purpose ones to build a hybrid dataset. Extensive experiments and\nanalyses demonstrate that our framework can considerably enhance LLMs'\nabstraction ability with strong generalization performance while maintaining\ntheir general instruction-following abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.10646v1.pdf"
    },
    {
        "title": "Can Separators Improve Chain-of-Thought Prompting?",
        "authors": [
            "Yoonjeong Park",
            "Hyunjin Kim",
            "Chanyeol Choi",
            "Junseong Kim",
            "Jy-yong Sohn"
        ],
        "published": "2024-02-16T12:46:16Z",
        "summary": "Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large language models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce CoT-Sep, a novel method that strategically employs\nseparators at the end of each exemplar in CoT prompting. These separators are\ndesigned to help the LLMs understand their thought processes better while\nreasoning. It turns out that CoT-Sep significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B. Interestingly, the type/location of\nseparators should be chosen appropriately to boost the reasoning capability of\nCoT.",
        "pdf_link": "https://arxiv.org/pdf/2402.10645v1.pdf"
    },
    {
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
        "authors": [
            "Dayou Du",
            "Yijia Zhang",
            "Shijie Cao",
            "Jiaqi Guo",
            "Ting Cao",
            "Xiaowen Chu",
            "Ningyi Xu"
        ],
        "published": "2024-02-16T12:27:15Z",
        "summary": "The upscaling of Large Language Models (LLMs) has yielded impressive advances\nin natural language processing, yet it also poses significant deployment\nchallenges. Weight quantization has emerged as a widely embraced solution to\nreduce memory and computational demands. This paper introduces BitDistiller, a\nframework that synergizes Quantization-Aware Training (QAT) with Knowledge\nDistillation (KD) to boost the performance of LLMs at ultra-low precisions\n(sub-4-bit). Specifically, BitDistiller first incorporates a tailored\nasymmetric quantization and clipping technique to maximally preserve the\nfidelity of quantized weights, and then proposes a novel Confidence-Aware\nKullback-Leibler Divergence (CAKLD) objective, which is employed in a\nself-distillation manner to enable faster convergence and superior model\nperformance. Empirical evaluations demonstrate that BitDistiller significantly\nsurpasses existing methods in both 3-bit and 2-bit configurations on general\nlanguage understanding and complex reasoning benchmarks. Notably, BitDistiller\nis shown to be more cost-effective, demanding fewer data and training\nresources. The code is available at https://github.com/DD-DuDa/BitDistiller.",
        "pdf_link": "https://arxiv.org/pdf/2402.10631v1.pdf"
    },
    {
        "title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement",
        "authors": [
            "Yihong Tang",
            "Jiao Ou",
            "Che Liu",
            "Fuzheng Zhang",
            "Di Zhang",
            "Kun Gai"
        ],
        "published": "2024-02-16T12:12:05Z",
        "summary": "The advent of Large Language Models (LLMs) has propelled dialogue generation\ninto new realms, particularly in the field of role-playing systems (RPSs).\nWhile enhanced with ordinary role-relevant training dialogues, existing\nLLM-based RPSs still struggle to align with roles when handling intricate and\ntrapped queries in boundary scenarios. In this paper, we design the Modular\nORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve\nthe role-playing LLMs' performance. MORTISE can produce highly role-relevant\naggressive queries through the collaborative effort of multiple LLM-based\nmodules, and formulate corresponding responses to create an adversarial\ntraining dataset via a consistent response generator. We select 190 Chinese and\nEnglish roles to construct aggressive queries to benchmark existing\nrole-playing LLMs. Through comprehensive evaluation, we find that existing\nmodels exhibit a general deficiency in role alignment capabilities. We further\nselect 180 of the roles to collect an adversarial training dataset (named\nRoleAD) and retain the other 10 roles for testing. Experiments on models\nimproved by RoleAD indicate that our adversarial dataset ameliorates this\ndeficiency, with the improvements demonstrating a degree of generalizability in\nordinary scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.10618v1.pdf"
    },
    {
        "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements",
        "authors": [
            "Ming Li",
            "Jiuhai Chen",
            "Lichang Chen",
            "Tianyi Zhou"
        ],
        "published": "2024-02-16T12:00:34Z",
        "summary": "Making LLMs speak for different, especially minority groups of people, and\ngenerate statements supporting their diverse or even controversial perspectives\nis critical to creating an inclusive environment. However, existing LLMs lack\nsufficient controllability to the stance of their generated content, which\noften contains inconsistent, neutral, or biased statements. In this paper, we\nimprove the controllability of LLMs in generating statements supporting an\nargument the user defined in the prompt. We find that multi-round debates\nbetween two LLMs with opposite stances generate higher-quality and more salient\nstatements for each, which are important training data to improve the\ncontrollability of LLMs. Motivated by this, we develop a novel debate & tuning\n(\"DEBATunE\") pipeline finetuning LLMs to generate the statements obtained via\ndebate. To examine DEBATunE, we curate the largest dataset of debate topics so\nfar, which covers 710 controversial topics and corresponding arguments for each\ntopic. Evaluations by the GPT-4 judge with a novel controversy controllability\nmetric show that LLMs' capability of expressing diverse perspectives is\nsignificantly improved by DEBATunE. Moreover, such controllability can be\ngeneralized to unseen topics, generating high-quality statements supporting\ncontroversial arguments. Our codes, models, and data will be released at\nhttps://github.com/tianyi-lab/DEBATunE.",
        "pdf_link": "https://arxiv.org/pdf/2402.10614v1.pdf"
    },
    {
        "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
        "authors": [
            "Hanxing Ding",
            "Liang Pang",
            "Zihao Wei",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-16T11:55:40Z",
        "summary": "Hallucinations pose a significant challenge for the practical implementation\nof large language models (LLMs). The utilization of parametric knowledge in\ngenerating factual content is constrained by the limited knowledge of LLMs,\npotentially resulting in internal hallucinations. While incorporating external\ninformation can help fill knowledge gaps, it also introduces the risk of\nirrelevant information, thereby increasing the likelihood of external\nhallucinations. A careful and balanced integration of the parametric knowledge\nwithin LLMs with external information is crucial to alleviate hallucinations.\nIn this study, we present Rowen, a novel approach that enhances LLMs with a\nselective retrieval augmentation process tailored to address hallucinated\noutputs. This process is governed by a multilingual semantic-aware detection\nmodule, which evaluates the consistency of the perturbed responses across\nvarious languages for the same queries. Upon detecting inconsistencies\nindicative of hallucinations, Rowen activates the retrieval of external\ninformation to rectify the model outputs. Rowen adeptly harmonizes the\nintrinsic parameters in LLMs with external knowledge sources, effectively\nmitigating hallucinations by ensuring a balanced integration of internal\nreasoning and external evidence. Through a comprehensive empirical analysis, we\ndemonstrate that Rowen surpasses the current state-of-the-art in both detecting\nand mitigating hallucinated content within the outputs of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.10612v1.pdf"
    },
    {
        "title": "Jailbreaking Proprietary Large Language Models using Word Substitution Cipher",
        "authors": [
            "Divij Handa",
            "Advait Chirmule",
            "Bimal Gajera",
            "Chitta Baral"
        ],
        "published": "2024-02-16T11:37:05Z",
        "summary": "Large Language Models (LLMs) are aligned to moral and ethical guidelines but\nremain susceptible to creative prompts called Jailbreak that can bypass the\nalignment process. However, most jailbreaking prompts contain harmful questions\nin the natural language (mainly English), which can be detected by the LLM\nthemselves. In this paper, we present jailbreaking prompts encoded using\ncryptographic techniques. We first present a pilot study on the\nstate-of-the-art LLM, GPT-4, in decoding several safe sentences that have been\nencrypted using various cryptographic techniques and find that a\nstraightforward word substitution cipher can be decoded most effectively.\nMotivated by this result, we use this encoding technique for writing\njailbreaking prompts. We present a mapping of unsafe words with safe words and\nask the unsafe question using these mapped words. Experimental results show an\nattack success rate (up to 59.42%) of our proposed jailbreaking approach on\nstate-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro.\nAdditionally, we discuss the over-defensiveness of these models. We believe\nthat our work will encourage further research in making these LLMs more robust\nwhile maintaining their decoding capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.10601v1.pdf"
    },
    {
        "title": "Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks",
        "authors": [
            "Niall Taylor",
            "Upamanyu Ghose",
            "Omid Rohanian",
            "Mohammadmahdi Nouriborji",
            "Andrey Kormilitzin",
            "David Clifton",
            "Alejo Nevado-Holgado"
        ],
        "published": "2024-02-16T11:30:11Z",
        "summary": "The entry of large language models (LLMs) into research and commercial spaces\nhas led to a trend of ever-larger models, with initial promises of\ngeneralisability, followed by a widespread desire to downsize and create\nspecialised models without the need for complete fine-tuning, using Parameter\nEfficient Fine-tuning (PEFT) methods. We present an investigation into the\nsuitability of different PEFT methods to clinical decision-making tasks, across\na range of model sizes, including extremely small models with as few as $25$\nmillion parameters.\n  Our analysis shows that the performance of most PEFT approaches varies\nsignificantly from one task to another, with the exception of LoRA, which\nmaintains relatively high performance across all model sizes and tasks,\ntypically approaching or matching full fine-tuned performance. The\neffectiveness of PEFT methods in the clinical domain is evident, particularly\nfor specialised models which can operate on low-cost, in-house computing\ninfrastructure. The advantages of these models, in terms of speed and reduced\ntraining costs, dramatically outweighs any performance gain from large\nfoundation LLMs. Furthermore, we highlight how domain-specific pre-training\ninteracts with PEFT methods and model size, and discuss how these factors\ninterplay to provide the best efficiency-performance trade-off. Full code\navailable at: tbd.",
        "pdf_link": "https://arxiv.org/pdf/2402.10597v1.pdf"
    },
    {
        "title": "Do Llamas Work in English? On the Latent Language of Multilingual Transformers",
        "authors": [
            "Chris Wendler",
            "Veniamin Veselovsky",
            "Giovanni Monea",
            "Robert West"
        ],
        "published": "2024-02-16T11:21:28Z",
        "summary": "We ask whether multilingual language models trained on unbalanced,\nEnglish-dominated corpora use English as an internal pivot language -- a\nquestion of key importance for understanding how language models function and\nthe origins of linguistic bias. Focusing on the Llama-2 family of transformer\nmodels, our study uses carefully constructed non-English prompts with a unique\ncorrect single-token continuation. From layer to layer, transformers gradually\nmap an input embedding of the final prompt token to an output embedding from\nwhich next-token probabilities are computed. Tracking intermediate embeddings\nthrough their high-dimensional space reveals three distinct phases, whereby\nintermediate embeddings (1) start far away from output token embeddings; (2)\nalready allow for decoding a semantically correct next token in the middle\nlayers, but give higher probability to its version in English than in the input\nlanguage; (3) finally move into an input-language-specific region of the\nembedding space. We cast these results into a conceptual model where the three\nphases operate in \"input space\", \"concept space\", and \"output space\",\nrespectively. Crucially, our evidence suggests that the abstract \"concept\nspace\" lies closer to English than to other languages, which may have important\nconsequences regarding the biases held by multilingual language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10588v2.pdf"
    },
    {
        "title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs",
        "authors": [
            "Zae Myung Kim",
            "Kwang Hee Lee",
            "Preston Zhu",
            "Vipul Raheja",
            "Dongyeop Kang"
        ],
        "published": "2024-02-16T11:20:30Z",
        "summary": "With the advent of large language models (LLM), the line between\nhuman-crafted and machine-generated texts has become increasingly blurred. This\npaper delves into the inquiry of identifying discernible and unique linguistic\nproperties in texts that were written by humans, particularly uncovering the\nunderlying discourse structures of texts beyond their surface structures.\nIntroducing a novel methodology, we leverage hierarchical parse trees and\nrecursive hypergraphs to unveil distinctive discourse patterns in texts\nproduced by both LLMs and humans. Empirical findings demonstrate that, although\nboth LLMs and humans generate distinct discourse patterns influenced by\nspecific domains, human-written texts exhibit more structural variability,\nreflecting the nuanced nature of human writing in different domains. Notably,\nincorporating hierarchical discourse features enhances binary classifiers'\noverall performance in distinguishing between human-written and\nmachine-generated texts, even on out-of-distribution and paraphrased samples.\nThis underscores the significance of incorporating hierarchical discourse\nfeatures in the analysis of text patterns. The code and dataset will be\navailable at [TBA].",
        "pdf_link": "https://arxiv.org/pdf/2402.10586v1.pdf"
    },
    {
        "title": "LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty",
        "authors": [
            "Zhen Zhang",
            "Yuhua Zhao",
            "Hang Gao",
            "Mengting Hu"
        ],
        "published": "2024-02-16T11:02:29Z",
        "summary": "Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations.",
        "pdf_link": "https://arxiv.org/pdf/2402.10573v2.pdf"
    },
    {
        "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine",
        "authors": [
            "Erblin Isaku",
            "Christoph Laaber",
            "Hassan Sartaj",
            "Shaukat Ali",
            "Thomas Schwitalla",
            "Jan F. Nyg\u00e5rd"
        ],
        "published": "2024-02-16T10:56:15Z",
        "summary": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
        "pdf_link": "https://arxiv.org/pdf/2404.03664v2.pdf"
    },
    {
        "title": "InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?",
        "authors": [
            "Yogesh Tripathi",
            "Raghav Donakanti",
            "Sahil Girhepuje",
            "Ishan Kavathekar",
            "Bhaskara Hanuma Vedula",
            "Gokul S Krishnan",
            "Shreya Goyal",
            "Anmol Goel",
            "Balaraman Ravindran",
            "Ponnurangam Kumaraguru"
        ],
        "published": "2024-02-16T10:54:10Z",
        "summary": "Recent advancements in language technology and Artificial Intelligence have\nresulted in numerous Language Models being proposed to perform various tasks in\nthe legal domain ranging from predicting judgments to generating summaries.\nDespite their immense potential, these models have been proven to learn and\nexhibit societal biases and make unfair predictions. In this study, we explore\nthe ability of Large Language Models (LLMs) to perform legal tasks in the\nIndian landscape when social factors are involved. We present a novel metric,\n$\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which\nencapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'\nsafety by considering its performance in the $\\textit{Binary Statutory\nReasoning}$ task and its fairness exhibition with respect to various axes of\ndisparities in the Indian society. Task performance and fairness scores of\nLLaMA and LLaMA--2 models indicate that the proposed $LSS_{\\beta}$ metric can\neffectively determine the readiness of a model for safe usage in the legal\nsector. We also propose finetuning pipelines, utilising specialised legal\ndatasets, as a potential method to mitigate bias and improve model safety. The\nfinetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\\beta}$,\nimproving their usability in the Indian legal domain. Our code is publicly\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2402.10567v3.pdf"
    },
    {
        "title": "SPAR: Personalized Content-Based Recommendation via Long Engagement Attention",
        "authors": [
            "Chiyu Zhang",
            "Yifei Sun",
            "Jun Chen",
            "Jie Lei",
            "Muhammad Abdul-Mageed",
            "Sinong Wang",
            "Rong Jin",
            "Sem Park",
            "Ning Yao",
            "Bo Long"
        ],
        "published": "2024-02-16T10:36:38Z",
        "summary": "Leveraging users' long engagement histories is essential for personalized\ncontent recommendations. The success of pretrained language models (PLMs) in\nNLP has led to their use in encoding user histories and candidate items,\nframing content recommendations as textual semantic matching tasks. However,\nexisting works still struggle with processing very long user historical text\nand insufficient user-item interaction. In this paper, we introduce a\ncontent-based recommendation framework, SPAR, which effectively tackles the\nchallenges of holistic user interest extraction from the long user engagement\nhistory. It achieves so by leveraging PLM, poly-attention layers and attention\nsparsity mechanisms to encode user's history in a session-based manner. The\nuser and item side features are sufficiently fused for engagement prediction\nwhile maintaining standalone representations for both sides, which is efficient\nfor practical model deployment. Moreover, we enhance user profiling by\nexploiting large language model (LLM) to extract global interests from user\nengagement history. Extensive experiments on two benchmark datasets demonstrate\nthat our framework outperforms existing state-of-the-art (SoTA) methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.10555v1.pdf"
    },
    {
        "title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts",
        "authors": [
            "Xiaobo Guo",
            "Soroush Vosoughi"
        ],
        "published": "2024-02-16T10:35:18Z",
        "summary": "Aspect-based summarization has seen significant advancements, especially in\nstructured text. Yet, summarizing disordered, large-scale texts, like those\nfound in social media and customer feedback, remains a significant challenge.\nCurrent research largely targets predefined aspects within structured texts,\nneglecting the complexities of dynamic and disordered environments. Addressing\nthis gap, we introduce Disordered-DABS, a novel benchmark for dynamic\naspect-based summarization tailored to unstructured text. Developed by adapting\nexisting datasets for cost-efficiency and scalability, our comprehensive\nexperiments and detailed human evaluations reveal that Disordered-DABS poses\nunique challenges to contemporary summarization models, including\nstate-of-the-art language models such as GPT-3.5.",
        "pdf_link": "https://arxiv.org/pdf/2402.10554v1.pdf"
    },
    {
        "title": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models",
        "authors": [
            "Minghan Wang",
            "Thuy-Trang Vu",
            "Ehsan Shareghi",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-16T10:32:16Z",
        "summary": "Simultaneous machine translation (SimulMT) presents a challenging trade-off\nbetween translation quality and latency. Recent studies have shown that LLMs\ncan achieve good performance in SimulMT tasks. However, this often comes at the\nexpense of high inference cost and latency. In this paper, we propose a\nconversational SimulMT framework to enhance the inference efficiency of\nLLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments\nwith Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of\nLLM in translation quality while achieving comparable computational latency to\nspecialized SimulMT models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10552v1.pdf"
    },
    {
        "title": "Properties and Challenges of LLM-Generated Explanations",
        "authors": [
            "Jenny Kunz",
            "Marco Kuhlmann"
        ],
        "published": "2024-02-16T09:37:54Z",
        "summary": "The self-rationalising capabilities of large language models (LLMs) have been\nexplored in restricted settings, using task/specific data sets. However,\ncurrent LLMs do not (only) rely on specifically annotated data; nonetheless,\nthey frequently explain their outputs. The properties of the generated\nexplanations are influenced by the pre-training corpus and by the target data\nused for instruction fine-tuning. As the pre-training corpus includes a large\namount of human-written explanations \"in the wild\", we hypothesise that LLMs\nadopt common properties of human explanations. By analysing the outputs for a\nmulti-domain instruction fine-tuning data set, we find that generated\nexplanations show selectivity and contain illustrative elements, but less\nfrequently are subjective or misleading. We discuss reasons and consequences of\nthe properties' presence or absence. In particular, we outline positive and\nnegative implications depending on the goals and user groups of the\nself-rationalising system.",
        "pdf_link": "https://arxiv.org/pdf/2402.10532v1.pdf"
    },
    {
        "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
        "authors": [
            "Xin Xu",
            "Shizhe Diao",
            "Can Yang",
            "Yang Wang"
        ],
        "published": "2024-02-16T09:29:50Z",
        "summary": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of 5.1% increase in the F1\nscore across all 45 subsets within R2PE. We further demonstrate our PDS's\nefficacy in advancing open-domain QA accuracy. Data and code are available at\nhttps://github.com/XinXU-USTC/R2PE.",
        "pdf_link": "https://arxiv.org/pdf/2402.10528v1.pdf"
    },
    {
        "title": "Zero-shot sampling of adversarial entities in biomedical question answering",
        "authors": [
            "R. Patrick Xian",
            "Alex J. Lee",
            "Vincent Wang",
            "Qiming Cui",
            "Russell Ro",
            "Reza Abbasi-Asl"
        ],
        "published": "2024-02-16T09:29:38Z",
        "summary": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications. In\nhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilities\nis essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples in natural language processing tasks raises questions about their\npotential guises in other settings. Here, we propose a powerscaled\ndistance-weighted sampling scheme in embedding space to discover diverse\nadversarial entities as distractors. We demonstrate its advantage over random\nsampling in adversarial question answering on biomedical topics. Our approach\nenables the exploration of different regions on the attack surface, which\nreveals two regimes of adversarial entities that markedly differ in their\ncharacteristics. Moreover, we show that the attacks successfully manipulate\ntoken-wise Shapley value explanations, which become deceptive in the\nadversarial setting. Our investigations illustrate the brittleness of domain\nknowledge in LLMs and reveal a shortcoming of standard evaluations for\nhigh-capacity models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10527v1.pdf"
    },
    {
        "title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models",
        "authors": [
            "Minsuk Kahng",
            "Ian Tenney",
            "Mahima Pushkarna",
            "Michael Xieyang Liu",
            "James Wexler",
            "Emily Reif",
            "Krystal Kallarackal",
            "Minsuk Chang",
            "Michael Terry",
            "Lucas Dixon"
        ],
        "published": "2024-02-16T09:14:49Z",
        "summary": "Automatic side-by-side evaluation has emerged as a promising approach to\nevaluating the quality of responses from large language models (LLMs). However,\nanalyzing the results from this evaluation approach raises scalability and\ninterpretability challenges. In this paper, we present LLM Comparator, a novel\nvisual analytics tool for interactively analyzing results from automatic\nside-by-side evaluation. The tool supports interactive workflows for users to\nunderstand when and why a model performs better or worse than a baseline model,\nand how the responses from two models are qualitatively different. We\niteratively designed and developed the tool by closely working with researchers\nand engineers at a large technology company. This paper details the user\nchallenges we identified, the design and development of the tool, and an\nobservational study with participants who regularly evaluate their models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10524v1.pdf"
    },
    {
        "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
        "authors": [
            "Yeonhong Park",
            "Jake Hyun",
            "SangLyul Cho",
            "Bonggeun Sim",
            "Jae W. Lee"
        ],
        "published": "2024-02-16T09:06:06Z",
        "summary": "Recently, considerable efforts have been directed towards compressing Large\nLanguage Models (LLMs), which showcase groundbreaking capabilities across\ndiverse applications but entail significant deployment costs due to their large\nsizes. Meanwhile, much less attention has been given to mitigating the costs\nassociated with deploying multiple LLMs of varying sizes despite its practical\nsignificance. Thus, this paper introduces \\emph{any-precision LLM}, extending\nthe concept of any-precision DNN to LLMs. Addressing challenges in\nany-precision LLM, we propose a lightweight method for any-precision\nquantization of LLMs, leveraging a post-training quantization framework, and\ndevelop a specialized software engine for its efficient serving. As a result,\nour solution significantly reduces the high costs of deploying multiple,\ndifferent-sized LLMs by overlaying LLMs quantized to varying bit-widths, such\nas 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit\nLLM. All the supported LLMs with varying bit-widths demonstrate\nstate-of-the-art model quality and inference throughput, proving itself to be a\ncompelling option for deployment of multiple, different-sized LLMs. The source\ncode will be publicly available soon.",
        "pdf_link": "https://arxiv.org/pdf/2402.10517v1.pdf"
    },
    {
        "title": "Provably Sample Efficient RLHF via Active Preference Optimization",
        "authors": [
            "Nirjhar Das",
            "Souradip Chakraborty",
            "Aldo Pacchiano",
            "Sayak Ray Chowdhury"
        ],
        "published": "2024-02-16T08:19:34Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning\nLarge Language Models (LLMs) with human preferences. While these aligned\ngenerative models have demonstrated impressive capabilities across various\ntasks, the dependence on high-quality human preference data poses a costly\nbottleneck in practical implementation of RLHF. Hence better and adaptive\nstrategies for data collection is needed. To this end, we frame RLHF as a\ncontextual preference bandit problem with prompts as contexts and show that the\nnaive way of collecting preference data by choosing prompts uniformly at random\nleads to a policy that suffers an $\\Omega(1)$ suboptimality gap in rewards.\nThen we propose $\\textit{Active Preference Optimization}$ ($\\texttt{APO}$), an\nalgorithm that actively selects prompts to collect preference data. Under the\nBradley-Terry-Luce (BTL) preference model, \\texttt{APO} achieves sample\nefficiency without compromising on policy performance. We show that given a\nsample budget of $T$, the suboptimality gap of a policy learned via\n$\\texttt{APO}$ scales as $O(1/\\sqrt{T})$. Next, we propose a compute-efficient\nbatch version of $\\texttt{APO}$ with minor modification and evaluate its\nperformance in practice. Experimental evaluations on a human preference dataset\nvalidate \\texttt{APO}'s efficacy as a sample-efficient and practical solution\nto data collection for RLHF, facilitating alignment of LLMs with human\npreferences in a cost-effective and scalable manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.10500v1.pdf"
    },
    {
        "title": "Comparing Hallucination Detection Metrics for Multilingual Generation",
        "authors": [
            "Haoqiang Kang",
            "Terra Blevins",
            "Luke Zettlemoyer"
        ],
        "published": "2024-02-16T08:10:34Z",
        "summary": "While many automatic hallucination detection techniques have been proposed\nfor English texts, their effectiveness in multilingual contexts remains\nunexplored. This paper aims to bridge the gap in understanding how these\nhallucination detection metrics perform on non-English languages. We evaluate\nthe efficacy of various detection metrics, including lexical metrics like ROUGE\nand Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at\ndetecting hallucinations in biographical summaries in many languages; we also\nevaluate how correlated these different metrics are to gauge whether they\nmeasure the same phenomena. Our empirical analysis reveals that while lexical\nmetrics show limited effectiveness, NLI-based metrics perform well in\nhigh-resource languages at the sentence level. In contrast, NLI-based metrics\noften fail to detect atomic fact hallucinations. Our findings highlight\nexisting gaps in multilingual hallucination detection and motivate future\nresearch to develop more robust detection methods for LLM hallucination in\nother languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.10496v1.pdf"
    },
    {
        "title": "Unsupervised LLM Adaptation for Question Answering",
        "authors": [
            "Kuniaki Saito",
            "Kihyuk Sohn",
            "Chen-Yu Lee",
            "Yoshitaka Ushiku"
        ],
        "published": "2024-02-16T06:29:16Z",
        "summary": "Large language models (LLM) learn diverse knowledge present in the\nlarge-scale training dataset via self-supervised training. Followed by\ninstruction-tuning, LLM acquires the ability to return correct information for\ndiverse questions. However, adapting these pre-trained LLMs to new target\ndomains, such as different organizations or periods, for the question-answering\n(QA) task incurs a substantial annotation cost. To tackle this challenge, we\npropose a novel task, unsupervised LLM adaptation for question answering. In\nthis task, we leverage a pre-trained LLM, a publicly available QA dataset\n(source data), and unlabeled documents from the target domain. Our goal is to\nlearn LLM that can answer questions about the target domain. We introduce one\nsynthetic and two real datasets to evaluate models fine-tuned on the source and\ntarget data, and reveal intriguing insights; (i) fine-tuned models exhibit the\nability to provide correct answers for questions about the target domain even\nthough they do not see any questions about the information described in the\nunlabeled documents, but (ii) they have difficulties in accessing information\nlocated in the middle or at the end of documents, and (iii) this challenge can\nbe partially mitigated by replacing input tokens with random ones during\nadaptation.",
        "pdf_link": "https://arxiv.org/pdf/2402.12170v1.pdf"
    },
    {
        "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
        "authors": [
            "Zekun Li",
            "Zhiyu Zoey Chen",
            "Mike Ross",
            "Patrick Huber",
            "Seungwhan Moon",
            "Zhaojiang Lin",
            "Xin Luna Dong",
            "Adithya Sagar",
            "Xifeng Yan",
            "Paul A. Crook"
        ],
        "published": "2024-02-16T06:13:18Z",
        "summary": "Large language models (LLMs) are increasingly prevalent in conversational\nsystems due to their advanced understanding and generative capabilities in\ngeneral contexts. However, their effectiveness in task-oriented dialogues\n(TOD), which requires not only response generation but also effective dialogue\nstate tracking (DST) within specific tasks and domains, remains less\nsatisfying. In this work, we propose a novel approach FnCTOD for solving DST\nwith LLMs through function calling. This method improves zero-shot DST,\nallowing adaptation to diverse domains without extensive data collection or\nmodel tuning. Our experimental results demonstrate that our approach achieves\nexceptional performance with both modestly sized open-source and also\nproprietary LLMs: with in-context prompting it enables various 7B or 13B\nparameter models to surpass the previous state-of-the-art (SOTA) achieved by\nChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% Avg. JGA.\nIndividual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%,\nrespectively. We also show that by fine-tuning on a small collection of diverse\ntask-oriented dialogues, we can equip modestly sized models, specifically a 13B\nparameter LLaMA2-Chat model, with function-calling capabilities and DST\nperformance comparable to ChatGPT while maintaining their chat capabilities. We\nplan to open-source experimental code and model.",
        "pdf_link": "https://arxiv.org/pdf/2402.10466v1.pdf"
    },
    {
        "title": "QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning",
        "authors": [
            "Hossein Rajabzadeh",
            "Mojtaba Valipour",
            "Tianshu Zhu",
            "Marzieh Tahaei",
            "Hyock Ju Kwon",
            "Ali Ghodsi",
            "Boxing Chen",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2024-02-16T05:42:17Z",
        "summary": "Finetuning large language models requires huge GPU memory, restricting the\nchoice to acquire Larger models. While the quantized version of the Low-Rank\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\nQLoRA and outperforms when employing its optimal rank.",
        "pdf_link": "https://arxiv.org/pdf/2402.10462v1.pdf"
    },
    {
        "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing",
        "authors": [
            "Chenhui Hu",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2024-02-16T05:29:59Z",
        "summary": "Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. In this paper, lifelong editing is\nsynonymous with lifelong knowledge editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named WilKE, which\nselects editing layer based on the pattern matching degree of editing knowledge\nacross different layers. Experimental results demonstrate that, in lifelong\nediting, WilKE exhibits an average improvement of 46.2\\% and 67.8\\% on editing\nGPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.10987v1.pdf"
    },
    {
        "title": "Steering Conversational Large Language Models for Long Emotional Support Conversations",
        "authors": [
            "Navid Madani",
            "Sougata Saha",
            "Rohini Srihari"
        ],
        "published": "2024-02-16T05:03:01Z",
        "summary": "In this study, we address the challenge of consistently following emotional\nsupport strategies in long conversations by large language models (LLMs). We\nintroduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic\nmeasure designed to evaluate the effectiveness of LLMs in adhering to strategic\nprompts in emotional support contexts. By analyzing conversations within the\nEmotional Support Conversations dataset (ESConv) using LLaMA models, we\ndemonstrate that SRA is significantly correlated with a model's ability to\nsustain the outlined strategy throughout the interactions. Our findings reveal\nthat the application of SRA-informed prompts leads to enhanced strategic\nadherence, resulting in conversations that more reliably exhibit the desired\nemotional support strategies over longer conversations. Furthermore, we\ncontribute a comprehensive, multi-branch synthetic conversation dataset for\nESConv, featuring a variety of strategy continuations informed by our optimized\nprompting method. The code and data are publicly available on our Github.",
        "pdf_link": "https://arxiv.org/pdf/2402.10453v1.pdf"
    },
    {
        "title": "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models",
        "authors": [
            "Wenchao Dong",
            "Assem Zhunis",
            "Hyojin Chin",
            "Jiyoung Han",
            "Meeyoung Cha"
        ],
        "published": "2024-02-16T03:54:48Z",
        "summary": "We explored cultural biases-individualism vs. collectivism-in ChatGPT across\nthree Western languages (i.e., English, German, and French) and three Eastern\nlanguages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an\nindividualistic persona in Western languages, its collectivism scores (i.e.,\nout-group values) exhibited a more negative trend, surpassing their positive\norientation towards individualism (i.e., in-group values). Conversely, when a\ncollectivistic persona was assigned to ChatGPT in Eastern languages, a similar\npattern emerged with more negative responses toward individualism (i.e.,\nout-group values) as compared to collectivism (i.e., in-group values). The\nresults indicate that when imbued with a particular social identity, ChatGPT\ndiscerns in-group and out-group, embracing in-group values while eschewing\nout-group values. Notably, the negativity towards the out-group, from which\nprejudices and discrimination arise, exceeded the positivity towards the\nin-group. The experiment was replicated in the political domain, and the\nresults remained consistent. Furthermore, this replication unveiled an\nintrinsic Democratic bias in Large Language Models (LLMs), aligning with\nearlier findings and providing integral insights into mitigating such bias\nthrough prompt engineering. Extensive robustness checks were performed using\nvarying hyperparameter and persona setup methods, with or without social\nidentity labels, across other popular language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10436v1.pdf"
    },
    {
        "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Alex Nguyen",
            "Jingbo Shang"
        ],
        "published": "2024-02-16T03:39:37Z",
        "summary": "Instruction-tuning language models has become a crucial step in aligning them\nfor general use. Typically, this process involves extensive training on large\ndatasets, incurring high training costs. In this paper, we introduce a novel\ntraining data selection based on the learning percentage of the samples. We\nassert that current language models possess the capability to autonomously\nselect high-quality training data, leading to comparable or improved\nperformance compared to training on the entire dataset. Our experiments span\ndifferent-sized models, revealing that this characteristic holds for models\nranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an\ninteresting finding that the data hardness transfers across model sizes, and a\nsmaller 350M model can effectively curate high-quality training data with hard\nsamples for a larger 13B model, resulting in an equally or superior\ninstruction-tuned model compared to training on the complete dataset. Utilizing\nopen-sourced OPT and Llama-2 models up to 13B in size, two publicly available\ninstruction-tuning training datasets and evaluated by both automatic metrics &\nhumans, our paper introduces a novel approach to training data selection,\nshowcasing a more efficient alternative.",
        "pdf_link": "https://arxiv.org/pdf/2402.10430v1.pdf"
    },
    {
        "title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection",
        "authors": [
            "Herun Wan",
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Heng Wang",
            "Yulia Tsvetkov",
            "Minnan Luo"
        ],
        "published": "2024-02-16T03:24:56Z",
        "summary": "Large language models are limited by challenges in factuality and\nhallucinations to be directly employed off-the-shelf for judging the veracity\nof news articles, where factual accuracy is paramount. In this work, we propose\nDELL that identifies three key stages in misinformation detection where LLMs\ncould be incorporated as part of the pipeline: 1) LLMs could \\emph{generate\nnews reactions} to represent diverse perspectives and simulate user-news\ninteraction networks; 2) LLMs could \\emph{generate explanations} for proxy\ntasks (e.g., sentiment, stance) to enrich the contexts of news articles and\nproduce experts specializing in various aspects of news understanding; 3) LLMs\ncould \\emph{merge task-specific experts} and provide an overall prediction by\nincorporating the predictions and confidence scores of varying experts.\nExtensive experiments on seven datasets with three LLMs demonstrate that DELL\noutperforms state-of-the-art baselines by up to 16.8\\% in macro f1-score.\nFurther analysis reveals that the generated reactions and explanations are\ngreatly helpful in misinformation detection, while our proposed LLM-guided\nexpert merging helps produce better-calibrated predictions.",
        "pdf_link": "https://arxiv.org/pdf/2402.10426v1.pdf"
    },
    {
        "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting",
        "authors": [
            "Jiaheng Wei",
            "Yuanshun Yao",
            "Jean-Francois Ton",
            "Hongyi Guo",
            "Andrew Estornell",
            "Yang Liu"
        ],
        "published": "2024-02-16T02:32:06Z",
        "summary": "LLM hallucination, i.e. generating factually incorrect yet seemingly\nconvincing answers, is currently a major threat to the trustworthiness and\nreliability of LLMs. The first step towards solving this complicated problem is\nto measure it. However, existing hallucination metrics require to have a\nbenchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers\nwritten by humans. Such requirement makes hallucination measurement costly and\nprone to human errors. In this work, we propose Factualness Evaluations via\nWeighting LLMs (FEWL), the first hallucination metric that is specifically\ndesigned for the scenario when gold-standard answers are absent. FEWL leverages\nthe answers from off-the-shelf LLMs that serve as a proxy of gold-standard\nanswers. The key challenge is how to quantify the expertise of reference LLMs\nresourcefully. We show FEWL has certain theoretical guarantees and demonstrate\nempirically it gives more accurate hallucination measures than naively using\nreference LLMs. We also show how to leverage FEWL to reduce hallucination\nthrough both in-context learning and supervised finetuning. Last, we build a\nlarge-scale benchmark dataset to facilitate LLM hallucination research.",
        "pdf_link": "https://arxiv.org/pdf/2402.10412v1.pdf"
    },
    {
        "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
        "authors": [
            "Jun Zhuang",
            "Casey Kennington"
        ],
        "published": "2024-02-16T02:21:59Z",
        "summary": "As new research on Large Language Models (LLMs) continues, it is difficult to\nkeep up with new research and models. To help researchers synthesize the new\nresearch many have written survey papers, but even those have become numerous.\nIn this paper, we develop a method to automatically assign survey papers to a\ntaxonomy. We collect the metadata of 144 LLM survey papers and explore three\nparadigms to classify papers within the taxonomy. Our work indicates that\nleveraging graph structure information on co-category graphs can significantly\noutperform the language models in two paradigms; pre-trained language models'\nfine-tuning and zero-shot/few-shot classifications using LLMs. We find that our\nmodel surpasses an average human recognition level and that fine-tuning LLMs\nusing weak labels generated by a smaller model, such as the GCN in this study,\ncan be more effective than using ground-truth labels, revealing the potential\nof weak-to-strong generalization in the taxonomy classification task.",
        "pdf_link": "https://arxiv.org/pdf/2402.10409v1.pdf"
    },
    {
        "title": "Chain of Logic: Rule-Based Reasoning with Large Language Models",
        "authors": [
            "Sergio Servantez",
            "Joe Barrow",
            "Kristian Hammond",
            "Rajiv Jain"
        ],
        "published": "2024-02-16T01:54:43Z",
        "summary": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to\ndraw conclusions by accurately applying a rule to a set of facts. We explore\ncausal language models as rule-based reasoners, specifically with respect to\ncompositional rules - rules consisting of multiple elements which form a\ncomplex logical expression. Reasoning about compositional rules is challenging\nbecause it requires multiple reasoning steps, and attending to the logical\nrelationships between elements. We introduce a new prompting method, Chain of\nLogic, which elicits rule-based reasoning through decomposition (solving\nelements as independent threads of logic), and recomposition (recombining these\nsub-answers to resolve the underlying logical expression). This method was\ninspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a\nsequential reasoning approach used by lawyers. We evaluate chain of logic\nacross eight rule-based reasoning tasks involving three distinct compositional\nrules from the LegalBench benchmark and demonstrate it consistently outperforms\nother prompting methods, including chain of thought and self-ask, using\nopen-source and commercial language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10400v2.pdf"
    },
    {
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
        "authors": [
            "Ajay Patel",
            "Colin Raffel",
            "Chris Callison-Burch"
        ],
        "published": "2024-02-16T00:10:26Z",
        "summary": "Large language models (LLMs) have become a dominant and important tool for\nNLP researchers in a wide range of tasks. Today, many researchers use LLMs in\nsynthetic data generation, task evaluation, fine-tuning, distillation, and\nother model-in-the-loop research workflows. However, challenges arise when\nusing these models that stem from their scale, their closed source nature, and\nthe lack of standardized tooling for these new and emerging workflows. The\nrapid rise to prominence of these models and these unique challenges has had\nimmediate adverse impacts on open science and on the reproducibility of work\nthat uses them. In this paper, we introduce DataDreamer, an open source Python\nlibrary that allows researchers to write simple code to implement powerful LLM\nworkflows. DataDreamer also helps researchers adhere to best practices that we\npropose to encourage open science and reproducibility. The library and\ndocumentation are available at https://github.com/datadreamer-dev/DataDreamer .",
        "pdf_link": "https://arxiv.org/pdf/2402.10379v1.pdf"
    },
    {
        "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Emmanuel Morin",
            "Pierre-Antoine Gourraud",
            "Mickael Rouvier",
            "Richard Dufour"
        ],
        "published": "2024-02-15T23:39:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.",
        "pdf_link": "https://arxiv.org/pdf/2402.10373v1.pdf"
    },
    {
        "title": "Can we Soft Prompt LLMs for Graph Learning Tasks?",
        "authors": [
            "Zheyuan Liu",
            "Xiaoxin He",
            "Yijun Tian",
            "Nitesh V. Chawla"
        ],
        "published": "2024-02-15T23:09:42Z",
        "summary": "Graph plays an important role in representing complex relationships in\nreal-world applications such as social networks, biological data and citation\nnetworks. In recent years, Large Language Models (LLMs) have achieved\ntremendous success in various domains, which makes applying LLMs to graphs\nparticularly appealing. However, directly applying LLMs to graph modalities\npresents unique challenges due to the discrepancy and mismatch between the\ngraph and text modalities. Hence, to further investigate LLMs' potential for\ncomprehending graph information, we introduce GraphPrompter, a novel framework\ndesigned to align graph information with LLMs via soft prompts. Specifically,\nGraphPrompter consists of two main components: a graph neural network to encode\ncomplex graph information and an LLM that effectively processes textual\ninformation. Comprehensive experiments on various benchmark datasets under node\nclassification and link prediction tasks demonstrate the effectiveness of our\nproposed method. The GraphPrompter framework unveils the substantial\ncapabilities of LLMs as predictors in graph-related tasks, enabling researchers\nto utilize LLMs across a spectrum of real-world graph scenarios more\neffectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.10359v2.pdf"
    },
    {
        "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
        "authors": [
            "Kang He",
            "Yinghan Long",
            "Kaushik Roy"
        ],
        "published": "2024-02-15T22:54:24Z",
        "summary": "Prompt learning is susceptible to intrinsic bias present in pre-trained\nlanguage models (LMs), resulting in sub-optimal performance of prompt-based\nzero/few-shot learning. In this work, we propose a null-input prompting method\nto calibrate intrinsic bias encoded in pre-trained LMs. Different from prior\nefforts that address intrinsic bias primarily for social fairness and often\ninvolve excessive computational cost, our objective is to explore enhancing\nLMs' performance in downstream zero/few-shot learning while emphasizing the\nefficiency of intrinsic bias calibration. Specifically, we leverage a diverse\nset of auto-selected null-meaning inputs generated from GPT-4 to prompt\npre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected\nprobability distribution, we formulate a distribution disparity loss for bias\ncalibration, where we exclusively update bias parameters ($0.1\\%$ of total\nparameters) of LMs towards equal probability distribution. Experimental results\nshow that the calibration promotes an equitable starting point for LMs while\npreserving language modeling abilities. Across a wide range of datasets,\nincluding sentiment analysis and topic classification, our method significantly\nimproves zero/few-shot learning performance of LMs for both in-context learning\nand prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
        "pdf_link": "https://arxiv.org/pdf/2402.10353v1.pdf"
    },
    {
        "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
        "authors": [
            "Jing Su",
            "Chufeng Jiang",
            "Xin Jin",
            "Yuxin Qiao",
            "Tingsong Xiao",
            "Hongda Ma",
            "Rong Wei",
            "Zhi Jing",
            "Jiajun Xu",
            "Junhong Lin"
        ],
        "published": "2024-02-15T22:43:02Z",
        "summary": "This systematic literature review comprehensively examines the application of\nLarge Language Models (LLMs) in forecasting and anomaly detection, highlighting\nthe current state of research, inherent challenges, and prospective future\ndirections. LLMs have demonstrated significant potential in parsing and\nanalyzing extensive datasets to identify patterns, predict future events, and\ndetect anomalous behavior across various domains. However, this review\nidentifies several critical challenges that impede their broader adoption and\neffectiveness, including the reliance on vast historical datasets, issues with\ngeneralizability across different contexts, the phenomenon of model\nhallucinations, limitations within the models' knowledge boundaries, and the\nsubstantial computational resources required. Through detailed analysis, this\nreview discusses potential solutions and strategies to overcome these\nobstacles, such as integrating multimodal data, advancements in learning\nmethodologies, and emphasizing model explainability and computational\nefficiency. Moreover, this review outlines critical trends that are likely to\nshape the evolution of LLMs in these fields, including the push toward\nreal-time processing, the importance of sustainable modeling practices, and the\nvalue of interdisciplinary collaboration. Conclusively, this review underscores\nthe transformative impact LLMs could have on forecasting and anomaly detection\nwhile emphasizing the need for continuous innovation, ethical considerations,\nand practical solutions to realize their full potential.",
        "pdf_link": "https://arxiv.org/pdf/2402.10350v1.pdf"
    },
    {
        "title": "On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities",
        "authors": [
            "Xiyang Wu",
            "Ruiqi Xian",
            "Tianrui Guan",
            "Jing Liang",
            "Souradip Chakraborty",
            "Fuxiao Liu",
            "Brian Sadler",
            "Dinesh Manocha",
            "Amrit Singh Bedi"
        ],
        "published": "2024-02-15T22:01:45Z",
        "summary": "In this paper, we highlight the critical issues of robustness and safety\nassociated with integrating large language models (LLMs) and vision-language\nmodels (VLMs) into robotics applications. Recent works have focused on using\nLLMs and VLMs to improve the performance of robotics tasks, such as\nmanipulation, navigation, etc. However, such integration can introduce\nsignificant vulnerabilities, in terms of their susceptibility to adversarial\nattacks due to the language models, potentially leading to catastrophic\nconsequences. By examining recent works at the interface of LLMs/VLMs and\nrobotics, we show that it is easy to manipulate or misguide the robot's\nactions, leading to safety hazards. We define and provide examples of several\nplausible adversarial attacks, and conduct experiments on three prominent robot\nframeworks integrated with a language model, including KnowNo VIMA, and\nInstruct2Act, to assess their susceptibility to these attacks. Our empirical\nfindings reveal a striking vulnerability of LLM/VLM-robot integrated systems:\nsimple adversarial attacks can significantly undermine the effectiveness of\nLLM/VLM-robot integrated systems. Specifically, our data demonstrate an average\nperformance deterioration of 21.2% under prompt attacks and a more alarming\n30.2% under perception attacks. These results underscore the critical need for\nrobust countermeasures to ensure the safe and reliable deployment of the\nadvanced LLM/VLM-based robotic systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.10340v3.pdf"
    },
    {
        "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Dong Yu",
            "Fei Liu"
        ],
        "published": "2024-02-15T20:26:07Z",
        "summary": "Large language models hold significant potential for integrating various data\ntypes, such as text documents and database records, for advanced analytics.\nHowever, blending text and numerical data presents substantial challenges. LLMs\nneed to process and cross-reference entities and numbers, handle data\ninconsistencies and redundancies, and develop planning capabilities such as\nbuilding a working memory for managing complex data queries. In this paper, we\nintroduce four novel tasks centered around sports data analytics to evaluate\nthe numerical reasoning and information fusion capabilities of LLMs. These\ntasks involve providing LLMs with detailed, play-by-play sports game\ndescriptions, then challenging them with adversarial scenarios such as new game\nrules, longer durations, scrambled narratives, and analyzing key statistics in\ngame summaries. We conduct extensive experiments on NBA and NFL games to assess\nthe performance of LLMs on these tasks. Our benchmark, SportsMetrics,\nintroduces a new mechanism for assessing LLMs' numerical reasoning and fusion\nskills.",
        "pdf_link": "https://arxiv.org/pdf/2402.10979v1.pdf"
    },
    {
        "title": "How to Discern Important Urgent News?",
        "authors": [
            "Oleg Vasilyev",
            "John Bohannon"
        ],
        "published": "2024-02-15T20:08:07Z",
        "summary": "We found that a simple property of clusters in a clustered dataset of news\ncorrelate strongly with importance and urgency of news (IUN) as assessed by\nLLM. We verified our finding across different news datasets, dataset sizes,\nclustering algorithms and embeddings. The found correlation should allow using\nclustering (as an alternative to LLM) for identifying the most important urgent\nnews, or for filtering out unimportant articles.",
        "pdf_link": "https://arxiv.org/pdf/2402.10302v1.pdf"
    },
    {
        "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
        "authors": [
            "Bryan Wang",
            "Yuliang Li",
            "Zhaoyang Lv",
            "Haijun Xia",
            "Yan Xu",
            "Raj Sodhi"
        ],
        "published": "2024-02-15T19:53:11Z",
        "summary": "Video creation has become increasingly popular, yet the expertise and effort\nrequired for editing often pose barriers to beginners. In this paper, we\nexplore the integration of large language models (LLMs) into the video editing\nworkflow to reduce these barriers. Our design vision is embodied in LAVE, a\nnovel system that provides LLM-powered agent assistance and language-augmented\nediting features. LAVE automatically generates language descriptions for the\nuser's footage, serving as the foundation for enabling the LLM to process\nvideos and assist in editing tasks. When the user provides editing objectives,\nthe agent plans and executes relevant actions to fulfill them. Moreover, LAVE\nallows users to edit videos through either the agent or direct UI manipulation,\nproviding flexibility and enabling manual refinement of agent actions. Our user\nstudy, which included eight participants ranging from novices to proficient\neditors, demonstrated LAVE's effectiveness. The results also shed light on user\nperceptions of the proposed LLM-assisted editing paradigm and its impact on\nusers' creativity and sense of co-creation. Based on these findings, we propose\ndesign implications to inform the future development of agent-assisted content\nediting.",
        "pdf_link": "https://arxiv.org/pdf/2402.10294v1.pdf"
    },
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": [
            "Huizhuo Yuan",
            "Zixiang Chen",
            "Kaixuan Ji",
            "Quanquan Gu"
        ],
        "published": "2024-02-15T18:59:18Z",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "pdf_link": "https://arxiv.org/pdf/2402.10210v1.pdf"
    },
    {
        "title": "A StrongREJECT for Empty Jailbreaks",
        "authors": [
            "Alexandra Souly",
            "Qingyuan Lu",
            "Dillon Bowen",
            "Tu Trinh",
            "Elvis Hsieh",
            "Sana Pandey",
            "Pieter Abbeel",
            "Justin Svegliato",
            "Scott Emmons",
            "Olivia Watkins",
            "Sam Toyer"
        ],
        "published": "2024-02-15T18:58:09Z",
        "summary": "The rise of large language models (LLMs) has drawn attention to the existence\nof \"jailbreaks\" that allow the models to be used maliciously. However, there is\nno standard benchmark for measuring the severity of a jailbreak, leaving\nauthors of jailbreak papers to create their own. We show that these benchmarks\noften include vague or unanswerable questions and use grading criteria that are\nbiased towards overestimating the misuse potential of low-quality model\nresponses. Some jailbreak techniques make the problem worse by decreasing the\nquality of model responses even on benign questions: we show that several\njailbreaking techniques substantially reduce the zero-shot performance of GPT-4\non MMLU. Jailbreaks can also make it harder to elicit harmful responses from an\n\"uncensored\" open-source model. We present a new benchmark, StrongREJECT, which\nbetter discriminates between effective and ineffective jailbreaks by using a\nhigher-quality question set and a more accurate response grading algorithm. We\nshow that our new grading scheme better accords with human judgment of response\nquality and overall jailbreak effectiveness, especially on the sort of\nlow-quality responses that contribute the most to over-estimation of jailbreak\nperformance on existing benchmarks. We release our code and data at\nhttps://github.com/alexandrasouly/strongreject.",
        "pdf_link": "https://arxiv.org/pdf/2402.10260v1.pdf"
    },
    {
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "authors": [
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "published": "2024-02-15T18:55:41Z",
        "summary": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding substantially\noutperforms the standard greedy decoding.",
        "pdf_link": "https://arxiv.org/pdf/2402.10200v1.pdf"
    },
    {
        "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents",
        "authors": [
            "Lingbo Mo",
            "Zeyi Liao",
            "Boyuan Zheng",
            "Yu Su",
            "Chaowei Xiao",
            "Huan Sun"
        ],
        "published": "2024-02-15T18:51:32Z",
        "summary": "Language agents powered by large language models (LLMs) have seen exploding\ndevelopment. Their capability of using language as a vehicle for thought and\ncommunication lends an incredible level of flexibility and versatility. People\nhave quickly capitalized on this capability to connect LLMs to a wide range of\nexternal components and environments: databases, tools, the Internet, robotic\nembodiment, etc. Many believe an unprecedentedly powerful automation technology\nis emerging. However, new automation technologies come with new safety risks,\nespecially for intricate systems like language agents. There is a surprisingly\nlarge gap between the speed and scale of their development and deployment and\nour understanding of their safety risks. Are we building a house of cards? In\nthis position paper, we present the first systematic effort in mapping\nadversarial attacks against language agents. We first present a unified\nconceptual framework for agents with three major components: Perception, Brain,\nand Action. Under this framework, we present a comprehensive discussion and\npropose 12 potential attack scenarios against different components of an agent,\ncovering different attack strategies (e.g., input manipulation, adversarial\ndemonstrations, jailbreaking, backdoors). We also draw connections to\nsuccessful attack strategies previously applied to LLMs. We emphasize the\nurgency to gain a thorough understanding of language agent risks before their\nwidespread deployment.",
        "pdf_link": "https://arxiv.org/pdf/2402.10196v1.pdf"
    },
    {
        "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
        "authors": [
            "James Liu",
            "Guangxuan Xiao",
            "Kai Li",
            "Jason D. Lee",
            "Song Han",
            "Tri Dao",
            "Tianle Cai"
        ],
        "published": "2024-02-15T18:50:06Z",
        "summary": "Large Language Models (LLMs) are typically trained in two phases:\npre-training on large internet-scale datasets, and fine-tuning for downstream\ntasks. Given the higher computational demand of pre-training, it's intuitive to\nassume that fine-tuning adds less new information to the model, and is thus\nmore compressible. We explore this assumption by decomposing the weights of\nfine-tuned models into their pre-trained components and an additional delta. We\nintroduce a simple method, BitDelta, which successfully quantizes this delta\ndown to 1 bit without compromising performance. This interesting finding not\nonly highlights the potential redundancy of information added during\nfine-tuning, but also has significant implications for the multi-tenant serving\nand multi-tenant storage of fine-tuned models. By enabling the use of a single\nhigh-precision base model accompanied by multiple 1-bit deltas, BitDelta\ndramatically reduces GPU memory requirements by more than 10x, which can also\nbe translated to enhanced generation latency in multi-tenant settings. We\nvalidate BitDelta through experiments across Llama-2 and Mistral model\nfamilies, and on models up to 70B parameters, showcasing minimal performance\ndegradation over all tested settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.10193v2.pdf"
    },
    {
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Xuchao Zhang",
            "Wei Cheng",
            "Yanchi Liu",
            "Yiyou Sun",
            "Mika Oishi",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Jie Ji",
            "Guangji Bai",
            "Liang Zhao",
            "Haifeng Chen"
        ],
        "published": "2024-02-15T18:46:24Z",
        "summary": "In-context learning has emerged as a groundbreaking ability of Large Language\nModels (LLMs) and revolutionized various fields by providing a few\ntask-relevant demonstrations in the prompt. However, trustworthy issues with\nLLM's response, such as hallucination, have also been actively discussed.\nExisting works have been devoted to quantifying the uncertainty in LLM's\nresponse, but they often overlook the complex nature of LLMs and the uniqueness\nof in-context learning. In this work, we delve into the predictive uncertainty\nof LLMs associated with in-context learning, highlighting that such\nuncertainties may stem from both the provided demonstrations (aleatoric\nuncertainty) and ambiguities tied to the model's configurations (epistemic\nuncertainty). We propose a novel formulation and corresponding estimation\nmethod to quantify both types of uncertainties. The proposed method offers an\nunsupervised way to understand the prediction of in-context learning in a\nplug-and-play fashion. Extensive experiments are conducted to demonstrate the\neffectiveness of the decomposition. The code and data are available at:\nhttps://github.com/lingchen0331/UQ_ICL.",
        "pdf_link": "https://arxiv.org/pdf/2402.10189v2.pdf"
    },
    {
        "title": "Language Models with Conformal Factuality Guarantees",
        "authors": [
            "Christopher Mohri",
            "Tatsunori Hashimoto"
        ],
        "published": "2024-02-15T18:31:53Z",
        "summary": "Guaranteeing the correctness and factuality of language model (LM) outputs is\na major open problem. In this work, we propose conformal factuality, a\nframework that can ensure high probability correctness guarantees for LMs by\nconnecting language modeling and conformal prediction. We observe that the\ncorrectness of an LM output is equivalent to an uncertainty quantification\nproblem, where the uncertainty sets are defined as the entailment set of an\nLM's output. Using this connection, we show that conformal prediction in\nlanguage models corresponds to a back-off algorithm that provides high\nprobability correctness guarantees by progressively making LM outputs less\nspecific (and expanding the associated uncertainty sets). This approach applies\nto any black-box LM and requires very few human-annotated samples. Evaluations\nof our approach on closed book QA (FActScore, NaturalQuestions) and reasoning\ntasks (MATH) show that our approach can provide 80-90% correctness guarantees\nwhile retaining the majority of the LM's original output.",
        "pdf_link": "https://arxiv.org/pdf/2402.10978v1.pdf"
    },
    {
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Moshkov",
            "Sean Narenthiran",
            "Daria Gitman",
            "Fei Jia",
            "Igor Gitman"
        ],
        "published": "2024-02-15T18:26:11Z",
        "summary": "Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.",
        "pdf_link": "https://arxiv.org/pdf/2402.10176v1.pdf"
    },
    {
        "title": "Generative AI and Process Systems Engineering: The Next Frontier",
        "authors": [
            "Benjamin Decardi-Nelson",
            "Abdulelah S. Alshehri",
            "Akshay Ajagekar",
            "Fengqi You"
        ],
        "published": "2024-02-15T18:20:42Z",
        "summary": "This article explores how emerging generative artificial intelligence (GenAI)\nmodels, such as large language models (LLMs), can enhance solution\nmethodologies within process systems engineering (PSE). These cutting-edge\nGenAI models, particularly foundation models (FMs), which are pre-trained on\nextensive, general-purpose datasets, offer versatile adaptability for a broad\nrange of tasks, including responding to queries, image generation, and complex\ndecision-making. Given the close relationship between advancements in PSE and\ndevelopments in computing and systems technologies, exploring the synergy\nbetween GenAI and PSE is essential. We begin our discussion with a compact\noverview of both classic and emerging GenAI models, including FMs, and then\ndive into their applications within key PSE domains: synthesis and design,\noptimization and integration, and process monitoring and control. In each\ndomain, we explore how GenAI models could potentially advance PSE\nmethodologies, providing insights and prospects for each area. Furthermore, the\narticle identifies and discusses potential challenges in fully leveraging GenAI\nwithin PSE, including multiscale modeling, data requirements, evaluation\nmetrics and benchmarks, and trust and safety, thereby deepening the discourse\non effective GenAI integration into systems analysis, design, optimization,\noperations, monitoring, and control. This paper provides a guide for future\nresearch focused on the applications of emerging GenAI in PSE.",
        "pdf_link": "https://arxiv.org/pdf/2402.10977v1.pdf"
    },
    {
        "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
        "authors": [
            "Ali AhmadiTeshnizi",
            "Wenzhi Gao",
            "Madeleine Udell"
        ],
        "published": "2024-02-15T18:19:18Z",
        "summary": "Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the-art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. This paper introduces\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve (mixed integer) linear programming problems from their natural language\ndescriptions. OptiMUS can develop mathematical models, write and debug solver\ncode, evaluate the generated solutions, and improve its model and code based on\nthese evaluations. OptiMUS utilizes a modular structure to process problems,\nallowing it to handle problems with long descriptions and complex data without\nlong prompts. Experiments demonstrate that OptiMUS outperforms existing\nstate-of-the-art methods on easy datasets by more than $20\\%$ and on hard\ndatasets (including a new dataset, NLP4LP, released with this paper that\nfeatures long and complex problems) by more than $30\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2402.10172v1.pdf"
    },
    {
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "authors": [
            "Mahyar Abbasian",
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Pengfei Zhang",
            "Nitish Nagesh",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2024-02-15T18:00:02Z",
        "summary": "Effective diabetes management is crucial for maintaining health in diabetic\npatients. Large Language Models (LLMs) have opened new avenues for diabetes\nmanagement, facilitating their efficacy. However, current LLM-based approaches\nare limited by their dependence on general sources and lack of integration with\ndomain-specific knowledge, leading to inaccurate responses. In this paper, we\npropose a knowledge-infused LLM-powered conversational health agent (CHA) for\ndiabetic patients. We customize and leverage the open-source openCHA framework,\nenhancing our CHA with external knowledge and analytical capabilities. This\nintegration involves two key components: 1) incorporating the American Diabetes\nAssociation dietary guidelines and the Nutritionix information and 2) deploying\nanalytical tools that enable nutritional intake calculation and comparison with\nthe guidelines. We compare the proposed CHA with GPT4. Our evaluation includes\n100 diabetes-related questions on daily meal choices and assessing the\npotential risks associated with the suggested diet. Our findings show that the\nproposed agent demonstrates superior performance in generating responses to\nmanage essential nutrients.",
        "pdf_link": "https://arxiv.org/pdf/2402.10153v2.pdf"
    },
    {
        "title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles",
        "authors": [
            "Yinhong Liu",
            "Yimai Fang",
            "David Vandyke",
            "Nigel Collier"
        ],
        "published": "2024-02-15T17:40:02Z",
        "summary": "In light of recent advances in large language models (LLMs), the expectations\nfor the next generation of virtual assistants include enhanced naturalness and\nadaptability across diverse usage scenarios. However, the creation of\nhigh-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be\nslow and costly. To address these challenges, we introduce Task-Oriented\nAutomatic Dialogs (TOAD), a novel and scalable TOD dataset along with its\nautomatic generation pipeline. The TOAD dataset simulates realistic app context\ninteraction and provide a variety of system response style options. Two aspects\nof system response styles are considered, verbosity level and users' expression\nmirroring. We benchmark TOAD on two response generation tasks and the results\nshow that modelling more verbose or responses without user expression mirroring\nis more challenging.",
        "pdf_link": "https://arxiv.org/pdf/2402.10137v2.pdf"
    },
    {
        "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning",
        "authors": [
            "Ming Li",
            "Lichang Chen",
            "Jiuhai Chen",
            "Shwai He",
            "Jiuxiang Gu",
            "Tianyi Zhou"
        ],
        "published": "2024-02-15T17:06:21Z",
        "summary": "Instruction tuning is critical to large language models (LLMs) for achieving\nbetter instruction following and task adaptation capabilities but its success\nheavily relies on the training data quality. Many recent methods focus on\nimproving the data quality but often overlook the compatibility of the data\nwith the student model being finetuned. This paper introduces Selective\nReflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection\nand introspection for improving existing data quality with the data selection\ncapability of the student LLM, to automatically refine existing\ninstruction-tuning data. This teacher-student collaboration produces\nhigh-quality and student-compatible instruction-response pairs, resulting in\nsample-efficient instruction tuning and LLMs of superior performance. Selective\nReflection-Tuning is a data augmentation and synthesis that generally improves\nLLM finetuning and self-improvement without collecting brand-new data. We apply\nour method to Alpaca and WizardLM data and achieve much stronger and top-tier\n7B and 13B LLMs. Our codes, models, and data will be released at\nhttps://github.com/tianyi-lab/Reflection_Tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.10110v1.pdf"
    },
    {
        "title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction",
        "authors": [
            "Denis Jered McInerney",
            "William Dickinson",
            "Lucy C. Flynn",
            "Andrea C. Young",
            "Geoffrey S. Young",
            "Jan-Willem van de Meent",
            "Byron C. Wallace"
        ],
        "published": "2024-02-15T17:05:48Z",
        "summary": "Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.",
        "pdf_link": "https://arxiv.org/pdf/2402.10109v2.pdf"
    },
    {
        "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
        "authors": [
            "Cheng Kang",
            "Xinye Chen",
            "Yong Hu",
            "Daniel Novak"
        ],
        "published": "2024-02-15T17:02:48Z",
        "summary": "Improving the controllability, portability, and inference speed of diffusion\nlanguage models (DLMs) is a key challenge in natural language generation. While\nrecent research has shown significant success in complex text generation with\nlanguage models, the memory and computational power are still very demanding\nand fall short of expectations, which naturally results in low portability and\ninstability for the models. To mitigate these issues, numerous well-established\nmethods were proposed for neural network quantization. To further enhance their\nportability of independent deployment as well as improve their stability\nevaluated by language perplexity, we propose a novel approach called the\nQuantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM\nbuilds upon the recent successful controllable DLMs by remodeling the\ntask-specific embedding space via quantization. This leads to a gradient-based\ncontroller for the generation tasks, and more stable intermediate latent\nvariables are obtained, which naturally brings in an accelerated convergence as\nwell as better controllability. Additionally, the adaption fine-tuning method\nis employed to reduce tunable weights. Experimental results on five challenging\nfine-grained control tasks demonstrate that QE-CDLM compares favorably to\nexisting methods in terms of quality and feasibility, achieving better\nperplexity and lightweight fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.10107v1.pdf"
    },
    {
        "title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving",
        "authors": [
            "Jiaxin Zhang",
            "Zhongzhi Li",
            "Mingliang Zhang",
            "Fei Yin",
            "Chenglin Liu",
            "Yashar Moshfeghi"
        ],
        "published": "2024-02-15T16:59:41Z",
        "summary": "Recent advancements in Large Language Models (LLMs) and Multi-Modal Models\n(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,\ntheir proficiency in tackling geometry math problems, which necessitates an\nintegrated understanding of both textual and visual information, has not been\nthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,\na comprehensive collection that includes a main subset of 2000 problems, a 750\nproblem subset focusing on backward reasoning, an augmented subset of 2000\nproblems, and a hard subset of 300 problems. This benchmark facilitates a\ndeeper investigation into the performance of LLMs and MMs on solving geometry\nmath problems. Our evaluation of ten LLMs and MMs across these varied subsets\nreveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on\nthe main subset but only a 6.00\\% accuracy on the challenging subset. This\nhighlights the critical need for testing models against datasets on which they\nhave not been pre-trained. Additionally, our findings indicate that GPT-series\nmodels perform more effectively on problems they have rephrased, suggesting a\npromising method for enhancing model capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.10104v1.pdf"
    },
    {
        "title": "Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4",
        "authors": [
            "Ting Fang Tan",
            "Kabilan Elangovan",
            "Liyuan Jin",
            "Yao Jie",
            "Li Yong",
            "Joshua Lim",
            "Stanley Poh",
            "Wei Yan Ng",
            "Daniel Lim",
            "Yuhe Ke",
            "Nan Liu",
            "Daniel Shu Wei Ting"
        ],
        "published": "2024-02-15T16:43:41Z",
        "summary": "Purpose: To assess the alignment of GPT-4-based evaluation to human clinician\nexperts, for the evaluation of responses to ophthalmology-related patient\nqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology\nquestions and paired answers were created by ophthalmologists to represent\ncommonly asked patient questions, divided into fine-tuning (368; 92%), and\ntesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,\nLLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,\nadditional 8 glaucoma QnA pairs were included. 200 responses to the testing\ndataset were generated by 5 fine-tuned LLMs for evaluation. A customized\nclinical evaluation rubric was used to guide GPT-4 evaluation, grounded on\nclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4\nevaluation was then compared against ranking by 5 clinicians for clinical\nalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest\n(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),\nLLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4\nevaluation demonstrated significant agreement with human clinician rankings,\nwith Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80\nrespectively; while correlation based on Cohen Kappa was more modest at 0.50.\nNotably, qualitative analysis and the glaucoma sub-analysis revealed clinical\ninaccuracies in the LLM-generated responses, which were appropriately\nidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignment\nof GPT-4 evaluation highlighted its potential to streamline the clinical\nevaluation of LLM chatbot responses to healthcare-related queries. By\ncomplementing the existing clinician-dependent manual grading, this efficient\nand automated evaluation could assist the validation of future developments in\nLLM applications for healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2402.10083v1.pdf"
    },
    {
        "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence",
        "authors": [
            "Weixiang Zhao",
            "Zhuojun Li",
            "Shilong Wang",
            "Yang Wang",
            "Yulin Hu",
            "Yanyan Zhao",
            "Chen Wei",
            "Bing Qin"
        ],
        "published": "2024-02-15T16:36:04Z",
        "summary": "Emotional Intelligence (EI), consisting of emotion perception, emotion\ncognition and emotion expression, plays the critical roles in improving user\ninteraction experience for the current large language model (LLM) based\nconversational general AI assistants. Previous works mainly focus on raising\nthe emotion perception ability of them via naive fine-tuning on EI-related\nclassification or regression tasks. However, this leads to the incomplete\nenhancement of EI and catastrophic forgetting of the general intelligence (GI).\nTo this end, we first introduce \\textsc{EiBench}, a large-scale collection of\nEI-related tasks in the text-to-text formation with task instructions that\ncovers all three aspects of EI, which lays a solid foundation for the\ncomprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular\n\\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement\nmethod (\\textbf{MoEI}), consisting of Modular Parameter Expansion and\nintra-inter modulation, is proposed to comprehensively enhance the EI of LLMs\nwithout compromise their GI. Extensive experiments on two representative\nLLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness\nof MoEI to improving EI while maintain GI.",
        "pdf_link": "https://arxiv.org/pdf/2402.10073v1.pdf"
    },
    {
        "title": "Towards Safer Large Language Models through Machine Unlearning",
        "authors": [
            "Zheyuan Liu",
            "Guangyao Dou",
            "Zhaoxuan Tan",
            "Yijun Tian",
            "Meng Jiang"
        ],
        "published": "2024-02-15T16:28:34Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated their\nvast potential across various domains, attributed to their extensive\npretraining knowledge and exceptional generalizability. However, LLMs often\nencounter challenges in generating harmful content when faced with problematic\nprompts. To address this problem, existing work attempted to implement a\ngradient ascent based approach to prevent LLMs from producing harmful output.\nWhile these methods can be effective, they frequently impact the model utility\nin responding to normal prompts. To address this gap, we introduce Selective\nKnowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,\ndesigned to eliminate harmful knowledge while preserving utility on normal\nprompts. Specifically, SKU is consisted of two stages: harmful knowledge\nacquisition stage and knowledge negation stage. The first stage aims to\nidentify and acquire harmful knowledge within the model, whereas the second is\ndedicated to remove this knowledge. SKU selectively isolates and removes\nharmful knowledge in model parameters, ensuring the model's performance remains\nrobust on normal prompts. Our experiments conducted across various LLM\narchitectures demonstrate that SKU identifies a good balance point between\nremoving harmful information and preserving utility.",
        "pdf_link": "https://arxiv.org/pdf/2402.10058v1.pdf"
    },
    {
        "title": "Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination",
        "authors": [
            "Yijiang River Dong",
            "Hongzhou Lin",
            "Mikhail Belkin",
            "Ramon Huerta",
            "Ivan Vuli\u0107"
        ],
        "published": "2024-02-15T16:21:14Z",
        "summary": "While displaying impressive generation capabilities across many tasks, Large\nLanguage Models (LLMs) still struggle with crucial issues of privacy violation\nand unwanted exposure of sensitive data. This raises an essential question: how\nshould we prevent such undesired behavior of LLMs while maintaining their\nstrong generation and natural language understanding (NLU) capabilities? In\nthis work, we introduce a novel approach termed deliberate imagination in the\ncontext of LLM unlearning. Instead of trying to forget memorized data, we\nemploy a self-distillation framework, guiding LLMs to deliberately imagine\nalternative scenarios. As demonstrated in a wide range of experiments, the\nproposed method not only effectively unlearns targeted text but also preserves\nthe LLMs' capabilities in open-ended generation tasks as well as in NLU tasks.\nOur results demonstrate the usefulness of this approach across different models\nand sizes, and also with parameter-efficient fine-tuning, offering a novel\npathway to addressing the challenges with private and sensitive data in LLM\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2402.10052v1.pdf"
    },
    {
        "title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting",
        "authors": [
            "Somnath Sendhil Kumar",
            "Dhruv Jain",
            "Eshaan Agarwal",
            "Raunak Pandey"
        ],
        "published": "2024-02-15T16:15:38Z",
        "summary": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in\nfunction-calling, these advancements primarily rely on accessing the functions'\nresponses. This methodology is practical for simpler APIs but faces scalability\nissues with irreversible APIs that significantly impact the system, such as a\ndatabase deletion API. Similarly, processes requiring extensive time for each\nAPI call and those necessitating forward planning, like automated action\npipelines, present complex challenges. Furthermore, scenarios often arise where\na generalized approach is needed because algorithms lack direct access to the\nspecific implementations of these functions or secrets to use them. Traditional\ntool planning methods are inadequate in these cases, compelling the need to\noperate within black-box environments. Unlike their performance in tool\nmanipulation, LLMs excel in black-box tasks, such as program synthesis.\nTherefore, we harness the program synthesis capabilities of LLMs to strategize\ntool usage in black-box settings, ensuring solutions are verified prior to\nimplementation. We introduce TOPGUN, an ingeniously crafted approach leveraging\nprogram synthesis for black box tool planning. Accompanied by SwissNYF, a\ncomprehensive suite that integrates black-box algorithms for planning and\nverification tasks, addressing the aforementioned challenges and enhancing the\nversatility and effectiveness of LLMs in complex API interactions. The public\ncode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.",
        "pdf_link": "https://arxiv.org/pdf/2402.10051v1.pdf"
    },
    {
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models",
        "authors": [
            "Saeed Khaki",
            "JinJin Li",
            "Lan Ma",
            "Liu Yang",
            "Prathap Ramachandra"
        ],
        "published": "2024-02-15T16:00:58Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.10038v2.pdf"
    },
    {
        "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
        "authors": [
            "Yaoyiran Li",
            "Anna Korhonen",
            "Ivan Vuli\u0107"
        ],
        "published": "2024-02-15T15:43:05Z",
        "summary": "Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.",
        "pdf_link": "https://arxiv.org/pdf/2402.10024v1.pdf"
    },
    {
        "title": "LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition",
        "authors": [
            "Jinyuan Li",
            "Han Li",
            "Di Sun",
            "Jiahao Wang",
            "Wenkun Zhang",
            "Zan Wang",
            "Gang Pan"
        ],
        "published": "2024-02-15T14:54:33Z",
        "summary": "Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nModule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.09989v3.pdf"
    },
    {
        "title": "Case Study: Testing Model Capabilities in Some Reasoning Tasks",
        "authors": [
            "Min Zhang",
            "Sato Takumi",
            "Jack Zhang",
            "Jun Wang"
        ],
        "published": "2024-02-15T14:21:30Z",
        "summary": "Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.09967v1.pdf"
    },
    {
        "title": "Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation",
        "authors": [
            "Jiashu Pu",
            "Yajing Wan",
            "Yuru Zhang",
            "Jing Chen",
            "Ling Cheng",
            "Qian Shao",
            "Yongzhu Chang",
            "Tangjie Lv",
            "Rongsheng Zhang"
        ],
        "published": "2024-02-15T14:03:33Z",
        "summary": "Previous in-context learning (ICL) research has focused on tasks such as\nclassification, machine translation, text2table, etc., while studies on whether\nICL can improve human-like dialogue generation are scarce. Our work fills this\ngap by systematically investigating the ICL capabilities of large language\nmodels (LLMs) in persona-based dialogue generation, conducting extensive\nexperiments on high-quality real human Chinese dialogue datasets. From\nexperimental results, we draw three conclusions: 1) adjusting prompt\ninstructions is the most direct, effective, and economical way to improve\ngeneration quality; 2) randomly retrieving demonstrations (demos) achieves the\nbest results, possibly due to the greater diversity and the amount of effective\ninformation; counter-intuitively, retrieving demos with a context identical to\nthe query performs the worst; 3) even when we destroy the multi-turn\nassociations and single-turn semantics in the demos, increasing the number of\ndemos still improves dialogue performance, proving that LLMs can learn from\ncorrupted dialogue demos. Previous explanations of the ICL mechanism, such as\n$n$-gram induction head, cannot fully account for this phenomenon.",
        "pdf_link": "https://arxiv.org/pdf/2402.09954v2.pdf"
    },
    {
        "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
        "authors": [
            "Ridwan Taiwo",
            "Idris Temitope Bello",
            "Sulemana Fatoama Abdulai",
            "Abdul-Mugis Yussif",
            "Babatunde Abiodun Salami",
            "Abdullahi Saka",
            "Tarek Zayed"
        ],
        "published": "2024-02-15T13:39:55Z",
        "summary": "The construction industry is a vital sector of the global economy, but it\nfaces many productivity challenges in various processes, such as design,\nplanning, procurement, inspection, and maintenance. Generative artificial\nintelligence (AI), which can create novel and realistic data or content, such\nas text, image, video, or code, based on some input or prior knowledge, offers\ninnovative and disruptive solutions to address these challenges. However, there\nis a gap in the literature on the current state, opportunities, and challenges\nof generative AI in the construction industry. This study aims to fill this gap\nby providing a state-of-the-art analysis of generative AI in construction, with\nthree objectives: (1) to review and categorize the existing and emerging\ngenerative AI opportunities and challenges in the construction industry; (2) to\npropose a framework for construction firms to build customized generative AI\nsolutions using their own data, comprising steps such as data collection,\ndataset curation, training custom large language model (LLM), model evaluation,\nand deployment; and (3) to demonstrate the framework via a case study of\ndeveloping a generative model for querying contract documents. The results show\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\nprovides academics and construction professionals with a comprehensive analysis\nand practical framework to guide the adoption of generative AI techniques to\nenhance productivity, quality, safety, and sustainability across the\nconstruction industry.",
        "pdf_link": "https://arxiv.org/pdf/2402.09939v1.pdf"
    },
    {
        "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
        "authors": [
            "Jiaxiang Liu",
            "Tong Zhou",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2024-02-15T12:20:02Z",
        "summary": "Mitigating the hallucinations of Large Language Models (LLMs) and enhancing\nthem is a crucial task. Although some existing methods employ model\nself-enhancement techniques, they fall short of effectively addressing unknown\nfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails\nto address the generalization across different KG sources and the enhancement\nof open-ended answer questions simultaneously. To tackle these limitations,\nthere is a framework that combines Pseudo-Graph Generation and Atomic Knowledge\nVerification proposed. The enhancement of LLM using KG in an open-ended\nquestion-answering setting is implemented by leveraging the Pseudo-Graph\nGeneration. Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,\nwe observe a minimum accuracy improvement of 7.5. Moreover, there is also\ndemonstration that this framework exhibits generalizability across different KG\nsources. In summary, our results pave the way for enhancing LLMs by\nincorporating Pseudo- and Multisource-KGs, particularly in the context of\nopen-ended questions.",
        "pdf_link": "https://arxiv.org/pdf/2402.09911v1.pdf"
    },
    {
        "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
        "authors": [
            "Andr\u00e9 V. Duarte",
            "Xuandong Zhao",
            "Arlindo L. Oliveira",
            "Lei Li"
        ],
        "published": "2024-02-15T12:17:15Z",
        "summary": "How can we detect if copyrighted content was used in the training process of\na language model, considering that the training data is typically undisclosed?\nWe are motivated by the premise that a language model is likely to identify\nverbatim excerpts from its training text. We propose DE-COP, a method to\ndetermine whether a piece of copyrighted content was included in training.\nDE-COP's core approach is to probe an LLM with multiple-choice questions, whose\noptions include both verbatim text and their paraphrases. We construct\nBookTection, a benchmark with excerpts from 165 books published prior and\nsubsequent to a model's training cutoff, along with their paraphrases. Our\nexperiments show that DE-COP surpasses the prior best method by 9.6% in\ndetection performance (AUC) on models with logits available. Moreover, DE-COP\nalso achieves an average accuracy of 72% for detecting suspect books on fully\nblack-box models where prior methods give $\\approx$ 4% accuracy. Our code and\ndatasets are available at https://github.com/avduarte333/DE-COP_Method",
        "pdf_link": "https://arxiv.org/pdf/2402.09910v1.pdf"
    },
    {
        "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence",
        "authors": [
            "Timothy R. McIntosh",
            "Teo Susnjak",
            "Tong Liu",
            "Paul Watters",
            "Malka N. Halgamuge"
        ],
        "published": "2024-02-15T11:08:10Z",
        "summary": "The rapid rise in popularity of Large Language Models (LLMs) with emerging\ncapabilities has spurred public curiosity to evaluate and compare different\nLLMs, leading many researchers to propose their LLM benchmarks. Noticing\npreliminary inadequacies in those benchmarks, we embarked on a study to\ncritically assess 23 state-of-the-art LLM benchmarks, using our novel unified\nevaluation framework through the lenses of people, process, and technology,\nunder the pillars of functionality and security. Our research uncovered\nsignificant limitations, including biases, difficulties in measuring genuine\nreasoning, adaptability, implementation inconsistencies, prompt engineering\ncomplexity, evaluator diversity, and the overlooking of cultural and\nideological norms in one comprehensive assessment. Our discussions emphasized\nthe urgent need for standardized methodologies, regulatory certainties, and\nethical guidelines in light of Artificial Intelligence (AI) advancements,\nincluding advocating for an evolution from static benchmarks to dynamic\nbehavioral profiling to accurately capture LLMs' complex behaviors and\npotential risks. Our study highlighted the necessity for a paradigm shift in\nLLM evaluation methodologies, underlining the importance of collaborative\nefforts for the development of universally accepted benchmarks and the\nenhancement of AI systems' integration into society.",
        "pdf_link": "https://arxiv.org/pdf/2402.09880v1.pdf"
    },
    {
        "title": "Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks",
        "authors": [
            "\u00c1lvaro Huertas-Garc\u00eda",
            "Alejandro Mart\u00edn",
            "Javier Huertas-Tato",
            "David Camacho"
        ],
        "published": "2024-02-15T10:58:22Z",
        "summary": "Adversarial attacks represent a substantial challenge in Natural Language\nProcessing (NLP). This study undertakes a systematic exploration of this\nchallenge in two distinct phases: vulnerability evaluation and resilience\nenhancement of Transformer-based models under adversarial attacks.\n  In the evaluation phase, we assess the susceptibility of three Transformer\nconfigurations, encoder-decoder, encoder-only, and decoder-only setups, to\nadversarial attacks of escalating complexity across datasets containing\noffensive language and misinformation. Encoder-only models manifest a 14% and\n21% performance drop in offensive language detection and misinformation\ndetection tasks, respectively. Decoder-only models register a 16% decrease in\nboth tasks, while encoder-decoder models exhibit a maximum performance drop of\n14% and 26% in the respective tasks.\n  The resilience-enhancement phase employs adversarial training, integrating\npre-camouflaged and dynamically altered data. This approach effectively reduces\nthe performance drop in encoder-only models to an average of 5% in offensive\nlanguage detection and 2% in misinformation detection tasks. Decoder-only\nmodels, occasionally exceeding original performance, limit the performance drop\nto 7% and 2% in the respective tasks. Although not surpassing the original\nperformance, Encoder-decoder models can reduce the drop to an average of 6% and\n2% respectively.\n  Results suggest a trade-off between performance and robustness, with some\nmodels maintaining similar performance while gaining robustness. Our study and\nadversarial training techniques have been incorporated into an open-source tool\nfor generating camouflaged datasets. However, methodology effectiveness depends\non the specific camouflage technique and data encountered, emphasizing the need\nfor continued exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.09874v1.pdf"
    },
    {
        "title": "MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music",
        "authors": [
            "Zihao Wang",
            "Shuyu Li",
            "Tao Zhang",
            "Qi Wang",
            "Pengfei Yu",
            "Jinyang Luo",
            "Yan Liu",
            "Ming Xi",
            "Kejun Zhang"
        ],
        "published": "2024-02-15T10:55:01Z",
        "summary": "The rapidly evolving multimodal Large Language Models (LLMs) urgently require\nnew benchmarks to uniformly evaluate their performance on understanding and\ntextually describing music. However, due to semantic gaps between Music\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\nbetween professionals and the public, and low precision of annotations,\nexisting music description datasets cannot serve as benchmarks. To this end, we\npresent MuChin, the first open-source music description benchmark in Chinese\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\nunderstanding and describing music. We established the Caichong Music\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\nmulti-stage assurance method, and recruited both amateurs and professionals to\nensure the precision of annotations and alignment with popular semantics.\nUtilizing this method, we built a dataset with multi-dimensional,\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\ncarefully selected 1,000 high-quality entries to serve as the test set for\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\nand amateurs in terms of music description, and empirically demonstrated the\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\nMuChin to evaluate existing music understanding models on their ability to\nprovide colloquial descriptions of music. All data related to the benchmark and\nthe code for scoring have been open-sourced.",
        "pdf_link": "https://arxiv.org/pdf/2402.09871v2.pdf"
    },
    {
        "title": "LAPDoc: Layout-Aware Prompting for Documents",
        "authors": [
            "Marcel Lamott",
            "Yves-Noel Weweler",
            "Adrian Ulges",
            "Faisal Shafait",
            "Dirk Krechel",
            "Darko Obradovic"
        ],
        "published": "2024-02-15T10:00:49Z",
        "summary": "Recent advances in training large language models (LLMs) using massive\namounts of solely textual data lead to strong generalization across many\ndomains and tasks, including document-specific tasks. Opposed to that there is\na trend to train multi-modal transformer architectures tailored for document\nunderstanding that are designed specifically to fuse textual inputs with the\ncorresponding document layout. This involves a separate fine-tuning step for\nwhich additional training data is required. At present, no document\ntransformers with comparable generalization to LLMs are available That raises\nthe question which type of model is to be preferred for document understanding\ntasks. In this paper we investigate the possibility to use purely text-based\nLLMs for document-specific tasks by using layout enrichment. We explore drop-in\nmodifications and rule-based methods to enrich purely textual LLM prompts with\nlayout information. In our experiments we investigate the effects on the\ncommercial ChatGPT model and the open-source LLM Solar. We demonstrate that\nusing our approach both LLMs show improved performance on various standard\ndocument benchmarks. In addition, we study the impact of noisy OCR and layout\nerrors, as well as the limitations of LLMs when it comes to utilizing document\nlayout. Our results indicate that layout enrichment can improve the performance\nof purely text-based LLMs for document understanding by up to 15% compared to\njust using plain document text. In conclusion, this approach should be\nconsidered for the best model choice between text-based LLM or multi-modal\ndocument transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.09841v1.pdf"
    },
    {
        "title": "EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Shangyu Xing",
            "Fei Zhao",
            "Zhen Wu",
            "Tuo An",
            "Weihao Chen",
            "Chunhui Li",
            "Jianbing Zhang",
            "Xinyu Dai"
        ],
        "published": "2024-02-15T08:58:03Z",
        "summary": "Multimodal large language models (MLLMs) have attracted increasing attention\nin the past few years, but they may still generate descriptions that include\nobjects not present in the corresponding images, a phenomenon known as object\nhallucination. To eliminate hallucinations, existing methods manually annotate\npaired responses with and without hallucinations, and then employ various\nalignment algorithms to improve the alignment capability between images and\ntext. However, they not only demand considerable computation resources during\nthe finetuning stage but also require expensive human annotation to construct\npaired data needed by the alignment algorithms. To address these issues, we\nborrow the idea of unlearning and propose an efficient fine-grained unlearning\nframework (EFUF), which can eliminate hallucinations without the need for\npaired data. Extensive experiments show that our method consistently reduces\nhallucinations while preserving the generation quality with modest\ncomputational overhead. Our code and datasets will be publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2402.09801v1.pdf"
    },
    {
        "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
        "authors": [
            "Shengrui Li",
            "Xueting Han",
            "Jing Bai"
        ],
        "published": "2024-02-15T08:03:12Z",
        "summary": "The considerable size of Large Language Models (LLMs) presents notable\ndeployment challenges, particularly on resource-constrained hardware.\nStructured pruning, offers an effective means to compress LLMs, thereby\nreducing storage costs and enhancing inference speed for more efficient\nutilization. In this work, we study data-efficient and resource-efficient\nstructure pruning methods to obtain smaller yet still powerful models.\nKnowledge Distillation is well-suited for pruning, as the intact model can\nserve as an excellent teacher for pruned students. However, it becomes\nchallenging in the context of LLMs due to memory constraints. To address this,\nwe propose an efficient progressive Numerous-teacher pruning method\n(NutePrune). NutePrune mitigates excessive memory costs by loading only one\nintact model and integrating it with various masks and LoRA modules, enabling\nit to seamlessly switch between teacher and student roles. This approach allows\nus to leverage numerous teachers with varying capacities to progressively guide\nthe pruned model, enhancing overall performance. Extensive experiments across\nvarious tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot\nexperiments, NutePrune retains 97.17% of the performance of the original model\nat 20% sparsity and 95.07% at 25% sparsity.",
        "pdf_link": "https://arxiv.org/pdf/2402.09773v1.pdf"
    },
    {
        "title": "Aligning Crowd Feedback via Distributional Preference Reward Modeling",
        "authors": [
            "Dexun Li",
            "Cong Zhang",
            "Kuicai Dong",
            "Derrick Goh Xin Deik",
            "Ruiming Tang",
            "Yong Liu"
        ],
        "published": "2024-02-15T07:29:43Z",
        "summary": "Deep Reinforcement Learning is widely used for aligning Large Language Models\n(LLM) with human preference. However, the conventional reward modelling has\npredominantly depended on human annotations provided by a select cohort of\nindividuals. Such dependence may unintentionally result in models that are\nskewed to reflect the inclinations of these annotators, thereby failing to\nrepresent the expectations of the wider population adequately. In this paper,\nwe introduce the Distributional Preference Reward Model (DPRM), a simple yet\neffective framework to align large language models with a diverse set of human\npreferences. To this end, we characterize the preferences by a beta\ndistribution, which can dynamically adapt to fluctuations in preference trends.\nOn top of that, we design an optimal-transportation-based loss to calibrate\nDPRM to align with the preference distribution. Finally, the expected reward is\nutilized to fine-tune an LLM policy to generate responses favoured by the\npopulation. Our experiments show that DPRM significantly enhances the alignment\nof LLMs with population preference, yielding more accurate, unbiased, and\ncontextually appropriate responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.09764v2.pdf"
    },
    {
        "title": "Grounding Language Model with Chunking-Free In-Context Retrieval",
        "authors": [
            "Hongjin Qian",
            "Zheng Liu",
            "Kelong Mao",
            "Yujia Zhou",
            "Zhicheng Dou"
        ],
        "published": "2024-02-15T07:22:04Z",
        "summary": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval\napproach, specifically tailored for Retrieval-Augmented Generation (RAG)\nsystems. Traditional RAG systems often struggle with grounding responses using\nprecise evidence text due to the challenges of processing lengthy documents and\nfiltering out irrelevant content. Commonly employed solutions, such as document\nchunking and adapting language models to handle longer contexts, have their\nlimitations. These methods either disrupt the semantic coherence of the text or\nfail to effectively address the issues of noise and inaccuracy in evidence\nretrieval.\n  CFIC addresses these challenges by circumventing the conventional chunking\nprocess. It utilizes the encoded hidden states of documents for in-context\nretrieval, employing auto-aggressive decoding to accurately identify the\nspecific evidence text required for user queries, eliminating the need for\nchunking. CFIC is further enhanced by incorporating two decoding strategies,\nnamely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies\nnot only improve the efficiency of the retrieval process but also ensure that\nthe fidelity of the generated grounding text evidence is maintained. Our\nevaluations of CFIC on a range of open QA datasets demonstrate its superiority\nin retrieving relevant and accurate evidence, offering a significant\nimprovement over traditional methods. By doing away with the need for document\nchunking, CFIC presents a more streamlined, effective, and efficient retrieval\nsolution, making it a valuable advancement in the field of RAG systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.09760v1.pdf"
    },
    {
        "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish",
        "authors": [
            "Szymon Ruci\u0144ski"
        ],
        "published": "2024-02-15T07:17:10Z",
        "summary": "This study explores the potential of fine-tuning foundational English Large\nLanguage Models (LLMs) for generating Polish text. The first step involves\nLanguage Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,\nconsisting of 276 million Polish tokens. The LAPT is followed by additional\nfine-tuning aimed at solving nine KLEJ challenges. Our trained model\nCurie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02\namong decoder-based Polish models but also closely rivals the performance of\nthe best Polish encoder-decoder models with a less than 2% gap on 8 out of 9\ntasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn\nPolish. The LAPT was completed in less than five days using a consumer GPU,\nhighlighting the method's efficiency. The proficiency of the model in Polish\nwas significantly enhanced, demonstrating the viability of this approach for\nadding new languages to existing LLMs by training just 1.2% of its parameters.\nTo contribute to the community's collaborative progress, the model has been\nreleased as open-source.",
        "pdf_link": "https://arxiv.org/pdf/2402.09759v1.pdf"
    },
    {
        "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
        "authors": [
            "Wenxiao Wang",
            "Wei Chen",
            "Yicong Luo",
            "Yongliu Long",
            "Zhengkai Lin",
            "Liye Zhang",
            "Binbin Lin",
            "Deng Cai",
            "Xiaofei He"
        ],
        "published": "2024-02-15T06:58:30Z",
        "summary": "Transformer based large language models have achieved tremendous success.\nHowever, the significant memory and computational costs incurred during the\ninference process make it challenging to deploy large models on\nresource-constrained devices. In this paper, we investigate compression and\nefficient inference methods for large language models from an algorithmic\nperspective. Regarding taxonomy, similar to smaller models, compression and\nacceleration algorithms for large language models can still be categorized into\nquantization, pruning, distillation, compact architecture design, dynamic\nnetworks. However, Large language models have two prominent characteristics\ncompared to smaller models: (1) Most of compression algorithms require\nfinetuning or even retraining the model after compression. The most notable\naspect of large models is the very high cost associated with model finetuning\nor training. Therefore, many algorithms for large models, such as quantization\nand pruning, start to explore tuning-free algorithms. (2) Large models\nemphasize versatility and generalization rather than performance on a single\ntask. Hence, many algorithms, such as knowledge distillation, focus on how to\npreserving their versatility and generalization after compression. Since these\ntwo characteristics were not very pronounced in early large models, we further\ndistinguish large language models into medium models and ``real'' large models.\nAdditionally, we also provide an introduction to some mature frameworks for\nefficient inference of large models, which can support basic compression or\nacceleration algorithms, greatly facilitating model deployment for users.",
        "pdf_link": "https://arxiv.org/pdf/2402.09748v1.pdf"
    },
    {
        "title": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
        "authors": [
            "Zhihao Fan",
            "Jialong Tang",
            "Wei Chen",
            "Siyuan Wang",
            "Zhongyu Wei",
            "Jun Xi",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2024-02-15T06:46:48Z",
        "summary": "The incorporation of Large Language Models (LLMs) in healthcare marks a\nsignificant advancement. However, the application has predominantly been\nlimited to discriminative and question-answering tasks, which does not fully\nleverage their interactive potential. To address this limitation, our paper\npresents AI Hospital, a framework designed to build a real-time interactive\ndiagnosis environment. To simulate the procedure, we collect high-quality\nmedical records to create patient, examiner, and medical director agents. AI\nHospital is then utilized for the interactive evaluation and collaboration of\nLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark\nwhere various LLMs serve as intern doctors for interactive diagnosis.\nSubsequently, to improve diagnostic accuracy, we introduce a collaborative\nmechanism that involves iterative discussions and a dispute resolution process\nunder the supervision of the medical director. In our experiments, we validate\nthe reliability of AI Hospital. The results not only explore the feasibility of\napply LLMs in clinical consultation but also confirm the effectiveness of the\ndispute resolution focused collaboration method.",
        "pdf_link": "https://arxiv.org/pdf/2402.09742v2.pdf"
    },
    {
        "title": "Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data",
        "authors": [
            "Haoyang Liu",
            "Yijiang Li",
            "Jinglin Jian",
            "Yuxuan Cheng",
            "Jianrong Lu",
            "Shuyi Guo",
            "Jinglei Zhu",
            "Mianchen Zhang",
            "Miantong Zhang",
            "Haohan Wang"
        ],
        "published": "2024-02-15T06:30:12Z",
        "summary": "Machine learning has emerged as a powerful tool for scientific discovery,\nenabling researchers to extract meaningful insights from complex datasets. For\ninstance, it has facilitated the identification of disease-predictive genes\nfrom gene expression data, significantly advancing healthcare. However, the\ntraditional process for analyzing such datasets demands substantial human\neffort and expertise for the data selection, processing, and analysis. To\naddress this challenge, we introduce a novel framework, a Team of AI-made\nScientists (TAIS), designed to streamline the scientific discovery pipeline.\nTAIS comprises simulated roles, including a project manager, data engineer, and\ndomain expert, each represented by a Large Language Model (LLM). These roles\ncollaborate to replicate the tasks typically performed by data scientists, with\na specific focus on identifying disease-predictive genes. Furthermore, we have\ncurated a benchmark dataset to assess TAIS's effectiveness in gene\nidentification, demonstrating our system's potential to significantly enhance\nthe efficiency and scope of scientific exploration. Our findings represent a\nsolid step towards automating scientific discovery through large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.12391v2.pdf"
    },
    {
        "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States",
        "authors": [
            "Hanyu Duan",
            "Yi Yang",
            "Kar Yan Tam"
        ],
        "published": "2024-02-15T06:14:55Z",
        "summary": "Large Language Models (LLMs) can make up answers that are not real, and this\nis known as hallucination. This research aims to see if, how, and to what\nextent LLMs are aware of hallucination. More specifically, we check whether and\nhow an LLM reacts differently in its hidden states when it answers a question\nright versus when it hallucinates. To do this, we introduce an experimental\nframework which allows examining LLM's hidden states in different hallucination\nsituations. Building upon this framework, we conduct a series of experiments\nwith language models in the LLaMA family (Touvron et al., 2023). Our empirical\nfindings suggest that LLMs react differently when processing a genuine response\nversus a fabricated one. We then apply various model interpretation techniques\nto help understand and explain the findings better. Moreover, informed by the\nempirical observations, we show great potential of using the guidance derived\nfrom LLM's hidden representation space to mitigate hallucination. We believe\nthis work provides insights into how LLMs produce hallucinated answers and how\nto make them occur less often.",
        "pdf_link": "https://arxiv.org/pdf/2402.09733v1.pdf"
    },
    {
        "title": "AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns",
        "authors": [
            "Ashfak Md Shibli",
            "Mir Mehedi A. Pritom",
            "Maanak Gupta"
        ],
        "published": "2024-02-15T05:49:22Z",
        "summary": "SMS phishing, also known as \"smishing\", is a growing threat that tricks users\ninto disclosing private information or clicking into URLs with malicious\ncontent through fraudulent mobile text messages. In recent past, we have also\nobserved a rapid advancement of conversational generative AI chatbot services\n(e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large\nlanguage models (LLMs). These AI chatbots certainly have a lot of utilities but\nit is not systematically understood how they can play a role in creating\nthreats and attacks. In this paper, we propose AbuseGPT method to show how the\nexisting generative AI-based chatbot services can be exploited by attackers in\nreal world to create smishing texts and eventually lead to craftier smishing\ncampaigns. To the best of our knowledge, there is no pre-existing work that\nevidently shows the impacts of these generative text-based models on creating\nSMS phishing. Thus, we believe this study is the first of its kind to shed\nlight on this emerging cybersecurity threat. We have found strong empirical\nevidences to show that attackers can exploit ethical standards in the existing\ngenerative AI-based chatbot services by crafting prompt injection attacks to\ncreate newer smishing campaigns. We also discuss some future research\ndirections and guidelines to protect the abuse of generative AI-based services\nand safeguard users from smishing attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.09728v1.pdf"
    },
    {
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "authors": [
            "Kuang-Huei Lee",
            "Xinyun Chen",
            "Hiroki Furuta",
            "John Canny",
            "Ian Fischer"
        ],
        "published": "2024-02-15T05:40:21Z",
        "summary": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3-20x.",
        "pdf_link": "https://arxiv.org/pdf/2402.09727v2.pdf"
    },
    {
        "title": "Best Arm Identification for Prompt Learning under a Limited Budget",
        "authors": [
            "Chengshuai Shi",
            "Kun Yang",
            "Jing Yang",
            "Cong Shen"
        ],
        "published": "2024-02-15T05:31:13Z",
        "summary": "The remarkable instruction-following capability of large language models\n(LLMs) has sparked a growing interest in automatically learning suitable\nprompts. However, while many effective methods have been proposed, the cost\nincurred during the learning process (e.g., accessing LLM and evaluating the\nresponses) has not been considered. To overcome this limitation, this work\nexplicitly incorporates a finite budget constraint into prompt learning.\nTowards developing principled solutions, a novel connection is established\nbetween prompt learning and fixed-budget best arm identification (BAI-FB) in\nmulti-armed bandits (MAB). Based on this connection, a general framework TRIPLE\n(besT aRm Identification for Prompt LEarning) is proposed to harness the power\nof BAI-FB in prompt learning systematically. Unique characteristics of prompt\nlearning further lead to two embedding-based enhancements of TRIPLE by\nexploiting the ideas of clustering and function approximation. Extensive\nexperiments on multiple well-adopted tasks using both GPT 3.5 and Llama2\ndemonstrate the significant performance improvement of TRIPLE over the previous\nbaselines while satisfying the limited budget constraints.",
        "pdf_link": "https://arxiv.org/pdf/2402.09723v2.pdf"
    },
    {
        "title": "PAL: Proxy-Guided Black-Box Attack on Large Language Models",
        "authors": [
            "Chawin Sitawarin",
            "Norman Mu",
            "David Wagner",
            "Alexandre Araujo"
        ],
        "published": "2024-02-15T02:54:49Z",
        "summary": "Large Language Models (LLMs) have surged in popularity in recent months, but\nthey have demonstrated concerning capabilities to generate harmful content when\nmanipulated. While techniques like safety fine-tuning aim to minimize harmful\nuse, recent works have shown that LLMs remain vulnerable to attacks that elicit\ntoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs\n(PAL), the first optimization-based attack on LLMs in a black-box query-only\nsetting. In particular, it relies on a surrogate model to guide the\noptimization and a sophisticated loss designed for real-world LLM APIs. Our\nattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on\nLlama-2-7B, compared to 4% for the current state of the art. We also propose\nGCG++, an improvement to the GCG attack that reaches 94% ASR on white-box\nLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple\nbaseline for query-based attacks. We believe the techniques proposed in this\nwork will enable more comprehensive safety testing of LLMs and, in the long\nterm, the development of better security guardrails. The code can be found at\nhttps://github.com/chawins/pal.",
        "pdf_link": "https://arxiv.org/pdf/2402.09674v1.pdf"
    },
    {
        "title": "How to Train Data-Efficient LLMs",
        "authors": [
            "Noveen Sachdeva",
            "Benjamin Coleman",
            "Wang-Cheng Kang",
            "Jianmo Ni",
            "Lichan Hong",
            "Ed H. Chi",
            "James Caverlee",
            "Julian McAuley",
            "Derek Zhiyuan Cheng"
        ],
        "published": "2024-02-15T02:27:57Z",
        "summary": "The training of large language models (LLMs) is expensive. In this paper, we\nstudy data-efficient approaches for pre-training LLMs, i.e., techniques that\naim to optimize the Pareto frontier of model quality and training resource/data\nconsumption. We seek to understand the tradeoffs associated with data selection\nroutines based on (i) expensive-to-compute data-quality estimates, and (ii)\nmaximization of coverage and diversity-based measures in the feature space. Our\nfirst technique, Ask-LLM, leverages the zero-shot reasoning capabilities of\ninstruction-tuned LLMs to directly assess the quality of a training example. To\ntarget coverage, we propose Density sampling, which models the data\ndistribution to select a diverse sample. In our comparison of 19 samplers,\ninvolving hundreds of evaluation tasks and pre-training runs, we find that\nAsk-LLM and Density are the best methods in their respective categories.\nCoverage sampling can recover the performance of the full data, while models\ntrained on Ask-LLM data consistently outperform full-data training -- even when\nwe reject 90% of the original dataset, while converging up to 70% faster.",
        "pdf_link": "https://arxiv.org/pdf/2402.09668v1.pdf"
    },
    {
        "title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning",
        "authors": [
            "Changshu Liu",
            "Shizhuo Dylan Zhang",
            "Ali Reza Ibrahimzada",
            "Reyhaneh Jabbarvand"
        ],
        "published": "2024-02-15T02:24:46Z",
        "summary": "Solely relying on test passing to evaluate Large Language Models (LLMs) for\ncode synthesis may result in unfair assessment or promoting models with data\nleakage. As an alternative, we introduce CodeMind, a framework designed to\ngauge the code reasoning abilities of LLMs. CodeMind currently supports three\ncode reasoning tasks: Independent Execution Reasoning (IER), Dependent\nExecution Reasoning (DER), and Specification Reasoning (SR). The first two\nevaluate models to predict the execution output of an arbitrary code or code\nthe model could correctly synthesize. The third one evaluates the extent to\nwhich LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different\nprogramming languages using CodeMind shows that LLMs fairly follow control flow\nconstructs and, in general, explain how inputs evolve to output, specifically\nfor simple programs and the ones they can correctly synthesize. However, their\nperformance drops for code with higher complexity, non-trivial logical and\narithmetic operators, non-primitive types, and API calls. Furthermore, we\nobserve that, while correlated, specification reasoning (essential for code\nsynthesis) does not imply execution reasoning (essential for broader\nprogramming tasks such as testing and debugging): ranking LLMs based on test\npassing can be different compared to code reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.09664v4.pdf"
    },
    {
        "title": "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
        "authors": [
            "Wanli Yang",
            "Fei Sun",
            "Xinyu Ma",
            "Xun Liu",
            "Dawei Yin",
            "Xueqi Cheng"
        ],
        "published": "2024-02-15T01:50:38Z",
        "summary": "Although model editing has shown promise in revising knowledge in Large\nLanguage Models (LLMs), its impact on the inherent capabilities of LLMs is\noften overlooked. In this work, we reveal a critical phenomenon: even a single\nedit can trigger model collapse, manifesting as significant performance\ndegradation in various benchmark tasks. However, benchmarking LLMs after each\nedit, while necessary to prevent such collapses, is impractically\ntime-consuming and resource-intensive. To mitigate this, we propose using\nperplexity as a surrogate metric, validated by extensive experiments\ndemonstrating its strong correlation with downstream tasks performance. We\nfurther conduct an in-depth study on sequential editing, a practical setting\nfor real-world scenarios, across various editing methods and LLMs, focusing on\nhard cases from our previous single edit studies. The results indicate that\nnearly all examined editing methods result in model collapse after only few\nedits. To facilitate further research, we have utilized GPT-3.5 to develop a\nnew dataset, HardEdit, based on those hard cases. This dataset aims to\nestablish the foundation for pioneering research in reliable model editing and\nthe mechanisms underlying editing-induced model collapse. We hope this work can\ndraw the community's attention to the potential risks inherent in model editing\npractices.",
        "pdf_link": "https://arxiv.org/pdf/2402.09656v3.pdf"
    },
    {
        "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
        "authors": [
            "Chao Wang",
            "Hehe Fan",
            "Ruijie Quan",
            "Yi Yang"
        ],
        "published": "2024-02-15T01:22:30Z",
        "summary": "Protein research is crucial in various fundamental disciplines, but\nunderstanding their intricate structure-function relationships remains\nchallenging. Recent Large Language Models (LLMs) have made significant strides\nin comprehending task-specific knowledge, suggesting the potential for\nChatGPT-like systems specialized in protein to facilitate basic research. In\nthis work, we introduce ProtChatGPT, which aims at learning and understanding\nprotein structures via natural languages. ProtChatGPT enables users to upload\nproteins, ask questions, and engage in interactive conversations to produce\ncomprehensive answers. The system comprises protein encoders, a\nProtein-Language Pertaining Transformer (PLP-former), a projection adapter, and\nan LLM. The protein first undergoes protein encoders and PLP-former to produce\nprotein embeddings, which are then projected by the adapter to conform with the\nLLM. The LLM finally combines user questions with projected embeddings to\ngenerate informative answers. Experiments show that ProtChatGPT can produce\npromising responses to proteins and their corresponding questions. We hope that\nProtChatGPT could form the basis for further exploration and application in\nprotein research. Code and our pre-trained model will be publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2402.09649v1.pdf"
    },
    {
        "title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question",
        "authors": [
            "Letian Peng",
            "Yuwei Zhang",
            "Zilong Wang",
            "Jayanth Srinivasa",
            "Gaowen Liu",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2024-02-15T01:02:41Z",
        "summary": "This work aims to build a text embedder that can capture characteristics of\ntexts specified by user instructions. Despite its tremendous potential to\ndeploy user-oriented embeddings, none of previous approaches provides a\nconcrete solution for it. This paper offers a new viewpoint, which treats the\ninstruction as a question about the input text and encodes the expected answers\nto obtain the representation accordingly. Intuitively, texts with the same\n(implicit) semantics would share similar answers following the instruction,\nthus leading to more similar embeddings. Specifically, we propose InBedder that\ninstantiates this embed-via-answering idea by only fine-tuning language models\non abstractive question answering tasks. InBedder demonstrates significantly\nimproved instruction-following capabilities according to our proposed\ninstruction awareness tests and instruction robustness tests, when applied to\nboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based\nLMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering\noutcomes, achieved by applying different instructions to the same corpus,\ndemonstrates a high degree of interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2402.09642v1.pdf"
    },
    {
        "title": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations",
        "authors": [
            "Xinyuan Wang",
            "Liang Wu",
            "Liangjie Hong",
            "Hao Liu",
            "Yanjie Fu"
        ],
        "published": "2024-02-14T23:12:09Z",
        "summary": "The extraordinary performance of large language models has not only reshaped\nthe research landscape in the field of NLP but has also demonstrated its\nexceptional applicative potential in various domains. However, the potential of\nthese models in mining relationships from graph data remains under-explored.\nGraph neural networks, as a popular research area in recent years, have\nnumerous studies on relationship mining. Yet, current cutting-edge research in\ngraph neural networks has not been effectively integrated with large language\nmodels, leading to limited efficiency and capability in graph relationship\nmining tasks. A primary challenge is the inability of LLMs to deeply exploit\nthe edge information in graphs, which is critical for understanding complex\nnode relationships. This gap limits the potential of LLMs to extract meaningful\ninsights from graph structures, limiting their applicability in more complex\ngraph-based analysis. We focus on how to utilize existing LLMs for mining and\nunderstanding relationships in graph data, applying these techniques to\nrecommendation tasks. We propose an innovative framework that combines the\nstrong contextual representation capabilities of LLMs with the relationship\nextraction and analysis functions of GNNs for mining relationships in graph\ndata. Specifically, we design a new prompt construction framework that\nintegrates relational information of graph data into natural language\nexpressions, aiding LLMs in more intuitively grasping the connectivity\ninformation within graph data. Additionally, we introduce graph relationship\nunderstanding and analysis functions into LLMs to enhance their focus on\nconnectivity information in graph data. Our evaluation on real-world datasets\ndemonstrates the framework's ability to understand connectivity information in\ngraph data.",
        "pdf_link": "https://arxiv.org/pdf/2402.09617v1.pdf"
    },
    {
        "title": "Probabilistic Reasoning in Generative Large Language Models",
        "authors": [
            "Aliakbar Nafar",
            "Kristen Brent Venable",
            "Parisa Kordjamshidi"
        ],
        "published": "2024-02-14T23:05:44Z",
        "summary": "This paper considers the challenges that Large Language Models (LLMs) face\nwhen reasoning over text that includes information involving uncertainty\nexplicitly quantified via probability values. This type of reasoning is\nrelevant to a variety of contexts ranging from everyday conversations to\nmedical decision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we first introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We then\nleverage this new dataset to thoroughly illustrate the specific limitations of\nLLMs for tasks involving probabilistic reasoning and present several strategies\nthat map the problem to different formal representations, including Python\ncode, probabilistic inference algorithms, and probabilistic logical\nprogramming. We conclude by providing an evaluation of our methods on BLInD and\non an adaptation of a causal reasoning question-answering dataset, which\nfurther shows their practical effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.09614v1.pdf"
    },
    {
        "title": "Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications",
        "authors": [
            "David Oniani",
            "Jordan Hilsman",
            "Chengxi Zang",
            "Junmei Wang",
            "Lianjin Cai",
            "Jan Zawala",
            "Yanshan Wang"
        ],
        "published": "2024-02-14T21:33:13Z",
        "summary": "A drug molecule is a substance that changes the organism's mental or physical\nstate. Every approved drug has an indication, which refers to the therapeutic\nuse of that drug for treating a particular medical condition. While the Large\nLanguage Model (LLM), a generative Artificial Intelligence (AI) technique, has\nrecently demonstrated effectiveness in translating between molecules and their\ntextual descriptions, there remains a gap in research regarding their\napplication in facilitating the translation between drug molecules and\nindications, or vice versa, which could greatly benefit the drug discovery\nprocess. The capability of generating a drug from a given indication would\nallow for the discovery of drugs targeting specific diseases or targets and\nultimately provide patients with better treatments. In this paper, we first\npropose a new task, which is the translation between drug molecules and\ncorresponding indications, and then test existing LLMs on this new task.\nSpecifically, we consider nine variations of the T5 LLM and evaluate them on\ntwo public datasets obtained from ChEMBL and DrugBank. Our experiments show the\nearly results of using LLMs for this task and provide a perspective on the\nstate-of-the-art. We also emphasize the current limitations and discuss future\nwork that has the potential to improve the performance on this task. The\ncreation of molecules from indications, or vice versa, will allow for more\nefficient targeting of diseases and significantly reduce the cost of drug\ndiscovery, with the potential to revolutionize the field of drug discovery in\nthe era of generative AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.09588v2.pdf"
    },
    {
        "title": "Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems",
        "authors": [
            "Liang Zhang",
            "Zhelun Chen"
        ],
        "published": "2024-02-14T21:19:33Z",
        "summary": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of rule-based parts in MLC; combining them, LLM further packages\nthese insights into a coherent, human-understandable narrative. The paper\npresents a case study to demonstrate the feasibility of the developed IML\nframework for model predictive control-based precooling under demand response\nevents in a virtual testbed. The results indicate that the developed framework\ngenerates and explains the control signals in accordance with the rule-based\nrationale.",
        "pdf_link": "https://arxiv.org/pdf/2402.09584v1.pdf"
    },
    {
        "title": "Rationality Report Cards: Assessing the Economic Rationality of Large Language Models",
        "authors": [
            "Narun Raman",
            "Taylor Lundy",
            "Samuel Amouyal",
            "Yoav Levine",
            "Kevin Leyton-Brown",
            "Moshe Tennenholtz"
        ],
        "published": "2024-02-14T20:05:26Z",
        "summary": "There is increasing interest in using LLMs as decision-making \"agents.\" Doing\nso includes many degrees of freedom: which model should be used; how should it\nbe prompted; should it be asked to introspect, conduct chain-of-thought\nreasoning, etc? Settling these questions -- and more broadly, determining\nwhether an LLM agent is reliable enough to be trusted -- requires a methodology\nfor assessing such an agent's economic rationality. In this paper, we provide\none. We begin by surveying the economic literature on rational decision making,\ntaxonomizing a large set of fine-grained \"elements\" that an agent should\nexhibit, along with dependencies between them. We then propose a benchmark\ndistribution that quantitatively scores an LLMs performance on these elements\nand, combined with a user-provided rubric, produces a \"rationality report\ncard.\" Finally, we describe the results of a large-scale empirical experiment\nwith 14 different LLMs, characterizing the both current state of the art and\nthe impact of different model sizes on models' ability to exhibit rational\nbehavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.09552v1.pdf"
    },
    {
        "title": "How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?",
        "authors": [
            "Congcong Wen",
            "Jiazhao Liang",
            "Shuaihang Yuan",
            "Hao Huang",
            "Yi Fang"
        ],
        "published": "2024-02-14T19:45:17Z",
        "summary": "In the field of robotics and automation, navigation systems based on Large\nLanguage Models (LLMs) have recently shown impressive performance. However, the\nsecurity aspects of these systems have received relatively less attention. This\npaper pioneers the exploration of vulnerabilities in LLM-based navigation\nmodels in urban outdoor environments, a critical area given the technology's\nwidespread application in autonomous driving, logistics, and emergency\nservices. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)\nAttack that manipulates LLM-based navigation models by appending\ngradient-derived suffixes to the original navigational prompt, leading to\nincorrect actions. We conducted comprehensive experiments on an LLMs-based\nnavigation model that employs various LLMs for reasoning. Our results, derived\nfrom the Touchdown and Map2Seq street-view datasets under both few-shot\nlearning and fine-tuning configurations, demonstrate notable performance\ndeclines across three metrics in the face of both white-box and black-box\nattacks. These results highlight the generalizability and transferability of\nthe NPS Attack, emphasizing the need for enhanced security in LLM-based\nnavigation systems. As an initial countermeasure, we propose the Navigational\nPrompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant\nkeywords to reduce the impact of adversarial suffixes. While initial findings\nindicate that this strategy enhances navigational safety, there remains a\ncritical need for the wider research community to develop stronger defense\nmethods to effectively tackle the real-world challenges faced by these systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.09546v1.pdf"
    },
    {
        "title": "Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls",
        "authors": [
            "Liwei Lin",
            "Gus Xia",
            "Yixiao Zhang",
            "Junyan Jiang"
        ],
        "published": "2024-02-14T19:00:01Z",
        "summary": "Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To bridge this gap, we introduce a novel\nParameter-Efficient Fine-Tuning (PEFT) method. This approach enables\nautoregressive language models to seamlessly address music inpainting tasks.\nAdditionally, our PEFT method integrates frame-level content-based controls,\nfacilitating track-conditioned music refinement and score-conditioned music\narrangement. We apply this method to fine-tune MusicGen, a leading\nautoregressive music generation model. Our experiments demonstrate promising\nresults across multiple music editing tasks, offering more flexible controls\nfor future AI-driven music editing tools. A demo\npage\\footnote{\\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and\nsource codes\\footnote{\\url{https://github.com/Kikyo-16/airgen}.} are available\nonline.",
        "pdf_link": "https://arxiv.org/pdf/2402.09508v1.pdf"
    },
    {
        "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
        "authors": [
            "Siwei Yang",
            "Bingchen Zhao",
            "Cihang Xie"
        ],
        "published": "2024-02-14T18:59:33Z",
        "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2402.09404v1.pdf"
    },
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": [
            "Kaixuan Ji",
            "Jiafan He",
            "Quanquan Gu"
        ],
        "published": "2024-02-14T18:58:40Z",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "pdf_link": "https://arxiv.org/pdf/2402.09401v1.pdf"
    },
    {
        "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
        "authors": [
            "Harry Dong",
            "Xinyu Yang",
            "Zhenyu Zhang",
            "Zhangyang Wang",
            "Yuejie Chi",
            "Beidi Chen"
        ],
        "published": "2024-02-14T18:54:56Z",
        "summary": "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient.",
        "pdf_link": "https://arxiv.org/pdf/2402.09398v1.pdf"
    },
    {
        "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
        "authors": [
            "Botao Yu",
            "Frazier N. Baker",
            "Ziqi Chen",
            "Xia Ning",
            "Huan Sun"
        ],
        "published": "2024-02-14T18:42:25Z",
        "summary": "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing research\nindicates that their performance on chemistry tasks is discouragingly low. In\nthis paper, however, we demonstrate that our developed LLMs can achieve very\nstrong results on a comprehensive set of chemistry tasks, outperforming the\nmost advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish\nthis, we propose SMolInstruct, a large-scale, comprehensive, and high-quality\ndataset for instruction tuning. It contains 14 selected chemistry tasks and\nover three million samples, laying a solid foundation for training and\nevaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of\nopen-source LLMs, among which, we find that Mistral serves as the best base\nmodel for chemistry tasks. Our analysis further demonstrates the critical role\nof the proposed dataset in driving the performance improvements.",
        "pdf_link": "https://arxiv.org/pdf/2402.09391v3.pdf"
    },
    {
        "title": "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation",
        "authors": [
            "Yihao Fang",
            "Stephen W. Thomas",
            "Xiaodan Zhu"
        ],
        "published": "2024-02-14T18:41:19Z",
        "summary": "With the widespread adoption of large language models (LLMs) in numerous\napplications, the challenge of factuality and the propensity for hallucinations\nraises significant concerns. To address this issue, particularly in\nretrieval-augmented in-context learning, we introduce the hierarchical graph of\nthoughts (HGOT), a structured, multi-layered graph approach designed to enhance\nthe retrieval of pertinent passages during in-context learning. The framework\nutilizes the emergent planning capabilities of LLMs, employing the\ndivide-and-conquer strategy to break down complex queries into manageable\nsub-queries. It refines self-consistency majority voting for answer selection,\nwhich incorporates the recently proposed citation recall and precision metrics\nto assess the quality of thoughts, linking an answer's credibility\nintrinsically to the thought's quality. This methodology introduces a weighted\nsystem in majority voting, prioritizing answers based on the citation quality\nof their thoughts. Additionally, we propose a scoring mechanism for evaluating\nretrieved passages, considering factors such as citation frequency and quality,\nself-consistency confidence, and the retrieval module's ranking. Experiments\nreveal that HGOT outperforms other retrieval-augmented in-context learning\nmethods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and\nRetrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its\nefficacy in enhancing the factuality of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.09390v1.pdf"
    },
    {
        "title": "Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking",
        "authors": [
            "Yi Fung",
            "Ruining Zhao",
            "Jae Doo",
            "Chenkai Sun",
            "Heng Ji"
        ],
        "published": "2024-02-14T18:16:54Z",
        "summary": "Pretrained large language models have revolutionized many applications but\nstill face challenges related to cultural bias and a lack of cultural\ncommonsense knowledge crucial for guiding cross-culture communication and\ninteractions. Recognizing the shortcomings of existing methods in capturing the\ndiverse and rich cultures across the world, this paper introduces a novel\napproach for massively multicultural knowledge acquisition. Specifically, our\nmethod strategically navigates from densely informative Wikipedia documents on\ncultural topics to an extensive network of linked pages. Leveraging this\nvaluable source of data collection, we construct the CultureAtlas dataset,\nwhich covers a wide range of sub-country level geographical regions and\nethnolinguistic groups, with data cleaning and preprocessing to ensure textual\nassertion sentence self-containment, as well as fine-grained cultural profile\ninformation extraction. Our dataset not only facilitates the evaluation of\nlanguage model performance in culturally diverse contexts but also serves as a\nfoundational tool for the development of culturally sensitive and aware\nlanguage models. Our work marks an important step towards deeper understanding\nand bridging the gaps of cultural disparities in AI, to promote a more\ninclusive and balanced representation of global cultures in the digital domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.09369v1.pdf"
    },
    {
        "title": "Copyright Traps for Large Language Models",
        "authors": [
            "Matthieu Meeus",
            "Igor Shilov",
            "Manuel Faysse",
            "Yves-Alexandre de Montjoye"
        ],
        "published": "2024-02-14T18:09:53Z",
        "summary": "Questions of fair use of copyright-protected content to train Large Language\nModels (LLMs) are being very actively debated. Document-level inference has\nbeen proposed as a new task: inferring from black-box access to the trained\nmodel whether a piece of content has been seen during training. SOTA methods\nhowever rely on naturally occurring memorization of (part of) the content.\nWhile very effective against models that memorize a lot, we hypothesize--and\nlater confirm--that they will not work against models that do not naturally\nmemorize, e.g. medium-size 1B models. We here propose to use copyright traps,\nthe inclusion of fictitious entries in original content, to detect the use of\ncopyrighted materials in LLMs with a focus on models where memorization does\nnot naturally occur. We carefully design an experimental setup, randomly\ninserting traps into original content (books) and train a 1.3B LLM. We first\nvalidate that the use of content in our target model would be undetectable\nusing existing methods. We then show, contrary to intuition, that even\nmedium-length trap sentences repeated a significant number of times (100) are\nnot detectable using existing methods. However, we show that longer sequences\nrepeated a large number of times can be reliably detected (AUC=0.75) and used\nas copyright traps. We further improve these results by studying how the number\nof times a sequence is seen improves detectability, how sequences with higher\nperplexity tend to be memorized more, and how taking context into account\nfurther improves detectability.",
        "pdf_link": "https://arxiv.org/pdf/2402.09363v1.pdf"
    },
    {
        "title": "HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference",
        "authors": [
            "Yashas Samaga B L",
            "Varun Yerram",
            "Chong You",
            "Srinadh Bhojanapalli",
            "Sanjiv Kumar",
            "Prateek Jain",
            "Praneeth Netrapalli"
        ],
        "published": "2024-02-14T18:04:36Z",
        "summary": "Autoregressive decoding with generative Large Language Models (LLMs) on\naccelerators (GPUs/TPUs) is often memory-bound where most of the time is spent\non transferring model parameters from high bandwidth memory (HBM) to cache. On\nthe other hand, recent works show that LLMs can maintain quality with\nsignificant sparsity/redundancy in the feedforward (FFN) layers by\nappropriately training the model to operate on a top-$k$ fraction of\nrows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the\ntransfer of model parameters, and hence latency. However, exploiting this\nsparsity for improving latency is hindered by the fact that identifying top\nrows/columns is data-dependent and is usually performed using full matrix\noperations, severely limiting potential gains. To address these issues, we\nintroduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of\ntwo novel components: (i) a compression scheme to cheaply predict top-$k$\nrows/columns with high recall, followed by full computation restricted to the\npredicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate\ntop-$k$ operator. We demonstrate that on a one billion parameter model, HiRE\napplied to both the softmax as well as feedforward layers, achieves almost\nmatching pretraining and downstream accuracy, and speeds up inference latency\nby $1.47\\times$ on a single TPUv5e device.",
        "pdf_link": "https://arxiv.org/pdf/2402.09360v1.pdf"
    },
    {
        "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
            "Maryam Amirizaniani",
            "Jihan Yao",
            "Adrian Lavergne",
            "Elizabeth Snell Okada",
            "Aman Chadha",
            "Tanya Roosta",
            "Chirag Shah"
        ],
        "published": "2024-02-14T17:49:31Z",
        "summary": "As LLMs become more pervasive across various users and scenarios, identifying\npotential issues when using these models becomes essential. Examples include\nbias, inconsistencies, and hallucination. Although auditing the LLM for these\nproblems is desirable, it is far from being easy or solved. An effective method\nis to probe the LLM using different versions of the same question. This could\nexpose inconsistencies in its knowledge or operation, indicating potential for\nbias or hallucination. However, to operationalize this auditing method at\nscale, we need an approach to create those probes reliably and automatically.\nIn this paper we propose an automatic and scalable solution, where one uses a\ndifferent LLM along with human-in-the-loop. This approach offers verifiability\nand transparency, while avoiding circular reliance on the same LLMs, and\nincreasing scientific rigor and generalizability. Specifically, we present a\nnovel methodology with two phases of verification using humans: standardized\nevaluation criteria to verify responses, and a structured prompt template to\ngenerate desired probes. Experiments on a set of questions from TruthfulQA\ndataset show that we can generate a reliable set of probes from one LLM that\ncan be used to audit inconsistencies in a different LLM. The criteria for\ngenerating and applying auditing probes is generalizable to various LLMs\nregardless of the underlying structure or training mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.09346v2.pdf"
    },
    {
        "title": "AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach",
        "authors": [
            "Maryam Amirizaniani",
            "Tanya Roosta",
            "Aman Chadha",
            "Chirag Shah"
        ],
        "published": "2024-02-14T17:31:04Z",
        "summary": "As Large Language Models (LLMs) gain wider adoption in various contexts, it\nbecomes crucial to ensure they are reasonably safe, consistent, and reliable\nfor an application at hand. This may require probing or auditing them. Probing\nLLMs with varied iterations of a single question could reveal potential\ninconsistencies in their knowledge or functionality. However, a tool for\nperforming such audits with simple workflow and low technical threshold is\nlacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to\nevaluate the performance of various LLMs in a methodical way. AuditLLM's core\nfunctionality lies in its ability to test a given LLM by auditing it using\nmultiple probes generated from a single question, thereby identifying any\ninconsistencies in the model's understanding or operation. A reasonably robust,\nreliable, and consistent LLM should output semantically similar responses for a\nquestion asked differently or by different people. Based on this assumption,\nAuditLLM produces easily interpretable results regarding the LLM's\nconsistencies from a single question that the user enters. A certain level of\ninconsistency has been shown to be an indicator of potential bias,\nhallucinations, and other issues. One could then use the output of AuditLLM to\nfurther investigate issues with the aforementioned LLM. To facilitate\ndemonstration and practical uses, AuditLLM offers two key modes: (1) Live mode\nwhich allows instant auditing of LLMs by analyzing responses to real-time\nqueries; (2) Batch mode which facilitates comprehensive LLM auditing by\nprocessing multiple queries at once for in-depth analysis. This tool is\nbeneficial for both researchers and general users, as it enhances our\nunderstanding of LLMs' capabilities in generating responses, using a\nstandardized auditing platform.",
        "pdf_link": "https://arxiv.org/pdf/2402.09334v1.pdf"
    },
    {
        "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization",
        "authors": [
            "Feifan Song",
            "Yuxuan Fan",
            "Xin Zhang",
            "Peiyi Wang",
            "Houfeng Wang"
        ],
        "published": "2024-02-14T17:14:34Z",
        "summary": "Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to\nensure the generation of safe content. Due to the heavy cost associated with\nfine-tuning, fine-tuning-free methods have emerged, typically modifying LLM\ndecoding with external auxiliary methods. However, these methods do not\nessentially enhance the LLM itself. In this paper, we rethink the derivation\nprocedures of DPO, based on which we conversely build an instant scorer using\nthe states of the LLM before and after In-context Learning (ICL). Accordingly,\nwe propose a novel approach called In-Context Direct Preference Optimization\n(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with\nICL, generating well-aligned responses as estimated by the aforementioned\ninstant scorer, thereby enhancing the final performance. ICDPO can be further\nenhanced with a two-stage retriever and an upgraded scorer, both offering\nbenefits. Extensive experiments show its effectiveness, particularly in\noutperforming two fine-tuning-free baselines, and it exhibits competitiveness\nwith SFT + LoRA. We also conduct detailed analyses to offer comprehensive\ninsights into ICDPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.09320v1.pdf"
    },
    {
        "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code",
        "authors": [
            "Vahid Majdinasab",
            "Amin Nikanjam",
            "Foutse Khomh"
        ],
        "published": "2024-02-14T16:41:35Z",
        "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
        "pdf_link": "https://arxiv.org/pdf/2402.09299v1.pdf"
    },
    {
        "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
        "authors": [
            "Zhichen Dong",
            "Zhanhui Zhou",
            "Chao Yang",
            "Jing Shao",
            "Yu Qiao"
        ],
        "published": "2024-02-14T16:14:03Z",
        "summary": "Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.",
        "pdf_link": "https://arxiv.org/pdf/2402.09283v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies",
        "authors": [
            "Yining Huang",
            "Keke Tang",
            "Meilian Chen"
        ],
        "published": "2024-02-14T16:10:45Z",
        "summary": "Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural\nLanguage Processing (NLP), showing potential in traditional tasks such as Named\nEntity Recognition (NER). Our study explores a three-phase training strategy\nthat harnesses GPT-4's capabilities to enhance the BERT model's performance on\nNER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC\ndataset without fine-tuning. We then train BERT using a mix of original and\nLLM-annotated data, analyzing the efficacy of LLM annotations against\ntraditional methods. The second phase involves comparative experiments with\ndifferent training regimens, assessing the synergy between distilled and\noriginal data. We observe that sequential strategies, particularly a simple mix\nof training first with distilled data followed by original data, significantly\nboost performance. In the third phase, we investigate various data blending\ntechniques, including sigmoid and power decay functions, to optimize the\ntraining process further. Our results indicate that a strategic mix of\ndistilled and original data markedly elevates the NER capabilities of BERT. Our\napproach presents a scalable methodology that reduces manual annotation costs\nand increases efficiency, making it especially pertinent in resource-limited\nand closed-network environments. The study concludes that while the 'Simple\nMix' strategy yields the best results, understanding its underlying mechanisms\nrequires further research. Future work will also focus on refining prompt\ndesigns and enhancing annotation selection processes, aiming to extend our\nmethodology to diverse NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.09282v4.pdf"
    },
    {
        "title": "Personalized Large Language Models",
        "authors": [
            "Stanis\u0142aw Wo\u017aniak",
            "Bart\u0142omiej Koptyra",
            "Arkadiusz Janz",
            "Przemys\u0142aw Kazienko",
            "Jan Koco\u0144"
        ],
        "published": "2024-02-14T15:55:30Z",
        "summary": "Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.09269v1.pdf"
    },
    {
        "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation",
        "authors": [
            "Xiaoying Zhang",
            "Baolin Peng",
            "Ye Tian",
            "Jingyan Zhou",
            "Lifeng Jin",
            "Linfeng Song",
            "Haitao Mi",
            "Helen Meng"
        ],
        "published": "2024-02-14T15:52:42Z",
        "summary": "Despite showing increasingly human-like abilities, large language models\n(LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even\nwhen they hold relevant knowledge. To address these hallucinations, current\napproaches typically necessitate high-quality human factuality annotations. In\nthis work, we explore Self-Alignment for Factuality, where we leverage the\nself-evaluation capability of an LLM to provide training signals that steer the\nmodel towards factuality. Specifically, we incorporate Self-Eval, a\nself-evaluation component, to prompt an LLM to validate the factuality of its\nown generated responses solely based on its internal knowledge. Additionally,\nwe design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's\nself-evaluation ability by improving the model's confidence estimation and\ncalibration. We then utilize these self-annotated responses to fine-tune the\nmodel via Direct Preference Optimization algorithm. We show that the proposed\nself-alignment approach substantially enhances factual accuracy over Llama\nfamily models across three key knowledge-intensive tasks on TruthfulQA and\nBioGEN.",
        "pdf_link": "https://arxiv.org/pdf/2402.09267v1.pdf"
    },
    {
        "title": "SyntaxShap: Syntax-aware Explainability Method for Text Generation",
        "authors": [
            "Kenza Amara",
            "Rita Sevastjanova",
            "Mennatallah El-Assady"
        ],
        "published": "2024-02-14T15:45:56Z",
        "summary": "To harness the power of large language models in safety-critical domains we\nneed to ensure the explainability of their predictions. However, despite the\nsignificant attention to model interpretability, there remains an unexplored\ndomain in explaining sequence-to-sequence tasks using methods tailored for\ntextual data. This paper introduces SyntaxShap, a local, model-agnostic\nexplainability method for text generation that takes into consideration the\nsyntax in the text data. The presented work extends Shapley values to account\nfor parsing-based syntactic dependencies. Taking a game theoric approach,\nSyntaxShap only considers coalitions constraint by the dependency tree. We\nadopt a model-based evaluation to compare SyntaxShap and its weighted form to\nstate-of-the-art explainability methods adapted to text generation tasks, using\ndiverse metrics including faithfulness, complexity, coherency, and semantic\nalignment of the explanations to the model. We show that our syntax-aware\nmethod produces explanations that help build more faithful, coherent, and\ninterpretable explanations for predictions by autoregressive models.",
        "pdf_link": "https://arxiv.org/pdf/2402.09259v1.pdf"
    },
    {
        "title": "Scaling the Authoring of AutoTutors with Large Language Models",
        "authors": [
            "Sankalan Pal Chowdhury",
            "Vil\u00e9m Zouhar",
            "Mrinmaya Sachan"
        ],
        "published": "2024-02-14T14:53:56Z",
        "summary": "Large Language Models (LLMs) have found several use cases in education,\nranging from automatic question generation to essay evaluation. In this paper,\nwe explore the potential of using Large Language Models (LLMs) to author\nIntelligent Tutoring Systems. A common pitfall of LLMs is their straying from\ndesired pedagogical strategies such as leaking the answer to the student, and\nin general, providing no guarantees. We posit that while LLMs with certain\nguardrails can take the place of subject experts, the overall pedagogical\ndesign still needs to be handcrafted for the best learning results. Based on\nthis principle, we create a sample end-to-end tutoring system named MWPTutor,\nwhich uses LLMs to fill in the state space of a pre-defined finite state\ntransducer. This approach retains the structure and the pedagogy of traditional\ntutoring systems that has been developed over the years by learning scientists\nbut brings in additional flexibility of LLM-based approaches. Through a human\nevaluation study on two datasets based on math word problems, we show that our\nhybrid approach achieves a better overall tutoring score than an instructed,\nbut otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the\nscope for the community to improve its performance by improving individual\nmodules or using different teaching strategies that it can follow",
        "pdf_link": "https://arxiv.org/pdf/2402.09216v2.pdf"
    },
    {
        "title": "Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling",
        "authors": [
            "Yuhui Shi",
            "Qiang Sheng",
            "Juan Cao",
            "Hao Mi",
            "Beizhe Hu",
            "Danding Wang"
        ],
        "published": "2024-02-14T14:32:16Z",
        "summary": "With the rapidly increasing application of large language models (LLMs),\ntheir abuse has caused many undesirable societal problems such as fake news,\nacademic dishonesty, and information pollution. This makes AI-generated text\n(AIGT) detection of great importance. Among existing methods, white-box methods\nare generally superior to black-box methods in terms of performance and\ngeneralizability, but they require access to LLMs' internal states and are not\napplicable to black-box settings. In this paper, we propose to estimate word\ngeneration probabilities as pseudo white-box features via multiple re-sampling\nto help improve AIGT detection under the black-box setting. Specifically, we\ndesign POGER, a proxy-guided efficient re-sampling method, which selects a\nsmall subset of representative words (e.g., 10 words) for performing multiple\nre-sampling in black-box AIGT detection. Experiments on datasets containing\ntexts from humans and seven LLMs show that POGER outperforms all baselines in\nmacro F1 under black-box, partial white-box, and out-of-distribution settings\nand maintains lower re-sampling costs than its existing counterparts.",
        "pdf_link": "https://arxiv.org/pdf/2402.09199v1.pdf"
    },
    {
        "title": "(Ir)rationality and Cognitive Biases in Large Language Models",
        "authors": [
            "Olivia Macmillan-Scott",
            "Mirco Musolesi"
        ],
        "published": "2024-02-14T14:17:21Z",
        "summary": "Do large language models (LLMs) display rational reasoning? LLMs have been\nshown to contain human biases due to the data they have been trained on;\nwhether this is reflected in rational reasoning remains less clear. In this\npaper, we answer this question by evaluating seven language models using tasks\nfrom the cognitive psychology literature. We find that, like humans, LLMs\ndisplay irrationality in these tasks. However, the way this irrationality is\ndisplayed does not reflect that shown by humans. When incorrect answers are\ngiven by LLMs to these tasks, they are often incorrect in ways that differ from\nhuman-like biases. On top of this, the LLMs reveal an additional layer of\nirrationality in the significant inconsistency of the responses. Aside from the\nexperimental results, this paper seeks to make a methodological contribution by\nshowing how we can assess and compare different capabilities of these types of\nmodels, in this case with respect to rational reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.09193v2.pdf"
    },
    {
        "title": "Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",
        "authors": [
            "Rui Zhang",
            "Hongwei Li",
            "Rui Wen",
            "Wenbo Jiang",
            "Yuan Zhang",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2024-02-14T13:47:35Z",
        "summary": "The increasing demand for customized Large Language Models (LLMs) has led to\nthe development of solutions like GPTs. These solutions facilitate tailored LLM\ncreation via natural language prompts without coding. However, the\ntrustworthiness of third-party custom versions of LLMs remains an essential\nconcern. In this paper, we propose the first instruction backdoor attacks\nagainst applications integrated with untrusted customized LLMs (e.g., GPTs).\nSpecifically, these attacks embed the backdoor into the custom version of LLMs\nby designing prompts with backdoor instructions, outputting the attacker's\ndesired result when inputs contain the pre-defined triggers. Our attack\nincludes 3 levels of attacks: word-level, syntax-level, and semantic-level,\nwhich adopt different types of triggers with progressive stealthiness. We\nstress that our attacks do not require fine-tuning or any modification to the\nbackend LLMs, adhering strictly to GPTs development guidelines. We conduct\nextensive experiments on 4 prominent LLMs and 5 benchmark text classification\ndatasets. The results show that our instruction backdoor attacks achieve the\ndesired attack performance without compromising utility. Additionally, we\npropose an instruction-ignoring defense mechanism and demonstrate its partial\neffectiveness in mitigating such attacks. Our findings highlight the\nvulnerability and the potential risks of LLM customization such as GPTs.",
        "pdf_link": "https://arxiv.org/pdf/2402.09179v2.pdf"
    },
    {
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks",
        "authors": [
            "Yixin Cheng",
            "Markos Georgopoulos",
            "Volkan Cevher",
            "Grigorios G. Chrysos"
        ],
        "published": "2024-02-14T13:45:19Z",
        "summary": "Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired by\nhuman practices of indirect context to elicit harmful information, we focus on\na new attack form called Contextual Interaction Attack. The idea relies on the\nautoregressive nature of the generation process in LLMs. We contend that the\nprior context--the information preceding the attack query--plays a pivotal role\nin enabling potent Jailbreaking attacks. Specifically, we propose an approach\nthat leverages preliminary question-answer pairs to interact with the LLM. By\ndoing so, we guide the responses of the model toward revealing the 'desired'\nharmful information. We conduct experiments on four different LLMs and\ndemonstrate the efficacy of this attack, which is black-box and can also\ntransfer across LLMs. We believe this can lead to further developments and\nunderstanding of the context vector in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.09177v1.pdf"
    },
    {
        "title": "Attacking Large Language Models with Projected Gradient Descent",
        "authors": [
            "Simon Geisler",
            "Tom Wollschl\u00e4ger",
            "M. H. I. Abdalla",
            "Johannes Gasteiger",
            "Stephan G\u00fcnnemann"
        ],
        "published": "2024-02-14T13:13:26Z",
        "summary": "Current LLM alignment methods are readily broken through specifically crafted\nadversarial prompts. While crafting adversarial prompts using discrete\noptimization is highly effective, such attacks typically use more than 100,000\nLLM calls. This high computational cost makes them unsuitable for, e.g.,\nquantitative analyses and adversarial training. To remedy this, we revisit\nProjected Gradient Descent (PGD) on the continuously relaxed input prompt.\nAlthough previous attempts with ordinary gradient-based attacks largely failed,\nwe show that carefully controlling the error introduced by the continuous\nrelaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one\norder of magnitude faster than state-of-the-art discrete optimization to\nachieve the same devastating attack results.",
        "pdf_link": "https://arxiv.org/pdf/2402.09154v1.pdf"
    },
    {
        "title": "Into the Unknown: Self-Learning Large Language Models",
        "authors": [
            "Teddy Ferdinan",
            "Jan Koco\u0144",
            "Przemys\u0142aw Kazienko"
        ],
        "published": "2024-02-14T12:56:58Z",
        "summary": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. Using the hallucination score, we introduce a new\nconcept of Points in The Unknown (PiUs), along with one extrinsic and three\nintrinsic methods for automatic PiUs identification. It facilitates the\ncreation of a self-learning loop that focuses exclusively on the knowledge gap\nin Points in The Unknown, resulting in a reduced hallucination score. We also\ndeveloped evaluation metrics for gauging an LLM's self-learning capability. Our\nexperiments revealed that 7B-Mistral models that have been finetuned or aligned\nare capable of self-learning considerably well. Our self-learning concept\nallows more efficient LLM updates and opens new perspectives for knowledge\nexchange. It may also increase public trust in AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.09147v1.pdf"
    },
    {
        "title": "Exploring the Adversarial Capabilities of Large Language Models",
        "authors": [
            "Lukas Struppek",
            "Minh Hieu Le",
            "Dominik Hintersdorf",
            "Kristian Kersting"
        ],
        "published": "2024-02-14T12:28:38Z",
        "summary": "The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.",
        "pdf_link": "https://arxiv.org/pdf/2402.09132v3.pdf"
    },
    {
        "title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models",
        "authors": [
            "Nadav Schneider",
            "Niranjan Hasabnis",
            "Vy A. Vo",
            "Tal Kadosh",
            "Neva Krien",
            "Mihai Capot\u0103",
            "Abdul Wasay",
            "Guy Tamir",
            "Ted Willke",
            "Nesreen Ahmed",
            "Yuval Pinter",
            "Timothy Mattson",
            "Gal Oren"
        ],
        "published": "2024-02-14T12:24:21Z",
        "summary": "The imperative need to scale computation across numerous nodes highlights the\nsignificance of efficient parallel computing, particularly in the realm of\nMessage Passing Interface (MPI) integration. The challenging parallel\nprogramming task of generating MPI-based parallel programs has remained\nunexplored. This study first investigates the performance of state-of-the-art\nlanguage models in generating MPI-based parallel programs. Findings reveal that\nwidely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual\ncode models) exhibit notable performance degradation, when generating MPI-based\nprograms compared to general-purpose programs. In contrast, domain-specific\nmodels such as MonoCoder, which are pretrained on MPI-related programming\nlanguages of C and C++, outperform larger models. Subsequently, we introduce a\ndedicated downstream task of MPI-based program generation by fine-tuning\nMonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose\nan innovative preprocessing for completion only after observing the whole code,\nthus enabling better completion with a wider context. Comparative analysis\nagainst GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation\nmethod, demonstrates that MPIrigen excels in generating accurate MPI functions\nup to 0.8 accuracy in location and function predictions, and with more than 0.9\naccuracy for argument predictions. The success of this tailored solution\nunderscores the importance of domain-specific fine-tuning in optimizing\nlanguage models for parallel computing code generation, paving the way for a\nnew generation of automatic parallelization tools. The sources of this work are\navailable at our GitHub MPIrigen repository:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen",
        "pdf_link": "https://arxiv.org/pdf/2402.09126v1.pdf"
    },
    {
        "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues",
        "authors": [
            "Zhiyuan Chang",
            "Mingyang Li",
            "Yi Liu",
            "Junjie Wang",
            "Qing Wang",
            "Yang Liu"
        ],
        "published": "2024-02-14T11:11:51Z",
        "summary": "With the development of LLMs, the security threats of LLMs are getting more\nand more attention. Numerous jailbreak attacks have been proposed to assess the\nsecurity defense of LLMs. Current jailbreak attacks primarily utilize scenario\ncamouflage techniques. However their explicitly mention of malicious intent\nwill be easily recognized and defended by LLMs. In this paper, we propose an\nindirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense\nstrategy and obtain malicious response by implicitly providing LLMs with some\nclues about the original malicious query. In addition, inspired by the wisdom\nof \"When unable to attack, defend\" from Sun Tzu's Art of War, we adopt a\ndefensive stance to gather clues about the original malicious query through\nLLMs. Extensive experimental results show that Puzzler achieves a query success\nrate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than\nbaselines. Furthermore, when tested against the state-of-the-art jailbreak\ndetection approaches, Puzzler proves to be more effective at evading detection\ncompared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.09091v2.pdf"
    },
    {
        "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
        "authors": [
            "Leo Schwinn",
            "David Dobre",
            "Sophie Xhonneux",
            "Gauthier Gidel",
            "Stephan Gunnemann"
        ],
        "published": "2024-02-14T10:20:03Z",
        "summary": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment.",
        "pdf_link": "https://arxiv.org/pdf/2402.09063v1.pdf"
    },
    {
        "title": "L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects",
        "authors": [
            "Yutaro Yamada",
            "Khyathi Chandu",
            "Yuchen Lin",
            "Jack Hessel",
            "Ilker Yildirim",
            "Yejin Choi"
        ],
        "published": "2024-02-14T09:51:05Z",
        "summary": "Diffusion-based image generation models such as DALL-E 3 and Stable\nDiffusion-XL demonstrate remarkable capabilities in generating images with\nrealistic and unique compositions. Yet, these models are not robust in\nprecisely reasoning about physical and spatial configurations of objects,\nespecially when instructed with unconventional, thereby out-of-distribution\ndescriptions, such as \"a chair with five legs\". In this paper, we propose a\nlanguage agent with chain-of-3D-thoughts (L3GO), an inference-time approach\nthat can reason about part-based 3D mesh generation of unconventional objects\nthat current data-driven diffusion models struggle with. More concretely, we\nuse large language models as agents to compose a desired object via\ntrial-and-error within the 3D simulation environment. To facilitate our\ninvestigation, we develop a new benchmark, Unconventionally Feasible Objects\n(UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender\nwhere language agents can build and compose atomic building blocks via API\ncalls. Human and automatic GPT-4V evaluations show that our approach surpasses\nthe standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D\nmesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our\napproach outperforms other state-of-the-art text-to-2D image and text-to-3D\nmodels based on human evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.09052v1.pdf"
    },
    {
        "title": "FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems",
        "authors": [
            "Yiming He",
            "Jia Zou",
            "Xiaokai Zhang",
            "Na Zhu",
            "Tuo Leng"
        ],
        "published": "2024-02-14T09:44:28Z",
        "summary": "The application of contemporary artificial intelligence techniques to address\ngeometric problems and automated deductive proof has always been a grand\nchallenge to the interdiscipline field of mathematics and artificial\nIntelligence. This is the fourth article in a series of our works, in our\nprevious work, we established of a geometric formalized system known as\nFormalGeo. Moreover we annotated approximately 7000 geometric problems, forming\nthe FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can\nachieve interpretable algebraic equation solving and human-like deductive\nreasoning, it often experiences timeouts due to the complexity of the search\nstrategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which\nutilizes the language model to predict theorem sequences for solving geometry\nproblems. We compared the effectiveness of various Transformer architectures,\nsuch as BART or T5, in theorem prediction, implementing pruning in the search\nprocess of FGPS, thereby improving its performance in solving geometry\nproblems. Our results demonstrate a significant increase in the problem-solving\nrate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising\nfrom 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in\nsolving time and search steps across problems of varying difficulty levels.",
        "pdf_link": "https://arxiv.org/pdf/2402.09047v1.pdf"
    },
    {
        "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks",
        "authors": [
            "Jiwon Song",
            "Kyungseok Oh",
            "Taesu Kim",
            "Hyungjun Kim",
            "Yulhwa Kim",
            "Jae-Joon Kim"
        ],
        "published": "2024-02-14T09:01:13Z",
        "summary": "Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB successfully\naccelerates LLM inference without compromising the linguistic capabilities of\nthese models, making it a promising technique for optimizing the efficiency of\nLLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB",
        "pdf_link": "https://arxiv.org/pdf/2402.09025v1.pdf"
    },
    {
        "title": "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications",
        "authors": [
            "Negar Arabzadeh",
            "Julia Kiseleva",
            "Qingyun Wu",
            "Chi Wang",
            "Ahmed Awadallah",
            "Victor Dibia",
            "Adam Fourney",
            "Charles Clarke"
        ],
        "published": "2024-02-14T08:46:15Z",
        "summary": "The rapid development in the field of Large Language Models (LLMs) has led to\na surge in applications that facilitate collaboration among multiple agents to\nassist humans in their daily tasks. However, a significant gap remains in\nassessing whether LLM-powered applications genuinely enhance user experience\nand task execution efficiency. This highlights the pressing need for methods to\nverify utility of LLM-powered applications, particularly by ensuring alignment\nbetween the application's functionality and end-user needs. We introduce\nAgentEval provides an implementation for the math problems, a novel framework\ndesigned to simplify the utility verification process by automatically\nproposing a set of criteria tailored to the unique purpose of any given\napplication. This allows for a comprehensive assessment, quantifying the\nutility of an application against the suggested criteria. We present a\ncomprehensive analysis of the robustness of quantifier's work.",
        "pdf_link": "https://arxiv.org/pdf/2402.09015v3.pdf"
    },
    {
        "title": "Multi-Query Focused Disaster Summarization via Instruction-Based Prompting",
        "authors": [
            "Philipp Seeberger",
            "Korbinian Riedhammer"
        ],
        "published": "2024-02-14T08:22:58Z",
        "summary": "Automatic summarization of mass-emergency events plays a critical role in\ndisaster management. The second edition of CrisisFACTS aims to advance disaster\nsummarization based on multi-stream fact-finding with a focus on web sources\nsuch as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to\ndevelop systems that can extract key facts from several disaster-related\nevents, which ultimately serve as a summary. This paper describes our method to\ntackle this challenging task. We follow previous work and propose to use a\ncombination of retrieval, reranking, and an embarrassingly simple\ninstruction-following summarization. The two-stage retrieval pipeline relies on\nBM25 and MonoT5, while the summarizer module is based on the open-source Large\nLanguage Model (LLM) LLaMA-13b. For summarization, we explore a Question\nAnswering (QA)-motivated prompting approach and find the evidence useful for\nextracting query-relevant facts. The automatic metrics and human evaluation\nshow strong results but also highlight the gap between open-source and\nproprietary systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.09008v1.pdf"
    },
    {
        "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
        "authors": [
            "Zhangchen Xu",
            "Fengqing Jiang",
            "Luyao Niu",
            "Jinyuan Jia",
            "Bill Yuchen Lin",
            "Radha Poovendran"
        ],
        "published": "2024-02-14T06:54:31Z",
        "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications such as code generation and chatbot assistance,\nextensive efforts have been made to align LLM behavior with human values,\nincluding safety. Jailbreak attacks, aiming to provoke unintended and unsafe\nbehaviors from LLMs, remain a significant/leading LLM safety threat. In this\npaper, we aim to defend LLMs against jailbreak attacks by introducing\nSafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and\nharmless responses to user queries. Our insight in developing SafeDecoding is\nbased on the observation that, even though probabilities of tokens representing\nharmful contents outweigh those representing harmless responses, safety\ndisclaimers still appear among the top tokens after sorting tokens by\nprobability in descending order. This allows us to mitigate jailbreak attacks\nby identifying safety disclaimers and amplifying their token probabilities,\nwhile simultaneously attenuating the probabilities of token sequences that are\naligned with the objectives of jailbreak attacks. We perform extensive\nexperiments on five LLMs using six state-of-the-art jailbreak attacks and four\nbenchmark datasets. Our results show that SafeDecoding significantly reduces\nthe attack success rate and harmfulness of jailbreak attacks without\ncompromising the helpfulness of responses to benign user queries. SafeDecoding\noutperforms six defense methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.08983v2.pdf"
    },
    {
        "title": "GrounDial: Human-norm Grounded Safe Dialog Response Generation",
        "authors": [
            "Siwon Kim",
            "Shuyang Dai",
            "Mohammad Kachuee",
            "Shayan Ray",
            "Tara Taghavi",
            "Sungroh Yoon"
        ],
        "published": "2024-02-14T06:25:50Z",
        "summary": "Current conversational AI systems based on large language models (LLMs) are\nknown to generate unsafe responses, agreeing to offensive user input or\nincluding toxic content. Previous research aimed to alleviate the toxicity, by\nfine-tuning LLM with manually annotated safe dialogue histories. However, the\ndependency on additional tuning requires substantial costs. To remove the\ndependency, we propose GrounDial, where response safety is achieved by\ngrounding responses to commonsense social rules without requiring fine-tuning.\nA hybrid approach of in-context learning and human-norm-guided decoding of\nGrounDial enables the response to be quantitatively and qualitatively safer\neven without additional data or tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.08968v1.pdf"
    }
]