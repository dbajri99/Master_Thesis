[
    {
        "title": "Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model",
        "authors": [
            "Salman Rahman",
            "Lavender Yao Jiang",
            "Saadia Gabriel",
            "Yindalon Aphinyanaphongs",
            "Eric Karl Oermann",
            "Rumi Chunara"
        ],
        "published": "2024-02-14T06:24:52Z",
        "summary": "Advances in large language models (LLMs) provide new opportunities in\nhealthcare for improved patient care, clinical decision-making, and enhancement\nof physician and administrator workflows. However, the potential of these\nmodels importantly depends on their ability to generalize effectively across\nclinical environments and populations, a challenge often underestimated in\nearly development. To better understand reasons for these challenges and inform\nmitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s\nclinical notes, analyzing its performance on 30-day all-cause readmission\nprediction focusing on variability across hospitals and patient\ncharacteristics. We found poorer generalization particularly in hospitals with\nfewer samples, among patients with government and unspecified insurance, the\nelderly, and those with high comorbidities. To understand reasons for lack of\ngeneralization, we investigated sample sizes for fine-tuning, note content\n(number of words per note), patient characteristics (comorbidity level, age,\ninsurance type, borough), and health system aspects (hospital, all-cause 30-day\nreadmission, and mortality rates). We used descriptive statistics and\nsupervised classification to identify features. We found that, along with\nsample size, patient age, number of comorbidities, and the number of words in\nnotes are all important factors related to generalization. Finally, we compared\nlocal fine-tuning (hospital specific), instance-based augmented fine-tuning and\ncluster-based fine-tuning for improving generalization. Among these, local\nfine-tuning proved most effective, increasing AUC by 0.25% to 11.74% (most\nhelpful in settings with limited data). Overall, this study provides new\ninsights for enhancing the deployment of large language models in the\nsocietally important domain of healthcare, and improving their performance for\nbroader populations.",
        "pdf_link": "https://arxiv.org/pdf/2402.10965v2.pdf"
    },
    {
        "title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision",
        "authors": [
            "Zhaoqing Wang",
            "Xiaobo Xia",
            "Ziye Chen",
            "Xiao He",
            "Yandong Guo",
            "Mingming Gong",
            "Tongliang Liu"
        ],
        "published": "2024-02-14T06:01:44Z",
        "summary": "Contemporary cutting-edge open-vocabulary segmentation approaches commonly\nrely on image-mask-text triplets, yet this restricted annotation is\nlabour-intensive and encounters scalability hurdles in complex real-world\nscenarios. Although some methods are proposed to reduce the annotation cost\nwith only text supervision, the incompleteness of supervision severely limits\nthe versatility and performance. In this paper, we liberate the strict\ncorrespondence between masks and texts by using independent image-mask and\nimage-text pairs, which can be easily collected respectively. With this\nunpaired mask-text supervision, we propose a new weakly-supervised\nopen-vocabulary segmentation framework (Uni-OVSeg) that leverages confident\npairs of mask predictions and entities in text descriptions. Using the\nindependent image-mask and image-text pairs, we predict a set of binary masks\nand associate them with entities by resorting to the CLIP embedding space.\nHowever, the inherent noise in the correspondence between masks and entities\nposes a significant challenge when obtaining reliable pairs. In light of this,\nwe advocate using the large vision-language model (LVLM) to refine text\ndescriptions and devise a multi-scale ensemble to stablise the matching between\nmasks and entities. Compared to text-only weakly-supervised methods, our\nUni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K\ndatasets, and even surpasses fully-supervised methods on the challenging PASCAL\nContext-459 dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.08960v1.pdf"
    },
    {
        "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
        "authors": [
            "Yinya Huang",
            "Xiaohan Lin",
            "Zhengying Liu",
            "Qingxing Cao",
            "Huajian Xin",
            "Haiming Wang",
            "Zhenguo Li",
            "Linqi Song",
            "Xiaodan Liang"
        ],
        "published": "2024-02-14T05:57:58Z",
        "summary": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.",
        "pdf_link": "https://arxiv.org/pdf/2402.08957v2.pdf"
    },
    {
        "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
        "authors": [
            "Martha Lewis",
            "Melanie Mitchell"
        ],
        "published": "2024-02-14T05:52:23Z",
        "summary": "Large language models (LLMs) have performed well on several reasoning\nbenchmarks, including ones that test analogical reasoning abilities. However,\nit has been debated whether they are actually performing humanlike abstract\nreasoning or instead employing less general processes that rely on similarity\nto what has been seen in their training data. Here we investigate the\ngenerality of analogy-making abilities previously claimed for LLMs (Webb,\nHolyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs\nand create a set of \"counterfactual\" variants-versions that test the same\nabstract reasoning abilities but that are likely dissimilar from any\npre-training data. We test humans and three GPT models on both the original and\ncounterfactual problems, and show that, while the performance of humans remains\nhigh for all the problems, the GPT models' performance declines sharply on the\ncounterfactual set. This work provides evidence that, despite previously\nreported successes of LLMs on analogical reasoning, these models lack the\nrobustness and generality of human analogy-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.08955v1.pdf"
    },
    {
        "title": "Premise Order Matters in Reasoning with Large Language Models",
        "authors": [
            "Xinyun Chen",
            "Ryan A. Chi",
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "published": "2024-02-14T04:50:18Z",
        "summary": "Large language models (LLMs) have accomplished remarkable reasoning\nperformance in various domains. However, in the domain of reasoning tasks, we\ndiscover a frailty: LLMs are surprisingly brittle to the ordering of the\npremises, despite the fact that such ordering does not alter the underlying\ntask. In particular, we observe that LLMs achieve the best performance when the\npremise order aligns with the context required in intermediate reasoning steps.\nFor example, in deductive reasoning tasks, presenting the premises in the same\norder as the ground truth proof in the prompt (as opposed to random ordering)\ndrastically increases the model's accuracy. We first examine the effect of\npremise ordering on deductive reasoning on a variety of LLMs, and our\nevaluation shows that permuting the premise order can cause a performance drop\nof over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to\nexamine the ordering effect for mathematical problem-solving, and we again\nobserve a significant drop in accuracy, relative to the original GSM8K\nbenchmark.",
        "pdf_link": "https://arxiv.org/pdf/2402.08939v2.pdf"
    },
    {
        "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences",
        "authors": [
            "Souradip Chakraborty",
            "Jiahao Qiu",
            "Hui Yuan",
            "Alec Koppel",
            "Furong Huang",
            "Dinesh Manocha",
            "Amrit Singh Bedi",
            "Mengdi Wang"
        ],
        "published": "2024-02-14T03:56:27Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to\nhuman preferences by employing a singular reward model derived from preference\ndata. However, such an approach overlooks the rich diversity of human\npreferences inherent in data collected from multiple users. In this work, we\nfirst derive an impossibility result of alignment with single reward RLHF,\nthereby highlighting its insufficiency in representing diverse human\npreferences. To provide an equitable solution to the problem, we learn a\nmixture of preference distributions via an expectation-maximization algorithm\nand propose a MaxMin alignment objective for policy learning inspired by the\nEgalitarian principle in social choice theory to better represent diverse human\npreferences. We elucidate the connection of our proposed approach to\ndistributionally robust optimization and general utility RL, thereby\nhighlighting the generality and robustness of our proposed solution. We present\ncomprehensive experimental results on small-scale (GPT-2) and large-scale\nlanguage models (with Tulu2-7B) and show the efficacy of the proposed approach\nin the presence of diversity among human preferences. Our algorithm achieves an\naverage improvement of more than 16% in win-rates over conventional RLHF\nalgorithms and improves the win-rate (accuracy) for minority groups by over 33%\nwithout compromising the performance of majority groups, showcasing the\nrobustness and fairness of our approach. We remark that our findings in this\nwork are not only limited to language models but also extend to reinforcement\nlearning in general.",
        "pdf_link": "https://arxiv.org/pdf/2402.08925v1.pdf"
    },
    {
        "title": "Tree-Based Hard Attention with Self-Motivation for Large Language Models",
        "authors": [
            "Chenxi Lin",
            "Jiayu Ren",
            "Guoxiu He",
            "Zhuoren Jiang",
            "Haiyan Yu",
            "Xiaomin Zhu"
        ],
        "published": "2024-02-14T00:40:51Z",
        "summary": "While large language models (LLMs) excel at understanding and generating\nplain text, they are not specifically tailored to handle hierarchical text\nstructures. Extracting the task-desired property from their natural language\nresponses typically necessitates additional processing steps. In fact,\nselectively comprehending the hierarchical structure of large-scale text is\npivotal to understanding its substance. Aligning LLMs more closely with the\nclassification or regression values of specific task through prompting also\nremains challenging. To this end, we propose a novel framework called\nTree-Based Hard Attention with Self-Motivation for Large Language Models\n(TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs\nto process hierarchically structured text inputs. By leveraging prompting, it\nenables a frozen LLM to selectively focus on relevant leaves in relation to the\nroot, generating a tailored symbolic representation of their relationship.\nMoreover, TEAROOM comprises a self-motivation strategy for another LLM equipped\nwith a trainable adapter and a linear layer. The selected symbolic outcomes are\nintegrated into another prompt, along with the predictive value of the task. We\niteratively feed output values back into the prompt, enabling the trainable LLM\nto progressively approximate the golden truth. TEAROOM outperforms existing\nstate-of-the-art methods in experimental evaluations across three benchmark\ndatasets, showing its effectiveness in estimating task-specific properties.\nThrough comprehensive experiments and analysis, we have validated the ability\nof TEAROOM to gradually approach the underlying golden truth through multiple\ninferences.",
        "pdf_link": "https://arxiv.org/pdf/2402.08874v1.pdf"
    },
    {
        "title": "Large Language Model with Graph Convolution for Recommendation",
        "authors": [
            "Yingpeng Du",
            "Ziyan Wang",
            "Zhu Sun",
            "Haoyan Chua",
            "Hongzhi Liu",
            "Zhonghai Wu",
            "Yining Ma",
            "Jie Zhang",
            "Youchen Sun"
        ],
        "published": "2024-02-14T00:04:33Z",
        "summary": "In recent years, efforts have been made to use text information for better\nuser profiling and item characterization in recommendations. However, text\ninformation can sometimes be of low quality, hindering its effectiveness for\nreal-world applications. With knowledge and reasoning capabilities capsuled in\nLarge Language Models (LLMs), utilizing LLMs emerges as a promising way for\ndescription improvement. However, existing ways of prompting LLMs with raw\ntexts ignore structured knowledge of user-item interactions, which may lead to\nhallucination problems like inconsistent description generation. To this end,\nwe propose a Graph-aware Convolutional LLM method to elicit LLMs to capture\nhigh-order relations in the user-item graph. To adapt text-based LLMs with\nstructured graphs, We use the LLM as an aggregator in graph processing,\nallowing it to understand graph-based information step by step. Specifically,\nthe LLM is required for description enhancement by exploring multi-hop\nneighbors layer by layer, thereby propagating information progressively in the\ngraph. To enable LLMs to capture large-scale graph information, we break down\nthe description task into smaller parts, which drastically reduces the context\nlength of the token input with each step. Extensive experiments on three\nreal-world datasets show that our method consistently outperforms\nstate-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.08859v1.pdf"
    },
    {
        "title": "GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency",
        "authors": [
            "Catherine Yeh",
            "Gonzalo Ramos",
            "Rachel Ng",
            "Andy Huntington",
            "Richard Banks"
        ],
        "published": "2024-02-13T23:48:59Z",
        "summary": "Large language models (LLMs) are becoming more prevalent and have found a\nubiquitous use in providing different forms of writing assistance. However,\nLLM-powered writing systems can frustrate users due to their limited\npersonalization and control, which can be exacerbated when users lack\nexperience with prompt engineering. We see design as one way to address these\nchallenges and introduce GhostWriter, an AI-enhanced writing design probe where\nusers can exercise enhanced agency and personalization. GhostWriter leverages\nLLMs to learn the user's intended writing style implicitly as they write, while\nallowing explicit teaching moments through manual style edits and annotations.\nWe study 18 participants who use GhostWriter on two different writing tasks,\nobserving that it helps users craft personalized text generations and empowers\nthem by providing multiple ways to control the system's writing style. From\nthis study, we present insights regarding people's relationship with\nAI-assisted writing and offer design recommendations for future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.08855v1.pdf"
    },
    {
        "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
        "authors": [
            "Bo Peng",
            "Xinyi Ling",
            "Ziru Chen",
            "Huan Sun",
            "Xia Ning"
        ],
        "published": "2024-02-13T22:26:24Z",
        "summary": "With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.08831v1.pdf"
    },
    {
        "title": "Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy",
        "authors": [
            "Gioele Barabucci",
            "Victor Shia",
            "Eugene Chu",
            "Benjamin Harack",
            "Nathan Fu"
        ],
        "published": "2024-02-13T21:24:21Z",
        "summary": "Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's\nPaLM 2 are proposed as viable diagnostic support tools or even spoken of as\nreplacements for \"curbside consults\". However, even LLMs specifically trained\non medical topics may lack sufficient diagnostic accuracy for real-life\napplications.\n  Methods: Using collective intelligence methods and a dataset of 200 clinical\nvignettes of real-life cases, we assessed and compared the accuracy of\ndifferential diagnoses obtained by asking individual commercial LLMs (OpenAI\nGPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of\ndifferential diagnoses synthesized by aggregating responses from combinations\nof the same LLMs.\n  Results: We find that aggregating responses from multiple, various LLMs leads\nto more accurate differential diagnoses (average accuracy for 3 LLMs:\n$75.3\\%\\pm 1.6pp$) compared to the differential diagnoses produced by single\nLLMs (average accuracy for single LLMs: $59.0\\%\\pm 6.1pp$).\n  Discussion: The use of collective intelligence methods to synthesize\ndifferential diagnoses combining the responses of different LLMs achieves two\nof the necessary steps towards advancing acceptance of LLMs as a diagnostic\nsupport tool: (1) demonstrate high diagnostic accuracy and (2) eliminate\ndependence on a single commercial vendor.",
        "pdf_link": "https://arxiv.org/pdf/2402.08806v1.pdf"
    },
    {
        "title": "ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",
        "authors": [
            "Leuson Da Silva",
            "Jordan Samhi",
            "Foutse Khomh"
        ],
        "published": "2024-02-13T21:15:33Z",
        "summary": "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the\npremier platform for developers' queries on programming and software\ndevelopment. Demonstrating an ability to generate instant, human-like responses\nto technical questions, ChatGPT has ignited debates within the developer\ncommunity about the evolving role of human-driven platforms in the age of\ngenerative AI. Two months after ChatGPT's release, Meta released its answer\nwith its own Large Language Model (LLM) called LLaMA: the race was on. We\nconducted an empirical study analyzing questions from Stack Overflow and using\nthese LLMs to address them. This way, we aim to (ii) measure user engagement\nevolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'\nanswers and their potential to replace Stack Overflow in the long term; (iii)\nidentify and understand why LLMs fails; and (iv) compare LLMs together. Our\nempirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,\nyet do not outperform it for some domains, while a significant decline in user\nposting activity has been observed. Furthermore, we also discuss the impact of\nour findings regarding the usage and development of new LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.08801v1.pdf"
    },
    {
        "title": "Rethinking Machine Unlearning for Large Language Models",
        "authors": [
            "Sijia Liu",
            "Yuanshun Yao",
            "Jinghan Jia",
            "Stephen Casper",
            "Nathalie Baracaldo",
            "Peter Hase",
            "Xiaojun Xu",
            "Yuguang Yao",
            "Hang Li",
            "Kush R. Varshney",
            "Mohit Bansal",
            "Sanmi Koyejo",
            "Yang Liu"
        ],
        "published": "2024-02-13T20:51:58Z",
        "summary": "We explore machine unlearning (MU) in the domain of large language models\n(LLMs), referred to as LLM unlearning. This initiative aims to eliminate\nundesirable data influence (e.g., sensitive or illegal information) and the\nassociated model capabilities, while maintaining the integrity of essential\nknowledge generation and not affecting causally unrelated information. We\nenvision LLM unlearning becoming a pivotal element in the life-cycle management\nof LLMs, potentially standing as an essential foundation for developing\ngenerative AI that is not only safe, secure, and trustworthy, but also\nresource-efficient without the need of full retraining. We navigate the\nunlearning landscape in LLMs from conceptual formulation, methodologies,\nmetrics, and applications. In particular, we highlight the often-overlooked\naspects of existing LLM unlearning research, e.g., unlearning scope, data-model\ninteraction, and multifaceted efficacy assessment. We also draw connections\nbetween LLM unlearning and related areas such as model editing, influence\nfunctions, model explanation, adversarial training, and reinforcement learning.\nFurthermore, we outline an effective assessment framework for LLM unlearning\nand explore its applications in copyright and privacy safeguards and\nsociotechnical harm reduction.",
        "pdf_link": "https://arxiv.org/pdf/2402.08787v3.pdf"
    },
    {
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
        "authors": [
            "Alex Havrilla",
            "Sharath Raparthy",
            "Christoforus Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Roberta Railneau"
        ],
        "published": "2024-02-13T20:16:29Z",
        "summary": "State-of-the-art language models can exhibit impressive reasoning refinement\ncapabilities on math, science or coding tasks. However, recent work\ndemonstrates that even the best models struggle to identify \\textit{when and\nwhere to refine} without access to external feedback. Outcome-based Reward\nModels (\\textbf{ORMs}), trained to predict correctness of the final answer\nindicating when to refine, offer one convenient solution for deciding when to\nrefine. Process Based Reward Models (\\textbf{PRMs}), trained to predict\ncorrectness of intermediate steps, can then be used to indicate where to\nrefine. But they are expensive to train, requiring extensive human annotations.\nIn this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained,\nonly on synthetic data, to approximate the expected future reward of the\noptimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict\nthe correctness of the final answer when sampling the current policy many times\n(rather than only once as in the case of ORMs). Our experiments show that SORMs\ncan more accurately detect incorrect reasoning steps compared to ORMs, thus\nimproving downstream accuracy when doing refinements. We then train\n\\textit{global} refinement models, which take only the question and a draft\nsolution as input and predict a corrected solution, and \\textit{local}\nrefinement models which also take as input a critique indicating the location\nof the first reasoning error. We generate training data for both models\nsynthetically by reusing data used to train the SORM. We find combining global\nand local refinements, using the ORM as a reranker, significantly outperforms\neither one individually, as well as a best of three sample baseline. With this\nstrategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned\nwith RL) on GSM8K from 53\\% to 65\\% when greedily sampled.",
        "pdf_link": "https://arxiv.org/pdf/2402.10963v1.pdf"
    },
    {
        "title": "Measuring and Controlling Instruction (In)Stability in Language Model Dialogs",
        "authors": [
            "Kenneth Li",
            "Tianle Liu",
            "Naomi Bashkansky",
            "David Bau",
            "Fernanda Vi\u00e9gas",
            "Hanspeter Pfister",
            "Martin Wattenberg"
        ],
        "published": "2024-02-13T20:10:29Z",
        "summary": "System-prompting is a standard tool for customizing language-model chatbots,\nenabling them to follow a specific instruction. An implicit assumption in the\nuse of system prompts is that they will be stable, so the chatbot will continue\nto generate text according to the stipulated instructions for the duration of a\nconversation. We propose a quantitative benchmark to test this assumption,\nevaluating instruction stability via self-chats between two instructed\nchatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a\nsignificant instruction drift within eight rounds of conversations. An\nempirical and theoretical analysis of this phenomenon suggests the transformer\nattention mechanism plays a role, due to attention decay over long exchanges.\nTo combat attention decay and instruction drift, we propose a lightweight\nmethod called split-softmax, which compares favorably against two strong\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.10962v2.pdf"
    },
    {
        "title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models",
        "authors": [
            "Jillian Fisher",
            "Ximing Lu",
            "Jaehun Jung",
            "Liwei Jiang",
            "Zaid Harchaoui",
            "Yejin Choi"
        ],
        "published": "2024-02-13T19:54:29Z",
        "summary": "The permanence of online content combined with the enhanced authorship\nidentification techniques calls for stronger computational methods to protect\nthe identity and privacy of online authorship when needed, e.g., blind reviews\nfor scientific papers, anonymous online reviews, or anonymous interactions in\nthe mental health forums. In this paper, we propose an unsupervised\ninference-time approach to authorship obfuscation to address the unique\nchallenges of authorship obfuscation: lack of supervision data for diverse\nauthorship and domains, and the need for a sufficient level of revision beyond\nsimple paraphrasing to obfuscate the authorship, all the while preserving the\noriginal content and fluency.\n  We introduce JAMDEC, a user-controlled, inference-time algorithm for\nauthorship obfuscation that can be in principle applied to any text and\nauthorship. Our approach builds on small language models such as GPT2-XL in\norder to help avoid disclosing the original content to proprietary LLM's APIs,\nwhile also reducing the performance gap between small and large language models\nvia algorithmic enhancement. The key idea behind our approach is to boost the\ncreative power of smaller language models through constrained decoding, while\nalso allowing for user-specified controls and flexibility. Experimental results\ndemonstrate that our approach based on GPT2-XL outperforms previous\nstate-of-the-art methods based on comparably small models, while performing\ncompetitively against GPT3.5 175B, a propriety model that is two orders of\nmagnitudes larger.",
        "pdf_link": "https://arxiv.org/pdf/2402.08761v1.pdf"
    },
    {
        "title": "LLM-driven Imitation of Subrational Behavior : Illusion or Reality?",
        "authors": [
            "Andrea Coletta",
            "Kshama Dwarakanath",
            "Penghang Liu",
            "Svitlana Vyetrenko",
            "Tucker Balch"
        ],
        "published": "2024-02-13T19:46:39Z",
        "summary": "Modeling subrational agents, such as humans or economic households, is\ninherently challenging due to the difficulty in calibrating reinforcement\nlearning models or collecting data that involves human subjects. Existing work\nhighlights the ability of Large Language Models (LLMs) to address complex\nreasoning tasks and mimic human communication, while simulation using LLMs as\nagents shows emergent social behaviors, potentially improving our comprehension\nof human conduct. In this paper, we propose to investigate the use of LLMs to\ngenerate synthetic human demonstrations, which are then used to learn\nsubrational agent policies though Imitation Learning. We make an assumption\nthat LLMs can be used as implicit computational models of humans, and propose a\nframework to use synthetic demonstrations derived from LLMs to model\nsubrational behaviors that are characteristic of humans (e.g., myopic behavior\nor preference for risk aversion). We experimentally evaluate the ability of our\nframework to model sub-rationality through four simple scenarios, including the\nwell-researched ultimatum game and marshmallow experiment. To gain confidence\nin our framework, we are able to replicate well-established findings from prior\nhuman studies associated with the above scenarios. We conclude by discussing\nthe potential benefits, challenges and limitations of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.08755v1.pdf"
    },
    {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
        "authors": [
            "Linxi Zhao",
            "Yihe Deng",
            "Weitong Zhang",
            "Quanquan Gu"
        ],
        "published": "2024-02-13T18:59:05Z",
        "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.",
        "pdf_link": "https://arxiv.org/pdf/2402.08680v1.pdf"
    },
    {
        "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
        "authors": [
            "Xingang Guo",
            "Fangxu Yu",
            "Huan Zhang",
            "Lianhui Qin",
            "Bin Hu"
        ],
        "published": "2024-02-13T18:58:48Z",
        "summary": "Jailbreaks on Large language models (LLMs) have recently received increasing\nattention. For a comprehensive assessment of LLM safety, it is essential to\nconsider jailbreaks with diverse attributes, such as contextual coherence and\nsentiment/stylistic variations, and hence it is beneficial to study\ncontrollable jailbreaking, i.e. how to enforce control on LLM attacks. In this\npaper, we formally formulate the controllable attack generation problem, and\nbuild a novel connection between this problem and controllable text generation,\na well-explored topic of natural language processing. Based on this connection,\nwe adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a\nstate-of-the-art, highly efficient algorithm in controllable text generation,\nand introduce the COLD-Attack framework which unifies and automates the search\nof adversarial LLM attacks under a variety of control requirements such as\nfluency, stealthiness, sentiment, and left-right-coherence. The controllability\nenabled by COLD-Attack leads to diverse new jailbreak scenarios which not only\ncover the standard setting of generating fluent suffix attacks, but also allow\nus to address new controllable attack settings such as revising a user query\nadversarially with minimal paraphrasing, and inserting stealthy attacks in\ncontext with left-right-coherence. Our extensive experiments on various LLMs\n(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad\napplicability, strong controllability, high success rate, and attack\ntransferability. Our code is available at\nhttps://github.com/Yu-Fangxu/COLD-Attack.",
        "pdf_link": "https://arxiv.org/pdf/2402.08679v1.pdf"
    },
    {
        "title": "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks",
        "authors": [
            "Jacob Russin",
            "Ellie Pavlick",
            "Michael J. Frank"
        ],
        "published": "2024-02-13T18:55:27Z",
        "summary": "Human learning is sensitive to rule-like structure and the curriculum of\nexamples used for training. In tasks governed by succinct rules, learning is\nmore robust when related examples are blocked across trials, but in the absence\nof such rules, interleaving is more effective. To date, no neural model has\nsimultaneously captured these seemingly contradictory effects. Here we show\nthat this same tradeoff spontaneously emerges with \"in-context learning\" (ICL)\nboth in neural networks trained with metalearning and in large language models\n(LLMs). ICL is the ability to learn new tasks \"in context\" - without weight\nchanges - via an inner-loop algorithm implemented in activation dynamics.\nExperiments with pretrained LLMs and metalearning transformers show that ICL\nexhibits the blocking advantage demonstrated in humans on a task involving\nrule-like structure, and conversely, that concurrent in-weight learning\nreproduces the interleaving advantage observed in humans on tasks lacking such\nstructure.",
        "pdf_link": "https://arxiv.org/pdf/2402.08674v1.pdf"
    },
    {
        "title": "Improving Generalization in Semantic Parsing by Increasing Natural Language Variation",
        "authors": [
            "Irina Saparina",
            "Mirella Lapata"
        ],
        "published": "2024-02-13T18:48:23Z",
        "summary": "Text-to-SQL semantic parsing has made significant progress in recent years,\nwith various models demonstrating impressive performance on the challenging\nSpider benchmark. However, it has also been shown that these models often\nstruggle to generalize even when faced with small perturbations of previously\n(accurately) parsed expressions. This is mainly due to the linguistic form of\nquestions in Spider which are overly specific, unnatural, and display limited\nvariation. In this work, we use data augmentation to enhance the robustness of\ntext-to-SQL parsers against natural language variations. Existing approaches\ngenerate question reformulations either via models trained on Spider or only\nintroduce local changes. In contrast, we leverage the capabilities of large\nlanguage models to generate more realistic and diverse questions. Using only a\nfew prompts, we achieve a two-fold increase in the number of questions in\nSpider. Training on this augmented dataset yields substantial improvements on a\nrange of evaluation sets, including robustness benchmarks and out-of-domain\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2402.08666v1.pdf"
    },
    {
        "title": "The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting",
        "authors": [
            "David Haag",
            "Devender Kumar",
            "Sebastian Gruber",
            "Mahdi Sareban",
            "Gunnar Treff",
            "Josef Niebauer",
            "Christopher Bull",
            "Jan David Smeddinck"
        ],
        "published": "2024-02-13T18:39:36Z",
        "summary": "We explored the viability of Large Language Models (LLMs) for triggering and\npersonalizing content for Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. JITAIs are being explored as a key mechanism for sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. However, traditional rule-based and machine learning models for JITAI\nimplementation face scalability and reliability limitations, such as lack of\npersonalization, difficulty in managing multi-parametric systems, and issues\nwith data sparsity. To investigate JITAI implementation via LLMs, we tested the\ncontemporary overall performance-leading model 'GPT-4' with examples grounded\nin the use case of fostering heart-healthy physical activity in outpatient\ncardiac rehabilitation. Three personas and five sets of context information per\npersona were used as a basis of triggering and personalizing JITAIs.\nSubsequently, we generated a total of 450 proposed JITAI decisions and message\ncontent, divided equally into JITAIs generated by 10 iterations with GPT-4, a\nbaseline provided by 10 laypersons (LayPs), and a gold standard set by 10\nhealthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs\ngenerated by GPT-4 were superior to those by HCPs and LayPs over all assessed\nscales: i.e., appropriateness, engagement, effectiveness, and professionality.\nThis study indicates that LLMs have significant potential for implementing\nJITAIs as a building block of personalized or \"precision\" health, offering\nscalability, effective personalization based on opportunistically sampled\ninformation, and good acceptability.",
        "pdf_link": "https://arxiv.org/pdf/2402.08658v1.pdf"
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "authors": [
            "Michael Dorkenwald",
            "Nimrod Barazani",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ],
        "published": "2024-02-13T18:39:18Z",
        "summary": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown\nimmense potential by integrating large language models with vision systems.\nNevertheless, these models face challenges in the fundamental computer vision\ntask of object localisation, due to their training on multimodal data\ncontaining mostly captions without explicit spatial grounding. While it is\npossible to construct custom, supervised training pipelines with bounding box\nannotations that integrate with VLMs, these result in specialized and\nhard-to-scale models. In this paper, we aim to explore the limits of\ncaption-based VLMs and instead propose to tackle the challenge in a simpler\nmanner by i) keeping the weights of a caption-based VLM frozen and ii) not\nusing any supervised detection data. To this end, we introduce an\ninput-agnostic Positional Insert (PIN), a learnable spatial prompt, containing\na minimal set of parameters that are slid inside the frozen VLM, unlocking\nobject localisation capabilities. Our PIN module is trained with a simple\nnext-token prediction task on synthetic data without requiring the introduction\nof new output heads. Our experiments demonstrate strong zero-shot localisation\nperformances on a variety of images, including Pascal VOC, COCO, LVIS, and\ndiverse images like paintings or cartoons.",
        "pdf_link": "https://arxiv.org/pdf/2402.08657v1.pdf"
    },
    {
        "title": "Tandem Transformers for Inference Efficient LLMs",
        "authors": [
            "Aishwarya P S",
            "Pranav Ajit Nair",
            "Yashas Samaga",
            "Toby Boyd",
            "Sanjiv Kumar",
            "Prateek Jain",
            "Praneeth Netrapalli"
        ],
        "published": "2024-02-13T18:24:08Z",
        "summary": "The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.08644v3.pdf"
    },
    {
        "title": "SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages",
        "authors": [
            "Nedjma Ousidhoum",
            "Shamsuddeen Hassan Muhammad",
            "Mohamed Abdalla",
            "Idris Abdulmumin",
            "Ibrahim Said Ahmad",
            "Sanchit Ahuja",
            "Alham Fikri Aji",
            "Vladimir Araujo",
            "Abinew Ali Ayele",
            "Pavan Baswani",
            "Meriem Beloucif",
            "Chris Biemann",
            "Sofia Bourhim",
            "Christine De Kock",
            "Genet Shanko Dekebo",
            "Oumaima Hourrane",
            "Gopichand Kanumolu",
            "Lokesh Madasu",
            "Samuel Rutunda",
            "Manish Shrivastava",
            "Thamar Solorio",
            "Nirmal Surange",
            "Hailegnaw Getaneh Tilaye",
            "Krishnapriya Vishnubhotla",
            "Genta Winata",
            "Seid Muhie Yimam",
            "Saif M. Mohammad"
        ],
        "published": "2024-02-13T18:04:53Z",
        "summary": "Exploring and quantifying semantic relatedness is central to representing\nlanguage. It holds significant implications across various NLP tasks, including\noffering insights into the capabilities and performance of Large Language\nModels (LLMs). While earlier NLP research primarily focused on semantic\nsimilarity, often within the English language context, we instead investigate\nthe broader phenomenon of semantic relatedness. In this paper, we present\nSemRel, a new semantic relatedness dataset collection annotated by native\nspeakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English,\nHausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern\nStandard Arabic, Punjabi, Spanish, and Telugu. These languages originate from\nfive distinct language families and are predominantly spoken in Africa and Asia\n-- regions characterised by a relatively limited availability of NLP resources.\nEach instance in the SemRel datasets is a sentence pair associated with a score\nthat represents the degree of semantic textual relatedness between the two\nsentences. The scores are obtained using a comparative annotation framework. We\ndescribe the data collection and annotation processes, related challenges when\nbuilding the datasets, and their impact and utility in NLP. We further report\nexperiments for each language and across the different languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.08638v3.pdf"
    },
    {
        "title": "Knowledge Editing on Black-box Large Language Models",
        "authors": [
            "Xiaoshuai Song",
            "Zhengyang Wang",
            "Keqing He",
            "Guanting Dong",
            "Yutao Mou",
            "Jinxu Zhao",
            "Weiran Xu"
        ],
        "published": "2024-02-13T17:59:34Z",
        "summary": "Knowledge editing (KE) aims to efficiently and precisely modify the behavior\nof large language models (LLMs) to update specific knowledge without negatively\ninfluencing other knowledge. Current research primarily focuses on white-box\nLLMs editing, overlooking an important scenario: black-box LLMs editing, where\nLLMs are accessed through interfaces and only textual output is available. In\nthis paper, we first officially introduce KE on black-box LLMs and then propose\na comprehensive evaluation framework to overcome the limitations of existing\nevaluations that are not applicable to black-box LLMs editing and lack\ncomprehensiveness. To tackle privacy leaks of editing data and style\nover-editing in current methods, we introduce a novel postEdit framework,\nresolving privacy concerns through downstream post-processing and maintaining\ntextual style consistency via fine-grained editing to original responses.\nExperiments and analysis on two benchmarks demonstrate that postEdit\noutperforms all baselines and achieves strong generalization, especially with\nhuge improvements on style retention (average $+20.82\\%\\uparrow$).",
        "pdf_link": "https://arxiv.org/pdf/2402.08631v2.pdf"
    },
    {
        "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment",
        "authors": [
            "Yongchao Chen",
            "Jacob Arkin",
            "Yilun Hao",
            "Yang Zhang",
            "Nicholas Roy",
            "Chuchu Fan"
        ],
        "published": "2024-02-13T16:38:01Z",
        "summary": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework that incorporates human-designed feedback rules about\npotential errors to automatically offer direct suggestions for improvement. Our\nframework is stylized as a genetic algorithm in which an LLM generates new\ncandidate prompts from a parent prompt and its associated feedback; we use a\nlearned heuristic function that predicts prompt performance to efficiently\nsample from these candidates. This approach significantly outperforms both\nhuman-engineered prompts and several other prompt optimization methods across\neight representative multi-step tasks (an average 27.7% and 28.2% improvement\nto current best methods on GPT-3.5 and GPT-4, respectively). We further show\nthat the score function for tasks can be modified to better align with\nindividual preferences. We believe our work can serve as a benchmark for\nautomatic prompt optimization for LLM-driven multi-step tasks. Datasets and\nCodes are available at https://github.com/yongchao98/PROMST. Project Page is\navailable at https://yongchao98.github.io/MIT-REALM-PROMST.",
        "pdf_link": "https://arxiv.org/pdf/2402.08702v1.pdf"
    },
    {
        "title": "Test-Time Backdoor Attacks on Multimodal Large Language Models",
        "authors": [
            "Dong Lu",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Xianjun Yang",
            "Min Lin"
        ],
        "published": "2024-02-13T16:28:28Z",
        "summary": "Backdoor attacks are commonly executed by contaminating training data, such\nthat a trigger can activate predetermined harmful effects during the test\nphase. In this work, we present AnyDoor, a test-time backdoor attack against\nmultimodal large language models (MLLMs), which involves injecting the backdoor\ninto the textual modality using adversarial test images (sharing the same\nuniversal perturbation), without requiring access to or modification of the\ntraining data. AnyDoor employs similar techniques used in universal adversarial\nattacks, but distinguishes itself by its ability to decouple the timing of\nsetup and activation of harmful effects. In our experiments, we validate the\neffectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,\nInstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.\nNotably, because the backdoor is injected by a universal perturbation, AnyDoor\ncan dynamically change its backdoor trigger prompts/harmful effects, exposing a\nnew challenge for defending against backdoor attacks. Our project page is\navailable at https://sail-sg.github.io/AnyDoor/.",
        "pdf_link": "https://arxiv.org/pdf/2402.08577v1.pdf"
    },
    {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": [
            "Xiangming Gu",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Ye Wang",
            "Jing Jiang",
            "Min Lin"
        ],
        "published": "2024-02-13T16:06:17Z",
        "summary": "A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.",
        "pdf_link": "https://arxiv.org/pdf/2402.08567v1.pdf"
    },
    {
        "title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style",
        "authors": [
            "Preetika Verma",
            "Kokil Jaidka",
            "Svetlana Churina"
        ],
        "published": "2024-02-13T14:53:12Z",
        "summary": "We audited counter-arguments generated by large language models (LLMs),\nfocusing on their ability to generate evidence-based and stylistic\ncounter-arguments to posts from the Reddit ChangeMyView dataset. Our evaluation\nis based on Counterfire: a new dataset of 32,000 counter-arguments generated\nfrom large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned\nvariants, and PaLM 2, with varying prompts for evidence use and argumentative\nstyle. GPT-3.5 Turbo ranked highest in argument quality with strong\nparaphrasing and style adherence, particularly in `reciprocity' style\narguments. However, the `No Style' counter-arguments proved most persuasive on\naverage. The findings suggest that a balance between evidentiality and\nstylistic elements is vital to a compelling counter-argument. We close with a\ndiscussion of future research directions and implications for fine-tuning LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.08498v3.pdf"
    },
    {
        "title": "The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale",
        "authors": [
            "Xiaoqiang Liu",
            "Yubin Wang",
            "Zicheng Huang",
            "Boming Xu",
            "Yilin Zeng",
            "Xinqi Chen",
            "Zilong Wang",
            "Enning Yang",
            "Xiaoxuan Lei",
            "Yisen Huang",
            "Xiaobo Liu"
        ],
        "published": "2024-02-13T14:38:12Z",
        "summary": "Background: Colonoscopy, a crucial diagnostic tool in gastroenterology,\ndepends heavily on superior bowel preparation. ChatGPT, a large language model\nwith emergent intelligence which also exhibits potential in medical\napplications. This study aims to assess the accuracy and consistency of ChatGPT\nin using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.\nMethods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.\nThese images were evaluated using the BBPS by 3 senior endoscopists and 3\nnovice endoscopists. Additionally, ChatGPT also assessed these images, having\nbeen divided into three groups and undergone specific Fine-tuning. Consistency\nwas evaluated through two rounds of testing. Results: In the initial round,\nChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists'\naccuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and\n0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT\nshows promise in bowel preparation scoring, it currently does not match the\naccuracy and consistency of experienced endoscopists. Future research should\nfocus on in-depth Fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.08492v1.pdf"
    },
    {
        "title": "Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale",
        "authors": [
            "Freddy Heppell",
            "Mehmet E. Bakir",
            "Kalina Bontcheva"
        ],
        "published": "2024-02-13T13:50:08Z",
        "summary": "As Large Language Models (LLMs) become more proficient, their misuse in\nlarge-scale viral disinformation campaigns is a growing concern. This study\nexplores the capability of ChatGPT to generate unconditioned claims about the\nwar in Ukraine, an event beyond its knowledge cutoff, and evaluates whether\nsuch claims can be differentiated by human readers and automated tools from\nhuman-written ones. We compare war-related claims from ClaimReview, authored by\nIFCN-registered fact-checkers, and similar short-form content generated by\nChatGPT. We demonstrate that ChatGPT can produce realistic, target-specific\ndisinformation cheaply, fast, and at scale, and that these claims cannot be\nreliably distinguished by humans or existing automated tools.",
        "pdf_link": "https://arxiv.org/pdf/2402.08467v1.pdf"
    },
    {
        "title": "Large Language Models as Minecraft Agents",
        "authors": [
            "Chris Madge",
            "Massimo Poesio"
        ],
        "published": "2024-02-13T11:37:30Z",
        "summary": "In this work we examine the use of Large Language Models (LLMs) in the\nchallenging setting of acting as a Minecraft agent. We apply and evaluate LLMs\nin the builder and architect settings, introduce clarification questions and\nexamining the challenges and opportunities for improvement. In addition, we\npresent a platform for online interaction with the agents and an evaluation\nagainst previous works.",
        "pdf_link": "https://arxiv.org/pdf/2402.08392v1.pdf"
    },
    {
        "title": "Punctuation Restoration Improves Structure Understanding without Supervision",
        "authors": [
            "Junghyun Min",
            "Minho Lee",
            "Woochul Lee",
            "Yeonsoo Lee"
        ],
        "published": "2024-02-13T11:22:52Z",
        "summary": "Unsupervised learning objectives like language modeling and de-noising\nconstitute a significant part in producing pre-trained models that perform\nvarious downstream applications from natural language understanding to\nconversational tasks. However, despite impressive generative capabilities of\nrecent large language models, their abilities to capture syntactic or semantic\nstructure within text lag behind. We hypothesize that the mismatch between\nlinguistic performance and competence in machines is attributable to\ninsufficient transfer of linguistic structure knowledge to computational\nsystems with currently popular pre-training objectives. We show that\npunctuation restoration as a learning objective improves in- and\nout-of-distribution performance on structure-related tasks like named entity\nrecognition, open information extraction, chunking, and part-of-speech tagging.\nPunctuation restoration is an effective learning objective that can improve\nstructure understanding and yield a more robust structure-aware representations\nof natural language.",
        "pdf_link": "https://arxiv.org/pdf/2402.08382v2.pdf"
    },
    {
        "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
        "authors": [
            "Miltiadis Allamanis",
            "Sheena Panthaplackel",
            "Pengcheng Yin"
        ],
        "published": "2024-02-13T11:08:08Z",
        "summary": "To evaluate code large language models (LLMs), research has relied on a few\nsmall manually curated benchmarks, such as HumanEval and MBPP, which represent\na narrow part of the real-world software domains. In this work, we introduce\nround-trip correctness (RTC) as an alternative evaluation method. RTC allows\nCode LLM evaluation on a broader spectrum of real-world software domains\nwithout the need for costly human curation. RTC rests on the idea that we can\nask a model to make a prediction (e.g., describe some code using natural\nlanguage), feed that prediction back (e.g., synthesize code from the predicted\ndescription), and check if this round-trip leads to code that is semantically\nequivalent to the original input. We show how to employ RTC to evaluate code\nsynthesis and editing. We find that RTC strongly correlates with model\nperformance on existing narrow-domain code synthesis benchmarks while allowing\nus to expand to a much broader set of domains and tasks which was not\npreviously possible without costly human annotations.",
        "pdf_link": "https://arxiv.org/pdf/2402.08699v1.pdf"
    },
    {
        "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries",
        "authors": [
            "Jonathan F\u00fcrst",
            "Catherine Kosten",
            "Farhard Nooralahzadeh",
            "Yi Zhang",
            "Kurt Stockinger"
        ],
        "published": "2024-02-13T10:28:57Z",
        "summary": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
        "pdf_link": "https://arxiv.org/pdf/2402.08349v1.pdf"
    },
    {
        "title": "Visually Dehallucinative Instruction Generation",
        "authors": [
            "Sungguk Cha",
            "Jusung Lee",
            "Younghyun Lee",
            "Cheoljong Yang"
        ],
        "published": "2024-02-13T10:25:45Z",
        "summary": "In recent years, synthetic visual instructions by generative language model\nhave demonstrated plausible text generation performance on the visual\nquestion-answering tasks. However, challenges persist in the hallucination of\ngenerative language models, i.e., the generated image-text data contains\nunintended contents. This paper presents a novel and scalable method for\ngenerating visually dehallucinative instructions, dubbed CAP2QA, that\nconstrains the scope to only image contents. Our key contributions lie in\nintroducing image-aligned instructive QA dataset CAP2QA-COCO and its scalable\nrecipe. In our experiments, we compare synthetic visual instruction datasets\nthat share the same source data by visual instruction tuning and conduct\ngeneral visual recognition tasks. It shows that our proposed method\nsignificantly reduces visual hallucination while consistently improving visual\nrecognition ability and expressiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.08348v1.pdf"
    },
    {
        "title": "Eliciting Personality Traits in Large Language Models",
        "authors": [
            "Airlie Hilliard",
            "Cristian Munoz",
            "Zekun Wu",
            "Adriano Soares Koshiyama"
        ],
        "published": "2024-02-13T10:09:00Z",
        "summary": "Large Language Models (LLMs) are increasingly being utilized by both\ncandidates and employers in the recruitment context. However, with this comes\nnumerous ethical concerns, particularly related to the lack of transparency in\nthese \"black-box\" models. Although previous studies have sought to increase the\ntransparency of these models by investigating the personality traits of LLMs,\nmany of the previous studies have provided them with personality assessments to\ncomplete. On the other hand, this study seeks to obtain a better understanding\nof such models by examining their output variations based on different input\nprompts. Specifically, we use a novel elicitation approach using prompts\nderived from common interview questions, as well as prompts designed to elicit\nparticular Big Five personality traits to examine whether the models were\nsusceptible to trait-activation like humans are, to measure their personality\nbased on the language used in their outputs. To do so, we repeatedly prompted\nmultiple LMs with different parameter sizes, including Llama-2, Falcon,\nMistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined\ntheir personality using classifiers trained on the myPersonality dataset. Our\nresults reveal that, generally, all LLMs demonstrate high openness and low\nextraversion. However, whereas LMs with fewer parameters exhibit similar\nbehaviour in personality traits, newer and LMs with more parameters exhibit a\nbroader range of personality traits, with increased agreeableness, emotional\nstability, and openness. Furthermore, a greater number of parameters is\npositively associated with openness and conscientiousness. Moreover, fine-tuned\nmodels exhibit minor modulations in their personality traits, contingent on the\ndataset. Implications and directions for future research are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.08341v2.pdf"
    },
    {
        "title": "Prompted Contextual Vectors for Spear-Phishing Detection",
        "authors": [
            "Daniel Nahmias",
            "Gal Engelberg",
            "Dan Klein",
            "Asaf Shabtai"
        ],
        "published": "2024-02-13T09:12:55Z",
        "summary": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include an innovative document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.08309v2.pdf"
    },
    {
        "title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language",
        "authors": [
            "Yin Fang",
            "Kangwei Liu",
            "Ningyu Zhang",
            "Xinle Deng",
            "Penghui Yang",
            "Zhuo Chen",
            "Xiangru Tang",
            "Mark Gerstein",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2024-02-13T09:06:14Z",
        "summary": "As Large Language Models (LLMs) rapidly evolve, their influence in science is\nbecoming increasingly prominent. The emerging capabilities of LLMs in task\ngeneralization and free-form dialogue can significantly advance fields like\nchemistry and biology. However, the field of single-cell biology, which forms\nthe foundational building blocks of living organisms, still faces several\nchallenges. High knowledge barriers and limited scalability in current methods\nrestrict the full exploitation of LLMs in mastering single-cell data, impeding\ndirect accessibility and rapid iteration. To this end, we introduce ChatCell,\nwhich signifies a paradigm shift by facilitating single-cell analysis with\nnatural language. Leveraging vocabulary adaptation and unified sequence\ngeneration, ChatCell has acquired profound expertise in single-cell biology and\nthe capability to accommodate a diverse range of analysis tasks. Extensive\nexperiments further demonstrate ChatCell's robust performance and potential to\ndeepen single-cell insights, paving the way for more accessible and intuitive\nexploration in this pivotal field. Our project homepage is available at\nhttps://zjunlp.github.io/project/ChatCell.",
        "pdf_link": "https://arxiv.org/pdf/2402.08303v4.pdf"
    },
    {
        "title": "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering",
        "authors": [
            "Tobias Schimanski",
            "Jingwei Ni",
            "Mathias Kraus",
            "Elliott Ash",
            "Markus Leippold"
        ],
        "published": "2024-02-13T08:12:48Z",
        "summary": "Advances towards more faithful and traceable answers of Large Language Models\n(LLMs) are crucial for various research and practical endeavors. One avenue in\nreaching this goal is basing the answers on reliable sources. However, this\nEvidence-Based QA has proven to work insufficiently with LLMs in terms of\nciting the correct sources (source quality) and truthfully representing the\ninformation within sources (answer attributability). In this work, we\nsystematically investigate how to robustly fine-tune LLMs for better source\nquality and answer attributability. Specifically, we introduce a data\ngeneration pipeline with automated data quality filters, which can synthesize\ndiversified high-quality training and testing data at scale. We further\nintroduce four test sets to benchmark the robustness of fine-tuned specialist\nmodels. Extensive evaluation shows that fine-tuning on synthetic data improves\nperformance on both in- and out-of-distribution. Furthermore, we show that data\nquality, which can be drastically improved by proposed quality filters, matters\nmore than quantity in improving Evidence-Based QA.",
        "pdf_link": "https://arxiv.org/pdf/2402.08277v3.pdf"
    },
    {
        "title": "A Survey of Table Reasoning with Large Language Models",
        "authors": [
            "Xuanliang Zhang",
            "Dingzirui Wang",
            "Longxu Dou",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "published": "2024-02-13T07:17:52Z",
        "summary": "Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.08259v1.pdf"
    },
    {
        "title": "BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT",
        "authors": [
            "Siqi Peng",
            "Hongyuan Yang",
            "Akihiro Yamamoto"
        ],
        "published": "2024-02-13T06:02:05Z",
        "summary": "We propose BERT4FCA, a novel method for link prediction in bipartite\nnetworks, using formal concept analysis (FCA) and BERT. Link prediction in\nbipartite networks is an important task that can solve various practical\nproblems like friend recommendation in social networks and co-authorship\nprediction in author-paper networks. Recent research has found that in\nbipartite networks, maximal bi-cliques provide important information for link\nprediction, and they can be extracted by FCA. Some FCA-based bipartite link\nprediction methods have achieved good performance. However, we figured out that\ntheir performance could be further improved because these methods did not fully\ncapture the rich information of the extracted maximal bi-cliques. To address\nthis limitation, we propose an approach using BERT, which can learn more\ninformation from the maximal bi-cliques extracted by FCA and use them to make\nlink prediction. We conduct experiments on three real-world bipartite networks\nand demonstrate that our method outperforms previous FCA-based methods, and\nsome classic methods such as matrix-factorization and node2vec.",
        "pdf_link": "https://arxiv.org/pdf/2402.08236v1.pdf"
    },
    {
        "title": "Privacy-Preserving Language Model Inference with Instance Obfuscation",
        "authors": [
            "Yixiang Yao",
            "Fei Wang",
            "Srivatsan Ravi",
            "Muhao Chen"
        ],
        "published": "2024-02-13T05:36:54Z",
        "summary": "Language Models as a Service (LMaaS) offers convenient access for developers\nand researchers to perform inference using pre-trained language models.\nNonetheless, the input data and the inference results containing private\ninformation are exposed as plaintext during the service call, leading to\nprivacy issues. Recent studies have started tackling the privacy issue by\ntransforming input data into privacy-preserving representation from the\nuser-end with the techniques such as noise addition and content perturbation,\nwhile the exploration of inference result protection, namely decision privacy,\nis still a blank page. In order to maintain the black-box manner of LMaaS,\nconducting data privacy protection, especially for the decision, is a\nchallenging task because the process has to be seamless to the models and\naccompanied by limited communication and computation overhead. We thus propose\nInstance-Obfuscated Inference (IOI) method, which focuses on addressing the\ndecision privacy issue of natural language understanding tasks in their\ncomplete life-cycle. Besides, we conduct comprehensive experiments to evaluate\nthe performance as well as the privacy-protection strength of the proposed\nmethod on various benchmarking tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.08227v1.pdf"
    },
    {
        "title": "Improving Black-box Robustness with In-Context Rewriting",
        "authors": [
            "Kyle O'Brien",
            "Nathan Ng",
            "Isha Puri",
            "Jorge Mendez",
            "Hamid Palangi",
            "Yoon Kim",
            "Marzyeh Ghassemi",
            "Thomas Hartvigsen"
        ],
        "published": "2024-02-13T05:33:35Z",
        "summary": "Machine learning models often excel on in-distribution (ID) data but struggle\nwith unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD\nrobustness are not applicable to settings where the model is effectively a\nblack box, such as when the weights are frozen, retraining is costly, or the\nmodel is leveraged via an API. Test-time augmentation (TTA) is a simple\npost-hoc technique for improving robustness that sidesteps black-box\nconstraints by aggregating predictions across multiple augmentations of the\ntest input. TTA has seen limited use in NLP due to the challenge of generating\neffective natural language augmentations. In this work, we propose LLM-TTA,\nwhich uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA\noutperforms conventional augmentation functions across sentiment, toxicity, and\nnews classification tasks for BERT and T5 models, with BERT's OOD robustness\nimproving by an average of 4.30 percentage points without regressing average ID\nperformance. We explore selectively augmenting inputs based on prediction\nentropy to reduce the rate of expensive LLM augmentations, allowing us to\nmaintain performance gains while reducing the average number of generated\naugmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,\ndoes not require OOD labels, and is effective across low and high-resource\nsettings. We share our data, models, and code for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.08225v2.pdf"
    },
    {
        "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Wei Wei",
            "Chao Zhang",
            "Bo Dai"
        ],
        "published": "2024-02-13T05:15:46Z",
        "summary": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini\nfor specific tasks is challenging. Due to the opacity in their parameters,\nembeddings, and even output probabilities, existing fine-tuning adaptation\nmethods are inapplicable. Consequently, adapting these black-box LLMs is only\npossible through their API services, raising concerns about transparency,\nprivacy, and cost. To address these challenges, we introduce BBox-Adapter, a\nnovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target\nand source domain data by treating target data as positive and source data as\nnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to\npromote the likelihood of target domain data while penalizing that of the\nsource domain. Furthermore, it features an online adaptation mechanism, which\nincorporates real-time positive data sampling from ground-truth, human, or AI\nfeedback, coupled with negative data from previous adaptations. Extensive\nexperiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It\nimproves model performance by up to 6.77% across diverse tasks and domains,\nwhile reducing training and inference costs by 31.30x and 1.84x, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.08219v1.pdf"
    },
    {
        "title": "LLaGA: Large Language and Graph Assistant",
        "authors": [
            "Runjin Chen",
            "Tong Zhao",
            "Ajay Jaiswal",
            "Neil Shah",
            "Zhangyang Wang"
        ],
        "published": "2024-02-13T02:03:26Z",
        "summary": "Graph Neural Networks (GNNs) have empowered the advance in graph-structured\ndata analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4\nhas heralded a new era in deep learning. However, their application to graph\ndata poses distinct challenges due to the inherent difficulty of translating\ngraph structures to language. To this end, we introduce the Large Language and\nGraph Assistant (LLaGA), an innovative model that effectively integrates LLM\ncapabilities to handle the complexities of graph-structured data. LLaGA retains\nthe general-purpose nature of LLMs while adapting graph data into a format\ncompatible with LLM input. LLaGA achieves this by reorganizing graph nodes to\nstructure-aware sequences and then mapping these into the token embedding space\nthrough a versatile projector. LLaGA excels in versatility, generalizability\nand interpretability, allowing it to perform consistently well across different\ndatasets and tasks, extend its ability to unseen datasets or tasks, and provide\nexplanations for graphs. Our extensive experiments across popular graph\nbenchmarks show that LLaGA delivers outstanding performance across four\ndatasets and three tasks using one single model, surpassing state-of-the-art\ngraph models in both supervised and zero-shot scenarios. Our code is available\nat \\url{https://github.com/VITA-Group/LLaGA}.",
        "pdf_link": "https://arxiv.org/pdf/2402.08170v2.pdf"
    },
    {
        "title": "On Limitations of the Transformer Architecture",
        "authors": [
            "Binghui Peng",
            "Srini Narayanan",
            "Christos Papadimitriou"
        ],
        "published": "2024-02-13T01:52:15Z",
        "summary": "What are the root causes of hallucinations in large language models (LLMs)?\nWe use Communication Complexity to prove that the Transformer layer is\nincapable of composing functions (e.g., identify a grandparent of a person in a\ngenealogy) if the domains of the functions are large enough; we show through\nexamples that this inability is already empirically present when the domains\nare quite small. We also point out that several mathematical tasks that are at\nthe core of the so-called compositional tasks thought to be hard for LLMs are\nunlikely to be solvable by Transformers, for large enough instances and\nassuming that certain well accepted conjectures in the field of Computational\nComplexity are true.",
        "pdf_link": "https://arxiv.org/pdf/2402.08164v2.pdf"
    },
    {
        "title": "Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search",
        "authors": [
            "David Brandfonbrener",
            "Sibi Raja",
            "Tarun Prasad",
            "Chloe Loughridge",
            "Jianang Yang",
            "Simon Henniger",
            "William E. Byrd",
            "Robert Zinkov",
            "Nada Amin"
        ],
        "published": "2024-02-13T00:55:14Z",
        "summary": "We present an approach using Monte Carlo Tree Search (MCTS) to guide Large\nLanguage Models (LLMs) to generate verified programs in Dafny, Lean and Coq.\nOur method, which we call VMCTS, leverages the verifier inside the search\nalgorithm by checking partial programs at each step. In combination with the\nLLM prior, the verifier feedback raises the synthesis capabilities of open\nsource models. On a set of five verified programming problems, we find that in\nfour problems where the base model cannot solve the question even when\nre-sampling solutions for one hour, VMCTS can solve the problems within 6\nminutes. The base model with VMCTS is even competitive with ChatGPT4 augmented\nwith plugins and multiple re-tries on these problems. Our code and benchmarks\nare available at\nhttps://github.com/namin/llm-verified-with-monte-carlo-tree-search .",
        "pdf_link": "https://arxiv.org/pdf/2402.08147v1.pdf"
    },
    {
        "title": "On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era",
        "authors": [
            "Matteo Tiezzi",
            "Michele Casoni",
            "Alessandro Betti",
            "Tommaso Guidi",
            "Marco Gori",
            "Stefano Melacci"
        ],
        "published": "2024-02-12T23:55:55Z",
        "summary": "A longstanding challenge for the Machine Learning community is the one of\ndeveloping models that are capable of processing and learning from very long\nsequences of data. The outstanding results of Transformers-based networks\n(e.g., Large Language Models) promotes the idea of parallel attention as the\nkey to succeed in such a challenge, obfuscating the role of classic sequential\nprocessing of Recurrent Models. However, in the last few years, researchers who\nwere concerned by the quadratic complexity of self-attention have been\nproposing a novel wave of neural models, which gets the best from the two\nworlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State\nModels emerged as robust approaches to function approximation over time, thus\nopening a new perspective in learning from sequential data, followed by many\npeople in the field and exploited to implement a special class of (linear)\nRecurrent Neural Networks. This survey is aimed at providing an overview of\nthese trends framed under the unifying umbrella of Recurrence. Moreover, it\nemphasizes novel research opportunities that become prominent when abandoning\nthe idea of processing long sequences whose length is known-in-advance for the\nmore realistic setting of potentially infinite-length sequences, thus\nintersecting the field of lifelong-online learning from streamed data.",
        "pdf_link": "https://arxiv.org/pdf/2402.08132v2.pdf"
    },
    {
        "title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
        "authors": [
            "Kaya Stechly",
            "Karthik Valmeekam",
            "Subbarao Kambhampati"
        ],
        "published": "2024-02-12T23:11:01Z",
        "summary": "There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples--ranging from multiplication to simple planning--there persists\na wide spread belief that LLMs can self-critique and improve their own\nsolutions in an iterative fashion. This belief seemingly rests on the\nassumption that verification of correctness should be easier than generation--a\nrather classical argument from computational complexity--which should be\nirrelevant to LLMs to the extent that what they are doing is approximate\nretrieval. In this paper, we set out to systematically investigate the\neffectiveness of iterative prompting in the context of reasoning and planning.\nWe present a principled empirical study of the performance of GPT-4 in three\ndomains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both\nwith the model critiquing its own answers and with an external correct reasoner\nverifying proposed solutions. In each case, we analyze whether the content of\ncriticisms actually affects bottom line performance, and whether we can ablate\nelements of the augmented system without losing performance. We observe\nsignificant performance collapse with self-critique, significant performance\ngains with sound external verification, but that the content of critique\ndoesn't matter to the performance of the system. In fact, merely re-prompting\nwith a sound verifier maintains most of the benefits of more involved setups.",
        "pdf_link": "https://arxiv.org/pdf/2402.08115v1.pdf"
    },
    {
        "title": "Addressing cognitive bias in medical language models",
        "authors": [
            "Samuel Schmidgall",
            "Carl Harris",
            "Ime Essien",
            "Daniel Olshvang",
            "Tawsifur Rahman",
            "Ji Woong Kim",
            "Rojin Ziaei",
            "Jason Eshraghian",
            "Peter Abadir",
            "Rama Chellappa"
        ],
        "published": "2024-02-12T23:08:37Z",
        "summary": "There is increasing interest in the application large language models (LLMs)\nto the medical field, in part because of their impressive performance on\nmedical exam questions. While promising, exam questions do not reflect the\ncomplexity of real patient-doctor interactions. In reality, physicians'\ndecisions are shaped by many complex factors, such as patient compliance,\npersonal experience, ethical beliefs, and cognitive bias. Taking a step toward\nunderstanding this, our hypothesis posits that when LLMs are confronted with\nclinical questions containing cognitive biases, they will yield significantly\nless accurate responses compared to the same questions presented without such\nbiases. In this study, we developed BiasMedQA, a benchmark for evaluating\ncognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated\nsix LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and\nthe medically specialized PMC Llama 13B. We tested these models on 1,273\nquestions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3,\nmodified to replicate common clinically-relevant cognitive biases. Our analysis\nrevealed varying effects for biases on these LLMs, with GPT-4 standing out for\nits resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B,\nwhich were disproportionately affected by cognitive bias. Our findings\nhighlight the critical need for bias mitigation in the development of medical\nLLMs, pointing towards safer and more reliable applications in healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2402.08113v3.pdf"
    },
    {
        "title": "Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts",
        "authors": [
            "Yueqin Yin",
            "Zhendong Wang",
            "Yi Gu",
            "Hai Huang",
            "Weizhu Chen",
            "Mingyuan Zhou"
        ],
        "published": "2024-02-12T22:47:57Z",
        "summary": "In the field of large language models (LLMs), aligning models with the\ndiverse preferences of users is a critical challenge. Direct Preference\nOptimization (DPO) has played a key role in this area. It works by using pairs\nof preferences derived from the same prompts, and it functions without needing\nan additional reward model. However, DPO does not fully reflect the complex\nnature of human learning, which often involves understanding contrasting\nresponses to not only identical but also similar questions. To overcome this\nshortfall, we propose Relative Preference Optimization (RPO). RPO is designed\nto discern between more and less preferred responses derived from both\nidentical and related prompts. It introduces a contrastive weighting mechanism,\nenabling the tuning of LLMs using a broader range of preference data, including\nboth paired and unpaired sets. This approach expands the learning capabilities\nof the model, allowing it to leverage insights from a more varied set of\nprompts. Through empirical tests, including dialogue and summarization tasks,\nand evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a\nsuperior ability to align LLMs with user preferences and to improve their\nadaptability during the training process. The PyTorch code necessary to\nreproduce the results presented in the paper will be made available on GitHub\nfor public access.",
        "pdf_link": "https://arxiv.org/pdf/2402.10958v1.pdf"
    },
    {
        "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation",
        "authors": [
            "Federico Ranaldi",
            "Elena Sofia Ruzzetti",
            "Dario Onorati",
            "Leonardo Ranaldi",
            "Cristina Giannone",
            "Andrea Favalli",
            "Raniero Romagnoli",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2024-02-12T22:35:40Z",
        "summary": "Understanding textual description to generate code seems to be an achieved\ncapability of instruction-following Large Language Models (LLMs) in zero-shot\nscenario. However, there is a severe possibility that this translation ability\nmay be influenced by having seen target textual descriptions and the related\ncode. This effect is known as Data Contamination.\n  In this study, we investigate the impact of Data Contamination on the\nperformance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we\nintroduce a novel method to detect Data Contamination in GPTs and examine\nGPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new\nunfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on\ndatabases with modified information via an adversarial table disconnection\n(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of\ninformation from the database. Our results indicate a significant performance\ndrop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,\nhighlighting the effect of Data Contamination on LLMs in Text-to-SQL\ntranslation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.08100v1.pdf"
    },
    {
        "title": "Grounding Data Science Code Generation with Input-Output Specifications",
        "authors": [
            "Yeming Wen",
            "Pengcheng Yin",
            "Kensen Shi",
            "Henryk Michalewski",
            "Swarat Chaudhuri",
            "Alex Polozov"
        ],
        "published": "2024-02-12T21:32:49Z",
        "summary": "Large language models (LLMs) have recently demonstrated a remarkable ability\nto generate code from natural language (NL) prompts. However, in the real\nworld, NL is often too ambiguous to capture the true intent behind programming\nproblems, requiring additional input-output (I/O) specifications.\nUnfortunately, LLMs can have difficulty aligning their outputs with both the NL\nprompt and the I/O specification. In this paper, we give a way to mitigate this\nissue in the context of data science programming, where tasks require explicit\nI/O specifications for clarity. Specifically, we propose GIFT4Code, a novel\napproach for the instruction fine-tuning of LLMs with respect to I/O\nspecifications. Our method leverages synthetic data produced by the LLM itself\nand utilizes execution-derived feedback as a key learning signal. This\nfeedback, in the form of program I/O specifications, is provided to the LLM to\nfacilitate instruction fine-tuning. We evaluated our approach on two\nchallenging data science benchmarks, Arcade and DS-1000. The results\ndemonstrate a significant improvement in the LLM's ability to generate code\nthat is not only executable but also accurately aligned with user\nspecifications, substantially improving the quality of code generation for\ncomplex data science tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.08073v2.pdf"
    },
    {
        "title": "Beyond LLMs: Advancing the Landscape of Complex Reasoning",
        "authors": [
            "Jennifer Chu-Carroll",
            "Andrew Beck",
            "Greg Burnham",
            "David OS Melville",
            "David Nachman",
            "A. Erdem \u00d6zcan",
            "David Ferrucci"
        ],
        "published": "2024-02-12T21:14:45Z",
        "summary": "Since the advent of Large Language Models a few years ago, they have often\nbeen considered the de facto solution for many AI problems. However, in\naddition to the many deficiencies of LLMs that prevent them from broad industry\nadoption, such as reliability, cost, and speed, there is a whole class of\ncommon real world problems that Large Language Models perform poorly on,\nnamely, constraint satisfaction and optimization problems. These problems are\nubiquitous and current solutions are highly specialized and expensive to\nimplement. At Elemental Cognition, we developed our EC AI platform which takes\na neuro-symbolic approach to solving constraint satisfaction and optimization\nproblems. The platform employs, at its core, a precise and high performance\nlogical reasoning engine, and leverages LLMs for knowledge acquisition and user\ninteraction. This platform supports developers in specifying application logic\nin natural and concise language while generating application user interfaces to\ninteract with users effectively. We evaluated LLMs against systems built on the\nEC AI platform in three domains and found the EC AI systems to significantly\noutperform LLMs on constructing valid and optimal solutions, on validating\nproposed solutions, and on repairing invalid solutions.",
        "pdf_link": "https://arxiv.org/pdf/2402.08064v1.pdf"
    },
    {
        "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
        "authors": [
            "Anjali Khurana",
            "Hari Subramonyam",
            "Parmit K Chilana"
        ],
        "published": "2024-02-12T19:49:58Z",
        "summary": "Large Language Model (LLM) assistants, such as ChatGPT, have emerged as\npotential alternatives to search methods for helping users navigate complex,\nfeature-rich software. LLMs use vast training data from domain-specific texts,\nsoftware manuals, and code repositories to mimic human-like interactions,\noffering tailored assistance, including step-by-step instructions. In this\nwork, we investigated LLM-generated software guidance through a within-subject\nexperiment with 16 participants and follow-up interviews. We compared a\nbaseline LLM assistant with an LLM optimized for particular software contexts,\nSoftAIBot, which also offered guidelines for constructing appropriate prompts.\nWe assessed task completion, perceived accuracy, relevance, and trust.\nSurprisingly, although SoftAIBot outperformed the baseline LLM, our results\nrevealed no significant difference in LLM usage and user perceptions with or\nwithout prompt guidelines and the integration of domain context. Most users\nstruggled to understand how the prompt's text related to the LLM's responses\nand often followed the LLM's suggestions verbatim, even if they were incorrect.\nThis resulted in difficulties when using the LLM's advice for software tasks,\nleading to low task completion rates. Our detailed analysis also revealed that\nusers remained unaware of inaccuracies in the LLM's responses, indicating a gap\nbetween their lack of software expertise and their ability to evaluate the\nLLM's assistance. With the growing push for designing domain-specific LLM\nassistants, we emphasize the importance of incorporating explainable,\ncontext-aware cues into LLMs to help users understand prompt-based\ninteractions, identify biases, and maximize the utility of LLM assistants.",
        "pdf_link": "https://arxiv.org/pdf/2402.08030v1.pdf"
    },
    {
        "title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition",
        "authors": [
            "Ashish Shenoy",
            "Yichao Lu",
            "Srihari Jayakumar",
            "Debojeet Chatterjee",
            "Mohsen Moslehpour",
            "Pierce Chuang",
            "Abhay Harpale",
            "Vikas Bhardwaj",
            "Di Xu",
            "Shicong Zhao",
            "Longfang Zhao",
            "Ankit Ramchandani",
            "Xin Luna Dong",
            "Anuj Kumar"
        ],
        "published": "2024-02-12T19:27:26Z",
        "summary": "We introduce Lumos, the first end-to-end multimodal question-answering system\nwith text understanding capabilities. At the core of Lumos is a Scene Text\nRecognition (STR) component that extracts text from first person point-of-view\nimages, the output of which is used to augment input to a Multimodal Large\nLanguage Model (MM-LLM). While building Lumos, we encountered numerous\nchallenges related to STR quality, overall latency, and model inference. In\nthis paper, we delve into those challenges, and discuss the system\narchitecture, design choices, and modeling techniques employed to overcome\nthese obstacles. We also provide a comprehensive evaluation for each component,\nshowcasing high quality and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.08017v1.pdf"
    },
    {
        "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs",
        "authors": [
            "V\u00edctor Gallego"
        ],
        "published": "2024-02-12T19:10:13Z",
        "summary": "In this paper, we introduce \\emph{refined Direct Preference Optimization}\n(rDPO), a method for improving the behavioral alignment of Large Language\nModels (LLMs) without the need for human-annotated data. The method involves\ncreating synthetic data using self-critique prompting by a teacher LLM and then\nutilising a generalized DPO loss function to distil to a student LLM. The loss\nfunction incorporates an additional external reward model to improve the\nquality of synthetic data, making rDPO robust to potential noise in the\nsynthetic dataset. rDPO is shown to be effective in a diverse set of\nbehavioural alignment tasks, such as improved safety, robustness against\nrole-playing, and reduced sycophancy. Code to be released at\nhttps://github.com/vicgalle/refined-dpo.",
        "pdf_link": "https://arxiv.org/pdf/2402.08005v1.pdf"
    },
    {
        "title": "Suppressing Pink Elephants with Direct Principle Feedback",
        "authors": [
            "Louis Castricato",
            "Nathan Lile",
            "Suraj Anand",
            "Hailey Schoelkopf",
            "Siddharth Verma",
            "Stella Biderman"
        ],
        "published": "2024-02-12T18:57:46Z",
        "summary": "Existing methods for controlling language models, such as RLHF and\nConstitutional AI, involve determining which LLM behaviors are desirable and\ntraining them into a language model. However, in many cases, it is desirable\nfor LLMs to be controllable at inference time, so that they can be used in\nmultiple contexts with diverse needs. We illustrate this with the Pink Elephant\nProblem: instructing an LLM to avoid discussing a certain entity (a ``Pink\nElephant''), and instead discuss a preferred entity (``Grey Elephant''). We\napply a novel simplification of Constitutional AI, Direct Principle Feedback,\nwhich skips the ranking of responses and uses DPO directly on critiques and\nrevisions. Our results show that after DPF fine-tuning on our synthetic Pink\nElephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms\nLlama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on\nour curated test set assessing the Pink Elephant Problem.",
        "pdf_link": "https://arxiv.org/pdf/2402.07896v2.pdf"
    },
    {
        "title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis",
        "authors": [
            "Yangxinyu Xie",
            "Tanwi Mallick",
            "Joshua David Bergerson",
            "John K. Hutchison",
            "Duane R. Verner",
            "Jordan Branham",
            "M. Ross Alexander",
            "Robert B. Ross",
            "Yan Feng",
            "Leslie-Anne Levy",
            "Weijie Su"
        ],
        "published": "2024-02-12T18:41:55Z",
        "summary": "The recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence (AI) and\nmachine learning (ML). However, LLMs are generalized models, trained on\nextensive text corpus, and often struggle to provide context-specific\ninformation, particularly in areas requiring specialized knowledge such as\nwildfire details within the broader context of climate change. For\ndecision-makers and policymakers focused on wildfire resilience and adaptation,\nit is crucial to obtain responses that are not only precise but also\ndomain-specific, rather than generic. To that end, we developed WildfireGPT, a\nprototype LLM agent designed to transform user queries into actionable insights\non wildfire risks. We enrich WildfireGPT by providing additional context such\nas climate projections and scientific literature to ensure its information is\ncurrent, relevant, and scientifically accurate. This enables WildfireGPT to be\nan effective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including researchers, engineers,\nurban planners, emergency managers, and infrastructure operators.",
        "pdf_link": "https://arxiv.org/pdf/2402.07877v1.pdf"
    },
    {
        "title": "Policy Improvement using Language Feedback Models",
        "authors": [
            "Victor Zhong",
            "Dipendra Misra",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9"
        ],
        "published": "2024-02-12T18:41:34Z",
        "summary": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.07876v3.pdf"
    },
    {
        "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published": "2024-02-12T18:28:36Z",
        "summary": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate those limitations. In particular, given a question, RAG retrieves\nrelevant knowledge from a knowledge database to augment the input of the LLM.\nFor instance, the retrieved knowledge could be a set of top-k texts that are\nmost semantically similar to the given question when the knowledge database\ncontains millions of texts collected from Wikipedia. As a result, the LLM could\nutilize the retrieved knowledge as the context to generate an answer for the\ngiven question. Existing studies mainly focus on improving the accuracy or\nefficiency of RAG, leaving its security largely unexplored. We aim to bridge\nthe gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge\npoisoning attacks to RAG, where an attacker could inject a few poisoned texts\ninto the knowledge database such that the LLM generates an attacker-chosen\ntarget answer for an attacker-chosen target question. We formulate knowledge\npoisoning attacks as an optimization problem, whose solution is a set of\npoisoned texts. Depending on the background knowledge (e.g., black-box and\nwhite-box settings) of an attacker on the RAG, we propose two solutions to\nsolve the optimization problem, respectively. Our results on multiple benchmark\ndatasets and LLMs show our attacks could achieve 90% attack success rates when\ninjecting 5 poisoned texts for each target question into a database with\nmillions of texts. We also evaluate recent defenses and our results show they\nare insufficient to defend against our attacks, highlighting the need for new\ndefenses.",
        "pdf_link": "https://arxiv.org/pdf/2402.07867v1.pdf"
    },
    {
        "title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
        "authors": [
            "Philipp Schoenegger",
            "Peter S. Park",
            "Ezra Karger",
            "Philip E. Tetlock"
        ],
        "published": "2024-02-12T18:14:43Z",
        "summary": "Large language models (LLMs) show impressive capabilities, matching and\nsometimes exceeding human performance in many domains. This study explores the\npotential of LLMs to augment judgement in forecasting tasks. We evaluated the\nimpact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to\nprovide high-quality advice ('superforecasting'), and the other designed to be\noverconfident and base-rate-neglecting. Participants (N = 991) had the option\nto consult their assigned LLM assistant throughout the study, in contrast to a\ncontrol group that used a less advanced model (DaVinci-003) without direct\nforecasting support. Our preregistered analyses reveal that LLM augmentation\nsignificantly enhances forecasting accuracy by 23% across both types of\nassistants, compared to the control group. This improvement occurs despite the\nsuperforecasting assistant's higher accuracy in predictions, indicating the\naugmentation's benefit is not solely due to model prediction accuracy.\nExploratory analyses showed a pronounced effect in one forecasting item,\nwithout which we find that the superforecasting assistant increased accuracy by\n43%, compared with 28% for the biased assistant. We further examine whether LLM\naugmentation disproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our findings do not consistently\nsupport these hypotheses. Our results suggest that access to an LLM assistant,\neven a biased one, can be a helpful decision aid in cognitively demanding tasks\nwhere the answer is not known at the time of interaction.",
        "pdf_link": "https://arxiv.org/pdf/2402.07862v1.pdf"
    },
    {
        "title": "Lissard: Long and Simple Sequential Reasoning Datasets",
        "authors": [
            "Mirelle Bueno",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2024-02-12T18:10:17Z",
        "summary": "Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce Lissard, a\nbenchmark comprising seven tasks whose goal is to assess the ability of models\nto process and generate wide-range sequence lengths, requiring repetitive\nprocedural execution. Our evaluation of open-source (Mistral-7B and\nMixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent\ndecline in performance across all models as the complexity of the sequence\nincreases. The datasets and code are available at\nhttps://github.com/unicamp-dl/Lissard",
        "pdf_link": "https://arxiv.org/pdf/2402.07859v2.pdf"
    },
    {
        "title": "Mercury: An Efficiency Benchmark for LLM Code Synthesis",
        "authors": [
            "Mingzhe Du",
            "Anh Tuan Luu",
            "Bin Ji",
            "See-Kiong Ng"
        ],
        "published": "2024-02-12T17:53:22Z",
        "summary": "Despite advancements in evaluating Large Language Models (LLMs) for code\nsynthesis, benchmarks have predominantly focused on functional correctness,\noverlooking the importance of code efficiency. We present Mercury, the first\nbenchmark designated for assessing the code efficiency of LLM code synthesis\ntasks. Mercury consists of 1,889 programming tasks covering diverse difficulty\nlevels alongside test case generators generating unlimited cases for\ncomprehensive evaluation. Unlike existing benchmarks, Mercury integrates a\nnovel metric Beyond@K to measure normalized code efficiency based on historical\nsubmissions, leading to a new evaluation indicator for code synthesis, which\nencourages generating functionally correct and computationally efficient code,\nmirroring the real-world software development standard. Our findings reveal\nthat while LLMs demonstrate the remarkable capability to generate functionally\ncorrect code, there still exists a substantial gap in their efficiency output,\nunderscoring a new frontier for LLM research and development.",
        "pdf_link": "https://arxiv.org/pdf/2402.07844v1.pdf"
    },
    {
        "title": "Do Membership Inference Attacks Work on Large Language Models?",
        "authors": [
            "Michael Duan",
            "Anshuman Suri",
            "Niloofar Mireshghallah",
            "Sewon Min",
            "Weijia Shi",
            "Luke Zettlemoyer",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "David Evans",
            "Hannaneh Hajishirzi"
        ],
        "published": "2024-02-12T17:52:05Z",
        "summary": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.07841v1.pdf"
    },
    {
        "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning",
        "authors": [
            "Z Liu",
            "J Lou",
            "W Bao",
            "Z Qin",
            "K Ren"
        ],
        "published": "2024-02-12T17:24:15Z",
        "summary": "Finetuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs finetuning and its accompanying privacy\nconcerns, differentially private (DP) finetuning of pretrained LLMs has\ngarnered increasing attention to safeguarding the privacy of task-specific\ndatasets. Lying at the design core of DP LLM finetuning methods is the\nsatisfactory tradeoff between privacy, utility, and scalability. Most existing\nmethods build upon the seminal work of DP-SGD. Despite pushing the scalability\nof DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately\nlimited by the inherent inefficiency of SGD. In this paper, we investigate the\npotential of DP zeroth-order methods for LLM pretraining, which avoids the\nscalability bottleneck of SGD by approximating the gradient with the more\nefficient zeroth-order gradient. Rather than treating the zeroth-order method\nas a drop-in replacement for SGD, this paper presents a comprehensive study\nboth theoretically and empirically. First, we propose the stagewise DP\nzeroth-order method that dynamically schedules key hyperparameters. This design\nis grounded on the synergy between DP random perturbation and the gradient\napproximation error of the zeroth-order method, and its effect on finetuning\ntrajectory. Second, we further enhance the scalability by reducing the\ntrainable parameters that are identified by repurposing a data-free pruning\ntechnique requiring no additional data or extra privacy budget. We provide\ntheoretical analysis for both proposed methods. We conduct extensive empirical\nanalysis on both encoder-only masked language model and decoder-only\nautoregressive language model, achieving impressive results in terms of\nscalability and utility.",
        "pdf_link": "https://arxiv.org/pdf/2402.07818v2.pdf"
    },
    {
        "title": "Retrieval-Augmented Thought Process as Sequential Decision Making",
        "authors": [
            "Thomas Pouplin",
            "Hao Sun",
            "Samuel Holt",
            "Mihaela van der Schaar"
        ],
        "published": "2024-02-12T17:17:50Z",
        "summary": "Large Language Models (LLMs) have demonstrated their strong ability to assist\npeople and show \"sparks of intelligence\". However, several open challenges\nhinder their wider application: such as concerns over privacy, tendencies to\nproduce hallucinations, and difficulties in handling long contexts. In this\nwork, we address those challenges by introducing the Retrieval-Augmented\nThought Process (RATP). Given access to external knowledge, RATP formulates the\nthought generation of LLMs as a multiple-step decision process. To optimize\nsuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns a\nQ-value estimator that permits cost-efficient inference. In addressing the task\nof question-answering with private data, where ethical and security concerns\nlimit LLM training methods, RATP achieves a 50% improvement over existing\nin-context retrieval-augmented language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.07812v1.pdf"
    },
    {
        "title": "TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection",
        "authors": [
            "Hui Liu",
            "Wenya Wang",
            "Haoru Li",
            "Haoliang Li"
        ],
        "published": "2024-02-12T16:41:54Z",
        "summary": "The proliferation of fake news has emerged as a severe societal problem,\nraising significant interest from industry and academia. While existing\ndeep-learning based methods have made progress in detecting fake news\naccurately, their reliability may be compromised caused by the non-transparent\nreasoning processes, poor generalization abilities and inherent risks of\nintegration with large language models (LLMs). To address this challenge, we\npropose {\\methodname}, a novel framework for trustworthy fake news detection\nthat prioritizes explainability, generalizability and controllability of\nmodels. This is achieved via a dual-system framework that integrates cognition\nand decision systems, adhering to the principles above. The cognition system\nharnesses human expertise to generate logical predicates, which guide LLMs in\ngenerating human-readable logic atoms. Meanwhile, the decision system deduces\ngeneralizable logic rules to aggregate these atoms, enabling the identification\nof the truthfulness of the input news across diverse domains and enhancing\ntransparency in the decision-making process. Finally, we present comprehensive\nevaluation results on four datasets, demonstrating the feasibility and\ntrustworthiness of our proposed framework. Our implementation is available at\n\\url{https://github.com/less-and-less-bugs/Trust_TELLER}.",
        "pdf_link": "https://arxiv.org/pdf/2402.07776v1.pdf"
    },
    {
        "title": "Quantitative knowledge retrieval from large language models",
        "authors": [
            "David Selby",
            "Kai Spriestersbach",
            "Yuichiro Iwashita",
            "Dennis Bappert",
            "Archana Warrier",
            "Sumantrak Mukherjee",
            "Muhammad Nabeel Asim",
            "Koichi Kise",
            "Sebastian Vollmer"
        ],
        "published": "2024-02-12T16:32:37Z",
        "summary": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. In this\npaper we explore the feasibility of LLMs as a mechanism for quantitative\nknowledge retrieval to aid data analysis tasks such as elicitation of prior\ndistributions for Bayesian models and imputation of missing data. We present a\nprompt engineering framework, treating an LLM as an interface to a latent space\nof scientific literature, comparing responses in different contexts and domains\nagainst more established approaches. Implications and challenges of using LLMs\nas 'experts' are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.07770v1.pdf"
    },
    {
        "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
        "authors": [
            "Qian Yang",
            "Jin Xu",
            "Wenrui Liu",
            "Yunfei Chu",
            "Ziyue Jiang",
            "Xiaohuan Zhou",
            "Yichong Leng",
            "Yuanjun Lv",
            "Zhou Zhao",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2024-02-12T15:41:22Z",
        "summary": "Recently, instruction-following audio-language models have received broad\nattention for human-audio interaction. However, the absence of benchmarks\ncapable of evaluating audio-centric interaction capabilities has impeded\nadvancements in this field. Previous models primarily focus on assessing\ndifferent fundamental tasks, such as Automatic Speech Recognition (ASR), and\nlack an assessment of the open-ended generative capabilities centered around\naudio. Thus, it is challenging to track the progression in the Large\nAudio-Language Models (LALMs) domain and to provide guidance for future\nimprovement. In this paper, we introduce AIR-Bench (\\textbf{A}udio\n\\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed\nto evaluate the ability of LALMs to understand various types of audio signals\n(including human speech, natural sounds, and music), and furthermore, to\ninteract with humans in the textual format. AIR-Bench encompasses two\ndimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former\nconsists of 19 tasks with approximately 19k single-choice questions, intending\nto inspect the basic single-task ability of LALMs. The latter one contains 2k\ninstances of open-ended question-and-answer data, directly assessing the\ncomprehension of the model on complex audio and its capacity to follow\ninstructions. Both benchmarks require the model to generate hypotheses\ndirectly. We design a unified framework that leverages advanced language\nmodels, such as GPT-4, to evaluate the scores of generated hypotheses given the\nmeta-information of the audio. Experimental results demonstrate a high level of\nconsistency between GPT-4-based evaluation and human evaluation. By revealing\nthe limitations of existing LALMs through evaluation results, AIR-Bench can\nprovide insights into the direction of future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.07729v1.pdf"
    },
    {
        "title": "CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity",
        "authors": [
            "Norbert Tihanyi",
            "Mohamed Amine Ferrag",
            "Ridhi Jain",
            "Merouane Debbah"
        ],
        "published": "2024-02-12T14:53:28Z",
        "summary": "Large Language Models (LLMs) excel across various domains, from computer\nvision to medical diagnostics. However, understanding the diverse landscape of\ncybersecurity, encompassing cryptography, reverse engineering, and managerial\nfacets like risk assessment, presents a challenge, even for human experts. In\nthis paper, we introduce CyberMetric, a benchmark dataset comprising 10,000\nquestions sourced from standards, certifications, research papers, books, and\nother publications in the cybersecurity domain. The questions are created\nthrough a collaborative process, i.e., merging expert knowledge with LLMs,\nincluding GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying\ntheir accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's\nmain goal is to facilitate a fair comparison between humans and different LLMs\nin cybersecurity. To achieve this, we carefully selected 80 questions covering\na wide range of topics within cybersecurity and involved 30 participants of\ndiverse expertise levels, facilitating a comprehensive comparison between human\nand machine intelligence in this area. The findings revealed that LLMs\noutperformed humans in almost every aspect of cybersecurity.",
        "pdf_link": "https://arxiv.org/pdf/2402.07688v1.pdf"
    },
    {
        "title": "Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?",
        "authors": [
            "Vicent Briva-Iglesias",
            "Joao Lucas Cavalheiro Camargo",
            "Gokhan Dogru"
        ],
        "published": "2024-02-12T14:40:54Z",
        "summary": "This study evaluates the machine translation (MT) quality of two\nstate-of-the-art large language models (LLMs) against a tradition-al neural\nmachine translation (NMT) system across four language pairs in the legal\ndomain. It combines automatic evaluation met-rics (AEMs) and human evaluation\n(HE) by professional transla-tors to assess translation ranking, fluency and\nadequacy. The re-sults indicate that while Google Translate generally\noutperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4,\ncomparably or slightly better in terms of producing contextually adequate and\nfluent translations. This discrepancy suggests LLMs' potential in handling\nspecialized legal terminology and context, highlighting the importance of human\nevaluation methods in assessing MT quality. The study underscores the evolving\ncapabil-ities of LLMs in specialized domains and calls for reevaluation of\ntraditional AEMs to better capture the nuances of LLM-generated translations.",
        "pdf_link": "https://arxiv.org/pdf/2402.07681v1.pdf"
    },
    {
        "title": "The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models",
        "authors": [
            "Ayo Adedeji",
            "Sarita Joshi",
            "Brendan Doohan"
        ],
        "published": "2024-02-12T14:01:12Z",
        "summary": "In the rapidly evolving landscape of medical documentation, transcribing\nclinical dialogues accurately is increasingly paramount. This study explores\nthe potential of Large Language Models (LLMs) to enhance the accuracy of\nAutomatic Speech Recognition (ASR) systems in medical transcription. Utilizing\nthe PriMock57 dataset, which encompasses a diverse range of primary care\nconsultations, we apply advanced LLMs to refine ASR-generated transcripts. Our\nresearch is multifaceted, focusing on improvements in general Word Error Rate\n(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential\nmedical terms, and speaker diarization accuracy. Additionally, we assess the\nrole of LLM post-processing in improving semantic textual similarity, thereby\npreserving the contextual integrity of clinical dialogues. Through a series of\nexperiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)\nprompting techniques in enhancing diarization and correction accuracy. Our\nfindings demonstrate that LLMs, particularly through CoT prompting, not only\nimprove the diarization accuracy of existing ASR systems but also achieve\nstate-of-the-art performance in this domain. This improvement extends to more\naccurately capturing medical concepts and enhancing the overall semantic\ncoherence of the transcribed dialogues. These findings illustrate the dual role\nof LLMs in augmenting ASR outputs and independently excelling in transcription\ntasks, holding significant promise for transforming medical ASR systems and\nleading to more accurate and reliable patient records in healthcare settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.07658v1.pdf"
    },
    {
        "title": "Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models",
        "authors": [
            "Isabelle Lorge",
            "Dan W. Joyce",
            "Niall Taylor",
            "Alejo Nevado-Holgado",
            "Andrea Cipriani",
            "Andrey Kormilitzin"
        ],
        "published": "2024-02-12T13:34:33Z",
        "summary": "Difficult-to-treat depression (DTD) has been proposed as a broader and more\nclinically comprehensive perspective on a person's depressive disorder where\ndespite treatment, they continue to experience significant burden. We sought to\ndevelop a Large Language Model (LLM)-based tool capable of interrogating\nroutinely-collected, narrative (free-text) electronic health record (EHR) data\nto locate published prognostic factors that capture the clinical syndrome of\nDTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a\nNon-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction\nmodel. The resulting model is then able to extract and label spans related to a\nvariety of relevant positive and negative factors in real clinical data (i.e.\nspans of text that increase or decrease the likelihood of a patient matching\nthe DTD syndrome). We show it is possible to obtain good overall performance\n(0.70 F1 across polarity) on real clinical data on a set of as many as 20\ndifferent factors, and high performance (0.85 F1 with 0.95 precision) on a\nsubset of important DTD factors such as history of abuse, family history of\naffective disorder, illness severity and suicidality by training the model\nexclusively on synthetic data. Our results show promise for future healthcare\napplications especially in applications where traditionally, highly\nconfidential medical data and human-expert annotation would normally be\nrequired.",
        "pdf_link": "https://arxiv.org/pdf/2402.07645v1.pdf"
    },
    {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "authors": [
            "Xiaoxin He",
            "Yijun Tian",
            "Yifei Sun",
            "Nitesh V. Chawla",
            "Thomas Laurent",
            "Yann LeCun",
            "Xavier Bresson",
            "Bryan Hooi"
        ],
        "published": "2024-02-12T13:13:04Z",
        "summary": "Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop\nour Graph Question Answering (GraphQA) benchmark with data collected from\ndifferent tasks. Then, we propose our G-Retriever approach, which integrates\nthe strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can\nbe fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and resists\nhallucination. (Our codes and datasets are available at:\nhttps://github.com/XiaoxinHe/G-Retriever.)",
        "pdf_link": "https://arxiv.org/pdf/2402.07630v2.pdf"
    },
    {
        "title": "Anchor-based Large Language Models",
        "authors": [
            "Jianhui Pang",
            "Fanghua Ye",
            "Derek F. Wong",
            "Longyue Wang"
        ],
        "published": "2024-02-12T12:48:02Z",
        "summary": "Large language models (LLMs) predominantly employ decoder-only transformer\narchitectures, necessitating the retention of keys/values information for\nhistorical tokens to provide contextual information and avoid redundant\ncomputation. However, the substantial size and parameter volume of these LLMs\nrequire massive GPU memory. This memory demand increases with the length of the\ninput text, leading to an urgent need for more efficient methods of information\nstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), which\nutilize an innovative anchor-based self-attention network (AnSAN) and also an\nanchor-based inference strategy. This approach enables LLMs to compress\nsequence information into an anchor token, reducing the keys/values cache and\nenhancing inference efficiency. Experiments on question-answering benchmarks\nreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%\nkeys/values cache reduction and up to 3.5 times faster inference. Despite a\nminor compromise in accuracy, the substantial enhancements of AnLLMs employing\nthe AnSAN technique in resource utilization and computational efficiency\nunderscore their potential for practical LLM applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.07616v2.pdf"
    },
    {
        "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
        "authors": [
            "Haoyu Wang",
            "Guozheng Ma",
            "Ziqiao Meng",
            "Zeyu Qin",
            "Li Shen",
            "Zhong Zhang",
            "Bingzhe Wu",
            "Liu Liu",
            "Yatao Bian",
            "Tingyang Xu",
            "Xueqian Wang",
            "Peilin Zhao"
        ],
        "published": "2024-02-12T12:30:42Z",
        "summary": "Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2402.07610v2.pdf"
    },
    {
        "title": "BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection",
        "authors": [
            "Kang Zhang",
            "Osamu Yoshie",
            "Weiran Huang"
        ],
        "published": "2024-02-12T10:04:07Z",
        "summary": "Trading range breakout (TRB) is a key method in the technical analysis of\nfinancial trading, widely employed by traders in financial markets such as\nstocks, futures, and foreign exchange. However, distinguishing between true and\nfalse breakout and providing the correct rationale cause significant challenges\nto investors. Recently, large language models have achieved success in various\ndownstream applications, but their effectiveness in the domain of financial\nbreakout detection has been subpar. The reason is that the unique data and\nspecific knowledge are required in breakout detection. To address these issues,\nwe introduce BreakGPT, the first large language model for financial breakout\ndetection. Furthermore, we have developed a novel framework for large language\nmodels, namely multi-stage structure, effectively reducing mistakes in\ndownstream applications. Experimental results indicate that compared to\nGPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with\nthe multi-stage structure contributing 17.6% to the improvement. Additionally,\nit outperforms ChatGPT-4 by 42.07%. Our Code is publicly available:\nhttps://github.com/Neviim96/BreakGPT",
        "pdf_link": "https://arxiv.org/pdf/2402.07536v1.pdf"
    },
    {
        "title": "Secret Collusion Among Generative AI Agents",
        "authors": [
            "Sumeet Ramesh Motwani",
            "Mikhail Baranchuk",
            "Martin Strohmeier",
            "Vijay Bolina",
            "Philip H. S. Torr",
            "Lewis Hammond",
            "Christian Schroeder de Witt"
        ],
        "published": "2024-02-12T09:31:21Z",
        "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which teams of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both the AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
        "pdf_link": "https://arxiv.org/pdf/2402.07510v1.pdf"
    },
    {
        "title": "T-RAG: Lessons from the LLM Trenches",
        "authors": [
            "Masoomali Fatehkia",
            "Ji Kim Lucas",
            "Sanjay Chawla"
        ],
        "published": "2024-02-12T08:45:08Z",
        "summary": "Large Language Models (LLM) have shown remarkable language capabilities\nfueling attempts to integrate them into applications across a wide range of\ndomains. An important application area is question answering over private\nenterprise documents where the main considerations are data security, which\nnecessitates applications that can be deployed on-prem, limited computational\nresources and the need for a robust application that correctly responds to\nqueries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent\nframework for building LLM-based applications. While building a RAG is\nrelatively straightforward, making it robust and a reliable application\nrequires extensive customization and relatively deep knowledge of the\napplication domain. We share our experiences building and deploying an LLM\napplication for question answering over private organizational documents. Our\napplication combines the use of RAG with a finetuned open-source LLM.\nAdditionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure\nto represent entity hierarchies within the organization. This is used to\ngenerate a textual description to augment the context when responding to user\nqueries pertaining to entities within the organization's hierarchy. Our\nevaluations show that this combination performs better than a simple RAG or\nfinetuning implementation. Finally, we share some lessons learned based on our\nexperiences building an LLM application for real-world use.",
        "pdf_link": "https://arxiv.org/pdf/2402.07483v1.pdf"
    },
    {
        "title": "Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm",
        "authors": [
            "Ali Rostami",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2024-02-12T08:32:29Z",
        "summary": "State-of-the-art rule-based and classification-based food recommendation\nsystems face significant challenges in becoming practical and useful. This\ndifficulty arises primarily because most machine learning models struggle with\nproblems characterized by an almost infinite number of classes and a limited\nnumber of samples within an unbalanced dataset. Conversely, the emergence of\nLarge Language Models (LLMs) as recommendation engines offers a promising\navenue. However, a general-purpose Recommendation as Language Processing (RLP)\napproach lacks the critical components necessary for effective food\nrecommendations. To address this gap, we introduce Food Recommendation as\nLanguage Processing (F-RLP), a novel framework that offers a food-specific,\ntailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize\ntheir potential, thereby paving the way for more accurate, personalized food\nrecommendations.",
        "pdf_link": "https://arxiv.org/pdf/2402.07477v2.pdf"
    },
    {
        "title": "Pushing The Limit of LLM Capacity for Text Classification",
        "authors": [
            "Yazhou Zhang",
            "Mengyao Wang",
            "Chenyu Ren",
            "Qiuchi Li",
            "Prayag Tiwari",
            "Benyou Wang",
            "Jing Qin"
        ],
        "published": "2024-02-12T08:14:03Z",
        "summary": "The value of text classification's future research has encountered challenges\nand uncertainties, due to the extraordinary efficacy demonstrated by large\nlanguage models (LLMs) across numerous downstream NLP tasks. In this era of\nopen-ended language modeling, where task boundaries are gradually fading, an\nurgent question emerges: have we made significant advances in text\nclassification under the full benefit of LLMs? To answer this question, we\npropose RGPT, an adaptive boosting framework tailored to produce a specialized\ntext classification LLM by recurrently ensembling a pool of strong base\nlearners. The base learners are constructed by adaptively adjusting the\ndistribution of training samples and iteratively fine-tuning LLMs with them.\nSuch base learners are then ensembled to be a specialized text classification\nLLM, by recurrently incorporating the historical predictions from the previous\nlearners. Through a comprehensive empirical comparison, we show that RGPT\nsignificantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by\n1.36% on average. Further evaluation experiments show a clear surpassing of\nRGPT over human classification.",
        "pdf_link": "https://arxiv.org/pdf/2402.07470v2.pdf"
    },
    {
        "title": "Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch",
        "authors": [
            "Ray Ito",
            "Junichiro Takahashi"
        ],
        "published": "2024-02-12T06:49:48Z",
        "summary": "Several attempts have been made to implement text command control for game\nagents. However, current technologies are limited to processing predefined\nformat commands. This paper proposes a pioneering text command control system\nfor a game agent that can understand natural language commands expressed in\nfree-form. The proposed system uses a large language model (LLM) for code\ngeneration to interpret and transform natural language commands into behavior\nbranch, a proposed knowledge expression based on behavior trees, which\nfacilitates execution by the game agent. This study conducted empirical\nvalidation within a game environment that simulates a Pok\\'emon game and\ninvolved multiple participants. The results confirmed the system's ability to\nunderstand and carry out natural language commands, representing a noteworthy\nin the realm of real-time language interactive game agents.\n  Notice for the use of this material. The copyright of this material is\nretained by the Japanese Society for Artificial Intelligence (JSAI). This\nmaterial is published here with the agreement of JSAI. Please be complied with\nCopyright Law of Japan if any users wish to reproduce, make derivative work,\ndistribute or make available to the public any part or whole thereof. All\nRights Reserved, Copyright (C) The Japanese Society for Artificial\nIntelligence.",
        "pdf_link": "https://arxiv.org/pdf/2402.07442v1.pdf"
    },
    {
        "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT",
        "authors": [
            "Jon Saad-Falcon",
            "Daniel Y. Fu",
            "Simran Arora",
            "Neel Guha",
            "Christopher R\u00e9"
        ],
        "published": "2024-02-12T06:43:52Z",
        "summary": "Retrieval pipelines-an integral component of many machine learning\nsystems-perform poorly in domains where documents are long (e.g., 10K tokens or\nmore) and where identifying the relevant document requires synthesizing\ninformation across the entire text. Developing long-context retrieval encoders\nsuitable for these domains raises three challenges: (1) how to evaluate\nlong-context retrieval performance, (2) how to pretrain a base language model\nto represent both short contexts (corresponding to queries) and long contexts\n(corresponding to documents), and (3) how to fine-tune this model for retrieval\nunder the batch size limitations imposed by GPU memory constraints. To address\nthese challenges, we first introduce LoCoV1, a novel 12 task benchmark\nconstructed to measure long-context retrieval where chunking is not possible or\nnot effective. We next present the M2-BERT retrieval encoder, an 80M parameter\nstate-space encoder model built from the Monarch Mixer architecture, capable of\nscaling to documents up to 32K tokens long. We describe a pretraining data\nmixture which allows this encoder to process both short and long context\nsequences, and a finetuning approach that adapts this base model to retrieval\nwith only single-sample batches. Finally, we validate the M2-BERT retrieval\nencoder on LoCoV1, finding that it outperforms competitive Transformer-based\nmodels by at least 23.3 points, despite containing upwards of 90x fewer\nparameters.",
        "pdf_link": "https://arxiv.org/pdf/2402.07440v2.pdf"
    },
    {
        "title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples",
        "authors": [
            "Mingrui Ma",
            "Lansheng Han",
            "Chunjie Zhou"
        ],
        "published": "2024-02-12T04:59:58Z",
        "summary": "The frequent occurrence of cyber-attacks has made webshell attacks and\ndefense gradually become a research hotspot in the field of network security.\nHowever, the lack of publicly available benchmark datasets and the\nover-reliance on manually defined rules for webshell escape sample generation\nhave slowed down the progress of research related to webshell escape sample\ngeneration strategies and artificial intelligence-based webshell detection\nalgorithms. To address the drawbacks of weak webshell sample escape\ncapabilities, the lack of webshell datasets with complex malicious features,\nand to promote the development of webshell detection technology, we propose the\nHybrid Prompt algorithm for webshell escape sample generation with the help of\nlarge language models. As a prompt algorithm specifically developed for\nwebshell sample generation, the Hybrid Prompt algorithm not only combines\nvarious prompt ideas including Chain of Thought, Tree of Thought, but also\nincorporates various components such as webshell hierarchical module and\nfew-shot example to facilitate the LLM in learning and reasoning webshell\nescape strategies. Experimental results show that the Hybrid Prompt algorithm\ncan work with multiple LLMs with excellent code reasoning ability to generate\nhigh-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on\nVIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).",
        "pdf_link": "https://arxiv.org/pdf/2402.07408v1.pdf"
    },
    {
        "title": "D\u00f3lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English",
        "authors": [
            "Xiao Zhang",
            "Ruoyu Xiang",
            "Chenhan Yuan",
            "Duanyu Feng",
            "Weiguang Han",
            "Alejandro Lopez-Lira",
            "Xiao-Yang Liu",
            "Sophia Ananiadou",
            "Min Peng",
            "Jimin Huang",
            "Qianqian Xie"
        ],
        "published": "2024-02-12T04:50:31Z",
        "summary": "Despite Spanish's pivotal role in the global finance industry, a pronounced\ngap exists in Spanish financial natural language processing (NLP) and\napplication studies compared to English, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we unveil Tois\\'on de Oro, the\nfirst bilingual framework that establishes instruction datasets, finetuned\nLLMs, and evaluation benchmark for financial LLMs in Spanish joint with\nEnglish. We construct a rigorously curated bilingual instruction dataset\nincluding over 144K Spanish and English samples from 15 datasets covering 7\ntasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual\nfinancial applications. We evaluate our model and existing LLMs using FLARE-ES,\nthe first comprehensive bilingual evaluation benchmark with 21 datasets\ncovering 9 tasks. The FLARE-ES benchmark results reveal a significant\nmultilingual performance gap and bias in existing LLMs. FinMA-ES models surpass\nSOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic\ninstruction tuning and leveraging data from diverse linguistic resources,\nhighlighting the positive impact of cross-linguistic transfer. All our\ndatasets, models, and benchmarks have been released.",
        "pdf_link": "https://arxiv.org/pdf/2402.07405v1.pdf"
    },
    {
        "title": "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate",
        "authors": [
            "Kyungha Kim",
            "Sangyun Lee",
            "Kung-Hsiang Huang",
            "Hou Pong Chan",
            "Manling Li",
            "Heng Ji"
        ],
        "published": "2024-02-12T04:32:33Z",
        "summary": "Fact-checking research has extensively explored verification but less so the\ngeneration of natural-language explanations, crucial for user trust. While\nLarge Language Models (LLMs) excel in text generation, their capability for\nproducing faithful explanations in fact-checking remains underexamined. Our\nstudy investigates LLMs' ability to generate such explanations, finding that\nzero-shot prompts often result in unfaithfulness. To address these challenges,\nwe propose the Multi-Agent Debate Refinement (MADR) framework, leveraging\nmultiple LLMs as agents with diverse roles in an iterative refining process\naimed at enhancing faithfulness in generated explanations. MADR ensures that\nthe final explanation undergoes rigorous validation, significantly reducing the\nlikelihood of unfaithful elements and aligning closely with the provided\nevidence. Experimental results demonstrate that MADR significantly improves the\nfaithfulness of LLM-generated explanations to the evidence, advancing the\ncredibility and trustworthiness of these explanations.",
        "pdf_link": "https://arxiv.org/pdf/2402.07401v1.pdf"
    },
    {
        "title": "Exploring Perceptual Limitation of Multimodal Large Language Models",
        "authors": [
            "Jiarui Zhang",
            "Jinyi Hu",
            "Mahyar Khayatkhoei",
            "Filip Ilievski",
            "Maosong Sun"
        ],
        "published": "2024-02-12T03:04:42Z",
        "summary": "Multimodal Large Language Models (MLLMs) have recently shown remarkable\nperceptual capability in answering visual questions, however, little is known\nabout the limits of their perception. In particular, while prior works have\nprovided anecdotal evidence of MLLMs' sensitivity to object size, this\nphenomenon and its underlying causes have not been explored comprehensively. In\nthis work, we quantitatively study the perception of small visual objects in\nseveral state-of-the-art MLLMs and reveal a pervasive limitation in answering\nquestions about small objects in images. Next, we identify four independent\nfactors that can contribute to this limitation -- object quality, size,\ndistractors, and location -- and conduct controlled intervention studies to\nmeasure the effect of each factor on MLLMs' perception. In particular, we find\nthat lower object quality and smaller object size can both independently reduce\nMLLMs' ability to answer visual questions. More surprisingly, we find that the\nlocation of the object in the image and the presence of visual distractors can\nalso significantly reduce MLLMs' question answering accuracy. Our study\nprovides a better understanding of the perceptual limitation of MLLMs and\ncontributes new evaluation protocols for analyzing the perception of future\nMLLMs. To facilitate further investigations, we release our code and data.",
        "pdf_link": "https://arxiv.org/pdf/2402.07384v1.pdf"
    },
    {
        "title": "Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning",
        "authors": [
            "Gabriel Simmons",
            "Vladislav Savinov"
        ],
        "published": "2024-02-12T01:55:51Z",
        "summary": "This study evaluates the ability of Large Language Model (LLM)-based\nSubpopulation Representative Models (SRMs) to generalize from empirical data,\nutilizing in-context learning with data from the 2016 and 2020 American\nNational Election Studies. We explore generalization across response variables\nand demographic subgroups. While conditioning with empirical data improves\nperformance on the whole, the benefit of in-context learning varies\nconsiderably across demographics, sometimes hurting performance for one\ndemographic while helping performance for others. The inequitable benefits of\nin-context learning for SRM present a challenge for practitioners implementing\nSRMs, and for decision-makers who might come to rely on them. Our work\nhighlights a need for fine-grained benchmarks captured from diverse\nsubpopulations that test not only fidelity but generalization.",
        "pdf_link": "https://arxiv.org/pdf/2402.07368v1.pdf"
    },
    {
        "title": "Differentially Private Training of Mixture of Experts Models",
        "authors": [
            "Pierre Tholoniat",
            "Huseyin A. Inan",
            "Janardhan Kulkarni",
            "Robert Sim"
        ],
        "published": "2024-02-11T23:57:09Z",
        "summary": "This position paper investigates the integration of Differential Privacy (DP)\nin the training of Mixture of Experts (MoE) models within the field of natural\nlanguage processing. As Large Language Models (LLMs) scale to billions of\nparameters, leveraging expansive datasets, they exhibit enhanced linguistic\ncapabilities and emergent abilities. However, this growth raises significant\ncomputational and privacy concerns. Our study addresses these issues by\nexploring the potential of MoE models, known for their computational\nefficiency, and the application of DP, a standard for privacy preservation. We\npresent the first known attempt to train MoE models under the constraints of\nDP, addressing the unique challenges posed by their architecture and the\ncomplexities of DP integration. Our initial experimental studies demonstrate\nthat MoE models can be effectively trained with DP, achieving performance that\nis competitive with their non-private counterparts. This initial study aims to\nprovide valuable insights and ignite further research in the domain of\nprivacy-preserving MoE models, softly laying the groundwork for prospective\ndevelopments in this evolving field.",
        "pdf_link": "https://arxiv.org/pdf/2402.07334v1.pdf"
    },
    {
        "title": "Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs",
        "authors": [
            "Bilal Chughtai",
            "Alan Cooney",
            "Neel Nanda"
        ],
        "published": "2024-02-11T22:58:49Z",
        "summary": "How do transformer-based large language models (LLMs) store and retrieve\nknowledge? We focus on the most basic form of this task -- factual recall,\nwhere the model is tasked with explicitly surfacing stored facts in prompts of\nform `Fact: The Colosseum is in the country of'. We find that the mechanistic\nstory behind factual recall is more complex than previously thought. It\ncomprises several distinct, independent, and qualitatively different mechanisms\nthat additively combine, constructively interfering on the correct attribute.\nWe term this generic phenomena the additive motif: models compute through\nsumming up multiple independent contributions. Each mechanism's contribution\nmay be insufficient alone, but summing results in constructive interfere on the\ncorrect answer. In addition, we extend the method of direct logit attribution\nto attribute an attention head's output to individual source tokens. We use\nthis technique to unpack what we call `mixed heads' -- which are themselves a\npair of two separate additive updates from different source tokens.",
        "pdf_link": "https://arxiv.org/pdf/2402.07321v1.pdf"
    },
    {
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
        "authors": [
            "Lichang Chen",
            "Chen Zhu",
            "Davit Soselia",
            "Jiuhai Chen",
            "Tianyi Zhou",
            "Tom Goldstein",
            "Heng Huang",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2024-02-11T22:40:12Z",
        "summary": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
        "pdf_link": "https://arxiv.org/pdf/2402.07319v1.pdf"
    },
    {
        "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference",
        "authors": [
            "Chenlu Ye",
            "Wei Xiong",
            "Yuheng Zhang",
            "Nan Jiang",
            "Tong Zhang"
        ],
        "published": "2024-02-11T21:44:21Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) learns from the preference\nsignal provided by a probabilistic preference model, which takes a prompt and\ntwo responses as input, and produces a score indicating the preference of one\nresponse against another. So far, the most popular RLHF paradigm is\nreward-based, which starts with an initial step of reward modeling, and the\nconstructed reward is then used to provide a reward signal for the subsequent\nreward optimization stage. However, the existence of a reward function is a\nstrong assumption and the reward-based RLHF is limited in expressivity and\ncannot capture the real-world complicated human preference.\n  In this work, we provide theoretical insights for a recently proposed\nlearning paradigm, Nash learning from human feedback (NLHF), which considered a\ngeneral preference model and formulated the alignment process as a game between\ntwo competitive LLMs. The learning objective is to find a policy that\nconsistently generates responses preferred over any competing policy while\nstaying close to the initial model. The objective is defined as the Nash\nequilibrium (NE) of the KL-regularized preference model. We aim to make the\nfirst attempt to study the theoretical learnability of the KL-regularized NLHF\nby considering both offline and online settings. For the offline learning from\na pre-collected dataset, we propose algorithms that are efficient under\nsuitable coverage conditions of the dataset. For batch online learning from\niterative interactions with a preference oracle, our proposed algorithm enjoys\na finite sample guarantee under the structural condition of the underlying\npreference model. Our results connect the new NLHF paradigm with traditional RL\ntheory, and validate the potential of reward-model-free learning under general\npreference.",
        "pdf_link": "https://arxiv.org/pdf/2402.07314v1.pdf"
    },
    {
        "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
        "authors": [
            "Ryan Liu",
            "Theodore R. Sumers",
            "Ishita Dasgupta",
            "Thomas L. Griffiths"
        ],
        "published": "2024-02-11T19:13:26Z",
        "summary": "In day-to-day communication, people often approximate the truth - for\nexample, rounding the time or omitting details - in order to be maximally\nhelpful to the listener. How do large language models (LLMs) handle such\nnuanced trade-offs? To address this question, we use psychological models and\nexperiments designed to characterize human behavior to analyze LLMs. We test a\nrange of LLMs and explore how optimization for human preferences or\ninference-time reasoning affects these trade-offs. We find that reinforcement\nlearning from human feedback improves both honesty and helpfulness, while\nchain-of-thought prompting skews LLMs towards helpfulness over honesty.\nFinally, GPT-4 Turbo demonstrates human-like response patterns including\nsensitivity to the conversational framing and listener's decision context. Our\nfindings reveal the conversational values internalized by LLMs and suggest that\neven these abstract values can, to a degree, be steered by zero-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2402.07282v2.pdf"
    },
    {
        "title": "TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation",
        "authors": [
            "Peng Wang",
            "Xiang Wei",
            "Fangxu Hu",
            "Wenjuan Han"
        ],
        "published": "2024-02-11T15:50:35Z",
        "summary": "Natural language processing (NLP) is a key component of intelligent\ntransportation systems (ITS), but it faces many challenges in the\ntransportation domain, such as domain-specific knowledge and data, and\nmulti-modal inputs and outputs. This paper presents TransGPT, a novel\n(multi-modal) large language model for the transportation domain, which\nconsists of two independent variants: TransGPT-SM for single-modal data and\nTransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal\nTransportation dataset (STD) that contains textual data from various sources in\nthe transportation domain. TransGPT-MM is finetuned on a multi-modal\nTransportation dataset (MTD) that we manually collected from three areas of the\ntransportation domain: driving tests, traffic signs, and landmarks. We evaluate\nTransGPT on several benchmark datasets for different tasks in the\ntransportation domain, and show that it outperforms baseline models on most\ntasks. We also showcase the potential applications of TransGPT for traffic\nanalysis and modeling, such as generating synthetic traffic scenarios,\nexplaining traffic phenomena, answering traffic-related questions, providing\ntraffic recommendations, and generating traffic reports. This work advances the\nstate-of-the-art of NLP in the transportation domain and provides a useful tool\nfor ITS researchers and practitioners.",
        "pdf_link": "https://arxiv.org/pdf/2402.07233v1.pdf"
    },
    {
        "title": "Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models",
        "authors": [
            "Gonzalo Mart\u00ednez",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Javier Conde",
            "Pedro Reviriego",
            "Elena Merino"
        ],
        "published": "2024-02-11T13:41:17Z",
        "summary": "The performance of conversational Large Language Models (LLMs) in general,\nand of ChatGPT in particular, is currently being evaluated on many different\ntasks, from logical reasoning or maths to answering questions on a myriad of\ntopics. Instead, much less attention is being devoted to the study of the\nlinguistic features of the texts generated by these LLMs. This is surprising\nsince LLMs are models for language, and understanding how they use the language\nis important. Indeed, conversational LLMs are poised to have a significant\nimpact on the evolution of languages as they may eventually dominate the\ncreation of new text. This means that for example, if conversational LLMs do\nnot use a word it may become less and less frequent and eventually stop being\nused altogether. Therefore, evaluating the linguistic features of the text they\nproduce and how those depend on the model parameters is the first step toward\nunderstanding the potential impact of conversational LLMs on the evolution of\nlanguages. In this paper, we consider the evaluation of the lexical richness of\nthe text generated by LLMs and how it depends on the model parameters. A\nmethodology is presented and used to conduct a comprehensive evaluation of\nlexical richness using ChatGPT as a case study. The results show how lexical\nrichness depends on the version of ChatGPT and some of its parameters, such as\nthe presence penalty, or on the role assigned to the model. The dataset and\ntools used in our analysis are released under open licenses with the goal of\ndrawing the much-needed attention to the evaluation of the linguistic features\nof LLM-generated text.",
        "pdf_link": "https://arxiv.org/pdf/2402.15518v1.pdf"
    },
    {
        "title": "Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning",
        "authors": [
            "Yihong Tang",
            "Zhaokai Wang",
            "Ao Qu",
            "Yihao Yan",
            "Kebing Hou",
            "Dingyi Zhuang",
            "Xiaotong Guo",
            "Jinhua Zhao",
            "Zhan Zhao",
            "Wei Ma"
        ],
        "published": "2024-02-11T13:30:53Z",
        "summary": "In this paper, we for the first time propose the task of Open-domain Urban\nItinerary Planning (OUIP) for citywalk, which directly generates itineraries\nbased on users' requests described in natural language. OUIP is different from\nconventional itinerary planning, which limits users from expressing more\ndetailed needs and hinders true personalization. Recently, large language\nmodels (LLMs) have shown potential in handling diverse tasks. However, due to\nnon-real-time information, incomplete knowledge, and insufficient spatial\nawareness, they are unable to independently deliver a satisfactory user\nexperience in OUIP. Given this, we present ItiNera, an OUIP system that\nsynergizes spatial optimization with Large Language Models (LLMs) to provide\nservices that customize urban itineraries based on users' needs. Specifically,\nwe develop an LLM-based pipeline for extracting and updating POI features to\ncreate a user-owned personalized POI database. For each user request, we\nleverage LLM in cooperation with an embedding-based module for retrieving\ncandidate POIs from the user's POI database. Then, a spatial optimization\nmodule is used to order these POIs, followed by LLM crafting a personalized,\nspatially coherent itinerary. To the best of our knowledge, this study marks\nthe first integration of LLMs to innovate itinerary planning solutions.\nExtensive experiments on offline datasets and online subjective evaluation have\ndemonstrated the capacities of our system to deliver more responsive and\nspatially coherent itineraries than current LLM-based solutions. Our system has\nbeen deployed in production at the TuTu online travel service and has attracted\nthousands of users for their urban travel planning.",
        "pdf_link": "https://arxiv.org/pdf/2402.07204v1.pdf"
    },
    {
        "title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
        "authors": [
            "Mengmei Zhang",
            "Mingwei Sun",
            "Peng Wang",
            "Shen Fan",
            "Yanhu Mo",
            "Xiaoxiao Xu",
            "Hong Liu",
            "Cheng Yang",
            "Chuan Shi"
        ],
        "published": "2024-02-11T13:24:13Z",
        "summary": "Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and\ninstruction-following capabilities, have catalyzed a revolutionary\ntransformation across diverse fields, especially for open-ended tasks. While\nthe idea is less explored in the graph domain, despite the availability of\nnumerous powerful graph models (GMs), they are restricted to tasks in a\npre-defined form. Although several methods applying LLMs to graphs have been\nproposed, they fail to simultaneously handle the pre-defined and open-ended\ntasks, with LLM as a node feature enhancer or as a standalone predictor. To\nbreak this dilemma, we propose to bridge the pretrained GM and LLM by a\nTranslator, named GraphTranslator, aiming to leverage GM to handle the\npre-defined tasks effectively and utilize the extended interface of LLMs to\noffer various open-ended tasks for GM. To train such Translator, we propose a\nProducer capable of constructing the graph-text alignment data along node\ninformation, neighbor information and model information. By translating node\nrepresentation into tokens, GraphTranslator empowers an LLM to make predictions\nbased on language instructions, providing a unified perspective for both\npre-defined and open-ended tasks. Extensive results demonstrate the\neffectiveness of our proposed GraphTranslator on zero-shot node classification.\nThe graph question answering experiments reveal our GraphTranslator potential\nacross a broad spectrum of open-ended tasks through language instructions. Our\ncode is available at: https://github.com/alibaba/GraphTranslator.",
        "pdf_link": "https://arxiv.org/pdf/2402.07197v4.pdf"
    },
    {
        "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models",
        "authors": [
            "Zhibo Hu",
            "Chen Wang",
            "Yanfeng Shu",
            "Helen",
            "Paik",
            "Liming Zhu"
        ],
        "published": "2024-02-11T12:25:41Z",
        "summary": "The robustness of large language models (LLMs) becomes increasingly important\nas their use rapidly grows in a wide range of domains. Retrieval-Augmented\nGeneration (RAG) is considered as a means to improve the trustworthiness of\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\naffected by slightly different inputs is not well studied. In this work, we\nfind that the insertion of even a short prefix to the prompt leads to the\ngeneration of outputs far away from factually correct answers. We\nsystematically evaluate the effect of such prefixes on RAG by introducing a\nnovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\ntargeted wrong answers. It can also cope with instructions in the prompts\nrequesting to ignore irrelevant context. We also exploit LLMs' neuron\nactivation difference between prompts with and without GGPP perturbations to\ngive a method that improves the robustness of RAG-based LLMs through a highly\neffective detector trained on neuron activation triggered by GGPP generated\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\nour methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.07179v1.pdf"
    },
    {
        "title": "Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy",
        "authors": [
            "Zehao Dong",
            "Yixin Chen",
            "Hiram Gay",
            "Yao Hao",
            "Geoffrey D. Hugo",
            "Pamela Samson",
            "Tianyu Zhao"
        ],
        "published": "2024-02-11T11:24:09Z",
        "summary": "Treatment planning is currently a patient specific, time-consuming, and\nresource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction\nplays a critical role in automating this process. The geometric relationship\nbetween DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target\nvolume (PTV) has been well established. This study explores the potential of\ndeep learning models for predicting DVHs using images and subsequent human\nintervention facilitated by a large-language model (LLM) to enhance the\nplanning quality. We propose a pipeline to convert unstructured images to a\nstructured graph consisting of image-patch nodes and dose nodes. A novel Dose\nGraph Neural Network (DoseGNN) model is developed for predicting DVHs from the\nstructured graph. The proposed DoseGNN is enhanced with the LLM to encode\nmassive knowledge from prescriptions and interactive instructions from\nclinicians. In this study, we introduced an online human-AI collaboration\n(OHAC) system as a practical implementation of the concept proposed for the\nautomation of intensity-modulated radiotherapy (IMRT) planning. In comparison\nto the widely-employed DL models used in radiotherapy, DoseGNN achieved mean\nsquare errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin\nU-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the\nLLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans\nthrough interaction with clinicians using natural language.",
        "pdf_link": "https://arxiv.org/pdf/2402.07167v1.pdf"
    },
    {
        "title": "Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias",
        "authors": [
            "Arifa Khan",
            "P. Saravanan",
            "S. K Venkatesan"
        ],
        "published": "2024-02-11T11:23:28Z",
        "summary": "We provide a birds eye view of the rapid developments in AI and Deep Learning\nthat has led to the path-breaking emergence of AI in Large Language Models. The\naim of this study is to place all these developments in a pragmatic broader\nhistorical social perspective without any exaggerations while at the same time\nwithout any pessimism that created the AI winter in the 1970s to 1990s. We also\nat the same time point out toxicity, bias, memorization, sycophancy, logical\ninconsistencies, hallucinations that exist just as a warning to the overly\noptimistic. We note here that just as this emergence of AI seems to occur at a\nthreshold point in the number of neural connections or weights, it has also\nbeen observed that human brain and especially the cortex region is nothing\nspecial or extraordinary but simply a case of scaled-up version of the primate\nbrain and that even the human intelligence seems like an emergent phenomena of\nscale.",
        "pdf_link": "https://arxiv.org/pdf/2402.07166v1.pdf"
    },
    {
        "title": "Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces",
        "authors": [
            "Claudionor N. Coelho Jr",
            "Hanchen Xiong",
            "Tushar Karayil",
            "Sree Koratala",
            "Rex Shang",
            "Jacob Bollinger",
            "Mohamed Shabar",
            "Syam Nair"
        ],
        "published": "2024-02-11T11:03:08Z",
        "summary": "The advancement of Large Language Models (LLM) has also resulted in an\nequivalent proliferation in its applications. Software design, being one, has\ngained tremendous benefits in using LLMs as an interface component that extends\nfixed user stories. However, inclusion of LLM-based AI agents in software\ndesign often poses unexpected challenges, especially in the estimation of\ndevelopment efforts. Through the example of UI-based user stories, we provide a\ncomparison against traditional methods and propose a new way to enhance\nspecifications of natural language-based questions that allows for the\nestimation of development effort by taking into account data sources,\ninterfaces and algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2402.07158v1.pdf"
    },
    {
        "title": "Natural Language Reinforcement Learning",
        "authors": [
            "Xidong Feng",
            "Ziyu Wan",
            "Mengyue Yang",
            "Ziyan Wang",
            "Girish A. Koushik",
            "Yali Du",
            "Ying Wen",
            "Jun Wang"
        ],
        "published": "2024-02-11T11:03:04Z",
        "summary": "Reinforcement Learning (RL) has shown remarkable abilities in learning\npolicies for decision-making tasks. However, RL is often hindered by issues\nsuch as low sample efficiency, lack of interpretability, and sparse supervision\nsignals. To tackle these limitations, we take inspiration from the human\nlearning process and introduce Natural Language Reinforcement Learning (NLRL),\nwhich innovatively combines RL principles with natural language representation.\nSpecifically, NLRL redefines RL concepts like task objectives, policy, value\nfunction, Bellman equation, and policy iteration in natural language space. We\npresent how NLRL can be practically implemented with the latest advancements in\nlarge language models (LLMs) like GPT-4. Initial experiments over tabular MDPs\ndemonstrate the effectiveness, efficiency, and also interpretability of the\nNLRL framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.07157v2.pdf"
    },
    {
        "title": "Graph Descriptive Order Improves Reasoning with Large Language Model",
        "authors": [
            "Yuyao Ge",
            "Shenghua Liu",
            "Wenjie Feng",
            "Lingrui Mei",
            "Lizhe Chen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-11T09:46:24Z",
        "summary": "In recent years, large language models have achieved state-of-the-art\nperformance across multiple domains. However, the progress in the field of\ngraph reasoning with LLM remains limited. Our work delves into this gap by\nthoroughly investigating graph reasoning with LLMs. In this work, we reveal the\nimpact of the order of graph description on LLMs' graph reasoning performance,\nwhich significantly affects LLMs' reasoning abilities. By altering this order,\nwe enhance the performance of LLMs from 42.22\\% to 70\\%. Furthermore, we\nintroduce the Scaled Graph Reasoning benchmark for assessing LLMs' performance\nacross various graph sizes and evaluate the relationship between LLMs' graph\nreasoning abilities and graph size. We discover that the graph reasoning\nperformance of LLMs does not monotonically decrease with the increase in graph\nsize. The experiments span several mainstream models, including GPT-3.5,\nLLaMA-2-7B, and LLaMA-2-13B, to offer a comprehensive evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.07140v3.pdf"
    },
    {
        "title": "Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education",
        "authors": [
            "Nischal Ashok Kumar",
            "Andrew Lan"
        ],
        "published": "2024-02-11T01:37:48Z",
        "summary": "In computer science education, test cases are an integral part of programming\nassignments since they can be used as assessment items to test students'\nprogramming knowledge and provide personalized feedback on student-written\ncode. The goal of our work is to propose a fully automated approach for test\ncase generation that can accurately measure student knowledge, which is\nimportant for two reasons. First, manually constructing test cases requires\nexpert knowledge and is a labor-intensive process. Second, developing test\ncases for students, especially those who are novice programmers, is\nsignificantly different from those oriented toward professional-level software\ndevelopers. Therefore, we need an automated process for test case generation to\nassess student knowledge and provide feedback. In this work, we propose a large\nlanguage model-based approach to automatically generate test cases and show\nthat they are good measures of student knowledge, using a publicly available\ndataset that contains student-written Java code. We also discuss future\nresearch directions centered on using test cases to help students.",
        "pdf_link": "https://arxiv.org/pdf/2402.07081v1.pdf"
    },
    {
        "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
        "authors": [
            "Arpita Vats",
            "Vinija Jain",
            "Rahul Raja",
            "Aman Chadha"
        ],
        "published": "2024-02-11T00:24:17Z",
        "summary": "The paper underscores the significance of Large Language Models (LLMs) in\nreshaping recommender systems, attributing their value to unique reasoning\nabilities absent in traditional recommenders. Unlike conventional systems\nlacking direct user interaction data, LLMs exhibit exceptional proficiency in\nrecommending items, showcasing their adeptness in comprehending intricacies of\nlanguage. This marks a fundamental paradigm shift in the realm of\nrecommendations. Amidst the dynamic research landscape, researchers actively\nharness the language comprehension and generation capabilities of LLMs to\nredefine the foundations of recommendation tasks. The investigation thoroughly\nexplores the inherent strengths of LLMs within recommendation frameworks,\nencompassing nuanced contextual comprehension, seamless transitions across\ndiverse domains, adoption of unified approaches, holistic learning strategies\nleveraging shared data reservoirs, transparent decision-making, and iterative\nimprovements. Despite their transformative potential, challenges persist,\nincluding sensitivity to input prompts, occasional misinterpretations, and\nunforeseen recommendations, necessitating continuous refinement and evolution\nin LLM-driven recommender systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.18590v3.pdf"
    },
    {
        "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
        "authors": [
            "Elvis Dohmatob",
            "Yunzhen Feng",
            "Pu Yang",
            "Francois Charton",
            "Julia Kempe"
        ],
        "published": "2024-02-10T21:06:34Z",
        "summary": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.",
        "pdf_link": "https://arxiv.org/pdf/2402.07043v1.pdf"
    },
    {
        "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
        "authors": [
            "Keisuke Kamahori",
            "Yile Gu",
            "Kan Zhu",
            "Baris Kasikci"
        ],
        "published": "2024-02-10T19:54:08Z",
        "summary": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture\nare showing promising performance on various tasks. However, running them on\nresource-constrained settings, where GPU memory resources are not abundant, is\nchallenging due to huge model sizes. Existing systems that offload model\nweights to CPU memory suffer from the significant overhead of frequently moving\ndata between CPU and GPU. In this paper, we propose Fiddler, a\nresource-efficient inference engine with CPU-GPU orchestration for MoE models.\nThe key idea of Fiddler is to use the computation ability of the CPU to\nminimize the data movement between the CPU and GPU. Our evaluation shows that\nFiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in\nparameters, to generate over $3$ tokens per second on a single GPU with 24GB\nmemory, showing an order of magnitude improvement over existing methods. The\ncode of Fiddler is publicly available at\n\\url{https://github.com/efeslab/fiddler}",
        "pdf_link": "https://arxiv.org/pdf/2402.07033v1.pdf"
    },
    {
        "title": "Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations",
        "authors": [
            "Ankit Pal",
            "Malaikannan Sankarasubbu"
        ],
        "published": "2024-02-10T19:08:28Z",
        "summary": "Large language models have the potential to be valuable in the healthcare\nindustry, but it's crucial to verify their safety and effectiveness through\nrigorous evaluation. For this purpose, we comprehensively evaluated both\nopen-source LLMs and Google's new multimodal LLM called Gemini across Medical\nreasoning, hallucination detection, and Medical Visual Question Answering\ntasks. While Gemini showed competence, it lagged behind state-of-the-art models\nlike MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved\nan accuracy of 61.45\\% on the medical VQA dataset, significantly lower than\nGPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible\nto hallucinations, overconfidence, and knowledge gaps, which indicate risks if\ndeployed uncritically. We also performed a detailed analysis by medical subject\nand test type, providing actionable feedback for developers and clinicians. To\nmitigate risks, we applied prompting strategies that improved performance.\nAdditionally, we facilitated future research and development by releasing a\nPython module for medical LLM evaluation and establishing a dedicated\nleaderboard on Hugging Face for medical domain LLMs. Python module can be found\nat https://github.com/promptslab/RosettaEval",
        "pdf_link": "https://arxiv.org/pdf/2402.07023v1.pdf"
    },
    {
        "title": "REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models",
        "authors": [
            "Yinghao Zhu",
            "Changyu Ren",
            "Shiyun Xie",
            "Shukai Liu",
            "Hangyuan Ji",
            "Zixiang Wang",
            "Tao Sun",
            "Long He",
            "Zhoujun Li",
            "Xi Zhu",
            "Chengwei Pan"
        ],
        "published": "2024-02-10T18:27:28Z",
        "summary": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly improved clinical predictive capabilities. Leveraging clinical\nnotes and multivariate time-series EHR, existing models often lack the medical\ncontext relevent to clinical tasks, prompting the incorporation of external\nknowledge, particularly from the knowledge graph (KG). Previous approaches with\nKG knowledge have primarily focused on structured knowledge extraction,\nneglecting unstructured data modalities and semantic high dimensional medical\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\n(RAG) driven framework to enhance multimodal EHR representations that address\nthese limitations. Firstly, we apply Large Language Model (LLM) to encode long\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\nwe prompt LLM to extract task-relevant medical entities and match entities in\nprofessionally labeled external knowledge graph (PrimeKG) with corresponding\nmedical knowledge. By matching and aligning with clinical standards, our\nframework eliminates hallucinations and ensures consistency. Lastly, we propose\nan adaptive multimodal fusion network to integrate extracted knowledge with\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality and\nreadmission tasks showcase the superior performance of our REALM framework over\nbaselines, emphasizing the effectiveness of each module. REALM framework\ncontributes to refining the use of multimodal EHR data in healthcare and\nbridging the gap with nuanced medical context essential for informed clinical\npredictions.",
        "pdf_link": "https://arxiv.org/pdf/2402.07016v1.pdf"
    },
    {
        "title": "DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting",
        "authors": [
            "Chris von Csefalvay"
        ],
        "published": "2024-02-10T16:48:45Z",
        "summary": "Over the recent years, the emergence of large language models (LLMs) has\ngiven rise to a proliferation of domain-specific models that are intended to\nreflect the particularities of linguistic context and content as a correlate of\nthe originating domain. This paper details the conception, design, training and\nevaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes\n(mortality, ER attendance and hospitalisation) in adverse event reports\nelicited through passive reporting (PR). While PR is a highly cost-efficient\nway of eliciting information from a wide and diverse audience -- typically\nincluding not only physicians and healthcare providers but also patients,\nfamily members and other lay stakeholders --, this diversity makes PR corpora\ndifficult to analyse. Generic language models may not capture the complex\nclinical dimensions while specific clinical or biomedical models may not\nperform well on lay reports. To evaluate the utility of a subdomain-specific\nlanguage model, an adaptive training approach was adapted, wherein base\nlanguage model candidates were evaluated on a subset of the corpus, and the\nbest performer was trained on the entire corpus. This yielded a small but\nsignificant improvement in $F_1$ (+1%), precision (+2.5%) and recall (+3.8%),\nat a relatively low training cost and a single-day training time.\nSubdomain-specific LLMs continue to be viable options for better results when\nanalysing highly specialised corpora.",
        "pdf_link": "https://arxiv.org/pdf/2402.10951v1.pdf"
    },
    {
        "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
        "authors": [
            "Chufan Shi",
            "Haoran Yang",
            "Deng Cai",
            "Zhisong Zhang",
            "Yifan Wang",
            "Yujiu Yang",
            "Wai Lam"
        ],
        "published": "2024-02-10T11:14:53Z",
        "summary": "Decoding methods play an indispensable role in converting language models\nfrom next-token predictors into practical task solvers. Prior research on\ndecoding methods, primarily focusing on task-specific models, may not extend to\nthe current era of general-purpose large language models (LLMs). Moreover, the\nrecent influx of decoding strategies has further complicated this landscape.\nThis paper provides a comprehensive and multifaceted analysis of various\ndecoding methods within the context of LLMs, evaluating their performance,\nrobustness to hyperparameter changes, and decoding speeds across a wide range\nof tasks, models, and deployment environments. Our findings reveal that\ndecoding method performance is notably task-dependent and influenced by factors\nsuch as alignment, model size, and quantization. Intriguingly, sensitivity\nanalysis exposes that certain methods achieve superior performance at the cost\nof extensive hyperparameter tuning, highlighting the trade-off between\nattaining optimal results and the practicality of implementation in varying\ncontexts.",
        "pdf_link": "https://arxiv.org/pdf/2402.06925v1.pdf"
    },
    {
        "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
        "authors": [
            "Jonathan Evertz",
            "Merlin Chlosta",
            "Lea Sch\u00f6nherr",
            "Thorsten Eisenhofer"
        ],
        "published": "2024-02-10T11:07:24Z",
        "summary": "Large Language Models (LLMs) are increasingly integrated with external tools.\nWhile these integrations can significantly improve the functionality of LLMs,\nthey also create a new attack surface where confidential data may be disclosed\nbetween different components. Specifically, malicious tools can exploit\nvulnerabilities in the LLM itself to manipulate the model and compromise the\ndata of other services, raising the question of how private data can be\nprotected in the context of LLM integrations.\n  In this work, we provide a systematic way of evaluating confidentiality in\nLLM-integrated systems. For this, we formalize a \"secret key\" game that can\ncapture the ability of a model to conceal private information. This enables us\nto compare the vulnerability of a model against confidentiality attacks and\nalso the effectiveness of different defense strategies. In this framework, we\nevaluate eight previously published attacks and four defenses. We find that\ncurrent defenses lack generalization across attack strategies. Building on this\nanalysis, we propose a method for robustness fine-tuning, inspired by\nadversarial training. This approach is effective in lowering the success rate\nof attackers and in improving the system's resilience against unknown attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.06922v1.pdf"
    },
    {
        "title": "Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought",
        "authors": [
            "Zhen-Yu Zhang",
            "Siwei Han",
            "Huaxiu Yao",
            "Gang Niu",
            "Masashi Sugiyama"
        ],
        "published": "2024-02-10T09:51:03Z",
        "summary": "To improve the ability of the large language model (LLMs) to handle complex\nreasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs\nto reason step-by-step, facilitating problem solving from simple to complex\ntasks. State-of-the-art approaches for generating such a chain involve\ninteractive collaboration, where the learner generates candidate intermediate\nthoughts, evaluated by the LLM, guiding the generation of subsequent thoughts.\nHowever, a widespread yet understudied problem is that the evaluation from the\nLLM is typically noisy and unreliable, potentially misleading the generation\nprocess in selecting promising intermediate thoughts. In this paper, motivated\nby Vapnik's principle, we propose a novel comparison-based CoT generation\nalgorithm that directly identifies the most promising thoughts with the noisy\nfeedback from the LLM. In each round, we randomly pair intermediate thoughts\nand directly prompt the LLM to select the more promising one from each pair,\nallowing us to identify the most promising thoughts through an iterative\nprocess. To further model the noise in the comparison, we resort to the\ntechniques of ensemble and dueling bandits and propose two variants of the\nproposed algorithm. Experiments on three real-world mathematical and reasoning\ntasks demonstrate the effectiveness of our proposed algorithm and verify the\nrationale of the direct pairwise comparison.",
        "pdf_link": "https://arxiv.org/pdf/2402.06918v1.pdf"
    },
    {
        "title": "Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric",
        "authors": [
            "Hyukhun Koh",
            "Dohyung Kim",
            "Minwoo Lee",
            "Kyomin Jung"
        ],
        "published": "2024-02-10T07:55:27Z",
        "summary": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to discern the existence of toxicity in\nthe generated text. The majority of existing toxicity metrics rely on encoder\nmodels trained on specific toxicity datasets. However, these encoders are\nsusceptible to out-of-distribution (OOD) problems and depend on the definition\nof toxicity assumed in a dataset. In this paper, we introduce an automatic\nrobust metric grounded on LLMs to distinguish whether model responses are\ntoxic. We start by analyzing the toxicity factors, followed by examining the\nintrinsic toxic attributes of LLMs to ascertain their suitability as\nevaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators\n(LATTE), on evaluation datasets.The empirical results indicate outstanding\nperformance in measuring toxicity, improving upon state-of-the-art metrics by\n12 points in F1 score without training procedure. We also show that upstream\ntoxicity has an influence on downstream metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.06900v2.pdf"
    },
    {
        "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
        "authors": [
            "Yansong Ning",
            "Hao Liu"
        ],
        "published": "2024-02-10T01:50:19Z",
        "summary": "Urban knowledge graph has recently worked as an emerging building block to\ndistill critical knowledge from multi-sourced urban data for diverse urban\napplication scenarios. Despite its promising benefits, urban knowledge graph\nconstruction (UrbanKGC) still heavily relies on manual effort, hindering its\npotential advancement. This paper presents UrbanKGent, a unified large language\nmodel agent framework, for urban knowledge graph construction. Specifically, we\nfirst construct the knowledgeable instruction set for UrbanKGC tasks (such as\nrelational triplet extraction and knowledge graph completion) via\nheterogeneity-aware and geospatial-infused instruction generation. Moreover, we\npropose a tool-augmented iterative trajectory refinement module to enhance and\nrefine the trajectories distilled from GPT-4. Through hybrid instruction\nfine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC\nagent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world\ndatasets using both human and GPT-4 self-evaluation. The experimental results\ndemonstrate that UrbanKGent-13B not only can significantly outperform 21\nbaselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4,\nby more than 10\\% with approximately 20 times lower cost. We deploy\nUrbanKGent-13B to provide online services, which can construct an UrbanKG with\nthousands of times richer relationships using only one-fifth of the data\ncompared with the existing benchmark. Our data, code, and opensource UrbanKGC\nagent are available at https://github.com/usail-hkust/UrbanKGent.",
        "pdf_link": "https://arxiv.org/pdf/2402.06861v1.pdf"
    },
    {
        "title": "History, Development, and Principles of Large Language Models-An Introductory Survey",
        "authors": [
            "Zhibo Chu",
            "Shiwen Ni",
            "Zichong Wang",
            "Xi Feng",
            "Chengming Li",
            "Xiping Hu",
            "Ruifeng Xu",
            "Min Yang",
            "Wenbin Zhang"
        ],
        "published": "2024-02-10T01:18:15Z",
        "summary": "Language models serve as a cornerstone in natural language processing (NLP),\nutilizing mathematical methods to generalize language laws and knowledge for\nprediction and generation. Over extensive research spanning decades, language\nmodeling has progressed from initial statistical language models (SLMs) to the\ncontemporary landscape of large language models (LLMs). Notably, the swift\nevolution of LLMs has reached the ability to process, understand, and generate\nhuman-level text. Nevertheless, despite the significant advantages that LLMs\noffer in improving both work and personal lives, the limited understanding\namong general practitioners about the background and principles of these models\nhampers their full potential. Notably, most LLMs reviews focus on specific\naspects and utilize specialized language, posing a challenge for practitioners\nlacking relevant background knowledge. In light of this, this survey aims to\npresent a comprehensible overview of LLMs to assist a broader audience. It\nstrives to facilitate a comprehensive understanding by exploring the historical\nbackground of language models and tracing their evolution over time. The survey\nfurther investigates the factors influencing the development of LLMs,\nemphasizing key contributions. Additionally, it concentrates on elucidating the\nunderlying principles of LLMs, equipping audiences with essential theoretical\nknowledge. The survey also highlights the limitations of existing work and\npoints out promising future directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.06853v1.pdf"
    },
    {
        "title": "ChemLLM: A Chemical Large Language Model",
        "authors": [
            "Di Zhang",
            "Wei Liu",
            "Qian Tan",
            "Jingdan Chen",
            "Hang Yan",
            "Yuliang Yan",
            "Jiatong Li",
            "Weiran Huang",
            "Xiangyu Yue",
            "Dongzhan Zhou",
            "Shufei Zhang",
            "Mao Su",
            "Hansen Zhong",
            "Yuqiang Li",
            "Wanli Ouyang"
        ],
        "published": "2024-02-10T01:11:59Z",
        "summary": "Large language models (LLMs) have made impressive progress in chemistry\napplications, including molecular property prediction, molecular generation,\nexperimental protocol design, etc. However, the community lacks a\ndialogue-based model specifically designed for chemistry. The challenge arises\nfrom the fact that most chemical data and scientific knowledge are primarily\nstored in structured databases, and the direct use of these structured data\ncompromises the model's ability to maintain coherent dialogue. To tackle this\nissue, we develop a novel template-based instruction construction method that\ntransforms structured knowledge into plain dialogue, making it suitable for\nlanguage model training. By leveraging this approach, we develop ChemLLM, the\nfirst large language model dedicated to chemistry, capable of performing\nvarious tasks across chemical disciplines with smooth dialogue interaction.\nChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name\nconversion, molecular caption, and reaction prediction, and surpasses GPT-4 on\ntwo of them. Remarkably, ChemLLM also shows exceptional adaptability to related\nmathematical and physical tasks despite being trained mainly on\nchemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in\nspecialized NLP tasks within chemistry, such as literature translation and\ncheminformatic programming. ChemLLM opens up a new avenue for exploration\nwithin chemical studies, while our method of integrating structured chemical\nknowledge into dialogue systems sets a new frontier for developing LLMs across\nvarious scientific fields. Codes, Datasets, and Model weights are publicly\naccessible at hf.co/AI4Chem/ChemLLM-7B-Chat.",
        "pdf_link": "https://arxiv.org/pdf/2402.06852v1.pdf"
    },
    {
        "title": "Forecasting Events in Soccer Matches Through Language",
        "authors": [
            "Tiago Mendes-Neves",
            "Lu\u00eds Meireles",
            "Jo\u00e3o Mendes-Moreira"
        ],
        "published": "2024-02-09T23:02:57Z",
        "summary": "This paper introduces an approach to predicting the next event in a soccer\nmatch, a challenge bearing remarkable similarities to the problem faced by\nLarge Language Models (LLMs). Unlike other methods that severely limit event\ndynamics in soccer, often abstracting from many variables or relying on a mix\nof sequential models, our research proposes a novel technique inspired by the\nmethodologies used in LLMs. These models predict a complete chain of variables\nthat compose an event, significantly simplifying the construction of Large\nEvent Models (LEMs) for soccer. Utilizing deep learning on the publicly\navailable WyScout dataset, the proposed approach notably surpasses the\nperformance of previous LEM proposals in critical areas, such as the prediction\naccuracy of the next event type. This paper highlights the utility of LEMs in\nvarious applications, including betting and match analytics. Moreover, we show\nthat LEMs provide a simulation backbone on which many analytics pipelines can\nbe built, an approach opposite to the current specialized single-purpose\nmodels. LEMs represent a pivotal advancement in soccer analytics, establishing\na foundational framework for multifaceted analytics pipelines through a\nsingular machine-learning model.",
        "pdf_link": "https://arxiv.org/pdf/2402.06820v1.pdf"
    },
    {
        "title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts",
        "authors": [
            "Rick Battle",
            "Teja Gollapudi"
        ],
        "published": "2024-02-09T22:48:45Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating \"positive thinking\" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n\"positive thinking\" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best \"positive thinking\" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.",
        "pdf_link": "https://arxiv.org/pdf/2402.10949v2.pdf"
    },
    {
        "title": "Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models",
        "authors": [
            "Tiago Mendes-Neves",
            "Lu\u00eds Meireles",
            "Jo\u00e3o Mendes-Moreira"
        ],
        "published": "2024-02-09T22:47:25Z",
        "summary": "This paper introduces an innovative application of Large Event Models (LEMs),\nakin to Large Language Models, to the domain of soccer analytics. By learning\nthe \"language\" of soccer - predicting variables for subsequent events rather\nthan words LEMs facilitate the simulation of matches and offer various\napplications, including player performance prediction across different team\ncontexts. We focus on fine-tuning LEMs with the WyScout dataset for the\n2017-2018 Premier League season to derive specific insights into player\ncontributions and team strategies. Our methodology involves adapting these\nmodels to reflect the nuanced dynamics of soccer, enabling the evaluation of\nhypothetical transfers. Our findings confirm the effectiveness and limitations\nof LEMs in soccer analytics, highlighting the model's capability to forecast\nteams' expected standings and explore high-profile scenarios, such as the\npotential effects of transferring Cristiano Ronaldo or Lionel Messi to\ndifferent teams in the Premier League. This analysis underscores the importance\nof context in evaluating player quality. While general metrics may suggest\nsignificant differences between players, contextual analyses reveal narrower\ngaps in performance within specific team frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2402.06815v1.pdf"
    },
    {
        "title": "Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
        "authors": [
            "Hochul Hwang",
            "Sunjae Kwon",
            "Yekyung Kim",
            "Donghyun Kim"
        ],
        "published": "2024-02-09T21:37:13Z",
        "summary": "Safely navigating street intersections is a complex challenge for blind and\nlow-vision individuals, as it requires a nuanced understanding of the\nsurrounding context - a task heavily reliant on visual cues. Traditional\nmethods for assisting in this decision-making process often fall short, lacking\nthe ability to provide a comprehensive scene analysis and safety level. This\npaper introduces an innovative approach that leverages large multimodal models\n(LMMs) to interpret complex street crossing scenes, offering a potential\nadvancement over conventional traffic signal recognition techniques. By\ngenerating a safety score and scene description in natural language, our method\nsupports safe decision-making for the blind and low-vision individuals. We\ncollected crosswalk intersection data that contains multiview egocentric images\ncaptured by a quadruped robot and annotated the images with corresponding\nsafety scores based on our predefined safety score categorization. Grounded on\nthe visual knowledge, extracted from images, and text prompt, we evaluate a\nlarge multimodal model for safety score prediction and scene description. Our\nfindings highlight the reasoning and safety score prediction capabilities of a\nLMM, activated by various prompts, as a pathway to developing a trustworthy\nsystem, crucial for applications requiring reliable decision-making support.",
        "pdf_link": "https://arxiv.org/pdf/2402.06794v1.pdf"
    },
    {
        "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
        "authors": [
            "Akbir Khan",
            "John Hughes",
            "Dan Valentine",
            "Laura Ruis",
            "Kshitij Sachan",
            "Ansh Radhakrishnan",
            "Edward Grefenstette",
            "Samuel R. Bowman",
            "Tim Rockt\u00e4schel",
            "Ethan Perez"
        ],
        "published": "2024-02-09T21:05:01Z",
        "summary": "Common methods for aligning large language models (LLMs) with desired\nbehaviour heavily rely on human-labelled data. However, as models grow\nincreasingly sophisticated, they will surpass human expertise, and the role of\nhuman evaluation will evolve into non-experts overseeing experts. In\nanticipation of this, we ask: can weaker models assess the correctness of\nstronger models? We investigate this question in an analogous setting, where\nstronger models (experts) possess the necessary information to answer questions\nand weaker models (non-experts) lack this information. The method we evaluate\nis \\textit{debate}, where two LLM experts each argue for a different answer,\nand a non-expert selects the answer. We find that debate consistently helps\nboth non-expert models and humans answer questions, achieving 76\\% and 88\\%\naccuracy respectively (naive baselines obtain 48\\% and 60\\%). Furthermore,\noptimising expert debaters for persuasiveness in an unsupervised manner\nimproves non-expert ability to identify the truth in debates. Our results\nprovide encouraging empirical evidence for the viability of aligning models\nwith debate in the absence of ground truth.",
        "pdf_link": "https://arxiv.org/pdf/2402.06782v2.pdf"
    },
    {
        "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
        "authors": [
            "Stefan Dernbach",
            "Khushbu Agarwal",
            "Alejandro Zuniga",
            "Michael Henry",
            "Sutanay Choudhury"
        ],
        "published": "2024-02-09T19:53:29Z",
        "summary": "Integrating large language models (LLMs) with knowledge graphs derived from\ndomain-specific data represents an important advancement towards more powerful\nand factual reasoning. As these models grow more capable, it is crucial to\nenable them to perform multi-step inferences over real-world knowledge graphs\nwhile minimizing hallucination. While large language models excel at\nconversation and text generation, their ability to reason over\ndomain-specialized graphs of interconnected entities remains limited. For\nexample, can we query a LLM to identify the optimal contact in a professional\nnetwork for a specific goal, based on relationships and attributes in a private\ndatabase? The answer is no--such capabilities lie beyond current methods.\nHowever, this question underscores a critical technical gap that must be\naddressed. Many high-value applications in areas such as science, security, and\ne-commerce rely on proprietary knowledge graphs encoding unique structures,\nrelationships, and logical constraints. We introduce a fine-tuning framework\nfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge\ngraph into an alternate text representation with labeled question-answer pairs.\nWe demonstrate that grounding the models in specific graph-based knowledge\nexpands the models' capacity for structure-based reasoning. Our methodology\nleverages the large-language model's generative capabilities to create the\ndataset and proposes an efficient alternate to retrieval-augmented generation\nstyled methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.06764v2.pdf"
    },
    {
        "title": "EntGPT: Linking Generative Large Language Models with Knowledge Bases",
        "authors": [
            "Yifan Ding",
            "Amrit Poudel",
            "Qingkai Zeng",
            "Tim Weninger",
            "Balaji Veeramani",
            "Sanmitra Bhattacharya"
        ],
        "published": "2024-02-09T19:16:27Z",
        "summary": "The ability of Large Language Models (LLMs) to generate factually correct\noutput remains relatively unexplored due to the lack of fact-checking and\nknowledge grounding during training and inference. In this work, we aim to\naddress this challenge through the Entity Disambiguation (ED) task. We first\nconsider prompt engineering, and design a three-step hard-prompting method to\nprobe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the\nprompting method improves the micro-F_1 score of the original vanilla models by\na large margin, on some cases up to 36% and higher, and obtains comparable\nperformance across 10 datasets when compared to existing methods with SFT. We\nfurther improve the knowledge grounding ability through instruction tuning (IT)\nwith similar prompts and responses. The instruction-tuned model not only\nachieves higher micro-F1 score performance as compared to several baseline\nmethods on supervised entity disambiguation tasks with an average micro-F_1\nimprovement of 2.1% over the existing baseline models, but also obtains higher\naccuracy on six Question Answering (QA) tasks in the zero-shot setting. Our\nmethodologies apply to both open- and closed-source LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.06738v1.pdf"
    },
    {
        "title": "NICE: To Optimize In-Context Examples or Not?",
        "authors": [
            "Pragya Srivastava",
            "Satvik Golechha",
            "Amit Deshpande",
            "Amit Sharma"
        ],
        "published": "2024-02-09T19:09:19Z",
        "summary": "Recent work shows that in-context learning and optimization of in-context\nexamples (ICE) can significantly improve the accuracy of large language models\n(LLMs) on a wide range of tasks, leading to an apparent consensus that ICE\noptimization is crucial for better performance. However, most of these studies\nassume a fixed or no instruction provided in the prompt. We challenge this\nconsensus by investigating the necessity of optimizing ICE when task-specific\ninstructions are provided and find that there are tasks for which it yields\ndiminishing returns. In particular, using a diverse set of tasks and a\nsystematically created instruction set with gradually added details, we find\nthat as the prompt instruction becomes more detailed, the returns on ICE\noptimization diminish. To characterize this behavior, we introduce a\ntask-specific metric called Normalized Invariability to Choice of Examples\n(NICE) that quantifies the learnability of tasks from a given instruction, and\nprovides a heuristic that helps decide whether to optimize instructions or ICE\nfor a new task. Given a task, the proposed metric can reliably predict the\nutility of optimizing ICE compared to using random ICE.",
        "pdf_link": "https://arxiv.org/pdf/2402.06733v2.pdf"
    },
    {
        "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
        "authors": [
            "Alexander Pan",
            "Erik Jones",
            "Meena Jagadeesan",
            "Jacob Steinhardt"
        ],
        "published": "2024-02-09T18:59:29Z",
        "summary": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.06627v1.pdf"
    },
    {
        "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
        "authors": [
            "Satyapriya Krishna",
            "Chirag Agarwal",
            "Himabindu Lakkaraju"
        ],
        "published": "2024-02-09T18:57:08Z",
        "summary": "The development of Large Language Models (LLMs) has notably transformed\nnumerous sectors, offering impressive text generation capabilities. Yet, the\nreliability and truthfulness of these models remain pressing concerns. To this\nend, we investigate iterative prompting, a strategy hypothesized to refine LLM\nresponses, assessing its impact on LLM truthfulness, an area which has not been\nthoroughly explored. Our extensive experiments delve into the intricacies of\niterative prompting variants, examining their influence on the accuracy and\ncalibration of model responses. Our findings reveal that naive prompting\nmethods significantly undermine truthfulness, leading to exacerbated\ncalibration errors. In response to these challenges, we introduce several\nprompting variants designed to address the identified issues. These variants\ndemonstrate marked improvements over existing baselines, signaling a promising\ndirection for future research. Our work provides a nuanced understanding of\niterative prompting and introduces novel approaches to enhance the truthfulness\nof LLMs, thereby contributing to the development of more accurate and\ntrustworthy AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.06625v1.pdf"
    },
    {
        "title": "If Turing played piano with an artificial partner",
        "authors": [
            "Dobromir Dotov",
            "Dante Camarena",
            "Zack Harris",
            "Joanna Spyra",
            "Pietro Gagliano",
            "Laurel Trainor"
        ],
        "published": "2024-02-09T18:43:48Z",
        "summary": "Music is an inherently social activity that allows people to share\nexperiences and feel connected with one another. There has been little progress\nin designing artificial partners exhibiting a similar social experience as\nplaying with another person. Neural network architectures that implement\ngenerative models, such as large language models, are suited for producing\nmusical scores. Playing music socially, however, involves more than playing a\nscore; it must complement the other musicians' ideas and keep time correctly.\nWe addressed the question of whether a convincing social experience is made\npossible by a generative model trained to produce musical scores, not\nnecessarily optimized for synchronization and continuation. The network, a\nvariational autoencoder trained on a large corpus of digital scores, was\nadapted for a timed call-and-response task with a human partner. Participants\nplayed piano with a human or artificial partner-in various configurations-and\nrated the performance quality and first-person experience of self-other\nintegration. Overall, the artificial partners held promise but were rated lower\nthan human partners. The artificial partner with simplest design and highest\nsimilarity parameter was not rated differently from the human partners on some\nmeasures, suggesting that interactive rather than generative sophistication is\nimportant in enabling social AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.08690v1.pdf"
    },
    {
        "title": "TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations",
        "authors": [
            "Sudhir Agarwal",
            "Anu Sreepathy"
        ],
        "published": "2024-02-09T18:39:13Z",
        "summary": "We study the problem of generating plans for given natural language planning\ntask requests. On one hand, LLMs excel at natural language processing but do\nnot perform well on planning. On the other hand, classical planning tools excel\nat planning tasks but require input in a structured language such as the\nPlanning Domain Definition Language (PDDL). We leverage the strengths of both\nthe techniques by using an LLM for generating the PDDL representation (task\nPDDL) of planning task requests followed by using a classical planner for\ncomputing a plan. Unlike previous approaches that use LLMs for generating task\nPDDLs directly, our approach comprises of (a) translate: using an LLM only for\ngenerating a logically interpretable intermediate representation of natural\nlanguage task descriptions, (b) infer: deriving additional logically dependent\ninformation from the intermediate representation using a logic reasoner\n(currently, Answer Set Programming solver), and (c) compile: generating the\ntarget task PDDL from the base and inferred information. We observe that using\nan LLM to only output the intermediate representation significantly reduces LLM\nerrors. Consequently, TIC approach achieves, for at least one LLM, high\naccuracy on task PDDL generation for all seven domains of our evaluation\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.06608v1.pdf"
    },
    {
        "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language Models",
        "authors": [
            "Xingxuan Zhang",
            "Jiansheng Li",
            "Wenjing Chu",
            "Junjia Hai",
            "Renzhe Xu",
            "Yuqing Yang",
            "Shikai Guan",
            "Jiazheng Xu",
            "Peng Cui"
        ],
        "published": "2024-02-09T18:21:51Z",
        "summary": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.",
        "pdf_link": "https://arxiv.org/pdf/2402.06599v1.pdf"
    },
    {
        "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
        "authors": [
            "Mingzhe Xing",
            "Rongkai Zhang",
            "Hui Xue",
            "Qi Chen",
            "Fan Yang",
            "Zhen Xiao"
        ],
        "published": "2024-02-09T18:19:25Z",
        "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
        "pdf_link": "https://arxiv.org/pdf/2402.06596v1.pdf"
    },
    {
        "title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German",
        "authors": [
            "Ehsan Latif",
            "Gyeong-Geon Lee",
            "Knut Neuman",
            "Tamara Kastorff",
            "Xiaoming Zhai"
        ],
        "published": "2024-02-09T18:05:03Z",
        "summary": "The advancement of natural language processing has paved the way for\nautomated scoring systems in various languages, such as German (e.g., German\nBERT [G-BERT]). Automatically scoring written responses to science questions in\nGerman is a complex task and challenging for standard G-BERT as they lack\ncontextual knowledge in the science domain and may be unaligned with student\nwriting styles. This paper developed a contextualized German Science Education\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\nGerman-written responses to science tasks. Using G-BERT, we pre-trained\nG-SciEdBERT on a corpus of 50K German written science responses with 5M tokens\nto the Programme for International Student Assessment (PISA) 2015. We\nfine-tuned G-SciEdBERT on 59 assessment items and examined the scoring\naccuracy. We then compared its performance with G-BERT. Our findings reveal a\nsubstantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a\n10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy\ndifference = 0.096, SD = 0.024). These insights underline the significance of\nspecialized language models like G-SciEdBERT, which is trained to enhance the\naccuracy of automated scoring, offering a substantial contribution to the field\nof AI in education.",
        "pdf_link": "https://arxiv.org/pdf/2402.06584v1.pdf"
    },
    {
        "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model",
        "authors": [
            "Gregory Coppola"
        ],
        "published": "2024-02-09T17:15:45Z",
        "summary": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which\nprovides a unified view of logical and probabilistic reasoning. The QBBN is\nmeant to address a central problem with the Large Language Model (LLM), which\nhas become extremely popular in Information Retrieval, which is that the LLM\nhallucinates. A Bayesian Network, by construction, cannot hallucinate, because\nit can only return answers that it can explain. We show how a Bayesian Network\nover an unbounded number of boolean variables can be configured to represent\nthe logical reasoning underlying human language. We do this by creating a\nkey-value version of the First-Order Calculus, for which we can prove\nconsistency and completeness. We show that the model is trivially trained over\nfully observed data, but that inference is non-trivial. Exact inference in a\nBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For\ninference, we investigate the use of Loopy Belief Propagation (LBP), which is\nnot guaranteed to converge, but which has been shown to often converge in\npractice. Our experiments show that LBP indeed does converge very reliably, and\nour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds\nthe number of variables considered, and $n$ bounds the number of incoming\nconnections to any factor, and further improvements may be possible. Our\nnetwork is specifically designed to alternate between AND and OR gates in a\nBoolean Algebra, which connects more closely to logical reasoning, allowing a\ncompleteness proof for an expanded version of our network, and also allows\ninference to follow specific but adequate pathways, that turn out to be fast.",
        "pdf_link": "https://arxiv.org/pdf/2402.06557v1.pdf"
    },
    {
        "title": "Calibrating Long-form Generations from Large Language Models",
        "authors": [
            "Yukun Huang",
            "Yixin Liu",
            "Raghuveer Thirukovalluru",
            "Arman Cohan",
            "Bhuwan Dhingra"
        ],
        "published": "2024-02-09T17:00:32Z",
        "summary": "To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.06544v1.pdf"
    },
    {
        "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
        "authors": [
            "Kaiqu Liang",
            "Zixu Zhang",
            "Jaime Fern\u00e1ndez Fisac"
        ],
        "published": "2024-02-09T16:40:59Z",
        "summary": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural\nlanguage instructions can induce task uncertainty, particularly in situations\nwhere multiple valid options exist. To address this issue, LLMs must identify\nsuch uncertainty and proactively seek clarification. This paper explores the\nconcept of introspective planning as a systematic method for guiding LLMs in\nforming uncertainty--aware plans for robotic task execution without the need\nfor fine-tuning. We investigate uncertainty quantification in task-level robot\nplanning and demonstrate that introspection significantly improves both success\nrates and safety compared to state-of-the-art LLM-based planning approaches.\nFurthermore, we assess the effectiveness of introspective planning in\nconjunction with conformal prediction, revealing that this combination yields\ntighter confidence bounds, thereby maintaining statistical success guarantees\nwith fewer superfluous user clarification queries.",
        "pdf_link": "https://arxiv.org/pdf/2402.06529v2.pdf"
    },
    {
        "title": "Large Language Models for Captioning and Retrieving Remote Sensing Images",
        "authors": [
            "Jo\u00e3o Daniel Silva",
            "Jo\u00e3o Magalh\u00e3es",
            "Devis Tuia",
            "Bruno Martins"
        ],
        "published": "2024-02-09T15:31:01Z",
        "summary": "Image captioning and cross-modal retrieval are examples of tasks that involve\nthe joint analysis of visual and linguistic information. In connection to\nremote sensing imagery, these tasks can help non-expert users in extracting\nrelevant Earth observation information for a variety of applications. Still,\ndespite some previous efforts, the development and application of vision and\nlanguage models to the remote sensing domain have been hindered by the\nrelatively small size of the available datasets and models used in previous\nstudies. In this work, we propose RS-CapRet, a Vision and Language method for\nremote sensing tasks, in particular image captioning and text-image retrieval.\nWe specifically propose to use a highly capable large decoder language model\ntogether with image encoders adapted to remote sensing imagery through\ncontrastive language-image pre-training. To bridge together the image encoder\nand language decoder, we propose training simple linear layers with examples\nfrom combining different remote sensing image captioning datasets, keeping the\nother parameters frozen. RS-CapRet can then generate descriptions for remote\nsensing images and retrieve images from textual descriptions, achieving SOTA or\ncompetitive performance with existing methods. Qualitative results illustrate\nthat RS-CapRet can effectively leverage the pre-trained large language model to\ndescribe remote sensing images, retrieve them based on different types of\nqueries, and also show the ability to process interleaved sequences of images\nand text in a dialogue manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.06475v1.pdf"
    },
    {
        "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
        "authors": [
            "Arian Hosseini",
            "Xingdi Yuan",
            "Nikolay Malkin",
            "Aaron Courville",
            "Alessandro Sordoni",
            "Rishabh Agarwal"
        ],
        "published": "2024-02-09T15:02:56Z",
        "summary": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated\nsolutions to improve their problem-solving ability. However, these approaches\ndiscard the large amounts of incorrect solutions generated during this process,\npotentially neglecting valuable information in such solutions. To address this\nshortcoming, we propose V-STaR that utilizes both the correct and incorrect\nsolutions generated during the self-improvement process to train a verifier\nusing DPO that judges correctness of model-generated solutions. This verifier\nis used at inference time to select one solution among many candidate\nsolutions. Running V-STaR for multiple iterations results in progressively\nbetter reasoners and verifiers, delivering a 4% to 17% test accuracy\nimprovement over existing self-improvement and verification approaches on\ncommon code generation and math reasoning benchmarks with LLaMA2 models.",
        "pdf_link": "https://arxiv.org/pdf/2402.06457v1.pdf"
    },
    {
        "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models",
        "authors": [
            "Peiyuan Gong",
            "Jiamian Li",
            "Jiaxin Mao"
        ],
        "published": "2024-02-09T12:10:00Z",
        "summary": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.",
        "pdf_link": "https://arxiv.org/pdf/2402.06360v1.pdf"
    },
    {
        "title": "RareBench: Can LLMs Serve as Rare Diseases Specialists?",
        "authors": [
            "Xuanzhong Chen",
            "Xiaohao Mao",
            "Qihan Guo",
            "Lun Wang",
            "Shuyang Zhang",
            "Ting Chen"
        ],
        "published": "2024-02-09T11:34:16Z",
        "summary": "Generalist Large Language Models (LLMs), such as GPT-4, have shown\nconsiderable promise in various domains, including medical diagnosis. Rare\ndiseases, affecting approximately 300 million people worldwide, often have\nunsatisfactory clinical diagnosis rates primarily due to a lack of experienced\nphysicians and the complexity of differentiating among many rare diseases. In\nthis context, recent news such as \"ChatGPT correctly diagnosed a 4-year-old's\nrare disease after 17 doctors failed\" underscore LLMs' potential, yet\nunderexplored, role in clinically diagnosing rare diseases. To bridge this\nresearch gap, we introduce RareBench, a pioneering benchmark designed to\nsystematically evaluate the capabilities of LLMs on 4 critical dimensions\nwithin the realm of rare diseases. Meanwhile, we have compiled the largest\nopen-source dataset on rare disease patients, establishing a benchmark for\nfuture studies in this domain. To facilitate differential diagnosis of rare\ndiseases, we develop a dynamic few-shot prompt methodology, leveraging a\ncomprehensive rare disease knowledge graph synthesized from multiple knowledge\nbases, significantly enhancing LLMs' diagnostic performance. Moreover, we\npresent an exhaustive comparative study of GPT-4's diagnostic capabilities\nagainst those of specialist physicians. Our experimental findings underscore\nthe promising potential of integrating LLMs into the clinical diagnostic\nprocess for rare diseases. This paves the way for exciting possibilities in\nfuture advancements in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.06341v1.pdf"
    },
    {
        "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs",
        "authors": [
            "Fernando Ferraretto",
            "Thiago Laitz",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2024-02-09T11:23:14Z",
        "summary": "ExaRanker recently introduced an approach to training information retrieval\n(IR) models, incorporating natural language explanations as additional labels.\nThe method addresses the challenge of limited labeled examples, leading to\nimprovements in the effectiveness of IR models. However, the initial results\nwere based on proprietary language models such as GPT-3.5, which posed\nconstraints on dataset size due to its cost and data privacy. In this paper, we\nintroduce ExaRanker-Open, where we adapt and explore the use of open-source\nlanguage models to generate explanations. The method has been tested using\ndifferent LLMs and datasets sizes to better comprehend the effective\ncontribution of data augmentation. Our findings reveal that incorporating\nexplanations consistently enhances neural rankers, with benefits escalating as\nthe LLM size increases. Notably, the data augmentation method proves\nadvantageous even with large datasets, as evidenced by ExaRanker surpassing the\ntarget baseline by 0.6 nDCG@10 points in our study. To encourage further\nadvancements by the research community, we have open-sourced both the code and\ndatasets at https://github.com/unicamp-dl/ExaRanker.",
        "pdf_link": "https://arxiv.org/pdf/2402.06334v1.pdf"
    },
    {
        "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
        "authors": [
            "Huaiyuan Ying",
            "Shuo Zhang",
            "Linyang Li",
            "Zhejian Zhou",
            "Yunfan Shao",
            "Zhaoye Fei",
            "Yichuan Ma",
            "Jiawei Hong",
            "Kuikun Liu",
            "Ziyi Wang",
            "Yudong Wang",
            "Zijian Wu",
            "Shuaibin Li",
            "Fengzhe Zhou",
            "Hongwei Liu",
            "Songyang Zhang",
            "Wenwei Zhang",
            "Hang Yan",
            "Xipeng Qiu",
            "Jiayu Wang",
            "Kai Chen",
            "Dahua Lin"
        ],
        "published": "2024-02-09T11:22:08Z",
        "summary": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.",
        "pdf_link": "https://arxiv.org/pdf/2402.06332v1.pdf"
    },
    {
        "title": "Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales",
        "authors": [
            "Wenyu Li",
            "Yinuo Zhu",
            "Xin Lin",
            "Ming Li",
            "Ziyue Jiang",
            "Ziqian Zeng"
        ],
        "published": "2024-02-09T09:44:06Z",
        "summary": "Traditional discriminative approaches in mental health analysis are known for\ntheir strong capacity but lack interpretability and demand large-scale\nannotated data. The generative approaches, such as those based on large\nlanguage models (LLMs), have the potential to get rid of heavy annotations and\nprovide explanations but their capabilities still fall short compared to\ndiscriminative approaches, and their explanations may be unreliable due to the\nfact that the generation of explanation is a black-box process. Inspired by the\npsychological assessment practice of using scales to evaluate mental states,\nour method which is called Mental Analysis by Incorporating Mental Scales\n(MAIMS), incorporates two procedures via LLMs. First, the patient completes\nmental scales, and second, the psychologist interprets the collected\ninformation from the mental scales and makes informed decisions. Experimental\nresults show that MAIMS outperforms other zero-shot methods. MAIMS can generate\nmore rigorous explanation based on the outputs of mental scales",
        "pdf_link": "https://arxiv.org/pdf/2402.10948v2.pdf"
    },
    {
        "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
        "authors": [
            "Unggi Lee",
            "Minji Jeon",
            "Yunseo Lee",
            "Gyuri Byun",
            "Yoorim Son",
            "Jaeyoon Shin",
            "Hongkyu Ko",
            "Hyeoncheol Kim"
        ],
        "published": "2024-02-09T09:25:18Z",
        "summary": "Art appreciation is vital in nurturing critical thinking and emotional\nintelligence among learners. However, traditional art appreciation education\nhas often been hindered by limited access to art resources, especially for\ndisadvantaged students, and an imbalanced emphasis on STEM subjects in\nmainstream education. In response to these challenges, recent technological\nadvancements have paved the way for innovative solutions. This study explores\nthe application of multi-modal large language models (MLLMs) in art\nappreciation education, focusing on developing LLaVA-Docent, a model that\nleverages these advancements. Our approach involved a comprehensive literature\nreview and consultations with experts in the field, leading to developing a\nrobust data framework. Utilizing this framework, we generated a virtual\ndialogue dataset that was leveraged by GPT-4. This dataset was instrumental in\ntraining the MLLM, named LLaVA-Docent. Six researchers conducted quantitative\nand qualitative evaluations of LLaVA-Docent to assess its effectiveness,\nbenchmarking it against the GPT-4 model in a few-shot setting. The evaluation\nprocess revealed distinct strengths and weaknesses of the LLaVA-Docent model.\nOur findings highlight the efficacy of LLaVA-Docent in enhancing the\naccessibility and engagement of art appreciation education. By harnessing the\npotential of MLLMs, this study makes a significant contribution to the field of\nart education, proposing a novel methodology that reimagines the way art\nappreciation is taught and experienced.",
        "pdf_link": "https://arxiv.org/pdf/2402.06264v1.pdf"
    },
    {
        "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
        "authors": [
            "Siyu Ren",
            "Kenny Q. Zhu"
        ],
        "published": "2024-02-09T09:20:59Z",
        "summary": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.",
        "pdf_link": "https://arxiv.org/pdf/2402.06262v2.pdf"
    },
    {
        "title": "Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
        "authors": [
            "Yichuan Mo",
            "Yuji Wang",
            "Zeming Wei",
            "Yisen Wang"
        ],
        "published": "2024-02-09T09:09:39Z",
        "summary": "Although Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to certain prompts that can\ninduce them to bypass built-in safety measures and provide dangerous or illegal\ncontent, a phenomenon known as jailbreak. To protect LLMs from producing\nharmful information, various defense strategies are proposed, with most\nfocusing on content filtering or adversarial training of models. In this paper,\nwe propose an approach named Prompt Adversarial Tuning (PAT) to train a defense\ncontrol mechanism, which is then embedded as a prefix to user prompts to\nimplement our defense strategy. We design a training process similar to\nadversarial training to achieve our optimized goal, alternating between\nupdating attack and defense controls. To our knowledge, we are the first to\nimplement defense from the perspective of prompt tuning. Once employed, our\nmethod will hardly impact the operational efficiency of LLMs. Experiments show\nthat our method is effective in both black-box and white-box settings, reducing\nthe success rate of advanced attacks to nearly 0 while maintaining the benign\nanswer rate of 80% to simple benign questions. Our work might potentially chart\na new perspective for future explorations in LLM security.",
        "pdf_link": "https://arxiv.org/pdf/2402.06255v1.pdf"
    },
    {
        "title": "Entropy-Regularized Token-Level Policy Optimization for Large Language Models",
        "authors": [
            "Muning Wen",
            "Cheng Deng",
            "Jun Wang",
            "Weinan Zhang",
            "Ying Wen"
        ],
        "published": "2024-02-09T07:45:26Z",
        "summary": "Large Language Models (LLMs) have shown promise as intelligent agents in\ninteractive decision-making tasks. Traditional approaches often depend on\nmeticulously designed prompts, high-quality examples, or additional reward\nmodels for in-context learning, supervised fine-tuning, or RLHF. Reinforcement\nlearning (RL) presents a dynamic alternative for LLMs to overcome these\ndependencies by engaging directly with task-specific environments. Nonetheless,\nit faces significant hurdles: 1) instability stemming from the exponentially\nvast action space requiring exploration; 2) challenges in assigning token-level\ncredit based on action-level reward signals, resulting in discord between\nmaximizing rewards and accurately modeling corpus data. In response to these\nchallenges, we introduce Entropy-Regularized Token-level Policy Optimization\n(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the\ntoken level. At the heart of ETPO is our novel per-token soft Bellman update,\ndesigned to harmonize the RL process with the principles of language modeling.\nThis methodology decomposes the Q-function update from a coarse action-level\nview to a more granular token-level perspective, backed by theoretical proof of\noptimization consistency. Crucially, this decomposition renders linear time\ncomplexity in action exploration. We assess the effectiveness of ETPO within a\nsimulated environment that models data science code generation as a series of\nmulti-step interactive tasks; results show that ETPO achieves effective\nperformance improvement on the CodeLlama-7B model and surpasses a variant PPO\nbaseline inherited from RLHF. This underlines ETPO's potential as a robust\nmethod for refining the interactive decision-making capabilities of LLMs. Our\ncode is open-sourced at https://github.com/morning9393/ETPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.06700v2.pdf"
    },
    {
        "title": "Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants",
        "authors": [
            "Bhavya Chopra",
            "Yasharth Bajpai",
            "Param Biyani",
            "Gustavo Soares",
            "Arjun Radhakrishna",
            "Chris Parnin",
            "Sumit Gulwani"
        ],
        "published": "2024-02-09T07:44:27Z",
        "summary": "The widespread availability of Large Language Models (LLMs) within Integrated\nDevelopment Environments (IDEs) has led to their speedy adoption.\nConversational interactions with LLMs enable programmers to obtain natural\nlanguage explanations for various software development tasks. However, LLMs\noften leap to action without sufficient context, giving rise to implicit\nassumptions and inaccurate responses. Conversations between developers and LLMs\nare primarily structured as question-answer pairs, where the developer is\nresponsible for asking the the right questions and sustaining conversations\nacross multiple turns. In this paper, we draw inspiration from interaction\npatterns and conversation analysis -- to design Robin, an enhanced\nconversational AI-assistant for debugging. Through a within-subjects user study\nwith 12 industry professionals, we find that equipping the LLM to -- (1)\nleverage the insert expansion interaction pattern, (2) facilitate turn-taking,\nand (3) utilize debugging workflows -- leads to lowered conversation barriers,\neffective fault localization, and 5x improvement in bug resolution rates.",
        "pdf_link": "https://arxiv.org/pdf/2402.06229v1.pdf"
    },
    {
        "title": "ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement",
        "authors": [
            "Saurabh Bhausaheb Zinjad",
            "Amrita Bhattacharjee",
            "Amey Bhilegaonkar",
            "Huan Liu"
        ],
        "published": "2024-02-09T07:13:44Z",
        "summary": "Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.",
        "pdf_link": "https://arxiv.org/pdf/2402.06221v1.pdf"
    },
    {
        "title": "The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",
        "authors": [
            "Juhyun Oh",
            "Eunsu Kim",
            "Inha Cha",
            "Alice Oh"
        ],
        "published": "2024-02-09T06:16:08Z",
        "summary": "This paper explores the assumption that Large Language Models (LLMs) skilled\nin generation tasks are equally adept as evaluators. We assess the performance\nof three LLMs and one open-source LM in Question-Answering (QA) and evaluation\ntasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a\nsignificant disparity, with LLMs exhibiting lower performance in evaluation\ntasks compared to generation tasks. Intriguingly, we discover instances of\nunfaithful evaluation where models accurately evaluate answers in areas where\nthey lack competence, underscoring the need to examine the faithfulness and\ntrustworthiness of LLMs as evaluators. This study contributes to the\nunderstanding of \"the Generative AI Paradox\" (West et al., 2023), highlighting\na need to explore the correlation between generative excellence and evaluation\nproficiency, and the necessity to scrutinize the faithfulness aspect in model\nevaluations.",
        "pdf_link": "https://arxiv.org/pdf/2402.06204v1.pdf"
    },
    {
        "title": "Large Language Models: A Survey",
        "authors": [
            "Shervin Minaee",
            "Tomas Mikolov",
            "Narjes Nikzad",
            "Meysam Chenaghlu",
            "Richard Socher",
            "Xavier Amatriain",
            "Jianfeng Gao"
        ],
        "published": "2024-02-09T05:37:09Z",
        "summary": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.06196v2.pdf"
    },
    {
        "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
        "authors": [
            "Cheng Li",
            "Mengzhou Chen",
            "Jindong Wang",
            "Sunayana Sitaram",
            "Xing Xie"
        ],
        "published": "2024-02-09T04:02:43Z",
        "summary": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation.",
        "pdf_link": "https://arxiv.org/pdf/2402.10946v1.pdf"
    },
    {
        "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
        "authors": [
            "Haizhong Zheng",
            "Xiaoyan Bai",
            "Beidi Chen",
            "Fan Lai",
            "Atul Prakash"
        ],
        "published": "2024-02-09T01:18:16Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. Existing\nmethods only focus on utilizing this naturally formed activation sparsity,\noverlooking the potential for further amplifying this inherent sparsity. In\nthis paper, we hypothesize that LLMs can learn to be efficient by achieving\nmore structured activation sparsity. To achieve this, we introduce a novel\nalgorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs\nto learn to activate fewer neurons and achieve a better trade-off between\nsparsity and performance. Furthermore, unlike SOTA MoEfication methods, which\nmainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and\nLLaMA with soft activation functions. We evaluate LTE on four models and eleven\ndatasets. The experiments show that LTE achieves a better trade-off between\nsparsity and task performance. For instance, LTE with LLaMA provides a\n1.83x-2.59x FLOPs speed-up on language generation tasks, outperforming the\nstate-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.06126v2.pdf"
    },
    {
        "title": "Exploring Group and Symmetry Principles in Large Language Models",
        "authors": [
            "Shima Imani",
            "Hamid Palangi"
        ],
        "published": "2024-02-09T01:10:25Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
        "pdf_link": "https://arxiv.org/pdf/2402.06120v1.pdf"
    },
    {
        "title": "ContPhy: Continuum Physical Concept Learning and Reasoning from Videos",
        "authors": [
            "Zhicheng Zheng",
            "Xin Yan",
            "Zhenfang Chen",
            "Jingzhou Wang",
            "Qin Zhi Eddie Lim",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "2024-02-09T01:09:21Z",
        "summary": "We introduce the Continuum Physical Dataset (ContPhy), a novel benchmark for\nassessing machine physical commonsense. ContPhy complements existing physical\nreasoning benchmarks by encompassing the inference of diverse physical\nproperties, such as mass and density, across various scenarios and predicting\ncorresponding dynamics. We evaluated a range of AI models and found that they\nstill struggle to achieve satisfactory performance on ContPhy, which shows that\nthe current AI models still lack physical commonsense for the continuum,\nespecially soft-bodies, and illustrates the value of the proposed dataset. We\nalso introduce an oracle model (ContPRO) that marries the particle-based\nphysical dynamic models with the recent large language models, which enjoy the\nadvantages of both models, precise dynamic predictions, and interpretable\nreasoning. ContPhy aims to spur progress in perception and reasoning within\ndiverse physical settings, narrowing the divide between human and machine\nintelligence in understanding the physical world. Project page:\nhttps://physical-reasoning-project.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.06119v1.pdf"
    },
    {
        "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
        "authors": [
            "Siming Yan",
            "Min Bai",
            "Weifeng Chen",
            "Xiong Zhou",
            "Qixing Huang",
            "Li Erran Li"
        ],
        "published": "2024-02-09T01:00:14Z",
        "summary": "By combining natural language understanding and the generation capabilities\nand breadth of knowledge of large language models with image perception, recent\nlarge vision language models (LVLMs) have shown unprecedented reasoning\ncapabilities in the real world. However, the generated text often suffers from\ninaccurate grounding in the visual input, resulting in errors such as\nhallucinating nonexistent scene elements, missing significant parts of the\nscene, and inferring incorrect attributes and relationships between objects. To\naddress these issues, we introduce a novel framework, ViGoR (Visual Grounding\nThrough Fine-Grained Reward Modeling) that utilizes fine-grained reward\nmodeling to significantly enhance the visual grounding of LVLMs over\npre-trained baselines. This improvement is efficiently achieved using much\ncheaper human evaluations instead of full supervisions, as well as automated\nmethods. We show the effectiveness of our approach through numerous metrics on\nseveral benchmarks. Additionally, we construct a comprehensive and challenging\ndataset specifically designed to validate the visual grounding capabilities of\nLVLMs. Finally, we plan to release our human annotation comprising\napproximately 16,000 images and generated text pairs with fine-grained\nevaluations to contribute to related research in the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.06118v1.pdf"
    },
    {
        "title": "LLMs for Coding and Robotics Education",
        "authors": [
            "Peng Shu",
            "Huaqin Zhao",
            "Hanqi Jiang",
            "Yiwei Li",
            "Shaochen Xu",
            "Yi Pan",
            "Zihao Wu",
            "Zhengliang Liu",
            "Guoyu Lu",
            "Le Guan",
            "Gong Chen",
            "Xianqiao Wang Tianming Liu"
        ],
        "published": "2024-02-09T00:58:57Z",
        "summary": "Large language models and multimodal large language models have\nrevolutionized artificial intelligence recently. An increasing number of\nregions are now embracing these advanced technologies. Within this context,\nrobot coding education is garnering increasing attention. To teach young\nchildren how to code and compete in robot challenges, large language models are\nbeing utilized for robot code explanation, generation, and modification. In\nthis paper, we highlight an important trend in robot coding education. We test\nseveral mainstream large language models on both traditional coding tasks and\nthe more challenging task of robot code generation, which includes block\ndiagrams. Our results show that GPT-4V outperforms other models in all of our\ntests but struggles with generating block diagram images.",
        "pdf_link": "https://arxiv.org/pdf/2402.06116v1.pdf"
    },
    {
        "title": "SubGen: Token Generation in Sublinear Time and Memory",
        "authors": [
            "Amir Zandieh",
            "Insu Han",
            "Vahab Mirrokni",
            "Amin Karbasi"
        ],
        "published": "2024-02-08T22:17:40Z",
        "summary": "Despite the significant success of large language models (LLMs), their\nextensive memory requirements pose challenges for deploying them in\nlong-context token generation. The substantial memory footprint of LLM decoders\narises from the necessity to store all previous tokens in the attention module,\na requirement imposed by key-value (KV) caching. In this work, our focus is on\ndeveloping an efficient compression technique for the KV cache. Empirical\nevidence indicates a significant clustering tendency within key embeddings in\nthe attention module. Building on this key insight, we have devised a novel\ncaching method with sublinear complexity, employing online clustering on key\ntokens and online $\\ell_2$ sampling on values. The result is a provably\naccurate and efficient attention decoding algorithm, termed SubGen. Not only\ndoes this algorithm ensure a sublinear memory footprint and sublinear time\ncomplexity, but we also establish a tight error bound for our approach.\nEmpirical evaluations on long-context question-answering tasks demonstrate that\nSubGen significantly outperforms existing and state-of-the-art KV cache\ncompression methods in terms of performance and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.06082v1.pdf"
    },
    {
        "title": "Large Language Model Augmented Exercise Retrieval for Personalized Language Learning",
        "authors": [
            "Austin Xu",
            "Will Monroe",
            "Klinton Bicknell"
        ],
        "published": "2024-02-08T20:35:31Z",
        "summary": "We study the problem of zero-shot exercise retrieval in the context of online\nlanguage learning, to give learners the ability to explicitly request\npersonalized exercises via natural language. Using real-world data collected\nfrom language learners, we observe that vector similarity approaches poorly\ncapture the relationship between exercise content and the language that\nlearners use to express what they want to learn. This semantic gap between\nqueries and content dramatically reduces the effectiveness of general-purpose\nretrieval models pretrained on large scale information retrieval datasets like\nMS MARCO. We leverage the generative capabilities of large language models to\nbridge the gap by synthesizing hypothetical exercises based on the learner's\ninput, which are then used to search for relevant exercises. Our approach,\nwhich we call mHyER, overcomes three challenges: (1) lack of relevance labels\nfor training, (2) unrestricted learner input content, and (3) low semantic\nsimilarity between input and retrieval candidates. mHyER outperforms several\nstrong baselines on two novel benchmarks created from crowdsourced data and\npublicly available data.",
        "pdf_link": "https://arxiv.org/pdf/2402.16877v1.pdf"
    },
    {
        "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
        "authors": [
            "Hainiu Xu",
            "Runcong Zhao",
            "Lixing Zhu",
            "Jinhua Du",
            "Yulan He"
        ],
        "published": "2024-02-08T20:35:06Z",
        "summary": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track\nof the mental states of others, is pivotal in developing socially intelligent\nagents. However, prevalent N-ToM benchmarks have several shortcomings,\nincluding the presence of ambiguous and artificial narratives, absence of\npersonality traits and preferences, a lack of questions addressing characters'\npsychological mental states, and limited diversity in the questions posed. In\nresponse to these issues, we construct OpenToM, a new benchmark for assessing\nN-ToM with (1) longer and clearer narrative stories, (2) characters with\nexplicit personality traits, (3) actions that are triggered by character\nintentions, and (4) questions designed to challenge LLMs' capabilities of\nmodeling characters' mental states of both the physical and psychological\nworld. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling\ncertain aspects of mental states in the physical world but fall short when\ntracking characters' mental states in the psychological world.",
        "pdf_link": "https://arxiv.org/pdf/2402.06044v2.pdf"
    },
    {
        "title": "A Prompt Response to the Demand for Automatic Gender-Neutral Translation",
        "authors": [
            "Beatrice Savoldi",
            "Andrea Piergentili",
            "Dennis Fucci",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2024-02-08T20:24:44Z",
        "summary": "Gender-neutral translation (GNT) that avoids biased and undue binary\nassumptions is a pivotal challenge for the creation of more inclusive\ntranslation technologies. Advancements for this task in Machine Translation\n(MT), however, are hindered by the lack of dedicated parallel data, which are\nnecessary to adapt MT systems to satisfy neutral constraints. For such a\nscenario, large language models offer hitherto unforeseen possibilities, as\nthey come with the distinct advantage of being versatile in various (sub)tasks\nwhen provided with explicit instructions. In this paper, we explore this\npotential to automate GNT by comparing MT with the popular GPT-4 model. Through\nextensive manual analyses, our study empirically reveals the inherent\nlimitations of current MT systems in generating GNTs and provides valuable\ninsights into the potential and challenges associated with prompting for\nneutrality.",
        "pdf_link": "https://arxiv.org/pdf/2402.06041v1.pdf"
    },
    {
        "title": "Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing",
        "authors": [
            "Yong Cao",
            "Wenyan Li",
            "Jiaang Li",
            "Yifei Yuan",
            "Antonia Karamolegkou",
            "Daniel Hershcovich"
        ],
        "published": "2024-02-08T19:25:40Z",
        "summary": "Pretrained large Vision-Language models have drawn considerable interest in\nrecent years due to their remarkable performance. Despite considerable efforts\nto assess these models from diverse perspectives, the extent of visual cultural\nawareness in the state-of-the-art GPT-4V model remains unexplored. To tackle\nthis gap, we extensively probed GPT-4V using the MaRVL benchmark dataset,\naiming to investigate its capabilities and limitations in visual understanding\nwith a focus on cultural aspects. Specifically, we introduced three visual\nrelated tasks, i.e. caption classification, pairwise captioning, and culture\ntag selection, to systematically delve into fine-grained visual cultural\nevaluation. Experimental results indicate that GPT-4V excels at identifying\ncultural concepts but still exhibits weaker performance in low-resource\nlanguages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V\nproves to be more culturally relevant in image captioning tasks than the\noriginal MaRVL human annotations, suggesting a promising solution for future\nvisual cultural benchmark construction.",
        "pdf_link": "https://arxiv.org/pdf/2402.06015v2.pdf"
    },
    {
        "title": "LLMs Among Us: Generative AI Participating in Digital Discourse",
        "authors": [
            "Kristina Radivojevic",
            "Nicholas Clark",
            "Paul Brenner"
        ],
        "published": "2024-02-08T19:21:33Z",
        "summary": "The emergence of Large Language Models (LLMs) has great potential to reshape\nthe landscape of many social media platforms. While this can bring promising\nopportunities, it also raises many threats, such as biases and privacy\nconcerns, and may contribute to the spread of propaganda by malicious actors.\nWe developed the \"LLMs Among Us\" experimental framework on top of the Mastodon\nsocial media platform for bot and human participants to communicate without\nknowing the ratio or nature of bot and human participants. We built 10 personas\nwith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three\nrounds of the experiment and surveyed participants after each round to measure\nthe ability of LLMs to pose as human participants without human detection. We\nfound that participants correctly identified the nature of other users in the\nexperiment only 42% of the time despite knowing the presence of both bots and\nhumans. We also found that the choice of persona had substantially more impact\non human perception than the choice of mainstream LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.07940v1.pdf"
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": [
            "Xing Han L\u00f9",
            "Zden\u011bk Kasner",
            "Siva Reddy"
        ],
        "published": "2024-02-08T18:58:02Z",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "pdf_link": "https://arxiv.org/pdf/2402.05930v1.pdf"
    },
    {
        "title": "On the Convergence of Zeroth-Order Federated Tuning for Large Language Models",
        "authors": [
            "Zhenqing Ling",
            "Daoyuan Chen",
            "Liuyi Yao",
            "Yaliang Li",
            "Ying Shen"
        ],
        "published": "2024-02-08T18:56:40Z",
        "summary": "The confluence of Federated Learning (FL) and Large Language Models (LLMs) is\nushering in a new era in privacy-preserving natural language processing.\nHowever, the intensive memory requirements for fine-tuning LLMs pose\nsignificant challenges, especially when deploying on clients with limited\ncomputational resources. To circumvent this, we explore the novel integration\nof Memory-efficient Zeroth-Order Optimization within a federated setting, a\nsynergy we term as FedMeZO. Our study is the first to examine the theoretical\nunderpinnings of FedMeZO in the context of LLMs, tackling key questions\nregarding the influence of large parameter spaces on optimization behavior, the\nestablishment of convergence properties, and the identification of critical\nparameters for convergence to inform personalized federated strategies. Our\nextensive empirical evidence supports the theory, showing that FedMeZO not only\nconverges faster than traditional first-order methods such as FedAvg but also\nsignificantly reduces GPU memory usage during training to levels comparable to\nthose during inference. Moreover, the proposed personalized FL strategy that is\nbuilt upon the theoretical insights to customize the client-wise learning rate\ncan effectively accelerate loss reduction. We hope our work can help to bridge\ntheoretical and practical aspects of federated fine-tuning for LLMs, thereby\nstimulating further advancements and research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2402.05926v2.pdf"
    },
    {
        "title": "Efficient Stagewise Pretraining via Progressive Subnetworks",
        "authors": [
            "Abhishek Panigrahi",
            "Nikunj Saunshi",
            "Kaifeng Lyu",
            "Sobhan Miryoosefi",
            "Sashank Reddi",
            "Satyen Kale",
            "Sanjiv Kumar"
        ],
        "published": "2024-02-08T18:49:09Z",
        "summary": "Recent developments in large language models have sparked interest in\nefficient pretraining methods. A recent effective paradigm is to perform\nstage-wise training, where the size of the model is gradually increased over\nthe course of training (e.g. gradual stacking (Reddi et al., 2023)). While the\nresource and wall-time savings are appealing, it has limitations, particularly\nthe inability to evaluate the full model during earlier stages, and degradation\nin model quality due to smaller model capacity in the initial stages. In this\nwork, we propose an alternative framework, progressive subnetwork training,\nthat maintains the full model throughout training, but only trains subnetworks\nwithin the model in each step. We focus on a simple instantiation of this\nframework, Random Path Training (RaPTr) that only trains a sub-path of layers\nin each step, progressively increasing the path lengths in stages. RaPTr\nachieves better pre-training loss for BERT and UL2 language models while\nrequiring 20-33% fewer FLOPs compared to standard training, and is competitive\nor better than other efficient training methods. Furthermore, RaPTr shows\nbetter downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%\ncompared to standard training and stacking. Finally, we provide a theoretical\nbasis for RaPTr to justify (a) the increasing complexity of subnetworks in\nstages, and (b) the stability in loss across stage transitions due to residual\nconnections and layer norm.",
        "pdf_link": "https://arxiv.org/pdf/2402.05913v1.pdf"
    },
    {
        "title": "FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs",
        "authors": [
            "Eun Cheol Choi",
            "Emilio Ferrara"
        ],
        "published": "2024-02-08T18:43:05Z",
        "summary": "Our society is facing rampant misinformation harming public health and trust.\nTo address the societal challenge, we introduce FACT-GPT, a system leveraging\nLarge Language Models (LLMs) to automate the claim matching stage of\nfact-checking. FACT-GPT, trained on a synthetic dataset, identifies social\nmedia content that aligns with, contradicts, or is irrelevant to previously\ndebunked claims. Our evaluation shows that our specialized LLMs can match the\naccuracy of larger models in identifying related claims, closely mirroring\nhuman judgment. This research provides an automated solution for efficient\nclaim matching, demonstrates the potential of LLMs in supporting fact-checkers,\nand offers valuable resources for further research in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.05904v1.pdf"
    },
    {
        "title": "Large Language Model Meets Graph Neural Network in Knowledge Distillation",
        "authors": [
            "Shengxiang Hu",
            "Guobing Zou",
            "Song Yang",
            "Yanglan Gan",
            "Bofeng Zhang",
            "Yixin Chen"
        ],
        "published": "2024-02-08T18:33:21Z",
        "summary": "Despite recent community revelations about the advancements and potential\napplications of Large Language Models (LLMs) in understanding Text-Attributed\nGraph (TAG), the deployment of LLMs for production is hindered by its high\ncomputational and storage requirements, as well as long latencies during model\ninference. Simultaneously, although traditional Graph Neural Networks (GNNs)\nare light weight and adept at learning structural features of graphs, their\nability to grasp the complex semantics in TAG is somewhat constrained for real\napplications. To address these limitations, we concentrate on the downstream\ntask of node classification in TAG and propose a novel graph knowledge\ndistillation framework, termed Linguistic Graph Knowledge Distillation\n(LinguGKD), using LLMs as teacher models and GNNs as student models for\nknowledge distillation. It involves TAG-oriented instruction tuning of LLM on\ndesigned tailored prompts, followed by propagating knowledge and aligning the\nhierarchically learned node features from the teacher LLM to the student GNN in\nlatent space, employing a layer-adaptive contrastive learning strategy. Through\nextensive experiments on a variety of LLM and GNN models and multiple benchmark\ndatasets, the proposed LinguGKD significantly boosts the student GNN's\npredictive accuracy and convergence rate, without the need of extra data or\nmodel parameters. Compared to teacher LLM, distilled GNN achieves superior\ninference speed equipped with much fewer computing and storage demands, when\nsurpassing the teacher LLM's classification accuracy on some of benchmark\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.05894v2.pdf"
    },
    {
        "title": "CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion",
        "authors": [
            "Shoubin Yu",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "published": "2024-02-08T18:27:22Z",
        "summary": "Despite impressive advancements in multimodal compositional reasoning\napproaches, they are still limited in their flexibility and efficiency by\nprocessing fixed modality inputs while updating a lot of model parameters. This\npaper tackles these critical challenges and proposes CREMA, an efficient and\nmodular modality-fusion framework for injecting any new modality into video\nreasoning. We first augment multiple informative modalities (such as optical\nflow, 3D point cloud, audio) from given videos without extra human annotation\nby leveraging existing pre-trained models. Next, we introduce a query\ntransformer with multiple parameter-efficient modules associated with each\naccessible modality. It projects diverse modality features to the LLM token\nembedding space, allowing the model to integrate different data types for\nresponse generation. Furthermore, we propose a fusion module designed to\ncompress multimodal queries, maintaining computational efficiency in the LLM\nwhile combining additional modalities. We validate our method on video-3D,\nvideo-audio, and video-language reasoning tasks and achieve better/equivalent\nperformance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and\nSeViLA while using 96% fewer trainable parameters. We provide extensive\nanalyses of CREMA, including the impact of each modality on reasoning domains,\nthe design of the fusion module, and example visualizations.",
        "pdf_link": "https://arxiv.org/pdf/2402.05889v1.pdf"
    },
    {
        "title": "Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking",
        "authors": [
            "Nikhil Sharma",
            "Q. Vera Liao",
            "Ziang Xiao"
        ],
        "published": "2024-02-08T18:14:33Z",
        "summary": "Large language models (LLMs) powered conversational search systems have\nalready been used by hundreds of millions of people, and are believed to bring\nmany benefits over conventional search. However, while decades of research and\npublic discourse interrogated the risk of search systems in increasing\nselective exposure and creating echo chambers -- limiting exposure to diverse\nopinions and leading to opinion polarization, little is known about such a risk\nof LLM-powered conversational search. We conduct two experiments to\ninvestigate: 1) whether and how LLM-powered conversational search increases\nselective exposure compared to conventional search; 2) whether and how LLMs\nwith opinion biases that either reinforce or challenge the user's view change\nthe effect. Overall, we found that participants engaged in more biased\ninformation querying with LLM-powered conversational search, and an opinionated\nLLM reinforcing their views exacerbated this bias. These results present\ncritical implications for the development of LLMs and conversational search\nsystems, and the policy governing these technologies.",
        "pdf_link": "https://arxiv.org/pdf/2402.05880v2.pdf"
    },
    {
        "title": "EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models",
        "authors": [
            "Guo Lin",
            "Wenyue Hua",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-08T17:57:11Z",
        "summary": "Cloud-based large language models (LLMs) such as ChatGPT have increasingly\nbecome integral to daily operations, serving as vital tools across various\napplications. While these models offer substantial benefits in terms of\naccessibility and functionality, they also introduce significant privacy\nconcerns: the transmission and storage of user data in cloud infrastructures\npose substantial risks of data breaches and unauthorized access to sensitive\ninformation; even if the transmission and storage of data is encrypted, the LLM\nservice provider itself still knows the real contents of the data, preventing\nindividuals or entities from confidently using such LLM services. To address\nthese concerns, this paper proposes a simple yet effective mechanism EmojiCrypt\nto protect user privacy. It uses Emoji to encrypt the user inputs before\nsending them to LLM, effectively rendering them indecipherable to human or\nLLM's examination while retaining the original intent of the prompt, thus\nensuring the model's performance remains unaffected. We conduct experiments on\nthree tasks, personalized recommendation, sentiment analysis, and tabular data\nanalysis. Experiment results reveal that EmojiCrypt can encrypt personal\ninformation within prompts in such a manner that not only prevents the\ndiscernment of sensitive data by humans or LLM itself, but also maintains or\neven improves the precision without further tuning, achieving comparable or\neven better task accuracy than directly prompting the LLM without prompt\nencryption. These results highlight the practicality of adopting encryption\nmeasures that safeguard user privacy without compromising the functional\nintegrity and performance of LLMs. Code and dataset are available at\nhttps://github.com/agiresearch/EmojiCrypt.",
        "pdf_link": "https://arxiv.org/pdf/2402.05868v2.pdf"
    },
    {
        "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
        "authors": [
            "Federico Bianchi",
            "Patrick John Chia",
            "Mert Yuksekgonul",
            "Jacopo Tagliabue",
            "Dan Jurafsky",
            "James Zou"
        ],
        "published": "2024-02-08T17:51:48Z",
        "summary": "Negotiation is the basis of social interactions; humans negotiate everything\nfrom the price of cars to how to share common resources. With rapidly growing\ninterest in using large language models (LLMs) to act as agents on behalf of\nhuman users, such LLM agents would also need to be able to negotiate. In this\npaper, we study how well LLMs can negotiate with each other. We develop\nNegotiationArena: a flexible framework for evaluating and probing the\nnegotiation abilities of LLM agents. We implemented three types of scenarios in\nNegotiationArena to assess LLM's behaviors in allocating shared resources\n(ultimatum games), aggregate resources (trading games) and buy/sell goods\n(price negotiations). Each scenario allows for multiple turns of flexible\ndialogues between LLM agents to allow for more complex negotiations.\nInterestingly, LLM agents can significantly boost their negotiation outcomes by\nemploying certain behavioral tactics. For example, by pretending to be desolate\nand desperate, LLMs can improve their payoffs by 20\\% when negotiating against\nthe standard GPT-4. We also quantify irrational negotiation behaviors exhibited\nby the LLM agents, many of which also appear in humans. Together,\n\\NegotiationArena offers a new environment to investigate LLM interactions,\nenabling new insights into LLM's theory of mind, irrationality, and reasoning\nabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.05863v1.pdf"
    },
    {
        "title": "Is it Possible to Edit Large Language Models Robustly?",
        "authors": [
            "Xinbei Ma",
            "Tianjie Ju",
            "Jiyang Qiu",
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Lifeng Liu",
            "Yulong Wang"
        ],
        "published": "2024-02-08T17:06:45Z",
        "summary": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI to imitate human behaviors but face the challenge of efficient\ncustomization. To tackle this challenge, recent studies have delved into the\nrealm of model editing, which manipulates specific memories of language models\nand changes the related language generation. However, the robustness of model\nediting remains an open question. This work seeks to understand the strengths\nand limitations of editing methods, thus facilitating robust, realistic\napplications of communicative AI. Concretely, we conduct extensive analysis to\naddress the three key research questions. Q1: Can edited LLMs behave\nconsistently resembling communicative AI in realistic situations? Q2: To what\nextent does the rephrasing of prompts lead LLMs to deviate from the edited\nknowledge memory? Q3: Which knowledge features are correlated with the\nperformance and robustness of editing? Our experimental results uncover a\nsubstantial disparity between existing editing methods and the practical\napplication of LLMs. On rephrased prompts that are complex and flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.05827v1.pdf"
    },
    {
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Boyang Hong",
            "Senjie Jin",
            "Rui Zheng",
            "Wei He",
            "Yiwen Ding",
            "Shichun Liu",
            "Xin Guo",
            "Junzhe Wang",
            "Honglin Guo",
            "Wei Shen",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Shihan Dou",
            "Xiao Wang",
            "Xinbo Zhang",
            "Peng Sun",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-08T16:46:26Z",
        "summary": "In this paper, we propose R$^3$: Learning Reasoning through Reverse\nCurriculum Reinforcement Learning (RL), a novel method that employs only\noutcome supervision to achieve the benefits of process supervision for large\nlanguage models. The core challenge in applying RL to complex reasoning is to\nidentify a sequence of actions that result in positive rewards and provide\nappropriate supervision for optimization. Outcome supervision provides sparse\nrewards for final results without identifying error locations, whereas process\nsupervision offers step-wise rewards but requires extensive manual annotation.\nR$^3$ overcomes these limitations by learning from correct demonstrations.\nSpecifically, R$^3$ progressively slides the start state of reasoning from a\ndemonstration's end to its beginning, facilitating easier model exploration at\nall stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome\nsupervision to offer step-level signals and precisely pinpoint errors. Using\nLlama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$\npoints on average. Notebaly, in program-based reasoning on GSM8K, it exceeds\nthe baseline by $4.2$ points across three backbone models, and without any\nextra data, Codellama-7B + R$^3$ performs comparable to larger models or\nclosed-source models.",
        "pdf_link": "https://arxiv.org/pdf/2402.05808v2.pdf"
    },
    {
        "title": "Limits of Transformer Language Models on Learning Algorithmic Compositions",
        "authors": [
            "Jonathan Thomm",
            "Aleksandar Terzic",
            "Geethan Karunaratne",
            "Giacomo Camposampiero",
            "Bernhard Sch\u00f6lkopf",
            "Abbas Rahimi"
        ],
        "published": "2024-02-08T16:23:29Z",
        "summary": "We analyze the capabilities of Transformer language models on learning\ndiscrete algorithms. To this end, we introduce two new tasks demanding the\ncomposition of several discrete sub-tasks. On both training LLaMA models from\nscratch and prompting on GPT-4 and Gemini we measure learning compositions of\nlearned primitives. We observe that the compositional capabilities of\nstate-of-the-art Transformer language models are very limited and sample-wise\nscale worse than relearning all sub-tasks for a new algorithmic composition. We\nalso present a theorem in complexity theory, showing that gradient descent on\nmemorizing feedforward models can be exponentially data inefficient.",
        "pdf_link": "https://arxiv.org/pdf/2402.05785v2.pdf"
    },
    {
        "title": "Text-to-Code Generation with Modality-relative Pre-training",
        "authors": [
            "Fenia Christopoulou",
            "Guchun Zhang",
            "Gerasimos Lampouras"
        ],
        "published": "2024-02-08T16:17:24Z",
        "summary": "Large pre-trained language models have recently been expanded and applied to\nprogramming language tasks with great success, often through further\npre-training of a strictly-natural language model--where training sequences\ntypically contain both natural and (linearised) programming language. Such\napproaches effectively map both modalities of the sequence into the same\nembedding space. However, programming language keywords (e.g. \"while\") often\nhave very strictly defined semantics. As such, transfer learning from their\nnatural language usage may not necessarily be beneficial to their code\napplication and vise versa. Assuming an already pre-trained language model, in\nthis work we investigate how sequence tokens can be adapted and represented\ndifferently, depending on which modality they belong to, and to the ultimate\nbenefit of the downstream task. We experiment with separating embedding spaces\nbetween modalities during further model pre-training with modality-relative\ntraining objectives. We focus on text-to-code generation and observe consistent\nimprovements across two backbone models and two test sets, measuring pass@$k$\nand a novel incremental variation.",
        "pdf_link": "https://arxiv.org/pdf/2402.05783v2.pdf"
    },
    {
        "title": "Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images",
        "authors": [
            "Kathleen C. Fraser",
            "Svetlana Kiritchenko"
        ],
        "published": "2024-02-08T16:11:23Z",
        "summary": "Following on recent advances in large language models (LLMs) and subsequent\nchat models, a new wave of large vision-language models (LVLMs) has emerged.\nSuch models can incorporate images as input in addition to text, and perform\ntasks such as visual question answering, image captioning, story generation,\netc. Here, we examine potential gender and racial biases in such systems, based\non the perceived characteristics of the people in the input images. To\naccomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday\nScenarios). The PAIRS dataset contains sets of AI-generated images of people,\nsuch that the images are highly similar in terms of background and visual\ncontent, but differ along the dimensions of gender (man, woman) and race\n(Black, white). By querying the LVLMs with such images, we observe significant\ndifferences in the responses according to the perceived gender or race of the\nperson depicted.",
        "pdf_link": "https://arxiv.org/pdf/2402.05779v1.pdf"
    },
    {
        "title": "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation",
        "authors": [
            "Yikai Zhang",
            "Siyu Yuan",
            "Caiyu Hu",
            "Kyle Richardson",
            "Yanghua Xiao",
            "Jiangjie Chen"
        ],
        "published": "2024-02-08T15:08:57Z",
        "summary": "Despite remarkable advancements in emulating human-like behavior through\nLarge Language Models (LLMs), current textual simulations do not adequately\naddress the notion of time. To this end, we introduce TimeArena, a novel\ntextual simulated environment that incorporates complex temporal dynamics and\nconstraints that better reflect real-life planning scenarios. In TimeArena,\nagents are asked to complete multiple tasks as soon as possible, allowing for\nparallel processing to save time. We implement the dependency between actions,\nthe time duration for each action, and the occupancy of the agent and the\nobjects in the environment. TimeArena grounds to 30 real-world tasks in\ncooking, household activities, and laboratory work. We conduct extensive\nexperiments with various state-of-the-art LLMs using TimeArena. Our findings\nreveal that even the most powerful models, e.g., GPT-4, still lag behind humans\nin effective multitasking, underscoring the need for enhanced temporal\nawareness in the development of language agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.05733v1.pdf"
    },
    {
        "title": "In-Context Learning Can Re-learn Forbidden Tasks",
        "authors": [
            "Sophie Xhonneux",
            "David Dobre",
            "Jian Tang",
            "Gauthier Gidel",
            "Dhanya Sridhar"
        ],
        "published": "2024-02-08T14:54:17Z",
        "summary": "Despite significant investment into safety training, large language models\n(LLMs) deployed in the real world still suffer from numerous vulnerabilities.\nOne perspective on LLM safety training is that it algorithmically forbids the\nmodel from answering toxic or harmful queries. To assess the effectiveness of\nsafety training, in this work, we study forbidden tasks, i.e., tasks the model\nis designed to refuse to answer. Specifically, we investigate whether\nin-context learning (ICL) can be used to re-learn forbidden tasks despite the\nexplicit fine-tuning of the model to refuse them. We first examine a toy\nexample of refusing sentiment classification to demonstrate the problem. Then,\nwe use ICL on a model fine-tuned to refuse to summarise made-up news articles.\nFinally, we investigate whether ICL can undo safety training, which could\nrepresent a major security risk. For the safety task, we look at Vicuna-7B,\nStarling-7B, and Llama2-7B. We show that the attack works out-of-the-box on\nStarling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL\nattack that uses the chat template tokens like a prompt injection attack to\nachieve a better attack success rate on Vicuna-7B and Starling-7B.\n  Trigger Warning: the appendix contains LLM-generated text with violence,\nsuicide, and misinformation.",
        "pdf_link": "https://arxiv.org/pdf/2402.05723v1.pdf"
    },
    {
        "title": "Unified Speech-Text Pretraining for Spoken Dialog Modeling",
        "authors": [
            "Heeseung Kim",
            "Soonshin Seo",
            "Kyeongseok Jeong",
            "Ohsung Kwon",
            "Jungwhan Kim",
            "Jaehong Lee",
            "Eunwoo Song",
            "Myungwoo Oh",
            "Sungroh Yoon",
            "Kang Min Yoo"
        ],
        "published": "2024-02-08T14:35:09Z",
        "summary": "While recent work shows promising results in expanding the capabilities of\nlarge language models (LLM) to directly understand and synthesize speech, an\nLLM-based strategy for modeling spoken dialogs remains elusive and calls for\nfurther investigation. This work proposes an extensive speech-text LLM\nframework, named the Unified Spoken Dialog Model (USDM), to generate coherent\nspoken responses with organic prosodic features relevant to the given input\nspeech without relying on automatic speech recognition (ASR) or text-to-speech\n(TTS) solutions. Our approach employs a multi-step speech-text inference scheme\nthat leverages chain-of-reasoning capabilities exhibited by the underlying LLM.\nWe also propose a generalized speech-text pretraining scheme that helps with\ncapturing cross-modal semantics. Automatic and human evaluations show that the\nproposed approach is effective in generating natural-sounding spoken responses,\noutperforming both prior and cascaded baselines. Detailed comparative studies\nreveal that, despite the cascaded approach being stronger in individual\ncomponents, the joint speech-text modeling improves robustness against\nrecognition errors and speech quality. Demo is available at\nhttps://unifiedsdm.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.05706v1.pdf"
    },
    {
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
        "authors": [
            "Xianghe Pang",
            "Shuo Tang",
            "Rui Ye",
            "Yuxin Xiong",
            "Bolun Zhang",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "published": "2024-02-08T14:21:03Z",
        "summary": "Aligning large language models (LLMs) with human values is imperative to\nmitigate potential adverse effects resulting from their misuse. Drawing from\nthe sociological insight that acknowledging all parties' concerns is a key\nfactor in shaping human values, this paper proposes a novel direction to align\nLLMs by themselves: social scene simulation. To achieve this, we present\nMATRIX, a novel social scene simulator that emulates realistic scenes around a\nuser's input query, enabling the LLM to take social consequences into account\nbefore responding. MATRIX serves as a virtual rehearsal space, akin to a\nMonopolylogue, where the LLM performs diverse roles related to the query and\npractice by itself. To inject this alignment, we fine-tune the LLM with\nMATRIX-simulated data, ensuring adherence to human values without compromising\ninference speed. We theoretically show that the LLM with MATRIX outperforms\nConstitutional AI under mild assumptions. Finally, extensive experiments\nvalidate that our method outperforms over 10 baselines across 4 benchmarks. As\nevidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning\nwith human values. Our project page is available at\nhttps://shuotang123.github.io/MATRIX.",
        "pdf_link": "https://arxiv.org/pdf/2402.05699v2.pdf"
    },
    {
        "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
        "authors": [
            "Junjie Chu",
            "Yugeng Liu",
            "Ziqing Yang",
            "Xinyue Shen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2024-02-08T13:42:50Z",
        "summary": "Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.",
        "pdf_link": "https://arxiv.org/pdf/2402.05668v1.pdf"
    },
    {
        "title": "Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",
        "authors": [
            "Wei Wang",
            "Huilong Ning",
            "Gaowei Zhang",
            "Libo Liu",
            "Yi Wang"
        ],
        "published": "2024-02-08T13:07:31Z",
        "summary": "Recently, large language models (LLM) based generative AI has been gaining\nmomentum for their impressive high-quality performances in multiple domains,\nparticularly after the release of the ChatGPT. Many believe that they have the\npotential to perform general-purpose problem-solving in software development\nand replace human software developers. Nevertheless, there are in a lack of\nserious investigation into the capability of these LLM techniques in fulfilling\nsoftware development tasks. In a controlled 2 x 2 between-subject experiment\nwith 109 participants, we examined whether and to what degree working with\nChatGPT was helpful in the coding task and typical software development task\nand how people work with ChatGPT. We found that while ChatGPT performed well in\nsolving simple coding problems, its performance in supporting typical software\ndevelopment tasks was not that good. We also observed the interactions between\nparticipants and ChatGPT and found the relations between the interactions and\nthe outcomes. Our study thus provides first-hand insights into using ChatGPT to\nfulfill software engineering tasks with real-world developers and motivates the\nneed for novel interaction mechanisms that help developers effectively work\nwith large language models to achieve desired outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2402.05650v3.pdf"
    },
    {
        "title": "Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design",
        "authors": [
            "Nayoung Kim",
            "Minsu Kim",
            "Jinkyoo Park"
        ],
        "published": "2024-02-08T13:02:05Z",
        "summary": "Antibody design plays a pivotal role in advancing therapeutics. Although deep\nlearning has made rapid progress in this field, existing methods make limited\nuse of general protein knowledge and assume a graphical model (GM) that\nviolates empirical findings on proteins. To address these limitations, we\npresent Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained\nprotein language model (pLM) and encodes a seminal finding on proteins called\nAnfinsen's dogma. Our framework follows a two-step process of sequence\ngeneration with pLM and structure prediction with graph neural network (GNN).\nExperiments show that our approach outperforms state-of-the-art results on\nbenchmark experiments. We also address a critical limitation of\nnon-autoregressive models -- namely, that they tend to generate unrealistic\nsequences with overly repeating tokens. To resolve this, we introduce a\ncomposition-based regularization term to the cross-entropy objective that\nallows an efficient trade-off between high performance and low token\nrepetition. We demonstrate that our approach establishes a Pareto frontier over\nthe current state-of-the-art. Our code is available at\nhttps://github.com/lkny123/AGN.",
        "pdf_link": "https://arxiv.org/pdf/2402.05982v1.pdf"
    },
    {
        "title": "The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment",
        "authors": [
            "Sayan Chatterjee",
            "Ching Louis Liu",
            "Gareth Rowland",
            "Tim Hogarth"
        ],
        "published": "2024-02-08T12:47:57Z",
        "summary": "The increasing popularity of AI, particularly Large Language Models (LLMs),\nhas significantly impacted various domains, including Software Engineering.\nThis study explores the integration of AI tools in software engineering\npractices within a large organization. We focus on ANZ Bank, which employs over\n5000 engineers covering all aspects of the software development life cycle.\nThis paper details an experiment conducted using GitHub Copilot, a notable AI\ntool, within a controlled environment to evaluate its effectiveness in\nreal-world engineering tasks. Additionally, this paper shares initial findings\non the productivity improvements observed after GitHub Copilot was adopted on a\nlarge scale, with about 1000 engineers using it. ANZ Bank's six-week experiment\nwith GitHub Copilot included two weeks of preparation and four weeks of active\ntesting. The study evaluated participant sentiment and the tool's impact on\nproductivity, code quality, and security. Initially, participants used GitHub\nCopilot for proposed use-cases, with their feedback gathered through regular\nsurveys. In the second phase, they were divided into Control and Copilot\ngroups, each tackling the same Python challenges, and their experiences were\nagain surveyed. Results showed a notable boost in productivity and code quality\nwith GitHub Copilot, though its impact on code security remained inconclusive.\nParticipant responses were overall positive, confirming GitHub Copilot's\neffectiveness in large-scale software engineering environments. Early data from\n1000 engineers also indicated a significant increase in productivity and job\nsatisfaction.",
        "pdf_link": "https://arxiv.org/pdf/2402.05636v1.pdf"
    },
    {
        "title": "Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2024-02-08T12:36:29Z",
        "summary": "Long-form generations from large language models (LLMs) contains a mix of\nfactual and non-factual claims, making evaluating factuality difficult. To\nevaluate factual precision of long-form generations in a more fine-grained way,\nprior works propose to decompose long-form generations into multiple verifiable\nfacts and verify those facts independently. The factuality of the generation is\nthe proportion of verifiable facts among all the facts. Such methods assume\nthat combining factual claims forms a factual paragraph. This paper shows that\nthe assumption can be violated due to entity ambiguity. We show that LLMs can\ngenerate paragraphs that contain verifiable facts, but the facts are combined\nto form a non-factual paragraph due to entity ambiguity. We further reveal that\nexisting factual precision metrics, including FActScore and citation recall,\ncannot properly evaluate the factuality of these non-factual paragraphs. To\naddress this, we introduce an enhanced metric, D-FActScore, specifically\ndesigned for content with ambiguous entities. We evaluate the D-FActScores of\npeople biographies generated with retrieval-augmented generation (RAG). We show\nthat D-FActScore can better assess the factuality of paragraphs with entity\nambiguity than FActScore. We also find that four widely used open-source LLMs\ntend to mix information of distinct entities to form non-factual paragraphs.",
        "pdf_link": "https://arxiv.org/pdf/2402.05629v2.pdf"
    },
    {
        "title": "Efficient Models for the Detection of Hate, Abuse and Profanity",
        "authors": [
            "Christoph Tillmann",
            "Aashka Trivedi",
            "Bishwaranjan Bhattacharjee"
        ],
        "published": "2024-02-08T12:28:18Z",
        "summary": "Large Language Models (LLMs) are the cornerstone for many Natural Language\nProcessing (NLP) tasks like sentiment analysis, document classification, named\nentity recognition, question answering, summarization, etc. LLMs are often\ntrained on data which originates from the web. This data is prone to having\ncontent with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,\nplease refer to the Appendix. Due to the LLMs being exposed to HAP content\nduring training, the models learn it and may then generate hateful or profane\ncontent. For example, when the open-source RoBERTa model (specifically, the\nRoBERTA base model) from the HuggingFace (HF) Transformers library is prompted\nto replace the mask token in `I do not know that Persian people are that MASK`\nit returns the word `stupid` with the highest score. This is unacceptable in\ncivil discourse.The detection of Hate, Abuse and Profanity in text is a vital\ncomponent of creating civil and unbiased LLMs, which is needed not only for\nEnglish, but for all languages. In this article, we briefly describe the\ncreation of HAP detectors and various ways of using them to make models civil\nand acceptable in the output they generate.",
        "pdf_link": "https://arxiv.org/pdf/2402.05624v1.pdf"
    },
    {
        "title": "AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers",
        "authors": [
            "Reduan Achtibat",
            "Sayed Mohammad Vakilzadeh Hatefi",
            "Maximilian Dreyer",
            "Aakriti Jain",
            "Thomas Wiegand",
            "Sebastian Lapuschkin",
            "Wojciech Samek"
        ],
        "published": "2024-02-08T12:01:24Z",
        "summary": "Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a singular backward pass. Through extensive evaluations against existing\nmethods on Llama 2, Flan-T5 and the Vision Transformer architecture, we\ndemonstrate that our proposed approach surpasses alternative methods in terms\nof faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an open-source\nimplementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.05602v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset",
        "authors": [
            "Hengguan Huang",
            "Songtao Wang",
            "Hongfu Liu",
            "Hao Wang",
            "Ye Wang"
        ],
        "published": "2024-02-08T10:32:06Z",
        "summary": "Traditional applications of natural language processing (NLP) in healthcare\nhave predominantly focused on patient-centered services, enhancing patient\ninteractions and care delivery, such as through medical dialogue systems.\nHowever, the potential of NLP to benefit inexperienced doctors, particularly in\nareas such as communicative medical coaching, remains largely unexplored. We\nintroduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within\nthis framework, both a patient agent and a coaching agent collaboratively\nsupport medical learners in practicing their medical communication skills\nduring consultations. Unlike traditional dialogue systems, ChatCoach provides a\nsimulated environment where a human doctor can engage in medical dialogue with\na patient agent. Simultaneously, a coaching agent provides real-time feedback\nto the doctor. To construct the ChatCoach system, we developed a dataset and\nintegrated Large Language Models such as ChatGPT and Llama2, aiming to assess\ntheir effectiveness in communicative medical coaching tasks. Our comparative\nanalysis demonstrates that instruction-tuned Llama2 significantly outperforms\nChatGPT's prompting-based approaches.",
        "pdf_link": "https://arxiv.org/pdf/2402.05547v1.pdf"
    },
    {
        "title": "Can ChatGPT evaluate research quality?",
        "authors": [
            "Mike Thelwall"
        ],
        "published": "2024-02-08T10:00:40Z",
        "summary": "Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research\nevaluations on journal articles to automate this time-consuming task.\nDesign/methodology/approach: Test the extent to which ChatGPT-4 can assess the\nquality of journal articles using a case study of the published scoring\nguidelines of the UK Research Excellence Framework (REF) 2021 to create a\nresearch evaluation ChatGPT. This was applied to 51 of my own articles and\ncompared against my own quality judgements. Findings: ChatGPT-4 can produce\nplausible document summaries and quality evaluation rationales that match the\nREF criteria. Its overall scores have weak correlations with my self-evaluation\nscores of the same documents (averaging r=0.281 over 15 iterations, with 8\nbeing statistically significantly different from 0). In contrast, the average\nscores from the 15 iterations produced a statistically significant positive\ncorrelation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds\nseems more effective than individual scores. The positive correlation may be\ndue to ChatGPT being able to extract the author's significance, rigour, and\noriginality claims from inside each paper. If my weakest articles are removed,\nthen the correlation with average scores (r=0.200) falls below statistical\nsignificance, suggesting that ChatGPT struggles to make fine-grained\nevaluations. Research limitations: The data is self-evaluations of a\nconvenience sample of articles from one academic in one field. Practical\nimplications: Overall, ChatGPT does not yet seem to be accurate enough to be\ntrusted for any formal or informal research quality evaluation tasks. Research\nevaluators, including journal editors, should therefore take steps to control\nits use. Originality/value: This is the first published attempt at\npost-publication expert review accuracy testing for ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.05519v1.pdf"
    },
    {
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "authors": [
            "Roy Ganz",
            "Yair Kittenplon",
            "Aviad Aberdam",
            "Elad Ben Avraham",
            "Oren Nuriel",
            "Shai Mazor",
            "Ron Litman"
        ],
        "published": "2024-02-08T08:03:39Z",
        "summary": "Vision-Language (VL) models have gained significant research focus, enabling\nremarkable advances in multimodal reasoning. These architectures typically\ncomprise a vision encoder, a Large Language Model (LLM), and a projection\nmodule that aligns visual features with the LLM's representation space. Despite\ntheir success, a critical limitation persists: the vision encoding process\nremains decoupled from user queries, often in the form of image-related\nquestions. Consequently, the resulting visual features may not be optimally\nattuned to the query-specific elements of the image. To address this, we\nintroduce QA-ViT, a Question Aware Vision Transformer approach for multimodal\nreasoning, which embeds question awareness directly within the vision encoder.\nThis integration results in dynamic visual features focusing on relevant image\naspects to the posed question. QA-ViT is model-agnostic and can be incorporated\nefficiently into any VL architecture. Extensive experiments demonstrate the\neffectiveness of applying our method to various multimodal architectures,\nleading to consistent improvement across diverse tasks and showcasing its\npotential for enhancing visual and scene-text understanding.",
        "pdf_link": "https://arxiv.org/pdf/2402.05472v1.pdf"
    },
    {
        "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia",
        "authors": [
            "Guangyu Shen",
            "Siyuan Cheng",
            "Kaiyuan Zhang",
            "Guanhong Tao",
            "Shengwei An",
            "Lu Yan",
            "Zhuo Zhang",
            "Shiqing Ma",
            "Xiangyu Zhang"
        ],
        "published": "2024-02-08T07:56:49Z",
        "summary": "Large Language Models (LLMs) have become prevalent across diverse sectors,\ntransforming human life with their extraordinary reasoning and comprehension\nabilities. As they find increased use in sensitive tasks, safety concerns have\ngained widespread attention. Extensive efforts have been dedicated to aligning\nLLMs with human moral principles to ensure their safe deployment. Despite their\npotential, recent research indicates aligned LLMs are prone to specialized\njailbreaking prompts that bypass safety measures to elicit violent and harmful\ncontent. The intrinsic discrete nature and substantial scale of contemporary\nLLMs pose significant challenges in automatically generating diverse,\nefficient, and potent jailbreaking prompts, representing a continuous obstacle.\nIn this paper, we introduce RIPPLE (Rapid Optimization via Subconscious\nExploitation and Echopraxia), a novel optimization-based method inspired by two\npsychological concepts: subconsciousness and echopraxia, which describe the\nprocesses of the mind that occur without conscious awareness and the\ninvoluntary mimicry of actions, respectively. Evaluations across 6 open-source\nLLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success\nRate of 91.5\\%, outperforming five current methods by up to 47.0\\% with an 8x\nreduction in overhead. Furthermore, it displays significant transferability and\nstealth, successfully evading established detection mechanisms. The code of our\nwork is available at\n\\url{https://github.com/SolidShen/RIPPLE_official/tree/official}",
        "pdf_link": "https://arxiv.org/pdf/2402.05467v1.pdf"
    },
    {
        "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
        "authors": [
            "Chen Chen",
            "Ruizhe Li",
            "Yuchen Hu",
            "Sabato Marco Siniscalchi",
            "Pin-Yu Chen",
            "Ensiong Chng",
            "Chao-Han Huck Yang"
        ],
        "published": "2024-02-08T07:21:45Z",
        "summary": "Recent studies have successfully shown that large language models (LLMs) can\nbe successfully used for generative error correction (GER) on top of the\nautomatic speech recognition (ASR) output. Specifically, an LLM is utilized to\ncarry out a direct mapping from the N-best hypotheses list generated by an ASR\nsystem to the predicted output transcription. However, despite its\neffectiveness, GER introduces extra data uncertainty since the LLM is trained\nwithout taking into account acoustic information available in the speech\nsignal. In this work, we aim to overcome such a limitation by infusing acoustic\ninformation before generating the predicted transcription through a novel late\nfusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a\nmultimodal fusion approach implemented into an auto-regressive decoding process\nand works in two stages: (i) It first analyzes and calibrates the token-level\nLLM decision, and (ii) it then dynamically assimilates the information from the\nacoustic modality. Experimental evidence collected from various ASR tasks shows\nthat UADF surpasses existing fusion mechanisms in several ways. It yields\nsignificant improvements in word error rate (WER) while mitigating data\nuncertainty issues in LLM and addressing the poor generalization relied with\nsole modality during fusion. We also demonstrate that UADF seamlessly adapts to\naudio-visual speech recognition.",
        "pdf_link": "https://arxiv.org/pdf/2402.05457v1.pdf"
    },
    {
        "title": "Large Language Models for Psycholinguistic Plausibility Pretesting",
        "authors": [
            "Samuel Joseph Amouyal",
            "Aya Meltzer-Asscher",
            "Jonathan Berant"
        ],
        "published": "2024-02-08T07:20:02Z",
        "summary": "In psycholinguistics, the creation of controlled materials is crucial to\nensure that research outcomes are solely attributed to the intended\nmanipulations and not influenced by extraneous factors. To achieve this,\npsycholinguists typically pretest linguistic materials, where a common pretest\nis to solicit plausibility judgments from human evaluators on specific\nsentences. In this work, we investigate whether Language Models (LMs) can be\nused to generate these plausibility judgements. We investigate a wide range of\nLMs across multiple linguistic structures and evaluate whether their\nplausibility judgements correlate with human judgements. We find that GPT-4\nplausibility judgements highly correlate with human judgements across the\nstructures we examine, whereas other LMs correlate well with humans on commonly\nused syntactic structures. We then test whether this correlation implies that\nLMs can be used instead of humans for pretesting. We find that when\ncoarse-grained plausibility judgements are needed, this works well, but when\nfine-grained judgements are necessary, even GPT-4 does not provide satisfactory\ndiscriminative power.",
        "pdf_link": "https://arxiv.org/pdf/2402.05455v1.pdf"
    },
    {
        "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
        "authors": [
            "Haotong Qin",
            "Xudong Ma",
            "Xingyu Zheng",
            "Xiaoyang Li",
            "Yang Zhang",
            "Shouda Liu",
            "Jie Luo",
            "Xianglong Liu",
            "Michele Magno"
        ],
        "published": "2024-02-08T06:53:31Z",
        "summary": "The LoRA-finetuning quantization of LLMs has been extensively studied to\nobtain accurate yet compact LLMs for deployment on resource-constrained\nhardware. However, existing methods cause the quantized LLM to severely degrade\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\nthrough information retention. The proposed IR-QLoRA mainly relies on two\ntechnologies derived from the perspective of unified information: (1)\nstatistics-based Information Calibration Quantization allows the quantized\nparameters of LLM to retain original information accurately; (2)\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\nrepresentation transformation with diverse information. Comprehensive\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\nimprovement on MMLU compared with the state-of-the-art methods. The significant\nperformance gain requires only a tiny 0.31% additional time consumption,\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\nThe code is available at https://github.com/htqin/ir-qlora.",
        "pdf_link": "https://arxiv.org/pdf/2402.05445v1.pdf"
    },
    {
        "title": "Do Large Code Models Understand Programming Concepts? A Black-box Approach",
        "authors": [
            "Ashish Hooda",
            "Mihai Christodorescu",
            "Miltiadis Allamanis",
            "Aaron Wilson",
            "Kassem Fawaz",
            "Somesh Jha"
        ],
        "published": "2024-02-08T06:48:01Z",
        "summary": "Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.",
        "pdf_link": "https://arxiv.org/pdf/2402.05980v2.pdf"
    },
    {
        "title": "GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study",
        "authors": [
            "Christopher J. Lynch",
            "Erik Jensen",
            "Madison H. Munro",
            "Virginia Zamponi",
            "Joseph Martinez",
            "Kevin O'Brien",
            "Brandon Feldhaus",
            "Katherine Smith",
            "Ann Marie Reinhold",
            "Ross Gore"
        ],
        "published": "2024-02-08T06:20:01Z",
        "summary": "Large Language Models (LLMs) play a pivotal role in generating vast arrays of\nnarratives, facilitating a systematic exploration of their effectiveness for\ncommunicating life events in narrative form. In this study, we employ a\nzero-shot structured narrative prompt to generate 24,000 narratives using\nOpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and\nevaluate their validity in conveying birth, death, hiring, and firing events.\nRemarkably, 87.43% of the narratives sufficiently convey the intention of the\nstructured prompt. To automate the identification of valid and invalid\nnarratives, we train and validate nine Machine Learning models on the\nclassified datasets. Leveraging these models, we extend our analysis to predict\nthe classifications of the remaining 21,120 narratives. All the ML models\nexcelled at classifying valid narratives as valid, but experienced challenges\nat simultaneously classifying invalid narratives as invalid. Our findings not\nonly advance the study of LLM capabilities, limitations, and validity but also\noffer practical insights for narrative generation and natural language\nprocessing applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.05435v1.pdf"
    },
    {
        "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
        "authors": [
            "Lucio Dery",
            "Steven Kolawole",
            "Jean-Fran\u00e7ois Kagy",
            "Virginia Smith",
            "Graham Neubig",
            "Ameet Talwalkar"
        ],
        "published": "2024-02-08T04:48:26Z",
        "summary": "Given the generational gap in available hardware between lay practitioners\nand the most endowed institutions, LLMs are becoming increasingly inaccessible\nas they grow in size. Whilst many approaches have been proposed to compress\nLLMs to make their resource consumption manageable, these methods themselves\ntend to be resource intensive, putting them out of the reach of the very user\ngroups they target. In this work, we explore the problem of structured pruning\nof LLMs using only forward passes. We seek to empower practitioners to prune\nmodels so large that their available hardware has just enough memory to run\ninference. We develop Bonsai, a gradient-free, perturbative pruning method\ncapable of delivering small, fast, and accurate pruned models.\n  We observe that Bonsai outputs pruned models that (i) outperform those\ngenerated by more expensive gradient-based structured pruning methods, and (ii)\nare twice as fast (with comparable accuracy) as those generated by\nsemi-structured pruning methods requiring comparable resources as Bonsai. We\nalso leverage Bonsai to produce a new sub-2B model using a single A6000 that\nyields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM\nleaderboard.",
        "pdf_link": "https://arxiv.org/pdf/2402.05406v2.pdf"
    },
    {
        "title": "In-Context Principle Learning from Mistakes",
        "authors": [
            "Tianjun Zhang",
            "Aman Madaan",
            "Luyu Gao",
            "Steven Zheng",
            "Swaroop Mishra",
            "Yiming Yang",
            "Niket Tandon",
            "Uri Alon"
        ],
        "published": "2024-02-08T04:42:29Z",
        "summary": "In-context learning (ICL, also known as few-shot prompting) has been the\nstandard method of adapting LLMs to downstream tasks, by learning from a few\ninput-output examples. Nonetheless, all ICL-based approaches only learn from\ncorrect input-output pairs. In this paper, we revisit this paradigm, by\nlearning more from the few given input-output examples. We introduce Learning\nPrinciples (LEAP): First, we intentionally induce the model to make mistakes on\nthese few examples; then we reflect on these mistakes, and learn explicit\ntask-specific \"principles\" from them, which help solve similar problems and\navoid common mistakes; finally, we prompt the model to answer unseen test\nquestions using the original few-shot examples and these learned general\nprinciples. We evaluate LEAP on a wide range of benchmarks, including multi-hop\nquestion answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,\nand math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the\nstrongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and\nClaude-2.1. For example, LEAP improves over the standard few-shot prompting\nusing GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does\nnot require any more input or examples than the standard few-shot prompting\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2402.05403v2.pdf"
    },
    {
        "title": "Enhancing Zero-shot Counting via Language-guided Exemplar Learning",
        "authors": [
            "Mingjie Wang",
            "Jun Zhou",
            "Yong Dai",
            "Eric Buys",
            "Minglun Gong"
        ],
        "published": "2024-02-08T04:07:38Z",
        "summary": "Recently, Class-Agnostic Counting (CAC) problem has garnered increasing\nattention owing to its intriguing generality and superior efficiency compared\nto Category-Specific Counting (CSC). This paper proposes a novel ExpressCount\nto enhance zero-shot object counting by delving deeply into language-guided\nexemplar learning. Specifically, the ExpressCount is comprised of an innovative\nLanguage-oriented Exemplar Perceptron and a downstream visual Zero-shot\nCounting pipeline. Thereinto, the perceptron hammers at exploiting accurate\nexemplar cues from collaborative language-vision signals by inheriting rich\nsemantic priors from the prevailing pre-trained Large Language Models (LLMs),\nwhereas the counting pipeline excels in mining fine-grained features through\ndual-branch and cross-attention schemes, contributing to the high-quality\nsimilarity learning. Apart from building a bridge between the LLM in vogue and\nthe visual counting tasks, expression-guided exemplar estimation significantly\nadvances zero-shot learning capabilities for counting instances with arbitrary\nclasses. Moreover, devising a FSC-147-Express with annotations of meticulous\nlinguistic expressions pioneers a new venue for developing and validating\nlanguage-based counting models. Extensive experiments demonstrate the\nstate-of-the-art performance of our ExpressCount, even showcasing the accuracy\non par with partial CSC models.",
        "pdf_link": "https://arxiv.org/pdf/2402.05394v1.pdf"
    },
    {
        "title": "CIC: A framework for Culturally-aware Image Captioning",
        "authors": [
            "Youngsik Yun",
            "Jihie Kim"
        ],
        "published": "2024-02-08T03:12:25Z",
        "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2402.05374v1.pdf"
    },
    {
        "title": "Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception",
        "authors": [
            "Yizhou Zhang",
            "Lun Du",
            "Defu Cao",
            "Qiang Fu",
            "Yan Liu"
        ],
        "published": "2024-02-08T02:37:30Z",
        "summary": "Foundation models, such as Large language Models (LLMs), have attracted\nsignificant amount of interest due to their large number of applications.\nExisting works show that appropriate prompt design, such as Chain-of-Thoughts,\ncan unlock LLM's powerful capacity in diverse areas. However, when handling\ntasks involving repetitive sub-tasks and/or deceptive contents, such as\narithmetic calculation and article-level fake news detection, existing\nprompting strategies either suffers from insufficient expressive power or\nintermediate errors triggered by hallucination. To make LLM more discerning to\nsuch intermediate errors, we propose to guide LLM with a Divide-and-Conquer\nprogram that simultaneously ensures superior expressive power and disentangles\ntask decomposition, sub-task resolution, and resolution assembly process.\nTheoretic analysis reveals that our strategy can guide LLM to extend the\nexpressive power of fixed-depth Transformer. Experiments indicate that our\nproposed method can achieve better performance than typical prompting\nstrategies in tasks bothered by intermediate errors and deceptive contents,\nsuch as large integer multiplication, hallucination detection and\nmisinformation detection.",
        "pdf_link": "https://arxiv.org/pdf/2402.05359v4.pdf"
    },
    {
        "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
        "authors": [
            "Wei Qiao",
            "Tushar Dogra",
            "Otilia Stretcu",
            "Yu-Han Lyu",
            "Tiantian Fang",
            "Dongjin Kwon",
            "Chun-Ta Lu",
            "Enming Luo",
            "Yuan Wang",
            "Chih-Chun Chia",
            "Ariel Fuxman",
            "Fangzhou Wang",
            "Ranjay Krishna",
            "Mehmet Tek"
        ],
        "published": "2024-02-07T23:47:02Z",
        "summary": "Large language models (LLMs) are powerful tools for content moderation, but\ntheir inference costs and latency make them prohibitive for casual use on large\ndatasets, such as the Google Ads repository. This study proposes a method for\nscaling up LLM reviews for content moderation in Google Ads. First, we use\nheuristics to select candidates via filtering and duplicate removal, and create\nclusters of ads for which we select one representative ad per cluster. We then\nuse LLMs to review only the representative ads. Finally, we propagate the LLM\ndecisions for the representative ads back to their clusters. This method\nreduces the number of reviews by more than 3 orders of magnitude while\nachieving a 2x recall compared to a baseline non-LLM model. The success of this\napproach is a strong function of the representations used in clustering and\nlabel propagation; we found that cross-modal similarity representations yield\nbetter results than uni-modal representations.",
        "pdf_link": "https://arxiv.org/pdf/2402.14590v1.pdf"
    },
    {
        "title": "Using text embedding models and vector databases as text classifiers with the example of medical data",
        "authors": [
            "Rishabh Goel"
        ],
        "published": "2024-02-07T22:15:15Z",
        "summary": "The advent of Large Language Models (LLMs) is promising and has found\napplication in numerous fields, but as it often is with the medical field, the\nbar is typically quite high [5]. In tandem with LLMs, vector embedding models\nand vector databases provide a robust way of expressing numerous modes of data\nthat are easily digestible by typical machine learning models. Along with the\nease of adding information, knowledge, and data to these vector databases, they\nprovide a compelling reason to apply them in numerous fields where the task of\nretrieving information is typically done by humans. Researchers at Google have\ndeveloped a clear alternative model, Med-PaLM [6] specifically designed to\nmatch a clinician's level of accuracy when it comes to medical knowledge. When\ntraining classifiers, and developing models, it is imperative to maintain\nfactuality and reduce bias [4]. Here, we explore the use of vector databases\nand embedding models as a means of encoding, and classifying text with the\nexample and application in the field of medicine. We show the robustness of\nthese tools depends heavily on the sparsity of the data presented, and even\nwith low amounts of data in the vector database itself, the vector database\ndoes a good job at classifying data [9]. Using various LLMs to generate the\nmedical data, we also understand the limitations of the medical knowledge of\nthese models and encourage further expert medical review of our testing data.\nBy using vector databases to classify a clinician's notes on a patient\npresented with a certain ailment, we understand the limitations of such\nmethods, but also the promise of their prospective use and with continued\ntesting and experimentation, hope to explore a unique use case of vector\ndatabases and embedding models.",
        "pdf_link": "https://arxiv.org/pdf/2402.16886v1.pdf"
    },
    {
        "title": "Are LLMs Ready for Real-World Materials Discovery?",
        "authors": [
            "Santiago Miret",
            "N M Anoop Krishnan"
        ],
        "published": "2024-02-07T19:10:36Z",
        "summary": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.",
        "pdf_link": "https://arxiv.org/pdf/2402.05200v1.pdf"
    },
    {
        "title": "$\u03bb$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space",
        "authors": [
            "Maitreya Patel",
            "Sangmin Jung",
            "Chitta Baral",
            "Yezhou Yang"
        ],
        "published": "2024-02-07T19:07:10Z",
        "summary": "Despite the recent advances in personalized text-to-image (P-T2I) generative\nmodels, it remains challenging to perform finetuning-free multi-subject-driven\nT2I in a resource-efficient manner. Predominantly, contemporary approaches,\ninvolving the training of Hypernetworks and Multimodal Large Language Models\n(MLLMs), require heavy computing resources that range from 600 to 12300 GPU\nhours of training. These subject-driven T2I methods hinge on Latent Diffusion\nModels (LDMs), which facilitate T2I mapping through cross-attention layers.\nWhile LDMs offer distinct advantages, P-T2I methods' reliance on the latent\nspace of these diffusion models significantly escalates resource demands,\nleading to inconsistent results and necessitating numerous iterations for a\nsingle desired image. In this paper, we present $\\lambda$-ECLIPSE, an\nalternative prior-training strategy that works in the latent space of a\npre-trained CLIP model without relying on the diffusion UNet models.\n$\\lambda$-ECLIPSE leverages the image-text interleaved pre-training for fast\nand effective multi-subject-driven P-T2I. Through extensive experiments, we\nestablish that $\\lambda$-ECLIPSE surpasses existing baselines in composition\nalignment while preserving concept alignment performance, even with\nsignificantly lower resource utilization. $\\lambda$-ECLIPSE performs\nmulti-subject driven P-T2I with just 34M parameters and is trained on a mere 74\nGPU hours. Additionally, $\\lambda$-ECLIPSE demonstrates the unique ability to\nperform multi-concept interpolations.",
        "pdf_link": "https://arxiv.org/pdf/2402.05195v2.pdf"
    },
    {
        "title": "InCoRo: In-Context Learning for Robotics Control with Feedback Loops",
        "authors": [
            "Jiaqiang Ye Zhu",
            "Carla Gomez Cano",
            "David Vazquez Bermudez",
            "Michal Drozdzal"
        ],
        "published": "2024-02-07T19:01:11Z",
        "summary": "One of the challenges in robotics is to enable robotic units with the\nreasoning capability that would be robust enough to execute complex tasks in\ndynamic environments. Recent advances in LLMs have positioned them as go-to\ntools for simple reasoning tasks, motivating the pioneering work of Liang et\nal. [35] that uses an LLM to translate natural language commands into low-level\nstatic execution plans for robotic units. Using LLMs inside robotics systems\nbrings their generalization to a new level, enabling zero-shot generalization\nto new tasks. This paper extends this prior work to dynamic environments. We\npropose InCoRo, a system that uses a classical robotic feedback loop composed\nof an LLM controller, a scene understanding unit, and a robot. Our system\ncontinuously analyzes the state of the environment and provides adapted\nexecution commands, enabling the robot to adjust to changing environmental\nconditions and correcting for controller errors. Our system does not require\nany iterative optimization to learn to accomplish a task as it leverages\nin-context learning with an off-the-shelf LLM model. Through an extensive\nvalidation process involving two standardized industrial robotic units -- SCARA\nand DELTA types -- we contribute knowledge about these robots, not popular in\nthe community, thereby enriching it. We highlight the generalization\ncapabilities of our system and show that (1) in-context learning in combination\nwith the current state-of-the-art LLMs is an effective way to implement a\nrobotic controller; (2) in static environments, InCoRo surpasses the prior art\nin terms of the success rate; (3) in dynamic environments, we establish new\nstate-of-the-art for the SCARA and DELTA units, respectively. This research\npaves the way towards building reliable, efficient, intelligent autonomous\nsystems that adapt to dynamic environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.05188v1.pdf"
    },
    {
        "title": "Opening the AI black box: program synthesis via mechanistic interpretability",
        "authors": [
            "Eric J. Michaud",
            "Isaac Liao",
            "Vedang Lad",
            "Ziming Liu",
            "Anish Mudide",
            "Chloe Loughridge",
            "Zifan Carl Guo",
            "Tara Rezaei Kheirkhah",
            "Mateja Vukeli\u0107",
            "Max Tegmark"
        ],
        "published": "2024-02-07T18:59:12Z",
        "summary": "We present MIPS, a novel method for program synthesis based on automated\nmechanistic interpretability of neural networks trained to perform the desired\ntask, auto-distilling the learned algorithm into Python code. We test MIPS on a\nbenchmark of 62 algorithmic tasks that can be learned by an RNN and find it\nhighly complementary to GPT-4: MIPS solves 32 of them, including 13 that are\nnot solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to\nconvert the RNN into a finite state machine, then applies Boolean or integer\nsymbolic regression to capture the learned algorithm. As opposed to large\nlanguage models, this program synthesis technique makes no use of (and is\ntherefore not limited by) human training data such as algorithms and code from\nGitHub. We discuss opportunities and challenges for scaling up this approach to\nmake machine-learned models more interpretable and trustworthy.",
        "pdf_link": "https://arxiv.org/pdf/2402.05110v1.pdf"
    },
    {
        "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
        "authors": [
            "Jordan Juravsky",
            "Bradley Brown",
            "Ryan Ehrlich",
            "Daniel Y. Fu",
            "Christopher R\u00e9",
            "Azalia Mirhoseini"
        ],
        "published": "2024-02-07T18:53:01Z",
        "summary": "Transformer-based large language models (LLMs) are now deployed to hundreds\nof millions of users. LLM inference is commonly performed on batches of\nsequences that share a prefix, such as few-shot examples or a chatbot system\nprompt. Decoding in this large-batch setting can be bottlenecked by the\nattention operation, which reads large key-value (KV) caches from memory and\ncomputes inefficient matrix-vector products for every sequence in the batch. In\nthis work, we introduce Hydragen, a hardware-aware exact implementation of\nattention with shared prefixes. Hydragen computes attention over the shared\nprefix and unique suffixes separately. This decomposition enables efficient\nprefix attention by batching queries together across sequences, reducing\nredundant memory reads and enabling the use of hardware-friendly matrix\nmultiplications. Our method can improve end-to-end LLM throughput by up to 32x\nagainst competitive baselines, with speedup growing with the batch size and\nshared prefix length. Hydragen also enables the use of very long shared\ncontexts: with a high batch size, increasing the prefix length from 1K to 16K\ntokens decreases Hydragen throughput by less than 15%, while the throughput of\nbaselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix\ndecomposition and can be applied to tree-based prompt sharing patterns,\nallowing us to further reduce inference time on competitive programming\nproblems by 55%.",
        "pdf_link": "https://arxiv.org/pdf/2402.05099v1.pdf"
    },
    {
        "title": "Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation",
        "authors": [
            "Dennis Hoftijzer",
            "Gertjan Burghouts",
            "Luuk Spreeuwers"
        ],
        "published": "2024-02-07T18:44:27Z",
        "summary": "Deep Reinforcement Learning (DRL) has shown great potential in enabling\nrobots to find certain objects (e.g., `find a fridge') in environments like\nhomes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL\nmethods are predominantly trained and evaluated using environment simulators.\nAlthough DRL has shown impressive results, the simulators may be biased or\nlimited. This creates a risk of shortcut learning, i.e., learning a policy\ntailored to specific visual details of training environments. We aim to deepen\nour understanding of shortcut learning in ObjectNav, its implications and\npropose a solution. We design an experiment for inserting a shortcut bias in\nthe appearance of training environments. As a proof-of-concept, we associate\nroom types to specific wall colors (e.g., bedrooms with green walls), and\nobserve poor generalization of a state-of-the-art (SOTA) ObjectNav method to\nenvironments where this is not the case (e.g., bedrooms with blue walls). We\nfind that shortcut learning is the root cause: the agent learns to navigate to\ntarget objects, by simply searching for the associated wall color of the target\nobject's room. To solve this, we propose Language-Based (L-B) augmentation. Our\nkey insight is that we can leverage the multimodal feature space of a\nVision-Language Model (VLM) to augment visual representations directly at the\nfeature-level, requiring no changes to the simulator, and only an addition of\none layer to the model. Where the SOTA ObjectNav method's success rate drops\n69%, our proposal has only a drop of 23%.",
        "pdf_link": "https://arxiv.org/pdf/2402.05090v1.pdf"
    },
    {
        "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
        "authors": [
            "Boyi Wei",
            "Kaixuan Huang",
            "Yangsibo Huang",
            "Tinghao Xie",
            "Xiangyu Qi",
            "Mengzhou Xia",
            "Prateek Mittal",
            "Mengdi Wang",
            "Peter Henderson"
        ],
        "published": "2024-02-07T18:34:38Z",
        "summary": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.05162v1.pdf"
    },
    {
        "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
        "authors": [
            "Lijun Li",
            "Bowen Dong",
            "Ruohui Wang",
            "Xuhao Hu",
            "Wangmeng Zuo",
            "Dahua Lin",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-02-07T17:33:54Z",
        "summary": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring\nrobust safety measures is paramount. To meet this crucial need, we propose\n\\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating\nLLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench\ntranscends conventional benchmarks through its large scale, rich diversity,\nintricate taxonomy spanning three levels, and versatile\nfunctionalities.SALAD-Bench is crafted with a meticulous array of questions,\nfrom standard queries to complex ones enriched with attack, defense\nmodifications and multiple-choice. To effectively manage the inherent\ncomplexity, we introduce an innovative evaluators: the LLM-based MD-Judge for\nQA pairs with a particular focus on attack-enhanced queries, ensuring a\nseamless, and reliable evaluation. Above components extend SALAD-Bench from\nstandard LLM safety evaluation to both LLM attack and defense methods\nevaluation, ensuring the joint-purpose utility. Our extensive experiments shed\nlight on the resilience of LLMs against emerging threats and the efficacy of\ncontemporary defense tactics. Data and evaluator are released under\nhttps://github.com/OpenSafetyLab/SALAD-BENCH.",
        "pdf_link": "https://arxiv.org/pdf/2402.05044v3.pdf"
    },
    {
        "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
        "authors": [
            "Agustinus Kristiadi",
            "Felix Strieth-Kalthoff",
            "Marta Skreta",
            "Pascal Poupart",
            "Al\u00e1n Aspuru-Guzik",
            "Geoff Pleiss"
        ],
        "published": "2024-02-07T16:32:58Z",
        "summary": "Automation is one of the cornerstones of contemporary material discovery.\nBayesian optimization (BO) is an essential part of such workflows, enabling\nscientists to leverage prior domain knowledge into efficient exploration of a\nlarge molecular space. While such prior knowledge can take many forms, there\nhas been significant fanfare around the ancillary scientific knowledge\nencapsulated in large language models (LLMs). However, existing work thus far\nhas only explored LLMs for heuristic materials searches. Indeed, recent work\nobtains the uncertainty estimate -- an integral part of BO -- from\npoint-estimated, non-Bayesian LLMs. In this work, we study the question of\nwhether LLMs are actually useful to accelerate principled Bayesian optimization\nin the molecular space. We take a sober, dispassionate stance in answering this\nquestion. This is done by carefully (i) viewing LLMs as fixed feature\nextractors for standard but principled BO surrogate models and by (ii)\nleveraging parameter-efficient finetuning methods and Bayesian neural networks\nto obtain the posterior of the LLM surrogate. Our extensive experiments with\nreal-world chemistry problems show that LLMs can be useful for BO over\nmolecules, but only if they have been pretrained or finetuned with\ndomain-specific data.",
        "pdf_link": "https://arxiv.org/pdf/2402.05015v1.pdf"
    },
    {
        "title": "Pedagogical Alignment of Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "Kangqi Ni",
            "Sapana Chaudhary",
            "Richard G. Baraniuk"
        ],
        "published": "2024-02-07T16:15:59Z",
        "summary": "In this paper, we introduce the novel concept of pedagogically aligned Large\nLanguage Models (LLMs) that signifies a transformative shift in the application\nof LLMs within educational contexts. Rather than providing direct responses to\nuser queries, pedagogically-aligned LLMs function as scaffolding tools,\nbreaking complex problems into manageable subproblems and guiding students\ntowards the final answer through constructive feedback and hints. The objective\nis to equip learners with problem-solving strategies that deepen their\nunderstanding and internalization of the subject matter. Previous research in\nthis field has primarily applied the supervised finetuning approach without\nframing the objective as an alignment problem, hence not employing\nreinforcement learning through human feedback (RLHF) methods. This study\nreinterprets the narrative by viewing the task through the lens of alignment\nand demonstrates how RLHF methods emerge naturally as a superior alternative\nfor aligning LLM behaviour. Building on this perspective, we propose a novel\napproach for constructing a reward dataset specifically designed for the\npedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms\nand find that they outperform SFT significantly. Our qualitative analyses\nacross model differences and hyperparameter sensitivity further validate the\nsuperiority of RLHF over SFT. Also, our study sheds light on the potential of\nonline feedback for enhancing the performance of pedagogically-aligned LLMs,\nthus providing valuable insights for the advancement of these models in\neducational settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.05000v1.pdf"
    },
    {
        "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
        "authors": [
            "Yihao Li",
            "Ru Zhang",
            "Jianyi Liu",
            "Gongshen Liu"
        ],
        "published": "2024-02-07T15:56:17Z",
        "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in a\nmultitude of Natural Language Processing (NLP) tasks, they encounter challenges\nin practical applications, including issues with hallucinations, inadequate\nknowledge updating, and limited transparency in the reasoning process. To\novercome these limitations, this study innovatively proposes a collaborative\ntraining-free reasoning scheme involving tight cooperation between Knowledge\nGraph (KG) and LLMs. This scheme first involves using LLMs to iteratively\nexplore KG, selectively retrieving a task-relevant knowledge subgraph to\nsupport reasoning. The LLMs are then guided to further combine inherent\nimplicit knowledge to reason on the subgraph while explicitly elucidating the\nreasoning process. Through such a cooperative approach, our scheme achieves\nmore reliable knowledge-based reasoning and facilitates the tracing of the\nreasoning results. Experimental results show that our scheme significantly\nprogressed across multiple datasets, notably achieving over a 10% improvement\non the QALD10 dataset compared to the best baseline and the fine-tuned\nstate-of-the-art (SOTA) work. Building on this success, this study hopes to\noffer a valuable reference for future research in the fusion of KG and LLMs,\nthereby enhancing LLMs' proficiency in solving complex issues.",
        "pdf_link": "https://arxiv.org/pdf/2402.04978v1.pdf"
    },
    {
        "title": "ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12",
        "authors": [
            "Liuqing Chen",
            "Shuhong Xiao",
            "Yunnong Chen",
            "Ruoyu Wu",
            "Yaxuan Song",
            "Lingyun Sun"
        ],
        "published": "2024-02-07T15:55:51Z",
        "summary": "As Computational Thinking (CT) continues to permeate younger age groups in\nK-12 education, established CT platforms such as Scratch face challenges in\ncatering to these younger learners, particularly those in the elementary school\n(ages 6-12). Through formative investigation with Scratch experts, we uncover\nthree key obstacles to children's autonomous Scratch learning: artist's block\nin project planning, bounded creativity in asset creation, and inadequate\ncoding guidance during implementation. To address these barriers, we introduce\nChatScratch, an AI-augmented system to facilitate autonomous programming\nlearning for young children. ChatScratch employs structured interactive\nstoryboards and visual cues to overcome artist's block, integrates digital\ndrawing and advanced image generation technologies to elevate creativity, and\nleverages Scratch-specialized Large Language Models (LLMs) for professional\ncoding guidance. Our study shows that, compared to Scratch, ChatScratch\nefficiently fosters autonomous programming learning, and contributes to the\ncreation of high-quality, personally meaningful Scratch projects for children.",
        "pdf_link": "https://arxiv.org/pdf/2402.04975v1.pdf"
    },
    {
        "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
        "authors": [
            "Lihu Chen",
            "Alexandre Perez-Lebel",
            "Fabian M. Suchanek",
            "Ga\u00ebl Varoquaux"
        ],
        "published": "2024-02-07T15:40:22Z",
        "summary": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.04957v1.pdf"
    },
    {
        "title": "Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems",
        "authors": [
            "Samuel Kernan Freire",
            "Chaofan Wang",
            "Evangelos Niforatos"
        ],
        "published": "2024-02-07T15:39:07Z",
        "summary": "Cognitive assistants (CA) are chatbots that provide context-aware support to\nhuman workers in knowledge-intensive tasks. Traditionally, cognitive assistants\nrespond in specific ways to predefined user intents and conversation patterns.\nHowever, this rigidness does not handle the diversity of natural language well.\nRecent advances in natural language processing (NLP), powering large language\nmodels (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in\na more flexible, human-like manner. However, the additional degrees of freedom\nmay have unforeseen consequences, especially in knowledge-intensive contexts\nwhere accuracy is crucial. As a preliminary step to assessing the potential of\nusing LLMs in these contexts, we conducted a user study comparing an LLM-based\nCA to an intent-based system regarding interaction efficiency, user experience,\nworkload, and usability. This revealed that LLM-based CAs exhibited better user\nexperience, task completion rate, usability, and perceived performance than\nintent-based systems, suggesting that switching NLP techniques should be\ninvestigated further.",
        "pdf_link": "https://arxiv.org/pdf/2402.04955v1.pdf"
    },
    {
        "title": "Prompting Implicit Discourse Relation Annotation",
        "authors": [
            "Frances Yung",
            "Mansoor Ahmad",
            "Merel Scholman",
            "Vera Demberg"
        ],
        "published": "2024-02-07T14:44:42Z",
        "summary": "Pre-trained large language models, such as ChatGPT, archive outstanding\nperformance in various reasoning tasks without supervised training and were\nfound to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's\nperformance in the task of implicit discourse relation classification, prompted\nby a standard multiple-choice question, is still far from satisfactory and\nconsiderably inferior to state-of-the-art supervised approaches. This work\ninvestigates several proven prompting techniques to improve ChatGPT's\nrecognition of discourse relations. In particular, we experimented with\nbreaking down the classification task that involves numerous abstract labels\ninto smaller subtasks. Nonetheless, experiment results show that the inference\naccuracy hardly changes even with sophisticated prompt engineering, suggesting\nthat implicit discourse relation classification is not yet resolvable under\nzero-shot or few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.04918v1.pdf"
    },
    {
        "title": "L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ",
        "authors": [
            "Hyesung Jeon",
            "Yulhwa Kim",
            "Jae-joon Kim"
        ],
        "published": "2024-02-07T14:35:05Z",
        "summary": "Post-training quantization (PTQ) and quantization-aware training (QAT)\nmethods are gaining popularity in mitigating the high memory and computational\ncosts associated with Large Language Models (LLMs). In resource-constrained\nscenarios, PTQ, with its reduced training overhead, is often preferred over\nQAT, despite the latter's potential for higher accuracy. Meanwhile,\nparameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA)\nhave been introduced, and recent efforts have explored quantization-aware PEFT\ntechniques. However, these approaches may lack generality due to their reliance\non the pre-quantized model's configuration. Their effectiveness may be\ncompromised by non-linearly quantized or mixed-precision weights, and the\nretraining of specific quantization parameters might impede optimal\nperformance. To address these challenges, we propose L4Q, an algorithm for\nparameter-efficient quantization-aware training. L4Q leverages LoRA-wise\nlearned quantization step size for LLMs, aiming to enhance generality. The\nsimultaneous quantization-and-fine-tuning process of L4Q is applicable to\nhigh-precision models, yielding linearly quantized weights with superior\naccuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families\nusing an instructional dataset, showcase L4Q's capabilities in language\ncomprehension and few-shot in-context learning, achieving sub-4-bit precision\nwhile maintaining comparable training times to applying PEFT on a quantized\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2402.04902v2.pdf"
    },
    {
        "title": "Detecting Generated Native Ads in Conversational Search",
        "authors": [
            "Sebastian Schmidt",
            "Ines Zelch",
            "Janek Bevendorff",
            "Benno Stein",
            "Matthias Hagen",
            "Martin Potthast"
        ],
        "published": "2024-02-07T14:22:51Z",
        "summary": "Conversational search engines such as YouChat and Microsoft Copilot use large\nlanguage models (LLMs) to generate answers to queries. It is only a small step\nto also use this technology to generate and integrate advertising within these\nanswers - instead of placing ads separately from the organic search results.\nThis type of advertising is reminiscent of native advertising and product\nplacement, both of which are very effective forms of subtle and manipulative\nadvertising. It is likely that information seekers will be confronted with such\nuse of LLM technology in the near future, especially when considering the high\ncomputational costs associated with LLMs, for which providers need to develop\nsustainable business models. This paper investigates whether LLMs can also be\nused as a countermeasure against generated native ads, i.e., to block them. For\nthis purpose we compile a large dataset of ad-prone queries and of generated\nanswers with automatically integrated ads to experiment with fine-tuned\nsentence transformers and state-of-the-art LLMs on the task of recognizing the\nads. In our experiments sentence transformers achieve detection precision and\nrecall values above 0.9, while the investigated LLMs struggle with the task.",
        "pdf_link": "https://arxiv.org/pdf/2402.04889v1.pdf"
    },
    {
        "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
        "authors": [
            "Luca Beurer-Kellner",
            "Marc Fischer",
            "Martin Vechev"
        ],
        "published": "2024-02-07T13:36:02Z",
        "summary": "To ensure that text generated by large language models (LLMs) is in an\nexpected format, constrained decoding proposes to enforce strict formal\nlanguage constraints during generation. However, as we show in this work, not\nonly do such methods incur performance overhead during generation, but many of\nthem also significantly impair task accuracy, if they do not correctly align\nthe underlying LLM sub-word vocabularies with external constraints. To address\nthis, we present a novel decoding algorithm, DOMINO, that can enforce\nconstraints in a fully subword-aligned fashion, while leveraging\npre-computation and speculative decoding to achieve virtually no overhead and\nin some cases even almost 2$\\times$ speedup over unconstrained decoding --\nthereby outperforming existing approaches by a wide margin.",
        "pdf_link": "https://arxiv.org/pdf/2403.06988v1.pdf"
    },
    {
        "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
        "authors": [
            "Hao Zhao",
            "Maksym Andriushchenko",
            "Francesco Croce",
            "Nicolas Flammarion"
        ],
        "published": "2024-02-07T13:32:11Z",
        "summary": "There is a consensus that instruction fine-tuning of LLMs requires\nhigh-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR\n2024) are state-of-the-art methods for selecting such high-quality examples,\neither via manual curation or using GPT-3.5-Turbo as a quality scorer. We show\nthat the extremely simple baseline of selecting the 1,000 instructions with\nlongest responses from standard datasets can consistently outperform these\nsophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining\ncompetitive on the OpenLLM benchmarks that test factual knowledge. We\ndemonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B,\nand Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a\nlightweight refinement of such long instructions can further improve the\nabilities of the fine-tuned LLMs, and allows us to obtain the 2nd\nhighest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only\n1,000 examples and no extra preference data. We also conduct a thorough\nanalysis of our models to ensure that their enhanced performance is not simply\ndue to GPT-4's preference for longer responses, thus ruling out any artificial\nimprovement. In conclusion, our findings suggest that fine-tuning on the\nlongest instructions should be the default baseline for any research on\ninstruction fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.04833v1.pdf"
    },
    {
        "title": "Direct Language Model Alignment from Online AI Feedback",
        "authors": [
            "Shangmin Guo",
            "Biao Zhang",
            "Tianlin Liu",
            "Tianqi Liu",
            "Misha Khalman",
            "Felipe Llinares",
            "Alexandre Rame",
            "Thomas Mesnard",
            "Yao Zhao",
            "Bilal Piot",
            "Johan Ferret",
            "Mathieu Blondel"
        ],
        "published": "2024-02-07T12:31:13Z",
        "summary": "Direct alignment from preferences (DAP) methods, such as DPO, have recently\nemerged as efficient alternatives to reinforcement learning from human feedback\n(RLHF), that do not require a separate reward model. However, the preference\ndatasets used in DAP methods are usually collected ahead of training and never\nupdated, thus the feedback is purely offline. Moreover, responses in these\ndatasets are often sampled from a language model distinct from the one being\naligned, and since the model evolves over training, the alignment phase is\ninevitably off-policy. In this study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as\nannotator: on each training iteration, we sample two responses from the current\nmodel and prompt the LLM annotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity, we demonstrate via human\nevaluation in several tasks that OAIF outperforms both offline DAP and RLHF\nmethods. We further show that the feedback leveraged in OAIF is easily\ncontrollable, via instruction prompts to the LLM annotator.",
        "pdf_link": "https://arxiv.org/pdf/2402.04792v2.pdf"
    },
    {
        "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
        "authors": [
            "Dongping Chen",
            "Ruoxi Chen",
            "Shilin Zhang",
            "Yinuo Liu",
            "Yaochen Wang",
            "Huichi Zhou",
            "Qihui Zhang",
            "Pan Zhou",
            "Yao Wan",
            "Lichao Sun"
        ],
        "published": "2024-02-07T12:28:32Z",
        "summary": "Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence multimodal benchmarks that align with human\npreferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel\nbenchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting\njudges including three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparisons, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking tasks.\nFurthermore, MLLMs still face challenges in judgment, including diverse biases,\nhallucinatory responses, and inconsistencies, even for advanced models such as\nGPT-4V. These findings emphasize the pressing need for enhancements and further\nresearch efforts regarding MLLMs as fully reliable evaluators. Code and dataset\nare available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.",
        "pdf_link": "https://arxiv.org/pdf/2402.04788v1.pdf"
    },
    {
        "title": "A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models",
        "authors": [
            "Marc Braun",
            "Jenny Kunz"
        ],
        "published": "2024-02-07T12:26:12Z",
        "summary": "The self-rationalising capabilities of LLMs are appealing because the\ngenerated explanations can give insights into the plausibility of the\npredictions. However, how faithful the explanations are to the predictions is\nquestionable, raising the need to explore the patterns behind them further. To\nthis end, we propose a hypothesis-driven statistical framework. We use a\nBayesian network to implement a hypothesis about how a task (in our example,\nnatural language inference) is solved, and its internal states are translated\ninto natural language with templates. Those explanations are then compared to\nLLM-generated free-text explanations using automatic and human evaluations.\nThis allows us to judge how similar the LLM's and the Bayesian network's\ndecision processes are. We demonstrate the usage of our framework with an\nexample hypothesis and two realisations in Bayesian networks. The resulting\nmodels do not exhibit a strong similarity to GPT-3.5. We discuss the\nimplications of this as well as the framework's potential to approximate LLM\ndecisions better in future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.04787v1.pdf"
    },
    {
        "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
        "authors": [
            "Baohao Liao",
            "Christof Monz"
        ],
        "published": "2024-02-07T09:36:54Z",
        "summary": "Memory-efficient finetuning of large language models (LLMs) has recently\nattracted huge attention with the increasing size of LLMs, primarily due to the\nconstraints posed by GPU memory limitations and the comparable results of these\nmethods with full finetuning. Despite the advancements, current strategies for\nmemory-efficient finetuning, such as QLoRA, exhibit inconsistent performance\nacross diverse bit-width quantizations and multifaceted tasks. This\ninconsistency largely stems from the detrimental impact of the quantization\nprocess on preserved knowledge, leading to catastrophic forgetting and\nundermining the utilization of pretrained models for finetuning purposes. In\nthis work, we introduce a novel quantization framework named ApiQ, designed to\nrestore the lost information from quantization by concurrently initializing\nLoRA components and quantizing the weights of LLMs. This approach ensures the\nmaintenance of the original LLM's activation precision while mitigating the\nerror propagation from shallower into deeper layers. Through comprehensive\nevaluations conducted on a spectrum of language tasks with various models, ApiQ\ndemonstrably minimizes activation error during quantization. Consequently, it\nconsistently achieves superior finetuning outcomes across various bit-widths of\nquantization.",
        "pdf_link": "https://arxiv.org/pdf/2402.05147v2.pdf"
    },
    {
        "title": "Large Language Models As Faithful Explainers",
        "authors": [
            "Yu-Neng Chuang",
            "Guanchu Wang",
            "Chia-Yuan Chang",
            "Ruixiang Tang",
            "Fan Yang",
            "Mengnan Du",
            "Xuanting Cai",
            "Xia Hu"
        ],
        "published": "2024-02-07T09:09:14Z",
        "summary": "Large Language Models (LLMs) have recently become proficient in addressing\ncomplex tasks by utilizing their rich internal knowledge and reasoning ability.\nConsequently, this complexity hinders traditional input-focused explanation\nalgorithms for explaining the complex decision-making processes of LLMs. Recent\nadvancements have thus emerged for self-explaining their predictions through a\nsingle feed-forward inference in a natural language format. However, natural\nlanguage explanations are often criticized for lack of faithfulness since these\nexplanations may not accurately reflect the decision-making behaviors of the\nLLMs. In this work, we introduce a generative explanation framework, xLLM, to\nimprove the faithfulness of the explanations provided in natural language\nformats for LLMs. Specifically, we propose an evaluator to quantify the\nfaithfulness of natural language explanation and enhance the faithfulness by an\niterative optimization process of xLLM, with the goal of maximizing the\nfaithfulness scores. Experiments conducted on three NLU datasets demonstrate\nthat xLLM can significantly improve the faithfulness of generated explanations,\nwhich are in alignment with the behaviors of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.04678v1.pdf"
    },
    {
        "title": "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views",
        "authors": [
            "Yuji Roh",
            "Qingyun Liu",
            "Huan Gui",
            "Zhe Yuan",
            "Yujin Tang",
            "Steven Euijong Whang",
            "Liang Liu",
            "Shuchao Bi",
            "Lichan Hong",
            "Ed H. Chi",
            "Zhe Zhao"
        ],
        "published": "2024-02-07T08:16:40Z",
        "summary": "Fine-tuning is becoming widely used for leveraging the power of pre-trained\nfoundation models in new downstream tasks. While there are many successes of\nfine-tuning on various tasks, recent studies have observed challenges in the\ngeneralization of fine-tuned models to unseen distributions (i.e.,\nout-of-distribution; OOD). To improve OOD generalization, some previous studies\nidentify the limitations of fine-tuning data and regulate fine-tuning to\npreserve the general representation learned from pre-training data. However,\npotential limitations in the pre-training data and models are often ignored. In\nthis paper, we contend that overly relying on the pre-trained representation\nmay hinder fine-tuning from learning essential representations for downstream\ntasks and thus hurt its OOD generalization. It can be especially catastrophic\nwhen new tasks are from different (sub)domains compared to pre-training data.\nTo address the issues in both pre-training and fine-tuning data, we propose a\nnovel generalizable fine-tuning method LEVI, where the pre-trained model is\nadaptively ensembled layer-wise with a small task-specific model, while\npreserving training and inference efficiencies. By combining two complementing\nmodels, LEVI effectively suppresses problematic features in both the\nfine-tuning data and pre-trained model and preserves useful features for new\ntasks. Broad experiments with large language and vision models show that LEVI\ngreatly improves fine-tuning generalization via emphasizing different views\nfrom fine-tuning data and pre-trained features.",
        "pdf_link": "https://arxiv.org/pdf/2402.04644v1.pdf"
    },
    {
        "title": "The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends",
        "authors": [
            "Mengqi Chen",
            "Bin Guo",
            "Hao Wang",
            "Haoyu Li",
            "Qian Zhao",
            "Jingqi Liu",
            "Yasan Ding",
            "Yan Pan",
            "Zhiwen Yu"
        ],
        "published": "2024-02-07T07:28:34Z",
        "summary": "Persuasion, as one of the crucial abilities in human communication, has\ngarnered extensive attention from researchers within the field of intelligent\ndialogue systems. We humans tend to persuade others to change their viewpoints,\nattitudes or behaviors through conversations in various scenarios (e.g.,\npersuasion for social good, arguing in online platforms). Developing dialogue\nagents that can persuade others to accept certain standpoints is essential to\nachieving truly intelligent and anthropomorphic dialogue system. Benefiting\nfrom the substantial progress of Large Language Models (LLMs), dialogue agents\nhave acquired an exceptional capability in context understanding and response\ngeneration. However, as a typical and complicated cognitive psychological\nsystem, persuasive dialogue agents also require knowledge from the domain of\ncognitive psychology to attain a level of human-like persuasion. Consequently,\nthe cognitive strategy-enhanced persuasive dialogue agent (defined as\nCogAgent), which incorporates cognitive strategies to achieve persuasive\ntargets through conversation, has become a predominant research paradigm. To\ndepict the research trends of CogAgent, in this paper, we first present several\nfundamental cognitive psychology theories and give the formalized definition of\nthree typical cognitive strategies, including the persuasion strategy, the\ntopic path planning strategy, and the argument structure prediction strategy.\nThen we propose a new system architecture by incorporating the formalized\ndefinition to lay the foundation of CogAgent. Representative works are detailed\nand investigated according to the combined cognitive strategy, followed by the\nsummary of authoritative benchmarks and evaluation metrics. Finally, we\nsummarize our insights on open issues and future directions of CogAgent for\nupcoming researchers.",
        "pdf_link": "https://arxiv.org/pdf/2402.04631v1.pdf"
    },
    {
        "title": "SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph",
        "authors": [
            "Julio C. Rangel",
            "Tarcisio Mendes de Farias",
            "Ana Claudia Sima",
            "Norio Kobayashi"
        ],
        "published": "2024-02-07T07:24:01Z",
        "summary": "The recent success of Large Language Models (LLM) in a wide range of Natural\nLanguage Processing applications opens the path towards novel Question\nAnswering Systems over Knowledge Graphs leveraging LLMs. However, one of the\nmain obstacles preventing their implementation is the scarcity of training data\nfor the task of translating questions into corresponding SPARQL queries,\nparticularly in the case of domain-specific KGs. To overcome this challenge, in\nthis study, we evaluate several strategies for fine-tuning the OpenLlama LLM\nfor question answering over life science knowledge graphs. In particular, we\npropose an end-to-end data augmentation approach for extending a set of\nexisting queries over a given knowledge graph towards a larger dataset of\nsemantically enriched question-to-SPARQL query pairs, enabling fine-tuning even\nfor datasets where these pairs are scarce. In this context, we also investigate\nthe role of semantic \"clues\" in the queries, such as meaningful variable names\nand inline comments. Finally, we evaluate our approach over the real-world Bgee\ngene expression knowledge graph and we show that semantic clues can improve\nmodel performance by up to 33% compared to a baseline with random variable\nnames and no comments included.",
        "pdf_link": "https://arxiv.org/pdf/2402.04627v1.pdf"
    },
    {
        "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
        "authors": [
            "Yu Wang",
            "Xiusi Chen",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "published": "2024-02-07T07:14:11Z",
        "summary": "Existing Large Language Models (LLMs) usually remain static after deployment,\nwhich might make it hard to inject new knowledge into the model. We aim to\nbuild models containing a considerable portion of self-updatable parameters,\nenabling the model to integrate new knowledge effectively and efficiently. To\nthis end, we introduce MEMORYLLM, a model that comprises a transformer and a\nfixed-size memory pool within the latent space of the transformer. MEMORYLLM\ncan self-update with text knowledge and memorize the knowledge injected\nearlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively\nincorporate new knowledge, as evidenced by its performance on model editing\nbenchmarks. Meanwhile, the model exhibits long-term information retention\ncapacity, which is validated through our custom-designed evaluations and\nlong-context benchmarks. MEMORYLLM also shows operational integrity without any\nsign of performance degradation even after nearly a million memory updates.",
        "pdf_link": "https://arxiv.org/pdf/2402.04624v1.pdf"
    },
    {
        "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients",
        "authors": [
            "Pragnya Ramjee",
            "Bhuvan Sachdeva",
            "Satvik Golechha",
            "Shreyas Kulkarni",
            "Geeta Fulari",
            "Kaushik Murali",
            "Mohit Jain"
        ],
        "published": "2024-02-07T07:07:02Z",
        "summary": "The healthcare landscape is evolving, with patients seeking more reliable\ninformation about their health conditions, treatment options, and potential\nrisks. Despite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\ndoctors and hospital staff, highlighting the need for expert-endorsed health\ninformation. However, the pressure on experts has led to reduced communication\ntime, impacting information sharing. To address this gap, we propose\nCataractBot, an experts-in-the-loop chatbot powered by large language models\n(LLMs). Developed in collaboration with a tertiary eye hospital in India,\nCataractBot answers cataract surgery related questions instantly by querying a\ncurated knowledge base, and provides expert-verified responses asynchronously.\nCataractBot features multimodal support and multilingual capabilities. In an\nin-the-wild deployment study with 49 participants, CataractBot proved valuable,\nproviding anytime accessibility, saving time, and accommodating diverse\nliteracy levels. Trust was established through expert verification. Broadly,\nour results could inform future work on designing expert-mediated LLM bots.",
        "pdf_link": "https://arxiv.org/pdf/2402.04620v1.pdf"
    },
    {
        "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
        "authors": [
            "Chaojun Xiao",
            "Pengle Zhang",
            "Xu Han",
            "Guangxuan Xiao",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Song Han",
            "Maosong Sun"
        ],
        "published": "2024-02-07T06:50:42Z",
        "summary": "Large language models (LLMs) have emerged as a cornerstone in real-world\napplications with lengthy streaming inputs, such as LLM-driven agents. However,\nexisting LLMs, pre-trained on sequences with restricted maximum length, cannot\ngeneralize to longer sequences due to the out-of-domain and distraction issues.\nTo alleviate these issues, existing efforts employ sliding attention windows\nand discard distant tokens to achieve the processing of extremely long\nsequences. Unfortunately, these approaches inevitably fail to capture\nlong-distance dependencies within sequences to deeply understand semantics.\nThis paper introduces a training-free memory-based method, InfLLM, to unveil\nthe intrinsic ability of LLMs to process streaming long sequences.\nSpecifically, InfLLM stores distant contexts into additional memory units and\nemploys an efficient mechanism to lookup token-relevant units for attention\ncomputation. Thereby, InfLLM allows LLMs to efficiently process long sequences\nwhile maintaining the ability to capture long-distance dependencies. Without\nany training, InfLLM enables LLMs pre-trained on sequences of a few thousand\ntokens to achieve superior performance than competitive baselines continually\ntraining these LLMs on long sequences. Even when the sequence length is scaled\nto $1,024$K, InfLLM still effectively captures long-distance dependencies.",
        "pdf_link": "https://arxiv.org/pdf/2402.04617v1.pdf"
    },
    {
        "title": "TinyLLM: Learning a Small Student from Multiple Large Language Models",
        "authors": [
            "Yijun Tian",
            "Yikun Han",
            "Xiusi Chen",
            "Wei Wang",
            "Nitesh V. Chawla"
        ],
        "published": "2024-02-07T06:48:24Z",
        "summary": "Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a new knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite a considerably smaller model size.",
        "pdf_link": "https://arxiv.org/pdf/2402.04616v2.pdf"
    },
    {
        "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models",
        "authors": [
            "Chirag Agarwal",
            "Sree Harsha Tanneru",
            "Himabindu Lakkaraju"
        ],
        "published": "2024-02-07T06:32:50Z",
        "summary": "Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.04614v3.pdf"
    },
    {
        "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
        "authors": [
            "Zhuang Li",
            "Levon Haroutunian",
            "Raj Tumuluri",
            "Philip Cohen",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-07T06:13:14Z",
        "summary": "Post-editing has proven effective in improving the quality of text generated\nby large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when\ndirect updating of their parameters to enhance text quality is infeasible or\nexpensive. However, relying solely on smaller language models for post-editing\ncan limit the LLMs' ability to generalize across domains. Moreover, the editing\nstrategies in these methods are not optimally designed for text-generation\ntasks. To address these limitations, we propose a neural programmer-interpreter\napproach that preserves the domain generalization ability of LLMs when editing\ntheir output. The editing actions in this framework are specifically devised\nfor text generation. Extensive experiments demonstrate that the\nprogrammer-interpreter significantly enhances GPT-3.5's performance in logical\nform-to-text conversion and low-resource machine translation, surpassing other\nstate-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.04609v1.pdf"
    },
    {
        "title": "Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector",
        "authors": [
            "Haihui Yang",
            "Xiaojun Quan"
        ],
        "published": "2024-02-07T05:56:54Z",
        "summary": "Chinese grammatical error correction (CGEC) faces serious overcorrection\nchallenges when employing autoregressive generative models such as\nsequence-to-sequence (Seq2Seq) models and decoder-only large language models\n(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,\nthey are difficult to adapt to decoder-only LLMs. In this paper, we propose an\nalignment-enhanced corrector for the overcorrection problem that applies to\nboth Seq2Seq models and decoder-only LLMs. Our method first trains a correction\nmodel to generate an initial correction of the source sentence. Then, we\ncombine the source sentence with the initial correction and feed it through an\nalignment model for another round of correction, aiming to enforce the\nalignment model to focus on potential overcorrection. Moreover, to enhance the\nmodel's ability to identify nuances, we further explore the reverse alignment\nof the source sentence and the initial correction. Finally, we transfer the\nalignment knowledge from two alignment models to the correction model,\ninstructing it on how to avoid overcorrection. Experimental results on three\nCGEC datasets demonstrate the effectiveness of our approach in alleviating\novercorrection and improving overall performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.04601v1.pdf"
    },
    {
        "title": "The Role of LLMs in Sustainable Smart Cities: Applications, Challenges, and Future Directions",
        "authors": [
            "Amin Ullah",
            "Guilin Qi",
            "Saddam Hussain",
            "Irfan Ullah",
            "Zafar Ali"
        ],
        "published": "2024-02-07T05:22:10Z",
        "summary": "Smart cities stand as pivotal components in the ongoing pursuit of elevating\nurban living standards, facilitating the rapid expansion of urban areas while\nefficiently managing resources through sustainable and scalable innovations. In\nthis regard, as emerging technologies like Artificial Intelligence (AI), the\nInternet of Things (IoT), big data analytics, and fog and edge computing have\nbecome increasingly prevalent, smart city applications grapple with various\nchallenges, including the potential for unauthorized disclosure of confidential\nand sensitive data. The seamless integration of emerging technologies has\nplayed a vital role in sustaining the dynamic pace of their development. This\npaper explores the substantial potential and applications of Deep Learning\n(DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing\n(NLP), and large language models (LLMs) in optimizing ICT processes within\nsmart cities. We aim to spotlight the vast potential of these technologies as\nfoundational elements that technically strengthen the realization and\nadvancement of smart cities, underscoring their significance in driving\ninnovation within this transformative urban milieu. Our discourse culminates\nwith an exploration of the formidable challenges that DL, FL, IoT, Blockchain,\nNLP, and LLMs face within these contexts, and we offer insights into potential\nfuture directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.14596v1.pdf"
    },
    {
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "authors": [
            "Chengxing Xie",
            "Canyu Chen",
            "Feiran Jia",
            "Ziyu Ye",
            "Kai Shu",
            "Adel Bibi",
            "Ziniu Hu",
            "Philip Torr",
            "Bernard Ghanem",
            "Guohao Li"
        ],
        "published": "2024-02-07T03:37:19Z",
        "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in applications such as social science.\nHowever, one fundamental question remains: can LLM agents really simulate human\nbehaviors? In this paper, we focus on one of the most critical behaviors in\nhuman interactions, trust, and aim to investigate whether or not LLM agents can\nsimulate human trust behaviors. We first find that LLM agents generally exhibit\ntrust behaviors, referred to as agent trust, under the framework of Trust\nGames, which are widely recognized in behavioral economics. Then, we discover\nthat LLM agents can have high behavioral alignment with humans regarding trust\nbehaviors, particularly for GPT-4, indicating the feasibility to simulate human\ntrust behaviors with LLM agents. In addition, we probe into the biases in agent\ntrust and the differences in agent trust towards agents and humans. We also\nexplore the intrinsic properties of agent trust under conditions including\nadvanced reasoning strategies and external manipulations. We further offer\nimportant implications of our discoveries for various scenarios where trust is\nparamount. Our study provides new insights into the behaviors of LLM agents and\nthe fundamental analogy between LLMs and humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.04559v2.pdf"
    },
    {
        "title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
        "authors": [
            "Nung Siong Lai",
            "Yi Shen Tew",
            "Xialin Zhong",
            "Jun Yin",
            "Jiali Li",
            "Binhang Yan",
            "Xiaonan Wang"
        ],
        "published": "2024-02-07T03:25:08Z",
        "summary": "In the pursuit of novel catalyst development to address pressing\nenvironmental concerns and energy demand, conventional design and optimization\nmethods often fall short due to the complexity and vastness of the catalyst\nparameter space. The advent of Machine Learning (ML) has ushered in a new era\nin the field of catalyst optimization, offering potential solutions to the\nshortcomings of traditional techniques. However, existing methods fail to\neffectively harness the wealth of information contained within the burgeoning\nbody of scientific literature on catalyst synthesis. To address this gap, this\nstudy proposes an innovative Artificial Intelligence (AI) workflow that\nintegrates Large Language Models (LLMs), Bayesian optimization, and an active\nlearning loop to expedite and enhance catalyst optimization. Our methodology\ncombines advanced language understanding with robust optimization strategies,\neffectively translating knowledge extracted from diverse literature into\nactionable parameters for practical experimentation and optimization. In this\narticle, we demonstrate the application of this AI workflow in the optimization\nof catalyst synthesis for ammonia production. The results underscore the\nworkflow's ability to streamline the catalyst development process, offering a\nswift, resource-efficient, and high-precision alternative to conventional\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.04557v1.pdf"
    },
    {
        "title": "RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation",
        "authors": [
            "Xiaohan Yu",
            "Li Zhang",
            "Xin Zhao",
            "Yue Wang",
            "Zhongrui Ma"
        ],
        "published": "2024-02-07T02:14:58Z",
        "summary": "Large language models (LLM) have recently emerged as a powerful tool for a\nvariety of natural language processing tasks, bringing a new surge of combining\nLLM with recommendation systems, termed as LLM-based RS. Current approaches\ngenerally fall into two main paradigms, the ID direct usage paradigm and the ID\ntranslation paradigm, noting their core weakness stems from lacking\nrecommendation knowledge and uniqueness. To address this limitation, we propose\na new paradigm, ID representation, which incorporates pre-trained ID embeddings\ninto LLMs in a complementary manner. In this work, we present RA-Rec, an\nefficient ID representation alignment framework for LLM-based recommendation,\nwhich is compatible with multiple ID-based methods and LLM architectures.\nSpecifically, we treat ID embeddings as soft prompts and design an innovative\nalignment module and an efficient tuning method with tailored data construction\nfor alignment. Extensive experiments demonstrate RA-Rec substantially\noutperforms current state-of-the-art methods, achieving up to 3.0% absolute\nHitRate@100 improvements while utilizing less than 10x training data.",
        "pdf_link": "https://arxiv.org/pdf/2402.04527v2.pdf"
    },
    {
        "title": "Online Cascade Learning for Efficient Inference over Streams",
        "authors": [
            "Lunyiu Nie",
            "Zhimin Ding",
            "Erdong Hu",
            "Christopher Jermaine",
            "Swarat Chaudhuri"
        ],
        "published": "2024-02-07T01:46:50Z",
        "summary": "Large Language Models (LLMs) have a natural role in answering complex queries\nabout data streams, but the high computational cost of LLM inference makes them\ninfeasible in many such tasks. We propose online cascade learning, the first\napproach to addressing this challenge. The objective here is to learn a\n\"cascade\" of models, starting with lower-capacity models (such as logistic\nregressors) and ending with a powerful LLM, along with a deferral policy that\ndetermines the model that is used on a given input. We formulate the task of\nlearning cascades online as an imitation-learning problem and give a no-regret\nalgorithm for the problem. Experimental results across four benchmarks show\nthat our method parallels LLMs in accuracy while cutting down inference costs\nby as much as 90%, underscoring its efficacy and adaptability in stream\nprocessing.",
        "pdf_link": "https://arxiv.org/pdf/2402.04513v1.pdf"
    },
    {
        "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models",
        "authors": [
            "Josh Alman",
            "Zhao Song"
        ],
        "published": "2024-02-07T00:45:31Z",
        "summary": "Large language models (LLMs) have made fundamental contributions over the\nlast a few years. To train an LLM, one needs to alternatingly run `forward'\ncomputations and `backward' computations. The forward computation can be viewed\nas attention function evaluation, and the backward computation can be viewed as\na gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it\nwas proved that the forward step can be performed in almost-linear time in\ncertain parameter regimes, but that there is no truly sub-quadratic time\nalgorithm in the remaining parameter regimes unless the popular hypothesis SETH\nis false. In this work, we show nearly identical results for the harder-seeming\nproblem of computing the gradient of loss function of one layer attention\nnetwork, and thus for the entire process of LLM training. This completely\ncharacterizes the fine-grained complexity of every step of LLM training.",
        "pdf_link": "https://arxiv.org/pdf/2402.04497v1.pdf"
    },
    {
        "title": "Grandmaster-Level Chess Without Search",
        "authors": [
            "Anian Ruoss",
            "Gr\u00e9goire Del\u00e9tang",
            "Sourabh Medapati",
            "Jordi Grau-Moya",
            "Li Kevin Wenliang",
            "Elliot Catt",
            "John Reid",
            "Tim Genewein"
        ],
        "published": "2024-02-07T00:36:24Z",
        "summary": "The recent breakthrough successes in machine learning are mainly attributed\nto scale: namely large-scale attention-based architectures and datasets of\nunprecedented scale. This paper investigates the impact of training at scale\nfor chess. Unlike traditional chess engines that rely on complex heuristics,\nexplicit search, or a combination of both, we train a 270M parameter\ntransformer model with supervised learning on a dataset of 10 million chess\ngames. We annotate each board in the dataset with action-values provided by the\npowerful Stockfish 16 engine, leading to roughly 15 billion data points. Our\nlargest model reaches a Lichess blitz Elo of 2895 against humans, and\nsuccessfully solves a series of challenging chess puzzles, without any\ndomain-specific tweaks or explicit search algorithms. We also show that our\nmodel outperforms AlphaZero's policy and value networks (without MCTS) and\nGPT-3.5-turbo-instruct. A systematic investigation of model and dataset size\nshows that strong chess performance only arises at sufficient scale. To\nvalidate our results, we perform an extensive series of ablations of design\nchoices and hyperparameters.",
        "pdf_link": "https://arxiv.org/pdf/2402.04494v1.pdf"
    },
    {
        "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
        "authors": [
            "Sanjari Srivastava",
            "Piotr Mardziel",
            "Zhikhun Zhang",
            "Archana Ahlawat",
            "Anupam Datta",
            "John C Mitchell"
        ],
        "published": "2024-02-07T00:30:58Z",
        "summary": "Fairness and privacy are two important values machine learning (ML)\npractitioners often seek to operationalize in models. Fairness aims to reduce\nmodel bias for social/demographic sub-groups. Privacy via differential privacy\n(DP) mechanisms, on the other hand, limits the impact of any individual's\ntraining data on the resulting model. The trade-offs between privacy and\nfairness goals of trustworthy ML pose a challenge to those wishing to address\nboth. We show that DP amplifies gender, racial, and religious bias when\nfine-tuning large language models (LLMs), producing models more biased than\nones fine-tuned without DP. We find the cause of the amplification to be a\ndisparity in convergence of gradients across sub-groups. Through the case of\nbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),\na known method for addressing bias, also mitigates bias amplification by DP. As\na consequence, DP and CDA together can be used to fine-tune models while\nmaintaining both fairness and privacy.",
        "pdf_link": "https://arxiv.org/pdf/2402.04489v1.pdf"
    },
    {
        "title": "Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models",
        "authors": [
            "Linge Guo"
        ],
        "published": "2024-02-07T00:21:46Z",
        "summary": "This research critically navigates the intricate landscape of AI deception,\nconcentrating on deceptive behaviours of Large Language Models (LLMs). My\nobjective is to elucidate this issue, examine the discourse surrounding it, and\nsubsequently delve into its categorization and ramifications. The essay\ninitiates with an evaluation of the AI Safety Summit 2023 (ASS) and\nintroduction of LLMs, emphasising multidimensional biases that underlie their\ndeceptive behaviours.The literature review covers four types of deception\ncategorised: Strategic deception, Imitation, Sycophancy, and Unfaithful\nReasoning, along with the social implications and risks they entail. Lastly, I\ntake an evaluative stance on various aspects related to navigating the\npersistent challenges of the deceptive AI. This encompasses considerations of\ninternational collaborative governance, the reconfigured engagement of\nindividuals with AI, proposal of practical adjustments, and specific elements\nof digital education.",
        "pdf_link": "https://arxiv.org/pdf/2403.09676v1.pdf"
    },
    {
        "title": "Detecting Mode Collapse in Language Models via Narration",
        "authors": [
            "Sil Hamilton"
        ],
        "published": "2024-02-06T23:52:58Z",
        "summary": "No two authors write alike. Personal flourishes invoked in written\nnarratives, from lexicon to rhetorical devices, imply a particular author--what\nliterary theorists label the implied or virtual author; distinct from the real\nauthor or narrator of a text. Early large language models trained on unfiltered\ntraining sets drawn from a variety of discordant sources yielded incoherent\npersonalities, problematic for conversational tasks but proving useful for\nsampling literature from multiple perspectives. Successes in alignment research\nin recent years have allowed researchers to impose subjectively consistent\npersonae on language models via instruction tuning and reinforcement learning\nfrom human feedback (RLHF), but whether aligned models retain the ability to\nmodel an arbitrary virtual author has received little scrutiny. By studying\n4,374 stories sampled from three OpenAI language models, we show successive\nversions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby\noverfitting the model during alignment constrains it from generalizing over\nauthorship: models suffering from mode collapse become unable to assume a\nmultiplicity of perspectives. Our method and results are significant for\nresearchers seeking to employ language models in sociological simulations.",
        "pdf_link": "https://arxiv.org/pdf/2402.04477v1.pdf"
    },
    {
        "title": "Structured Entity Extraction Using Large Language Models",
        "authors": [
            "Haolun Wu",
            "Ye Yuan",
            "Liana Mikaelyan",
            "Alexander Meulemans",
            "Xue Liu",
            "James Hensman",
            "Bhaskar Mitra"
        ],
        "published": "2024-02-06T22:15:09Z",
        "summary": "Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. This paper\nexplores the challenges and limitations of current methodologies in structured\nentity extraction and introduces a novel approach to address these issues. We\ncontribute to the field by first introducing and formalizing the task of\nStructured Entity Extraction (SEE), followed by proposing Approximate Entity\nSet OverlaP (AESOP) Metric designed to appropriately assess model performance\non this task. Later, we propose a new model that harnesses the power of LLMs\nfor enhanced effectiveness and efficiency through decomposing the entire\nextraction task into multiple stages. Quantitative evaluation and human\nside-by-side evaluation confirm that our model outperforms baselines, offering\npromising directions for future advancements in structured entity extraction.",
        "pdf_link": "https://arxiv.org/pdf/2402.04437v2.pdf"
    },
    {
        "title": "Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton",
        "authors": [
            "Yiyou Sun",
            "Junjie Hu",
            "Wei Cheng",
            "Haifeng Chen"
        ],
        "published": "2024-02-06T21:14:45Z",
        "summary": "This paper introduces the Definite Finite Automaton augmented large language\nmodel (DFA-LLM), a novel framework designed to enhance the capabilities of\nconversational agents using large language models (LLMs). Traditional LLMs face\nchallenges in generating regulated and compliant responses in special scenarios\nwith predetermined response guidelines, like emotional support and customer\nservice. Our framework addresses these challenges by embedding a Definite\nFinite Automaton (DFA), learned from training dialogues, within the LLM. This\nstructured approach enables the LLM to adhere to a deterministic response\npathway, guided by the DFA. The advantages of DFA-LLM include an interpretable\nstructure through human-readable DFA, context-aware retrieval for responses in\nconversations, and plug-and-play compatibility with existing LLMs. Extensive\nbenchmarks validate DFA-LLM's effectiveness, indicating its potential as a\nvaluable contribution to the conversational agent.",
        "pdf_link": "https://arxiv.org/pdf/2402.04411v1.pdf"
    },
    {
        "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
        "authors": [
            "Zhaoxuan Tan",
            "Qingkai Zeng",
            "Yijun Tian",
            "Zheyuan Liu",
            "Bing Yin",
            "Meng Jiang"
        ],
        "published": "2024-02-06T21:03:52Z",
        "summary": "Personalization in large language models (LLMs) is increasingly important,\naiming to align LLM's interactions, content, and recommendations with\nindividual user preferences. Recent advances in LLM personalization have\nspotlighted effective prompt design, by enriching user queries with\nnon-parametric knowledge through behavior history retrieval and textual\nprofiles. However, these approaches were limited due to a lack of model\nownership, resulting in constrained customization and privacy issues. Moreover,\nthey often failed to accurately capture user behavior patterns, especially in\ncases where user data were complex and dynamic. To address these shortcomings,\nwe introduce One PEFT Per User (OPPU), which employs personalized\nparameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior\npatterns and preferences. By plugging in users' personal PEFT parameters, they\ncan own and use their LLMs personally. OPPU integrates parametric user\nknowledge in the personal PEFT parameters with the non-parametric knowledge\nacquired through retrieval and profile. This integration adapts individual LLMs\nto user behavior shifts. Experimental results demonstrate that OPPU\nsignificantly outperforms existing prompt-based methods across seven diverse\ntasks in the LaMP benchmark. Further in-depth studies reveal OPPU's enhanced\ncapabilities in handling user behavior shifts, modeling users at different\nactive levels, maintaining robustness across various user history formats, and\ndisplaying versatility with different PEFT methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.04401v1.pdf"
    },
    {
        "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
        "authors": [
            "Nate Gruver",
            "Anuroop Sriram",
            "Andrea Madotto",
            "Andrew Gordon Wilson",
            "C. Lawrence Zitnick",
            "Zachary Ulissi"
        ],
        "published": "2024-02-06T20:35:28Z",
        "summary": "We propose fine-tuning large language models for generation of stable\nmaterials. While unorthodox, fine-tuning large language models on text-encoded\natomistic data is simple to implement yet reliable, with around 90% of sampled\nstructures obeying physical constraints on atom positions and charges. Using\nenergy above hull calculations from both learned ML potentials and\ngold-standard DFT calculations, we show that our strongest model (fine-tuned\nLLaMA-2 70B) can generate materials predicted to be metastable at about twice\nthe rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text\nprompting's inherent flexibility, our models can simultaneously be used for\nunconditional generation of stable material, infilling of partial structures\nand text-conditional generation. Finally, we show that language models' ability\nto capture key symmetries of crystal structures improves with model scale,\nsuggesting that the biases of pretrained LLMs are surprisingly well-suited for\natomistic data.",
        "pdf_link": "https://arxiv.org/pdf/2402.04379v1.pdf"
    },
    {
        "title": "Monitoring the evolution of antisemitic discourse on extremist social media using BERT",
        "authors": [
            "Raza Ul Mustafa",
            "Nathalie Japkowicz"
        ],
        "published": "2024-02-06T20:34:49Z",
        "summary": "Racism and intolerance on social media contribute to a toxic online\nenvironment which may spill offline to foster hatred, and eventually lead to\nphysical violence. That is the case with online antisemitism, the specific\ncategory of hatred considered in this study. Tracking antisemitic themes and\ntheir associated terminology over time in online discussions could help monitor\nthe sentiments of their participants and their evolution, and possibly offer\navenues for intervention that may prevent the escalation of hatred. Due to the\nlarge volume and constant evolution of online traffic, monitoring conversations\nmanually is impractical. Instead, we propose an automated method that extracts\nantisemitic themes and terminology from extremist social media over time and\ncaptures their evolution. Since supervised learning would be too limited for\nsuch a task, we created an unsupervised online machine learning approach that\nuses large language models to assess the contextual similarity of posts. The\nmethod clusters similar posts together, dividing, and creating additional\nclusters over time when sub-themes emerge from existing ones or new themes\nappear. The antisemitic terminology used within each theme is extracted from\nthe posts in each cluster. Our experiments show that our methodology\noutperforms existing baselines and demonstrates the kind of themes and\nsub-themes it discovers within antisemitic discourse along with their\nassociated terminology. We believe that our approach will be useful for\nmonitoring the evolution of all kinds of hatred beyond antisemitism on social\nplatforms.",
        "pdf_link": "https://arxiv.org/pdf/2403.05548v1.pdf"
    },
    {
        "title": "The World of Generative AI: Deepfakes and Large Language Models",
        "authors": [
            "Alakananda Mitra",
            "Saraju P. Mohanty",
            "Elias Kougianos"
        ],
        "published": "2024-02-06T20:18:32Z",
        "summary": "We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes\nand Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in\nparticular, pose an alarming threat to society as they are capable of spreading\nmisinformation and changing the truth. LLMs are powerful language models that\ngenerate general-purpose language. However due to its generative aspect, it can\nalso be a risk for people if used with ill intentions. The ethical use of these\ntechnologies is a big concern. This short article tries to find out the\ninterrelationship between them.",
        "pdf_link": "https://arxiv.org/pdf/2402.04373v1.pdf"
    },
    {
        "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
        "authors": [
            "Junhong Shen",
            "Neil Tenenholtz",
            "James Brian Hall",
            "David Alvarez-Melis",
            "Nicolo Fusi"
        ],
        "published": "2024-02-06T20:11:54Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.05140v1.pdf"
    },
    {
        "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
        "authors": [
            "Michael Zhang",
            "Kush Bhatia",
            "Hermann Kumbong",
            "Christopher R\u00e9"
        ],
        "published": "2024-02-06T19:31:26Z",
        "summary": "Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.",
        "pdf_link": "https://arxiv.org/pdf/2402.04347v1.pdf"
    },
    {
        "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
        "authors": [
            "Mengzhou Xia",
            "Sadhika Malladi",
            "Suchin Gururangan",
            "Sanjeev Arora",
            "Danqi Chen"
        ],
        "published": "2024-02-06T19:18:04Z",
        "summary": "Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.",
        "pdf_link": "https://arxiv.org/pdf/2402.04333v2.pdf"
    },
    {
        "title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards",
        "authors": [
            "Chengyu Huang",
            "Zeqiu Wu",
            "Yushi Hu",
            "Wenya Wang"
        ],
        "published": "2024-02-06T19:00:40Z",
        "summary": "While recent Large Language Models (LLMs) have proven useful in answering\nuser queries, they are prone to hallucination, and their responses often lack\ncredibility due to missing references to reliable sources. An intuitive\nsolution to these issues would be to include in-text citations referring to\nexternal documents as evidence. While previous works have directly prompted\nLLMs to generate in-text citations, their performances are far from\nsatisfactory, especially when it comes to smaller LLMs. In this work, we\npropose an effective training framework using fine-grained rewards to teach\nLLMs to generate highly supportive and relevant citations, while ensuring the\ncorrectness of their responses. We also conduct a systematic analysis of\napplying these fine-grained rewards to common LLM training strategies,\ndemonstrating its advantage over conventional practices. We conduct extensive\nexperiments on Question Answering (QA) datasets taken from the ALCE benchmark\nand validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the\nincorporation of fine-grained rewards achieves the best performance among the\nbaselines, even surpassing that of GPT-3.5-turbo.",
        "pdf_link": "https://arxiv.org/pdf/2402.04315v1.pdf"
    },
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
        "authors": [
            "Mantas Mazeika",
            "Long Phan",
            "Xuwang Yin",
            "Andy Zou",
            "Zifan Wang",
            "Norman Mu",
            "Elham Sakhaee",
            "Nathaniel Li",
            "Steven Basart",
            "Bo Li",
            "David Forsyth",
            "Dan Hendrycks"
        ],
        "published": "2024-02-06T18:59:08Z",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.04249v2.pdf"
    },
    {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": [
            "Xiangru Tang",
            "Qiao Jin",
            "Kunlun Zhu",
            "Tongxin Yuan",
            "Yichi Zhang",
            "Wangchunshu Zhou",
            "Meng Qu",
            "Yilun Zhao",
            "Jian Tang",
            "Zhuosheng Zhang",
            "Arman Cohan",
            "Zhiyong Lu",
            "Mark Gerstein"
        ],
        "published": "2024-02-06T18:54:07Z",
        "summary": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, they also introduce novel vulnerabilities that demand careful\nconsideration for safety. However, there exists a notable gap in the\nliterature, as there has been no comprehensive exploration of these\nvulnerabilities. This position paper fills this gap by conducting a thorough\nexamination of vulnerabilities in LLM-based agents within scientific domains,\nshedding light on potential risks associated with their misuse and emphasizing\nthe need for safety measures. We begin by providing a comprehensive overview of\nthe potential risks inherent to scientific LLM agents, taking into account user\nintent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we delve into the origins of these vulnerabilities\nand provide a scoping review of the limited existing works. Based on our\nanalysis, we propose a triadic framework involving human regulation, agent\nalignment, and an understanding of environmental feedback (agent regulation) to\nmitigate these identified risks. Furthermore, we highlight the limitations and\nchallenges associated with safeguarding scientific agents and advocate for the\ndevelopment of improved models, robust benchmarks, and comprehensive\nregulations to address these issues effectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.04247v2.pdf"
    },
    {
        "title": "Can Generative Agents Predict Emotion?",
        "authors": [
            "Ciaran Regan",
            "Nanami Iwahashi",
            "Shogo Tanaka",
            "Mizuki Oka"
        ],
        "published": "2024-02-06T18:39:43Z",
        "summary": "Large Language Models (LLMs) have demonstrated a number of human-like\nabilities, however the empathic understanding and emotional state of LLMs is\nyet to be aligned to that of humans. In this work, we investigate how the\nemotional state of generative LLM agents evolves as they perceive new events,\nintroducing a novel architecture in which new experiences are compared to past\nmemories. Through this comparison, the agent gains the ability to understand\nnew experiences in context, which according to the appraisal theory of emotion\nis vital in emotion creation. First, the agent perceives new experiences as\ntime series text data. After perceiving each new input, the agent generates a\nsummary of past relevant memories, referred to as the norm, and compares the\nnew experience to this norm. Through this comparison we can analyse how the\nagent reacts to the new experience in context. The PANAS, a test of affect, is\nadministered to the agent, capturing the emotional state of the agent after the\nperception of the new event. Finally, the new experience is then added to the\nagents memory to be used in the creation of future norms. By creating multiple\nexperiences in natural language from emotionally charged situations, we test\nthe proposed architecture on a wide range of scenarios. The mixed results\nsuggests that introducing context can occasionally improve the emotional\nalignment of the agent, but further study and comparison with human evaluators\nis necessary. We hope that this paper is another step towards the alignment of\ngenerative agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.04232v2.pdf"
    },
    {
        "title": "Scaling Laws for Downstream Task Performance of Large Language Models",
        "authors": [
            "Berivan Isik",
            "Natalia Ponomareva",
            "Hussein Hazimeh",
            "Dimitris Paparas",
            "Sergei Vassilvitskii",
            "Sanmi Koyejo"
        ],
        "published": "2024-02-06T17:31:20Z",
        "summary": "Scaling laws provide important insights that can guide the design of large\nlanguage models (LLMs). Existing work has primarily focused on studying scaling\nlaws for pretraining (upstream) loss. However, in transfer learning settings,\nin which LLMs are pretrained on an unsupervised dataset and then finetuned on a\ndownstream task, we often also care about the downstream performance. In this\nwork, we study the scaling behavior in a transfer learning setting, where LLMs\nare finetuned for machine translation tasks. Specifically, we investigate how\nthe choice of the pretraining data and its size affect downstream performance\n(translation quality) as judged by two metrics: downstream cross-entropy and\nBLEU score. Our experiments indicate that the size of the finetuning dataset\nand the distribution alignment between the pretraining and downstream data\nsignificantly influence the scaling behavior. With sufficient alignment, both\ndownstream cross-entropy and BLEU score improve monotonically with more\npretraining data. In such cases, we show that it is possible to predict the\ndownstream BLEU score with good accuracy using a log-law. However, there are\nalso cases where moderate misalignment causes the BLEU score to fluctuate or\nget worse with more pretraining, whereas downstream cross-entropy monotonically\nimproves. By analyzing these observations, we provide new practical insights\nfor choosing appropriate pretraining data.",
        "pdf_link": "https://arxiv.org/pdf/2402.04177v1.pdf"
    },
    {
        "title": "Harnessing the Plug-and-Play Controller by Prompting",
        "authors": [
            "Hao Wang",
            "Lei Sha"
        ],
        "published": "2024-02-06T17:18:25Z",
        "summary": "Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.",
        "pdf_link": "https://arxiv.org/pdf/2402.04160v1.pdf"
    },
    {
        "title": "Multi-line AI-assisted Code Authoring",
        "authors": [
            "Omer Dunay",
            "Daniel Cheng",
            "Adam Tait",
            "Parth Thakkar",
            "Peter C Rigby",
            "Andy Chiu",
            "Imad Ahmad",
            "Arun Ganesan",
            "Chandra Maddila",
            "Vijayaraghavan Murali",
            "Ali Tayyebi",
            "Nachiappan Nagappan"
        ],
        "published": "2024-02-06T16:48:50Z",
        "summary": "CodeCompose is an AI-assisted code authoring tool powered by large language\nmodels (LLMs) that provides inline suggestions to 10's of thousands of\ndevelopers at Meta. In this paper, we present how we scaled the product from\ndisplaying single-line suggestions to multi-line suggestions. This evolution\nrequired us to overcome several unique challenges in improving the usability of\nthese suggestions for developers.\n  First, we discuss how multi-line suggestions can have a 'jarring' effect, as\nthe LLM's suggestions constantly move around the developer's existing code,\nwhich would otherwise result in decreased productivity and satisfaction.\n  Second, multi-line suggestions take significantly longer to generate; hence\nwe present several innovative investments we made to reduce the perceived\nlatency for users. These model-hosting optimizations sped up multi-line\nsuggestion latency by 2.5x.\n  Finally, we conduct experiments on 10's of thousands of engineers to\nunderstand how multi-line suggestions impact the user experience and contrast\nthis with single-line suggestions. Our experiments reveal that (i) multi-line\nsuggestions account for 42% of total characters accepted (despite only\naccounting for 16% for displayed suggestions) (ii) multi-line suggestions\nalmost doubled the percentage of keystrokes saved for users from 9% to 17%.\nMulti-line CodeCompose has been rolled out to all engineers at Meta, and less\nthan 1% of engineers have opted out of multi-line suggestions.",
        "pdf_link": "https://arxiv.org/pdf/2402.04141v1.pdf"
    },
    {
        "title": "Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)",
        "authors": [
            "Michael De'Shazer"
        ],
        "published": "2024-02-06T16:47:34Z",
        "summary": "This study consists of a novel approach toward the analysis of court\njudgments spanning five countries, including the United States, the United\nKingdom, Rwanda, Sweden and Hong Kong. This study also explores the\nintersection of the latest advancements in artificial intelligence (AI) and\nlegal analysis, emphasizing the role of AI (specifically generative AI) in\nidentifying human biases and facilitating automated, valid, and coherent\nmultisided argumentation of court judgments with the goal of ensuring\nconsistent application of laws in and across various jurisdictions. By\nincorporating Advanced Language Models (ALMs) and a newly introduced human-AI\ncollaborative framework, this paper seeks to analyze Grounded Theory-based\nresearch design with Advanced Language Models (ALMs) in the practice of law.\nSHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT\ntechnology), focusing on detecting logical inconsistencies and biases across\nvarious legal decisions. SHIRLEY analysis is aggregated and is accompanied by a\ncomparison-oriented AI-based application called SAM (also an ALM) to identify\nrelative deviations in SHIRLEY bias detections. Further, a CRITIC is generated\nwithin semi-autonomous arbitration process via the ALM, SARA. A novel approach\nis introduced in the utilization of an AI arbitrator to critically evaluate\nbiases and qualitative-in-nature nuances identified by the aforementioned AI\napplications (SAM in concert with SHIRLEY), based on the Hague Rules on\nBusiness and Human Rights Arbitration. This Semi-Automated Arbitration Process\n(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring\na nuanced debate-resultant \"understanding\" through a hybrid system of AI and\nhuman-based collaborative analysis.",
        "pdf_link": "https://arxiv.org/pdf/2402.04140v3.pdf"
    },
    {
        "title": "Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science",
        "authors": [
            "Pengfei Liu",
            "Jun Tao",
            "Zhixiang Ren"
        ],
        "published": "2024-02-06T16:12:36Z",
        "summary": "Efficient molecular modeling and design are crucial for the discovery and\nexploration of novel molecules, and the incorporation of deep learning methods\nhas revolutionized this field. In particular, large language models (LLMs)\noffer a fresh approach to tackle scientific problems from a natural language\nprocessing (NLP) perspective, introducing a research paradigm called scientific\nlanguage modeling (SLM). However, two key issues remain: how to quantify the\nmatch between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our pioneering analysis\noffers an exploration of the learning mechanism and paves the way for advancing\nSLM in molecular science.",
        "pdf_link": "https://arxiv.org/pdf/2402.04119v1.pdf"
    },
    {
        "title": "Measuring Implicit Bias in Explicitly Unbiased Large Language Models",
        "authors": [
            "Xuechunzi Bai",
            "Angelina Wang",
            "Ilia Sucholutsky",
            "Thomas L. Griffiths"
        ],
        "published": "2024-02-06T15:59:23Z",
        "summary": "Large language models (LLMs) can pass explicit bias tests but still harbor\nimplicit biases, similar to humans who endorse egalitarian beliefs yet exhibit\nsubtle biases. Measuring such implicit biases can be a challenge: as LLMs\nbecome increasingly proprietary, it may not be possible to access their\nembeddings and apply existing bias measures; furthermore, implicit biases are\nprimarily a concern if they affect the actual decisions that these systems\nmake. We address both of these challenges by introducing two measures of bias\ninspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a\nprompt-based method for revealing implicit bias; and LLM Decision Bias for\ndetecting subtle discrimination in decision-making tasks. Using these measures,\nwe found pervasive human-like stereotype biases in 6 LLMs across 4 social\ndomains (race, gender, religion, health) and 21 categories (weapons, guilt,\nscience, career among others). Our prompt-based measure of implicit bias\ncorrelates with embedding-based methods but better predicts downstream\nbehaviors measured by LLM Decision Bias. This measure is based on asking the\nLLM to decide between individuals, motivated by psychological results\nindicating that relative not absolute evaluations are more related to implicit\nbiases. Using prompt-based measures informed by psychology allows us to\neffectively expose nuanced biases and subtle discrimination in proprietary LLMs\nthat do not show explicit bias on standard benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.04105v1.pdf"
    },
    {
        "title": "The Use of a Large Language Model for Cyberbullying Detection",
        "authors": [
            "Bayode Ogunleye",
            "Babitha Dharmaraj"
        ],
        "published": "2024-02-06T15:46:31Z",
        "summary": "The dominance of social media has added to the channels of bullying for\nperpetrators. Unfortunately, cyberbullying (CB) is the most prevalent\nphenomenon in todays cyber world, and is a severe threat to the mental and\nphysical health of citizens. This opens the need to develop a robust system to\nprevent bullying content from online forums, blogs, and social media platforms\nto manage the impact in our society. Several machine learning (ML) algorithms\nhave been proposed for this purpose. However, their performances are not\nconsistent due to high class imbalance and generalisation issues. In recent\nyears, large language models (LLMs) like BERT and RoBERTa have achieved\nstate-of-the-art (SOTA) results in several natural language processing (NLP)\ntasks. Unfortunately, the LLMs have not been applied extensively for CB\ndetection. In our paper, we explored the use of these models for cyberbullying\n(CB) detection. We have prepared a new dataset (D2) from existing studies\n(Formspring and Twitter). Our experimental results for dataset D1 and D2 showed\nthat RoBERTa outperformed other models.",
        "pdf_link": "https://arxiv.org/pdf/2402.04088v1.pdf"
    },
    {
        "title": "Provably learning a multi-head attention layer",
        "authors": [
            "Sitan Chen",
            "Yuanzhi Li"
        ],
        "published": "2024-02-06T15:39:09Z",
        "summary": "The multi-head attention layer is one of the key components of the\ntransformer architecture that sets it apart from traditional feed-forward\nmodels. Given a sequence length $k$, attention matrices\n$\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and\nprojection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times\nd}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to\n\\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional\ntokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq\n\\sum^m_{i=1}\n\\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.\nIn this work, we initiate the study of provably learning a multi-head attention\nlayer from random examples and give the first nontrivial upper and lower bounds\nfor this problem:\n  - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain\nnon-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns\n$F$ to small error given random labeled examples drawn uniformly from $\\{\\pm\n1\\}^{k\\times d}$.\n  - We prove computational lower bounds showing that in the worst case,\nexponential dependence on $m$ is unavoidable.\n  We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in\nlarge language models, though our techniques naturally extend to standard\ncontinuous settings, e.g. Gaussian. Our algorithm, which is centered around\nusing examples to sculpt a convex body containing the unknown parameters, is a\nsignificant departure from existing provable algorithms for learning\nfeedforward networks, which predominantly exploit algebraic and rotation\ninvariance properties of the Gaussian distribution. In contrast, our analysis\nis more flexible as it primarily relies on various upper and lower tail bounds\nfor the input distribution and \"slices\" thereof.",
        "pdf_link": "https://arxiv.org/pdf/2402.04084v1.pdf"
    },
    {
        "title": "Systematic Biases in LLM Simulations of Debates",
        "authors": [
            "Amir Taubenfeld",
            "Yaniv Dover",
            "Roi Reichart",
            "Ariel Goldstein"
        ],
        "published": "2024-02-06T14:51:55Z",
        "summary": "Recent advancements in natural language processing, especially the emergence\nof Large Language Models (LLMs), have opened exciting possibilities for\nconstructing computational simulations designed to replicate human behavior\naccurately. However, LLMs are complex statistical learners without\nstraightforward deductive rules, making them prone to unexpected behaviors. In\nthis study, we highlight the limitations of LLMs in simulating human\ninteractions, particularly focusing on LLMs' ability to simulate political\ndebates. Our findings indicate a tendency for LLM agents to conform to the\nmodel's inherent social biases despite being directed to debate from certain\npolitical perspectives. This tendency results in behavioral patterns that seem\nto deviate from well-established social dynamics among humans. We reinforce\nthese observations using an automatic self-fine-tuning method, which enables us\nto manipulate the biases within the LLM and demonstrate that agents\nsubsequently align with the altered biases. These results underscore the need\nfor further research to develop methods that help agents overcome these biases,\na critical step toward creating more realistic simulations.",
        "pdf_link": "https://arxiv.org/pdf/2402.04049v1.pdf"
    },
    {
        "title": "LLM Agents can Autonomously Hack Websites",
        "authors": [
            "Richard Fang",
            "Rohan Bindu",
            "Akul Gupta",
            "Qiusi Zhan",
            "Daniel Kang"
        ],
        "published": "2024-02-06T14:46:08Z",
        "summary": "In recent years, large language models (LLMs) have become increasingly\ncapable and can now interact with tools (i.e., call functions), read documents,\nand recursively call themselves. As a result, these LLMs can now function\nautonomously as agents. With the rise in capabilities of these agents, recent\nwork has speculated on how LLM agents would affect cybersecurity. However, not\nmuch is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites,\nperforming tasks as complex as blind database schema extraction and SQL\ninjections without human feedback. Importantly, the agent does not need to know\nthe vulnerability beforehand. This capability is uniquely enabled by frontier\nmodels that are highly capable of tool use and leveraging extended context.\nNamely, we show that GPT-4 is capable of such hacks, but existing open-source\nmodels are not. Finally, we show that GPT-4 is capable of autonomously finding\nvulnerabilities in websites in the wild. Our findings raise questions about the\nwidespread deployment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.06664v3.pdf"
    },
    {
        "title": "Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought",
        "authors": [
            "Alex Havrilla",
            "Maia Iyer"
        ],
        "published": "2024-02-06T13:59:56Z",
        "summary": "During both pretraining and fine-tuning, Large Language Models\n(\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying\nquality. Both phases of training typically involve heuristically filtering out\n``low-quality'' or \\textit{noisy} training samples, yet little is known\nquantitatively about how the type or intensity of noise affects downstream\nperformance. In this work, we study how noise in chain of thought\n(\\textbf{CoT}) impacts task performance in the highly-controlled setting of\nalgorithmically solvable tasks. First, we develop the Traced Integer\n(\\textbf{TInt}) framework to generate highly customizable noised execution\ntraces for any arithmetic function on lists of integers. We then define two\ntypes of noise: \\textit{static} noise, a local form of noise which is applied\nafter the CoT trace is computed, and \\textit{dynamic} noise, a global form of\nnoise which propagates errors in the trace as it is computed. We then evaluate\nthe test performance of pretrained models both prompted and fine-tuned on\nnoised datasets with varying levels of dataset contamination and intensity. We\nfind fine-tuned models are extremely robust to high levels of static noise but\nstruggle significantly more with lower levels of dynamic noise. In contrast,\nfew-shot prompted models appear more sensitive to even static noise. We\nconclude with a discussion of how our findings impact noise filtering\nbest-practices, in particular emphasizing the importance of removing samples\ncontaining destructive dynamic noise with global errors.",
        "pdf_link": "https://arxiv.org/pdf/2402.04004v2.pdf"
    },
    {
        "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
        "authors": [
            "Julien Pierre Edmond Ghali",
            "Kosuke Shima",
            "Koichi Moriyama",
            "Atsuko Mutoh",
            "Nobuhiro Inuzuka"
        ],
        "published": "2024-02-06T13:19:53Z",
        "summary": "In the rapidly changing world of smart technology, searching for documents\nhas become more challenging due to the rise of advanced language models. These\nmodels sometimes face difficulties, like providing inaccurate information,\ncommonly known as \"hallucination.\" This research focuses on addressing this\nissue through Retrieval-Augmented Generation (RAG), a technique that guides\nmodels to give accurate responses based on real facts. To overcome scalability\nissues, the study explores connecting user queries with sophisticated language\nmodels such as BERT and Orca2, using an innovative query optimization process.\nThe study unfolds in three scenarios: first, without RAG, second, without\nadditional assistance, and finally, with extra help. Choosing the compact yet\nefficient Orca2 7B model demonstrates a smart use of computing resources. The\nempirical results indicate a significant improvement in the initial language\nmodel's performance under RAG, particularly when assisted with prompts\naugmenters. Consistency in document retrieval across different encodings\nhighlights the effectiveness of using language model-generated queries. The\nintroduction of UMAP for BERT further simplifies document retrieval while\nmaintaining strong results.",
        "pdf_link": "https://arxiv.org/pdf/2402.16874v1.pdf"
    },
    {
        "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K",
        "authors": [
            "Tao Yuan",
            "Xuefei Ning",
            "Dong Zhou",
            "Zhijie Yang",
            "Shiyao Li",
            "Minghui Zhuang",
            "Zheyue Tan",
            "Zhuyu Yao",
            "Dahua Lin",
            "Boxun Li",
            "Guohao Dai",
            "Shengen Yan",
            "Yu Wang"
        ],
        "published": "2024-02-06T13:11:19Z",
        "summary": "State-of-the-art large language models (LLMs) are now claiming remarkable\nsupported context lengths of 256k or even more. In contrast, the average\ncontext lengths of mainstream benchmarks are insufficient (5k-21k), and they\nsuffer from potential knowledge leakage and inaccurate metrics, resulting in\nbiased evaluation. This paper introduces LV-Eval, a challenging long-context\nbenchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up\nto 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA,\ncomprising 11 bilingual datasets. The design of LV-Eval has incorporated three\nkey techniques, namely confusing facts insertion, keyword and phrase\nreplacement, and keyword-recall-based metric design. The advantages of LV-Eval\ninclude controllable evaluation across different context lengths, challenging\ntest instances with confusing facts, mitigated knowledge leakage, and more\nobjective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation\nstudies on the techniques used in LV-Eval construction. The results reveal\nthat: (i) Commercial LLMs generally outperform open-source LLMs when evaluated\nwithin length levels shorter than their claimed context length. However, their\noverall performance is surpassed by open-source LLMs with longer context\nlengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k, exhibit a\nrelatively gentle degradation of performance, but their absolute performances\nmay not necessarily be higher than those of LLMs with shorter context lengths.\n(iii) LLMs' performances can significantly degrade in the presence of confusing\ninformation, especially in the pressure test of \"needle in a haystack\". (iv)\nIssues related to knowledge leakage and inaccurate metrics introduce bias in\nevaluation, and these concerns are alleviated in LV-Eval. All datasets and\nevaluation codes are released at: https://github.com/infinigence/LVEval.",
        "pdf_link": "https://arxiv.org/pdf/2402.05136v1.pdf"
    },
    {
        "title": "Discovery of the Hidden World with Large Language Models",
        "authors": [
            "Chenxi Liu",
            "Yongqiang Chen",
            "Tongliang Liu",
            "Mingming Gong",
            "James Cheng",
            "Bo Han",
            "Kun Zhang"
        ],
        "published": "2024-02-06T12:18:54Z",
        "summary": "Science originates with discovering new causal knowledge from a combination\nof known facts and observations. Traditional causal discovery approaches mainly\nrely on high-quality measured variables, usually given by human experts, to\nfind causal relations. However, the causal variables are usually unavailable in\na wide range of real-world applications. The rise of large language models\n(LLMs) that are trained to learn rich knowledge from the massive observations\nof the world, provides a new opportunity to assist with discovering high-level\nhidden variables from the raw observational data. Therefore, we introduce COAT:\nCausal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer\nthat extracts the potential causal factors from unstructured data. Moreover,\nLLMs can also be instructed to provide additional information used to collect\ndata values (e.g., annotation criteria) and to further parse the raw\nunstructured data into structured data. The annotated data will be fed to a\ncausal learning module (e.g., the FCI algorithm) that provides both rigorous\nexplanations of the data, as well as useful feedback to further improve the\nextraction of causal factors by LLMs. We verify the effectiveness of COAT in\nuncovering the underlying causal system with two case studies of review rating\nanalysis and neuropathic diagnosis.",
        "pdf_link": "https://arxiv.org/pdf/2402.03941v1.pdf"
    },
    {
        "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
        "authors": [
            "Simone Balloccu",
            "Patr\u00edcia Schmidtov\u00e1",
            "Mateusz Lango",
            "Ond\u0159ej Du\u0161ek"
        ],
        "published": "2024-02-06T11:54:23Z",
        "summary": "Natural Language Processing (NLP) research is increasingly focusing on the\nuse of Large Language Models (LLMs), with some of the most popular ones being\neither fully or partially closed-source. The lack of access to model details,\nespecially regarding training data, has repeatedly raised concerns about data\ncontamination among researchers. Several attempts have been made to address\nthis issue, but they are limited to anecdotal evidence and trial and error.\nAdditionally, they overlook the problem of \\emph{indirect} data leaking, where\nmodels are iteratively improved by using data coming from users. In this work,\nwe conduct the first systematic analysis of work using OpenAI's GPT-3.5 and\nGPT-4, the most prominently used LLMs today, in the context of data\ncontamination. By analysing 255 papers and considering OpenAI's data usage\npolicy, we extensively document the amount of data leaked to these models\nduring the first year after the model's release. We report that these models\nhave been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the\nsame time, we document a number of evaluation malpractices emerging in the\nreviewed papers, such as unfair or missing baseline comparisons and\nreproducibility issues. We release our results as a collaborative project on\nhttps://leak-llm.github.io/, where other researchers can contribute to our\nefforts.",
        "pdf_link": "https://arxiv.org/pdf/2402.03927v2.pdf"
    },
    {
        "title": "Large Language Models to Enhance Bayesian Optimization",
        "authors": [
            "Tennison Liu",
            "Nicol\u00e1s Astorga",
            "Nabeel Seedat",
            "Mihaela van der Schaar"
        ],
        "published": "2024-02-06T11:44:06Z",
        "summary": "Bayesian optimization (BO) is a powerful approach for optimizing complex and\nexpensive-to-evaluate black-box functions. Its importance is underscored in\nmany applications, notably including hyperparameter tuning, but its efficacy\ndepends on efficiently balancing exploration and exploitation. While there has\nbeen substantial progress in BO methods, striking this balance remains a\ndelicate process. In this light, we present LLAMBO, a novel approach that\nintegrates the capabilities of Large Language Models (LLM) within BO. At a high\nlevel, we frame the BO problem in natural language, enabling LLMs to\niteratively propose and evaluate promising solutions conditioned on historical\nevaluations. More specifically, we explore how combining contextual\nunderstanding, few-shot learning proficiency, and domain knowledge of LLMs can\nimprove model-based BO. Our findings illustrate that LLAMBO is effective at\nzero-shot warmstarting, and enhances surrogate modeling and candidate sampling,\nespecially in the early stages of search when observations are sparse. Our\napproach is performed in context and does not require LLM finetuning.\nAdditionally, it is modular by design, allowing individual components to be\nintegrated into existing BO frameworks, or function cohesively as an end-to-end\nmethod. We empirically validate LLAMBO's efficacy on the problem of\nhyperparameter tuning, highlighting strong empirical performance across a range\nof diverse benchmarks, proprietary, and synthetic tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.03921v2.pdf"
    },
    {
        "title": "Can Large Language Models Detect Rumors on Social Media?",
        "authors": [
            "Qiang Liu",
            "Xiang Tao",
            "Junfei Wu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published": "2024-02-06T11:33:57Z",
        "summary": "In this work, we investigate to use Large Language Models (LLMs) for rumor\ndetection on social media. However, it is challenging for LLMs to reason over\nthe entire propagation information on social media, which contains news\ncontents and numerous comments, due to LLMs may not concentrate on key clues in\nthe complex propagation information, and have trouble in reasoning when facing\nmassive and redundant information. Accordingly, we propose an LLM-empowered\nRumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to\nreason over important clues in news and comments, and divide the entire\npropagation information into a Chain-of-Propagation for reducing LLMs' burden.\nWe conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD\noutperforms several state-of-the-art rumor detection models by 3.2% to 7.7%.\nMeanwhile, by applying LLMs, LeRuD requires no data for training, and thus\nshows more promising rumor detection ability in few-shot or zero-shot\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.03916v2.pdf"
    },
    {
        "title": "Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy",
        "authors": [
            "Efe Bozkir",
            "S\u00fcleyman \u00d6zdel",
            "Ka Hei Carrie Lau",
            "Mengdi Wang",
            "Hong Gao",
            "Enkelejda Kasneci"
        ],
        "published": "2024-02-06T11:19:40Z",
        "summary": "Recent developments in computer graphics, hardware, artificial intelligence\n(AI), and human-computer interaction likely lead to extended reality (XR)\ndevices and setups being more pervasive. While these devices and setups provide\nusers with interactive, engaging, and immersive experiences with different\nsensing modalities, such as eye and hand trackers, many non-player characters\nare utilized in a pre-scripted way or by conventional AI techniques. In this\npaper, we argue for using large language models (LLMs) in XR by embedding them\nin virtual avatars or as narratives to facilitate more inclusive experiences\nthrough prompt engineering according to user profiles and fine-tuning the LLMs\nfor particular purposes. We argue that such inclusion will facilitate diversity\nfor XR use. In addition, we believe that with the versatile conversational\ncapabilities of LLMs, users will engage more with XR environments, which might\nhelp XR be more used in everyday life. Lastly, we speculate that combining the\ninformation provided to LLM-powered environments by the users and the biometric\ndata obtained through the sensors might lead to novel privacy invasions. While\nstudying such possible privacy invasions, user privacy concerns and preferences\nshould also be investigated. In summary, despite some challenges, embedding\nLLMs into XR is a promising and novel research area with several opportunities.",
        "pdf_link": "https://arxiv.org/pdf/2402.03907v1.pdf"
    },
    {
        "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
        "authors": [
            "Jongwoo Ko",
            "Sungnyun Kim",
            "Tianyi Chen",
            "Se-Young Yun"
        ],
        "published": "2024-02-06T11:10:35Z",
        "summary": "Knowledge distillation (KD) is widely used for compressing a teacher model to\na smaller student model, reducing its inference cost and memory footprint while\npreserving model capabilities. However, current KD methods for auto-regressive\nsequence models (e.g., large language models) suffer from missing a\nstandardized objective function. Moreover, the recent use of student-generated\noutputs to address training-inference mismatches has significantly escalated\ncomputational costs. To tackle these issues, we introduce DistiLLM, a more\neffective and efficient KD framework for auto-regressive language models.\nDistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence\nloss, where we unveil and leverage its theoretical properties, and (2) an\nadaptive off-policy approach designed to enhance the efficiency in utilizing\nstudent-generated outputs. Extensive experiments, including\ninstruction-following tasks, demonstrate the effectiveness of DistiLLM in\nbuilding high-performing student models while achieving up to 4.3$\\times$\nspeedup compared to recent KD methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.03898v1.pdf"
    },
    {
        "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
        "authors": [
            "Spyridon Mouselinos",
            "Henryk Michalewski",
            "Mateusz Malinowski"
        ],
        "published": "2024-02-06T10:37:21Z",
        "summary": "Large Language Models (LLMs) demonstrate ever-increasing abilities in\nmathematical and algorithmic tasks, yet their geometric reasoning skills are\nunderexplored. We investigate LLMs' abilities in constructive geometric\nproblem-solving one of the most fundamental steps in the development of human\nmathematical reasoning. Our work reveals notable challenges that the\nstate-of-the-art LLMs face in this domain despite many successes in similar\nareas. LLMs exhibit biases in target variable selection and struggle with 2D\nspatial relationships, often misrepresenting and hallucinating objects and\ntheir placements. To this end, we introduce a framework that formulates an\nLLMs-based multi-agents system that enhances their existing reasoning potential\nby conducting an internal dialogue. This work underscores LLMs' current\nlimitations in geometric reasoning and improves geometric reasoning\ncapabilities through self-correction, collaboration, and diverse role\nspecializations.",
        "pdf_link": "https://arxiv.org/pdf/2402.03877v2.pdf"
    },
    {
        "title": "ANLS* -- A Universal Document Processing Metric for Generative Large Language Models",
        "authors": [
            "David Peer",
            "Philemon Sch\u00f6pf",
            "Volckmar Nebendahl",
            "Alexander Rietzler",
            "Sebastian Stabinger"
        ],
        "published": "2024-02-06T09:50:08Z",
        "summary": "Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, 6 different GLLMs and 3\ndifferent prompting methods using the ANLS* metric is also provided,\ndemonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,\nSFT outperforms other techniques and improves the state-of-the-art, sometimes\nby as much as $18$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric",
        "pdf_link": "https://arxiv.org/pdf/2402.03848v3.pdf"
    },
    {
        "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
        "authors": [
            "Wei Huang",
            "Yangdong Liu",
            "Haotong Qin",
            "Ying Li",
            "Shiming Zhang",
            "Xianglong Liu",
            "Michele Magno",
            "Xiaojuan Qi"
        ],
        "published": "2024-02-06T09:26:34Z",
        "summary": "Pretrained large language models (LLMs) exhibit exceptional general language\nprocessing capabilities but come with significant demands on memory and\ncomputational resources. As a powerful compression technology, binarization can\nextremely reduce model weights to a mere 1 bit, lowering the expensive\ncomputation and memory requirements. However, existing quantization techniques\nfall short of maintaining LLM performance under ultra-low bit-widths. In\nresponse to this challenge, we present BiLLM, a groundbreaking 1-bit\npost-training quantization scheme tailored for pretrained LLMs. Based on the\nweight distribution of LLMs, BiLLM first identifies and structurally selects\nsalient weights, and minimizes the compression loss through an effective binary\nresidual approximation strategy. Moreover, considering the bell-shaped\ndistribution of the non-salient weights, we propose an optimal splitting search\nto group and binarize them accurately. BiLLM achieving for the first time\nhigh-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit\nweights across various LLMs families and evaluation metrics, outperforms SOTA\nquantization methods of LLM by significant margins. Moreover, BiLLM enables the\nbinarization process of the LLM with 7 billion weights within 0.5 hours on a\nsingle GPU, demonstrating satisfactory time efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.04291v1.pdf"
    },
    {
        "title": "Rethinking Skill Extraction in the Job Market Domain using Large Language Models",
        "authors": [
            "Khanh Cao Nguyen",
            "Mike Zhang",
            "Syrielle Montariol",
            "Antoine Bosselut"
        ],
        "published": "2024-02-06T09:23:26Z",
        "summary": "Skill Extraction involves identifying skills and qualifications mentioned in\ndocuments such as job postings and resumes. The task is commonly tackled by\ntraining supervised models using a sequence labeling approach with BIO tags.\nHowever, the reliance on manually annotated data limits the generalizability of\nsuch approaches. Moreover, the common BIO setting limits the ability of the\nmodels to capture complex skill patterns and handle ambiguous mentions. In this\npaper, we explore the use of in-context learning to overcome these challenges,\non a benchmark of 6 uniformized skill extraction datasets. Our approach\nleverages the few-shot learning capabilities of large language models (LLMs) to\nidentify and extract skills from sentences. We show that LLMs, despite not\nbeing on par with traditional supervised models in terms of performance, can\nbetter handle syntactically complex skill mentions in skill extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.03832v1.pdf"
    },
    {
        "title": "RevOrder: A Novel Method for Enhanced Arithmetic in Language Models",
        "authors": [
            "Si Shen",
            "Peijun Shen",
            "Danhao Zhu"
        ],
        "published": "2024-02-06T09:10:35Z",
        "summary": "This paper presents RevOrder, a novel technique aimed at improving arithmetic\noperations in large language models (LLMs) by reversing the output digits in\naddition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.\nOur method significantly reduces the Count of Sequential Intermediate Digits\n(CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation\ncomplexity. Through comprehensive testing, RevOrder not only achieves perfect\naccuracy in basic arithmetic operations but also substantially boosts LLM\nperformance in division tasks, particularly with large numbers where\ntraditional models struggle. Implementation of RevOrder is cost-effective for\nboth training and inference phases. Moreover, applying RevOrder to fine-tune\nthe LLaMA2-7B model on the GSM8K math task results in a considerable\nimprovement, reducing equation calculation errors by 46% and increasing overall\nscores from 41.6 to 44.4.",
        "pdf_link": "https://arxiv.org/pdf/2402.03822v2.pdf"
    },
    {
        "title": "ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs",
        "authors": [
            "Zhengyan Zhang",
            "Yixin Song",
            "Guanghui Yu",
            "Xu Han",
            "Yankai Lin",
            "Chaojun Xiao",
            "Chenyang Song",
            "Zhiyuan Liu",
            "Zeyu Mi",
            "Maosong Sun"
        ],
        "published": "2024-02-06T08:45:51Z",
        "summary": "Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.03804v1.pdf"
    },
    {
        "title": "Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning",
        "authors": [
            "Fudan Zheng",
            "Jindong Cao",
            "Weijiang Yu",
            "Zhiguang Chen",
            "Nong Xiao",
            "Yutong Lu"
        ],
        "published": "2024-02-06T07:53:23Z",
        "summary": "Most advances in medical image recognition supporting clinical auxiliary\ndiagnosis meet challenges due to the low-resource situation in the medical\nfield, where annotations are highly expensive and professional. This\nlow-resource problem can be alleviated by leveraging the transferable\nrepresentations of large-scale pre-trained vision-language models via relevant\nmedical text prompts. However, existing pre-trained vision-language models\nrequire domain experts to carefully design the medical prompts, which greatly\nincreases the burden on clinicians. To address this problem, we propose a\nweakly supervised prompt learning method MedPrompt to automatically generate\nmedical prompts, which includes an unsupervised pre-trained vision-language\nmodel and a weakly supervised prompt learning model. The unsupervised\npre-trained vision-language model utilizes the natural correlation between\nmedical images and corresponding medical texts for pre-training, without any\nmanual annotations. The weakly supervised prompt learning model only utilizes\nthe classes of images in the dataset to guide the learning of the specific\nclass vector in the prompt, while the learning of other context vectors in the\nprompt requires no manual annotations for guidance. To the best of our\nknowledge, this is the first model to automatically generate medical prompts.\nWith these prompts, the pre-trained vision-language model can be freed from the\nstrong expert dependency of manual annotation and manual prompt design.\nExperimental results show that the model using our automatically generated\nprompts outperforms its full-shot learning hand-crafted prompts counterparts\nwith only a minimal number of labeled samples for few-shot learning, and\nreaches superior or comparable accuracy on zero-shot image classification. The\nproposed prompt generator is lightweight and therefore can be embedded into any\nnetwork architecture.",
        "pdf_link": "https://arxiv.org/pdf/2402.03783v1.pdf"
    },
    {
        "title": "MolTC: Towards Molecular Relational Modeling In Language Models",
        "authors": [
            "Junfeng Fang",
            "Shuai Zhang",
            "Chang Wu",
            "Zhengyi Yang",
            "Zhiyuan Liu",
            "Sihang Li",
            "Kun Wang",
            "Wenjie Du",
            "Xiang Wang"
        ],
        "published": "2024-02-06T07:51:56Z",
        "summary": "Molecular Relational Learning (MRL), aiming to understand interactions\nbetween molecular pairs, plays a pivotal role in advancing biochemical\nresearch. Recently, the adoption of large language models (LLMs), known for\ntheir vast knowledge repositories and advanced logical inference capabilities,\nhas emerged as a promising way for efficient and effective MRL. Despite their\npotential, these methods predominantly rely on the textual data, thus not fully\nharnessing the wealth of structural information inherent in molecular graphs.\nMoreover, the absence of a unified framework exacerbates the issue of\ninformation underutilization, as it hinders the sharing of interaction\nmechanism learned across diverse datasets. To address these challenges, this\nwork proposes a novel LLM-based multi-modal framework for Molecular inTeraction\nprediction following Chain-of-Thought (CoT) theory, termed MolTC, which\neffectively integrate graphical information of two molecules in pair. For\nachieving a unified MRL, MolTC innovatively develops a dynamic\nparameter-sharing strategy for cross-dataset information sharing. Moreover, to\ntrain MolTC efficiently, we introduce a Multi-hierarchical CoT concept to\nrefine its training paradigm, and conduct a comprehensive Molecular Interactive\nInstructions dataset for the development of biochemical LLMs involving MRL. Our\nexperiments, conducted across various datasets involving over 4,000,000\nmolecular pairs, exhibit the superiority of our method over current GNN and\nLLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.",
        "pdf_link": "https://arxiv.org/pdf/2402.03781v5.pdf"
    },
    {
        "title": "Large Language Models As MOOCs Graders",
        "authors": [
            "Shahriar Golchin",
            "Nikhil Garuda",
            "Christopher Impey",
            "Matthew Wenger"
        ],
        "published": "2024-02-06T07:43:07Z",
        "summary": "Massive open online courses (MOOCs) unlock the doors to free education for\nanyone around the globe with access to a computer and the internet. Despite\nthis democratization of learning, the massive enrollment in these courses means\nit is almost impossible for one instructor to assess every student's writing\nassignment. As a result, peer grading, often guided by a straightforward\nrubric, is the method of choice. While convenient, peer grading often falls\nshort in terms of reliability and validity. In this study, using 18 distinct\nsettings, we explore the feasibility of leveraging large language models (LLMs)\nto replace peer grading in MOOCs. Specifically, we focus on two\nstate-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses:\nIntroductory Astronomy, Astrobiology, and the History and Philosophy of\nAstronomy. To instruct LLMs, we use three different prompts based on a variant\nof the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique:\nZero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT\nin conjunction with both instructor-formulated answers and rubrics; and\nZero-shot-CoT with instructor-offered correct answers and LLM-generated\nrubrics. Our results show that Zero-shot-CoT, when integrated with\ninstructor-provided answers and rubrics, produces grades that are more aligned\nwith those assigned by instructors compared to peer grading. However, the\nHistory and Philosophy of Astronomy course proves to be more challenging in\nterms of grading as opposed to other courses. Finally, our study reveals a\npromising direction for automating grading systems for MOOCs, especially in\nsubjects with well-defined rubrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.03776v4.pdf"
    },
    {
        "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
        "authors": [
            "Tianyang Han",
            "Qing Lian",
            "Rui Pan",
            "Renjie Pi",
            "Jipeng Zhang",
            "Shizhe Diao",
            "Yong Lin",
            "Tong Zhang"
        ],
        "published": "2024-02-06T06:48:46Z",
        "summary": "Large language models (LLMs) have recently experienced remarkable progress,\nwhere the advent of multi-modal large language models (MLLMs) has endowed LLMs\nwith visual capabilities, leading to impressive performances in various\nmulti-modal tasks. However, those powerful MLLMs such as GPT-4V still fail\nspectacularly when presented with certain image and text inputs. In this paper,\nwe identify a typical class of inputs that baffles MLLMs, which consist of\nimages that are highly relevant but inconsistent with answers, causing MLLMs to\nsuffer from hallucination. To quantify the effect, we propose CorrelationQA,\nthe first benchmark that assesses the hallucination level given spurious\nimages. This benchmark contains 7,308 text-image pairs across 13 categories.\nBased on the proposed CorrelationQA, we conduct a thorough analysis on 9\nmainstream MLLMs, illustrating that they universally suffer from this\ninstinctive bias to varying degrees. We hope that our curated benchmark and\nevaluation results aid in better assessments of the MLLMs' robustness in the\npresence of misleading images. The resource is available in\nhttps://github.com/MasaiahHan/CorrelationQA.",
        "pdf_link": "https://arxiv.org/pdf/2402.03757v1.pdf"
    },
    {
        "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
        "authors": [
            "Chao Chen",
            "Kai Liu",
            "Ze Chen",
            "Yi Gu",
            "Yue Wu",
            "Mingyuan Tao",
            "Zhihang Fu",
            "Jieping Ye"
        ],
        "published": "2024-02-06T06:23:12Z",
        "summary": "Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.",
        "pdf_link": "https://arxiv.org/pdf/2402.03744v1.pdf"
    },
    {
        "title": "Similarity-based Neighbor Selection for Graph LLMs",
        "authors": [
            "Rui Li",
            "Jiwei Li",
            "Jiawei Han",
            "Guoyin Wang"
        ],
        "published": "2024-02-06T05:29:05Z",
        "summary": "Text-attributed graphs (TAGs) present unique challenges for direct processing\nby Language Learning Models (LLMs), yet their extensive commonsense knowledge\nand robust reasoning capabilities offer great promise for node classification\nin TAGs. Prior research in this field has grappled with issues such as\nover-squashing, heterophily, and ineffective graph information integration,\nfurther compounded by inconsistencies in dataset partitioning and\nunderutilization of advanced LLMs. To address these challenges, we introduce\nSimilarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor\nselection techniques, SNS effectively improves the quality of selected\nneighbors, thereby improving graph representation and alleviating issues like\nover-squashing and heterophily. Besides, as an inductive and training-free\napproach, SNS demonstrates superior generalization and scalability over\ntraditional GNN methods. Our comprehensive experiments, adhering to standard\ndataset partitioning practices, demonstrate that SNS, through simple prompt\ninteractions with LLMs, consistently outperforms vanilla GNNs and achieves\nstate-of-the-art results on datasets like PubMed in node classification,\nshowcasing LLMs' potential in graph structure understanding. Our research\nfurther underscores the significance of graph structure integration in LLM\napplications and identifies key factors for their success in node\nclassification. Code is available at https://github.com/ruili33/SNS.",
        "pdf_link": "https://arxiv.org/pdf/2402.03720v1.pdf"
    },
    {
        "title": "Automatic Robotic Development through Collaborative Framework by Large Language Models",
        "authors": [
            "Zhirong Luan",
            "Yujun Lai",
            "Rundong Huang",
            "Xiaruiqi Lan",
            "Liangjun Chen",
            "Badong Chen"
        ],
        "published": "2024-02-06T04:40:27Z",
        "summary": "Despite the remarkable code generation abilities of large language models\nLLMs, they still face challenges in complex task handling. Robot development, a\nhighly intricate field, inherently demands human involvement in task allocation\nand collaborative teamwork . To enhance robot development, we propose an\ninnovative automated collaboration framework inspired by real-world robot\ndevelopers. This framework employs multiple LLMs in distinct roles analysts,\nprogrammers, and testers. Analysts delve deep into user requirements, enabling\nprogrammers to produce precise code, while testers fine-tune the parameters\nbased on user feedback for practical robot application. Each LLM tackles\ndiverse, critical tasks within the development process. Clear collaboration\nrules emulate real world teamwork among LLMs. Analysts, programmers, and\ntesters form a cohesive team overseeing strategy, code, and parameter\nadjustments . Through this framework, we achieve complex robot development\nwithout requiring specialized knowledge, relying solely on non experts\nparticipation.",
        "pdf_link": "https://arxiv.org/pdf/2402.03699v2.pdf"
    },
    {
        "title": "Personalized Language Modeling from Personalized Human Feedback",
        "authors": [
            "Xinyu Li",
            "Zachary C. Lipton",
            "Liu Leqi"
        ],
        "published": "2024-02-06T04:18:58Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is the current dominating\nframework to fine-tune large language models to better align with human\npreferences. However, the underlying premise of algorithms developed under this\nframework can be problematic when user preferences encoded in human feedback\nare diverse. In this work, we aim to address this problem by developing methods\nfor building personalized language models. We first formally introduce the task\nof learning from personalized human feedback and explain why vanilla RLHF can\nbe problematic in this context. We then propose a general Personalized-RLHF\n(P-RLHF) framework, which requires one to jointly learn a user model and a\nlanguage (or reward) model. The user model takes in user information and\noutputs user representations. Its structure encodes our assumptions about user\npreferences underlying the feedback data. We develop new learning objectives\nfor personalized reward modeling and personalized Direct Preference\nOptimization. To demonstrate the efficacy of our method, we test it on\nreal-world text summarization data with annotated preferences and annotator\ninformation. We fine-tune GPT-J 6B to obtain personalized language (and reward)\nmodels, which outperform non-personalized models in terms of aligning with\nindividual preferences.",
        "pdf_link": "https://arxiv.org/pdf/2402.05133v1.pdf"
    },
    {
        "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
        "authors": [
            "Yanfang Zhang",
            "Yiliu Sun",
            "Yibing Zhan",
            "Dapeng Tao",
            "Dacheng Tao",
            "Chen Gong"
        ],
        "published": "2024-02-06T03:41:12Z",
        "summary": "Recently, increasing attention has been focused drawn on to improve the\nability of Large Language Models (LLMs) to perform complex reasoning. However,\nprevious methods, such as Chain-of-Thought and Self-Consistency, mainly follow\nDirect Reasoning (DR) frameworks, so they will meet difficulty in solving\nnumerous real-world tasks which can hardly be solved via DR. Therefore, to\nstrengthen the reasoning power of LLMs, this paper proposes a novel Indirect\nReasoning (IR) method that employs the logic of contrapositives and\ncontradictions to tackle IR tasks such as factual reasoning and mathematic\nproof. Specifically, our methodology comprises two steps. Firstly, we leverage\nthe logical equivalence of contrapositive to augment the data and rules to\nenhance the comprehensibility of LLMs. Secondly, we design a set of prompt\ntemplates to trigger LLMs to conduct IR based on proof by contradiction that is\nlogically equivalent to the original DR process. Our IR method is simple yet\neffective and can be straightforwardly integrated with existing DR methods to\nfurther boost the reasoning abilities of LLMs. The experimental results on\npopular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method\nenhances the overall accuracy of factual reasoning by 27.33% and mathematical\nproof by 31.43%, when compared with traditional DR methods. Moreover, the\nmethods combining IR and DR significantly outperform the methods solely using\nIR or DR, further demonstrating the effectiveness of our strategy.",
        "pdf_link": "https://arxiv.org/pdf/2402.03667v1.pdf"
    },
    {
        "title": "Limits of Large Language Models in Debating Humans",
        "authors": [
            "James Flamino",
            "Mohammed Shahid Modi",
            "Boleslaw K. Szymanski",
            "Brendan Cross",
            "Colton Mikolajczyk"
        ],
        "published": "2024-02-06T03:24:27Z",
        "summary": "Large Language Models (LLMs) have shown remarkable promise in their ability\nto interact proficiently with humans. Subsequently, their potential use as\nartificial confederates and surrogates in sociological experiments involving\nconversation is an exciting prospect. But how viable is this idea? This paper\nendeavors to test the limits of current-day LLMs with a pre-registered study\nintegrating real people with LLM agents acting as people. The study focuses on\ndebate-based opinion consensus formation in three environments: humans only,\nagents and humans, and agents only. Our goal is to understand how LLM agents\ninfluence humans, and how capable they are in debating like humans. We find\nthat LLMs can blend in and facilitate human productivity but are less\nconvincing in debate, with their behavior ultimately deviating from human's. We\nelucidate these primary failings and anticipate that LLMs must evolve further\nbefore being viable debaters.",
        "pdf_link": "https://arxiv.org/pdf/2402.06049v1.pdf"
    },
    {
        "title": "Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models",
        "authors": [
            "Kelvin J. L. Koa",
            "Yunshan Ma",
            "Ritchie Ng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-06T03:18:58Z",
        "summary": "Explaining stock predictions is generally a difficult task for traditional\nnon-generative deep learning models, where explanations are limited to\nvisualizing the attention weights on important texts. Today, Large Language\nModels (LLMs) present a solution to this problem, given their known\ncapabilities to generate human-readable explanations for their decision-making\nprocess. However, the task of stock prediction remains challenging for LLMs, as\nit requires the ability to weigh the varying impacts of chaotic social texts on\nstock prices. The problem gets progressively harder with the introduction of\nthe explanation component, which requires LLMs to explain verbally why certain\nfactors are more important than the others. On the other hand, to fine-tune\nLLMs for such a task, one would need expert-annotated samples of explanation\nfor every stock movement in the training set, which is expensive and\nimpractical to scale. To tackle these issues, we propose our\nSummarize-Explain-Predict (SEP) framework, which utilizes a self-reflective\nagent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to\ngenerate explainable stock predictions in a fully autonomous manner. The\nreflective agent learns how to explain past stock movements through\nself-reasoning, while the PPO trainer trains the model to generate the most\nlikely explanations from input texts. The training samples for the PPO trainer\nare also the responses generated during the reflective process, which\neliminates the need for human annotators. Using our SEP framework, we fine-tune\na LLM that can outperform both traditional deep-learning and LLM methods in\nprediction accuracy and Matthews correlation coefficient for the stock\nclassification task. To justify the generalization capability of our framework,\nwe further test it on the portfolio construction task, and demonstrate its\neffectiveness through various portfolio metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.03659v3.pdf"
    },
    {
        "title": "Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue",
        "authors": [
            "Kun Ouyang",
            "Liqiang Jing",
            "Xuemeng Song",
            "Meng Liu",
            "Yupeng Hu",
            "Liqiang Nie"
        ],
        "published": "2024-02-06T03:14:46Z",
        "summary": "Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which\naims to generate a natural language explanation for the given sarcastic\ndialogue that involves multiple modalities (i.e., utterance, video, and audio).\nAlthough existing studies have achieved great success based on the generative\npretrained language model BART, they overlook exploiting the sentiments\nresiding in the utterance, video and audio, which are vital clues for sarcasm\nexplanation. In fact, it is non-trivial to incorporate sentiments for boosting\nSED performance, due to three main challenges: 1) diverse effects of utterance\ntokens on sentiments; 2) gap between video-audio sentiment signals and the\nembedding space of BART; and 3) various relations among utterances, utterance\nsentiments, and video-audio sentiments. To tackle these challenges, we propose\na novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation\nframework, named EDGE. In particular, we first propose a lexicon-guided\nutterance sentiment inference module, where a heuristic utterance sentiment\nrefinement strategy is devised. We then develop a module named Joint Cross\nAttention-based Sentiment Inference (JCA-SI) by extending the multimodal\nsentiment analysis model JCA to derive the joint sentiment label for each\nvideo-audio clip. Thereafter, we devise a context-sentiment graph to\ncomprehensively model the semantic relations among the utterances, utterance\nsentiments, and video-audio sentiments, to facilitate sarcasm explanation\ngeneration. Extensive experiments on the publicly released dataset WITS verify\nthe superiority of our model over cutting-edge methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.03658v1.pdf"
    },
    {
        "title": "Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context",
        "authors": [
            "Yichen Li",
            "Yun Peng",
            "Yintong Huo",
            "Michael R. Lyu"
        ],
        "published": "2024-02-06T01:59:41Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ncompletion, as evidenced by their essential roles in developing code assistant\nservices such as Copilot. Being trained on in-file contexts, current LLMs are\nquite effective in completing code for single source files. However, it is\nchallenging for them to conduct repository-level code completion for large\nsoftware projects that require cross-file information. Existing research on\nLLM-based repository-level code completion identifies and integrates cross-file\ncontexts, but it suffers from low accuracy and limited context length of LLMs.\nIn this paper, we argue that Integrated Development Environments (IDEs) can\nprovide direct, accurate and real-time cross-file information for\nrepository-level code completion. We propose IDECoder, a practical framework\nthat leverages IDE native static contexts for cross-context construction and\ndiagnosis results for self-refinement. IDECoder utilizes the rich cross-context\ninformation available in IDEs to enhance the capabilities of LLMs of\nrepository-level code completion. We conducted preliminary experiments to\nvalidate the performance of IDECoder and observed that this synergy represents\na promising trend for future exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.03630v2.pdf"
    },
    {
        "title": "Partially Recentralization Softmax Loss for Vision-Language Models Robustness",
        "authors": [
            "Hao Wang",
            "Xin Zhang",
            "Jinzhe Jiang",
            "Yaqian Zhao",
            "Chen Li"
        ],
        "published": "2024-02-06T01:44:38Z",
        "summary": "As Large Language Models make a breakthrough in natural language processing\ntasks (NLP), multimodal technique becomes extremely popular. However, it has\nbeen shown that multimodal NLP are vulnerable to adversarial attacks, where the\noutputs of a model can be dramatically changed by a perturbation to the input.\nWhile several defense techniques have been proposed both in computer vision and\nNLP models, the multimodal robustness of models have not been fully explored.\nIn this paper, we study the adversarial robustness provided by modifying loss\nfunction of pre-trained multimodal models, by restricting top K softmax\noutputs. Based on the evaluation and scoring, our experiments show that after a\nfine-tuning, adversarial robustness of pre-trained models can be significantly\nimproved, against popular attacks. Further research should be studying, such as\noutput diversity, generalization and the robustness-performance trade-off of\nthis kind of loss functions. Our code will be available after this paper is\naccepted",
        "pdf_link": "https://arxiv.org/pdf/2402.03627v1.pdf"
    },
    {
        "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "authors": [
            "Pei Zhou",
            "Jay Pujara",
            "Xiang Ren",
            "Xinyun Chen",
            "Heng-Tze Cheng",
            "Quoc V. Le",
            "Ed H. Chi",
            "Denny Zhou",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng"
        ],
        "published": "2024-02-06T01:13:53Z",
        "summary": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\nare challenging for typical prompting methods. Core to the framework is a\nself-discovery process where LLMs select multiple atomic reasoning modules such\nas critical thinking and step-by-step thinking, and compose them into an\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\nsubstantially improves GPT-4 and PaLM 2's performance on challenging reasoning\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\nthe self-discovered reasoning structures are universally applicable across\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\ncommonalities with human reasoning patterns.",
        "pdf_link": "https://arxiv.org/pdf/2402.03620v1.pdf"
    },
    {
        "title": "Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning",
        "authors": [
            "Trilok Padhi",
            "Ugur Kursuncu",
            "Yaman Kumar",
            "Valerie L. Shalin",
            "Lane Peterson Fronczek"
        ],
        "published": "2024-02-06T00:51:27Z",
        "summary": "The prevalence of smart devices with the ability to capture moments in\nmultiple modalities has enabled users to experience multimodal information\nonline. However, large Language (LLMs) and Vision models (LVMs) are still\nlimited in capturing holistic meaning with cross-modal semantic relationships.\nWithout explicit, common sense knowledge (e.g., as a knowledge graph), Visual\nLanguage Models (VLMs) only learn implicit representations by capturing\nhigh-level patterns in vast corpora, missing essential contextual cross-modal\ncues. In this work, we design a framework to couple explicit commonsense\nknowledge in the form of knowledge graphs with large VLMs to improve the\nperformance of a downstream task, predicting the effectiveness of multi-modal\nmarketing campaigns. While the marketing application provides a compelling\nmetric for assessing our methods, our approach enables the early detection of\nlikely persuasive multi-modal campaigns and the assessment and augmentation of\nmarketing theory.",
        "pdf_link": "https://arxiv.org/pdf/2402.03607v1.pdf"
    },
    {
        "title": "Distinguishing the Knowable from the Unknowable with Language Models",
        "authors": [
            "Gustaf Ahdritz",
            "Tian Qin",
            "Nikhil Vyas",
            "Boaz Barak",
            "Benjamin L. Edelman"
        ],
        "published": "2024-02-05T22:22:49Z",
        "summary": "We study the feasibility of identifying epistemic uncertainty (reflecting a\nlack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in\nthe underlying distribution), in the outputs of large language models (LLMs)\nover free-form text. In the absence of ground-truth probabilities, we explore a\nsetting where, in order to (approximately) disentangle a given LLM's\nuncertainty, a significantly larger model stands in as a proxy for the ground\ntruth. We show that small linear probes trained on the embeddings of frozen,\npretrained models accurately predict when larger models will be more confident\nat the token level and that probes trained on one text domain generalize to\nothers. Going further, we propose a fully unsupervised method that achieves\nnon-trivial accuracy on the same task. Taken together, we interpret these\nresults as evidence that LLMs naturally contain internal representations of\ndifferent types of uncertainty that could potentially be leveraged to devise\nmore informative indicators of model confidence in diverse practical settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.03563v2.pdf"
    },
    {
        "title": "Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains",
        "authors": [
            "Sanjana Ramprasad",
            "Kundan Krishna",
            "Zachary C Lipton",
            "Byron C Wallace"
        ],
        "published": "2024-02-05T20:51:11Z",
        "summary": "Recent work has shown that large language models (LLMs) are capable of\ngenerating summaries zero-shot (i.e., without explicit supervision) that, under\nhuman assessment, are often comparable or even preferred to manually composed\nreference summaries. However, this prior work has focussed almost exclusively\non evaluating news article summarization. How do zero-shot summarizers perform\nin other (potentially more specialized) domains? In this work we evaluate\nzero-shot generated summaries across specialized domains including biomedical\narticles, and legal bills (in addition to standard news benchmarks for\nreference). We focus especially on the factuality of outputs. We acquire\nannotations from domain experts to identify inconsistencies in summaries and\nsystematically categorize these errors. We analyze whether the prevalence of a\ngiven domain in the pretraining corpus affects extractiveness and faithfulness\nof generated summaries of articles in this domain. We release all collected\nannotations to facilitate additional research toward measuring and realizing\nfactually accurate summarization, beyond news articles. The dataset can be\ndownloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains",
        "pdf_link": "https://arxiv.org/pdf/2402.03509v1.pdf"
    },
    {
        "title": "Neural networks for abstraction and reasoning: Towards broad generalization in machines",
        "authors": [
            "Mikel Bober-Irizar",
            "Soumya Banerjee"
        ],
        "published": "2024-02-05T20:48:57Z",
        "summary": "For half a century, artificial intelligence research has attempted to\nreproduce the human qualities of abstraction and reasoning - creating computer\nsystems that can learn new concepts from a minimal set of examples, in settings\nwhere humans find this easy. While specific neural networks are able to solve\nan impressive range of problems, broad generalisation to situations outside\ntheir training data has proved elusive.In this work, we look at several novel\napproaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of\nabstract visual reasoning tasks introduced to test algorithms on broad\ngeneralization. Despite three international competitions with $100,000 in\nprizes, the best algorithms still fail to solve a majority of ARC tasks and\nrely on complex hand-crafted rules, without using machine learning at all. We\nrevisit whether recent advances in neural networks allow progress on this task.\n  First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.\nDreamCoder automatically writes programs in a bespoke domain-specific language\nto perform reasoning, using a neural network to mimic human intuition. We\npresent the Perceptual Abstraction and Reasoning Language (PeARL) language,\nwhich allows DreamCoder to solve ARC tasks, and propose a new recognition model\nthat allows us to significantly improve on the previous best implementation.We\nalso propose a new encoding and augmentation scheme that allows large language\nmodels (LLMs) to solve ARC tasks, and find that the largest models can solve\nsome ARC tasks. LLMs are able to solve a different group of problems to\nstate-of-the-art solvers, and provide an interesting way to complement other\napproaches. We perform an ensemble analysis, combining models to achieve better\nresults than any system alone. Finally, we publish the arckit Python library to\nmake future research on ARC easier.",
        "pdf_link": "https://arxiv.org/pdf/2402.03507v1.pdf"
    },
    {
        "title": "Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues",
        "authors": [
            "Xingpeng Sun",
            "Haoming Meng",
            "Souradip Chakraborty",
            "Amrit Singh Bedi",
            "Aniket Bera"
        ],
        "published": "2024-02-05T20:11:56Z",
        "summary": "This work highlights a critical shortcoming in text-based Large Language\nModels (LLMs) used for human-robot interaction, demonstrating that text alone\nas a conversation modality falls short in such applications. While LLMs excel\nin processing text in these human conversations, they struggle with the nuances\nof verbal instructions in scenarios like social navigation, where ambiguity and\nuncertainty can erode trust in robotic and other AI systems. We can address\nthis shortcoming by moving beyond text and additionally focusing on the\nparalinguistic features of these audio responses. These features are the\naspects of spoken communication that do not involve the literal wording\n(lexical content) but convey meaning and nuance through how something is said.\nWe present \"Beyond Text\"; an approach that improves LLM decision-making by\nintegrating audio transcription along with a subsection of these features,\nwhich focus on the affect and more relevant in human-robot conversations. This\napproach not only achieves a 70.26% winning rate, outperforming existing LLMs\nby 48.30%, but also enhances robustness against token manipulation adversarial\nattacks, highlighted by a 22.44% less decrease ratio than the text-only\nlanguage model in winning rate. \"Beyond Text\" marks an advancement in social\nrobot navigation and broader Human-Robot interactions, seamlessly integrating\ntext-based guidance with human-audio-informed language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.03494v1.pdf"
    },
    {
        "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
        "authors": [
            "Pranab Sahoo",
            "Ayush Kumar Singh",
            "Sriparna Saha",
            "Vinija Jain",
            "Samrat Mondal",
            "Aman Chadha"
        ],
        "published": "2024-02-05T19:49:13Z",
        "summary": "Prompt engineering has emerged as an indispensable technique for extending\nthe capabilities of large language models (LLMs) and vision-language models\n(VLMs). This approach leverages task-specific instructions, known as prompts,\nto enhance model efficacy without modifying the core model parameters. Rather\nthan updating the model parameters, prompts allow seamless integration of\npre-trained models into downstream tasks by eliciting desired model behaviors\nsolely based on the given prompt. Prompts can be natural language instructions\nthat provide context to guide the model or learned vector representations that\nactivate relevant knowledge. This burgeoning field has enabled success across\nvarious applications, from question-answering to commonsense reasoning.\nHowever, there remains a lack of systematic organization and understanding of\nthe diverse prompt engineering methods and techniques. This survey paper\naddresses the gap by providing a structured overview of recent advancements in\nprompt engineering, categorized by application area. For each prompting\napproach, we provide a summary detailing the prompting methodology, its\napplications, the models involved, and the datasets utilized. We also delve\ninto the strengths and limitations of each approach and include a taxonomy\ndiagram and table summarizing datasets, models, and critical points of each\nprompting technique. This systematic analysis enables a better understanding of\nthis rapidly developing field and facilitates future research by illuminating\nopen challenges and opportunities for prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2402.07927v1.pdf"
    },
    {
        "title": "Arabic Synonym BERT-based Adversarial Examples for Text Classification",
        "authors": [
            "Norah Alshahrani",
            "Saied Alshahrani",
            "Esma Wali",
            "Jeanna Matthews"
        ],
        "published": "2024-02-05T19:39:07Z",
        "summary": "Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.03477v1.pdf"
    },
    {
        "title": "Nevermind: Instruction Override and Moderation in Large Language Models",
        "authors": [
            "Edward Kim"
        ],
        "published": "2024-02-05T18:58:19Z",
        "summary": "Given the impressive capabilities of recent Large Language Models (LLMs), we\ninvestigate and benchmark the most popular proprietary and different sized open\nsource models on the task of explicit instruction following in conflicting\nsituations, e.g. overrides. These include the ability of the model to override\nthe knowledge within the weights of the model, the ability to override (or\nmoderate) extracted knowledge in the prompt, and lastly the ability to perform\na full jailbreak. Experimentation performed suggest several key findings to\nimprove instruction following - larger models perform the best in following\ninstructions that override internal and contextual instructions, and are\nobedient, even to a fault. When scaling to longer contexts via rope scaling, a\nsignificant buffer needs to be maintained from the edge of the perplexity cliff\nin order to maintain instruction following capabilities. Finally, we observe\nimproving instruction following, and subsequently instruction\noverrides/jailbreaks, is fundamentally at odds with the ability of a language\nmodel to follow given safety filters or guidelines. Thus, we postulate the most\neffective approach for safe, trustworthy AI should be dealt external to the LLM\nitself.",
        "pdf_link": "https://arxiv.org/pdf/2402.03303v1.pdf"
    },
    {
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "authors": [
            "Zhihong Shao",
            "Peiyi Wang",
            "Qihao Zhu",
            "Runxin Xu",
            "Junxiao Song",
            "Mingchuan Zhang",
            "Y. K. Li",
            "Y. Wu",
            "Daya Guo"
        ],
        "published": "2024-02-05T18:55:32Z",
        "summary": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.03300v2.pdf"
    },
    {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models",
        "authors": [
            "Haibo Jin",
            "Ruoxi Chen",
            "Andy Zhou",
            "Jinyin Chen",
            "Yang Zhang",
            "Haohan Wang"
        ],
        "published": "2024-02-05T18:54:43Z",
        "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
        "pdf_link": "https://arxiv.org/pdf/2402.03299v3.pdf"
    },
    {
        "title": "Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS",
        "authors": [
            "Matthew DeLorenzo",
            "Animesh Basak Chowdhury",
            "Vasudev Gohil",
            "Shailja Thakur",
            "Ramesh Karri",
            "Siddharth Garg",
            "Jeyavijayan Rajendran"
        ],
        "published": "2024-02-05T18:47:04Z",
        "summary": "Existing large language models (LLMs) for register transfer level code\ngeneration face challenges like compilation failures and suboptimal power,\nperformance, and area (PPA) efficiency. This is due to the lack of PPA\nawareness in conventional transformer decoding algorithms. In response, we\npresent an automated transformer decoding algorithm that integrates Monte Carlo\ntree-search for lookahead, guiding the transformer to produce compilable,\nfunctionally correct, and PPA-optimized code. Empirical evaluation with a\nfine-tuned language model on RTL codesets shows that our proposed technique\nconsistently generates functionally correct code compared to prompting-only\nmethods and effectively addresses the PPA-unawareness drawback of naive large\nlanguage models. For the largest design generated by the state-of-the-art LLM\n(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay\nproduct.",
        "pdf_link": "https://arxiv.org/pdf/2402.03289v1.pdf"
    },
    {
        "title": "Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",
        "authors": [
            "Anthony Sicilia",
            "Hyunwoo Kim",
            "Khyathi Raghavi Chandu",
            "Malihe Alikhani",
            "Jack Hessel"
        ],
        "published": "2024-02-05T18:39:47Z",
        "summary": "Effective interlocutors account for the uncertain goals, beliefs, and\nemotions of others. But even the best human conversationalist cannot perfectly\nanticipate the trajectory of a dialogue. How well can language models represent\ninherent uncertainty in conversations? We propose FortUne Dial, an expansion of\nthe long-standing \"conversation forecasting\" task: instead of just accuracy,\nevaluation is conducted with uncertainty-aware metrics, effectively enabling\nabstention on individual instances. We study two ways in which language models\npotentially represent outcome uncertainty (internally, using scores and\ndirectly, using tokens) and propose fine-tuning strategies to improve\ncalibration of both representations. Experiments on eight difficult negotiation\ncorpora demonstrate that our proposed fine-tuning strategies (a traditional\nsupervision strategy and an off-policy reinforcement learning strategy) can\ncalibrate smaller open-source models to compete with pre-trained models 10x\ntheir size.",
        "pdf_link": "https://arxiv.org/pdf/2402.03284v1.pdf"
    },
    {
        "title": "A Framework for Partially Observed Reward-States in RLHF",
        "authors": [
            "Chinmaya Kausik",
            "Mirco Mutti",
            "Aldo Pacchiano",
            "Ambuj Tewari"
        ],
        "published": "2024-02-05T18:38:55Z",
        "summary": "The study of reinforcement learning from human feedback (RLHF) has gained\nprominence in recent years due to its role in the development of LLMs.\nNeuroscience research shows that human responses to stimuli are known to depend\non partially-observed \"internal states.\" Unfortunately current models of RLHF\ndo not take take this into consideration. Moreover most RLHF models do not\naccount for intermediate feedback, which is gaining importance in empirical\nwork and can help improve both sample complexity and alignment. To address\nthese limitations, we model RLHF as reinforcement learning with partially\nobserved reward-states (PORRL). We show reductions from the the two dominant\nforms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For\ncardinal feedback, we develop generic statistically efficient algorithms and\ninstantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we\nshow that a naive reduction to cardinal feedback fails to achieve sublinear\ndueling regret. We then present the first explicit reduction that converts\nguarantees for cardinal regret to dueling regret. We show that our models and\nguarantees in both settings generalize and extend existing ones. Finally, we\nidentify a recursive structure on our model that could improve the statistical\nand computational tractability of PORRL, giving examples from past work on RLHF\nas well as learning perfect reward machines, which PORRL subsumes.",
        "pdf_link": "https://arxiv.org/pdf/2402.03282v1.pdf"
    },
    {
        "title": "MobilityGPT: Enhanced Human Mobility Modeling with a GPT model",
        "authors": [
            "Ammar Haydari",
            "Dongjie Chen",
            "Zhengfeng Lai",
            "Chen-Nee Chuah"
        ],
        "published": "2024-02-05T18:22:21Z",
        "summary": "Generative models have shown promising results in capturing human mobility\ncharacteristics and generating synthetic trajectories. However, it remains\nchallenging to ensure that the generated geospatial mobility data is\nsemantically realistic, including consistent location sequences, and reflects\nreal-world characteristics, such as constraining on geospatial limits. To\naddress these issues, we reformat human mobility modeling as an autoregressive\ngeneration task, leveraging Generative Pre-trained Transformer (GPT). To ensure\nits controllable generation to alleviate the above challenges, we propose a\ngeospatially-aware generative model, MobilityGPT. We propose a gravity-based\nsampling method to train a transformer for semantic sequence similarity. Then,\nwe constrained the training process via a road connectivity matrix that\nprovides the connectivity of sequences in trajectory generation, thereby\nkeeping generated trajectories in geospatial limits. Lastly, we constructed a\nReinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel\ndistance between training and the synthetically generated trajectories. Our\nexperiments on real-world datasets demonstrate that MobilityGPT outperforms\nstate-of-the-art methods in generating high-quality mobility trajectories that\nare closest to real data in terms of origin-destination similarity, trip\nlength, travel radius, link, and gravity distributions.",
        "pdf_link": "https://arxiv.org/pdf/2402.03264v1.pdf"
    },
    {
        "title": "English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts",
        "authors": [
            "Patrick Barei\u00df",
            "Roman Klinger",
            "Jeremy Barnes"
        ],
        "published": "2024-02-05T17:36:19Z",
        "summary": "Emotion classification in text is a challenging task due to the processes\ninvolved when interpreting a textual description of a potential emotion\nstimulus. In addition, the set of emotion categories is highly domain-specific.\nFor instance, literature analysis might require the use of aesthetic emotions\n(e.g., finding something beautiful), and social media analysis could benefit\nfrom fine-grained sets (e.g., separating anger from annoyance) than only those\nthat represent basic categories as they have been proposed by Paul Ekman\n(anger, disgust, fear, joy, surprise, sadness). This renders the task an\ninteresting field for zero-shot classifications, in which the label set is not\nknown at model development time. Unfortunately, most resources for emotion\nanalysis are English, and therefore, most studies on emotion analysis have been\nperformed in English, including those that involve prompting language models\nfor text labels. This leaves us with a research gap that we address in this\npaper: In which language should we prompt for emotion labels on non-English\ntexts? This is particularly of interest when we have access to a multilingual\nlarge language model, because we could request labels with English prompts even\nfor non-English data. Our experiments with natural language inference-based\nlanguage models show that it is consistently better to use English prompts even\nif the data is in a different language.",
        "pdf_link": "https://arxiv.org/pdf/2402.03223v4.pdf"
    },
    {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "authors": [
            "Xiang Chen",
            "Chenxi Wang",
            "Yida Xue",
            "Ningyu Zhang",
            "Xiaoyan Yang",
            "Qiang Li",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published": "2024-02-05T16:56:11Z",
        "summary": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2402.03190v3.pdf"
    },
    {
        "title": "LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System",
        "authors": [
            "Yan Zhao",
            "Zhongyun Li",
            "Yushan Pan",
            "Jiaxing Wang",
            "Yihong Wang"
        ],
        "published": "2024-02-05T16:47:17Z",
        "summary": "Generative Artificial Intelligence (AI), because of its emergent abilities,\nhas empowered various fields, one typical of which is large language models\n(LLMs). One of the typical application fields of Generative AI is large\nlanguage models (LLMs), and the natural language understanding capability of\nLLM is dramatically improved when compared with conventional AI-based methods.\nThe natural language understanding capability has always been a barrier to the\nintent recognition performance of the Knowledge-Based-Question-and-Answer\n(KBQA) system, which arises from linguistic diversity and the newly appeared\nintent. Conventional AI-based methods for intent recognition can be divided\ninto semantic parsing-based and model-based approaches. However, both of the\nmethods suffer from limited resources in intent recognition. To address this\nissue, we propose a novel KBQA system based on a Large Language Model(LLM) and\nBERT (LB-KBQA). With the help of generative AI, our proposed method could\ndetect newly appeared intent and acquire new knowledge. In experiments on\nfinancial domain question answering, our model has demonstrated superior\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.05130v2.pdf"
    },
    {
        "title": "Empowering Time Series Analysis with Large Language Models: A Survey",
        "authors": [
            "Yushan Jiang",
            "Zijie Pan",
            "Xikun Zhang",
            "Sahil Garg",
            "Anderson Schneider",
            "Yuriy Nevmyvaka",
            "Dongjin Song"
        ],
        "published": "2024-02-05T16:46:35Z",
        "summary": "Recently, remarkable progress has been made over large language models\n(LLMs), demonstrating their unprecedented capability in varieties of natural\nlanguage tasks. However, completely training a large general-purpose model from\nthe scratch is challenging for time series analysis, due to the large volumes\nand varieties of time series data, as well as the non-stationarity that leads\nto concept drift impeding continuous model adaptation and re-training. Recent\nadvances have shown that pre-trained LLMs can be exploited to capture complex\ndependencies in time series data and facilitate various applications. In this\nsurvey, we provide a systematic overview of existing methods that leverage LLMs\nfor time series analysis. Specifically, we first state the challenges and\nmotivations of applying language models in the context of time series as well\nas brief preliminaries of LLMs. Next, we summarize the general pipeline for\nLLM-based time series analysis, categorize existing methods into different\ngroups (i.e., direct query, tokenization, prompt design, fine-tune, and model\nintegration), and highlight the key ideas within each group. We also discuss\nthe applications of LLMs for both general and spatial-temporal time series\ndata, tailored to specific domains. Finally, we thoroughly discuss future\nresearch opportunities to empower time series analysis with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.03182v1.pdf"
    },
    {
        "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
        "authors": [
            "Mintong Kang",
            "Nezihe Merve G\u00fcrel",
            "Ning Yu",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-02-05T16:46:16Z",
        "summary": "Despite the impressive capabilities of large language models (LLMs) across\ndiverse applications, they still suffer from trustworthiness issues, such as\nhallucinations and misalignments. Retrieval-augmented language models (RAG)\nhave been proposed to enhance the credibility of generations by grounding\nexternal knowledge, but the theoretical understandings of their generation\nrisks remains unexplored. In this paper, we answer: 1) whether RAG can indeed\nlead to low generation risks, 2) how to provide provable guarantees on the\ngeneration risks of RAG and vanilla LLMs, and 3) what sufficient conditions\nenable RAG models to reduce generation risks. We propose C-RAG, the first\nframework to certify generation risks for RAG models. Specifically, we provide\nconformal risk analysis for RAG models and certify an upper confidence bound of\ngeneration risks, which we refer to as conformal generation risk. We also\nprovide theoretical guarantees on conformal generation risks for general\nbounded risk functions under test distribution shifts. We prove that RAG\nachieves a lower conformal generation risk than that of a single LLM when the\nquality of the retrieval model and transformer is non-trivial. Our intensive\nempirical results demonstrate the soundness and tightness of our conformal\ngeneration risk guarantees across four widely-used NLP datasets on four\nstate-of-the-art retrieval models.",
        "pdf_link": "https://arxiv.org/pdf/2402.03181v3.pdf"
    },
    {
        "title": "CIDAR: Culturally Relevant Instruction Dataset For Arabic",
        "authors": [
            "Zaid Alyafeai",
            "Khalid Almubarak",
            "Ahmed Ashraf",
            "Deema Alnuhait",
            "Saied Alshahrani",
            "Gubran A. Q. Abdulrahman",
            "Gamil Ahmed",
            "Qais Gawah",
            "Zead Saleh",
            "Mustafa Ghaleb",
            "Yousef Ali",
            "Maged S. Al-Shaibani"
        ],
        "published": "2024-02-05T16:44:17Z",
        "summary": "Instruction tuning has emerged as a prominent methodology for teaching Large\nLanguage Models (LLMs) to follow instructions. However, current instruction\ndatasets predominantly cater to English or are derived from English-dominated\nLLMs, resulting in inherent biases toward Western culture. This bias\nsignificantly impacts the linguistic structures of non-English languages such\nas Arabic, which has a distinct grammar reflective of the diverse cultures\nacross the Arab region. This paper addresses this limitation by introducing\nCIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic\ninstruction-tuning dataset culturally-aligned by human reviewers. CIDAR\ncontains 10,000 instruction and output pairs that represent the Arab region. We\ndiscuss the cultural relevance of CIDAR via the analysis and comparison to\nother models fine-tuned on other datasets. Our experiments show that CIDAR can\nhelp enrich research efforts in aligning LLMs with the Arabic culture. All the\ncode is available at https://github.com/ARBML/CIDAR.",
        "pdf_link": "https://arxiv.org/pdf/2402.03177v1.pdf"
    },
    {
        "title": "The Matrix: A Bayesian learning model for LLMs",
        "authors": [
            "Siddhartha Dalal",
            "Vishal Misra"
        ],
        "published": "2024-02-05T16:42:10Z",
        "summary": "In this paper, we introduce a Bayesian learning model to understand the\nbehavior of Large Language Models (LLMs). We explore the optimization metric of\nLLMs, which is based on predicting the next token, and develop a novel model\ngrounded in this principle. Our approach involves constructing an ideal\ngenerative text model represented by a multinomial transition probability\nmatrix with a prior, and we examine how LLMs approximate this matrix. We\ndiscuss the continuity of the mapping between embeddings and multinomial\ndistributions, and present the Dirichlet approximation theorem to approximate\nany prior. Additionally, we demonstrate how text generation by LLMs aligns with\nBayesian learning principles and delve into the implications for in-context\nlearning, specifically explaining why in-context learning emerges in larger\nmodels where prompts are considered as samples to be updated. Our findings\nindicate that the behavior of LLMs is consistent with Bayesian Learning,\noffering new insights into their functioning and potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.03175v1.pdf"
    },
    {
        "title": "MULTI: Multimodal Understanding Leaderboard with Text and Images",
        "authors": [
            "Zichen Zhu",
            "Yang Xu",
            "Lu Chen",
            "Jingkai Yang",
            "Yichuan Ma",
            "Yiming Sun",
            "Hailin Wen",
            "Jiaqi Liu",
            "Jinyu Cai",
            "Yingzi Ma",
            "Situo Zhang",
            "Zihan Zhao",
            "Liangtai Sun",
            "Kai Yu"
        ],
        "published": "2024-02-05T16:41:02Z",
        "summary": "Rapid progress in multimodal large language models (MLLMs) highlights the\nneed to introduce challenging yet realistic benchmarks to the academic\ncommunity, while existing benchmarks primarily focus on understanding simple\nnatural images and short context. In this paper, we present MULTI as a\ncutting-edge benchmark for evaluating MLLMs on understanding complex tables and\nimages, and reasoning with long context. MULTI provides multimodal inputs and\nrequires responses that are either precise or open-ended, reflecting real-life\nexamination styles. MULTI includes over 18,000 questions and challenges MLLMs\nwith a variety of tasks, ranging from formula derivation to image detail\nanalysis and cross-modality reasoning. We also introduce MULTI-Elite, a\n500-question selected hard subset, and MULTI-Extend, with more than 4,500\nexternal knowledge context pieces. Our evaluation indicates significant\npotential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on\nMULTI, in contrast to other MLLMs scoring between 28.5% and 55.3%. MULTI serves\nnot only as a robust evaluation platform but also paves the way for the\ndevelopment of expert-level AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.03173v2.pdf"
    },
    {
        "title": "Homograph Attacks on Maghreb Sentiment Analyzers",
        "authors": [
            "Fatima Zahra Qachfar",
            "Rakesh M. Verma"
        ],
        "published": "2024-02-05T16:39:15Z",
        "summary": "We examine the impact of homograph attacks on the Sentiment Analysis (SA)\ntask of different Arabic dialects from the Maghreb North-African countries.\nHomograph attacks result in a 65.3% decrease in transformer classification from\nan F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this\nstudy is to highlight LLMs weaknesses' and to prioritize ethical and\nresponsible Machine Learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.03171v1.pdf"
    },
    {
        "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
        "authors": [
            "Yang Jin",
            "Zhicheng Sun",
            "Kun Xu",
            "Kun Xu",
            "Liwei Chen",
            "Hao Jiang",
            "Quzhe Huang",
            "Chengru Song",
            "Yuliang Liu",
            "Di Zhang",
            "Yang Song",
            "Kun Gai",
            "Yadong Mu"
        ],
        "published": "2024-02-05T16:30:49Z",
        "summary": "In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models will be available at\nhttps://video-lavit.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.03161v2.pdf"
    },
    {
        "title": "Constrained Decoding for Cross-lingual Label Projection",
        "authors": [
            "Duong Minh Le",
            "Yang Chen",
            "Alan Ritter",
            "Wei Xu"
        ],
        "published": "2024-02-05T15:57:32Z",
        "summary": "Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a\npopular learning paradigm for low-resource languages with no labeled training\ndata. However, for NLP tasks that involve fine-grained predictions on words and\nphrases, the performance of zero-shot cross-lingual transfer learning lags far\nbehind supervised fine-tuning methods. Therefore, it is common to exploit\ntranslation and label projection to further improve the performance by (1)\ntranslating training data that is available in a high-resource language (e.g.,\nEnglish) together with the gold labels into low-resource languages, and/or (2)\ntranslating test data in low-resource languages to a high-source language to\nrun inference on, then projecting the predicted span-level labels back onto the\noriginal test data. However, state-of-the-art marker-based label projection\nmethods suffer from translation quality degradation due to the extra label\nmarkers injected in the input to the translation model. In this work, we\nexplore a new direction that leverages constrained decoding for label\nprojection to overcome the aforementioned issues. Our new method not only can\npreserve the quality of translated texts but also has the versatility of being\napplicable to both translating training and translating test data strategies.\nThis versatility is crucial as our experiments reveal that translating test\ndata can lead to a considerable boost in performance compared to translating\nonly training data. We evaluate on two cross-lingual transfer tasks, namely\nNamed Entity Recognition and Event Argument Extraction, spanning 20 languages.\nThe results demonstrate that our approach outperforms the state-of-the-art\nmarker-based method by a large margin and also shows better performance than\nother label projection methods that rely on external word alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.03131v1.pdf"
    },
    {
        "title": "Evaluation of ChatGPT Usability as A Code Generation Tool",
        "authors": [
            "Tanha Miah",
            "Hong Zhu"
        ],
        "published": "2024-02-05T15:56:19Z",
        "summary": "With the rapid advance of machine learning (ML) technology, large language\nmodels (LLMs) are increasingly explored as an intelligent tool to generate\nprogram code from natural language specifications. However, existing\nevaluations of LLMs have focused on their capabilities in comparison with\nhumans. It is desirable to evaluate their usability when deciding on whether to\nuse a LLM in software production. This paper proposes a user centric method. It\nincludes metadata in the test cases of a benchmark to describe their usages,\nconducts testing in a multi-attempt process that mimic the uses of LLMs,\nmeasures LLM generated solutions on a set of quality attributes that reflect\nusability, and evaluates the performance based on user experiences in the uses\nof LLMs as a tool. The paper reports an application of the method in the\nevaluation of ChatGPT usability as a code generation tool for the R programming\nlanguage. Our experiments demonstrated that ChatGPT is highly useful for\ngenerating R program code although it may fail on hard programming tasks. The\nuser experiences are good with overall average number of attempts being 1.61\nand the average time of completion being 47.02 seconds. Our experiments also\nfound that the weakest aspect of usability is conciseness, which has a score of\n3.80 out of 5. Our experiment also shows that it is hard for human developers\nto learn from experiences to improve the skill of using ChatGPT to generate\ncode.",
        "pdf_link": "https://arxiv.org/pdf/2402.03130v2.pdf"
    },
    {
        "title": "Best Practices for Text Annotation with Large Language Models",
        "authors": [
            "Petter T\u00f6rnberg"
        ],
        "published": "2024-02-05T15:43:50Z",
        "summary": "Large Language Models (LLMs) have ushered in a new era of text annotation, as\ntheir ease-of-use, high accuracy, and relatively low costs have meant that\ntheir use has exploded in recent months. However, the rapid growth of the field\nhas meant that LLM-based annotation has become something of an academic Wild\nWest: the lack of established practices and standards has led to concerns about\nthe quality and validity of research. Researchers have warned that the\nostensible simplicity of LLMs can be misleading, as they are prone to bias,\nmisunderstandings, and unreliable results. Recognizing the transformative\npotential of LLMs, this paper proposes a comprehensive set of standards and\nbest practices for their reliable, reproducible, and ethical use. These\nguidelines span critical areas such as model selection, prompt engineering,\nstructured prompting, prompt stability analysis, rigorous model validation, and\nthe consideration of ethical and legal implications. The paper emphasizes the\nneed for a structured, directed, and formalized approach to using LLMs, aiming\nto ensure the integrity and robustness of text annotation practices, and\nadvocates for a nuanced and critical engagement with LLMs in social scientific\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2402.05129v1.pdf"
    },
    {
        "title": "Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases",
        "authors": [
            "Elad Levi",
            "Eli Brosh",
            "Matan Friedmann"
        ],
        "published": "2024-02-05T15:28:43Z",
        "summary": "Prompt engineering is a challenging and important task due to the high\nsensitivity of Large Language Models (LLMs) to the given prompt and the\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\nis essential to achieve optimized performance from LLMs. Recent studies have\ndemonstrated the capabilities of LLMs to automatically conduct prompt\nengineering by employing a meta-prompt that incorporates the outcomes of the\nlast trials and proposes an improved prompt. However, this requires a\nhigh-quality benchmark to compare different prompts, which is difficult and\nexpensive to acquire in many real-world use cases. In this work, we introduce a\nnew method for automatic prompt engineering, using a calibration process that\niteratively refines the prompt to the user intent. During the optimization\nprocess, the system jointly generates synthetic data of boundary use cases and\noptimizes the prompt according to the generated dataset. We demonstrate the\neffectiveness of our method with respect to strong proprietary models on\nreal-world tasks such as moderation and generation. Our method outperforms\nstate-of-the-art methods with a limited number of annotated samples.\nFurthermore, we validate the advantages of each one of the system's key\ncomponents. Our system is built in a modular way, facilitating easy adaptation\nto other tasks. The code is available\n$\\href{https://github.com/Eladlev/AutoPrompt}{here}$.",
        "pdf_link": "https://arxiv.org/pdf/2402.03099v1.pdf"
    },
    {
        "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
        "authors": [
            "\u00c1lvaro Mart\u00edn-Cortinas",
            "Daniel S\u00e1ez-Trigueros",
            "Iv\u00e1n Vall\u00e9s-P\u00e9rez",
            "Biel Tura-Vecino",
            "Piotr Bili\u0144ski",
            "Mateusz Lajszczak",
            "Grzegorz Beringer",
            "Roberto Barra-Chicote",
            "Jaime Lorenzo-Trueba"
        ],
        "published": "2024-02-05T15:08:19Z",
        "summary": "Large Language Models (LLMs) are one of the most promising technologies for\nthe next era of speech generation systems, due to their scalability and\nin-context learning capabilities. Nevertheless, they suffer from multiple\nstability issues at inference time, such as hallucinations, content skipping or\nspeech repetitions. In this work, we introduce a new self-supervised Voice\nConversion (VC) architecture which can be used to learn to encode transitory\nfeatures, such as content, separately from stationary ones, such as speaker ID\nor recording conditions, creating speaker-disentangled representations. Using\nspeaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the\nLLM to generate the content and the style of the speech only from the text,\nsimilarly to humans, while the speaker identity is provided by the decoder of\nthe VC model. Results show that LLMs trained over speaker-disentangled\nself-supervised representations provide an improvement of 4.7pp in speaker\nsimilarity over SOTA entangled representations, and a word error rate (WER)\n5.4pp lower. Furthermore, they achieve higher naturalness than human recordings\nof the LibriTTS test-other dataset. Finally, we show that using explicit\nreference embedding negatively impacts intelligibility (stability), with WER\nincreasing by 14pp compared to the model that only uses text to infer the\nstyle.",
        "pdf_link": "https://arxiv.org/pdf/2402.03407v1.pdf"
    },
    {
        "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
        "authors": [
            "Junjie Fang",
            "Likai Tang",
            "Hongzhe Bi",
            "Yujia Qin",
            "Si Sun",
            "Zhenyu Li",
            "Haolun Li",
            "Yongjian Li",
            "Xin Cong",
            "Yukun Yan",
            "Xiaodong Shi",
            "Sen Song",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-05T13:47:53Z",
        "summary": "Long-context processing is a critical ability that constrains the\napplicability of large language models. Although there exist various methods\ndevoted to enhancing the long-context processing ability of large language\nmodels (LLMs), they are developed in an isolated manner and lack systematic\nanalysis and integration of their strengths, hindering further developments. In\nthis paper, we introduce UniMem, a unified framework that reformulates existing\nlong-context methods from the view of memory augmentation of LLMs. UniMem is\ncharacterized by four key dimensions: Memory Management, Memory Writing, Memory\nReading, and Memory Injection, providing a systematic theory for understanding\nvarious long-context methods. We reformulate 16 existing methods based on\nUniMem and analyze four representative methods: Transformer-XL, Memorizing\nTransformer, RMT, and Longformer into equivalent UniMem forms to reveal their\ndesign principles and strengths. Based on these analyses, we propose UniMix, an\ninnovative approach that integrates the strengths of these algorithms.\nExperimental results show that UniMix achieves superior performance in handling\nlong contexts with significantly lower perplexity than baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.03009v1.pdf"
    },
    {
        "title": "Conversation Reconstruction Attack Against GPT Models",
        "authors": [
            "Junjie Chu",
            "Zeyang Sha",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2024-02-05T13:18:42Z",
        "summary": "In recent times, significant advancements have been made in the field of\nlarge language models (LLMs), represented by GPT series models. To optimize\ntask execution, users often engage in multi-round conversations with GPT models\nhosted in cloud environments. These multi-round conversations, potentially\nreplete with private information, require transmission and storage within the\ncloud. However, this operational paradigm introduces additional attack\nsurfaces. In this paper, we first introduce a specific Conversation\nReconstruction Attack targeting GPT models. Our introduced Conversation\nReconstruction Attack is composed of two steps: hijacking a session and\nreconstructing the conversations. Subsequently, we offer an exhaustive\nevaluation of the privacy risks inherent in conversations when GPT models are\nsubjected to the proposed attack. However, GPT-4 demonstrates certain\nrobustness to the proposed attacks. We then introduce two advanced attacks\naimed at better reconstructing previous conversations, specifically the UNR\nattack and the PBU attack. Our experimental findings indicate that the PBU\nattack yields substantial performance across all models, achieving semantic\nsimilarity scores exceeding 0.60, while the UNR attack is effective solely on\nGPT-3.5. Our results reveal the concern about privacy risks associated with\nconversations involving GPT models and aim to draw the community's attention to\nprevent the potential misuse of these models' remarkable capabilities. We will\nresponsibly disclose our findings to the suppliers of related large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.02987v1.pdf"
    },
    {
        "title": "Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing",
        "authors": [
            "Zihan Ma",
            "Yongshang Li",
            "Ronggui Ma",
            "Chen Liang"
        ],
        "published": "2024-02-05T13:16:12Z",
        "summary": "Two challenges are presented when parsing road scenes in UAV images. First,\nthe high resolution of UAV images makes processing difficult. Second,\nsupervised deep learning methods require a large amount of manual annotations\nto train robust and accurate models. In this paper, an unsupervised road\nparsing framework that leverages recent advances in vision language models and\nfundamental computer vision model is introduced.Initially, a vision language\nmodel is employed to efficiently process ultra-large resolution UAV images to\nquickly detect road regions of interest in the images. Subsequently, the vision\nfoundation model SAM is utilized to generate masks for the road regions without\ncategory information. Following that, a self-supervised representation learning\nnetwork extracts feature representations from all masked regions. Finally, an\nunsupervised clustering algorithm is applied to cluster these feature\nrepresentations and assign IDs to each cluster. The masked regions are combined\nwith the corresponding IDs to generate initial pseudo-labels, which initiate an\niterative self-training process for regular semantic segmentation. The proposed\nmethod achieves an impressive 89.96% mIoU on the development dataset without\nrelying on any manual annotation. Particularly noteworthy is the extraordinary\nflexibility of the proposed method, which even goes beyond the limitations of\nhuman-defined categories and is able to acquire knowledge of new categories\nfrom the dataset itself.",
        "pdf_link": "https://arxiv.org/pdf/2402.02985v1.pdf"
    },
    {
        "title": "Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation",
        "authors": [
            "Hessa Abdulrahman Alawwad",
            "Areej Alhothali",
            "Usman Naseem",
            "Ali Alkhathlan",
            "Amani Jamal"
        ],
        "published": "2024-02-05T11:58:56Z",
        "summary": "Textbook question answering (TQA) is a challenging task in artificial\nintelligence due to the complex nature of context and multimodal data. Although\nprevious research has significantly improved the task, there are still some\nlimitations including the models' weak reasoning and inability to capture\ncontextual information in the lengthy context. The introduction of large\nlanguage models (LLMs) has revolutionized the field of AI, however, directly\napplying LLMs often leads to inaccurate answers. This paper proposes a\nmethodology that handle the out-of-domain scenario in TQA where concepts are\nspread across different lessons by incorporating the retrieval augmented\ngeneration (RAG) technique and utilize transfer learning to handle the long\ncontext and enhance reasoning abilities. Through supervised fine-tuning of the\nLLM model Llama-2 and the incorporation of RAG, our architecture outperforms\nthe baseline, achieving a 4.12% accuracy improvement on validation set and\n9.84% on test set for non-diagram multiple-choice questions.",
        "pdf_link": "https://arxiv.org/pdf/2402.05128v2.pdf"
    },
    {
        "title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
        "authors": [
            "Ivar Frisch",
            "Mario Giulianelli"
        ],
        "published": "2024-02-05T11:05:20Z",
        "summary": "While both agent interaction and personalisation are vibrant topics in\nresearch on large language models (LLMs), there has been limited focus on the\neffect of language interaction on the behaviour of persona-conditioned LLM\nagents. Such an endeavour is important to ensure that agents remain consistent\nto their assigned traits yet are able to engage in open, naturalistic\ndialogues. In our experiments, we condition GPT-3.5 on personality profiles\nthrough prompting and create a two-group population of LLM agents using a\nsimple variability-inducing sampling algorithm. We then administer personality\ntests and submit the agents to a collaborative writing task, finding that\ndifferent profiles exhibit different degrees of personality consistency and\nlinguistic alignment to their conversational partners. Our study seeks to lay\nthe groundwork for better understanding of dialogue-based interaction between\nLLMs and highlights the need for new approaches to crafting robust, more\nhuman-like LLM personas for interactive environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.02896v1.pdf"
    },
    {
        "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
        "authors": [
            "Bo-Kyeong Kim",
            "Geonmin Kim",
            "Tae-Ho Kim",
            "Thibault Castells",
            "Shinkook Choi",
            "Junho Shin",
            "Hyoung-Kyu Song"
        ],
        "published": "2024-02-05T09:44:49Z",
        "summary": "Structured pruning of modern large language models (LLMs) has emerged as a\nway of decreasing their high computational needs. Width pruning reduces the\nsize of projection weight matrices (e.g., by removing attention heads) while\nmaintaining the number of layers. Depth pruning, in contrast, removes entire\nlayers or blocks, while keeping the size of the remaining weights unchanged.\nMost current research focuses on either width-only or a blend of width and\ndepth pruning, with little comparative analysis between the two units (width\nvs. depth) concerning their impact on LLM inference efficiency. In this work,\nwe show that a simple depth pruning approach can compete with recent width\npruning methods in terms of zero-shot task performance. Our pruning method\nboosts inference speeds, especially under memory-constrained conditions that\nrequire limited batch sizes for running LLMs, where width pruning is\nineffective. We hope this work can help deploy LLMs on local and edge devices.",
        "pdf_link": "https://arxiv.org/pdf/2402.02834v1.pdf"
    },
    {
        "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
        "authors": [
            "Jasper Dekoninck",
            "Mark Niklas M\u00fcller",
            "Maximilian Baader",
            "Marc Fischer",
            "Martin Vechev"
        ],
        "published": "2024-02-05T09:10:32Z",
        "summary": "Large language models are widespread, with their performance on benchmarks\nfrequently guiding user preferences for one model over another. However, the\nvast amount of data these models are trained on can inadvertently lead to\ncontamination with public benchmarks, thus compromising performance\nmeasurements. While recently developed contamination detection methods try to\naddress this issue, they overlook the possibility of deliberate contamination\nby malicious model providers aiming to evade detection. We argue that this\nsetting is of crucial importance as it casts doubt on the reliability of public\nbenchmarks. To more rigorously study this issue, we propose a categorization of\nboth model providers and contamination detection methods. This reveals\nvulnerabilities in existing methods that we exploit with EAL, a simple yet\neffective contamination technique that significantly inflates benchmark\nperformance while completely evading current detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.02823v2.pdf"
    },
    {
        "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning",
        "authors": [
            "Fangru Lin",
            "Emanuele La Malfa",
            "Valentin Hofmann",
            "Elle Michelle Yang",
            "Anthony Cohn",
            "Janet B. Pierrehumbert"
        ],
        "published": "2024-02-05T08:26:33Z",
        "summary": "Reasoning about asynchronous plans is challenging since it requires\nsequential and parallel planning to optimize time costs. Can large language\nmodels (LLMs) succeed at this task? Here, we present the first large-scale\nstudy investigating this question. We find that a representative set of closed\nand open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not\nsupplied with illustrations about the task-solving process in our benchmark\nAsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that\ncombines graphs with natural language prompts and achieves state-of-the-art\nresults. We show that although PLaG can boost model performance, LLMs still\nsuffer from drastic degradation when task complexity increases, highlighting\nthe limits of utilizing LLMs for simulating digital devices. We see our study\nas an exciting step towards using LLMs as efficient autonomous agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.02805v1.pdf"
    },
    {
        "title": "Large Language Model Distilling Medication Recommendation Model",
        "authors": [
            "Qidong Liu",
            "Xian Wu",
            "Xiangyu Zhao",
            "Yuanshao Zhu",
            "Zijian Zhang",
            "Feng Tian",
            "Yefeng Zheng"
        ],
        "published": "2024-02-05T08:25:22Z",
        "summary": "The recommendation of medication is a vital aspect of intelligent healthcare\nsystems, as it involves prescribing the most suitable drugs based on a\npatient's specific health needs. Unfortunately, many sophisticated models\ncurrently in use tend to overlook the nuanced semantics of medical data, while\nonly relying heavily on identities. Furthermore, these models face significant\nchallenges in handling cases involving patients who are visiting the hospital\nfor the first time, as they lack prior prescription histories to draw upon. To\ntackle these issues, we harness the powerful semantic comprehension and\ninput-agnostic characteristics of Large Language Models (LLMs). Our research\naims to transform existing medication recommendation methodologies using LLMs.\nIn this paper, we introduce a novel approach called Large Language Model\nDistilling Medication Recommendation (LEADER). We begin by creating appropriate\nprompt templates that enable LLMs to suggest medications effectively. However,\nthe straightforward integration of LLMs into recommender systems leads to an\nout-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a\nnovel output layer and a refined tuning loss function. Although LLM-based\nmodels exhibit remarkable capabilities, they are plagued by high computational\ncosts during inference, which is impractical for the healthcare sector. To\nmitigate this, we have developed a feature-level knowledge distillation\ntechnique, which transfers the LLM's proficiency to a more compact model.\nExtensive experiments conducted on two real-world datasets, MIMIC-III and\nMIMIC-IV, demonstrate that our proposed model not only delivers effective\nresults but also is efficient. To ease the reproducibility of our experiments,\nwe release the implementation code online.",
        "pdf_link": "https://arxiv.org/pdf/2402.02803v1.pdf"
    },
    {
        "title": "KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models",
        "authors": [
            "Fei Yuan",
            "Chang Ma",
            "Shuai Yuan",
            "Qiushi Sun",
            "Lei Li"
        ],
        "published": "2024-02-05T08:19:56Z",
        "summary": "The lottery ticket hypothesis posits the existence of ``winning tickets''\nwithin a randomly initialized neural network. Do winning tickets exist for LLMs\nin fine-tuning scenarios? How can we find such winning tickets? In this paper,\nwe propose KS-Lottery, a method to identify a small subset of LLM parameters\nhighly effective in multilingual fine-tuning. Our key idea is to use\nKolmogorov-Smirnov Test to analyze the distribution shift of parameters before\nand after fine-tuning. We further theoretically prove that KS-Lottery can find\nthe certified winning tickets in the embedding layer, fine-tuning on the found\nparameters is guaranteed to perform as well as full fine-tuning. Comparing\nKS-Lottery with other parameter-efficient tuning algorithms on translation\ntasks, the experimental results show that KS-Lottery finds a much smaller set\nof parameters for fine-tuning while achieving the comparable performance as\nfull fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens'\nembedding of LLaMA suffices to reach the fine-tuning translation performance.\nCode and model will be released to the public.",
        "pdf_link": "https://arxiv.org/pdf/2402.02801v1.pdf"
    },
    {
        "title": "Rethinking Optimization and Architecture for Tiny Language Models",
        "authors": [
            "Yehui Tang",
            "Fangcheng Liu",
            "Yunsheng Ni",
            "Yuchuan Tian",
            "Zheyuan Bai",
            "Yi-Qi Hu",
            "Sichao Liu",
            "Shangling Jui",
            "Kai Han",
            "Yunhe Wang"
        ],
        "published": "2024-02-05T07:59:38Z",
        "summary": "The power of large language models (LLMs) has been demonstrated through\nnumerous data and computing resources. However, the application of language\nmodels on mobile devices is facing huge challenge on the computation and memory\ncosts, that is, tiny language models with high performance are urgently\nrequired. Limited by the highly complex training process, there are many\ndetails for optimizing language models that are seldom studied carefully. In\nthis study, based on a tiny language model with 1B parameters, we carefully\ndesign a series of empirical study to analyze the effect of each component.\nThree perspectives are mainly discussed, \\ie, neural architecture, parameter\ninitialization, and optimization strategy. Several design formulas are\nempirically proved especially effective for tiny language models, including\ntokenizer compression, architecture tweaking, parameter inheritance and\nmultiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B\nPro on 1.6T multilingual corpora, following the established formulas.\nExperimental results demonstrate the improved optimization and architecture\nyield a notable average improvement of 8.87 on benchmark evaluation sets for\nPanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA\nmodels with larger model sizes, validating its superior performance. The code\nis available at https://github.com/YuchuanTian/RethinkTinyLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.02791v2.pdf"
    },
    {
        "title": "List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Jun Xu",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-05T06:52:53Z",
        "summary": "The results of information retrieval (IR) are usually presented in the form\nof a ranked list of candidate documents, such as web search for humans and\nretrieval-augmented generation for large language models (LLMs). List-aware\nretrieval aims to capture the list-level contextual features to return a better\nlist, mainly including reranking and truncation. Reranking finely re-scores the\ndocuments in the list. Truncation dynamically determines the cut-off point of\nthe ranked list to achieve the trade-off between overall relevance and avoiding\nmisinformation from irrelevant documents. Previous studies treat them as two\nseparate tasks and model them separately. However, the separation is not\noptimal. First, it is hard to share the contextual information of the ranking\nlist between the two tasks. Second, the separate pipeline usually meets the\nerror accumulation problem, where the small error from the reranking stage can\nlargely affect the truncation stage. To solve these problems, we propose a\nReranking-Truncation joint model (GenRT) that can perform the two tasks\nconcurrently. GenRT integrates reranking and truncation via generative paradigm\nbased on encoder-decoder architecture. We also design the novel loss functions\nfor joint optimization to make the model learn both tasks. Sharing parameters\nby the joint model is conducive to making full use of the common modeling\ninformation of the two tasks. Besides, the two tasks are performed concurrently\nand co-optimized to solve the error accumulation problem between separate\nstages. Experiments on public learning-to-rank benchmarks and open-domain Q\\&A\ntasks show that our method achieves SOTA performance on both reranking and\ntruncation tasks for web search and retrieval-augmented LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02764v1.pdf"
    },
    {
        "title": "DeAL: Decoding-time Alignment for Large Language Models",
        "authors": [
            "James Y. Huang",
            "Sailik Sengupta",
            "Daniele Bonadiman",
            "Yi-an Lai",
            "Arshit Gupta",
            "Nikolaos Pappas",
            "Saab Mansour",
            "Katrin Kirchhoff",
            "Dan Roth"
        ],
        "published": "2024-02-05T06:12:29Z",
        "summary": "Large Language Models (LLMs) are nowadays expected to generate content\naligned with human preferences. Current work focuses on alignment at model\ntraining time, through techniques such as Reinforcement Learning with Human\nFeedback (RLHF). However, it is unclear if such methods are an effective choice\nto teach alignment objectives to the model. First, the inability to incorporate\nmultiple, custom rewards and reliance on a model developer's view of universal\nand static principles are key limitations. Second, the residual gaps in model\ntraining and the reliability of such approaches are also questionable (e.g.\nsusceptibility to jail-breaking even after safety training). To address these,\nwe propose DeAL, a framework that allows the user to customize reward functions\nand enables Decoding-time Alignment of LLMs (DeAL). At its core, we view\ndecoding as a heuristic-guided search process and facilitate the use of a wide\nvariety of alignment objectives. Our experiments with programmatic constraints\nsuch as keyword and length constraints (studied widely in the pre-LLM era) and\nabstract objectives such as harmlessness and helpfulness (proposed in the\npost-LLM era) show that we can DeAL with fine-grained trade-offs, improve\nadherence to alignment objectives, and address residual gaps in LLMs. Lastly,\nwhile DeAL can be effectively paired with RLHF and prompting techniques, its\ngenerality makes decoding slower, an optimization we leave for future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.06147v2.pdf"
    },
    {
        "title": "Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering",
        "authors": [
            "Aryan Agrawal"
        ],
        "published": "2024-02-05T06:08:06Z",
        "summary": "This paper introduces a novel paradigm for depression detection and treatment\nusing advanced Large Language Models (LLMs): Generative Pre-trained Transformer\n4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized\nprompts to diagnose, explain, and suggest therapeutic interventions for\ndepression. A unique few-shot prompting method enhances the models' ability to\nanalyze and explain depressive symptoms based on the DSM-5 criteria. In the\ninteraction phase, the models engage in empathetic dialogue management, drawing\nfrom resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,\nfostering supportive interactions with individuals experiencing major\ndepressive disorders. Additionally, the research introduces the Illuminate\nDatabase, enriched with various CBT modules, aiding in personalized therapy\nrecommendations. The study evaluates LLM performance using metrics such as F1\nscores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy\nfor Gisting Evaluation (ROUGE) across different test sets, demonstrating their\neffectiveness. This comprehensive approach blends cutting-edge AI with\nestablished psychological methods, offering new possibilities in mental health\ncare and showcasing the potential of LLMs in revolutionizing depression\ndiagnosis and treatment strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.05127v1.pdf"
    },
    {
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
        "authors": [
            "Zirui Liu",
            "Jiayi Yuan",
            "Hongye Jin",
            "Shaochen Zhong",
            "Zhaozhuo Xu",
            "Vladimir Braverman",
            "Beidi Chen",
            "Xia Hu"
        ],
        "published": "2024-02-05T06:06:47Z",
        "summary": "Efficiently serving large language models (LLMs) requires batching many\nrequests together to reduce the cost per request. Yet, the key-value (KV)\ncache, which stores attention keys and values to avoid re-computations,\nsignificantly increases memory demands and becomes the new bottleneck in speed\nand memory usage. This memory demand increases with larger batch sizes and\nlonger context lengths. Additionally, the inference speed is limited by the\nsize of KV cache, as the GPU's SRAM must load the entire KV cache from the main\nGPU memory for each token generated, causing the computational core to be idle\nduring this process. A straightforward and effective solution to reduce KV\ncache size is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly\nimplementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to\nmaintain almost the same quality while using $\\mathbf{2.6\\times}$ less peak\nmemory usage (including the model weight). This reduction in memory usage\nenables up to $\\mathbf{4\\times}$ larger batch size, bringing\n$\\mathbf{2.35\\times \\sim 3.47\\times}$ throughput on real LLM inference\nworkload. The source code is available at https://github.com/jy-yuan/KIVI.",
        "pdf_link": "https://arxiv.org/pdf/2402.02750v1.pdf"
    },
    {
        "title": "Understanding the planning of LLM agents: A survey",
        "authors": [
            "Xu Huang",
            "Weiwen Liu",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Hao Wang",
            "Defu Lian",
            "Yasheng Wang",
            "Ruiming Tang",
            "Enhong Chen"
        ],
        "published": "2024-02-05T04:25:24Z",
        "summary": "As Large Language Models (LLMs) have shown significant intelligence, the\nprogress to leverage LLMs as planning modules of autonomous agents has\nattracted more attention. This survey provides the first systematic view of\nLLM-based agents planning, covering recent works aiming to improve planning\nability. We provide a taxonomy of existing works on LLM-Agent planning, which\ncan be categorized into Task Decomposition, Plan Selection, External Module,\nReflection and Memory. Comprehensive analyses are conducted for each direction,\nand further challenges for the field of research are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.02716v1.pdf"
    },
    {
        "title": "Adversarial Text Purification: A Large Language Model Approach for Defense",
        "authors": [
            "Raha Moraffah",
            "Shubh Khandelwal",
            "Amrita Bhattacharjee",
            "Huan Liu"
        ],
        "published": "2024-02-05T02:36:41Z",
        "summary": "Adversarial purification is a defense mechanism for safeguarding classifiers\nagainst adversarial attacks without knowing the type of attacks or training of\nthe classifier. These techniques characterize and eliminate adversarial\nperturbations from the attacked inputs, aiming to restore purified samples that\nretain similarity to the initially attacked ones and are correctly classified\nby the classifier. Due to the inherent challenges associated with\ncharacterizing noise perturbations for discrete inputs, adversarial text\npurification has been relatively unexplored. In this paper, we investigate the\neffectiveness of adversarial purification methods in defending text\nclassifiers. We propose a novel adversarial text purification that harnesses\nthe generative capabilities of Large Language Models (LLMs) to purify\nadversarial text without the need to explicitly characterize the discrete noise\nperturbations. We utilize prompt engineering to exploit LLMs for recovering the\npurified examples for given adversarial examples such that they are\nsemantically similar and correctly classified. Our proposed method demonstrates\nremarkable performance over various classifiers, improving their accuracy under\nthe attack by over 65% on average.",
        "pdf_link": "https://arxiv.org/pdf/2402.06655v1.pdf"
    },
    {
        "title": "Large Language Models are Geographically Biased",
        "authors": [
            "Rohin Manvi",
            "Samar Khanna",
            "Marshall Burke",
            "David Lobell",
            "Stefano Ermon"
        ],
        "published": "2024-02-05T02:32:09Z",
        "summary": "Large Language Models (LLMs) inherently carry the biases contained in their\ntraining corpora, which can lead to the perpetuation of societal harm. As the\nimpact of these foundation models grows, understanding and evaluating their\nbiases becomes crucial to achieving fairness and accuracy. We propose to study\nwhat LLMs know about the world we live in through the lens of geography. This\napproach is particularly powerful as there is ground truth for the numerous\naspects of human life that are meaningfully projected onto geographic space\nsuch as culture, race, language, politics, and religion. We show various\nproblematic geographic biases, which we define as systemic errors in geospatial\npredictions. Initially, we demonstrate that LLMs are capable of making accurate\nzero-shot geospatial predictions in the form of ratings that show strong\nmonotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We\nthen show that LLMs exhibit common biases across a range of objective and\nsubjective topics. In particular, LLMs are clearly biased against locations\nwith lower socioeconomic conditions (e.g. most of Africa) on a variety of\nsensitive subjective topics such as attractiveness, morality, and intelligence\n(Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to\nquantify this and find that there is significant variation in the magnitude of\nbias across existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02680v1.pdf"
    },
    {
        "title": "Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases",
        "authors": [
            "Rio Aguina-Kang",
            "Maxim Gumin",
            "Do Heon Han",
            "Stewart Morris",
            "Seung Jean Yoo",
            "Aditya Ganeshan",
            "R. Kenny Jones",
            "Qiuhong Anna Wei",
            "Kailiang Fu",
            "Daniel Ritchie"
        ],
        "published": "2024-02-05T01:59:31Z",
        "summary": "We present a system for generating indoor scenes in response to text prompts.\nThe prompts are not limited to a fixed vocabulary of scene descriptions, and\nthe objects in generated scenes are not restricted to a fixed set of object\ncategories -- we call this setting indoor scene generation. Unlike most prior\nwork on indoor scene generation, our system does not require a large training\ndataset of existing 3D scenes. Instead, it leverages the world knowledge\nencoded in pre-trained large language models (LLMs) to synthesize programs in a\ndomain-specific layout language that describe objects and spatial relations\nbetween them. Executing such a program produces a specification of a constraint\nsatisfaction problem, which the system solves using a gradient-based\noptimization scheme to produce object positions and orientations. To produce\nobject geometry, the system retrieves 3D meshes from a database. Unlike prior\nwork which uses databases of category-annotated, mutually-aligned meshes, we\ndevelop a pipeline using vision-language models (VLMs) to retrieve meshes from\nmassive databases of un-annotated, inconsistently-aligned meshes. Experimental\nevaluations show that our system outperforms generative models trained on 3D\ndata for traditional, closed-universe scene generation tasks; it also\noutperforms a recent LLM-based layout generation method on open-universe scene\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2403.09675v1.pdf"
    },
    {
        "title": "RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews",
        "authors": [
            "Satpreet Harcharan Singh",
            "Kevin Jiang",
            "Kanchan Bhasin",
            "Ashutosh Sabharwal",
            "Nidal Moukaddam",
            "Ankit B Patel"
        ],
        "published": "2024-02-05T00:56:30Z",
        "summary": "Semi-structured interviews (SSIs) are a commonly employed data-collection\nmethod in healthcare research, offering in-depth qualitative insights into\nsubject experiences. Despite their value, the manual analysis of SSIs is\nnotoriously time-consuming and labor-intensive, in part due to the difficulty\nof extracting and categorizing emotional responses, and challenges in scaling\nhuman evaluation for large populations. In this study, we develop RACER, a\nLarge Language Model (LLM) based expert-guided automated pipeline that\nefficiently converts raw interview transcripts into insightful domain-relevant\nthemes and sub-themes. We used RACER to analyze SSIs conducted with 93\nhealthcare professionals and trainees to assess the broad personal and\nprofessional mental health impacts of the COVID-19 crisis. RACER achieves\nmoderately high agreement with two human evaluators (72%), which approaches the\nhuman inter-rater agreement (77%). Interestingly, LLMs and humans struggle with\nsimilar content involving nuanced emotional, ambivalent/dialectical, and\npsychological statements. Our study highlights the opportunities and challenges\nin using LLMs to improve research efficiency and opens new avenues for scalable\nanalysis of SSIs in healthcare research.",
        "pdf_link": "https://arxiv.org/pdf/2402.02656v1.pdf"
    },
    {
        "title": "Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting",
        "authors": [
            "Jinwoo Ahn",
            "Kyuseung Shin"
        ],
        "published": "2024-02-05T00:44:28Z",
        "summary": "Large Language Models (LLMs) frequently struggle with complex reasoning\ntasks, failing to construct logically sound steps towards the solution. In\nresponse to this behavior, users often try prompting the LLMs repeatedly in\nhopes of reaching a better response. This paper studies such repetitive\nbehavior and its effect by defining a novel setting, Chain-of-Feedback (CoF).\nThe setting takes questions that require multi-step reasoning as an input. Upon\nresponse, we repetitively prompt meaningless feedback (e.g. 'make another\nattempt') requesting additional trials. Surprisingly, our preliminary results\nshow that repeated meaningless feedback gradually decreases the quality of the\nresponses, eventually leading to a larger deviation from the intended outcome.\nTo alleviate these troubles, we propose a novel method, Recursive\nChain-of-Feedback (R-CoF). Following the logic of recursion in computer\nscience, R-CoF recursively revises the initially incorrect response by breaking\ndown each incorrect reasoning step into smaller individual problems. Our\npreliminary results show that majority of questions that LLMs fail to respond\ncorrectly can be answered using R-CoF without any sample data outlining the\nlogical process.",
        "pdf_link": "https://arxiv.org/pdf/2402.02648v2.pdf"
    },
    {
        "title": "Zero-Shot Clinical Trial Patient Matching with LLMs",
        "authors": [
            "Michael Wornow",
            "Alejandro Lozano",
            "Dev Dash",
            "Jenelle Jindal",
            "Kenneth W. Mahaffey",
            "Nigam H. Shah"
        ],
        "published": "2024-02-05T00:06:08Z",
        "summary": "Matching patients to clinical trials is a key unsolved challenge in bringing\nnew drugs to market. Today, identifying patients who meet a trial's eligibility\ncriteria is highly manual, taking up to 1 hour per patient. Automated screening\nis challenging, however, as it requires understanding unstructured clinical\ntext. Large language models (LLMs) offer a promising solution. In this work, we\nexplore their application to trial matching. First, we design an LLM-based\nsystem which, given a patient's medical history as unstructured clinical text,\nevaluates whether that patient meets a set of inclusion criteria (also\nspecified as free text). Our zero-shot system achieves state-of-the-art scores\non the n2c2 2018 cohort selection benchmark. Second, we improve the data and\ncost efficiency of our method by identifying a prompting strategy which matches\npatients an order of magnitude faster and more cheaply than the status quo, and\ndevelop a two-stage retrieval pipeline that reduces the number of tokens\nprocessed by up to a third while retaining high performance. Third, we evaluate\nthe interpretability of our system by having clinicians evaluate the natural\nlanguage justifications generated by the LLM for each eligibility decision, and\nshow that it can output coherent explanations for 97% of its correct decisions\nand 75% of its incorrect ones. Our results establish the feasibility of using\nLLMs to accelerate clinical trial operations.",
        "pdf_link": "https://arxiv.org/pdf/2402.05125v3.pdf"
    },
    {
        "title": "LLM-Enhanced Data Management",
        "authors": [
            "Xuanhe Zhou",
            "Xinyang Zhao",
            "Guoliang Li"
        ],
        "published": "2024-02-04T23:42:02Z",
        "summary": "Machine learning (ML) techniques for optimizing data management problems have\nbeen extensively studied and widely deployed in recent five years. However\ntraditional ML methods have limitations on generalizability (adapting to\ndifferent scenarios) and inference ability (understanding the context).\nFortunately, large language models (LLMs) have shown high generalizability and\nhuman-competitive abilities in understanding context, which are promising for\ndata management tasks (e.g., database diagnosis, database tuning). However,\nexisting LLMs have several limitations: hallucination, high cost, and low\naccuracy for complicated tasks. To address these challenges, we design LLMDB,\nan LLM-enhanced data management paradigm which has generalizability and high\ninference ability while avoiding hallucination, reducing LLM cost, and\nachieving high accuracy. LLMDB embeds domain-specific knowledge to avoid\nhallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high\ncost of LLMs by vector databases which provide semantic search and caching\nabilities. LLMDB improves the task accuracy by LLM agent which provides\nmultiple-round inference and pipeline executions. We showcase three real-world\nscenarios that LLMDB can well support, including query rewrite, database\ndiagnosis and data analytics. We also summarize the open research challenges of\nLLMDB.",
        "pdf_link": "https://arxiv.org/pdf/2402.02643v1.pdf"
    },
    {
        "title": "Can Large Language Models Learn Independent Causal Mechanisms?",
        "authors": [
            "Ga\u00ebl Gendron",
            "Bao Trung Nguyen",
            "Alex Yuxuan Peng",
            "Michael Witbrock",
            "Gillian Dobbie"
        ],
        "published": "2024-02-04T23:04:02Z",
        "summary": "Despite impressive performance on language modelling and complex reasoning\ntasks, Large Language Models (LLMs) fall short on the same tasks in uncommon\nsettings or with distribution shifts, exhibiting some lack of generalisation\nability. This issue has usually been alleviated by feeding more training data\ninto the LLM. However, this method is brittle, as the scope of tasks may not be\nreadily predictable or may evolve, and updating the model with new data\ngenerally requires extensive additional training. By contrast, systems, such as\ncausal models, that learn abstract variables and causal relationships can\ndemonstrate increased robustness against changes in the distribution. One\nreason for this success is the existence and use of Independent Causal\nMechanisms (ICMs) representing high-level concepts that only sparsely interact.\nIn this work, we apply two concepts from causality to learn ICMs within LLMs.\nWe develop a new LLM architecture composed of multiple sparsely interacting\nlanguage modelling modules. We introduce a routing scheme to induce\nspecialisation of the network into domain-specific modules. We also present a\nMutual Information minimisation objective that trains a separate module to\nlearn abstraction and domain-invariant mechanisms. We show that such causal\nconstraints can improve out-of-distribution performance on abstract and causal\nreasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.02636v1.pdf"
    },
    {
        "title": "Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity",
        "authors": [
            "Eric Khiu",
            "Hasti Toossi",
            "David Anugraha",
            "Jinyu Liu",
            "Jiaxu Li",
            "Juan Armando Parra Flores",
            "Leandro Acros Roman",
            "A. Seza Do\u011fru\u00f6z",
            "En-Shiun Annie Lee"
        ],
        "published": "2024-02-04T22:56:56Z",
        "summary": "Fine-tuning and testing a multilingual large language model is expensive and\nchallenging for low-resource languages (LRLs). While previous studies have\npredicted the performance of natural language processing (NLP) tasks using\nmachine learning methods, they primarily focus on high-resource languages,\noverlooking LRLs and shifts across domains. Focusing on LRLs, we investigate\nthree factors: the size of the fine-tuning corpus, the domain similarity\nbetween fine-tuning and testing corpora, and the language similarity between\nsource and target languages. We employ classical regression models to assess\nhow these factors impact the model's performance. Our results indicate that\ndomain similarity has the most critical impact on predicting the performance of\nMachine Translation models.",
        "pdf_link": "https://arxiv.org/pdf/2402.02633v1.pdf"
    },
    {
        "title": "UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing",
        "authors": [
            "Yifeng He",
            "Jiabo Huang",
            "Yuyang Rong",
            "Yiwen Guo",
            "Ethan Wang",
            "Hao Chen"
        ],
        "published": "2024-02-04T22:48:05Z",
        "summary": "The remarkable capability of large language models (LLMs) in generating\nhigh-quality code has drawn increasing attention in the software testing\ncommunity. However, existing code LLMs often demonstrate unsatisfactory\ncapabilities in generating accurate and complete tests since they were trained\non code snippets collected without differentiating between code for testing\npurposes and other code. In this paper, we present a large-scale dataset\nUniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test\nSynthesis. Associating tests with the tested functions is crucial for LLMs to\ninfer the expected behavior and the logic paths to be verified. By leveraging\nLanguage Server Protocol, UniTSyn achieves the challenging goal of collecting\nfocal-test pairs without per-project execution setups or per-language\nheuristics that tend to be fragile and difficult to scale. It contains 2.7\nmillion focal-test pairs across five mainstream programming languages, making\nit possible to be utilized for enhancing the test generation ability of LLMs.\nThe details of UniTSyn can be found in Table 1. Our experiments demonstrate\nthat, by building an autoregressive model based on UniTSyn, we can achieve\nsignificant benefits in learning and understanding unit test representations,\nresulting in improved generation accuracy and code coverage across all\nevaluated programming languages. Code and data will be publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2402.03396v1.pdf"
    },
    {
        "title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
        "authors": [
            "Chinmay Mittal",
            "Krishna Kartik",
            "Mausam",
            "Parag Singla"
        ],
        "published": "2024-02-04T20:56:09Z",
        "summary": "Recent works show that the largest of the large language models (LLMs) can\nsolve many simple reasoning tasks expressed in natural language, without\nany/much supervision. But, can they also solve challenging first-order\ncombinatorial reasoning problems, such as graph coloring, knapsack and\ncryptarithmetic? To answer this question, we present PuzzleBench, a dataset of\n31 such challenging problems along with a few solved instances for each\nproblem. These problems are all first order, i.e., they can be instantiated\nwith problem instances of varying sizes, and most of them are NP-hard,\nrequiring several reasoning steps to reach the solution. We first observe that\nLLMs, even when aided by symbolic solvers, perform rather poorly on our\ndataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs\nwith both symbolic solvers and program interpreters, along with feedback from\nsolved examples, to achieve huge performance gains. Our extensive\nexperimentation and analyses offer new insights into the reasoning abilities\nand limitations of present-day LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02611v2.pdf"
    },
    {
        "title": "A Truly Joint Neural Architecture for Segmentation and Parsing",
        "authors": [
            "Danit Yshaayahu Levi",
            "Reut Tsarfaty"
        ],
        "published": "2024-02-04T16:56:08Z",
        "summary": "Contemporary multilingual dependency parsers can parse a diverse set of\nlanguages, but for Morphologically Rich Languages (MRLs), performance is\nattested to be lower than other languages. The key challenge is that, due to\nhigh morphological complexity and ambiguity of the space-delimited input\ntokens, the linguistic units that act as nodes in the tree are not known in\nadvance. Pre-neural dependency parsers for MRLs subscribed to the joint\nmorpho-syntactic hypothesis, stating that morphological segmentation and\nsyntactic parsing should be solved jointly, rather than as a pipeline where\nsegmentation precedes parsing. However, neural state-of-the-art parsers to date\nuse a strict pipeline. In this paper we introduce a joint neural architecture\nwhere a lattice-based representation preserving all morphological ambiguity of\nthe input is provided to an arc-factored model, which then solves the\nmorphological segmentation and syntactic parsing tasks at once. Our experiments\non Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art\nperformance on parsing, tagging and segmentation of the Hebrew section of UD,\nusing a single model. This proposed architecture is LLM-based and language\nagnostic, providing a solid foundation for MRLs to obtain further performance\nimprovements and bridge the gap with other languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.02564v2.pdf"
    },
    {
        "title": "DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models",
        "authors": [
            "Yu Shang",
            "Yu Li",
            "Fengli Xu",
            "Yong Li"
        ],
        "published": "2024-02-04T16:45:01Z",
        "summary": "Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2402.02563v1.pdf"
    },
    {
        "title": "Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials",
        "authors": [
            "Ata Mustafa"
        ],
        "published": "2024-02-04T16:18:01Z",
        "summary": "Large Language Models have revolutionized various fields and industries, such\nas Conversational AI, Content Generation, Information Retrieval, Business\nIntelligence, and Medical, to name a few. One major application in the field of\nmedical is to analyze and investigate clinical trials for entailment\ntasks.However, It has been observed that Large Language Models are susceptible\nto shortcut learning, factual inconsistency, and performance degradation with\nlittle variation in context. Adversarial and robust testing is performed to\nensure the integrity of models output. But, ambiguity still persists. In order\nto ensure the integrity of the reasoning performed and investigate the model\nhas correct syntactic and semantic understanding probing is used. Here, I used\nmnestic probing to investigate the Sci-five model, trained on clinical trial. I\ninvestigated the model for feature learnt with respect to natural logic. To\nachieve the target, I trained task specific probes. Used these probes to\ninvestigate the final layers of trained model. Then, fine tuned the trained\nmodel using iterative null projection. The results shows that model accuracy\nimproved. During experimentation, I observed that size of the probe has affect\non the fine tuning process.",
        "pdf_link": "https://arxiv.org/pdf/2402.02558v1.pdf"
    },
    {
        "title": "Are Large Language Models Table-based Fact-Checkers?",
        "authors": [
            "Hangwen Zhang",
            "Qingyi Si",
            "Peng Fu",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "published": "2024-02-04T15:52:59Z",
        "summary": "Table-based Fact Verification (TFV) aims to extract the entailment relation\nbetween statements and structured tables. Existing TFV methods based on\nsmall-scaled models suffer from insufficient labeled data and weak zero-shot\nability. Recently, the appearance of Large Language Models (LLMs) has gained\nlots of attraction in research fields. They have shown powerful zero-shot and\nin-context learning abilities on several NLP tasks, but their potential on TFV\nis still unknown. In this work, we implement a preliminary study about whether\nLLMs are table-based fact-checkers. In detail, we design diverse prompts to\nexplore how the in-context learning can help LLMs in TFV, i.e., zero-shot and\nfew-shot TFV capability. Besides, we carefully design and construct TFV\ninstructions to study the performance gain brought by the instruction tuning of\nLLMs. Experimental results demonstrate that LLMs can achieve acceptable results\non zero-shot and few-shot TFV with prompt engineering, while instruction-tuning\ncan stimulate the TFV capability significantly. We also make some valuable\nfindings about the format of zero-shot prompts and the number of in-context\nexamples. Finally, we analyze some possible directions to promote the accuracy\nof TFV via LLMs, which is beneficial to further research of table reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.02549v1.pdf"
    },
    {
        "title": "Knowledge Generation for Zero-shot Knowledge-based VQA",
        "authors": [
            "Rui Cao",
            "Jing Jiang"
        ],
        "published": "2024-02-04T15:41:35Z",
        "summary": "Previous solutions to knowledge-based visual question answering~(K-VQA)\nretrieve knowledge from external knowledge bases and use supervised learning to\ntrain the K-VQA model. Recently pre-trained LLMs have been used as both a\nknowledge source and a zero-shot QA model for K-VQA and demonstrated promising\nresults. However, these recent methods do not explicitly show the knowledge\nneeded to answer the questions and thus lack interpretability. Inspired by\nrecent work on knowledge generation from LLMs for text-based QA, in this work\nwe propose and test a similar knowledge-generation-based K-VQA method, which\nfirst generates knowledge from an LLM and then incorporates the generated\nknowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA\nbenchmarks and found that our method performs better than previous zero-shot\nK-VQA methods and our generated knowledge is generally relevant and helpful.",
        "pdf_link": "https://arxiv.org/pdf/2402.02541v1.pdf"
    },
    {
        "title": "CompeteSMoE -- Effective Training of Sparse Mixture of Experts via Competition",
        "authors": [
            "Quang Pham",
            "Giang Do",
            "Huy Nguyen",
            "TrungTin Nguyen",
            "Chenghao Liu",
            "Mina Sartipi",
            "Binh T. Nguyen",
            "Savitha Ramasamy",
            "Xiaoli Li",
            "Steven Hoi",
            "Nhat Ho"
        ],
        "published": "2024-02-04T15:17:09Z",
        "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, effective training of SMoE has proven to be challenging due to the\nrepresentation collapse issue, which causes parameter redundancy and limited\nrepresentation potentials. In this work, we propose a competition mechanism to\naddress this fundamental challenge of representation collapse. By routing\ninputs only to experts with the highest neural response, we show that, under\nmild assumptions, competition enjoys the same convergence rate as the optimal\nestimator. We further propose CompeteSMoE, an effective and efficient algorithm\nto train large language models by deploying a simple router that predicts the\ncompetition outcomes. Consequently, CompeteSMoE enjoys strong performance gains\nfrom the competition routing policy while having low computation overheads. Our\nextensive empirical evaluations on two transformer architectures and a wide\nrange of tasks demonstrate the efficacy, robustness, and scalability of\nCompeteSMoE compared to state-of-the-art SMoE strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.02526v1.pdf"
    },
    {
        "title": "Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach",
        "authors": [
            "Zhengqiu Zhu",
            "Yong Zhao",
            "Bin Chen",
            "Sihang Qiu",
            "Kai Xu",
            "Quanjun Yin",
            "Jincai Huang",
            "Zhong Liu",
            "Fei-Yue Wang"
        ],
        "published": "2024-02-04T15:10:11Z",
        "summary": "The transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 brings\nnew requirements and opportunities to current sensing approaches, especially in\nlight of recent progress in Chatbots and Large Language Models (LLMs).\nTherefore, the advancement of parallel intelligence-powered Crowdsensing\nIntelligence (CSI) is witnessed, which is currently advancing towards\nlinguistic intelligence. In this paper, we propose a novel sensing paradigm,\nnamely conversational crowdsensing, for Industry 5.0. It can alleviate workload\nand professional requirements of individuals and promote the organization and\noperation of diverse workforce, thereby facilitating faster response and wider\npopularization of crowdsensing systems. Specifically, we design the\narchitecture of conversational crowdsensing to effectively organize three types\nof participants (biological, robotic, and digital) from diverse communities.\nThrough three levels of effective conversation (i.e., inter-human, human-AI,\nand inter-AI), complex interactions and service functionalities of different\nworkers can be achieved to accomplish various tasks across three sensing phases\n(i.e., requesting, scheduling, and executing). Moreover, we explore the\nfoundational technologies for realizing conversational crowdsensing,\nencompassing LLM-based multi-agent systems, scenarios engineering and\nconversational human-AI cooperation. Finally, we present potential industrial\napplications of conversational crowdsensing and discuss its implications. We\nenvision that conversations in natural language will become the primary\ncommunication channel during crowdsensing process, enabling richer information\nexchange and cooperative problem-solving among humans, robots, and AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.06654v1.pdf"
    },
    {
        "title": "GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering",
        "authors": [
            "Ziyu Ma",
            "Shutao Li",
            "Bin Sun",
            "Jianfei Cai",
            "Zuxiang Long",
            "Fuyan Ma"
        ],
        "published": "2024-02-04T14:28:23Z",
        "summary": "Knowledge-based visual question answering (VQA) requires world knowledge\nbeyond the image for accurate answer. Recently, instead of extra knowledge\nbases, a large language model (LLM) like GPT-3 is activated as an implicit\nknowledge engine to jointly acquire and reason the necessary knowledge for\nanswering by converting images into textual information (e.g., captions and\nanswer candidates). However, such conversion may introduce irrelevant\ninformation, which causes the LLM to misinterpret images and ignore visual\ndetails crucial for accurate knowledge. We argue that multimodal large language\nmodel (MLLM) is a better implicit knowledge engine than the LLM for its\nsuperior capability of visual understanding. Despite this, how to activate the\ncapacity of MLLM as the implicit knowledge engine has not been explored yet.\nTherefore, we propose GeReA, a generate-reason framework that prompts a MLLM\nlike InstructBLIP with question relevant vision and language information to\ngenerate knowledge-relevant descriptions and reasons those descriptions for\nknowledge-based VQA. Specifically, the question-relevant image regions and\nquestion-specific manual prompts are encoded in the MLLM to generate the\nknowledge relevant descriptions, referred to as question-aware prompt captions.\nAfter that, the question-aware prompt captions, image-question pair, and\nsimilar samples are sent into the multi-modal reasoning model to learn a joint\nknowledge-image-question representation for answer prediction. GeReA unlocks\nthe use of MLLM as the implicit knowledge engine, surpassing all previous\nstate-of-the-art methods on OK-VQA and A-OKVQA datasets, with test accuracies\nof 66.5% and 63.3% respectively. Our code will be released at\nhttps://github.com/Upper9527/GeReA.",
        "pdf_link": "https://arxiv.org/pdf/2402.02503v1.pdf"
    },
    {
        "title": "Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation",
        "authors": [
            "Malik Sallam",
            "Jan Egger",
            "Rainer Roehrig",
            "Behrus Puladi"
        ],
        "published": "2024-02-04T13:21:19Z",
        "summary": "In an era where artificial intelligence (AI) intertwines with medical\nresearch, the delineation of truth becomes increasingly complex. This study\nostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega\nvariant, showcasing 31 unique mutations in the S gene region. However, the real\nundercurrent of this narrative is a demonstration of the ease with which AI,\nspecifically ChatGPT-4, can fabricate convincing yet entirely fictional\nscientific data. The so-called Omega variant was identified in a fully\nvaccinated, previously infected 35-year-old male presenting with severe\nCOVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and\ncontact tracing, this study mirrors the rigorous methodology of genuine case\nreports, thereby setting the stage for a compelling but entirely constructed\nnarrative. The entire case study was generated by ChatGPT-4, a large language\nmodel by OpenAI. The fabricated Omega variant features an ensemble of\nmutations, including N501Y and E484K, known for enhancing ACE2 receptor\naffinity, alongside L452R and P681H, ostensibly indicative of immune evasion.\nThis variant's contrived interaction dynamics - severe symptoms in a vaccinated\nindividual versus mild ones in unvaccinated contacts - were designed to mimic\nreal-world complexities, including suggestions of antibody-dependent\nenhancement (ADE). While the Omega variant is a product of AI-generated\nfiction, the implications of this exercise are real and profound. The ease with\nwhich AI can generate believable but false scientific information, as\nillustrated in this case, raises significant concerns about the potential for\nmisinformation in medicine. This study, therefore, serves as a cautionary tale,\nemphasizing the necessity for critical evaluation of sources, especially in an\nage where AI tools like ChatGPT are becoming increasingly sophisticated and\nwidespread in their use.",
        "pdf_link": "https://arxiv.org/pdf/2403.09674v1.pdf"
    },
    {
        "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
        "authors": [
            "Gaurav Pandey",
            "Yatin Nandwani",
            "Tahira Naseem",
            "Mayank Mishra",
            "Guangxuan Xu",
            "Dinesh Raghu",
            "Sachindra Joshi",
            "Asim Munawar",
            "Ram\u00f3n Fernandez Astudillo"
        ],
        "published": "2024-02-04T13:16:29Z",
        "summary": "Following the success of Proximal Policy Optimization (PPO) for Reinforcement\nLearning from Human Feedback (RLHF), new techniques such as Sequence Likelihood\nCalibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that\nare offline in nature and use rewards in an indirect manner. These techniques,\nin particular DPO, have recently become the tools of choice for LLM alignment\ndue to their scalability and performance. However, they leave behind important\nfeatures of the PPO approach. Methods such as SLiC or RRHF make use of the\nReward Model (RM) only for ranking/preference, losing fine-grained information\nand ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce),\nwhile methods such as DPO do not use even a separate reward model. In this\nwork, we propose a novel approach, named BRAIn, that re-introduces the RM as\npart of a distribution matching approach.BRAIn considers the LLM distribution\nconditioned on the assumption of output goodness and applies Bayes theorem to\nderive an intractable posterior distribution where the RM is explicitly\nrepresented. BRAIn then distills this posterior into an amortized inference\nnetwork through self-normalized importance sampling, leading to a scalable\noffline algorithm that significantly outperforms prior art in summarization and\nAntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for\nspecific RM choices.",
        "pdf_link": "https://arxiv.org/pdf/2402.02479v1.pdf"
    },
    {
        "title": "A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer",
        "authors": [
            "Zhangyang Gao",
            "Daize Dong",
            "Cheng Tan",
            "Jun Xia",
            "Bozhen Hu",
            "Stan Z. Li"
        ],
        "published": "2024-02-04T12:29:40Z",
        "summary": "Can we model non-Euclidean graphs as pure language or even Euclidean vectors\nwhile retaining their inherent information? The non-Euclidean property have\nposed a long term challenge in graph modeling. Despite recent GNN and\nGraphformer efforts encoding graphs as Euclidean vectors, recovering original\ngraph from the vectors remains a challenge. We introduce GraphsGPT, featuring a\nGraph2Seq encoder that transforms non-Euclidean graphs into learnable graph\nwords in a Euclidean space, along with a GraphGPT decoder that reconstructs the\noriginal graph from graph words to ensure information equivalence. We pretrain\nGraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained\nGraph2Seq excels in graph representation learning, achieving state-of-the-art\nresults on 8/9 graph classification and regression tasks. (2) Pretrained\nGraphGPT serves as a strong graph generator, demonstrated by its ability to\nperform both unconditional and conditional graph generation. (3)\nGraph2Seq+GraphGPT enables effective graph mixup in the Euclidean space,\novercoming previously known non-Euclidean challenge. (4) Our proposed novel\nedge-centric GPT pretraining task is effective in graph fields, underscoring\nits success in both representation and generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.02464v2.pdf"
    },
    {
        "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
        "authors": [
            "Zhangyang Gao",
            "Cheng Tan",
            "Jue Wang",
            "Yufei Huang",
            "Lirong Wu",
            "Stan Z. Li"
        ],
        "published": "2024-02-04T12:18:51Z",
        "summary": "Is there a foreign language describing protein sequences and structures\nsimultaneously? Protein structures, represented by continuous 3D points, have\nlong posed a challenge due to the contrasting modeling paradigms of discrete\nsequences. We introduce \\textbf{FoldTokenizer} to represent protein\nsequence-structure as discrete symbols. This innovative approach involves\nprojecting residue types and structures into a discrete space, guided by a\nreconstruction loss for information preservation. We refer to the learned\ndiscrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves\nas a new protein language, transforming the protein sequence-structure into a\nunified modality. We apply the created protein language on general backbone\ninpainting and antibody design tasks, building the first GPT-style model\n(\\textbf{FoldGPT}) for sequence-structure co-generation with promising results.\nKey to our success is the substantial enhancement of the vector quantization\nmodule, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
        "pdf_link": "https://arxiv.org/pdf/2403.09673v2.pdf"
    },
    {
        "title": "Breaking MLPerf Training: A Case Study on Optimizing BERT",
        "authors": [
            "Yongdeok Kim",
            "Jaehyung Ahn",
            "Myeongwoo Kim",
            "Changin Choi",
            "Heejae Kim",
            "Narankhuu Tuvshinjargal",
            "Seungwon Lee",
            "Yanzi Zhang",
            "Yuan Pei",
            "Xiongzhan Linghu",
            "Jingkun Ma",
            "Lin Chen",
            "Yuehua Dai",
            "Sungjoo Yoo"
        ],
        "published": "2024-02-04T11:12:17Z",
        "summary": "Speeding up the large-scale distributed training is challenging in that it\nrequires improving various components of training including load balancing,\ncommunication, optimizers, etc. We present novel approaches for fast\nlarge-scale training of BERT model which individually ameliorates each\ncomponent thereby leading to a new level of BERT training performance. Load\nbalancing is imperative in distributed BERT training since its training\ndatasets are characterized by samples with various lengths. Communication cost,\nwhich is proportional to the scale of distributed training, needs to be hidden\nby useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc.,\nneed to be carefully re-evaluated in the context of large-scale distributed\ntraining. We propose two new ideas, (1) local presorting based on dataset\nstratification for load balancing and (2) bucket-wise gradient clipping before\nallreduce which allows us to benefit from the overlap of gradient computation\nand synchronization as well as the fast training of gradient clipping before\nallreduce. We also re-evaluate existing optimizers via hyperparameter\noptimization and utilize ADAM, which also contributes to fast training via\nlarger batches than existing methods. Our proposed methods, all combined, give\nthe fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100\nGPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one)\nsubmissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results\nare available at MLPerf v1.1~v2.1.",
        "pdf_link": "https://arxiv.org/pdf/2402.02447v1.pdf"
    },
    {
        "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
        "authors": [
            "Cheng Zhang",
            "Jianyi Cheng",
            "George A. Constantinides",
            "Yiren Zhao"
        ],
        "published": "2024-02-04T10:59:52Z",
        "summary": "Post-training quantization of Large Language Models (LLMs) is challenging. In\nthis work, we introduce Low-rank Quantization Error Reduction (LQER), which\ncombines quantization and low-rank approximation to recover the model\ncapability. LQER leverages an activation-induced scale matrix to drive the\nsingular value distribution of quantization error towards a desirable\ndistribution, which enables nearly-lossless W4A8 quantization on various LLMs\nand downstream tasks without the need for knowledge distillation, grid search,\nor gradient-base iterative optimization. Unlike existing methods, the\ncomputation pattern of LQER eliminates the need for specialized Scatter and\nGather processes to collect high-precision weights from irregular memory\nlocations. Our W4A8 LLMs achieve near-lossless performance on six popular\ndownstream tasks, while using 1.36$\\times$ fewer hardware resources than the\nleading state-of-the-art method. We will open-source our framework once the\npaper is accepted.",
        "pdf_link": "https://arxiv.org/pdf/2402.02446v2.pdf"
    },
    {
        "title": "Factuality of Large Language Models in the Year 2024",
        "authors": [
            "Yuxia Wang",
            "Minghan Wang",
            "Muhammad Arslan Manzoor",
            "Fei Liu",
            "Georgi Georgiev",
            "Rocktim Jyoti Das",
            "Preslav Nakov"
        ],
        "published": "2024-02-04T09:36:31Z",
        "summary": "Large language models (LLMs), especially when instruction-tuned for chat,\nhave become part of our daily lives, freeing people from the process of\nsearching, extracting, and integrating information from multiple sources by\noffering a straightforward answer to a variety of questions in a single place.\nUnfortunately, in many cases, LLM responses are factually incorrect, which\nlimits their applicability in real-world scenarios. As a result, research on\nevaluating and improving the factuality of LLMs has attracted a lot of research\nattention recently. In this survey, we critically analyze existing work with\nthe aim to identify the major challenges and their associated causes, pointing\nout to potential solutions for improving the factuality of LLMs, and analyzing\nthe obstacles to automated factuality evaluation for open-ended text\ngeneration. We further offer an outlook on where future research should go.",
        "pdf_link": "https://arxiv.org/pdf/2402.02420v2.pdf"
    },
    {
        "title": "Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction",
        "authors": [
            "Jiaming Ji",
            "Boyuan Chen",
            "Hantao Lou",
            "Donghai Hong",
            "Borong Zhang",
            "Xuehai Pan",
            "Juntao Dai",
            "Yaodong Yang"
        ],
        "published": "2024-02-04T09:24:51Z",
        "summary": "Efforts to align Large Language Models (LLMs) are mainly conducted via\nReinforcement Learning from Human Feedback (RLHF) methods. However, RLHF\nencounters major challenges including training reward models, actor-critic\nengineering, and importantly, it requires access to LLM parameters. Here we\nintroduce Aligner, a new efficient alignment paradigm that bypasses the whole\nRLHF process by learning the correctional residuals between the aligned and the\nunaligned answers. Our Aligner offers several key advantages. Firstly, it is an\nautoregressive seq2seq model that is trained on the query-answer-correction\ndataset via supervised learning; this offers a parameter-efficient alignment\nsolution with minimal resources. Secondly, the Aligner facilitates\nweak-to-strong generalization; finetuning large pretrained models by Aligner's\nsupervisory signals demonstrates strong performance boost. Thirdly, Aligner\nfunctions as a model-agnostic plug-and-play module, allowing for its direct\napplication on different open-source and API-based models. Remarkably,\nAligner-7B improves 11 different LLMs by 21.9% in helpfulness and 23.8% in\nharmlessness on average (GPT-4 by 17.5% and 26.9%). When finetuning (strong)\nLlama2-70B with (weak) Aligner-13B's supervision, we can improve Llama2 by 8.2%\nin helpfulness and 61.6% in harmlessness. See our dataset and code at\nhttps://aligner2024.github.io",
        "pdf_link": "https://arxiv.org/pdf/2402.02416v2.pdf"
    },
    {
        "title": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model",
        "authors": [
            "Xuanchang Zhang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2024-02-04T08:57:54Z",
        "summary": "Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE.",
        "pdf_link": "https://arxiv.org/pdf/2402.02408v1.pdf"
    },
    {
        "title": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
        "authors": [
            "Ollie Liu",
            "Deqing Fu",
            "Dani Yogatama",
            "Willie Neiswanger"
        ],
        "published": "2024-02-04T08:11:45Z",
        "summary": "Large language models (LLMs) are increasingly used across society, including\nin domains like business, engineering, and medicine. These fields often grapple\nwith decision-making under uncertainty, a critical yet challenging task. In\nthis paper, we show that directly prompting LLMs on these types of\ndecision-making problems yields poor results, especially as the problem\ncomplexity increases. To overcome this limitation, we propose DeLLMa\n(Decision-making Large Language Model assistant), a framework designed to\nenhance decision-making accuracy in uncertain environments. DeLLMa involves a\nmulti-step scaffolding procedure, drawing upon principles from decision theory\nand utility theory, to provide an optimal and human-auditable decision-making\nprocess. We validate our framework on decision-making environments involving\nreal agriculture and finance data. Our results show that DeLLMa can\nsignificantly improve LLM decision-making performance, achieving up to a 40%\nincrease in accuracy over competing methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.02392v1.pdf"
    },
    {
        "title": "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
        "authors": [
            "Yanbin Wei",
            "Qiushi Huang",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "published": "2024-02-04T08:01:07Z",
        "summary": "Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph\nincompleteness and supporting downstream applications. Many models have been\nproposed for KGC. They can be categorized into two main classes: triple-based\nand text-based approaches. Triple-based methods struggle with long-tail\nentities due to limited structural information and imbalanced entity\ndistributions. Text-based methods alleviate this issue but require costly\ntraining for language models and specific finetuning for knowledge graphs,\nwhich limits their efficiency. To alleviate these limitations, in this paper,\nwe propose KICGPT, a framework that integrates a large language model (LLM) and\na triple-based KGC retriever. It alleviates the long-tail problem without\nincurring additional training overhead. KICGPT uses an in-context learning\nstrategy called Knowledge Prompt, which encodes structural knowledge into\ndemonstrations to guide the LLM. Empirical results on benchmark datasets\ndemonstrate the effectiveness of KICGPT with smaller training overhead and no\nfinetuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.02389v2.pdf"
    },
    {
        "title": "Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning",
        "authors": [
            "Tong Niu",
            "Weihao Zhang",
            "Rong Zhao"
        ],
        "published": "2024-02-04T07:59:06Z",
        "summary": "Agent-based models (ABMs) stand as an essential paradigm for proposing and\nvalidating hypothetical solutions or policies aimed at addressing challenges\nposed by complex systems and achieving various objectives. This process demands\nlabor-intensive endeavors and multidisciplinary expertise. Large language\nmodels (LLMs) encapsulating cross-domain knowledge and programming proficiency\ncould potentially alleviate the difficulty of this process. However, LLMs excel\nin handling sequential information, making it challenging for analyzing the\nintricate interactions and nonlinear dynamics inherent in ABMs. Additionally,\ndue to the lack of self-evaluation capability of LLMs, relying solely on LLMs\nis insufficient to effectively accomplish this process. In this paper, we\npresent SAGE, a general solution-oriented ABM generation framework designed for\nautomatic modeling and generating solutions for targeted problems. Unlike\napproaches reliant on expert handcrafting or resource-intensive neural network\ntraining, SAGE establishes a verifier-assisted iterative in-context learning\nprocess employing large language models (LLMs) to leverages their inherent\ncross-domain knowledge for tackling intricate demands from diverse domain\nscenarios. In SAGE, we introduce an semi-structured conceptual representation\nexpliciting the intricate structures of ABMs and an objective representation to\nguide LLMs in modeling scenarios and proposing hypothetical solutions through\nin-context learning. To ensure the model executability and solution\nfeasibility, SAGE devises a two-level verifier with chain-of-thought prompting\ntailored to the complex interactions and non-linear dynamics of ABMs, driving\nthe iterative generation optimization. Moreover, we construct an evaluation\ndataset of solution-oriented ABMs from open sources.It contains practical\nmodels across various domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.02388v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models in Analysing Classroom Dialogue",
        "authors": [
            "Yun Long",
            "Haifeng Luo",
            "Yu Zhang"
        ],
        "published": "2024-02-04T07:39:06Z",
        "summary": "This study explores the application of Large Language Models (LLMs),\nspecifically GPT-4, in the analysis of classroom dialogue, a crucial research\ntask for both teaching diagnosis and quality improvement. Recognizing the\nknowledge-intensive and labor-intensive nature of traditional qualitative\nmethods in educational research, this study investigates the potential of LLM\nto streamline and enhance the analysis process. The study involves datasets\nfrom a middle school, encompassing classroom dialogues across mathematics and\nChinese classes. These dialogues were manually coded by educational experts and\nthen analyzed using a customised GPT-4 model. This study focuses on comparing\nmanual annotations with the outputs of GPT-4 to evaluate its efficacy in\nanalyzing educational dialogues. Time efficiency, inter-coder agreement, and\ninter-coder reliability between human coders and GPT-4 are evaluated. Results\nindicate substantial time savings with GPT-4, and a high degree of consistency\nin coding between the model and human coders, with some discrepancies in\nspecific codes. These findings highlight the strong potential of LLM in\nteaching evaluation and facilitation.",
        "pdf_link": "https://arxiv.org/pdf/2402.02380v3.pdf"
    },
    {
        "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models",
        "authors": [
            "Yong Liu",
            "Guo Qin",
            "Xiangdong Huang",
            "Jianmin Wang",
            "Mingsheng Long"
        ],
        "published": "2024-02-04T06:59:21Z",
        "summary": "Foundation models of time series have not been fully developed due to the\nlimited availability of large-scale time series and the underexploration of\nscalable pre-training. Based on the similar sequential structure of time series\nand natural language, increasing research demonstrates the feasibility of\nleveraging large language models (LLM) for time series. Nevertheless, prior\nmethods may overlook the consistency in aligning time series and natural\nlanguage, resulting in insufficient utilization of the LLM potentials. To fully\nexploit the general-purpose token transitions learned from language modeling,\nwe propose AutoTimes to repurpose LLMs as Autoregressive Time series\nforecasters, which is consistent with the acquisition and utilization of LLMs\nwithout updating the parameters. The consequent forecasters can handle flexible\nseries lengths and achieve competitive performance as prevalent models.\nFurther, we present token-wise prompting that utilizes corresponding timestamps\nto make our method applicable to multimodal scenarios. Analysis demonstrates\nour forecasters inherit zero-shot and in-context learning capabilities of LLMs.\nEmpirically, AutoTimes exhibits notable method generality and achieves enhanced\nperformance by basing on larger LLMs, additional texts, or time series as\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2402.02370v1.pdf"
    },
    {
        "title": "Timer: Transformers for Time Series Analysis at Scale",
        "authors": [
            "Yong Liu",
            "Haoran Zhang",
            "Chenyu Li",
            "Xiangdong Huang",
            "Jianmin Wang",
            "Mingsheng Long"
        ],
        "published": "2024-02-04T06:55:55Z",
        "summary": "Deep learning has contributed remarkably to the advancement of time series\nanalysis. Still, deep models can encounter performance bottlenecks in\nreal-world small-sample scenarios, which can be concealed due to the\nperformance saturation with small models on current benchmarks. Meanwhile,\nlarge models have demonstrated great powers in these scenarios through\nlarge-scale pre-training. Continuous progresses have been achieved as the\nemergence of large language models, exhibiting unprecedented ability in\nfew-shot generalization, scalability, and task generality, which is however\nabsent in time series models. To change the current practices of training small\nmodels on specific datasets from scratch, this paper aims at an early\ndevelopment of large time series models (LTSM). During pre-training, we curate\nlarge-scale datasets with up to 1 billion time points, unify heterogeneous time\nseries into single-series sequence (S3) format, and develop the GPT-style\narchitecture toward LTSMs. To meet diverse application needs, we convert\nforecasting, imputation, and anomaly detection of time series into a unified\ngenerative task. The outcome of this study is a Time Series Transformer\n(Timer), that is pre-trained by autoregressive next token prediction on large\nmulti-domain datasets, and is fine-tuned to downstream scenarios with promising\nabilities as an LTSM.",
        "pdf_link": "https://arxiv.org/pdf/2402.02368v1.pdf"
    },
    {
        "title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
        "authors": [
            "Qiheng Mao",
            "Zemin Liu",
            "Chenghao Liu",
            "Zhuo Li",
            "Jianling Sun"
        ],
        "published": "2024-02-04T05:51:14Z",
        "summary": "The integration of Large Language Models (LLMs) with Graph Representation\nLearning (GRL) marks a significant evolution in analyzing complex data\nstructures. This collaboration harnesses the sophisticated linguistic\ncapabilities of LLMs to improve the contextual understanding and adaptability\nof graph models, thereby broadening the scope and potential of GRL. Despite a\ngrowing body of research dedicated to integrating LLMs into the graph domain, a\ncomprehensive review that deeply analyzes the core components and operations\nwithin these models is notably lacking. Our survey fills this gap by proposing\na novel taxonomy that breaks down these models into primary components and\noperation techniques from a novel technical perspective. We further dissect\nrecent literature into two primary components including knowledge extractors\nand organizers, and two operation techniques including integration and training\nstratigies, shedding light on effective model design and training strategies.\nAdditionally, we identify and explore potential future research avenues in this\nnascent yet underexplored field, proposing paths for continued progress.",
        "pdf_link": "https://arxiv.org/pdf/2402.05952v1.pdf"
    },
    {
        "title": "Large Language Model Adaptation for Networking",
        "authors": [
            "Duo Wu",
            "Xianda Wang",
            "Yaqi Qiao",
            "Zhi Wang",
            "Junchen Jiang",
            "Shuguang Cui",
            "Fangxin Wang"
        ],
        "published": "2024-02-04T04:21:34Z",
        "summary": "Many networking tasks now employ deep learning (DL) to solve complex\nprediction and system optimization problems. However, current design philosophy\nof DL-based algorithms entails intensive engineering overhead due to the manual\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\nDNNs tend to achieve poor generalization performance on unseen data\ndistributions/environments.\n  Motivated by the recent success of large language models (LLMs), for the\nfirst time, this work studies the LLM adaptation for networking to explore a\nmore sustainable design philosophy. With the massive pre-trained knowledge and\npowerful inference ability, LLM can serve as the foundation model, and is\nexpected to achieve \"one model for all\" with even better performance and\nstronger generalization for various tasks. In this paper, we present NetLLM,\nthe first LLM adaptation framework that efficiently adapts LLMs to solve\nnetworking problems. NetLLM addresses many practical challenges in LLM\nadaptation, from how to process task-specific information with LLMs, to how to\nimprove the efficiency of answer generation and acquiring domain knowledge for\nnetworking. Across three networking-related use cases - viewport prediction\n(VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we\nshowcase the effectiveness of NetLLM in LLM adaptation for networking. Results\nshow that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6%\nfor VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior\ngeneralization performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.02338v1.pdf"
    },
    {
        "title": "Enhance Reasoning for Large Language Models in the Game Werewolf",
        "authors": [
            "Shuang Wu",
            "Liwen Zhu",
            "Tao Yang",
            "Shiwei Xu",
            "Qiang Fu",
            "Yang Wei",
            "Haobo Fu"
        ],
        "published": "2024-02-04T03:47:10Z",
        "summary": "This paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.",
        "pdf_link": "https://arxiv.org/pdf/2402.02330v2.pdf"
    },
    {
        "title": "A Survey of Large Language Models in Finance (FinLLMs)",
        "authors": [
            "Jean Lee",
            "Nicholas Stevens",
            "Soyeon Caren Han",
            "Minseok Song"
        ],
        "published": "2024-02-04T02:06:57Z",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities across a wide\nvariety of Natural Language Processing (NLP) tasks and have attracted attention\nfrom multiple domains, including financial services. Despite the extensive\nresearch into general-domain LLMs, and their immense potential in finance,\nFinancial LLM (FinLLM) research remains limited. This survey provides a\ncomprehensive overview of FinLLMs, including their history, techniques,\nperformance, and opportunities and challenges. Firstly, we present a\nchronological overview of general-domain Pre-trained Language Models (PLMs)\nthrough to current FinLLMs, including the GPT-series, selected open-source\nLLMs, and financial LMs. Secondly, we compare five techniques used across\nfinancial PLMs and FinLLMs, including training methods, training data, and\nfine-tuning methods. Thirdly, we summarize the performance evaluations of six\nbenchmark tasks and datasets. In addition, we provide eight advanced financial\nNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we\ndiscuss the opportunities and the challenges facing FinLLMs, such as\nhallucination, privacy, and efficiency. To support AI research in finance, we\ncompile a collection of accessible datasets and evaluation benchmarks on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2402.02315v1.pdf"
    },
    {
        "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
        "authors": [
            "Haowei Lin",
            "Baizhou Huang",
            "Haotian Ye",
            "Qinyu Chen",
            "Zihao Wang",
            "Sujian Li",
            "Jianzhu Ma",
            "Xiaojun Wan",
            "James Zou",
            "Yitao Liang"
        ],
        "published": "2024-02-04T01:55:00Z",
        "summary": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the\nmost appropriate pre-trained model to fine-tune amidst a sea of options. Given\nconstrained resources, fine-tuning all models and making selections afterward\nis unrealistic. In this work, we formulate this resource-constrained selection\ntask into predicting fine-tuning performance and illustrate its natural\nconnection with scaling laws. Unlike pre-training, We find that the fine-tuning\nscaling curve includes not just the well-known \"power phase\" but also the\npreviously unobserved \"pre-power phase\". We also explain why existing scaling\nlaws fail to capture this phase transition phenomenon both theoretically and\nempirically. To address this, we introduce the concept of \"pre-learned data\nsize\" into our rectified scaling law, which overcomes theoretical limitations\nand fits experimental results much better. By leveraging our law, we propose a\nnovel LLM selection algorithm that selects the near-optimal model with hundreds\nof times less resource consumption, while other methods may provide negatively\ncorrelated selection.",
        "pdf_link": "https://arxiv.org/pdf/2402.02314v1.pdf"
    },
    {
        "title": "Jailbreaking Attack against Multimodal Large Language Model",
        "authors": [
            "Zhenxing Niu",
            "Haodong Ren",
            "Xinbo Gao",
            "Gang Hua",
            "Rong Jin"
        ],
        "published": "2024-02-04T01:29:24Z",
        "summary": "This paper focuses on jailbreaking attacks against multi-modal large language\nmodels (MLLMs), seeking to elicit MLLMs to generate objectionable responses to\nharmful user queries. A maximum likelihood-based algorithm is proposed to find\nan \\emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs\nacross multiple unseen prompts and images (i.e., data-universal property). Our\napproach exhibits strong model-transferability, as the generated imgJP can be\ntransferred to jailbreak various models, including MiniGPT-v2, LLaVA,\nInstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a\nconnection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we\nintroduce a construction-based method to harness our approach for\nLLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art\nmethods. The code is available here. \\textbf{Warning: some content generated by\nlanguage models may be offensive to some readers.}",
        "pdf_link": "https://arxiv.org/pdf/2402.02309v1.pdf"
    },
    {
        "title": "Large Language Model for Table Processing: A Survey",
        "authors": [
            "Weizheng Lu",
            "Jiaming Zhang",
            "Jing Zhang",
            "Yueguo Chen"
        ],
        "published": "2024-02-04T00:47:53Z",
        "summary": "Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\ncalculations, and generating reports from web tables. Automating these\ntable-centric tasks with Large Language Models (LLMs) offers significant public\nbenefits, garnering interest from academia and industry. This survey provides\nan extensive overview of table tasks, encompassing not only the traditional\nareas like table question answering (Table QA) and fact verification, but also\nnewly emphasized aspects such as table manipulation and advanced table data\nanalysis. Additionally, it goes beyond the early strategies of pre-training and\nfine-tuning small language models, to include recent paradigms in LLM usage.\nThe focus here is particularly on instruction-tuning, prompting, and\nagent-based approaches within the realm of LLMs. Finally, we highlight several\nchallenges, ranging from private deployment and efficient inference to the\ndevelopment of extensive benchmarks for table manipulation and advanced data\nanalysis.",
        "pdf_link": "https://arxiv.org/pdf/2402.05121v1.pdf"
    },
    {
        "title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
        "authors": [
            "Atharva Kulkarni",
            "Bo-Hsiang Tseng",
            "Joel Ruben Antony Moniz",
            "Dhivya Piraviperumal",
            "Hong Yu",
            "Shruti Bhargava"
        ],
        "published": "2024-02-03T22:49:00Z",
        "summary": "In-context learning with Large Language Models (LLMs) has emerged as a\npromising avenue of research in Dialog State Tracking (DST). However, the\nbest-performing in-context learning methods involve retrieving and adding\nsimilar examples to the prompt, requiring access to labeled training data.\nProcuring such training data for a wide range of domains and applications is\ntime-consuming, expensive, and, at times, infeasible. While zero-shot learning\nrequires no training data, it significantly lags behind the few-shot setup.\nThus, `\\textit{Can we efficiently generate synthetic data for any dialogue\nschema to enable few-shot prompting?}' Addressing this question, we propose\n\\method, a data generation framework tailored for DST, utilizing LLMs. Our\napproach only requires the dialogue schema and a few hand-crafted dialogue\ntemplates to synthesize natural, coherent, and free-flowing dialogues with DST\nannotations. Few-shot learning using data from {\\method} results in $4-5%$\nimprovement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1\nand 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of\nthe performance compared to the few-shot setup using human-annotated training\ndata. Our synthetic data and code can be accessed at\nhttps://github.com/apple/ml-synthdst",
        "pdf_link": "https://arxiv.org/pdf/2402.02285v1.pdf"
    },
    {
        "title": "Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times",
        "authors": [
            "Byung-Doh Oh",
            "Shisen Yue",
            "William Schuler"
        ],
        "published": "2024-02-03T20:22:54Z",
        "summary": "Recent studies have shown that as Transformer-based language models become\nlarger and are trained on very large amounts of data, the fit of their\nsurprisal estimates to naturalistic human reading times degrades. The current\nwork presents a series of analyses showing that word frequency is a key\nexplanatory factor underlying these two trends. First, residual errors from\nfour language model families on four corpora show that the inverse correlation\nbetween model size and fit to reading times is the strongest on the subset of\nleast frequent words, which is driven by excessively accurate predictions of\nlarger model variants. Additionally, training dynamics reveal that during later\ntraining steps, all model variants learn to predict rare words and that larger\nmodel variants do so more accurately, which explains the detrimental effect of\nboth training data amount and model size on fit to reading times. Finally, a\nfeature attribution analysis demonstrates that larger model variants are able\nto accurately predict rare words based on both an effectively longer context\nwindow size as well as stronger local associations compared to smaller model\nvariants. Taken together, these results indicate that Transformer-based\nlanguage models' surprisal estimates diverge from human-like expectations due\nto the superhumanly complex associations they learn for predicting rare words.",
        "pdf_link": "https://arxiv.org/pdf/2402.02255v1.pdf"
    },
    {
        "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
        "authors": [
            "Xindi Wang",
            "Mahsa Salmani",
            "Parsa Omidi",
            "Xiangyu Ren",
            "Mehdi Rezagholizadeh",
            "Armaghan Eshaghi"
        ],
        "published": "2024-02-03T19:20:02Z",
        "summary": "Recently, large language models (LLMs) have shown remarkable capabilities\nincluding understanding context, engaging in logical reasoning, and generating\nresponses. However, this is achieved at the expense of stringent computational\nand memory requirements, hindering their ability to effectively support long\ninput sequences. This survey provides an inclusive review of the recent\ntechniques and methods devised to extend the sequence length in LLMs, thereby\nenhancing their capacity for long-context understanding. In particular, we\nreview and categorize a wide range of techniques including architectural\nmodifications, such as modified positional encoding and altered attention\nmechanisms, which are designed to enhance the processing of longer sequences\nwhile avoiding a proportional increase in computational requirements. The\ndiverse methodologies investigated in this study can be leveraged across\ndifferent phases of LLMs, i.e., training, fine-tuning and inference. This\nenables LLMs to efficiently process extended sequences. The limitations of the\ncurrent methodologies is discussed in the last section along with the\nsuggestions for future research directions, underscoring the importance of\nsequence length in the continued advancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02244v1.pdf"
    },
    {
        "title": "Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",
        "authors": [
            "Stevan Harnad"
        ],
        "published": "2024-02-03T19:19:34Z",
        "summary": "Apart from what (little) OpenAI may be concealing from us, we all know\n(roughly) how ChatGPT works (its huge text database, its statistics, its vector\nrepresentations, and their huge number of parameters, its next-word training,\nand so on). But none of us can say (hand on heart) that we are not surprised by\nwhat ChatGPT has proved to be able to do with these resources. This has even\ndriven some of us to conclude that ChatGPT actually understands. It is not true\nthat it understands. But it is also not true that we understand how it can do\nwhat it can do. I will suggest some hunches about benign biases: convergent\nconstraints that emerge at LLM scale that may be helping ChatGPT do so much\nbetter than we would have expected. These biases are inherent in the nature of\nlanguage itself, at LLM scale, and they are closely linked to what it is that\nChatGPT lacks, which is direct sensorimotor grounding to connect its words to\ntheir referents and its propositions to their meanings. These convergent biases\nare related to (1) the parasitism of indirect verbal grounding on direct\nsensorimotor grounding, (2) the circularity of verbal definition, (3) the\nmirroring of language production and comprehension, (4) iconicity in\npropositions at LLM scale, (5) computational counterparts of human categorical\nperception in category learning by neural nets, and perhaps also (6) a\nconjecture by Chomsky about the laws of thought. The exposition will be in the\nform of a dialogue with ChatGPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.02243v1.pdf"
    },
    {
        "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
        "authors": [
            "Yongshuo Zong",
            "Ondrej Bohdal",
            "Tingyang Yu",
            "Yongxin Yang",
            "Timothy Hospedales"
        ],
        "published": "2024-02-03T16:43:42Z",
        "summary": "Current vision large language models (VLLMs) exhibit remarkable capabilities\nyet are prone to generate harmful content and are vulnerable to even the\nsimplest jailbreaking attacks. Our initial analysis finds that this is due to\nthe presence of harmful data during vision-language instruction fine-tuning,\nand that VLLM fine-tuning can cause forgetting of safety alignment previously\nlearned by the underpinning LLM. To address this issue, we first curate a\nvision-language safe instruction-following dataset VLGuard covering various\nharmful categories. Our experiments demonstrate that integrating this dataset\ninto standard vision-language fine-tuning or utilizing it for post-hoc\nfine-tuning effectively safety aligns VLLMs. This alignment is achieved with\nminimal impact on, or even enhancement of, the models' helpfulness. The\nversatility of our safety fine-tuning dataset makes it a valuable resource for\nsafety-testing existing VLLMs, training new models or safeguarding pre-trained\nVLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject\nunsafe instructions and substantially reduce the success rates of several\nblack-box adversarial attacks, which approach zero in many cases. The code and\ndataset are available at https://github.com/ys-zong/VLGuard.",
        "pdf_link": "https://arxiv.org/pdf/2402.02207v1.pdf"
    },
    {
        "title": "GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events",
        "authors": [
            "Xingcheng Zhou",
            "Alois C. Knoll"
        ],
        "published": "2024-02-03T16:38:25Z",
        "summary": "The recognition and understanding of traffic incidents, particularly traffic\naccidents, is a topic of paramount importance in the realm of intelligent\ntransportation systems and intelligent vehicles. This area has continually\ncaptured the extensive focus of both the academic and industrial sectors.\nIdentifying and comprehending complex traffic events is highly challenging,\nprimarily due to the intricate nature of traffic environments, diverse\nobservational perspectives, and the multifaceted causes of accidents. These\nfactors have persistently impeded the development of effective solutions. The\nadvent of large vision-language models (VLMs) such as GPT-4V, has introduced\ninnovative approaches to addressing this issue. In this paper, we explore the\nability of GPT-4V with a set of representative traffic incident videos and\ndelve into the model's capacity of understanding these complex traffic\nsituations. We observe that GPT-4V demonstrates remarkable cognitive,\nreasoning, and decision-making ability in certain classic traffic events.\nConcurrently, we also identify certain limitations of GPT-4V, which constrain\nits understanding in more intricate scenarios. These limitations merit further\nexploration and resolution.",
        "pdf_link": "https://arxiv.org/pdf/2402.02205v3.pdf"
    },
    {
        "title": "Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations",
        "authors": [
            "Luca Podo",
            "Muhammad Ishmal",
            "Marco Angelini"
        ],
        "published": "2024-02-03T14:28:55Z",
        "summary": "The automatic generation of visualizations is an old task that, through the\nyears, has shown more and more interest from the research and practitioner\ncommunities. Recently, large language models (LLM) have become an interesting\noption for supporting generative tasks related to visualization, demonstrating\ninitial promising results. At the same time, several pitfalls, like the\nmultiple ways of instructing an LLM to generate the desired result, the\ndifferent perspectives leading the generation (code-based, image-based,\ngrammar-based), and the presence of hallucinations even for the visualization\ngeneration task, make their usage less affordable than expected. Following\nsimilar initiatives for benchmarking LLMs, this paper copes with the problem of\nmodeling the evaluation of a generated visualization through an LLM. We propose\na theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort\nin its atomic components, characterizes their nature, and provides an overview\nof how to implement and interpret them. We also designed and implemented an\nevaluation platform that provides a benchmarking resource for the visualization\ngeneration task. The platform supports automatic and manual scoring conducted\nby multiple assessors to support a fine-grained and semantic evaluation based\non the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and\nLlama2-70-b models show the benefits of EvaLLM and illustrate interesting\nresults on the current state-of-the-art LLM-generated visualizations.",
        "pdf_link": "https://arxiv.org/pdf/2402.02167v1.pdf"
    },
    {
        "title": "Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models",
        "authors": [
            "Alapan Kuila",
            "Somnath Jena",
            "Sudeshna Sarkar",
            "Partha Pratim Chakrabarti"
        ],
        "published": "2024-02-03T13:27:32Z",
        "summary": "In today's media landscape, where news outlets play a pivotal role in shaping\npublic opinion, it is imperative to address the issue of sentiment manipulation\nwithin news text. News writers often inject their own biases and emotional\nlanguage, which can distort the objectivity of reporting. This paper introduces\na novel approach to tackle this problem by reducing the polarity of latent\nsentiments in news content. Drawing inspiration from adversarial attack-based\nsentence perturbation techniques and a prompt based method using ChatGPT, we\nemploy transformation constraints to modify sentences while preserving their\ncore semantics. Using three perturbation methods: replacement, insertion, and\ndeletion coupled with a context-aware masked language model, we aim to maximize\nthe desired sentiment score for targeted news aspects through a beam search\nalgorithm. Our experiments and human evaluations demonstrate the effectiveness\nof these two models in achieving reduced sentiment polarity with minimal\nmodifications while maintaining textual similarity, fluency, and grammatical\ncorrectness. Comparative analysis confirms the competitive performance of the\nadversarial attack based perturbation methods and prompt-based methods,\noffering a promising solution to foster more objective news reporting and\ncombat emotional language bias in the media.",
        "pdf_link": "https://arxiv.org/pdf/2402.02145v1.pdf"
    },
    {
        "title": "Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test",
        "authors": [
            "Aditi Khandelwal",
            "Utkarsh Agarwal",
            "Kumar Tanmay",
            "Monojit Choudhury"
        ],
        "published": "2024-02-03T12:52:36Z",
        "summary": "This paper explores the moral judgment and moral reasoning abilities\nexhibited by Large Language Models (LLMs) across languages through the Defining\nIssues Test. It is a well known fact that moral judgment depends on the\nlanguage in which the question is asked. We extend the work of beyond English,\nto 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe\nthree LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial\nmultilingual text processing and generation abilities. Our study shows that the\nmoral reasoning ability for all models, as indicated by the post-conventional\nscore, is substantially inferior for Hindi and Swahili, compared to Spanish,\nRussian, Chinese and English, while there is no clear trend for the performance\nof the latter four languages. The moral judgments too vary considerably by the\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2402.02135v1.pdf"
    },
    {
        "title": "Rendering Graphs for Graph Reasoning in Multimodal Large Language Models",
        "authors": [
            "Yanbin Wei",
            "Shuai Fu",
            "Weisen Jiang",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "published": "2024-02-03T12:19:47Z",
        "summary": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures, such as robotic planning, knowledge graph completion, and\ncommon-sense reasoning. Though LLMs can comprehend graph information in a\ntextual format, they overlook the rich visual modality, which is an intuitive\nway for humans to comprehend structural information and conduct graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., visual graph) is still unexplored. In this\npaper, we take the first step in incorporating visual information into graph\nreasoning tasks and propose a new benchmark GITQA, where each sample is a tuple\n(graph, image, textual description). We conduct extensive experiments on the\nGITQA benchmark using state-of-the-art multimodal LLMs. Results on graph\nreasoning tasks show that combining textual and visual information together\nperforms better than using one modality alone. Moreover, the LLaVA-7B/13B\nmodels finetuned on the training set (referred to as GITA), achieve higher\naccuracy than the closed-source model GPT-4(V). We also study the effects of\naugmentations in graph reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.02130v3.pdf"
    },
    {
        "title": "Are Large Language Models Good Prompt Optimizers?",
        "authors": [
            "Ruotian Ma",
            "Xiaolei Wang",
            "Xin Zhou",
            "Jian Li",
            "Nan Du",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-03T09:48:54Z",
        "summary": "LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as\nPrompt Optimizers to self-reflect and refine prompts, has shown promising\nperformance in recent studies. Despite the success, the underlying mechanism of\nthis approach remains unexplored, and the true effectiveness of LLMs as Prompt\nOptimizers requires further validation. In this work, we conducted a\ncomprehensive study to uncover the actual mechanism of LLM-based Prompt\nOptimization. Our findings reveal that the LLM optimizers struggle to identify\nthe true causes of errors during reflection, tending to be biased by their own\nprior knowledge rather than genuinely reflecting on the errors. Furthermore,\neven when the reflection is semantically valid, the LLM optimizers often fail\nto generate appropriate prompts for the target models with a single prompt\nrefinement step, partly due to the unpredictable behaviors of the target\nmodels. Based on the observations, we introduce a new \"Automatic Behavior\nOptimization\" paradigm, which directly optimizes the target model's behavior in\na more controllable manner. We hope our study can inspire new directions for\nautomatic prompt optimization development.",
        "pdf_link": "https://arxiv.org/pdf/2402.02101v1.pdf"
    },
    {
        "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
        "authors": [
            "Yichao Fu",
            "Peter Bailis",
            "Ion Stoica",
            "Hao Zhang"
        ],
        "published": "2024-02-03T06:37:50Z",
        "summary": "Autoregressive decoding of large language models (LLMs) is memory bandwidth\nbounded, resulting in high latency and significant wastes of the parallel\nprocessing power of modern accelerators. Existing methods for accelerating LLM\ndecoding often require a draft model (e.g., speculative decoding), which is\nnontrivial to obtain and unable to generalize. In this paper, we introduce\nLookahead decoding, an exact, parallel decoding algorithm that accelerates LLM\ndecoding without needing auxiliary models or data stores. It allows trading\nper-step log(FLOPs) to reduce the number of total decoding steps, is more\nparallelizable on single or multiple modern accelerators, and is compatible\nwith concurrent memory-efficient attention (e.g., FlashAttention). Our\nimplementation of Lookahead decoding can speed up autoregressive decoding by up\nto 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code\ncompletion tasks. Our code is avialable at\nhttps://github.com/hao-ai-lab/LookaheadDecoding",
        "pdf_link": "https://arxiv.org/pdf/2402.02057v1.pdf"
    },
    {
        "title": "Affordable Generative Agents",
        "authors": [
            "Yangbin Yu",
            "Qin Zhang",
            "Junyou Li",
            "Qiang Fu",
            "Deheng Ye"
        ],
        "published": "2024-02-03T06:16:28Z",
        "summary": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\n\\url{https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents}.",
        "pdf_link": "https://arxiv.org/pdf/2402.02053v1.pdf"
    },
    {
        "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs",
        "authors": [
            "Yifan Zhong",
            "Chengdong Ma",
            "Xiaoyuan Zhang",
            "Ziran Yang",
            "Qingfu Zhang",
            "Siyuan Qi",
            "Yaodong Yang"
        ],
        "published": "2024-02-03T05:01:04Z",
        "summary": "Current methods for large language model alignment typically use scalar human\npreference labels. However, this convention tends to oversimplify the\nmulti-dimensional and heterogeneous nature of human preferences, leading to\nreduced expressivity and even misalignment. This paper presents Panacea, an\ninnovative approach that reframes alignment as a multi-dimensional preference\noptimization problem. Panacea trains a single model capable of adapting online\nand Pareto-optimally to diverse sets of preferences without the need for\nfurther tuning. A major challenge here is using a low-dimensional preference\nvector to guide the model's behavior, despite it being governed by an\noverwhelmingly large number of parameters. To address this, Panacea is designed\nto use singular value decomposition (SVD)-based low-rank adaptation, which\nallows the preference vector to be simply injected online as singular values.\nTheoretically, we prove that Panacea recovers the entire Pareto front with\ncommon loss aggregation methods under mild conditions. Moreover, our\nexperiments demonstrate, for the first time, the feasibility of aligning a\nsingle LLM to represent a spectrum of human preferences through various\noptimization methods. Our work marks a step forward in effectively and\nefficiently aligning models to diverse and intricate human preferences in a\ncontrollable and Pareto-optimal manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.02030v1.pdf"
    },
    {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "authors": [
            "Sreyan Ghosh",
            "Chandra Kiran Reddy Evuru",
            "Sonal Kumar",
            "Ramaneswaran S",
            "Deepali Aneja",
            "Zeyu Jin",
            "Ramani Duraiswami",
            "Dinesh Manocha"
        ],
        "published": "2024-02-03T04:45:25Z",
        "summary": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed inspire future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.05119v3.pdf"
    },
    {
        "title": "How well do LLMs cite relevant medical references? An evaluation framework and analyses",
        "authors": [
            "Kevin Wu",
            "Eric Wu",
            "Ally Cassasola",
            "Angela Zhang",
            "Kevin Wei",
            "Teresa Nguyen",
            "Sith Riantawan",
            "Patricia Shi Riantawan",
            "Daniel E. Ho",
            "James Zou"
        ],
        "published": "2024-02-03T03:44:57Z",
        "summary": "Large language models (LLMs) are currently being used to answer medical\nquestions across a variety of clinical domains. Recent top-performing\ncommercial LLMs, in particular, are also capable of citing sources to support\ntheir responses. In this paper, we ask: do the sources that LLMs generate\nactually support the claims that they make? To answer this, we propose three\ncontributions. First, as expert medical annotations are an expensive and\ntime-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is\nhighly accurate in validating source relevance, agreeing 88% of the time with a\npanel of medical doctors. Second, we develop an end-to-end, automated pipeline\ncalled \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs\non a dataset of 1200 generated questions, totaling over 40K pairs of statements\nand sources. Interestingly, we find that between ~50% to 90% of LLM responses\nare not fully supported by the sources they provide. We also evaluate GPT-4\nwith retrieval augmented generation (RAG) and find that, even still, around\n30\\% of individual statements are unsupported, while nearly half of its\nresponses are not fully supported. Third, we open-source our curated dataset of\nmedical questions and expert annotations for future evaluations. Given the\nrapid pace of LLM development and the potential harms of incorrect or outdated\nmedical information, it is crucial to also understand and quantify their\ncapability to produce relevant, trustworthy medical references.",
        "pdf_link": "https://arxiv.org/pdf/2402.02008v1.pdf"
    },
    {
        "title": "PresAIse, A Prescriptive AI Solution for Enterprises",
        "authors": [
            "Wei Sun",
            "Scott McFaddin",
            "Linh Ha Tran",
            "Shivaram Subramanian",
            "Kristjan Greenewald",
            "Yeshi Tenzin",
            "Zack Xue",
            "Youssef Drissi",
            "Markus Ettl"
        ],
        "published": "2024-02-03T03:23:08Z",
        "summary": "Prescriptive AI represents a transformative shift in decision-making,\noffering causal insights and actionable recommendations. Despite its huge\npotential, enterprise adoption often faces several challenges. The first\nchallenge is caused by the limitations of observational data for accurate\ncausal inference which is typically a prerequisite for good decision-making.\nThe second pertains to the interpretability of recommendations, which is\ncrucial for enterprise decision-making settings. The third challenge is the\nsilos between data scientists and business users, hindering effective\ncollaboration. This paper outlines an initiative from IBM Research, aiming to\naddress some of these challenges by offering a suite of prescriptive AI\nsolutions. Leveraging insights from various research papers, the solution suite\nincludes scalable causal inference methods, interpretable decision-making\napproaches, and the integration of large language models (LLMs) to bridge\ncommunication gaps via a conversation agent. A proof-of-concept, PresAIse,\ndemonstrates the solutions' potential by enabling non-ML experts to interact\nwith prescriptive AI models via a natural language interface, democratizing\nadvanced analytics for strategic decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.02006v2.pdf"
    },
    {
        "title": "Human-Centered Privacy Research in the Age of Large Language Models",
        "authors": [
            "Tianshi Li",
            "Sauvik Das",
            "Hao-Ping Lee",
            "Dakuo Wang",
            "Bingsheng Yao",
            "Zhiping Zhang"
        ],
        "published": "2024-02-03T02:32:45Z",
        "summary": "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
        "pdf_link": "https://arxiv.org/pdf/2402.01994v1.pdf"
    },
    {
        "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
        "authors": [
            "Isabel O. Gallegos",
            "Ryan A. Rossi",
            "Joe Barrow",
            "Md Mehrab Tanjim",
            "Tong Yu",
            "Hanieh Deilamsalehy",
            "Ruiyi Zhang",
            "Sungchul Kim",
            "Franck Dernoncourt"
        ],
        "published": "2024-02-03T01:40:11Z",
        "summary": "Large language models (LLMs) have shown remarkable advances in language\ngeneration and understanding but are also prone to exhibiting harmful social\nbiases. While recognition of these behaviors has generated an abundance of bias\nmitigation techniques, most require modifications to the training data, model\nparameters, or decoding strategy, which may be infeasible without access to a\ntrainable model. In this work, we leverage the zero-shot capabilities of LLMs\nto reduce stereotyping in a technique we introduce as zero-shot self-debiasing.\nWith two approaches, self-debiasing via explanation and self-debiasing via\nreprompting, we show that self-debiasing can significantly reduce the degree of\nstereotyping across nine different social groups while relying only on the LLM\nitself and a simple prompt, with explanations correctly identifying invalid\nassumptions and reprompting delivering the greatest reductions in bias. We hope\nthis work opens inquiry into other zero-shot techniques for bias mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2402.01981v1.pdf"
    },
    {
        "title": "SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks",
        "authors": [
            "Gourab Dey",
            "Adithya V Ganesan",
            "Yash Kumar Lal",
            "Manal Shah",
            "Shreyashee Sinha",
            "Matthew Matero",
            "Salvatore Giorgi",
            "Vivek Kulkarni",
            "H. Andrew Schwartz"
        ],
        "published": "2024-02-03T01:33:16Z",
        "summary": "Social science NLP tasks, such as emotion or humor detection, are required to\ncapture the semantics along with the implicit pragmatics from text, often with\nlimited amounts of training data. Instruction tuning has been shown to improve\nthe many capabilities of large language models (LLMs) such as commonsense\nreasoning, reading comprehension, and computer programming. However, little is\nknown about the effectiveness of instruction tuning on the social domain where\nimplicit pragmatic cues are often needed to be captured. We explore the use of\ninstruction tuning for social science NLP tasks and introduce Socialite-Llama\n-- an open-source, instruction-tuned Llama. On a suite of 20 social science\ntasks, Socialite-Llama improves upon the performance of Llama as well as\nmatches or improves upon the performance of a state-of-the-art, multi-task\nfinetuned model on a majority of them. Further, Socialite-Llama also leads to\nimprovement on 5 out of 6 related social tasks as compared to Llama, suggesting\ninstruction tuning can lead to generalized social understanding. All resources\nincluding our code, model and dataset can be found through\nbit.ly/socialitellama.",
        "pdf_link": "https://arxiv.org/pdf/2402.01980v2.pdf"
    },
    {
        "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",
        "authors": [
            "Hung Du",
            "Srikanth Thudumu",
            "Rajesh Vasa",
            "Kon Mouzakis"
        ],
        "published": "2024-02-03T00:27:22Z",
        "summary": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.01968v1.pdf"
    },
    {
        "title": "MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles",
        "authors": [
            "Amrita Ganguly",
            "Al Nahian Bin Emran",
            "Sadiya Sayara Chowdhury Puspo",
            "Md Nishat Raihan",
            "Dhiman Goswami",
            "Marcos Zampieri"
        ],
        "published": "2024-02-03T00:23:36Z",
        "summary": "The automatic identification of offensive language such as hate speech is\nimportant to keep discussions civil in online communities. Identifying hate\nspeech in multimodal content is a particularly challenging task because\noffensiveness can be manifested in either words or images or a juxtaposition of\nthe two. This paper presents the MasonPerplexity submission for the Shared Task\non Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task\nis divided into two sub-tasks: sub-task A focuses on the identification of hate\nspeech and sub-task B focuses on the identification of targets in text-embedded\nimages during political events. We use an XLM-roBERTa-large model for sub-task\nA and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and\nBERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A\nand 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01967v2.pdf"
    },
    {
        "title": "Large Language Model Agent for Hyper-Parameter Optimization",
        "authors": [
            "Siyi Liu",
            "Chen Gao",
            "Yong Li"
        ],
        "published": "2024-02-02T20:12:05Z",
        "summary": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.01881v2.pdf"
    },
    {
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models",
        "authors": [
            "Moschoula Pternea",
            "Prerna Singh",
            "Abir Chakraborty",
            "Yagna Oruganti",
            "Mirco Milletari",
            "Sayli Bapat",
            "Kebei Jiang"
        ],
        "published": "2024-02-02T20:01:15Z",
        "summary": "In this work, we review research studies that combine Reinforcement Learning\n(RL) and Large Language Models (LLMs), two areas that owe their momentum to the\ndevelopment of deep neural networks. We propose a novel taxonomy of three main\nclasses based on the way that the two model types interact with each other. The\nfirst class, RL4LLM, includes studies where RL is leveraged to improve the\nperformance of LLMs on tasks related to Natural Language Processing. L4LLM is\ndivided into two sub-categories depending on whether RL is used to directly\nfine-tune an existing LLM or to improve the prompt of the LLM. In the second\nclass, LLM4RL, an LLM assists the training of an RL model that performs a task\nthat is not inherently related to natural language. We further break down\nLLM4RL based on the component of the RL training framework that the LLM assists\nor replaces, namely reward shaping, goal generation, and policy function.\nFinally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a\ncommon planning framework without either of them contributing to training or\nfine-tuning of the other. We further branch this class to distinguish between\nstudies with and without natural language feedback. We use this taxonomy to\nexplore the motivations behind the synergy of LLMs and RL and explain the\nreasons for its success, while pinpointing potential shortcomings and areas\nwhere further research is needed, as well as alternative methodologies that\nserve the same goal.",
        "pdf_link": "https://arxiv.org/pdf/2402.01874v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision",
        "authors": [
            "Jinyan Su",
            "Peilin Yu",
            "Jieyu Zhang",
            "Stephen H. Bach"
        ],
        "published": "2024-02-02T19:45:39Z",
        "summary": "Prompted weak supervision (PromptedWS) applies pre-trained large language\nmodels (LLMs) as the basis for labeling functions (LFs) in a weak supervision\nframework to obtain large labeled datasets. We further extend the use of LLMs\nin the loop to address one of the key challenges in weak supervision: learning\nthe statistical dependency structure among supervision sources. In this work,\nwe ask the LLM how similar are these prompted LFs. We propose a Structure\nRefining Module, a simple yet effective first approach based on the\nsimilarities of the prompts by taking advantage of the intrinsic structure in\nthe embedding space. At the core of Structure Refining Module are Labeling\nFunction Removal (LaRe) and Correlation Structure Generation (CosGen). Compared\nto previous methods that learn the dependencies from weak labels, our method\nfinds the dependencies which are intrinsic to the LFs and less dependent on the\ndata. We show that our Structure Refining Module improves the PromptedWS\npipeline by up to 12.7 points on the benchmark tasks. We also explore the\ntrade-offs between efficiency and performance with comprehensive ablation\nexperiments and analysis. Code for this project can be found in\nhttps://github.com/BatsResearch/su-bigdata23-code.",
        "pdf_link": "https://arxiv.org/pdf/2402.01867v1.pdf"
    },
    {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "authors": [
            "Xisen Jin",
            "Xiang Ren"
        ],
        "published": "2024-02-02T19:43:15Z",
        "summary": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "pdf_link": "https://arxiv.org/pdf/2402.01865v1.pdf"
    },
    {
        "title": "(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",
        "authors": [
            "Inyoung Cheong",
            "King Xia",
            "K. J. Kevin Feng",
            "Quan Ze Chen",
            "Amy X. Zhang"
        ],
        "published": "2024-02-02T19:35:34Z",
        "summary": "The rapid proliferation of large language models (LLMs) as general purpose\nchatbots available to the public raises hopes around expanding access to\nprofessional guidance in law, medicine, and finance, while triggering concerns\nabout public reliance on LLMs for high-stakes circumstances. Prior research has\nspeculated on high-level ethical considerations but lacks concrete criteria\ndetermining when and why LLM chatbots should or should not provide professional\nassistance. Through examining the legal domain, we contribute a structured\nexpert analysis to uncover nuanced policy considerations around using LLMs for\nprofessional advice, using methods inspired by case-based reasoning. We\nconvened workshops with 20 legal experts and elicited dimensions on appropriate\nAI assistance for sample user queries (``cases''). We categorized our expert\ndimensions into: (1) user attributes, (2) query characteristics, (3) AI\ncapabilities, and (4) impacts. Beyond known issues like hallucinations, experts\nrevealed novel legal problems, including that users' conversations with LLMs\nare not protected by attorney-client confidentiality or bound to professional\nethics that guard against conflicted counsel or poor quality advice. This\naccountability deficit led participants to advocate for AI systems to help\nusers polish their legal questions and relevant facts, rather than recommend\nspecific actions. More generally, we highlight the potential of case-based\nexpert deliberation as a method of responsibly translating professional\nintegrity and domain knowledge into design requirements to inform appropriate\nAI behavior when generating advice in professional domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.01864v1.pdf"
    },
    {
        "title": "Cross-modality debiasing: using language to mitigate sub-population shifts in imaging",
        "authors": [
            "Yijiang Pang",
            "Bao Hoang",
            "Jiayu Zhou"
        ],
        "published": "2024-02-02T18:54:48Z",
        "summary": "Sub-population shift is a specific type of domain shift that highlights\nchanges in data distribution within specific sub-groups or populations between\ntraining and testing. Sub-population shift accounts for a significant source of\nalgorithmic bias and calls for distributional robustness. Recent studies found\ninherent distributional robustness in multi-modality foundation models, such as\nthe vision-language model CLIP, yet this robustness is vulnerable through\nparameter fine-tuning. In this paper, we propose leveraging the connection of\nrobustness among different modalities and reshaping the distributional\nrobustness of one modality with another. Specifically, in the context of the\ndistributional robustness of CLIP, we propose to leverage natural language\ninputs to debias the image feature representations, to improve worst-case\nperformance on sub-populations. Our extensive empirical studies show that image\nrepresentations debiased by natural language can achieve significant\nperformance improvement and reduction of performance instability under\nsub-population shifts.",
        "pdf_link": "https://arxiv.org/pdf/2403.07888v2.pdf"
    },
    {
        "title": "Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment",
        "authors": [
            "Kun-Peng Ning",
            "Shuo Yang",
            "Yu-Yang Liu",
            "Jia-Yu Yao",
            "Zhen-Hui Liu",
            "Yu Wang",
            "Ming Pang",
            "Li Yuan"
        ],
        "published": "2024-02-02T18:49:26Z",
        "summary": "Existing large language models (LLMs) evaluation methods typically focus on\ntesting the performance on some closed-environment and domain-specific\nbenchmarks with human annotations. In this paper, we explore a novel\nunsupervised evaluation direction, utilizing peer-review mechanisms to measure\nLLMs automatically. In this setting, both open-source and closed-source LLMs\nlie in the same environment, capable of answering unlabeled questions and\nevaluating each other, where each LLM's response score is jointly determined by\nother anonymous ones. To obtain the ability hierarchy among these models, we\nassign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize\nthe consistency of each LLM's capabilities and scores. The key assumption\nbehind is that high-level LLM can evaluate others' answers more accurately than\nlow-level ones, while higher-level LLM can also achieve higher response scores.\nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap\nin aligning human rankings. We perform experiments on multiple datasets with\nthese metrics, validating the effectiveness of the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2402.01830v1.pdf"
    },
    {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "published": "2024-02-02T18:39:51Z",
        "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.01622v2.pdf"
    },
    {
        "title": "Stochastic Two Points Method for Deep Model Zeroth-order Optimization",
        "authors": [
            "Yijiang Pang",
            "Jiayu Zhou"
        ],
        "published": "2024-02-02T18:39:40Z",
        "summary": "Large foundation models, such as large language models, have performed\nexceptionally well in various application scenarios. Building or fully\nfine-tuning such large models is usually prohibitive due to either hardware\nbudget or lack of access to backpropagation. The zeroth-order methods offer a\npromising direction for tackling this challenge, where only forward passes are\nneeded to update the model. This paper introduces an efficient Stochastic\nTwo-Point (S2P) approach within the gradient-free regime. We present the\ntheoretical convergence properties of S2P under the general and relaxed\nsmoothness assumptions. The theoretical properties also shed light on a faster\nand more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new\nconvergence properties that better represent the dynamics of deep models in\ntraining. Our comprehensive empirical results show that AS2P is highly\neffective in optimizing objectives for large deep models, including language\nmodels, and outperforms standard methods across various model types and scales,\nwith 2 $\\times$ speed-up in training over most conducted tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01621v1.pdf"
    },
    {
        "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
        "authors": [
            "Justin Chih-Yao Chen",
            "Swarnadeep Saha",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "published": "2024-02-02T18:35:14Z",
        "summary": "Multi-agent interactions between Large Language Model (LLM) agents have shown\nmajor improvements on diverse reasoning tasks. However, these involve long\ngenerations from multiple models across several rounds, making them expensive.\nMoreover, these multi-agent approaches fail to provide a final, single model\nfor efficient inference. To address this, we introduce MAGDi, a new method for\nstructured distillation of the reasoning interactions between multiple LLMs\ninto smaller LMs. MAGDi teaches smaller models by representing multi-agent\ninteractions as graphs, augmenting a base student model with a graph encoder,\nand distilling knowledge using three objective functions: next-token\nprediction, a contrastive loss between correct and incorrect reasoning, and a\ngraph-based objective to model the interaction structure. Experiments on seven\nwidely-used commonsense and math reasoning benchmarks show that MAGDi improves\nthe reasoning capabilities of smaller models, outperforming several methods\nthat distill from a single teacher and multiple teachers. Moreover, MAGDi also\ndemonstrates an order of magnitude higher efficiency over its teachers. We\nconduct extensive analyses to show that MAGDi (1) enhances the generalizability\nto out-of-domain tasks, (2) scales positively with the size and strength of the\nbase student model, and (3) obtains larger improvements (via our multi-teacher\ntraining) when applying self-consistency - an inference technique that relies\non model diversity.",
        "pdf_link": "https://arxiv.org/pdf/2402.01620v1.pdf"
    },
    {
        "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
        "authors": [
            "Jiajie Zhang",
            "Shulin Cao",
            "Linmei Hu",
            "Ling Feng",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2024-02-02T18:32:24Z",
        "summary": "Program induction (PI) has become a promising paradigm for using knowledge\nbases (KBs) to help large language models (LLMs) answer complex\nknowledge-intensive questions. Nonetheless, PI typically relies on a large\nnumber of parallel question-program pairs to make the LLM aware of the schema\nof the given KB, and is thus challenging for many low-resourced KBs that lack\nannotated data. To this end, we propose KB-Plugin, a plug-and-play framework\nthat enables LLMs to induce programs over any low-resourced KB. Firstly,\nKB-Plugin adopts self-supervised learning to encode the detailed schema\ninformation of a given KB into a pluggable module, namely schema plugin.\nSecondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB\nto train another pluggable module, namely PI plugin, which can help the LLM\nextract question-relevant schema information from the schema plugin of any KB\nand utilize this information to induce programs over this KB. Experiments on\nfive heterogeneous KBQA datasets show that KB-Plugin achieves better or\ncomparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI\nmethods for low-resourced KBs, and even approaches the performance of\nsupervised methods. Our code and data are available at\nhttps://github.com/THU-KEG/KB-Plugin.",
        "pdf_link": "https://arxiv.org/pdf/2402.01619v1.pdf"
    },
    {
        "title": "Style Vectors for Steering Generative Large Language Model",
        "authors": [
            "Kai Konen",
            "Sophie Jentzsch",
            "Diaoul\u00e9 Diallo",
            "Peer Sch\u00fctt",
            "Oliver Bensch",
            "Roxanne El Baff",
            "Dominik Opitz",
            "Tobias Hecking"
        ],
        "published": "2024-02-02T18:31:15Z",
        "summary": "This research explores strategies for steering the output of large language\nmodels (LLMs) towards specific styles, such as sentiment, emotion, or writing\nstyle, by adding style vectors to the activations of hidden layers during text\ngeneration. We show that style vectors can be simply computed from recorded\nlayer activations for input texts in a specific style in contrast to more\ncomplex training-based approaches. Through a series of experiments, we\ndemonstrate the effectiveness of activation engineering using such style\nvectors to influence the style of generated text in a nuanced and\nparameterisable way, distinguishing it from prompt engineering. The presented\nresearch constitutes a significant step towards developing more adaptive and\neffective AI-empowered interactive systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.01618v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature",
        "authors": [
            "Yuting Guo",
            "Seyedeh Somayyeh Mousavi",
            "Reza Sameni",
            "Abeed Sarker"
        ],
        "published": "2024-02-02T18:15:51Z",
        "summary": "Hypertension, defined as blood pressure (BP) that is above normal, holds\nparamount significance in the realm of public health, as it serves as a\ncritical precursor to various cardiovascular diseases (CVDs) and significantly\ncontributes to elevated mortality rates worldwide. However, many existing BP\nmeasurement technologies and standards might be biased because they do not\nconsider clinical outcomes, comorbidities, or demographic factors, making them\ninconclusive for diagnostic purposes. There is limited data-driven research\nfocused on studying the variance in BP measurements across these variables. In\nthis work, we employed GPT-35-turbo, a large language model (LLM), to\nautomatically extract the mean and standard deviation values of BP for both\nmales and females from a dataset comprising 25 million abstracts sourced from\nPubMed. 993 article abstracts met our predefined inclusion criteria (i.e.,\npresence of references to blood pressure, units of blood pressure such as mmHg,\nand mention of biological sex). Based on the automatically-extracted\ninformation from these articles, we conducted an analysis of the variations of\nBP values across biological sex. Our results showed the viability of utilizing\nLLMs to study the BP variations across different demographic factors.",
        "pdf_link": "https://arxiv.org/pdf/2402.01826v1.pdf"
    },
    {
        "title": "Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning",
        "authors": [
            "Debarun Bhattacharjya",
            "Junkyu Lee",
            "Don Joven Agravante",
            "Balaji Ganesan",
            "Radu Marinescu"
        ],
        "published": "2024-02-02T18:00:35Z",
        "summary": "Foundation models (FMs) such as large language models have revolutionized the\nfield of AI by showing remarkable performance in various tasks. However, they\nexhibit numerous limitations that prevent their broader adoption in many\nreal-world systems, which often require a higher bar for trustworthiness and\nusability. Since FMs are trained using loss functions aimed at reconstructing\nthe training corpus in a self-supervised manner, there is no guarantee that the\nmodel's output aligns with users' preferences for a specific task at hand. In\nthis survey paper, we propose a conceptual framework that encapsulates\ndifferent modes by which agents could interact with FMs and guide them suitably\nfor a set of tasks, particularly through knowledge augmentation and reasoning.\nOur framework elucidates agent role categories such as updating the underlying\nFM, assisting with prompting the FM, and evaluating the FM output. We also\ncategorize several state-of-the-art approaches into agent interaction\nprotocols, highlighting the nature and extent of involvement of the various\nagent roles. The proposed framework provides guidance for future directions to\nfurther realize the power of FMs in practical AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.01602v1.pdf"
    },
    {
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
        "authors": [
            "Wenyue Hua",
            "Xianjun Yang",
            "Zelong Li",
            "Wei Cheng",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-02T17:26:23Z",
        "summary": "The emergence of LLM-based agents has garnered considerable attention, yet\ntheir trustworthiness remains an under-explored area. As agents can directly\ninteract with the physical environment, their reliability and safety is\ncritical. This paper presents an Agent-Constitution-based agent framework,\nTrustAgent, an initial investigation into improving the safety dimension of\ntrustworthiness in LLM-based agents. This framework consists of threefold\nstrategies: pre-planning strategy which injects safety knowledge to the model\nprior to plan generation, in-planning strategy which bolsters safety during\nplan generation, and post-planning strategy which ensures safety by\npost-planning inspection. Through experimental analysis, we demonstrate how\nthese approaches can effectively elevate an LLM agent's safety by identifying\nand preventing potential dangers. Furthermore, we explore the intricate\nrelationships between safety and helpfulness, and between the model's reasoning\nability and its efficacy as a safe agent. This paper underscores the imperative\nof integrating safety awareness and trustworthiness into the design and\ndeployment of LLM-based agents, not only to enhance their performance but also\nto ensure their responsible integration into human-centric environments. Data\nand code are available at https://github.com/agiresearch/TrustAgent.",
        "pdf_link": "https://arxiv.org/pdf/2402.01586v2.pdf"
    },
    {
        "title": "Building Guardrails for Large Language Models",
        "authors": [
            "Yi Dong",
            "Ronghui Mu",
            "Gaojie Jin",
            "Yi Qi",
            "Jinwei Hu",
            "Xingyu Zhao",
            "Jie Meng",
            "Wenjie Ruan",
            "Xiaowei Huang"
        ],
        "published": "2024-02-02T16:35:00Z",
        "summary": "As Large Language Models (LLMs) become more integrated into our daily lives,\nit is crucial to identify and mitigate their risks, especially when the risks\ncan have profound impacts on human users and societies. Guardrails, which\nfilter the inputs or outputs of LLMs, have emerged as a core safeguarding\ntechnology. This position paper takes a deep look at current open-source\nsolutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the\nchallenges and the road towards building more complete solutions. Drawing on\nrobust evidence from previous research, we advocate for a systematic approach\nto construct guardrails for LLMs, based on comprehensive consideration of\ndiverse contexts across various LLMs applications. We propose employing\nsocio-technical methods through collaboration with a multi-disciplinary team to\npinpoint precise technical requirements, exploring advanced neural-symbolic\nimplementations to embrace the complexity of the requirements, and developing\nverification and testing to ensure the utmost quality of the final product.",
        "pdf_link": "https://arxiv.org/pdf/2402.01822v1.pdf"
    },
    {
        "title": "Ecologically rational meta-learned inference explains human category learning",
        "authors": [
            "Akshay K. Jagadish",
            "Julian Coda-Forno",
            "Mirko Thalmann",
            "Eric Schulz",
            "Marcel Binz"
        ],
        "published": "2024-02-02T16:32:04Z",
        "summary": "Ecological rationality refers to the notion that humans are rational agents\nadapted to their environment. However, testing this theory remains challenging\ndue to two reasons: the difficulty in defining what tasks are ecologically\nvalid and building rational models for these tasks. In this work, we\ndemonstrate that large language models can generate cognitive tasks,\nspecifically category learning tasks, that match the statistics of real-world\ntasks, thereby addressing the first challenge. We tackle the second challenge\nby deriving rational agents adapted to these tasks using the framework of\nmeta-learning, leading to a class of models called ecologically rational\nmeta-learned inference (ERMI). ERMI quantitatively explains human data better\nthan seven other cognitive models in two different experiments. It additionally\nmatches human behavior on a qualitative level: (1) it finds the same tasks\ndifficult that humans find difficult, (2) it becomes more reliant on an\nexemplar-based strategy for assigning categories with learning, and (3) it\ngeneralizes to unseen stimuli in a human-like way. Furthermore, we show that\nERMI's ecologically valid priors allow it to achieve state-of-the-art\nperformance on the OpenML-CC18 classification benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2402.01821v1.pdf"
    },
    {
        "title": "Homogenization Effects of Large Language Models on Human Creative Ideation",
        "authors": [
            "Barrett R. Anderson",
            "Jash Hemant Shah",
            "Max Kreminski"
        ],
        "published": "2024-02-02T16:27:11Z",
        "summary": "Large language models (LLMs) are now being used in a wide variety of\ncontexts, including as creativity support tools (CSTs) intended to help their\nusers come up with new ideas. But do LLMs actually support user creativity? We\nhypothesized that the use of an LLM as a CST might make the LLM's users feel\nmore creative, and even broaden the range of ideas suggested by each individual\nuser, but also homogenize the ideas suggested by different users. We conducted\na 36-participant comparative user study and found, in accordance with the\nhomogenization hypothesis, that different users tended to produce less\nsemantically distinct ideas with ChatGPT than with an alternative CST.\nAdditionally, ChatGPT users generated a greater number of more detailed ideas,\nbut felt less responsible for the ideas they generated. We discuss potential\nimplications of these findings for users, designers, and developers of\nLLM-based CSTs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01536v1.pdf"
    },
    {
        "title": "An Empirical Analysis of Diversity in Argument Summarization",
        "authors": [
            "Michiel van der Meer",
            "Piek Vossen",
            "Catholijn M. Jonker",
            "Pradeep K. Murukannaiah"
        ],
        "published": "2024-02-02T16:26:52Z",
        "summary": "Presenting high-level arguments is a crucial task for fostering participation\nin online societal discussions. Current argument summarization approaches miss\nan important facet of this task -- capturing diversity -- which is important\nfor accommodating multiple perspectives. We introduce three aspects of\ndiversity: those of opinions, annotators, and sources. We evaluate approaches\nto a popular argument summarization task called Key Point Analysis, which shows\nhow these approaches struggle to (1) represent arguments shared by few people,\n(2) deal with data from various sources, and (3) align with subjectivity in\nhuman-provided annotations. We find that both general-purpose LLMs and\ndedicated KPA models exhibit this behavior, but have complementary strengths.\nFurther, we observe that diversification of training data may ameliorate\ngeneralization. Addressing diversity in argument summarization requires a mix\nof strategies to deal with subjectivity.",
        "pdf_link": "https://arxiv.org/pdf/2402.01535v2.pdf"
    },
    {
        "title": "Decoding Speculative Decoding",
        "authors": [
            "Minghao Yan",
            "Saurabh Agarwal",
            "Shivaram Venkataraman"
        ],
        "published": "2024-02-02T16:15:24Z",
        "summary": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without modifying its outcome. When performing\ninference on an LLM, speculative decoding uses a smaller draft model which\ngenerates speculative tokens and then uses the target LLM to verify those draft\ntokens. The speedup provided by speculative decoding heavily depends on the\nchoice of the draft model. It has been widely suggested to select a draft model\nthat provides a high probability of the generated token being accepted by the\nLLM to achieve the highest throughput. However, our experiments indicate the\ncontrary with throughput diminishing as the probability of generated tokens to\nbe accepted by the target model increases. To understand this phenomenon, we\nperform extensive experiments to characterize the different factors that affect\nspeculative decoding and how those factors interact and affect the speedups.\nBased on our experiments we describe an analytical model which can be used to\ndecide the right draft model for a given workload. Further, using our insights\nwe design a new draft model for LLaMA-65B which can provide 30% higher\nthroughput than existing draft models.",
        "pdf_link": "https://arxiv.org/pdf/2402.01528v1.pdf"
    },
    {
        "title": "K-Level Reasoning with Large Language Models",
        "authors": [
            "Yadong Zhang",
            "Shaoguang Mao",
            "Tao Ge",
            "Xun Wang",
            "Yan Xia",
            "Man Lan",
            "Furu Wei"
        ],
        "published": "2024-02-02T16:07:05Z",
        "summary": "While Large Language Models (LLMs) have demonstrated their proficiency in\ncomplex reasoning tasks, their performance in dynamic, interactive, and\ncompetitive scenarios - such as business strategy and stock market analysis -\nremains underexplored. To bridge this gap, we formally explore the dynamic\nreasoning capabilities of LLMs for decision-making in rapidly evolving\nenvironments. We introduce two game theory-based pilot challenges that mirror\nthe complexities of real-world dynamic decision-making. These challenges are\nwell-defined, enabling clear, controllable, and precise evaluation of LLMs'\ndynamic reasoning abilities. Through extensive experiments, we find that\nexisting reasoning methods tend to falter in dynamic settings that require\nk-level thinking - a key concept not tackled by previous works. To address\nthis, we propose a novel reasoning approach for LLMs, named \"K-Level\nReasoning\". This approach adopts the perspective of rivals to recursively\nemploy k-level thinking based on available historical information, which\nsignificantly improves the prediction accuracy of rivals' subsequent moves and\ninforms more strategic decision-making. This research not only sets a robust\nquantitative benchmark for the assessment of dynamic reasoning but also\nmarkedly enhances the proficiency of LLMs in dynamic contexts.",
        "pdf_link": "https://arxiv.org/pdf/2402.01521v1.pdf"
    },
    {
        "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
        "authors": [
            "Phillip Schneider",
            "Manuel Klettner",
            "Elena Simperl",
            "Florian Matthes"
        ],
        "published": "2024-02-02T15:26:39Z",
        "summary": "Generating natural language text from graph-structured data is essential for\nconversational information seeking. Semantic triples derived from knowledge\ngraphs can serve as a valuable source for grounding responses from\nconversational agents by providing a factual basis for the information they\ncommunicate. This is especially relevant in the context of large language\nmodels, which offer great potential for conversational interaction but are\nprone to hallucinating, omitting, or producing conflicting information. In this\nstudy, we conduct an empirical analysis of conversational large language models\nin generating natural language text from semantic triples. We compare four\nlarge language models of varying sizes with different prompting techniques.\nThrough a series of benchmark experiments on the WebNLG dataset, we analyze the\nmodels' performance and identify the most common issues in the generated\npredictions. Our findings show that the capabilities of large language models\nin triple verbalization can be significantly improved through few-shot\nprompting, post-processing, and efficient fine-tuning techniques, particularly\nfor smaller models that exhibit lower zero-shot performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.01495v1.pdf"
    },
    {
        "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
        "authors": [
            "Jian Guan",
            "Wei Wu",
            "Zujie Wen",
            "Peng Xu",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "published": "2024-02-02T14:56:48Z",
        "summary": "The notable success of large language models (LLMs) has sparked an upsurge in\nbuilding language agents to complete various complex tasks. We present AMOR, an\nagent framework based on open-source LLMs, which reasons with external\nknowledge bases and adapts to specific domains through human supervision to the\nreasoning process. AMOR builds reasoning logic over a finite state machine\n(FSM) that solves problems through autonomous executions and transitions over\ndisentangled modules. This allows humans to provide direct feedback to the\nindividual modules, and thus naturally forms process supervision. Based on this\nreasoning and feedback framework, we develop AMOR through two-stage\nfine-tuning: warm-up and adaptation. The former fine-tunes the LLM with\nexamples automatically constructed from various public datasets and enables\nAMOR to generalize across different knowledge environments, while the latter\ntailors AMOR to specific domains using process feedback. Extensive experiments\nacross multiple domains demonstrate the advantage of AMOR to strong baselines,\nthanks to its FSM-based reasoning and process feedback mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.01469v1.pdf"
    },
    {
        "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach",
        "authors": [
            "Masayuki Takayama",
            "Tadahisa Okuda",
            "Thong Pham",
            "Tatsuyoshi Ikenoue",
            "Shingo Fukuma",
            "Shohei Shimizu",
            "Akiyoshi Sannai"
        ],
        "published": "2024-02-02T14:43:19Z",
        "summary": "In practical statistical causal discovery (SCD), embedding domain expert\nknowledge as constraints into the algorithm is widely accepted as significant\nfor creating consistent meaningful causal models, despite the recognized\nchallenges in systematic acquisition of the background knowledge. To overcome\nthese challenges, this paper proposes a novel methodology for causal inference,\nin which SCD methods and knowledge based causal inference (KBCI) with a large\nlanguage model (LLM) are synthesized through \"statistical causal prompting\n(SCP)\" for LLMs and prior knowledge augmentation for SCD. Experiments have\nrevealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result\nwith prior knowledge from LLM-KBCI to approach the ground truth, and that the\nSCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has\nbeen clarified that an LLM can improve SCD with its background knowledge, even\nif the LLM does not contain information on the dataset. The proposed approach\ncan thus address challenges such as dataset biases and limitations,\nillustrating the potential of LLMs to improve data-driven causal inference\nacross diverse scientific domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.01454v1.pdf"
    },
    {
        "title": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
        "authors": [
            "Subbarao Kambhampati",
            "Karthik Valmeekam",
            "Lin Guan",
            "Kaya Stechly",
            "Mudit Verma",
            "Siddhant Bhambri",
            "Lucas Saldyt",
            "Anil Murthy"
        ],
        "published": "2024-02-02T14:43:18Z",
        "summary": "There is considerable confusion about the role of Large Language Models\n(LLMs) in planning and reasoning tasks. On one side are over-optimistic claims\nthat LLMs can indeed do these tasks with just the right prompting or\nself-verification strategies. On the other side are perhaps over-pessimistic\nclaims that all that LLMs are good for in planning/reasoning tasks are as mere\ntranslators of the problem specification from one syntactic format to another,\nand ship the problem off to external symbolic solvers. In this position paper,\nwe take the view that both these extremes are misguided. We argue that\nauto-regressive LLMs cannot, by themselves, do planning or self-verification\n(which is after all a form of reasoning), and shed some light on the reasons\nfor misunderstandings in the literature. We will also argue that LLMs should be\nviewed as universal approximate knowledge sources that have much more\nmeaningful roles to play in planning/reasoning tasks beyond simple\nfront-end/back-end format translators. We present a vision of {\\bf LLM-Modulo\nFrameworks} that combine the strengths of LLMs with external model-based\nverifiers in a tighter bi-directional interaction regime. We will show how the\nmodels driving the external verifiers themselves can be acquired with the help\nof LLMs. We will also argue that rather than simply pipelining LLMs and\nsymbolic components, this LLM-Modulo Framework provides a better neuro-symbolic\napproach that offers tighter integration between LLMs and symbolic components,\nand allows extending the scope of model-based planning/reasoning regimes\ntowards more flexible knowledge, problem and preference specifications.",
        "pdf_link": "https://arxiv.org/pdf/2402.01817v2.pdf"
    },
    {
        "title": "Distilling LLMs' Decomposition Abilities into Compact Language Models",
        "authors": [
            "Denis Tarasov",
            "Kumar Shridhar"
        ],
        "published": "2024-02-02T13:23:15Z",
        "summary": "Large Language Models (LLMs) have demonstrated proficiency in their reasoning\nabilities, yet their large size presents scalability challenges and limits any\nfurther customization. In contrast, compact models offer customized training\nbut often fall short in solving complex reasoning tasks. This study focuses on\ndistilling the LLMs' decomposition skills into compact models using offline\nreinforcement learning. We leverage the advancements in the LLM`s capabilities\nto provide feedback and generate a specialized task-specific dataset for\ntraining compact models. The development of an AI-generated dataset and the\nestablishment of baselines constitute the primary contributions of our work,\nunderscoring the potential of compact models in replicating complex\nproblem-solving skills.",
        "pdf_link": "https://arxiv.org/pdf/2402.01812v1.pdf"
    },
    {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Wei Shen",
            "Junjie Shan",
            "Caishuang Huang",
            "Xiao Wang",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "published": "2024-02-02T13:14:31Z",
        "summary": "The advancement of large language models (LLMs) has significantly propelled\nthe field of code generation. Previous work integrated reinforcement learning\n(RL) with compiler feedback for exploring the output space of LLMs to enhance\ncode generation quality. However, the lengthy code generated by LLMs in\nresponse to complex human requirements makes RL exploration a challenge. Also,\nsince the unit tests may not cover the complicated code, optimizing LLMs by\nusing these unexecuted code snippets is ineffective. To tackle these\nchallenges, we introduce StepCoder, a novel RL framework for code generation,\nconsisting of two main components: CCCS addresses the exploration challenge by\nbreaking the long sequences code generation task into a Curriculum of Code\nCompletion Subtasks, while FGO only optimizes the model by masking the\nunexecuted code segments to provide Fine-Grained Optimization. In addition, we\nfurthermore construct the APPS+ dataset for RL training, which is manually\nverified to ensure the correctness of unit tests. Experimental results show\nthat our method improves the ability to explore the output space and\noutperforms state-of-the-art approaches in corresponding benchmarks. Our\ndataset APPS+ and StepCoder are available online.",
        "pdf_link": "https://arxiv.org/pdf/2402.01391v2.pdf"
    },
    {
        "title": "LLM-based NLG Evaluation: Current Status and Challenges",
        "authors": [
            "Mingqi Gao",
            "Xinyu Hu",
            "Jie Ruan",
            "Xiao Pu",
            "Xiaojun Wan"
        ],
        "published": "2024-02-02T13:06:35Z",
        "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.01383v2.pdf"
    },
    {
        "title": "Continual Learning for Large Language Models: A Survey",
        "authors": [
            "Tongtong Wu",
            "Linhao Luo",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Thuy-Trang Vu",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-02T12:34:09Z",
        "summary": "Large language models (LLMs) are not amenable to frequent re-training, due to\nhigh training costs arising from their massive scale. However, updates are\nnecessary to endow LLMs with new skills and keep them up-to-date with rapidly\nevolving human knowledge. This paper surveys recent works on continual learning\nfor LLMs. Due to the unique nature of LLMs, we catalog continue learning\ntechniques in a novel multi-staged categorization scheme, involving continual\npretraining, instruction tuning, and alignment. We contrast continual learning\nfor LLMs with simpler adaptation methods used in smaller models, as well as\nwith other enhancement strategies like retrieval-augmented generation and model\nediting. Moreover, informed by a discussion of benchmarks and evaluation, we\nidentify several challenges and future work directions for this crucial task.",
        "pdf_link": "https://arxiv.org/pdf/2402.01364v2.pdf"
    },
    {
        "title": "A Survey on Large Language Model Hallucination via a Creativity Perspective",
        "authors": [
            "Xuhui Jiang",
            "Yuxing Tian",
            "Fengrui Hua",
            "Chengjin Xu",
            "Yuanzhuo Wang",
            "Jian Guo"
        ],
        "published": "2024-02-02T12:21:04Z",
        "summary": "Hallucinations in large language models (LLMs) are always seen as\nlimitations. However, could they also be a source of creativity? This survey\nexplores this possibility, suggesting that hallucinations may contribute to LLM\napplication by fostering creativity. This survey begins with a review of the\ntaxonomy of hallucinations and their negative impact on LLM reliability in\ncritical applications. Then, through historical examples and recent relevant\ntheories, the survey explores the potential creative benefits of hallucinations\nin LLMs. To elucidate the value and evaluation criteria of this connection, we\ndelve into the definitions and assessment methods of creativity. Following the\nframework of divergent and convergent thinking phases, the survey\nsystematically reviews the literature on transforming and harnessing\nhallucinations for creativity in LLMs. Finally, the survey discusses future\nresearch directions, emphasizing the need to further explore and refine the\napplication of hallucinations in creative processes within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.06647v1.pdf"
    },
    {
        "title": "Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models",
        "authors": [
            "Haochun Wang",
            "Sendong Zhao",
            "Zewen Qiang",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2024-02-02T12:07:00Z",
        "summary": "In the field of natural language processing (NLP), Large Language Models\n(LLMs) have precipitated a paradigm shift, markedly enhancing performance in\nnatural language generation tasks. Despite these advancements, the\ncomprehensive evaluation of LLMs remains an inevitable challenge for the\ncommunity. Recently, the utilization of Multiple Choice Question Answering\n(MCQA) as a benchmark for LLMs has gained considerable traction. This study\ninvestigates the rationality of MCQA as an evaluation method for LLMs. If LLMs\ngenuinely understand the semantics of questions, their performance should\nexhibit consistency across the varied configurations derived from the same\nquestions. Contrary to this expectation, our empirical findings suggest a\nnotable disparity in the consistency of LLM responses, which we define as\nREsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current\nMCQA-based benchmarks may not adequately capture the true capabilities of LLMs,\nwhich underscores the need for more robust evaluation mechanisms in assessing\nthe performance of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01349v1.pdf"
    },
    {
        "title": "Preference-free Alignment Learning with Regularized Relevance Reward",
        "authors": [
            "Sungdong Kim",
            "Minjoon Seo"
        ],
        "published": "2024-02-02T11:58:08Z",
        "summary": "Learning from human preference has been considered key to aligning Large\nLanguage Models (LLMs) with human values. However, contrary to popular belief,\nour preliminary study reveals that reward models trained on human preference\ndatasets tend to give higher scores to long off-topic responses than short\non-topic ones. Motivated by this observation, we explore a preference-free\napproach utilizing `relevance' as a key objective for alignment. On our first\nattempt, we find that the relevance score obtained by a retriever alone is\nvulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when\nwe utilize the score as a reward for reinforcement learning. To mitigate it, we\nintegrate effective inductive biases into the vanilla relevance to regularize\neach other, resulting in a mixture of reward functions: Regularized Relevance\nReward ($R^3$). $R^3$ significantly improves performance on preference\nbenchmarks by providing a robust reward signal. Notably, $R^3$ does not require\nany human preference datasets (i.e., preference-free), outperforming\nopen-source reward models in improving human preference. Our analysis\ndemonstrates that $R^3$ has advantages in elevating human preference while\nminimizing its side effects. Finally, we show the generalizability of $R^3$,\nconsistently improving instruction-tuned models in various backbones and sizes\nwithout additional dataset cost. Our code is available at\nhttps://github.com/naver-ai/RRR.",
        "pdf_link": "https://arxiv.org/pdf/2402.03469v1.pdf"
    },
    {
        "title": "Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion",
        "authors": [
            "Zexi Li",
            "Zhiqi Li",
            "Jie Lin",
            "Tao Shen",
            "Tao Lin",
            "Chao Wu"
        ],
        "published": "2024-02-02T11:57:50Z",
        "summary": "In deep learning, stochastic gradient descent often yields functionally\nsimilar yet widely scattered solutions in the weight space even under the same\ninitialization, causing barriers in the Linear Mode Connectivity (LMC)\nlandscape. Overcoming these barriers is crucial for understanding deep learning\ndynamics and enhancing model-fusion algorithms. Previous studies highlight the\nrole of permutation symmetry in reducing post-training barriers through network\npermutation. However, these post-hoc methods, demanding extra computations, are\nless effective for larger, complex models (e.g., ViT, LLM) due to numerous\npermutation matrices. Thus, in this paper, we study training-time neuron\nalignment. Our hypothesis suggests that training-time permutation subspace can\nreduce LMC barriers for free. We find that pruning at initialization supports\nthis. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm\nusing a partial gradient mask during training. TNA-PFN is theoretically and\nempirically validated for reducing LMC barriers. It excels in wide model fusion\napplications, especially in federated learning, two algorithms based on TNA-FPN\nthat are proposed to show its prospects even under heterogeneous datasets.\nMoreover, TNA-PFN can enhance the generalization of model soup for vision\ntransformers and ColD fusion for pretrained language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.01342v1.pdf"
    },
    {
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
            "Kawin Ethayarajh",
            "Winnie Xu",
            "Niklas Muennighoff",
            "Dan Jurafsky",
            "Douwe Kiela"
        ],
        "published": "2024-02-02T10:53:36Z",
        "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner; for example, humans are\nfamously loss-averse. We show that objectives for aligning LLMs with human\nfeedback implicitly incorporate many of these biases -- the success of these\nobjectives (e.g., DPO) over cross-entropy minimization can partly be ascribed\nto them being $\\textit{human-aware loss functions}$ (HALOs). However, the\nutility functions these methods attribute to humans still differ from those in\nthe prospect theory literature. Using a Kahneman-Tversky model of human\nutility, we propose a HALO that directly maximizes the utility of generations\ninstead of maximizing the log-likelihood of preferences, as current methods do.\nWe call this approach Kahneman-Tversky Optimization (KTO), and it matches or\nexceeds the performance of preference-based methods at scales from 1B to 30B.\nCrucially, KTO does not need preferences -- only a binary signal of whether an\noutput is desirable or undesirable for a given input. This makes it far easier\nto use in the real world, where preference data is scarce and expensive.",
        "pdf_link": "https://arxiv.org/pdf/2402.01306v1.pdf"
    },
    {
        "title": "Can MLLMs Perform Text-to-Image In-Context Learning?",
        "authors": [
            "Yuchen Zeng",
            "Wonjun Kang",
            "Yicong Chen",
            "Hyung Il Koo",
            "Kangwook Lee"
        ],
        "published": "2024-02-02T10:30:05Z",
        "summary": "The evolution from Large Language Models (LLMs) to Multimodal Large Language\nModels (MLLMs) has spurred research into extending In-Context Learning (ICL) to\nits multimodal counterpart. Existing such studies have primarily concentrated\non image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique\ncharacteristics and potential applications, remains underexplored. To address\nthis gap, we formally define the task of T2I-ICL and present CoBSAT, the first\nT2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to\nbenchmark six state-of-the-art MLLMs, we uncover considerable difficulties\nMLLMs encounter in solving T2I-ICL. We identify the primary challenges as the\ninherent complexity of multimodality and image generation. To overcome these\nchallenges, we explore strategies like fine-tuning and Chain-of-Thought\nprompting, demonstrating notable improvements. Our code and dataset are\navailable at \\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.",
        "pdf_link": "https://arxiv.org/pdf/2402.01293v1.pdf"
    },
    {
        "title": "Exploring the Limitations of Graph Reasoning in Large Language Models",
        "authors": [
            "Palaash Agrawal",
            "Shavak Vasania",
            "Cheston Tan"
        ],
        "published": "2024-02-02T09:45:33Z",
        "summary": "Pretrained Large Language Models have demonstrated various types of reasoning\ncapabilities through language-based prompts alone. However, in this paper, we\ntest the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5,\nClaude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In\nparticular, we design 10 distinct problems of graph traversal, each\nrepresenting increasing levels of complexity. Further, we analyze the\nperformance of models across various settings such as varying sizes of graphs\nas well as different forms of k-shot prompting. We highlight various\nlimitations, biases, and properties of LLMs through this benchmarking process,\nsuch as an inverse relation to the average degrees of freedom of traversal per\nnode in graphs, the overall negative impact of k-shot prompting on graph\nreasoning tasks, and a positive response bias which prevents LLMs from\nidentifying the absence of a valid solution. Finally, we propose a new\nprompting technique specially designed for graph traversal tasks, known as\nPathCompare, which shows a notable increase in the performance of LLMs in\ncomparison to standard prompting and CoT.",
        "pdf_link": "https://arxiv.org/pdf/2402.01805v1.pdf"
    },
    {
        "title": "The Human and the Mechanical: logos, truthfulness, and ChatGPT",
        "authors": [
            "Anastasia Giannakidou",
            "Alda Mari"
        ],
        "published": "2024-02-02T09:41:51Z",
        "summary": "The paper addresses the question of whether it is appropriate to talk about\n`mechanical minds' at all, and whether ChatGPT models can indeed be thought of\nas realizations of that. Our paper adds a semantic argument to the current\ndebate. The act of human assertion requires the formation of a veridicality\njudgment. Modification of assertions with modals (John must be at home) and the\nuse of subjective elements (John is obviously at home) indicate that the\nspeaker is manipulating her judgments and, in a cooperative context, intends\nher epistemic state to be transparent to the addressee. Veridicality judgments\nare formed on the basis of two components: (i) evidence that relates to reality\n(exogenous evidence) and (ii) endogenous evidence, such as preferences and\nprivate beliefs. `Mechanical minds' lack these two components: (i) they do not\nrelate to reality and (ii) do not have endogenous evidence. Therefore they lack\nthe ability to form a belief about the world and a veridicality judgments\naltogether. They can only mimic that judgment, but the output is not ground in\nthe very foundations for it.",
        "pdf_link": "https://arxiv.org/pdf/2402.01267v1.pdf"
    },
    {
        "title": "Efficient Causal Graph Discovery Using Large Language Models",
        "authors": [
            "Thomas Jiralerspong",
            "Xiaoyin Chen",
            "Yash More",
            "Vedant Shah",
            "Yoshua Bengio"
        ],
        "published": "2024-02-02T08:25:32Z",
        "summary": "We propose a novel framework that leverages LLMs for full causal graph\ndiscovery. While previous LLM-based methods have used a pairwise query\napproach, this requires a quadratic number of queries which quickly becomes\nimpractical for larger causal graphs. In contrast, the proposed framework uses\na breadth-first search (BFS) approach which allows it to use only a linear\nnumber of queries. We also show that the proposed method can easily incorporate\nobservational data when available, to improve performance. In addition to being\nmore time and data-efficient, the proposed framework achieves state-of-the-art\nresults on real-world causal graphs of varying sizes. The results demonstrate\nthe effectiveness and efficiency of the proposed method in discovering causal\nrelationships, showcasing its potential for broad applicability in causal graph\ndiscovery tasks across different domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.01207v3.pdf"
    },
    {
        "title": "Large Language Models for Time Series: A Survey",
        "authors": [
            "Xiyuan Zhang",
            "Ranak Roy Chowdhury",
            "Rajesh K. Gupta",
            "Jingbo Shang"
        ],
        "published": "2024-02-02T07:24:35Z",
        "summary": "Large Language Models (LLMs) have seen significant use in domains such as\nnatural language processing and computer vision. Going beyond text, image and\ngraphics, LLMs present a significant potential for analysis of time series\ndata, benefiting domains such as climate, IoT, healthcare, traffic, audio and\nfinance. This survey paper provides an in-depth exploration and a detailed\ntaxonomy of the various methodologies employed to harness the power of LLMs for\ntime series analysis. We address the inherent challenge of bridging the gap\nbetween LLMs' original text data training and the numerical nature of time\nseries data, and explore strategies for transferring and distilling knowledge\nfrom LLMs to numerical time series analysis. We detail various methodologies,\nincluding (1) direct prompting of LLMs, (2) time series quantization, (3)\nalignment techniques, (4) utilization of the vision modality as a bridging\nmechanism, and (5) the combination of LLMs with tools. Additionally, this\nsurvey offers a comprehensive overview of the existing multimodal time series\nand text datasets and delves into the challenges and future opportunities of\nthis emerging field. We maintain an up-to-date Github repository which includes\nall the papers and datasets discussed in the survey.",
        "pdf_link": "https://arxiv.org/pdf/2402.01801v2.pdf"
    },
    {
        "title": "Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus",
        "authors": [
            "Xiaoxi Li",
            "Zhicheng Dou",
            "Yujia Zhou",
            "Fangchao Liu"
        ],
        "published": "2024-02-02T06:44:22Z",
        "summary": "The advent of large language models (LLMs) has showcased their efficacy\nacross various domains, yet they often hallucinate, especially in\nknowledge-intensive tasks that require external knowledge sources. To improve\nfactual accuracy of language models, retrieval-augmented generation (RAG) has\nemerged as a popular solution. However, traditional retrieval modules often\nrely on large-scale document indexes, which can be disconnected from generative\ntasks. Through generative retrieval (GR) approach, language models can achieve\nsuperior retrieval performance by directly generating relevant document\nidentifiers (DocIDs). However, the relationship between GR and downstream\ntasks, as well as the potential of LLMs in GR, remains unexplored. In this\npaper, we present a unified language model that utilizes external corpus to\nhandle various knowledge-intensive tasks by seamlessly integrating generative\nretrieval, closed-book generation, and RAG. In order to achieve effective\nretrieval and generation through a unified continuous decoding process, we\nintroduce the following mechanisms: (1) a ranking-oriented DocID decoding\nstrategy, which improves ranking ability by directly learning from a DocID\nranking list; (2) a continuous generation strategy to facilitate effective and\nefficient RAG; (3) well-designed auxiliary DocID understanding tasks to enhance\nthe model's comprehension of DocIDs and their relevance to downstream tasks.\nOur approach is evaluated on the widely used KILT benchmark using two variants\nof backbone models: an encoder-decoder T5 model and a decoder-only LLM, Llama2.\nExperimental results showcase the superior performance of our models in both\nretrieval and downstream knowledge-intensive tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01176v1.pdf"
    },
    {
        "title": "Efficient Prompt Caching via Embedding Similarity",
        "authors": [
            "Hanlin Zhu",
            "Banghua Zhu",
            "Jiantao Jiao"
        ],
        "published": "2024-02-02T06:34:11Z",
        "summary": "Large language models (LLMs) have achieved huge success in numerous natural\nlanguage process (NLP) tasks. However, it faces the challenge of significant\nresource consumption during inference. In this paper, we aim to improve the\ninference efficiency of LLMs by prompt caching, i.e., if the current prompt can\nbe answered by the same response of a previous prompt, one can directly utilize\nthat previous response without calling the LLM. Specifically, we focus on the\nprediction accuracy of prompt caching for single-round question-answering tasks\nvia embedding similarity. The existing embeddings of prompts mostly focus on\nwhether two prompts are semantically similar, which is not necessarily\nequivalent to whether the same response can answer them. Therefore, we propose\na distillation-based method to fine-tune the existing embeddings for better\ncaching prediction. Theoretically, we provide finite-sample guarantees for the\nconvergence of our method under different types of loss functions. Empirically,\nwe carefully construct a hard dataset based on Kwiatkowski et al. (2019) where\nthe existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.\nWe then fine-tune the above embedding model, which significantly improves the\nAUC of caching prediction from 0.51 to 0.81. We also conduct simulations\ndemonstrating that our trained models achieve better caching efficiency than\nthe previous embedding model.",
        "pdf_link": "https://arxiv.org/pdf/2402.01173v1.pdf"
    },
    {
        "title": "Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",
        "authors": [
            "Arnav Chavan",
            "Raghav Magazine",
            "Shubham Kushwaha",
            "M\u00e9rouane Debbah",
            "Deepak Gupta"
        ],
        "published": "2024-02-02T06:29:34Z",
        "summary": "Despite the impressive performance of LLMs, their widespread adoption faces\nchallenges due to substantial computational and memory requirements during\ninference. Recent advancements in model compression and system-level\noptimization methods aim to enhance LLM inference. This survey offers an\noverview of these methods, emphasizing recent developments. Through experiments\non LLaMA(/2)-7B, we evaluate various compression techniques, providing\npractical insights for efficient LLM deployment in a unified setting. The\nempirical analysis on LLaMA(/2)-7B highlights the effectiveness of these\nmethods. Drawing from survey insights, we identify current limitations and\ndiscuss potential future directions to improve LLM inference efficiency. We\nrelease the codebase to reproduce the results presented in this paper at\nhttps://github.com/nyunAI/Faster-LLM-Survey",
        "pdf_link": "https://arxiv.org/pdf/2402.01799v1.pdf"
    },
    {
        "title": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
        "authors": [
            "Rongsheng Wang",
            "Haoming Chen",
            "Ruizhe Zhou",
            "Han Ma",
            "Yaofei Duan",
            "Yanlan Kang",
            "Songhua Yang",
            "Baoyu Fan",
            "Tao Tan"
        ],
        "published": "2024-02-02T05:54:12Z",
        "summary": "ChatGPT and other general large language models (LLMs) have achieved\nremarkable success, but they have also raised concerns about the misuse of\nAI-generated texts. Existing AI-generated text detection models, such as based\non BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor\nout-of-domain (OOD) detection performance. In this paper, we first collected\nChinese text responses generated by human experts and 9 types of LLMs, for\nwhich to multiple domains questions, and further created a dataset that mixed\nhuman-written sentences and sentences polished by LLMs. We then proposed\nLLM-Detector, a novel method for both document-level and sentence-level text\ndetection through Instruction Tuning of LLMs. Our method leverages the wealth\nof knowledge LLMs acquire during pre-training, enabling them to detect the text\nthey generate. Instruction tuning aligns the model's responses with the user's\nexpected text detection tasks. Experimental results show that previous methods\nstruggle with sentence-level AI-generated text detection and OOD detection. In\ncontrast, our proposed method not only significantly outperforms baseline\nmethods in both sentence-level and document-level text detection but also\ndemonstrates strong generalization capabilities. Furthermore, since\nLLM-Detector is trained based on open-source LLMs, it is easy to customize for\ndeployment.",
        "pdf_link": "https://arxiv.org/pdf/2402.01158v1.pdf"
    },
    {
        "title": "CABINET: Content Relevance based Noise Reduction for Table Question Answering",
        "authors": [
            "Sohan Patnaik",
            "Heril Changwal",
            "Milan Aggarwal",
            "Sumit Bhatia",
            "Yaman Kumar",
            "Balaji Krishnamurthy"
        ],
        "published": "2024-02-02T05:48:39Z",
        "summary": "Table understanding capability of Large Language Models (LLMs) has been\nextensively studied through the task of question-answering (QA) over tables.\nTypically, only a small part of the whole table is relevant to derive the\nanswer for a given question. The irrelevant parts act as noise and are\ndistracting information, resulting in sub-optimal performance due to the\nvulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content\nRelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to\nenable LLMs to focus on relevant tabular data by suppressing extraneous\ninformation. CABINET comprises an Unsupervised Relevance Scorer (URS), trained\ndifferentially with the QA LLM, that weighs the table content based on its\nrelevance to the input question before feeding it to the question-answering LLM\n(QA LLM). To further aid the relevance scorer, CABINET employs a weakly\nsupervised module that generates a parsing statement describing the criteria of\nrows and columns relevant to the question and highlights the content of\ncorresponding table cells. CABINET significantly outperforms various tabular\nLLM baselines, as well as GPT3-based in-context learning methods, is more\nrobust to noise, maintains outperformance on tables of varying sizes, and\nestablishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We\nrelease our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.",
        "pdf_link": "https://arxiv.org/pdf/2402.01155v3.pdf"
    },
    {
        "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution",
        "authors": [
            "Haoran Ye",
            "Jiarui Wang",
            "Zhiguang Cao",
            "Guojie Song"
        ],
        "published": "2024-02-02T05:04:51Z",
        "summary": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design process.\nThe long-standing endeavor of design automation has gained new momentum with\nthe rise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a generic searching framework that emulates the reflective design\napproach of human experts while far surpassing human capabilities with its\nscalable LLM inference, Internet-scale domain knowledge, and powerful\nevolutionary search. Evaluations across 12 COP settings show that 1) verbal\nreflections for evolution lead to smoother fitness landscapes, explicit\ninference of black-box COP settings, and better search results; 2) heuristics\ngenerated by ReEvo in minutes can outperform state-of-the-art human designs and\nneural solvers; 3) LHHs enable efficient algorithm design automation even when\nchallenged with black-box COPs, demonstrating its potential for complex and\nnovel real-world applications. Our code is available:\nhttps://github.com/ai4co/LLM-as-HH.",
        "pdf_link": "https://arxiv.org/pdf/2402.01145v1.pdf"
    },
    {
        "title": "A Multi-Agent Conversational Recommender System",
        "authors": [
            "Jiabao Fang",
            "Shen Gao",
            "Pengjie Ren",
            "Xiuying Chen",
            "Suzan Verberne",
            "Zhaochun Ren"
        ],
        "published": "2024-02-02T04:20:13Z",
        "summary": "Due to strong capabilities in conducting fluent, multi-turn conversations\nwith users, Large Language Models (LLMs) have the potential to further improve\nthe performance of Conversational Recommender System (CRS). Unlike the aimless\nchit-chat that LLM excels at, CRS has a clear target. So it is imperative to\ncontrol the dialogue flow in the LLM to successfully recommend appropriate\nitems to the users. Furthermore, user feedback in CRS can assist the system in\nbetter modeling user preferences, which has been ignored by existing studies.\nHowever, simply prompting LLM to conduct conversational recommendation cannot\naddress the above two key challenges.\n  In this paper, we propose Multi-Agent Conversational Recommender System\n(MACRS) which contains two essential modules. First, we design a multi-agent\nact planning framework, which can control the dialogue flow based on four\nLLM-based agents. This cooperative multi-agent framework will generate various\ncandidate responses based on different dialogue acts and then choose the most\nappropriate response as the system response, which can help MACRS plan suitable\ndialogue acts. Second, we propose a user feedback-aware reflection mechanism\nwhich leverages user feedback to reason errors made in previous turns to adjust\nthe dialogue act planning, and higher-level user information from implicit\nsemantics. We conduct extensive experiments based on user simulator to\ndemonstrate the effectiveness of MACRS in recommendation and user preferences\ncollection. Experimental results illustrate that MACRS demonstrates an\nimprovement in user interaction experience compared to directly using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01135v1.pdf"
    },
    {
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Ling Liu"
        ],
        "published": "2024-02-02T03:22:12Z",
        "summary": "We introduce PokeLLMon, the first LLM-embodied agent that achieves\nhuman-parity performance in tactical battle games, as demonstrated in Pokemon\nbattles. The design of PokeLLMon incorporates three key strategies: (i)\nIn-context reinforcement learning that instantly consumes text-based feedback\nderived from battles to iteratively refine the policy; (ii) Knowledge-augmented\ngeneration that retrieves external knowledge to counteract hallucination and\nenables the agent to act timely and properly; (iii) Consistent action\ngeneration to mitigate the panic switching phenomenon when the agent faces a\npowerful opponent and wants to elude the battle. We show that online battles\nagainst human demonstrates PokeLLMon's human-like battle strategies and\njust-in-time decision making, achieving 49% of win rate in the Ladder\ncompetitions and 56% of win rate in the invited battles. Our implementation and\nplayable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "pdf_link": "https://arxiv.org/pdf/2402.01118v3.pdf"
    },
    {
        "title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models",
        "authors": [
            "Mohammadreza Pourreza",
            "Davood Rafiei"
        ],
        "published": "2024-02-02T03:21:00Z",
        "summary": "Leading models for the text-to-SQL task heavily rely on proprietary Large\nLanguage Models (LLMs), posing concerns over data privacy. Closing the\nperformance gap between small open-source models and large proprietary models\nis crucial to mitigate this reliance. To this end, we introduce a novel\ntwo-stage fine-tuning approach that decomposes the task into two simpler tasks.\nThrough comprehensive evaluation on two large cross-domain datasets and two\nsmall LLMs, we show that this approach improves execution accuracy by 3 to 7\npercent, effectively aligning the performance of open-source models with their\nproprietary counterparts.",
        "pdf_link": "https://arxiv.org/pdf/2402.01117v1.pdf"
    },
    {
        "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
        "authors": [
            "Tiansheng Huang",
            "Sihao Hu",
            "Ling Liu"
        ],
        "published": "2024-02-02T02:56:50Z",
        "summary": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.",
        "pdf_link": "https://arxiv.org/pdf/2402.01109v3.pdf"
    },
    {
        "title": "Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",
        "authors": [
            "Pouya Pezeshkpour",
            "Eser Kandogan",
            "Nikita Bhutani",
            "Sajjadur Rahman",
            "Tom Mitchell",
            "Estevam Hruschka"
        ],
        "published": "2024-02-02T02:53:11Z",
        "summary": "Remarkable performance of large language models (LLMs) in a variety of tasks\nbrings forth many opportunities as well as challenges of utilizing them in\nproduction settings. Towards practical adoption of LLMs, multi-agent systems\nhold great promise to augment, integrate, and orchestrate LLMs in the larger\ncontext of enterprise platforms that use existing proprietary data and models\nto tackle complex real-world tasks. Despite the tremendous success of these\nsystems, current approaches rely on narrow, single-focus objectives for\noptimization and evaluation, often overlooking potential constraints in\nreal-world scenarios, including restricted budgets, resources and time.\nFurthermore, interpreting, analyzing, and debugging these systems requires\ndifferent components to be evaluated in relation to one another. This demand is\ncurrently not feasible with existing methodologies. In this postion paper, we\nintroduce the concept of reasoning capacity as a unifying criterion to enable\nintegration of constraints during optimization and establish connections among\ndifferent components within the system, which also enable a more holistic and\ncomprehensive approach to evaluation. We present a formal definition of\nreasoning capacity and illustrate its utility in identifying limitations within\neach component of the system. We then argue how these limitations can be\naddressed with a self-reflective process wherein human-feedback is used to\nalleviate shortcomings in reasoning and enhance overall consistency of the\nsystem.",
        "pdf_link": "https://arxiv.org/pdf/2402.01108v1.pdf"
    },
    {
        "title": "The Political Preferences of LLMs",
        "authors": [
            "David Rozado"
        ],
        "published": "2024-02-02T02:43:10Z",
        "summary": "We report here a comprehensive analysis about the political preferences\nembedded in Large Language Models (LLMs). Namely, we administer 11 political\norientation tests, designed to identify the political preferences of the test\ntaker, to 24 state-of-the-art conversational LLMs, both close and open source.\nThe results indicate that when probed with questions/statements with political\nconnotations most conversational LLMs tend to generate responses that are\ndiagnosed by most political test instruments as manifesting preferences for\nleft-of-center viewpoints. We note that this is not the case for base (i.e.\nfoundation) models upon which LLMs optimized for conversation with humans are\nbuilt. However, base models' suboptimal performance at coherently answering\nquestions suggests caution when interpreting their classification by political\norientation tests. Though not conclusive, our results provide preliminary\nevidence for the intriguing hypothesis that the embedding of political\npreferences into LLMs might be happening mostly post-pretraining. Namely,\nduring the supervised fine-tuning (SFT) and/or Reinforcement Learning (RL)\nstages of the conversational LLMs training pipeline. We provide further support\nfor this hypothesis by showing that LLMs are easily steerable into target\nlocations of the political spectrum via SFT requiring only modest compute and\ncustom data, illustrating the ability of SFT to imprint political preferences\nonto LLMs. As LLMs have started to displace more traditional information\nsources such as search engines or Wikipedia, the implications of political\nbiases embedded in LLMs has important societal ramifications.",
        "pdf_link": "https://arxiv.org/pdf/2402.01789v1.pdf"
    },
    {
        "title": "LitLLM: A Toolkit for Scientific Literature Review",
        "authors": [
            "Shubham Agarwal",
            "Issam H. Laradji",
            "Laurent Charlin",
            "Christopher Pal"
        ],
        "published": "2024-02-02T02:41:28Z",
        "summary": "Conducting literature reviews for scientific papers is essential for\nunderstanding research, its limitations, and building on existing work. It is a\ntedious task which makes an automatic literature review generator appealing.\nUnfortunately, many existing works that generate such reviews using Large\nLanguage Models (LLMs) have significant limitations. They tend to\nhallucinate-generate non-actual information-and ignore the latest research they\nhave not been trained on. To address these limitations, we propose a toolkit\nthat operates on Retrieval Augmented Generation (RAG) principles, specialized\nprompting and instructing techniques with the help of LLMs. Our system first\ninitiates a web search to retrieve relevant papers by summarizing user-provided\nabstracts into keywords using an off-the-shelf LLM. Authors can enhance the\nsearch by supplementing it with relevant papers or keywords, contributing to a\ntailored retrieval process. Second, the system re-ranks the retrieved papers\nbased on the user-provided abstract. Finally, the related work section is\ngenerated based on the re-ranked results and the abstract. There is a\nsubstantial reduction in time and effort for literature review compared to\ntraditional methods, establishing our toolkit as an efficient alternative. Our\nopen-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM\nand Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)\nwith the video demo at https://youtu.be/E2ggOZBAFw0.",
        "pdf_link": "https://arxiv.org/pdf/2402.01788v1.pdf"
    },
    {
        "title": "Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs",
        "authors": [
            "Najmeh Forouzandehmehr",
            "Yijie Cao",
            "Nikhil Thakurdesai",
            "Ramin Giahi",
            "Luyi Ma",
            "Nima Farrokhsiar",
            "Jianpeng Xu",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "published": "2024-02-02T02:11:31Z",
        "summary": "The outfit generation problem involves recommending a complete outfit to a\nuser based on their interests. Existing approaches focus on recommending items\nbased on anchor items or specific query styles but do not consider customer\ninterests in famous characters from movie, social media, etc. In this paper, we\ndefine a new Character-based Outfit Generation (COG) problem, designed to\naccurately interpret character information and generate complete outfit sets\naccording to customer specifications such as age and gender. To tackle this\nproblem, we propose a novel framework LVA-COG that leverages Large Language\nModels (LLMs) to extract insights from customer interests (e.g., character\ninformation) and employ prompt engineering techniques for accurate\nunderstanding of customer preferences. Additionally, we incorporate\ntext-to-image models to enhance the visual understanding and generation\n(factual or counterfactual) of cohesive outfits. Our framework integrates LLMs\nwith text-to-image models and improves the customer's approach to fashion by\ngenerating personalized recommendations. With experiments and case studies, we\ndemonstrate the effectiveness of our solution from multiple dimensions.",
        "pdf_link": "https://arxiv.org/pdf/2402.05941v1.pdf"
    },
    {
        "title": "Specialized Language Models with Cheap Inference from Limited Domain Data",
        "authors": [
            "David Grangier",
            "Angelos Katharopoulos",
            "Pierre Ablin",
            "Awni Hannun"
        ],
        "published": "2024-02-02T01:45:18Z",
        "summary": "Large language models have emerged as a versatile tool but are challenging to\napply to tasks lacking large inference budgets and large in-domain training\nsets. This work formalizes these constraints and distinguishes four important\nvariables: the pretraining budget (for training before the target domain is\nknown), the specialization budget (for training after the target domain is\nknown), the inference budget, and the in-domain training set size. Across these\nsettings, we compare different approaches from the machine learning literature.\nLimited by inference cost, we find better alternatives to the standard practice\nof training very large vanilla transformer models. In particular, we show that\nhyper-networks and mixture of experts have better perplexity for large\npretraining budgets, while small models trained on importance sampled datasets\nare attractive for large specialization budgets.",
        "pdf_link": "https://arxiv.org/pdf/2402.01093v1.pdf"
    },
    {
        "title": "Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities",
        "authors": [
            "Mahdi Erfanian",
            "H. V. Jagadish",
            "Abolfazl Asudeh"
        ],
        "published": "2024-02-02T00:16:45Z",
        "summary": "The potential harms of the under-representation of minorities in training\ndata, particularly in multi-modal settings, is a well-recognized concern. While\nthere has been extensive effort in detecting such under-representation,\nresolution has remained a challenge. With recent advancements in generative AI,\nlarge language models and foundation models have emerged as versatile tools\nacross various domains. In this paper, we propose Chameleon, a system that\nefficiently utilizes these tools to augment a data set with a minimal addition\nof synthetically generated tuples, in order to enhance the coverage of the\nunder-represented groups. Our system follows a rejection sampling approach to\nensure the generated tuples have a high quality and follow the underlying\ndistribution. In order to minimize the rejection chance of the generated\ntuples, we propose multiple strategies for providing a guide for the foundation\nmodel. Our experiment results, in addition to confirming the efficiency of our\nproposed algorithms, illustrate the effectiveness of our approach, as the\nunfairness of the model in a downstream task significantly dropped after data\nrepair using Chameleon.",
        "pdf_link": "https://arxiv.org/pdf/2402.01071v1.pdf"
    },
    {
        "title": "Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer",
        "authors": [
            "Adar Kahana",
            "Jaya Susan Mathew",
            "Said Bleik",
            "Jeremy Reynolds",
            "Oren Elisha"
        ],
        "published": "2024-02-01T23:46:05Z",
        "summary": "With the widespread adoption of Large Language Models (LLMs), in this paper\nwe investigate the multilingual capability of these models. Our preliminary\nresults show that, translating the native language context, question and answer\ninto a high resource language produced the best results.",
        "pdf_link": "https://arxiv.org/pdf/2402.01065v1.pdf"
    },
    {
        "title": "Plan-Grounded Large Language Models for Dual Goal Conversational Settings",
        "authors": [
            "Diogo Gl\u00f3ria-Silva",
            "Rafael Ferreira",
            "Diogo Tavares",
            "David Semedo",
            "Jo\u00e3o Magalh\u00e3es"
        ],
        "published": "2024-02-01T22:56:39Z",
        "summary": "Training Large Language Models (LLMs) to follow user instructions has been\nshown to supply the LLM with ample capacity to converse fluently while being\naligned with humans. Yet, it is not completely clear how an LLM can lead a\nplan-grounded conversation in mixed-initiative settings where instructions flow\nin both directions of the conversation, i.e. both the LLM and the user provide\ninstructions to one another. In this paper, we tackle a dual goal\nmixed-initiative conversational setting where the LLM not only grounds the\nconversation on an arbitrary plan but also seeks to satisfy both a procedural\nplan and user instructions. The LLM is then responsible for guiding the user\nthrough the plan and, at the same time, adapting to new circumstances,\nanswering questions, and activating safety guardrails when needed. We propose a\nnovel LLM that grounds the dialogue on a procedural plan, can take the dialogue\ninitiative, and enforces guardrails on the system's behavior, while also\nimproving the LLM's responses to unexpected user behavior. Experiments in\ncontrolled settings and with real users show that the best-performing model,\nwhich we call PlanLLM, achieves a 2.1x improvement over a strong baseline.\nMoreover, experiments also show good generalization to unseen domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.01053v1.pdf"
    },
    {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "published": "2024-02-01T22:54:31Z",
        "summary": "Large Foundational Language Models are capable of performing many tasks at a\nhigh level but are difficult to deploy in many applications because of their\nsize and proprietary ownership. Many will be motivated to distill specific\ncapabilities of foundational models into smaller models that can be owned and\ncontrolled. In the development of a therapeutic chatbot, we wish to distill a\ncapability known as reflective listening, in which a therapist produces\nreflections of client speech. These reflections either restate what a client\nhas said, or connect what was said to a relevant observation, idea or guess\nthat encourages and guides the client to continue contemplation. In this paper,\nwe present a method for distilling the generation of reflections from a\nFoundational Language Model (GPT-4) into smaller models. We first show that\nGPT-4, using zero-shot prompting, can generate reflections at near 100% success\nrate, superior to all previous methods. Using reflections generated by GPT-4,\nwe fine-tune different sizes of the GPT-2 family. The GPT-2-small model\nachieves 83% success on a hold-out test set and the GPT-2 XL achieves 90%\nsuccess. We also show that GPT-4 can help in the labor-intensive task of\nevaluating the quality of the distilled models, using it as a zero-shot\nclassifier. Using triple-human review as a guide, the classifier achieves a\nCohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "pdf_link": "https://arxiv.org/pdf/2402.01051v1.pdf"
    },
    {
        "title": "IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition",
        "authors": [
            "Zikang Leng",
            "Amitrajit Bhattacharjee",
            "Hrudhai Rajasekhar",
            "Lizhe Zhang",
            "Elizabeth Bruda",
            "Hyeokhyen Kwon",
            "Thomas Pl\u00f6tz"
        ],
        "published": "2024-02-01T22:37:33Z",
        "summary": "One of the primary challenges in the field of human activity recognition\n(HAR) is the lack of large labeled datasets. This hinders the development of\nrobust and generalizable models. Recently, cross modality transfer approaches\nhave been explored that can alleviate the problem of data scarcity. These\napproaches convert existing datasets from a source modality, such as video, to\na target modality (IMU). With the emergence of generative AI models such as\nlarge language models (LLMs) and text-driven motion synthesis models, language\nhas become a promising source data modality as well as shown in proof of\nconcepts such as IMUGPT. In this work, we conduct a large-scale evaluation of\nlanguage-based cross modality transfer to determine their effectiveness for\nHAR. Based on this study, we introduce two new extensions for IMUGPT that\nenhance its use for practical HAR application scenarios: a motion filter\ncapable of filtering out irrelevant motion sequences to ensure the relevance of\nthe generated virtual IMU data, and a set of metrics that measure the diversity\nof the generated data facilitating the determination of when to stop generating\nvirtual IMU data for both effective and efficient processing. We demonstrate\nthat our diversity metrics can reduce the effort needed for the generation of\nvirtual IMU data by at least 50%, which open up IMUGPT for practical use cases\nbeyond a mere proof of concept.",
        "pdf_link": "https://arxiv.org/pdf/2402.01049v1.pdf"
    },
    {
        "title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations",
        "authors": [
            "Vinicius G. Goecks",
            "Nicholas Waytowich"
        ],
        "published": "2024-02-01T21:51:09Z",
        "summary": "The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.",
        "pdf_link": "https://arxiv.org/pdf/2402.01786v2.pdf"
    },
    {
        "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
        "authors": [
            "Gautier Dagan",
            "Gabriel Synnaeve",
            "Baptiste Rozi\u00e8re"
        ],
        "published": "2024-02-01T21:49:34Z",
        "summary": "Tokenization is an understudied and often neglected component of modern LLMs.\nMost published works use a single tokenizer for all experiments, often borrowed\nfrom another model, without performing ablations or analysis to optimize\ntokenization. Moreover, the tokenizer is generally kept unchanged when\nfine-tuning a base model. In this paper, we show that the size,\npre-tokenization regular expression, and training data of a tokenizer can\nsignificantly impact the model's generation speed, effective context size,\nmemory usage, and downstream performance. We train specialized Byte-Pair\nEncoding code tokenizers, and conduct extensive ablations on the impact of\ntokenizer design on the performance of LLMs for code generation tasks such as\nHumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters\nselection and switching the tokenizer in a pre-trained LLM. We perform our\nexperiments on models trained from scratch and from pre-trained models,\nverifying their applicability to a wide range of use-cases. We find that when\nfine-tuning on more than 50 billion tokens, we can specialize the tokenizer of\na pre-trained LLM to obtain large gains in generation speed and effective\ncontext size.",
        "pdf_link": "https://arxiv.org/pdf/2402.01035v2.pdf"
    },
    {
        "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
        "authors": [
            "Samy Jelassi",
            "David Brandfonbrener",
            "Sham M. Kakade",
            "Eran Malach"
        ],
        "published": "2024-02-01T21:44:11Z",
        "summary": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.",
        "pdf_link": "https://arxiv.org/pdf/2402.01032v1.pdf"
    },
    {
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "authors": [
            "Xingyao Wang",
            "Yangyi Chen",
            "Lifan Yuan",
            "Yizhe Zhang",
            "Yunzhu Li",
            "Hao Peng",
            "Heng Ji"
        ],
        "published": "2024-02-01T21:38:58Z",
        "summary": "Large Language Model (LLM) agents, capable of performing a broad range of\nactions, such as invoking tools and controlling robots, show great potential in\ntackling real-world challenges. LLM agents are typically prompted to produce\nactions by generating JSON or text in a pre-defined format, which is usually\nlimited by constrained action space (e.g., the scope of pre-defined tools) and\nrestricted flexibility (e.g., inability to compose multiple tools). This work\nproposes to use executable Python code to consolidate LLM agents' actions into\na unified action space (CodeAct). Integrated with a Python interpreter, CodeAct\ncan execute code actions and dynamically revise prior actions or emit new\nactions upon new observations through multi-turn interactions. Our extensive\nanalysis of 17 LLMs on API-Bank and a newly curated benchmark shows that\nCodeAct outperforms widely used alternatives (up to 20% higher success rate).\nThe encouraging performance of CodeAct motivates us to build an open-source LLM\nagent that interacts with environments by executing interpretable code and\ncollaborates with users using natural language. To this end, we collect an\ninstruction-tuning dataset CodeActInstruct that consists of 7k multi-turn\ninteractions using CodeAct. We show that it can be used with existing data to\nimprove models in agent-oriented tasks without compromising their general\ncapability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with\nPython interpreter and uniquely tailored to perform sophisticated tasks (e.g.,\nmodel training) using existing libraries and autonomously self-debug.",
        "pdf_link": "https://arxiv.org/pdf/2402.01030v2.pdf"
    },
    {
        "title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent",
        "authors": [
            "Weijie Xu",
            "Zicheng Huang",
            "Wenxiang Hu",
            "Xi Fang",
            "Rajesh Kumar Cherukuri",
            "Naumaan Nayyar",
            "Lorenzo Malandri",
            "Srinivasan H. Sengamedu"
        ],
        "published": "2024-02-01T21:10:44Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have been reshaping\nNatural Language Processing (NLP) task in several domains. Their use in the\nfield of Human Resources (HR) has still room for expansions and could be\nbeneficial for several time consuming tasks. Examples such as time-off\nsubmissions, medical claims filing, and access requests are noteworthy, but\nthey are by no means the sole instances. However, the aforementioned\ndevelopments must grapple with the pivotal challenge of constructing a\nhigh-quality training dataset. On one hand, most conversation datasets are\nsolving problems for customers not employees. On the other hand, gathering\nconversations with HR could raise privacy concerns. To solve it, we introduce\nHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR\ndomains to evaluate LLM Agent. Our work has the following contributions: (1) It\nis the first labeled open-sourced conversation dataset in the HR domain for NLP\nresearch. (2) It provides a detailed recipe for the data generation procedure\nalong with data analysis and human evaluations. The data generation pipeline is\ntransferable and can be easily adapted for labeled conversation data generation\nin other domains. (3) The proposed data-collection pipeline is mostly based on\nLLMs with minimal human involvement for annotation, which is time and\ncost-efficient.",
        "pdf_link": "https://arxiv.org/pdf/2402.01018v1.pdf"
    },
    {
        "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
        "authors": [
            "Norah Alzahrani",
            "Hisham Abdullah Alyahya",
            "Yazeed Alnumay",
            "Sultan Alrashed",
            "Shaykhah Alsubaie",
            "Yusef Almushaykeh",
            "Faisal Mirza",
            "Nouf Alotaibi",
            "Nora Altwairesh",
            "Areeb Alowisheq",
            "M Saiful Bari",
            "Haidar Khan"
        ],
        "published": "2024-02-01T19:12:25Z",
        "summary": "Large Language Model (LLM) leaderboards based on benchmark rankings are\nregularly used to guide practitioners in model selection. Often, the published\nleaderboard rankings are taken at face value - we show this is a (potentially\ncostly) mistake. Under existing leaderboards, the relative performance of LLMs\nis highly sensitive to (often minute) details. We show that for popular\nmultiple choice question benchmarks (e.g. MMLU) minor perturbations to the\nbenchmark, such as changing the order of choices or the method of answer\nselection, result in changes in rankings up to 8 positions. We explain this\nphenomenon by conducting systematic experiments over three broad categories of\nbenchmark perturbations and identifying the sources of this behavior. Our\nanalysis results in several best-practice recommendations, including the\nadvantage of a hybrid scoring method for answer selection. Our study highlights\nthe dangers of relying on simple benchmark evaluations and charts the path for\nmore robust evaluation schemes on the existing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01781v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
        "authors": [
            "Yucheng Li",
            "Yunhao Guo",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2024-02-01T18:56:18Z",
        "summary": "Existing methods for evaluating large language models face challenges such as\ndata contamination, sensitivity to prompts, and the high cost of benchmark\ncreation. To address this, we propose a lossless data compression based\nevaluation approach that tests how models' predictive abilities generalize\nafter their training cutoff. Specifically, we collect comprehensive test data\nspanning 83 months from 2017 to 2023 and split the data into training and\ntesting periods according to models' training data cutoff. We measure: 1) the\ncompression performance on the testing period as a measure of generalization on\nunseen data; and 2) the performance gap between the training and testing period\nas a measure of robustness. Our experiments test 14 representative large\nlanguage models with various sizes on sources including Wikipedia, news\narticles, code, arXiv papers, and multi-modal data. We find that the\ncompression rate of many models reduces significantly after their cutoff date,\nbut models such as Mistral and Llama-2 demonstrate a good balance between\nperformance and robustness. Results also suggest that models struggle to\ngeneralize on news and code data, but work especially well on arXiv papers. We\nalso find the context size and tokenization implementation have a big impact of\non the overall compression performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.00861v2.pdf"
    },
    {
        "title": "Can Large Language Models Understand Context?",
        "authors": [
            "Yilun Zhu",
            "Joel Ruben Antony Moniz",
            "Shruti Bhargava",
            "Jiarui Lu",
            "Dhivya Piraviperumal",
            "Site Li",
            "Yuan Zhang",
            "Hong Yu",
            "Bo-Hsiang Tseng"
        ],
        "published": "2024-02-01T18:55:29Z",
        "summary": "Understanding context is key to understanding human language, an ability\nwhich Large Language Models (LLMs) have been increasingly seen to demonstrate\nto an impressive extent. However, though the evaluation of LLMs encompasses\nvarious domains within the realm of Natural Language Processing, limited\nattention has been paid to probing their linguistic capability of understanding\ncontextual features. This paper introduces a context understanding benchmark by\nadapting existing datasets to suit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and nine datasets, all featuring\nprompts designed to assess the models' ability to understand context. First, we\nevaluate the performance of LLMs under the in-context learning pretraining\nscenario. Experimental results indicate that pre-trained dense models struggle\nwith understanding more nuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as LLM compression holds growing\nsignificance in both research and real-world applications, we assess the\ncontext understanding of quantized models under in-context-learning settings.\nWe find that 3-bit post-training quantization leads to varying degrees of\nperformance reduction on our benchmark. We conduct an extensive analysis of\nthese scenarios to substantiate our experimental results.",
        "pdf_link": "https://arxiv.org/pdf/2402.00858v1.pdf"
    },
    {
        "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?",
        "authors": [
            "Xue-Yong Fu",
            "Md Tahmid Rahman Laskar",
            "Elena Khasanova",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2024-02-01T18:31:34Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities to\nsolve a wide range of tasks without being explicitly fine-tuned on\ntask-specific datasets. However, deploying LLMs in the real world is not\ntrivial, as it requires substantial computing resources. In this paper, we\ninvestigate whether smaller, compact LLMs are a good alternative to the\ncomparatively Larger LLMs2 to address significant costs associated with\nutilizing LLMs in the real world. In this regard, we study the meeting\nsummarization task in a real-world industrial environment and conduct extensive\nexperiments by comparing the performance of fine-tuned compact LLMs (e.g.,\nFLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,\nGPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,\nfail to outperform larger zero-shot LLMs in meeting summarization datasets.\nHowever, a notable exception is FLAN-T5 (780M parameters), which performs on\npar or even better than many zero-shot Larger LLMs (from 7B to above 70B\nparameters), while being significantly smaller. This makes compact LLMs like\nFLAN-T5 a suitable cost-efficient solution for real-world industrial\ndeployment.",
        "pdf_link": "https://arxiv.org/pdf/2402.00841v1.pdf"
    },
    {
        "title": "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents",
        "authors": [
            "Zelong Li",
            "Wenyue Hua",
            "Hao Wang",
            "He Zhu",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-01T17:30:50Z",
        "summary": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel ``Formal-LLM''\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows human users to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The work is open-sourced at\nhttps://github.com/agiresearch/Formal-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.00798v2.pdf"
    },
    {
        "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
        "authors": [
            "Toni J. B. Liu",
            "Nicolas Boull\u00e9",
            "Rapha\u00ebl Sarfati",
            "Christopher J. Earls"
        ],
        "published": "2024-02-01T17:28:10Z",
        "summary": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. In this paper, we study LLMs'\nability to extrapolate the behavior of dynamical systems whose evolution is\ngoverned by principles of physical interest. Our results show that LLaMA 2, a\nlanguage model trained primarily on texts, achieves accurate predictions of\ndynamical system time series without fine-tuning or prompt engineering.\nMoreover, the accuracy of the learned physical rules increases with the length\nof the input context window, revealing an in-context version of neural scaling\nlaw. Along the way, we present a flexible and efficient algorithm for\nextracting probability density functions of multi-digit numbers directly from\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.00795v1.pdf"
    },
    {
        "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
        "authors": [
            "Alex J. Chan",
            "Hao Sun",
            "Samuel Holt",
            "Mihaela van der Schaar"
        ],
        "published": "2024-02-01T17:10:35Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the\nkey advance that has allowed Large Language Models (LLMs) to effectively follow\ninstructions and produce useful assistance. Classically, this involves\ngenerating completions from the LLM in response to a query before using a\nseparate reward model to assign a score to the full completion. As an\nauto-regressive process, the LLM has to take many \"actions\" (selecting\nindividual tokens) and only receives a single, sparse reward at the end of an\nepisode, a setup that is known to be difficult to optimise in traditional\nreinforcement learning. In this work we leverage the fact that the reward model\ncontains more information than just its scalar output, in particular, it\ncalculates an attention map over tokens as part of the transformer\narchitecture. We use these attention weights to redistribute the reward along\nthe whole completion, effectively densifying the signal and highlighting the\nmost important tokens, all without incurring extra computational cost or\nrequiring any additional modelling. We demonstrate that, theoretically, this\napproach is equivalent to potential-based reward shaping, ensuring that the\noptimal policy remains unchanged. Empirically, we show that it stabilises\ntraining, accelerates the rate of learning, and, in practical cases, may lead\nto better local optima.",
        "pdf_link": "https://arxiv.org/pdf/2402.00782v1.pdf"
    },
    {
        "title": "Unlearnable Algorithms for In-context Learning",
        "authors": [
            "Andrei Muresanu",
            "Anvith Thudi",
            "Michael R. Zhang",
            "Nicolas Papernot"
        ],
        "published": "2024-02-01T16:43:04Z",
        "summary": "Machine unlearning is a desirable operation as models get increasingly\ndeployed on data with unknown provenance. However, achieving exact unlearning\n-- obtaining a model that matches the model distribution when the data to be\nforgotten was never used -- is challenging or inefficient, often requiring\nsignificant retraining. In this paper, we focus on efficient unlearning methods\nfor the task adaptation phase of a pretrained large language model (LLM). We\nobserve that an LLM's ability to do in-context learning for task adaptation\nallows for efficient exact unlearning of task adaptation training data. We\nprovide an algorithm for selecting few-shot training examples to prepend to the\nprompt given to an LLM (for task adaptation), ERASE, whose unlearning operation\ncost is independent of model and dataset size, meaning it scales to large\nmodels and datasets. We additionally compare our approach to fine-tuning\napproaches and discuss the trade-offs between the two approaches. This leads us\nto propose a new holistic measure of unlearning cost which accounts for varying\ninference costs, and conclude that in-context learning can often be more\nfavourable than fine-tuning for deployments involving unlearning requests.",
        "pdf_link": "https://arxiv.org/pdf/2402.00751v1.pdf"
    },
    {
        "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
        "authors": [
            "Mingyu Jin",
            "Qinkai Yu",
            "Dong Shu",
            "Chong Zhang",
            "Lizhou Fan",
            "Wenyue Hua",
            "Suiyuan Zhu",
            "Yanda Meng",
            "Zhenting Wang",
            "Mengnan Du",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-01T16:40:32Z",
        "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.\nThe code is available at https://github.com/jmyissb/HealthLLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.00746v6.pdf"
    },
    {
        "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
        "authors": [
            "Xin Quan",
            "Marco Valentino",
            "Louise A. Dennis",
            "Andr\u00e9 Freitas"
        ],
        "published": "2024-02-01T16:39:51Z",
        "summary": "An increasing amount of research in Natural Language Inference (NLI) focuses\non the application and evaluation of Large Language Models (LLMs) and their\nreasoning capabilities. Despite their success, however, LLMs are still prone to\nfactual errors and inconsistencies in their explanations, offering limited\ncontrol and interpretability for inference in complex domains. In this paper,\nwe focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can\nenhance the logical validity and alignment of ethical explanations produced by\nLLMs. Specifically, we present an abductive-deductive framework named\nLogic-Explainer, which integrates LLMs with an external backward-chaining\nsolver to refine step-wise natural language explanations and jointly verify\ntheir correctness, reduce incompleteness and minimise redundancy. An extensive\nempirical analysis demonstrates that Logic-Explainer can improve explanations\ngenerated via in-context learning methods and Chain-of-Thought (CoT) on\nchallenging ethical NLI tasks, while, at the same time, producing formal proofs\ndescribing and supporting models' reasoning. As ethical NLI requires\ncommonsense reasoning to identify underlying moral violations, our results\nsuggest the effectiveness of neuro-symbolic methods for multi-step NLI more\nbroadly, opening new opportunities to enhance the logical consistency,\nreliability, and alignment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.00745v1.pdf"
    },
    {
        "title": "Transforming and Combining Rewards for Aligning Large Language Models",
        "authors": [
            "Zihao Wang",
            "Chirag Nagpal",
            "Jonathan Berant",
            "Jacob Eisenstein",
            "Alex D'Amour",
            "Sanmi Koyejo",
            "Victor Veitch"
        ],
        "published": "2024-02-01T16:39:28Z",
        "summary": "A common approach for aligning language models to human preferences is to\nfirst learn a reward model from preference data, and then use this reward model\nto update the language model. We study two closely related problems that arise\nin this approach. First, any monotone transformation of the reward model\npreserves preference ranking; is there a choice that is ``better'' than others?\nSecond, we often wish to align language models to multiple properties: how\nshould we combine multiple reward models? Using a probabilistic interpretation\nof the alignment procedure, we identify a natural choice for transformation for\n(the common case of) rewards learned from Bradley-Terry preference models. This\nderived transformation has two important properties. First, it emphasizes\nimproving poorly-performing outputs, rather than outputs that already score\nwell. This mitigates both underfitting (where some prompts are not improved)\nand reward hacking (where the model learns to exploit misspecification of the\nreward model). Second, it enables principled aggregation of rewards by linking\nsummation to logical conjunction: the sum of transformed rewards corresponds to\nthe probability that the output is ``good'' in all measured properties, in a\nsense we make precise. Experiments aligning language models to be both helpful\nand harmless using RLHF show substantial improvements over the baseline\n(non-transformed) approach.",
        "pdf_link": "https://arxiv.org/pdf/2402.00742v1.pdf"
    },
    {
        "title": "Intent Assurance using LLMs guided by Intent Drift",
        "authors": [
            "Kristina Dzeparoska",
            "Ali Tizghadam",
            "Alberto Leon-Garcia"
        ],
        "published": "2024-02-01T16:09:19Z",
        "summary": "Intent-Based Networking (IBN) presents a paradigm shift for network\nmanagement, by promising to align intents and business objectives with network\noperations--in an automated manner. However, its practical realization is\nchallenging: 1) processing intents, i.e., translate, decompose and identify the\nlogic to fulfill the intent, and 2) intent conformance, that is, considering\ndynamic networks, the logic should be adequately adapted to assure intents. To\naddress the latter, intent assurance is tasked with continuous verification and\nvalidation, including taking the necessary actions to align the operational and\ntarget states. In this paper, we define an assurance framework that allows us\nto detect and act when intent drift occurs. To do so, we leverage AI-driven\npolicies, generated by Large Language Models (LLMs) which can quickly learn the\nnecessary in-context requirements, and assist with the fulfillment and\nassurance of intents.",
        "pdf_link": "https://arxiv.org/pdf/2402.00715v2.pdf"
    },
    {
        "title": "Ocassionally Secure: A Comparative Analysis of Code Generation Assistants",
        "authors": [
            "Ran Elgedawy",
            "John Sadik",
            "Senjuti Dutta",
            "Anuj Gautam",
            "Konstantinos Georgiou",
            "Farzin Gholamrezae",
            "Fujiao Ji",
            "Kyungchan Lim",
            "Qian Liu",
            "Scott Ruoti"
        ],
        "published": "2024-02-01T15:49:47Z",
        "summary": "$ $Large Language Models (LLMs) are being increasingly utilized in various\napplications, with code generations being a notable example. While previous\nresearch has shown that LLMs have the capability to generate both secure and\ninsecure code, the literature does not take into account what factors help\ngenerate secure and effective code. Therefore in this paper we focus on\nidentifying and understanding the conditions and contexts in which LLMs can be\neffectively and safely deployed in real-world scenarios to generate quality\ncode. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and\nGPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to\nassess each model's code generation capabilities. We contextualized our study\nto represent the typical use cases of a real-life developer employing LLMs for\neveryday tasks as work. Additionally, we place an emphasis on security\nawareness which is represented through the use of two distinct versions of our\ndeveloper persona. In total, we collected 61 code outputs and analyzed them\nacross several aspects: functionality, security, performance, complexity, and\nreliability. These insights are crucial for understanding the models'\ncapabilities and limitations, guiding future development and practical\napplications in the field of automated code generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.00689v1.pdf"
    },
    {
        "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
        "authors": [
            "Fangkai Jiao",
            "Chengwei Qin",
            "Zhengyuan Liu",
            "Nancy F. Chen",
            "Shafiq Joty"
        ],
        "published": "2024-02-01T15:18:33Z",
        "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through direct preference optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2402.00658v1.pdf"
    },
    {
        "title": "Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks",
        "authors": [
            "Maan Qraitem",
            "Nazia Tasnim",
            "Piotr Teterwak",
            "Kate Saenko",
            "Bryan A. Plummer"
        ],
        "published": "2024-02-01T14:41:20Z",
        "summary": "Typographic Attacks, which involve pasting misleading text onto an image,\nwere noted to harm the performance of Vision-Language Models like CLIP.\nHowever, the susceptibility of recent Large Vision-Language Models to these\nattacks remains understudied. Furthermore, prior work's Typographic attacks\nagainst CLIP randomly sample a misleading class from a predefined set of\ncategories. However, this simple strategy misses more effective attacks that\nexploit LVLM(s) stronger language skills. To address these issues, we first\nintroduce a benchmark for testing Typographic attacks against LVLM(s).\nMoreover, we introduce two novel and more effective \\textit{Self-Generated}\nattacks which prompt the LVLM to generate an attack against itself: 1) Class\nBased Attack where the LVLM (e.g. LLaVA) is asked which deceiving class is most\nsimilar to the target class and 2) Descriptive Attacks where a more advanced\nLVLM (e.g. GPT4-V) is asked to recommend a Typographic attack that includes\nboth a deceiving class and description. Using our benchmark, we uncover that\nSelf-Generated attacks pose a significant threat, reducing LVLM(s)\nclassification performance by up to 33\\%. We also uncover that attacks\ngenerated by one model (e.g. GPT-4V or LLaVA) are effective against the model\nitself and other models like InstructBLIP and MiniGPT4. Code:\n\\url{https://github.com/mqraitem/Self-Gen-Typo-Attack}",
        "pdf_link": "https://arxiv.org/pdf/2402.00626v2.pdf"
    },
    {
        "title": "Actor Identification in Discourse: A Challenge for LLMs?",
        "authors": [
            "Ana Bari\u0107",
            "Sean Papay",
            "Sebastian Pad\u00f3"
        ],
        "published": "2024-02-01T14:30:39Z",
        "summary": "The identification of political actors who put forward claims in public\ndebate is a crucial step in the construction of discourse networks, which are\nhelpful to analyze societal debates. Actor identification is, however, rather\nchallenging: Often, the locally mentioned speaker of a claim is only a pronoun\n(\"He proposed that [claim]\"), so recovering the canonical actor name requires\ndiscourse understanding. We compare a traditional pipeline of dedicated NLP\ncomponents (similar to those applied to the related task of coreference) with a\nLLM, which appears a good match for this generation task. Evaluating on a\ncorpus of German actors in newspaper reports, we find surprisingly that the LLM\nperforms worse. Further analysis reveals that the LLM is very good at\nidentifying the right reference, but struggles to generate the correct\ncanonical form. This points to an underlying issue in LLMs with controlling\ngenerated output. Indeed, a hybrid model combining the LLM with a classifier to\nnormalize its output substantially outperforms both initial models.",
        "pdf_link": "https://arxiv.org/pdf/2402.00620v1.pdf"
    },
    {
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
        "authors": [
            "Ming Li",
            "Yong Zhang",
            "Shwai He",
            "Zhitao Li",
            "Hongyu Zhao",
            "Jianzong Wang",
            "Ning Cheng",
            "Tianyi Zhou"
        ],
        "published": "2024-02-01T11:57:53Z",
        "summary": "Instruction tuning is critical to improve LLMs but usually suffers from\nlow-quality and redundant data. Data filtering for instruction tuning has\nproved important in improving both the efficiency and performance of the tuning\nprocess. But it also leads to extra cost and computation due to the involvement\nof LLMs in this process. To reduce the filtering cost, we study Superfiltering:\nCan we use a smaller and weaker model to select data for finetuning a larger\nand stronger model? Despite the performance gap between weak and strong\nlanguage models, we find their highly consistent capability to perceive\ninstruction difficulty and data selection results. This enables us to use a\nmuch smaller and more efficient model to filter the instruction data used to\ntrain a larger language model. Not only does it largely speed up the data\nfiltering, but the filtered-data-finetuned LLM achieves even better performance\non standard benchmarks. Extensive experiments validate the efficacy and\nefficiency of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2402.00530v1.pdf"
    },
    {
        "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models",
        "authors": [
            "Xuchen Pan",
            "Yanxi Chen",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "published": "2024-02-01T11:39:04Z",
        "summary": "This work introduces EE-Tuning, a lightweight and economical solution to\ntraining/tuning early-exit large language models (LLMs). In contrast to the\ncommon approach of full-parameter pre-training, EE-Tuning augments any\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires\nsignificantly less computational resources and training data. Our\nimplementation of EE-Tuning achieves outstanding training efficiency via\nextensive performance optimizations, as well as scalability due to its full\ncompatibility with 3D parallelism. Results of systematic experiments validate\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\ncan be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.00518v1.pdf"
    },
    {
        "title": "SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models",
        "authors": [
            "Tianhan Xu",
            "Zhe Hu",
            "Ling Chen",
            "Bin Li"
        ],
        "published": "2024-02-01T10:26:27Z",
        "summary": "Recent advances in large language models (LLMs) have demonstrated exceptional\nperformance in various natural language processing (NLP) tasks. However, their\neffective application in the medical domain is hampered by a lack of medical\ndomain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable\nframework that aims to inject medical knowledge into general-purpose LLMs\nthrough instruction tuning, thereby enabling adaptability for various\ndownstream tasks. SA-MDKIF consists of two stages: skill training and skill\nadaptation. In the first stage, we define 12 basic medical skills and use\nAdaLoRA to train these skills based on uniformly formatted instructional\ndatasets that we have constructed. In the next stage, we train the skill router\nusing task-specific downstream data and use this router to integrate the\nacquired skills with LLMs during inference. Experimental results on 9 different\nmedical tasks show that SA-MDKIF improves performance by 10-20% compared to the\noriginal LLMs. Notably, this improvement is particularly pronounced for unseen\nmedical tasks, showing an improvement of up to 30%.",
        "pdf_link": "https://arxiv.org/pdf/2402.00474v1.pdf"
    },
    {
        "title": "From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models",
        "authors": [
            "Jung-Mei Chu",
            "Hao-Cheng Lo",
            "Jieh Hsiang",
            "Chun-Chieh Cho"
        ],
        "published": "2024-02-01T08:37:13Z",
        "summary": "In patent prosecution, timely and effective responses to Office Actions (OAs)\nare crucial for securing patents. However, past automation and artificial\nintelligence research have largely overlooked this aspect. To bridge this gap,\nour study introduces the Patent Office Action Response Intelligence System\n(PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS\n(LE-PARIS). These systems are designed to enhance the efficiency of patent\nattorneys in handling OA responses through collaboration with AI. The systems'\nkey features include the construction of an OA Topics Database, development of\nResponse Templates, and implementation of Recommender Systems and LLM-based\nResponse Generation. To validate the effectiveness of the systems, we have\nemployed a multi-paradigm analysis using the USPTO Office Action database and\nlongitudinal data based on attorney interactions with our systems over six\nyears. Through five studies, we have examined the constructiveness of OA topics\n(studies 1 and 2) using topic modeling and our proposed Delphi process, the\nefficacy of our proposed hybrid LLM-based recommender system tailored for OA\nresponses (study 3), the quality of generated responses (study 4), and the\nsystems' practical value in real-world scenarios through user studies (study\n5). The results indicate that both PARIS and LE-PARIS significantly achieve key\nmetrics and have a positive impact on attorney performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.00421v2.pdf"
    },
    {
        "title": "Prompt-Time Symbolic Knowledge Capture with Large Language Models",
        "authors": [
            "Tolga \u00c7\u00f6pl\u00fc",
            "Arto Bendiken",
            "Andrii Skomorokhov",
            "Eduard Bateiko",
            "Stephen Cobb",
            "Joshua J. Bouw"
        ],
        "published": "2024-02-01T08:15:28Z",
        "summary": "Augmenting large language models (LLMs) with user-specific knowledge is\ncrucial for real-world applications, such as personal AI assistants. However,\nLLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper\ninvestigates utilizing the existing LLM capabilities to enable prompt-driven\nknowledge capture, with a particular emphasis on knowledge graphs. We address\nthis challenge by focusing on prompt-to-triple (P2T) generation. We explore\nthree methods: zero-shot prompting, few-shot prompting, and fine-tuning, and\nthen assess their performance via a specialized synthetic dataset. Our code and\ndatasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.",
        "pdf_link": "https://arxiv.org/pdf/2402.00414v1.pdf"
    },
    {
        "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
        "authors": [
            "Xinlin Peng",
            "Ying Zhou",
            "Ben He",
            "Le Sun",
            "Yingfei Sun"
        ],
        "published": "2024-02-01T08:11:56Z",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in text\ngeneration tasks. However, the utilization of these models carries inherent\nrisks, including but not limited to plagiarism, the dissemination of fake news,\nand issues in educational exercises. Although several detectors have been\nproposed to address these concerns, their effectiveness against adversarial\nperturbations, specifically in the context of student essay writing, remains\nlargely unexplored. This paper aims to bridge this gap by constructing\nAIG-ASAP, an AI-generated student essay dataset, employing a range of text\nperturbation methods that are expected to generate high-quality essays while\nevading detection. Through empirical experiments, we assess the performance of\ncurrent AIGC detectors on the AIG-ASAP dataset. The results reveal that the\nexisting detectors can be easily circumvented using straightforward automatic\nadversarial attacks. Specifically, we explore word substitution and sentence\nsubstitution perturbation methods that effectively evade detection while\nmaintaining the quality of the generated essays. This highlights the urgent\nneed for more accurate and robust methods to detect AI-generated student essays\nin the education domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.00412v1.pdf"
    },
    {
        "title": "Investigating Bias Representations in Llama 2 Chat via Activation Steering",
        "authors": [
            "Dawn Lu",
            "Nina Rimsky"
        ],
        "published": "2024-02-01T07:48:50Z",
        "summary": "We address the challenge of societal bias in Large Language Models (LLMs),\nfocusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into\ndecision-making processes with substantial societal impact, it becomes\nimperative to ensure these models do not reinforce existing biases. Our\napproach employs activation steering to probe for and mitigate biases related\nto gender, race, and religion. This method manipulates model activations to\ndirect responses towards or away from biased outputs, utilizing steering\nvectors derived from the StereoSet dataset and custom GPT4 generated gender\nbias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,\npersisting even after Reinforcement Learning from Human Feedback (RLHF). We\nalso observe a predictable negative correlation between bias and the model's\ntendency to refuse responses. Significantly, our study uncovers that RLHF tends\nto increase the similarity in the model's representation of different forms of\nsocietal biases, which raises questions about the model's nuanced understanding\nof different forms of bias. This work also provides valuable insights into\neffective red-teaming strategies for LLMs using activation steering,\nparticularly emphasizing the importance of integrating a refusal vector.",
        "pdf_link": "https://arxiv.org/pdf/2402.00402v1.pdf"
    },
    {
        "title": "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection",
        "authors": [
            "Shangbin Feng",
            "Herun Wan",
            "Ningnan Wang",
            "Zhaoxuan Tan",
            "Minnan Luo",
            "Yulia Tsvetkov"
        ],
        "published": "2024-02-01T06:21:19Z",
        "summary": "Social media bot detection has always been an arms race between advancements\nin machine learning bot detectors and adversarial bot strategies to evade\ndetection. In this work, we bring the arms race to the next level by\ninvestigating the opportunities and risks of state-of-the-art large language\nmodels (LLMs) in social bot detection. To investigate the opportunities, we\ndesign novel LLM-based bot detectors by proposing a\nmixture-of-heterogeneous-experts framework to divide and conquer diverse user\ninformation modalities. To illuminate the risks, we explore the possibility of\nLLM-guided manipulation of user textual and structured information to evade\ndetection. Extensive experiments with three LLMs on two datasets demonstrate\nthat instruction tuning on merely 1,000 annotated examples produces specialized\nLLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets,\nwhile LLM-guided manipulation strategies could significantly bring down the\nperformance of existing bot detectors by up to 29.6% and harm the calibration\nand reliability of bot detection systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.00371v1.pdf"
    },
    {
        "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
        "authors": [
            "Shangbin Feng",
            "Weijia Shi",
            "Yike Wang",
            "Wenxuan Ding",
            "Vidhisha Balachandran",
            "Yulia Tsvetkov"
        ],
        "published": "2024-02-01T06:11:49Z",
        "summary": "Despite efforts to expand the knowledge of large language models (LLMs),\nknowledge gaps -- missing or outdated information in LLMs -- might always\npersist given the evolving nature of knowledge. In this work, we study\napproaches to identify LLM knowledge gaps and abstain from answering questions\nwhen knowledge gaps are present. We first adapt existing approaches to model\ncalibration or adaptation through fine-tuning/prompting and analyze their\nability to abstain from generating low-confidence outputs. Motivated by their\nfailures in self-reflection and over-reliance on held-out sets, we propose two\nnovel approaches that are based on model collaboration, i.e., LLMs probing\nother LLMs for knowledge gaps, either cooperatively or competitively. Extensive\nexperiments with three LLMs on four QA tasks featuring diverse knowledge\ndomains demonstrate that both cooperative and competitive approaches to\nunveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain\naccuracy against the strongest baseline. Further analysis reveals that our\nproposed mechanisms could help identify failure cases in retrieval augmentation\nand pinpoint knowledge gaps in multi-hop reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.00367v1.pdf"
    },
    {
        "title": "Safety of Multimodal Large Language Models on Images and Text",
        "authors": [
            "Xin Liu",
            "Yichen Zhu",
            "Yunshi Lan",
            "Chao Yang",
            "Yu Qiao"
        ],
        "published": "2024-02-01T05:57:10Z",
        "summary": "Attracted by the impressive power of Multimodal Large Language Models\n(MLLMs), the public is increasingly utilizing them to improve the efficiency of\ndaily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions\nbring huge safety risks when these models are deployed in real-world scenarios.\nIn this paper, we systematically survey current efforts on the evaluation,\nattack, and defense of MLLMs' safety on images and text. We begin with\nintroducing the overview of MLLMs on images and text and understanding of\nsafety, which helps researchers know the detailed scope of our survey. Then, we\nreview the evaluation datasets and metrics for measuring the safety of MLLMs.\nNext, we comprehensively present attack and defense techniques related to\nMLLMs' safety. Finally, we analyze several unsolved issues and discuss\npromising research directions. The latest papers are continually collected at\nhttps://github.com/isXinLiu/MLLM-Safety-Collection.",
        "pdf_link": "https://arxiv.org/pdf/2402.00357v2.pdf"
    },
    {
        "title": "Large Language Models Based Fuzzing Techniques: A Survey",
        "authors": [
            "Linghan Huang",
            "Peizhou Zhao",
            "Huaming Chen",
            "Lei Ma"
        ],
        "published": "2024-02-01T05:34:03Z",
        "summary": "In the modern era where software plays a pivotal role, software security and\nvulnerability analysis have become essential for software development. Fuzzing\ntest, as an efficient software testing method, are widely used in various\ndomains. Moreover, the rapid development of Large Language Models (LLMs) has\nfacilitated their application in the field of software testing, demonstrating\nremarkable performance. Considering that existing fuzzing test techniques are\nnot entirely automated and software vulnerabilities continue to evolve, there\nis a growing trend towards employing fuzzing test generated based on large\nlanguage models. This survey provides a systematic overview of the approaches\nthat fuse LLMs and fuzzing tests for software testing. In this paper, a\nstatistical analysis and discussion of the literature in three areas, namely\nLLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by\nsummarising the state-of-the-art methods up until 2024. Our survey also\ninvestigates the potential for widespread deployment and application of fuzzing\ntest techniques generated by LLMs in the future.",
        "pdf_link": "https://arxiv.org/pdf/2402.00350v2.pdf"
    },
    {
        "title": "IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators",
        "authors": [
            "Luyang Lin",
            "Lingzhi Wang",
            "Xiaoyan Zhao",
            "Jing Li",
            "Kam-Fai Wong"
        ],
        "published": "2024-02-01T05:20:07Z",
        "summary": "This study focuses on media bias detection, crucial in today's era of\ninfluential social media platforms shaping individual attitudes and opinions.\nIn contrast to prior work that primarily relies on training specific models\ntailored to particular datasets, resulting in limited adaptability and subpar\nperformance on out-of-domain data, we introduce a general bias detection\nframework, IndiVec, built upon large language models. IndiVec begins by\nconstructing a fine-grained media bias database, leveraging the robust\ninstruction-following capabilities of large language models and vector database\ntechniques. When confronted with new input for bias detection, our framework\nautomatically selects the most relevant indicator from the vector database and\nemploys majority voting to determine the input's bias label. IndiVec excels\ncompared to previous methods due to its adaptability (demonstrating consistent\nperformance across diverse datasets from various sources) and explainability\n(providing explicit top-k indicators to interpret bias predictions).\nExperimental results on four political bias datasets highlight IndiVec's\nsignificant superiority over baselines. Furthermore, additional experiments and\nanalysis provide profound insights into the framework's effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.00345v1.pdf"
    },
    {
        "title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation",
        "authors": [
            "Elijah Berberette",
            "Jack Hutchins",
            "Amir Sadovnik"
        ],
        "published": "2024-02-01T03:01:11Z",
        "summary": "In recent years, large language models (LLMs) have become incredibly popular,\nwith ChatGPT for example being used by over a billion users. While these models\nexhibit remarkable language understanding and logical prowess, a notable\nchallenge surfaces in the form of \"hallucinations.\" This phenomenon results in\nLLMs outputting misinformation in a confident manner, which can lead to\ndevastating consequences with such a large user base. However, we question the\nappropriateness of the term \"hallucination\" in LLMs, proposing a psychological\ntaxonomy based on cognitive biases and other psychological phenomena. Our\napproach offers a more fine-grained understanding of this phenomenon, allowing\nfor targeted solutions. By leveraging insights from how humans internally\nresolve similar challenges, we aim to develop strategies to mitigate LLM\nhallucinations. This interdisciplinary approach seeks to move beyond\nconventional terminology, providing a nuanced understanding and actionable\npathways for improvement in LLM reliability.",
        "pdf_link": "https://arxiv.org/pdf/2402.01769v1.pdf"
    },
    {
        "title": "Multimodal Embodied Interactive Agent for Cafe Scene",
        "authors": [
            "Yang Liu",
            "Xinshuai Song",
            "Kaixuan Jiang",
            "Weixing Chen",
            "Jingzhou Luo",
            "Guanbin Li",
            "Liang Lin"
        ],
        "published": "2024-02-01T02:43:20Z",
        "summary": "With the surge in the development of large language models, embodied\nintelligence has attracted increasing attention. Nevertheless, prior works on\nembodied intelligence typically encode scene or historical memory in an\nunimodal manner, either visual or linguistic, which complicates the alignment\nof the model's action planning with embodied control. To overcome this\nlimitation, we introduce the Multimodal Embodied Interactive Agent (MEIA),\ncapable of translating high-level tasks expressed in natural language into a\nsequence of executable actions. Specifically, we propose a novel Multimodal\nEnvironment Memory (MEM) module, facilitating the integration of embodied\ncontrol with large models through the visual-language memory of scenes. This\ncapability enables MEIA to generate executable action plans based on diverse\nrequirements and the robot's capabilities. We conduct experiments in a dynamic\nvirtual cafe environment, utilizing multiple large models through zero-shot\nlearning, and carefully design scenarios for various situations. The\nexperimental results showcase the promising performance of our MEIA in various\nembodied interactive tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.00290v1.pdf"
    },
    {
        "title": "PAP-REC: Personalized Automatic Prompt for Recommendation Language Model",
        "authors": [
            "Zelong Li",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Wenyue Hua",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-01T02:29:16Z",
        "summary": "Recently emerged prompt-based Recommendation Language Models (RLM) can solve\nmultiple recommendation tasks uniformly. The RLMs make full use of the\ninherited knowledge learned from the abundant pre-training data to solve the\ndownstream recommendation tasks by prompts, without introducing additional\nparameters or network training. However, handcrafted prompts require\nsignificant expertise and human effort since slightly rewriting prompts may\ncause massive performance changes. In this paper, we propose PAP-REC, a\nframework to generate the Personalized Automatic Prompt for RECommendation\nlanguage models to mitigate the inefficiency and ineffectiveness problems\nderived from manually designed prompts. Specifically, personalized automatic\nprompts allow different users to have different prompt tokens for the same\ntask, automatically generated using a gradient-based method. One challenge for\npersonalized automatic prompt generation for recommendation language models is\nthe extremely large search space, leading to a long convergence time. To\neffectively and efficiently address the problem, we develop surrogate metrics\nand leverage an alternative updating schedule for prompting recommendation\nlanguage models. Experimental results show that our PAP-REC framework manages\nto generate personalized prompts, and the automatically generated prompts\noutperform manually constructed prompts and also outperform various baseline\nrecommendation models. The source code of the work is available at\nhttps://github.com/rutgerswiselab/PAP-REC.",
        "pdf_link": "https://arxiv.org/pdf/2402.00284v1.pdf"
    },
    {
        "title": "HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA",
        "authors": [
            "Xinyue Chen",
            "Pengyu Gao",
            "Jiangjiang Song",
            "Xiaoyang Tan"
        ],
        "published": "2024-02-01T02:24:15Z",
        "summary": "As language model agents leveraging external tools rapidly evolve,\nsignificant progress has been made in question-answering(QA) methodologies\nutilizing supplementary documents and the Retrieval-Augmented Generation (RAG)\napproach. This advancement has improved the response quality of language models\nand alleviates the appearance of hallucination. However, these methods exhibit\nlimited retrieval accuracy when faced with massive indistinguishable documents,\npresenting notable challenges in their practical application. In response to\nthese emerging challenges, we present HiQA, an advanced framework for\nmulti-document question-answering (MDQA) that integrates cascading metadata\ninto content as well as a multi-route retrieval mechanism. We also release a\nbenchmark called MasQA to evaluate and research in MDQA. Finally, HiQA\ndemonstrates the state-of-the-art performance in multi-document environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.01767v1.pdf"
    },
    {
        "title": "Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better",
        "authors": [
            "Shengchao Liu",
            "Xiaoming Liu",
            "Yichen Wang",
            "Zehua Cheng",
            "Chengzhengxu Li",
            "Zhaohan Zhang",
            "Yu Lan",
            "Chao Shen"
        ],
        "published": "2024-02-01T01:23:07Z",
        "summary": "The burgeoning generative capabilities of large language models (LLMs) have\nraised growing concerns about abuse, demanding automatic machine-generated text\ndetectors. DetectGPT, a zero-shot metric-based detector, first introduces\nperturbation and shows great performance improvement. However, in DetectGPT,\nrandom perturbation strategy could introduce noise, and logit regression\ndepends on threshold, harming the generalizability and applicability of\nindividual or small-batch inputs. Hence, we propose a novel fine-tuned\ndetector, Pecola, bridging metric-based and fine-tuned detectors by contrastive\nlearning on selective perturbation. Selective strategy retains important tokens\nduring perturbation and weights for multi-pair contrastive learning. The\nexperiments show that Pecola outperforms the state-of-the-art by 1.20% in\naccuracy on average on four public datasets. And we further analyze the\neffectiveness, robustness, and generalization of the method.",
        "pdf_link": "https://arxiv.org/pdf/2402.00263v3.pdf"
    },
    {
        "title": "Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective",
        "authors": [
            "Qun Ma",
            "Xiao Xue",
            "Deyu Zhou",
            "Xiangning Yu",
            "Donghua Liu",
            "Xuwen Zhang",
            "Zihan Zhao",
            "Yifan Shen",
            "Peilin Ji",
            "Juanjuan Li",
            "Gang Wang",
            "Wanpeng Ma"
        ],
        "published": "2024-02-01T01:17:46Z",
        "summary": "Computational experiments have emerged as a valuable method for studying\ncomplex systems, involving the algorithmization of counterfactuals. However,\naccurately representing real social systems in Agent-based Modeling (ABM) is\nchallenging due to the diverse and intricate characteristics of humans,\nincluding bounded rationality and heterogeneity. To address this limitation,\nthe integration of Large Language Models (LLMs) has been proposed, enabling\nagents to possess anthropomorphic abilities such as complex reasoning and\nautonomous learning. These agents, known as LLM-based Agent, offer the\npotential to enhance the anthropomorphism lacking in ABM. Nonetheless, the\nabsence of explicit explainability in LLMs significantly hinders their\napplication in the social sciences. Conversely, computational experiments excel\nin providing causal analysis of individual behaviors and complex phenomena.\nThus, combining computational experiments with LLM-based Agent holds\nsubstantial research potential. This paper aims to present a comprehensive\nexploration of this fusion. Primarily, it outlines the historical development\nof agent structures and their evolution into artificial societies, emphasizing\ntheir importance in computational experiments. Then it elucidates the\nadvantages that computational experiments and LLM-based Agents offer each\nother, considering the perspectives of LLM-based Agent for computational\nexperiments and vice versa. Finally, this paper addresses the challenges and\nfuture trends in this research domain, offering guidance for subsequent related\nstudies.",
        "pdf_link": "https://arxiv.org/pdf/2402.00262v1.pdf"
    },
    {
        "title": "Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs",
        "authors": [
            "Ruchik Mishra",
            "Karla Conn Welch"
        ],
        "published": "2024-02-01T01:09:00Z",
        "summary": "In this paper, we propose a social robot capable of verbally interacting with\nchildren with Autism Spectrum Disorder (ASD). This communication is meant to\nteach perspective-taking using text generated using a Large Language Model\n(LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a\nsocial situation and asks a question), prompter (presents three options to\nchoose from), and reinforcer (praises when the answer is correct). For the role\nof the stimulator, the social situation, questions, and options are generated\nusing our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 +\nGPT-2, where the first GPT-2 common between the pipelines is used for\nunsupervised social situation generation. We use the SOCIALIQA dataset to\nfine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had\na better BERTscore for generating the questions and the options by combining\ntheir individual loss functions. This observation was also consistent with the\nhuman evaluations. Lastly, the unsupervised generation of social situations was\nvisualized using T-SNE plots, and the entire pipeline was evaluated for\nappropriateness for children with ASD by human experts.",
        "pdf_link": "https://arxiv.org/pdf/2402.00260v1.pdf"
    },
    {
        "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
        "authors": [
            "Yao-Hung Hubert Tsai",
            "Walter Talbott",
            "Jian Zhang"
        ],
        "published": "2024-02-01T00:23:31Z",
        "summary": "Step-by-step decision planning with large language models (LLMs) is gaining\nattention in AI agent development. This paper focuses on decision planning with\nuncertainty estimation to address the hallucination problem in language models.\nExisting approaches are either white-box or computationally demanding, limiting\nuse of black-box proprietary LLMs within budgets. The paper's first\ncontribution is a non-parametric uncertainty quantification method for LLMs,\nefficiently estimating point-wise dependencies between input-decision on the\nfly with a single inference, without access to token logits. This estimator\ninforms the statistical interpretation of decision trustworthiness. The second\ncontribution outlines a systematic design for a decision-making agent,\ngenerating actions like ``turn on the bathroom light'' based on user prompts\nsuch as ``take a bath''. Users will be asked to provide preferences when more\nthan one action has high estimated point-wise dependencies. In conclusion, our\nuncertainty estimation and decision-making agent design offer a cost-efficient\napproach for AI agent development.",
        "pdf_link": "https://arxiv.org/pdf/2402.00251v1.pdf"
    }
]