[
    {
        "title": "Knowledge-Centric Templatic Views of Documents",
        "authors": [
            "Isabel Cachola",
            "Silviu Cucerzan",
            "Allen Herring",
            "Vuksan Mijovic",
            "Erik Oveson",
            "Sujay Kumar Jauhar"
        ],
        "published": "2024-01-13T01:22:15Z",
        "summary": "Authors seeking to communicate with broader audiences often compose their\nideas about the same underlying knowledge in different documents and formats --\nfor example, as slide decks, newsletters, reports, brochures, etc. Prior work\nin document generation has generally considered the creation of each separate\nformat to be different a task, developing independent methods for generation\nand evaluation. This approach is suboptimal for the advancement of AI-supported\ncontent authoring from both research and application perspectives because it\nleads to fragmented learning processes, redundancy in models and methods, and\ndisjointed evaluation. Thus, in our work, we consider each of these documents\nto be templatic views of the same underlying knowledge, and we aim to unify the\ngeneration and evaluation of these templatic views of documents. We begin by\nintroducing an LLM-powered method to extract the most important information\nfrom an input document and represent this information in a structured format.\nWe show that this unified representation can be used to generate multiple\ntemplatic views with no supervision and with very little guidance, improving\nover strong baselines. We additionally introduce a unified evaluation method\nthat is template agnostic, and can be adapted to building document generators\nfor heterogeneous downstream applications. Finally, we conduct a human\nevaluation, which shows that humans prefer 82% of the downstream documents\ngenerated with our method. Furthermore, the newly proposed evaluation metric\ncorrelates more highly with human judgement than prior metrics, while providing\na unified evaluation method.",
        "pdf_link": "https://arxiv.org/pdf/2401.06945v1.pdf"
    },
    {
        "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
        "authors": [
            "Oindrila Saha",
            "Grant Van Horn",
            "Subhransu Maji"
        ],
        "published": "2024-01-04T08:39:13Z",
        "summary": "The zero-shot performance of existing vision-language models (VLMs) such as\nCLIP is limited by the availability of large-scale, aligned image and text\ndatasets in specific domains. In this work, we leverage two complementary\nsources of information -- descriptions of categories generated by large\nlanguage models (LLMs) and abundant, fine-grained image classification datasets\n-- to improve the zero-shot classification performance of VLMs across\nfine-grained domains. On the technical side, we develop methods to train VLMs\nwith this \"bag-level\" image-text supervision. We find that simply using these\nattributes at test-time does not improve performance, but our training\nstrategy, for example, on the iNaturalist dataset, leads to an average\nimprovement of 4-5% in zero-shot classification accuracy for novel categories\nof birds and flowers. Similar improvements are observed in domains where a\nsubset of the categories was used to fine-tune the model. By prompting LLMs in\nvarious ways, we generate descriptions that capture visual appearance, habitat,\nand geographic regions and pair them with existing attributes such as the\ntaxonomic structure of the categories. We systematically evaluate their ability\nto improve zero-shot categorization in natural domains. Our findings suggest\nthat geographic priors can be just as effective and are complementary to visual\nappearance. Our method also outperforms prior work on prompt-based tuning of\nVLMs. We release the benchmark, consisting of 14 datasets at\nhttps://github.com/cvl-umass/AdaptCLIPZS , which will contribute to future\nresearch in zero-shot recognition.",
        "pdf_link": "https://arxiv.org/pdf/2401.02460v2.pdf"
    },
    {
        "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "authors": [
            "Tianle Cai",
            "Yuhong Li",
            "Zhengyang Geng",
            "Hongwu Peng",
            "Jason D. Lee",
            "Deming Chen",
            "Tri Dao"
        ],
        "published": "2024-01-19T15:48:40Z",
        "summary": "The inference process in Large Language Models (LLMs) is often limited due to\nthe absence of parallelism in the auto-regressive decoding process, resulting\nin most operations being restricted by the memory bandwidth of accelerators.\nWhile methods such as speculative decoding have been suggested to address this\nissue, their implementation is impeded by the challenges associated with\nacquiring and maintaining a separate draft model. In this paper, we present\nMedusa, an efficient method that augments LLM inference by adding extra\ndecoding heads to predict multiple subsequent tokens in parallel. Using a\ntree-based attention mechanism, Medusa constructs multiple candidate\ncontinuations and verifies them simultaneously in each decoding step. By\nleveraging parallel processing, Medusa introduces only minimal overhead in\nterms of single-step latency while substantially reducing the number of\ndecoding steps required.\n  We present two levels of fine-tuning procedures for Medusa to meet the needs\nof different use cases: Medusa-1: Medusa is directly fine-tuned on top of a\nfrozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa\nis fine-tuned together with the backbone LLM, enabling better prediction\naccuracy of Medusa heads and higher speedup but needing a special training\nrecipe that preserves the backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.",
        "pdf_link": "https://arxiv.org/pdf/2401.10774v1.pdf"
    },
    {
        "title": "A Study on Large Language Models' Limitations in Multiple-Choice Question Answering",
        "authors": [
            "Aisha Khatun",
            "Daniel G. Brown"
        ],
        "published": "2024-01-15T20:42:16Z",
        "summary": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever.",
        "pdf_link": "https://arxiv.org/pdf/2401.07955v1.pdf"
    },
    {
        "title": "TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview",
        "authors": [
            "Mohammad Aliannejadi",
            "Zahra Abbasiantaeb",
            "Shubham Chatterjee",
            "Jeffery Dalton",
            "Leif Azzopardi"
        ],
        "published": "2024-01-02T18:40:03Z",
        "summary": "Conversational Information Seeking has evolved rapidly in the last few years\nwith the development of Large Language Models providing the basis for\ninterpreting and responding in a naturalistic manner to user requests. iKAT\nemphasizes the creation and research of conversational search agents that adapt\nresponses based on the user's prior interactions and present context. This\nmeans that the same question might yield varied answers, contingent on the\nuser's profile and preferences. The challenge lies in enabling Conversational\nSearch Agents (CSA) to incorporate personalized context to effectively guide\nusers through the relevant information to them. iKAT's first year attracted\nseven teams and a total of 24 runs. Most of the runs leveraged Large Language\nModels (LLMs) in their pipelines, with a few focusing on a\ngenerate-then-retrieve approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.01330v2.pdf"
    },
    {
        "title": "Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization",
        "authors": [
            "Jianfei Xiao",
            "Yancan Chen",
            "Yimin Ou",
            "Hanyi Yu",
            "Kai Shu",
            "Yiyong Xiao"
        ],
        "published": "2024-01-27T20:20:39Z",
        "summary": "Large language models (LLMs) like Llama, Baichuan and Bloom models show\nremarkable ability with instruction fine-tuning in many natural language tasks.\nNevertheless, for the dialogue summarization task, which aims to generate\nsummaries for different roles in dialogue, most of the state-of-the-art methods\nconduct on small models (e.g Bart and Bert). Existing methods try to add task\nspecified optimization on small models like adding global-local centrality\nscore to models. In this paper, we propose an instruction fine-tuning model:\nBaichuan2-Sum, for role-oriented diaglouge summarization. By setting different\ninstructions for different roles, the model can learn from the dialogue\ninteractions and output the expected summaries. Furthermore, we applied NEFTune\ntechnique to add suitable noise during training to improve the results. The\nexperiments demonstrate that the proposed model achieves the new\nstate-of-the-art results on two public dialogue summarization datasets: CSDS\nand SAMSUM. We release our model and related codes to facilitate future studies\non dialogue summarization task.",
        "pdf_link": "https://arxiv.org/pdf/2401.15496v3.pdf"
    },
    {
        "title": "Fast and Optimal Weight Update for Pruned Large Language Models",
        "authors": [
            "Vladim\u00edr Bo\u017ea"
        ],
        "published": "2024-01-01T23:10:23Z",
        "summary": "Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and optimal weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nCoupled with a simple iterative pruning mask selection, our algorithm achieves\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.",
        "pdf_link": "https://arxiv.org/pdf/2401.02938v1.pdf"
    },
    {
        "title": "CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling",
        "authors": [
            "Jawook Gu",
            "Han-Cheol Cho",
            "Jiho Kim",
            "Kihyun You",
            "Eun Kyoung Hong",
            "Byungseok Roh"
        ],
        "published": "2024-01-21T14:30:20Z",
        "summary": "Free-text radiology reports present a rich data source for various medical\ntasks, but effectively labeling these texts remains challenging. Traditional\nrule-based labeling methods fall short of capturing the nuances of diverse\nfree-text patterns. Moreover, models using expert-annotated data are limited by\ndata scarcity and pre-defined classes, impacting their performance, flexibility\nand scalability. To address these issues, our study offers three main\ncontributions: 1) We demonstrate the potential of GPT as an adept labeler using\ncarefully designed prompts. 2) Utilizing only the data labeled by GPT, we\ntrained a BERT-based labeler, CheX-GPT, which operates faster and more\nefficiently than its GPT counterpart. 3) To benchmark labeler performance, we\nintroduced a publicly available expert-annotated test set, MIMIC-500,\ncomprising 500 cases from the MIMIC validation set. Our findings demonstrate\nthat CheX-GPT not only excels in labeling accuracy over existing models, but\nalso showcases superior efficiency, flexibility, and scalability, supported by\nour introduction of the MIMIC-500 dataset for robust benchmarking. Code and\nmodels are available at https://github.com/kakaobrain/CheXGPT.",
        "pdf_link": "https://arxiv.org/pdf/2401.11505v1.pdf"
    },
    {
        "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
        "authors": [
            "Mengwei Xu",
            "Wangsong Yin",
            "Dongqi Cai",
            "Rongjie Yi",
            "Daliang Xu",
            "Qipeng Wang",
            "Bingyang Wu",
            "Yihao Zhao",
            "Chen Yang",
            "Shihe Wang",
            "Qiyang Zhang",
            "Zhenyan Lu",
            "Li Zhang",
            "Shangguang Wang",
            "Yuanchun Li",
            "Yunxin Liu",
            "Xin Jin",
            "Xuanzhe Liu"
        ],
        "published": "2024-01-16T03:35:26Z",
        "summary": "Large foundation models, including large language models (LLMs), vision\ntransformers (ViTs), diffusion, and LLM-based multimodal models, are\nrevolutionizing the entire machine learning lifecycle, from training to\ndeployment. However, the substantial advancements in versatility and\nperformance these models offer come at a significant cost in terms of hardware\nresources. To support the growth of these large models in a scalable and\nenvironmentally sustainable way, there has been a considerable focus on\ndeveloping resource-efficient strategies. This survey delves into the critical\nimportance of such research, examining both algorithmic and systemic aspects.\nIt offers a comprehensive analysis and valuable insights gleaned from existing\nliterature, encompassing a broad array of topics from cutting-edge model\narchitectures and training/serving algorithms to practical system designs and\nimplementations. The goal of this survey is to provide an overarching\nunderstanding of how current approaches are tackling the resource challenges\nposed by large foundation models and to potentially inspire future\nbreakthroughs in this field.",
        "pdf_link": "https://arxiv.org/pdf/2401.08092v1.pdf"
    },
    {
        "title": "E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models",
        "authors": [
            "Jinchang Hou",
            "Chang Ao",
            "Haihong Wu",
            "Xiangtao Kong",
            "Zhigang Zheng",
            "Daijia Tang",
            "Chengming Li",
            "Xiping Hu",
            "Ruifeng Xu",
            "Shiwen Ni",
            "Min Yang"
        ],
        "published": "2024-01-29T07:34:37Z",
        "summary": "With the accelerating development of Large Language Models (LLMs), many LLMs\nare beginning to be used in the Chinese K-12 education domain. The integration\nof LLMs and education is getting closer and closer, however, there is currently\nno benchmark for evaluating LLMs that focuses on the Chinese K-12 education\ndomain. Therefore, there is an urgent need for a comprehensive natural language\nprocessing benchmark to accurately assess the capabilities of various LLMs in\nthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,\nthe first comprehensive evaluation benchmark specifically designed for the\nChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice\nquestions at the primary, middle, and high school levels across a wide range of\nsubjects, including Chinese, English, Politics, History, Ethics, Physics,\nChemistry, Mathematics, and Geography. We conducted a comprehensive evaluation\nof E-EVAL on advanced LLMs, including both English-dominant and\nChinese-dominant models. Findings show that Chinese-dominant models perform\nwell compared to English-dominant models, with many scoring even above the GPT\n4.0. However, almost all models perform poorly in complex subjects such as\nmathematics. We also found that most Chinese-dominant LLMs did not achieve\nhigher scores at the primary school level compared to the middle school level.\nWe observe that the mastery of higher-order knowledge by the model does not\nnecessarily imply the mastery of lower-order knowledge as well. Additionally,\nthe experimental results indicate that the Chain of Thought (CoT) technique is\neffective only for the challenging science subjects, while Few-shot prompting\nis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze\nthe strengths and limitations of LLMs in educational applications, and to\ncontribute to the progress and development of Chinese K-12 education and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.15927v1.pdf"
    },
    {
        "title": "Monte Carlo Tree Search for Recipe Generation using GPT-2",
        "authors": [
            "Karan Taneja",
            "Richard Segal",
            "Richard Goodwin"
        ],
        "published": "2024-01-10T14:50:46Z",
        "summary": "Automatic food recipe generation methods provide a creative tool for chefs to\nexplore and to create new, and interesting culinary delights. Given the recent\nsuccess of large language models (LLMs), they have the potential to create new\nrecipes that can meet individual preferences, dietary constraints, and adapt to\nwhat is in your refrigerator. Existing research on using LLMs to generate\nrecipes has shown that LLMs can be finetuned to generate realistic-sounding\nrecipes. However, on close examination, these generated recipes often fail to\nmeet basic requirements like including chicken as an ingredient in chicken\ndishes. In this paper, we propose RecipeMC, a text generation method using\nGPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to\ndefine reward functions to put soft constraints on text generation and thus\nimprove the credibility of the generated recipes. Our results show that human\nevaluators prefer recipes generated with RecipeMC more often than recipes\ngenerated with other baseline methods when compared with real recipes.",
        "pdf_link": "https://arxiv.org/pdf/2401.05199v1.pdf"
    },
    {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
        "authors": [
            "Junyi Li",
            "Jie Chen",
            "Ruiyang Ren",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2024-01-06T12:40:45Z",
        "summary": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "pdf_link": "https://arxiv.org/pdf/2401.03205v1.pdf"
    },
    {
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "authors": [
            "Mingyu Jin",
            "Qinkai Yu",
            "Dong Shu",
            "Haiyan Zhao",
            "Wenyue Hua",
            "Yanda Meng",
            "Yongfeng Zhang",
            "Mengnan Du"
        ],
        "published": "2024-01-10T04:37:38Z",
        "summary": "Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.",
        "pdf_link": "https://arxiv.org/pdf/2401.04925v3.pdf"
    },
    {
        "title": "A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",
        "authors": [
            "Mohamed R. Shoaib",
            "Heba M. Emara",
            "Jun Zhao"
        ],
        "published": "2024-01-12T10:29:48Z",
        "summary": "This survey paper explores the transformative influence of frontier AI,\nfoundation models, and Large Language Models (LLMs) in the realm of Intelligent\nTransportation Systems (ITS), emphasizing their integral role in advancing\ntransportation intelligence, optimizing traffic management, and contributing to\nthe realization of smart cities. Frontier AI refers to the forefront of AI\ntechnology, encompassing the latest advancements, innovations, and experimental\ntechniques in the field, especially AI foundation models and LLMs. Foundation\nmodels, like GPT-4, are large, general-purpose AI models that provide a base\nfor a wide range of applications. They are characterized by their versatility\nand scalability. LLMs are obtained from finetuning foundation models with a\nspecific focus on processing and generating natural language. They excel in\ntasks like language understanding, text generation, translation, and\nsummarization. By leveraging vast textual data, including traffic reports and\nsocial media interactions, LLMs extract critical insights, fostering the\nevolution of ITS. The survey navigates the dynamic synergy between LLMs and\nITS, delving into applications in traffic management, integration into\nautonomous vehicles, and their role in shaping smart cities. It provides\ninsights into ongoing research, innovations, and emerging trends, aiming to\ninspire collaboration at the intersection of language, intelligence, and\nmobility for safer, more efficient, and sustainable transportation systems. The\npaper further surveys interactions between LLMs and various aspects of ITS,\nexploring roles in traffic management, facilitating autonomous vehicles, and\ncontributing to smart city development, while addressing challenges brought by\nfrontier AI and foundation models. This paper offers valuable inspiration for\nfuture research and innovation in the transformative domain of intelligent\ntransportation.",
        "pdf_link": "https://arxiv.org/pdf/2401.06831v1.pdf"
    },
    {
        "title": "XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model",
        "authors": [
            "Zhitao Wang",
            "Wei Wang",
            "Zirao Li",
            "Long Wang",
            "Can Yi",
            "Xinjie Xu",
            "Luyang Cao",
            "Hanjing Su",
            "Shouzhi Chen",
            "Jun Zhou"
        ],
        "published": "2024-01-05T08:24:30Z",
        "summary": "In past years, we have been dedicated to automating user acceptance testing\n(UAT) process of WeChat Pay, one of the most influential mobile payment\napplications in China. A system titled XUAT has been developed for this\npurpose. However, there is still a human-labor-intensive stage, i.e, test\nscripts generation, in the current system. Therefore, in this paper, we\nconcentrate on methods of boosting the automation level of the current system,\nparticularly the stage of test scripts generation. With recent notable\nsuccesses, large language models (LLMs) demonstrate significant potential in\nattaining human-like intelligence and there has been a growing research area\nthat employs LLMs as autonomous agents to obtain human-like decision-making\ncapabilities. Inspired by these works, we propose an LLM-powered multi-agent\ncollaborative system, named XUAT-Copilot, for automated UAT. The proposed\nsystem mainly consists of three LLM-based agents responsible for action\nplanning, state checking and parameter selecting, respectively, and two\nadditional modules for state sensing and case rewriting. The agents interact\nwith testing device, make human-like decision and generate action command in a\ncollaborative way. The proposed multi-agent system achieves a close\neffectiveness to human testers in our experimental studies and gains a\nsignificant improvement of Pass@1 accuracy compared with single-agent\narchitecture. More importantly, the proposed system has launched in the formal\ntesting environment of WeChat Pay mobile app, which saves a considerable amount\nof manpower in the daily development work.",
        "pdf_link": "https://arxiv.org/pdf/2401.02705v2.pdf"
    },
    {
        "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models",
        "authors": [
            "Terry Yue Zhuo",
            "Armel Zebaze",
            "Nitchakarn Suppattarachai",
            "Leandro von Werra",
            "Harm de Vries",
            "Qian Liu",
            "Niklas Muennighoff"
        ],
        "published": "2024-01-01T15:30:19Z",
        "summary": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.00788v1.pdf"
    },
    {
        "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "authors": [
            "Harvey Lederman",
            "Kyle Mahowald"
        ],
        "published": "2024-01-10T00:05:45Z",
        "summary": "Are LLMs cultural technologies like photocopiers or printing presses, which\ntransmit information but cannot create new content? A challenge for this idea,\nwhich we call bibliotechnism, is that LLMs often generate entirely novel text.\nWe begin (Part I) with a sustained defense of bibliotechnism against this\nchallenge showing how even entirely novel text may be meaningful only in a\nderivative sense, and arguing that, in particular, much novel text generated by\nLLMs is only derivatively meaningful. But we argue (Part II) that\nbibliotechnism faces a different, novel challenge, stemming from examples in\nwhich LLMs generate \"novel reference\", using novel names to refer to novel\nentities. Such examples could be smoothly explained if LLMs were not cultural\ntechnologies but possessed a limited form of agency (beliefs, desires, and\nintentions). According to interpretationism in the philosophy of mind, a system\nhas beliefs, desires and intentions if and only if its behavior is well\nexplained by the hypothesis that it has such states. So, according to\ninterpretationism, cases of novel reference provide evidence that LLMs have\nbeliefs, desires, and intentions. Given that interpretationism is a live\nhypothesis about the nature of these states, we suggest that cases of novel\nreference provide evidence that LLMs do have beliefs, desires, and intentions.",
        "pdf_link": "https://arxiv.org/pdf/2401.04854v2.pdf"
    },
    {
        "title": "Assessing and Understanding Creativity in Large Language Models",
        "authors": [
            "Yunpu Zhao",
            "Rui Zhang",
            "Wenyi Li",
            "Di Huang",
            "Jiaming Guo",
            "Shaohui Peng",
            "Yifan Hao",
            "Yuanbo Wen",
            "Xing Hu",
            "Zidong Du",
            "Qi Guo",
            "Ling Li",
            "Yunji Chen"
        ],
        "published": "2024-01-23T05:19:47Z",
        "summary": "In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.12491v1.pdf"
    },
    {
        "title": "LLM on FHIR -- Demystifying Health Records",
        "authors": [
            "Paul Schmiedmayer",
            "Adrit Rao",
            "Philipp Zagar",
            "Vishnu Ravi",
            "Aydin Zahedivash",
            "Arash Fereydooni",
            "Oliver Aalami"
        ],
        "published": "2024-01-25T17:45:34Z",
        "summary": "Objective: To enhance health literacy and accessibility of health information\nfor a diverse patient population by developing a patient-centered artificial\nintelligence (AI) solution using large language models (LLMs) and Fast\nHealthcare Interoperability Resources (FHIR) application programming interfaces\n(APIs). Materials and Methods: The research involved developing LLM on FHIR, an\nopen-source mobile application allowing users to interact with their health\nrecords using LLMs. The app is built on Stanford's Spezi ecosystem and uses\nOpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient\ndataset and evaluated by medical experts to assess the app's effectiveness in\nincreasing health literacy. The evaluation focused on the accuracy, relevance,\nand understandability of the LLM's responses to common patient questions.\nResults: LLM on FHIR demonstrated varying but generally high degrees of\naccuracy and relevance in providing understandable health information to\npatients. The app effectively translated medical data into patient-friendly\nlanguage and was able to adapt its responses to different patient profiles.\nHowever, challenges included variability in LLM responses and the need for\nprecise filtering of health data. Discussion and Conclusion: LLMs offer\nsignificant potential in improving health literacy and making health records\nmore accessible. LLM on FHIR, as a pioneering application in this field,\ndemonstrates the feasibility and challenges of integrating LLMs into patient\ncare. While promising, the implementation and pilot also highlight risks such\nas inconsistent responses and the importance of replicable output. Future\ndirections include better resource identification mechanisms and executing LLMs\non-device to enhance privacy and reduce costs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01711v1.pdf"
    },
    {
        "title": "Large Language Model Evaluation via Matrix Entropy",
        "authors": [
            "Lai Wei",
            "Zhiquan Tan",
            "Chenghai Li",
            "Jindong Wang",
            "Weiran Huang"
        ],
        "published": "2024-01-30T16:19:55Z",
        "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, extending their strong capabilities into multi-modal\ndomains. Thus, it is vital to define proper and diversified metrics for the\nevaluation of LLMs.\n  In this paper, we introduce matrix entropy, a novel metric rooted in\ninformation theory and geometry principles to quantify the data compression\nproficiency in LLMs. It reflects the model's ability to extract relevant\ninformation and eliminate unnecessary elements, thereby providing insight into\nthe language model's intrinsic capability. Specifically, we demonstrate its\napplicability in both single-modal (language) and multi-modal settings. For\nlanguage models, our findings reveal that the matrix entropy of representations\nfollows a scaling law type reduction when the model scales up, serving as a\ncomplement to the traditional loss scaling law. For the multi-modal setting, we\nalso propose an evaluation method based on matrix entropy for assessing\nalignment quality and we find that modern large multi-modal models exhibit\ngreat alignment performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.17139v1.pdf"
    },
    {
        "title": "CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM",
        "authors": [
            "Chengyue Yu",
            "Lei Zang",
            "Jiaotuan Wang",
            "Chenyi Zhuang",
            "Jinjie Gu"
        ],
        "published": "2024-01-07T15:00:36Z",
        "summary": "Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.",
        "pdf_link": "https://arxiv.org/pdf/2401.03512v3.pdf"
    },
    {
        "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
        "authors": [
            "Junjie Ye",
            "Yilong Wu",
            "Songyang Gao",
            "Caishuang Huang",
            "Sixian Li",
            "Guanyu Li",
            "Xiaoran Fan",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-01-16T12:45:15Z",
        "summary": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.",
        "pdf_link": "https://arxiv.org/pdf/2401.08326v2.pdf"
    },
    {
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
        "authors": [
            "Michael Ahn",
            "Debidatta Dwibedi",
            "Chelsea Finn",
            "Montse Gonzalez Arenas",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Brian Ichter",
            "Alex Irpan",
            "Nikhil Joshi",
            "Ryan Julian",
            "Sean Kirmani",
            "Isabel Leal",
            "Edward Lee",
            "Sergey Levine",
            "Yao Lu",
            "Isabel Leal",
            "Sharath Maddineni",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Pannag Sanketi",
            "Pierre Sermanet",
            "Quan Vuong",
            "Stefan Welker",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Steve Xu",
            "Zhuo Xu"
        ],
        "published": "2024-01-23T18:45:54Z",
        "summary": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
        "pdf_link": "https://arxiv.org/pdf/2401.12963v1.pdf"
    },
    {
        "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
        "authors": [
            "Fengbin Zhu",
            "Ziyang Liu",
            "Fuli Feng",
            "Chao Wang",
            "Moxin Li",
            "Tat-Seng Chua"
        ],
        "published": "2024-01-24T04:28:50Z",
        "summary": "In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.13223v2.pdf"
    },
    {
        "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
        "authors": [
            "Zhen Qin",
            "Weigao Sun",
            "Dong Li",
            "Xuyang Shen",
            "Weixuan Sun",
            "Yiran Zhong"
        ],
        "published": "2024-01-09T16:27:28Z",
        "summary": "Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.",
        "pdf_link": "https://arxiv.org/pdf/2401.04658v2.pdf"
    },
    {
        "title": "A Preliminary Study on Using Large Language Models in Software Pentesting",
        "authors": [
            "Kumar Shashwat",
            "Francis Hahn",
            "Xinming Ou",
            "Dmitry Goldgof",
            "Lawrence Hall",
            "Jay Ligatti",
            "S. Raj Rajgopalan",
            "Armin Ziaie Tabari"
        ],
        "published": "2024-01-30T21:42:59Z",
        "summary": "Large language models (LLM) are perceived to offer promising potentials for\nautomating security tasks, such as those found in security operation centers\n(SOCs). As a first step towards evaluating this perceived potential, we\ninvestigate the use of LLMs in software pentesting, where the main task is to\nautomatically identify software security vulnerabilities in source code. We\nhypothesize that an LLM-based AI agent can be improved over time for a specific\nsecurity task as human operators interact with it. Such improvement can be\nmade, as a first step, by engineering prompts fed to the LLM based on the\nresponses produced, to include relevant contexts and structures so that the\nmodel provides more accurate results. Such engineering efforts become\nsustainable if the prompts that are engineered to produce better results on\ncurrent tasks, also produce better results on future unknown tasks. To examine\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\n2,740 hand-crafted source code test cases containing various types of\nvulnerabilities. We divide the test cases into training and testing data, where\nwe engineer the prompts based on the training data (only), and evaluate the\nfinal system on the testing data. We compare the AI agent's performance on the\ntesting data against the performance of the agent without the prompt\nengineering. We also compare the AI agent's results against those from\nSonarQube, a widely used static code analyzer for security testing. We built\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\nboth chat completion and assistant APIs). The results show that using LLMs is a\nviable approach to build an AI agent for software pentesting that can improve\nthrough repeated use and prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.17459v1.pdf"
    },
    {
        "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",
        "authors": [
            "Saurav Pawar",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published": "2024-01-15T18:07:21Z",
        "summary": "The advent of Large Language Models (LLMs) represents a notable breakthrough\nin Natural Language Processing (NLP), contributing to substantial progress in\nboth text comprehension and generation. However, amidst these advancements, it\nis noteworthy that LLMs often face a limitation in terms of context length\nextrapolation. Understanding and extending the context length for LLMs is\ncrucial in enhancing their performance across various NLP applications. In this\nsurvey paper, we delve into the multifaceted aspects of exploring why it is\nessential, and the potential transformations that superior techniques could\nbring to NLP applications. We study the inherent challenges associated with\nextending context length and present an organized overview of the existing\nstrategies employed by researchers. Additionally, we discuss the intricacies of\nevaluating context extension techniques and highlight the open challenges that\nresearchers face in this domain. Furthermore, we explore whether there is a\nconsensus within the research community regarding evaluation standards and\nidentify areas where further agreement is needed. This comprehensive survey\naims to serve as a valuable resource for researchers, guiding them through the\nnuances of context length extension techniques and fostering discussions on\nfuture advancements in this evolving field.",
        "pdf_link": "https://arxiv.org/pdf/2401.07872v1.pdf"
    },
    {
        "title": "LLMs for Relational Reasoning: How Far are We?",
        "authors": [
            "Zhiming Li",
            "Yushi Cao",
            "Xiufeng Xu",
            "Junzhe Jiang",
            "Xu Liu",
            "Yon Shin Teo",
            "Shang-wei Lin",
            "Yang Liu"
        ],
        "published": "2024-01-17T08:22:52Z",
        "summary": "Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.",
        "pdf_link": "https://arxiv.org/pdf/2401.09042v1.pdf"
    },
    {
        "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
        "authors": [
            "Pau Rodriguez Inserte",
            "Mariam Nakhl\u00e9",
            "Raheel Qader",
            "Gaetan Caillaut",
            "Jingshu Liu"
        ],
        "published": "2024-01-26T11:04:01Z",
        "summary": "Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.",
        "pdf_link": "https://arxiv.org/pdf/2401.14777v1.pdf"
    },
    {
        "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
        "authors": [
            "Adib Hasan",
            "Ileana Rugina",
            "Alex Wang"
        ],
        "published": "2024-01-19T18:05:34Z",
        "summary": "Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type\nof attack that can coax these models into generating harmful and illegal\ncontent. In this paper, we show that pruning up to 20% of LLM parameters\nmarkedly increases their resistance to such attacks without additional training\nand without sacrificing their performance in standard benchmarks. Intriguingly,\nwe discovered that the enhanced safety observed post-pruning correlates to the\ninitial safety training level of the model, hinting that the effect of pruning\ncould be more general and may hold for other LLM behaviors beyond safety.\nAdditionally, we introduce a curated dataset of 225 harmful tasks across five\ncategories, inserted into ten different Jailbreaking prompts, showing that\npruning aids LLMs in concentrating attention on task-relevant tokens in\njailbreaking prompts. Lastly, our experiments reveal that the prominent chat\nmodels, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high\nsusceptibility to jailbreaking attacks, with some categories achieving nearly\n70-100% success rate. These insights underline the potential of pruning as a\ngeneralizable approach for improving LLM safety, reliability, and potentially\nother desired behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2401.10862v1.pdf"
    },
    {
        "title": "FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference",
        "authors": [
            "Zirui Liu",
            "Qingquan Song",
            "Qiang Charles Xiao",
            "Sathiya Keerthi Selvaraj",
            "Rahul Mazumder",
            "Aman Gupta",
            "Xia Hu"
        ],
        "published": "2024-01-08T17:29:16Z",
        "summary": "The large number of parameters in Pretrained Language Models enhance their\nperformance, but also make them resource-intensive, making it challenging to\ndeploy them on commodity hardware like a single GPU. Due to the memory and\npower limitations of these devices, model compression techniques are often used\nto decrease both the model's size and its inference latency. This usually\nresults in a trade-off between model accuracy and efficiency. Therefore,\noptimizing this balance is essential for effectively deploying LLMs on\ncommodity hardware. A significant portion of the efficiency challenge is the\nFeed-forward network (FFN) component, which accounts for roughly $\\frac{2}{3}$\ntotal parameters and inference latency. In this paper, we first observe that\nonly a few neurons of FFN module have large output norm for any input tokens,\na.k.a. heavy hitters, while the others are sparsely triggered by different\ntokens. Based on this observation, we explicitly split the FFN into two parts\naccording to the heavy hitters. We improve the efficiency-accuracy trade-off of\nexisting compression methods by allocating more resource to FFN parts with\nheavy hitters. In practice, our method can reduce model size by 43.1\\% and\nbring $1.25\\sim1.56\\times$ wall clock time speedup on different hardware with\nnegligible accuracy drop.",
        "pdf_link": "https://arxiv.org/pdf/2401.04044v1.pdf"
    },
    {
        "title": "IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification",
        "authors": [
            "Abdullah Alsuhaibani",
            "Hamad Zogan",
            "Imran Razzak",
            "Shoaib Jameel",
            "Guandong Xu"
        ],
        "published": "2024-01-08T17:07:37Z",
        "summary": "Language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have been very effective in various Natural Language\nProcessing (NLP) and text mining tasks including text classification. However,\nsome tasks still pose challenges for these models, including text\nclassification with limited labels. This can result in a cold-start problem.\nAlthough some approaches have attempted to address this problem through\nsingle-stage clustering as an intermediate training step coupled with a\npre-trained language model, which generates pseudo-labels to improve\nclassification, these methods are often error-prone due to the limitations of\nthe clustering algorithms. To overcome this, we have developed a novel\ntwo-stage intermediate clustering with subsequent fine-tuning that models the\npseudo-labels reliably, resulting in reduced prediction errors. The key novelty\nin our model, IDoFew, is that the two-stage clustering coupled with two\ndifferent clustering algorithms helps exploit the advantages of the\ncomplementary algorithms that reduce the errors in generating reliable\npseudo-labels for fine-tuning. Our approach has shown significant improvements\ncompared to strong comparative models.",
        "pdf_link": "https://arxiv.org/pdf/2401.04025v1.pdf"
    },
    {
        "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
        "authors": [
            "Pengying Wu",
            "Yao Mu",
            "Bingxian Wu",
            "Yi Hou",
            "Ji Ma",
            "Shanghang Zhang",
            "Chang Liu"
        ],
        "published": "2024-01-05T08:05:07Z",
        "summary": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)\ntask empowers agents to adeptly traverse unfamiliar environments and locate\nobjects from novel categories without prior explicit training. This paper\nintroduces VoroNav, a novel semantic exploration framework that proposes the\nReduced Voronoi Graph to extract exploratory paths and planning nodes from a\nsemantic map constructed in real time. By harnessing topological and semantic\ninformation, VoroNav designs text-based descriptions of paths and images that\nare readily interpretable by a large language model (LLM). In particular, our\napproach presents a synergy of path and farsight descriptions to represent the\nenvironmental context, enabling LLM to apply commonsense reasoning to ascertain\nwaypoints for navigation. Extensive evaluation on HM3D and HSSD validates\nVoroNav surpasses existing benchmarks in both success rate and exploration\nefficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6%\nSuccess and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate\nobstacle avoidance proficiency and perceptual efficiency further corroborate\nthe enhancements achieved by our method in ZSON planning. Project page:\nhttps://voro-nav.github.io",
        "pdf_link": "https://arxiv.org/pdf/2401.02695v2.pdf"
    },
    {
        "title": "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models",
        "authors": [
            "Divyanshu Aggarwal",
            "Ashutosh Sathe",
            "Ishaan Watts",
            "Sunayana Sitaram"
        ],
        "published": "2024-01-15T11:06:43Z",
        "summary": "Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for\nimproving the performance of Large Language Models (LLMs) without requiring\nmassive resources and compute. Prior work on multilingual evaluation has shown\nthat there is a large gap between the performance of LLMs on English and other\nlanguages. Further, there is also a large gap between the performance of\nsmaller open-source models and larger LLMs. Finetuning can be an effective way\nto bridge this gap and make language models more equitable. In this work, we\nfinetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual\ninstruction tuning datasets to determine its effect on model performance on six\ndownstream tasks covering forty languages in all. Additionally, we experiment\nwith various parameters, such as rank for low-rank adaptation and values of\nquantisation to determine their effects on downstream performance and find that\nhigher rank and higher quantisation values benefit low-resource languages. We\nfind that PEFT of smaller open-source models sometimes bridges the gap between\nthe performance of these models and the larger ones, however, English\nperformance can take a hit. We also find that finetuning sometimes improves\nperformance on low-resource languages, while degrading performance on\nhigh-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2401.07598v2.pdf"
    },
    {
        "title": "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting",
        "authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Naoaki Okazaki",
            "Timothy Baldwin"
        ],
        "published": "2024-01-28T06:50:10Z",
        "summary": "There exist both scalable tasks, like reading comprehension and\nfact-checking, where model performance improves with model size, and unscalable\ntasks, like arithmetic reasoning and symbolic reasoning, where model\nperformance does not necessarily improve with model size. Large language models\n(LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate\nincremental predictions even on unscalable tasks. Unfortunately, despite their\nexceptional reasoning abilities, LLMs tend to internalize and reproduce\ndiscriminatory societal biases. Whether CoT can provide discriminatory or\negalitarian rationalizations for the implicit information in unscalable tasks\nremains an open question.\n  In this study, we examine the impact of LLMs' step-by-step predictions on\ngender bias in unscalable tasks. For this purpose, we construct a benchmark for\nan unscalable task where the LLM is given a list of words comprising feminine,\nmasculine, and gendered occupational words, and is required to count the number\nof feminine and masculine words. In our CoT prompts, we require the LLM to\nexplicitly indicate whether each word in the word list is a feminine or\nmasculine before making the final predictions. With counting and handling the\nmeaning of words, this benchmark has characteristics of both arithmetic\nreasoning and symbolic reasoning. Experimental results in English show that\nwithout step-by-step prediction, most LLMs make socially biased predictions,\ndespite the task being as simple as counting words. Interestingly, CoT\nprompting reduces this unconscious social bias in LLMs and encourages fair\npredictions.",
        "pdf_link": "https://arxiv.org/pdf/2401.15585v1.pdf"
    },
    {
        "title": "LLMs for Test Input Generation for Semantic Caches",
        "authors": [
            "Zafaryab Rasool",
            "Scott Barnett",
            "David Willie",
            "Stefanus Kurniawan",
            "Sherwin Balugo",
            "Srikanth Thudumu",
            "Mohamed Abdelrazek"
        ],
        "published": "2024-01-16T06:16:33Z",
        "summary": "Large language models (LLMs) enable state-of-the-art semantic capabilities to\nbe added to software systems such as semantic search of unstructured documents\nand text generation. However, these models are computationally expensive. At\nscale, the cost of serving thousands of users increases massively affecting\nalso user experience. To address this problem, semantic caches are used to\ncheck for answers to similar queries (that may have been phrased differently)\nwithout hitting the LLM service. Due to the nature of these semantic cache\ntechniques that rely on query embeddings, there is a high chance of errors\nimpacting user confidence in the system. Adopting semantic cache techniques\nusually requires testing the effectiveness of a semantic cache (accurate cache\nhits and misses) which requires a labelled test set of similar queries and\nresponses which is often unavailable. In this paper, we present VaryGen, an\napproach for using LLMs for test input generation that produces similar\nquestions from unstructured text documents. Our novel approach uses the\nreasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise\nsubtle variations to queries, and 3) evaluate the synthesised test dataset. We\nevaluated our approach in the domain of a student question and answer system by\nqualitatively analysing 100 generated queries and result pairs, and conducting\nan empirical case study with an open source semantic cache. Our results show\nthat query pairs satisfy human expectations of similarity and our generated\ndata demonstrates failure cases of a semantic cache. Additionally, we also\nevaluate our approach on Qasper dataset. This work is an important first step\ninto test input generation for semantic applications and presents\nconsiderations for practitioners when calibrating a semantic cache.",
        "pdf_link": "https://arxiv.org/pdf/2401.08138v1.pdf"
    },
    {
        "title": "Knowledge Distillation for Closed-Source Language Models",
        "authors": [
            "Hongzhan Chen",
            "Xiaojun Quan",
            "Hehong Chen",
            "Ming Yan",
            "Ji Zhang"
        ],
        "published": "2024-01-13T08:43:32Z",
        "summary": "Closed-source language models such as GPT-4 have achieved remarkable\nperformance. Many recent studies focus on enhancing the capabilities of smaller\nmodels through knowledge distillation from closed-source language models.\nHowever, due to the incapability to directly access the weights, hidden states,\nand output distributions of these closed-source models, the distillation can\nonly be performed by fine-tuning smaller models with data samples generated by\nclosed-source language models, which constrains the effectiveness of knowledge\ndistillation. In this paper, we propose to estimate the output distributions of\nclosed-source language models within a Bayesian estimation framework, involving\nboth prior and posterior estimation. The prior estimation aims to derive a\nprior distribution by utilizing the corpus generated by closed-source language\nmodels, while the posterior estimation employs a proxy model to update the\nprior distribution and derive a posterior distribution. By leveraging the\nestimated output distribution of closed-source language models, traditional\nknowledge distillation can be executed. Experimental results demonstrate that\nour method surpasses the performance of current models directly fine-tuned on\ndata generated by closed-source language models.",
        "pdf_link": "https://arxiv.org/pdf/2401.07013v1.pdf"
    },
    {
        "title": "DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure",
        "authors": [
            "Junyi Ye",
            "Mengnan Du",
            "Guiling Wang"
        ],
        "published": "2024-01-27T17:06:53Z",
        "summary": "This paper introduces DataFrame question answering (QA), a novel task that\nutilizes large language models (LLMs) to generate Pandas queries for\ninformation retrieval and data analysis on dataframes, emphasizing safe and\nnon-revealing data handling. Our method, which solely relies on dataframe\ncolumn names, not only ensures data privacy but also significantly reduces the\ncontext window in the prompt, streamlining information processing and\naddressing major challenges in LLM-based data analysis. We propose DataFrame QA\nas a comprehensive framework that includes safe Pandas query generation and\ncode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1\nmetric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA',\ntailored for complex data analysis queries. Our findings indicate that GPT-4\nachieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,\nunderscoring its capability in securely retrieving and aggregating dataframe\nvalues and conducting sophisticated data analyses. This approach, deployable in\na zero-shot manner without prior training or adjustments, proves to be highly\nadaptable and secure for diverse applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.15463v1.pdf"
    },
    {
        "title": "GRATH: Gradual Self-Truthifying for Large Language Models",
        "authors": [
            "Weixin Chen",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-01-22T19:00:08Z",
        "summary": "Truthfulness is paramount for large language models (LLMs) as they are\nincreasingly deployed in real-world applications. However, existing LLMs still\nstruggle with generating truthful content, as evidenced by their modest\nperformance on benchmarks like TruthfulQA. To address this issue, we propose\nGRAdual self-truTHifying (GRATH), a novel post-processing method to enhance\ntruthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate\npairwise truthfulness training data with each pair containing a question and\nits correct and incorrect answers, and then optimizes the model via direct\npreference optimization (DPO) to learn from the truthfulness difference between\nanswer pairs. GRATH iteratively refines truthfulness data and updates the\nmodel, leading to a gradual improvement in model truthfulness in a\nself-supervised manner. Empirically, we evaluate GRATH using different 7B-LLMs\nand compare with LLMs with similar or even larger sizes on benchmark datasets.\nOur results show that GRATH effectively improves LLMs' truthfulness without\ncompromising other core capabilities. Notably, GRATH achieves state-of-the-art\nperformance on TruthfulQA, with MC1 accuracy of 54.71% and MC2 accuracy of\n69.10%, which even surpass those on 70B-LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.12292v2.pdf"
    },
    {
        "title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment",
        "authors": [
            "Keming Lu",
            "Bowen Yu",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2024-01-23T03:56:22Z",
        "summary": "Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.",
        "pdf_link": "https://arxiv.org/pdf/2401.12474v1.pdf"
    },
    {
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
        "authors": [
            "Pratyush Maini",
            "Skyler Seto",
            "He Bai",
            "David Grangier",
            "Yizhe Zhang",
            "Navdeep Jaitly"
        ],
        "published": "2024-01-29T18:19:08Z",
        "summary": "Large language models are trained on massive scrapes of the web, which are\noften unstructured, noisy, and poorly phrased. Current scaling laws show that\nlearning from such data requires an abundance of both compute and data, which\ngrows with the size of the model being trained. This is infeasible both because\nof the large compute costs and duration associated with pre-training, and the\nimpending scarcity of high-quality data on the web. In this work, we propose\nWeb Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an\noff-the-shelf instruction-tuned model prompted to paraphrase documents on the\nweb in specific styles such as \"like Wikipedia\" or in \"question-answer format\"\nto jointly pre-train LLMs on real and synthetic rephrases. First, we show that\nusing WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training\nby $\\sim3x$. At the same pre-training compute budget, it improves perplexity by\nmore than 10% on average across different subsets of the Pile, and improves\nzero-shot question answer accuracy across 13 tasks by more than 2%. Second, we\ninvestigate the impact of the re-phrasing style on the performance of the\nmodel, offering insights into how the composition of the training data can\nimpact the performance of LLMs in OOD settings. Our gains are attributed to the\nfact that re-phrased synthetic data has higher utility than just real data\nbecause it (i) incorporates style diversity that closely reflects downstream\nevaluation style, and (ii) has higher 'quality' than web-scraped data.",
        "pdf_link": "https://arxiv.org/pdf/2401.16380v1.pdf"
    },
    {
        "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
        "authors": [
            "Shuai Zhao",
            "Meihuizi Jia",
            "Luu Anh Tuan",
            "Fengjun Pan",
            "Jinming Wen"
        ],
        "published": "2024-01-11T14:38:19Z",
        "summary": "In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2401.05949v4.pdf"
    },
    {
        "title": "AugSumm: towards generalizable speech summarization using synthetic labels from large language model",
        "authors": [
            "Jee-weon Jung",
            "Roshan Sharma",
            "William Chen",
            "Bhiksha Raj",
            "Shinji Watanabe"
        ],
        "published": "2024-01-10T18:39:46Z",
        "summary": "Abstractive speech summarization (SSUM) aims to generate human-like summaries\nfrom speech. Given variations in information captured and phrasing, recordings\ncan be summarized in multiple ways. Therefore, it is more reasonable to\nconsider a probabilistic distribution of all potential summaries rather than a\nsingle summary. However, conventional SSUM models are mostly trained and\nevaluated with a single ground-truth (GT) human-annotated deterministic summary\nfor every recording. Generating multiple human references would be ideal to\nbetter represent the distribution statistically, but is impractical because\nannotation is expensive. We tackle this challenge by proposing AugSumm, a\nmethod to leverage large language models (LLMs) as a proxy for human annotators\nto generate augmented summaries for training and evaluation. First, we explore\nprompting strategies to generate synthetic summaries from ChatGPT. We validate\nthe quality of synthetic summaries using multiple metrics including human\nevaluation, where we find that summaries generated using AugSumm are perceived\nas more valid to humans. Second, we develop methods to utilize synthetic\nsummaries in training and evaluation. Experiments on How2 demonstrate that\npre-training on synthetic summaries and fine-tuning on GT summaries improves\nROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries\nare available at https://github.com/Jungjee/AugSumm.",
        "pdf_link": "https://arxiv.org/pdf/2401.06806v1.pdf"
    },
    {
        "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Yizhou Yu",
            "Liansheng Wang"
        ],
        "published": "2024-01-22T18:59:07Z",
        "summary": "Parameter-efficient fine-tuning (PEFT) that was initially developed for\nexploiting pre-trained large language models has recently emerged as an\neffective approach to perform transfer learning on computer vision tasks.\nHowever, the effectiveness of PEFT on medical vision foundation models is still\nunclear and remains to be explored. As a proof of concept, we conducted a\ndetailed empirical study on applying PEFT to chest radiography foundation\nmodels. Specifically, we delved into LoRA, a representative PEFT method, and\ncompared it against full-parameter fine-tuning (FFT) on two self-supervised\nradiography foundation models across three well-established chest radiograph\ndatasets. Our results showed that LoRA outperformed FFT in 13 out of 18\ntransfer learning tasks by at most 2.9% using fewer than 1% tunable parameters.\nCombining LoRA with foundation models, we set up new state-of-the-art on a\nrange of data-efficient learning tasks, such as an AUROC score of 80.6% using\n1% labeled data on NIH ChestX-ray14. We hope this study can evoke more\nattention from the community in the use of PEFT for transfer learning on\nmedical imaging tasks. Code and models are available at\nhttps://github.com/RL4M/MED-PEFT.",
        "pdf_link": "https://arxiv.org/pdf/2401.12215v1.pdf"
    },
    {
        "title": "WARM: On the Benefits of Weight Averaged Reward Models",
        "authors": [
            "Alexandre Ram\u00e9",
            "Nino Vieillard",
            "L\u00e9onard Hussenot",
            "Robert Dadashi",
            "Geoffrey Cideron",
            "Olivier Bachem",
            "Johan Ferret"
        ],
        "published": "2024-01-22T18:27:08Z",
        "summary": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.",
        "pdf_link": "https://arxiv.org/pdf/2401.12187v1.pdf"
    },
    {
        "title": "A Fast, Performant, Secure Distributed Training Framework For Large Language Model",
        "authors": [
            "Wei Huang",
            "Yinggui Wang",
            "Anda Cheng",
            "Aihui Zhou",
            "Chaofan Yu",
            "Lei Wang"
        ],
        "published": "2024-01-18T08:33:09Z",
        "summary": "The distributed (federated) LLM is an important method for co-training the\ndomain-specific LLM using siloed data. However, maliciously stealing model\nparameters and data from the server or client side has become an urgent problem\nto be solved. In this paper, we propose a secure distributed LLM based on model\nslicing. In this case, we deploy the Trusted Execution Environment (TEE) on\nboth the client and server side, and put the fine-tuned structure (LoRA or\nembedding of P-tuning v2) into the TEE. Then, secure communication is executed\nin the TEE and general environments through lightweight encryption. In order to\nfurther reduce the equipment cost as well as increase the model performance and\naccuracy, we propose a split fine-tuning scheme. In particular, we split the\nLLM by layers and place the latter layers in a server-side TEE (the client does\nnot need a TEE). We then combine the proposed Sparsification Parameter\nFine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream\ntask. Numerous experiments have shown that our method guarantees accuracy while\nmaintaining security.",
        "pdf_link": "https://arxiv.org/pdf/2401.09796v2.pdf"
    },
    {
        "title": "Transfer Learning for Text Diffusion Models",
        "authors": [
            "Kehang Han",
            "Kathleen Kenealy",
            "Aditya Barua",
            "Noah Fiedel",
            "Noah Constant"
        ],
        "published": "2024-01-30T17:11:56Z",
        "summary": "In this report, we explore the potential for text diffusion to replace\nautoregressive (AR) decoding for the training and deployment of large language\nmodels (LLMs). We are particularly interested to see whether pretrained AR\nmodels can be transformed into text diffusion models through a lightweight\nadaptation procedure we call ``AR2Diff''. We begin by establishing a strong\nbaseline setup for training text diffusion models. Comparing across multiple\narchitectures and pretraining objectives, we find that training a decoder-only\nmodel with a prefix LM objective is best or near-best across several tasks.\nBuilding on this finding, we test various transfer learning setups for text\ndiffusion models. On machine translation, we find that text diffusion\nunderperforms the standard AR approach. However, on code synthesis and\nextractive QA, we find diffusion models trained from scratch outperform AR\nmodels in many cases. We also observe quality gains from AR2Diff -- adapting AR\nmodels to use diffusion decoding. These results are promising given that text\ndiffusion is relatively underexplored and can be significantly faster than AR\ndecoding for long text generation.",
        "pdf_link": "https://arxiv.org/pdf/2401.17181v1.pdf"
    },
    {
        "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
        "authors": [
            "Xueyu Hu",
            "Ziyu Zhao",
            "Shuang Wei",
            "Ziwei Chai",
            "Qianli Ma",
            "Guoyin Wang",
            "Xuwu Wang",
            "Jing Su",
            "Jingjing Xu",
            "Ming Zhu",
            "Yao Cheng",
            "Jianbo Yuan",
            "Jiwei Li",
            "Kun Kuang",
            "Yang Yang",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "published": "2024-01-10T19:04:00Z",
        "summary": "In this paper, we introduce InfiAgent-DABench, the first benchmark\nspecifically designed to evaluate LLM-based agents on data analysis tasks.\nThese tasks require agents to end-to-end solving complex tasks by interacting\nwith an execution environment. This benchmark contains DAEval, a dataset\nconsisting of 257 data analysis questions derived from 52 CSV files, and an\nagent framework which incorporates LLMs to serve as data analysis agents for\nboth serving and evaluation. Since data analysis questions are often open-ended\nand hard to evaluate without human supervision, we adopt a format-prompting\ntechnique to convert each question into a closed-form format so that they can\nbe automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the\ncurrent challenges encountered in data analysis tasks. In addition, building on\ntop of our agent framework, we develop a specialized agent, DAAgent, which\nsurpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for\nInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent .",
        "pdf_link": "https://arxiv.org/pdf/2401.05507v3.pdf"
    },
    {
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Wenyu Huang",
            "Yang Deng",
            "Rui Wang",
            "Zezhong Wang",
            "Yufei Wang",
            "Fei Mi",
            "Jeff Z. Pan",
            "Kam-Fai Wong"
        ],
        "published": "2024-01-24T06:50:20Z",
        "summary": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.13256v1.pdf"
    },
    {
        "title": "Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning",
        "authors": [
            "Yuelyu Ji",
            "Zeshui Yu",
            "Yanshan Wang"
        ],
        "published": "2024-01-31T05:11:00Z",
        "summary": "In this study, we aim to address the task of assertion detection when\nextracting medical concepts from clinical notes, a key process in clinical\nnatural language processing (NLP). Assertion detection in clinical NLP usually\ninvolves identifying assertion types for medical concepts in the clinical text,\nnamely certainty (whether the medical concept is positive, negated, possible,\nor hypothetical), temporality (whether the medical concept is for present or\nthe past history), and experiencer (whether the medical concept is described\nfor the patient or a family member). These assertion types are essential for\nhealthcare professionals to quickly and clearly understand the context of\nmedical conditions from unstructured clinical texts, directly influencing the\nquality and outcomes of patient care. Although widely used, traditional\nmethods, particularly rule-based NLP systems and machine learning or deep\nlearning models, demand intensive manual efforts to create patterns and tend to\noverlook less common assertion types, leading to an incomplete understanding of\nthe context. To address this challenge, our research introduces a novel\nmethodology that utilizes Large Language Models (LLMs) pre-trained on a vast\narray of medical data for assertion detection. We enhanced the current method\nwith advanced reasoning techniques, including Tree of Thought (ToT), Chain of\nThought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank\nAdaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010\nassertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11\nimprovements over the previous works. To further assess the generalizability of\nour approach, we extended our evaluation to a local dataset that focused on\nsleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31\nhigher than the previous method.",
        "pdf_link": "https://arxiv.org/pdf/2401.17602v1.pdf"
    },
    {
        "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs",
        "authors": [
            "Shaoxiang Chen",
            "Zequn Jie",
            "Lin Ma"
        ],
        "published": "2024-01-29T13:48:36Z",
        "summary": "Instruction finetuning on a variety of image-text instruction data is the key\nto obtaining a versatile Multimodal Large Language Model (MLLM), and different\nconfigurations of the instruction data can lead to finetuned models with\ndifferent capabilities. However, we have discovered that data conflicts are\ninevitable when mixing instruction data from distinct domains, which can result\nin performance drops for tasks of a specific domain. To address this issue, we\npropose to apply an efficient Mixture of Experts (MoE) design, which is a\nsparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Within\nthe Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method\nby creating a set of LoRA experts specifically for the MLP layer, and route\neach token to the top-1 expert based on a routing function, allowing adaptive\nchoices for tokens from different domains. Since the LoRA experts are sparsely\nactivated, the training and inference cost are kept roughly constant compared\nto the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with our\nMoE design, our final model is named LLaVA-MoLE. Extensive experiments proved\nthat LLaVA-MoLE effectively mitigates the data conflict issue when mixing\nmultiple distinct instruction datasets with various configurations, and\nachieves consistent performance gains over the strong plain-LoRA baselines.\nMost importantly, on the mixed datasets, LLaVA-MoLE can even outperform the\nplain-LoRA baseline trained with twice the samples.",
        "pdf_link": "https://arxiv.org/pdf/2401.16160v2.pdf"
    },
    {
        "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
        "authors": [
            "Liang Zhang",
            "Katherine Jijo",
            "Spurthi Setty",
            "Eden Chung",
            "Fatima Javid",
            "Natan Vidra",
            "Tommy Clifford"
        ],
        "published": "2024-01-27T00:18:07Z",
        "summary": "Large Language Models (LLMs) generate responses to questions; however, their\neffectiveness is often hindered by sub-optimal quality of answers and\noccasional failures to provide accurate responses to questions. To address\nthese challenges, a fine-tuning process is employed, involving feedback and\nexamples to refine models. The objective is to enhance AI models through\ncontinuous feedback loops, utilizing metrics such as cosine similarity, LLM\nevaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like\nGPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on\nfinancial datasets, including the FinanceBench and RAG Instruct Benchmark\nTester Dataset, illustrating the necessity of fine-tuning. The results showcase\nthe capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,\nproviding superior question and answering capabilities. Notably, the\ncombination of fine-tuning the LLM with a process known as Retrieval Augmented\nGeneration (RAG) proves to generate responses with improved accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.01722v1.pdf"
    },
    {
        "title": "Cheetah: Natural Language Generation for 517 African Languages",
        "authors": [
            "Ife Adebara",
            "AbdelRahim Elmadany",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2024-01-02T06:24:13Z",
        "summary": "Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across six generation downstream\ntasks. In five of the six tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.",
        "pdf_link": "https://arxiv.org/pdf/2401.01053v3.pdf"
    },
    {
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "authors": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Yuan Li",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zhengliang Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "Bertie Vidgen",
            "Bhavya Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chunyuan Li",
            "Eric Xing",
            "Furong Huang",
            "Hao Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "Manolis Kellis",
            "Marinka Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "Suman Jana",
            "Tianlong Chen",
            "Tianming Liu",
            "Tianyi Zhou",
            "William Wang",
            "Xiang Li",
            "Xiangliang Zhang",
            "Xiao Wang",
            "Xing Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yinzhi Cao",
            "Yong Chen",
            "Yue Zhao"
        ],
        "published": "2024-01-10T22:07:21Z",
        "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2401.05561v4.pdf"
    },
    {
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "authors": [
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Bozhong Tian",
            "Peng Wang",
            "Shumin Deng",
            "Mengru Wang",
            "Zekun Xi",
            "Shengyu Mao",
            "Jintian Zhang",
            "Yuansheng Ni",
            "Siyuan Cheng",
            "Ziwen Xu",
            "Xin Xu",
            "Jia-Chen Gu",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Lei Liang",
            "Zhiqiang Zhang",
            "Xiaowei Zhu",
            "Jun Zhou",
            "Huajun Chen"
        ],
        "published": "2024-01-02T16:54:58Z",
        "summary": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.",
        "pdf_link": "https://arxiv.org/pdf/2401.01286v4.pdf"
    },
    {
        "title": "Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers",
        "authors": [
            "Lei Xu",
            "Sarah Alnegheimish",
            "Laure Berti-Equille",
            "Alfredo Cuesta-Infante",
            "Kalyan Veeramachaneni"
        ],
        "published": "2024-01-30T17:30:44Z",
        "summary": "In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric \\r{ho} to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2401.17196v1.pdf"
    },
    {
        "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
        "authors": [
            "Dawei Li",
            "Zhen Tan",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "published": "2024-01-28T08:56:49Z",
        "summary": "While textual information significantly enhances the performance of\npre-trained language models (PLMs) in knowledge graph completion (KGC), the\nstatic and noisy nature of existing corpora collected from Wikipedia articles\nor synsets definitions often limits the potential of PLM-based KGC models. To\nsurmount these challenges, we introduce the Contextualization Distillation\nstrategy, a versatile plug-in-and-play approach compatible with both\ndiscriminative and generative KGC frameworks. Our method begins by instructing\nlarge language models (LLMs) to transform compact, structural triplets into\ncontext-rich segments. Subsequently, we introduce two tailored auxiliary tasks,\nreconstruction and contextualization, allowing smaller KGC models to assimilate\ninsights from these enriched triplets. Comprehensive evaluations across diverse\ndatasets and KGC techniques highlight the efficacy and adaptability of our\napproach, revealing consistent performance enhancements irrespective of\nunderlying pipelines or architectures. Moreover, our analysis makes our method\nmore explainable and provides insight into generating path selection, as well\nas the choosing of suitable distillation tasks. All the code and data in this\nwork will be released at\nhttps://github.com/David-Li0406/Contextulization-Distillation",
        "pdf_link": "https://arxiv.org/pdf/2402.01729v3.pdf"
    },
    {
        "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering",
        "authors": [
            "Tianlong Li",
            "Shihan Dou",
            "Wenhao Liu",
            "Muling Wu",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2024-01-12T00:50:04Z",
        "summary": "Jailbreaking techniques aim to probe the boundaries of safety in large\nlanguage models (LLMs) by inducing them to generate toxic responses to\nmalicious queries, a significant concern within the LLM community. While\nexisting jailbreaking methods primarily rely on prompt engineering, altering\ninputs to evade LLM safety mechanisms, they suffer from low attack success\nrates and significant time overheads, rendering them inflexible. To overcome\nthese limitations, we propose a novel jailbreaking approach, named Jailbreaking\nLLMs through Representation Engineering (JRE). Our method requires only a small\nnumber of query pairs to extract ``safety patterns'' that can be used to\ncircumvent the target model's defenses, achieving unprecedented jailbreaking\nperformance. Building upon these findings, we also introduce a novel defense\nframework inspired by JRE principles, which demonstrates notable effectiveness.\nExtensive experimentation confirms the superior performance of the JRE attacks\nand the robustness of the JRE defense framework. We hope this study contributes\nto advancing the understanding of model safety issues through the lens of\nrepresentation engineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.06824v2.pdf"
    },
    {
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning",
        "authors": [
            "Wenhan Xia",
            "Chengwei Qin",
            "Elad Hazan"
        ],
        "published": "2024-01-08T14:26:49Z",
        "summary": "Fine-tuning is the primary methodology for tailoring pre-trained large\nlanguage models to specific tasks. As the model's scale and the diversity of\ntasks expand, parameter-efficient fine-tuning methods are of paramount\nimportance. One of the most widely used family of methods is low-rank\nadaptation (LoRA) and its variants. LoRA encodes weight update as the product\nof two low-rank matrices. Despite its advantages, LoRA falls short of\nfull-parameter fine-tuning in terms of generalization error for certain tasks.\n  We introduce Chain of LoRA (COLA), an iterative optimization framework\ninspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full\nparameter fine-tuning, without incurring additional computational costs or\nmemory overheads. COLA employs a residual learning procedure where it merges\nlearned LoRA modules into the pre-trained language model parameters and\nre-initilize optimization for new born LoRA modules. We provide theoretical\nconvergence guarantees as well as empirical results to validate the\neffectiveness of our algorithm. Across various models (OPT and llama-2) and\nseven benchmarking tasks, we demonstrate that COLA can consistently outperform\nLoRA without additional computational or memory costs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04151v1.pdf"
    },
    {
        "title": "A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models",
        "authors": [
            "Rui Ma",
            "Qiang Zhou",
            "Bangjun Xiao",
            "Yizhu Jin",
            "Daquan Zhou",
            "Xiuyu Li",
            "Aishani Singh",
            "Yi Qu",
            "Kurt Keutzer",
            "Xiaodong Xie",
            "Jingtong Hu",
            "Zhen Dong",
            "Shanghang Zhang"
        ],
        "published": "2024-01-04T11:14:01Z",
        "summary": "Copyright is a legal right that grants creators the exclusive authority to\nreproduce, distribute, and profit from their creative works. However, the\nrecent advancements in text-to-image generation techniques have posed\nsignificant challenges to copyright protection, as these methods have\nfacilitated the learning of unauthorized content, artistic creations, and\nportraits, which are subsequently utilized to generate and disseminate\nuncontrolled content. Especially, the use of stable diffusion, an emerging\nmodel for text-to-image generation, poses an increased risk of unauthorized\ncopyright infringement and distribution. Currently, there is a lack of\nsystematic studies evaluating the potential correlation between content\ngenerated by stable diffusion and those under copyright protection. Conducting\nsuch studies faces several challenges, including i) the intrinsic ambiguity\nrelated to copyright infringement in text-to-image models, ii) the absence of a\ncomprehensive large-scale dataset, and iii) the lack of standardized metrics\nfor defining copyright infringement. This work provides the first large-scale\nstandardized dataset and benchmark on copyright protection. Specifically, we\npropose a pipeline to coordinate CLIP, ChatGPT, and diffusion models to\ngenerate a dataset that contains anchor images, corresponding prompts, and\nimages generated by text-to-image models, reflecting the potential abuses of\ncopyright. Furthermore, we explore a suite of evaluation metrics to judge the\neffectiveness of copyright protection methods. The proposed dataset, benchmark\nlibrary, and evaluation metrics will be open-sourced to facilitate future\nresearch and application. The website and dataset can be accessed website\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.12052v2.pdf"
    },
    {
        "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
        "authors": [
            "Rafael Rivera Soto",
            "Kailin Koch",
            "Aleem Khan",
            "Barry Chen",
            "Marcus Bishop",
            "Nicholas Andrews"
        ],
        "published": "2024-01-12T17:26:51Z",
        "summary": "The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.",
        "pdf_link": "https://arxiv.org/pdf/2401.06712v2.pdf"
    },
    {
        "title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education",
        "authors": [
            "Wei Hung Pan",
            "Ming Jie Chok",
            "Jonathan Leong Shan Wong",
            "Yung Xin Shin",
            "Yeong Shian Poon",
            "Zhou Yang",
            "Chun Yong Chong",
            "David Lo",
            "Mei Kuan Lim"
        ],
        "published": "2024-01-08T05:53:52Z",
        "summary": "Educators are increasingly concerned about the usage of Large Language Models\n(LLMs) such as ChatGPT in programming education, particularly regarding the\npotential exploitation of imperfections in Artificial Intelligence Generated\nContent (AIGC) Detectors for academic misconduct. In this paper, we present an\nempirical study where the LLM is examined for its attempts to bypass detection\nby AIGC Detectors. This is achieved by generating code in response to a given\nquestion using different variants. We collected a dataset comprising 5,069\nsamples, with each sample consisting of a textual description of a coding\nproblem and its corresponding human-written Python solution codes. These\nsamples were obtained from various sources, including 80 from Quescol, 3,264\nfrom Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of\ncode problem variant prompts, which were used to instruct ChatGPT to generate\nthe outputs. Subsequently, we assessed the performance of five AIGC detectors.\nOur results demonstrate that existing AIGC Detectors perform poorly in\ndistinguishing between human-written code and AI-generated code.",
        "pdf_link": "https://arxiv.org/pdf/2401.03676v1.pdf"
    },
    {
        "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
        "authors": [
            "Mandar Kulkarni",
            "Praveen Tangarajan",
            "Kyung Kim",
            "Anusua Trivedi"
        ],
        "published": "2024-01-10T02:57:20Z",
        "summary": "With the advent of Large Language Models (LLM), conversational assistants\nhave become prevalent for domain use cases. LLMs acquire the ability to\ncontextual question answering through training, and Retrieval Augmented\nGeneration (RAG) further enables the bot to answer domain-specific questions.\nThis paper describes a RAG-based approach for building a chatbot that answers\nuser's queries using Frequently Asked Questions (FAQ) data. We train an\nin-house retrieval embedding model using infoNCE loss, and experimental results\ndemonstrate that the in-house model works significantly better than the\nwell-known general-purpose public embedding model, both in terms of retrieval\naccuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open\nAPI-based paid ChatGPT model. We noticed that a previously retrieved-context\ncould be used to generate an answer for specific patterns/sequences of queries\n(e.g., follow-up queries). Hence, there is a scope to optimize the number of\nLLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize\nthe number of LLM tokens using Reinforcement Learning (RL). Specifically, we\npropose a policy-based model external to the RAG, which interacts with the RAG\npipeline through policy actions and updates the policy to optimize the cost.\nThe policy model can perform two actions: to fetch FAQ context or skip\nretrieval. We use the open API-based GPT-4 as the reward model. We then train a\npolicy model using policy gradient on multiple training chat sessions. As a\npolicy model, we experimented with a public gpt-2 model and an in-house BERT\nmodel. With the proposed RL-based optimization combined with similarity\nthreshold, we are able to achieve significant cost savings while getting a\nslightly improved accuracy. Though we demonstrate results for the FAQ chatbot,\nthe proposed RL approach is generic and can be experimented with any existing\nRAG pipeline.",
        "pdf_link": "https://arxiv.org/pdf/2401.06800v1.pdf"
    },
    {
        "title": "TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models",
        "authors": [
            "Xue Zhang",
            "Xiangyu Shi",
            "Xinyue Lou",
            "Rui Qi",
            "Yufeng Chen",
            "Jinan Xu",
            "Wenjuan Han"
        ],
        "published": "2024-01-09T10:20:29Z",
        "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave shown excellent general capabilities, even exhibiting adaptability in many\nprofessional domains such as law, economics, transportation, and medicine.\nCurrently, many domain-specific benchmarks have been proposed to verify the\nperformance of (M)LLMs in specific fields. Among various domains,\ntransportation plays a crucial role in modern society as it impacts the\neconomy, the environment, and the quality of life for billions of people.\nHowever, it is unclear how much traffic knowledge (M)LLMs possess and whether\nthey can reliably perform transportation-related tasks. To address this gap, we\npropose TransportationGames, a carefully designed and thorough evaluation\nbenchmark for assessing (M)LLMs in the transportation domain. By\ncomprehensively considering the applications in real-world scenarios and\nreferring to the first three levels in Bloom's Taxonomy, we test the\nperformance of various (M)LLMs in memorizing, understanding, and applying\ntransportation knowledge by the selected tasks. The experimental results show\nthat although some models perform well in some tasks, there is still much room\nfor improvement overall. We hope the release of TransportationGames can serve\nas a foundation for future research, thereby accelerating the implementation\nand application of (M)LLMs in the transportation domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.04471v1.pdf"
    },
    {
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
        "authors": [
            "Tong Niu",
            "Caiming Xiong",
            "Semih Yavuz",
            "Yingbo Zhou"
        ],
        "published": "2024-01-13T01:46:20Z",
        "summary": "The field of natural language generation has witnessed significant\nadvancements in recent years, including the development of controllable text\ngeneration techniques. However, controlling the attributes of the generated\ntext remains a challenge, especially when aiming to avoid undesirable behavior\nsuch as toxicity. In this work, we introduce Detoxification Generator\n(DETOXIGEN), an inference-time algorithm that steers the generation away from\nunwanted styles. DETOXIGEN is an ensemble of a pre-trained language model\n(generator) and a detoxifier. The detoxifier is trained intentionally on the\ntoxic data representative of the undesirable attribute, encouraging it to\ngenerate text in that style exclusively. During the actual generation, we use\nthe trained detoxifier to produce undesirable tokens for the generator to\ncontrast against at each decoding step. This approach directly informs the\ngenerator to avoid generating tokens that the detoxifier considers highly\nlikely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS\nbenchmark (Gehman et al., 2020) with various language models as generators. We\nfind that it significantly outperforms previous approaches in detoxification\nmetrics while not compromising on the generation quality. Moreover, the\ndetoxifier is obtained by soft prompt-tuning using the same backbone language\nmodel as the generator. Hence, DETOXIGEN requires only a tiny amount of extra\nweights from the virtual tokens of the detoxifier to be loaded into GPU memory\nwhile decoding, making it a promising lightweight, practical, and\nparameter-efficient detoxification strategy.",
        "pdf_link": "https://arxiv.org/pdf/2401.06947v1.pdf"
    },
    {
        "title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
        "authors": [
            "Sahil Chopra"
        ],
        "published": "2024-01-11T08:12:47Z",
        "summary": "Part of speech tagging in zero-resource settings can be an effective approach\nfor low-resource languages when no labeled training data is available. Existing\nsystems use two main techniques for POS tagging i.e. pretrained multilingual\nlarge language models(LLM) or project the source language labels into the zero\nresource target language and train a sequence labeling model on it. We explore\nthe latter approach using the off-the-shelf alignment module and train a hidden\nMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setup\nwith English as a source language and French, German, and Spanish as target\nlanguages for part-of-speech tagging. Our conclusion is that projected\nalignment data in zero-resource language can be beneficial to predict POS tags.",
        "pdf_link": "https://arxiv.org/pdf/2401.05727v1.pdf"
    },
    {
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "authors": [
            "Siyu Yuan",
            "Kaitao Song",
            "Jiangjie Chen",
            "Xu Tan",
            "Yongliang Shen",
            "Ren Kan",
            "Dongsheng Li",
            "Deqing Yang"
        ],
        "published": "2024-01-11T15:45:11Z",
        "summary": "To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.",
        "pdf_link": "https://arxiv.org/pdf/2401.06201v3.pdf"
    },
    {
        "title": "Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control",
        "authors": [
            "Yongjun Kim",
            "Sejin Seo",
            "Jihong Park",
            "Mehdi Bennis",
            "Seong-Lyun Kim",
            "Junil Choi"
        ],
        "published": "2024-01-23T10:23:13Z",
        "summary": "In this work, we compare emergent communication (EC) built upon multi-agent\ndeep reinforcement learning (MADRL) and language-oriented semantic\ncommunication (LSC) empowered by a pre-trained large language model (LLM) using\nhuman language. In a multi-agent remote navigation task, with multimodal input\ndata comprising location and channel maps, it is shown that EC incurs high\ntraining cost and struggles when using multimodal data, whereas LSC yields high\ninference computing cost due to the LLM's large size. To address their\nrespective bottlenecks, we propose a novel framework of language-guided EC\n(LEC) by guiding the EC training using LSC via knowledge distillation (KD).\nSimulations corroborate that LEC achieves faster travel time while avoiding\nareas with poor channel conditions, as well as speeding up the MADRL training\nconvergence by up to 61.8% compared to EC.",
        "pdf_link": "https://arxiv.org/pdf/2401.12624v2.pdf"
    },
    {
        "title": "Language Detection for Transliterated Content",
        "authors": [
            "Selva Kumar S",
            "Afifah Khan Mohammed Ajmal Khan",
            "Chirag Manjeshwar",
            "Imadh Ajaz Banday"
        ],
        "published": "2024-01-09T15:40:54Z",
        "summary": "In the contemporary digital era, the Internet functions as an unparalleled\ncatalyst, dismantling geographical and linguistic barriers particularly evident\nin texting. This evolution facilitates global communication, transcending\nphysical distances and fostering dynamic cultural exchange. A notable trend is\nthe widespread use of transliteration, where the English alphabet is employed\nto convey messages in native languages, posing a unique challenge for language\ntechnology in accurately detecting the source language. This paper addresses\nthis challenge through a dataset of phone text messages in Hindi and Russian\ntransliterated into English utilizing BERT for language classification and\nGoogle Translate API for transliteration conversion. The research pioneers\ninnovative approaches to identify and convert transliterated text, navigating\nchallenges in the diverse linguistic landscape of digital communication.\nEmphasizing the pivotal role of comprehensive datasets for training Large\nLanguage Models LLMs like BERT, our model showcases exceptional proficiency in\naccurately identifying and classifying languages from transliterated text. With\na validation accuracy of 99% our models robust performance underscores its\nreliability. The comprehensive exploration of transliteration dynamics\nsupported by innovative approaches and cutting edge technologies like BERT,\npositions our research at the forefront of addressing unique challenges in the\nlinguistic landscape of digital communication. Beyond contributing to language\nidentification and transliteration capabilities this work holds promise for\napplications in content moderation, analytics and fostering a globally\nconnected community engaged in meaningful dialogue.",
        "pdf_link": "https://arxiv.org/pdf/2401.04619v1.pdf"
    },
    {
        "title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
        "authors": [
            "Yujun Mao",
            "Yoon Kim",
            "Yilun Zhou"
        ],
        "published": "2024-01-13T03:18:16Z",
        "summary": "Recent large language models (LLMs) have shown indications of mathematical\nreasoning ability. However it has not been clear how they would fare on more\nchallenging competition-level problems. And while self-generated verbalizations\nof intermediate reasoning steps (i.e., chain-of-thought prompting) have been\nshown to be helpful, whether LLMs can make use of helpful side information such\nas problem-specific hints has not been investigated before. In this paper, we\npropose a challenging benchmark dataset for enabling such analyses. The Concept\nand Hint-Annotated Math Problems (CHAMP) consists of high school math\ncompetition problems, annotated with concepts, or general math facts, and\nhints, or problem-specific tricks. These annotations allow us to explore the\neffects of additional information, such as relevant hints, misleading concepts,\nor related problems. This benchmark is difficult, with the best model only\nscoring 58.1% in standard settings. With concepts and hints, performance\nsometimes improves, indicating that some models can make use of such side\ninformation. We further annotate model-generated solutions for their\ncorrectness. Using this corpus, we find that models often arrive at the correct\nfinal answer through wrong reasoning steps. In addition, we test whether models\nare able to verify these solutions, and find that most models struggle. The\ndataset and code are available on the project website.",
        "pdf_link": "https://arxiv.org/pdf/2401.06961v1.pdf"
    },
    {
        "title": "Comparing Template-based and Template-free Language Model Probing",
        "authors": [
            "Sagi Shaier",
            "Kevin Bennett",
            "Lawrence E Hunter",
            "Katharina von der Wense"
        ],
        "published": "2024-01-31T19:07:37Z",
        "summary": "The differences between cloze-task language model (LM) probing with 1)\nexpert-made templates and 2) naturally-occurring text have often been\noverlooked. Here, we evaluate 16 different LMs on 10 probing English datasets\n-- 4 template-based and 6 template-free -- in general and biomedical domains to\nanswer the following research questions: (RQ1) Do model rankings differ between\nthe two approaches? (RQ2) Do models' absolute scores differ between the two\napproaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and\ndomain-specific models? Our findings are: 1) Template-free and template-based\napproaches often rank models differently, except for the top domain-specific\nmodels. 2) Scores decrease by up to 42% Acc@1 when comparing parallel\ntemplate-free and template-based prompts. 3) Perplexity is negatively\ncorrelated with accuracy in the template-free approach, but,\ncounter-intuitively, they are positively correlated for template-based probing.\n4) Models tend to predict the same answers frequently across prompts for\ntemplate-based probing, which is less common when employing template-free\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2402.00123v1.pdf"
    },
    {
        "title": "Multi-Candidate Speculative Decoding",
        "authors": [
            "Sen Yang",
            "Shujian Huang",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "published": "2024-01-12T17:15:23Z",
        "summary": "Large language models have shown impressive capabilities across a variety of\nNLP tasks, yet their generating text autoregressively is time-consuming. One\nway to speed them up is speculative decoding, which generates candidate\nsegments (a sequence of tokens) from a fast draft model that is then verified\nin parallel by the target model. However, the acceptance rate of candidate\ntokens receives limitations from several factors, such as the model, the\ndataset, and the decoding setup. This paper proposes sampling multiple\ncandidates from a draft model and then organising them in batches for\nverification. We design algorithms for efficient multi-candidate verification\nwhile maintaining the distribution of the target model. Our approach shows\nsignificant improvements in acceptance rates on multiple datasets and models,\nconsistently outperforming standard speculative decoding.",
        "pdf_link": "https://arxiv.org/pdf/2401.06706v1.pdf"
    },
    {
        "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
        "authors": [
            "Chen Liu",
            "Shibo He",
            "Qihang Zhou",
            "Shizhong Li",
            "Wenchao Meng"
        ],
        "published": "2024-01-26T09:51:07Z",
        "summary": "Self-supervised methods have gained prominence in time series anomaly\ndetection due to the scarcity of available annotations. Nevertheless, they\ntypically demand extensive training data to acquire a generalizable\nrepresentation map, which conflicts with scenarios of a few available samples,\nthereby limiting their performance. To overcome the limitation, we propose\n\\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly\ndetection approach where the student network is trained to mimic the features\nof the large language model (LLM)-based teacher network that is pretrained on\nlarge-scale datasets. During the testing phase, anomalies are detected when the\ndiscrepancy between the features of the teacher and student networks is large.\nTo circumvent the student network from learning the teacher network's feature\nof anomalous samples, we devise two key strategies. 1) Prototypical signals are\nincorporated into the student network to consolidate the normal feature\nextraction. 2) We use synthetic anomalies to enlarge the representation gap\nbetween the two networks. AnomalyLLM demonstrates state-of-the-art performance\non 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset.",
        "pdf_link": "https://arxiv.org/pdf/2401.15123v1.pdf"
    },
    {
        "title": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models",
        "authors": [
            "Xinpeng Ding",
            "Jinahua Han",
            "Hang Xu",
            "Xiaodan Liang",
            "Wei Zhang",
            "Xiaomeng Li"
        ],
        "published": "2024-01-02T01:54:22Z",
        "summary": "The rise of multimodal large language models (MLLMs) has spurred interest in\nlanguage-based driving tasks. However, existing research typically focuses on\nlimited tasks and often omits key multi-view and temporal information which is\ncrucial for robust autonomous driving. To bridge these gaps, we introduce\nNuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17\nsubtasks, where each task demands holistic information (e.g., temporal,\nmulti-view, and spatial), significantly elevating the challenge level. To\nobtain NuInstruct, we propose a novel SQL-based method to generate\ninstruction-response pairs automatically, which is inspired by the driving\nlogical progression of humans. We further present BEV-InMLLM, an end-to-end\nmethod for efficiently deriving instruction-aware Bird's-Eye-View (BEV)\nfeatures, language-aligned for large language models. BEV-InMLLM integrates\nmulti-view, spatial awareness, and temporal semantics to enhance MLLMs'\ncapabilities on NuInstruct tasks. Moreover, our proposed BEV injection module\nis a plug-and-play method for existing MLLMs. Our experiments on NuInstruct\ndemonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.\naround 9% improvement on various tasks. We plan to release our NuInstruct for\nfuture research development.",
        "pdf_link": "https://arxiv.org/pdf/2401.00988v1.pdf"
    },
    {
        "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning",
        "authors": [
            "Chenyu Wang",
            "Weixin Luo",
            "Qianyu Chen",
            "Haonan Mai",
            "Jindi Guo",
            "Sixun Dong",
            "Xiaohua",
            "Xuan",
            "Zhengxin Li",
            "Lin Ma",
            "Shenghua Gao"
        ],
        "published": "2024-01-19T14:44:37Z",
        "summary": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' perceiving tool-use ability\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the visual- or auditory-grounded instructions' information.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learnt LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featured by consisting of multi-modal input tools from\nHuggingFace. Another important feature of our dataset is that our dataset also\ncontains multiple potential choices for the same instruction due to the\nexistence of identical functions and synonymous functions, which provides more\npotential solutions for the same query. The experiments reveal that our\nMLLM-Tool is capable of recommending appropriate tools for multi-modal\ninstructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.",
        "pdf_link": "https://arxiv.org/pdf/2401.10727v2.pdf"
    },
    {
        "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series",
        "authors": [
            "Vijay Ekambaram",
            "Arindam Jati",
            "Nam H. Nguyen",
            "Pankaj Dayama",
            "Chandra Reddy",
            "Wesley M. Gifford",
            "Jayant Kalagnanam"
        ],
        "published": "2024-01-08T15:21:21Z",
        "summary": "Large pre-trained models for zero/few-shot learning excel in language and\nvision domains but encounter challenges in multivariate time series (TS) due to\nthe diverse nature and scarcity of publicly available pre-training data.\nConsequently, there has been a recent surge in utilizing pre-trained large\nlanguage models (LLMs) with token adaptations for TS forecasting. These\napproaches employ cross-domain transfer learning and surprisingly yield\nimpressive results. However, these models are typically very slow and large\n(~billion parameters) and do not consider cross-channel correlations. To\naddress this, we present Tiny Time Mixers (TTM), a significantly small model\nbased on the lightweight TSMixer architecture. TTM marks the first success in\ndeveloping fast and tiny general pre-trained models (<1M parameters),\nexclusively trained on public TS datasets, with effective transfer learning\ncapabilities for forecasting. To tackle the complexity of pre-training on\nmultiple datasets with varied temporal resolutions, we introduce several novel\nenhancements such as adaptive patching, dataset augmentation via downsampling,\nand resolution prefix tuning. Moreover, we employ a multi-level modeling\nstrategy to effectively model channel correlations and infuse exogenous signals\nduring fine-tuning, a crucial capability lacking in existing benchmarks. TTM\nshows significant accuracy gains (12-38\\%) over popular benchmarks in\nfew/zero-shot forecasting. It also drastically reduces the compute needs as\ncompared to LLM-TS methods, with a 14X cut in learnable parameters, 106X less\ntotal parameters, and substantial reductions in fine-tuning (65X) and inference\ntime (54X). In fact, TTM's zero-shot often surpasses the few-shot results in\nmany popular benchmarks, highlighting the efficacy of our approach. Models and\nsource code are available at https://huggingface.co/ibm/TTM",
        "pdf_link": "https://arxiv.org/pdf/2401.03955v5.pdf"
    },
    {
        "title": "A Computational Framework for Behavioral Assessment of LLM Therapists",
        "authors": [
            "Yu Ying Chiu",
            "Ashish Sharma",
            "Inna Wanyin Lin",
            "Tim Althoff"
        ],
        "published": "2024-01-01T17:32:28Z",
        "summary": "The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.",
        "pdf_link": "https://arxiv.org/pdf/2401.00820v1.pdf"
    },
    {
        "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese",
        "authors": [
            "Nicholas Kluge Corr\u00eaa",
            "Sophia Falk",
            "Shiza Fatimah",
            "Aniket Sen",
            "Nythamar de Oliveira"
        ],
        "published": "2024-01-30T00:25:54Z",
        "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, but their progress has yet to be equal across languages. While most\nLLMs are trained in high-resource languages like English, multilingual models\ngenerally underperform monolingual ones. Additionally, aspects of their\nmultilingual foundation sometimes restrict the byproducts they produce, like\ncomputational demands and licensing regimes. In this study, we document the\ndevelopment of open-foundation models tailored for use in low-resource\nsettings, their limitations, and their benefits. This is the TeenyTinyLlama\npair: two compact models for Brazilian Portuguese text generation. We release\nthem under the permissive Apache 2.0 license on GitHub and Hugging Face for\ncommunity use and further development. See\nhttps://github.com/Nkluge-correa/TeenyTinyLlama",
        "pdf_link": "https://arxiv.org/pdf/2401.16640v2.pdf"
    },
    {
        "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives",
        "authors": [
            "Suchita Pati",
            "Shaizeen Aga",
            "Mahzabeen Islam",
            "Nuwan Jayasena",
            "Matthew D. Sinclair"
        ],
        "published": "2024-01-30T01:55:34Z",
        "summary": "Large Language Models increasingly rely on distributed techniques for their\ntraining and inference. These techniques require communication across devices\nwhich can reduce scaling efficiency as the number of devices increases. While\nsome distributed techniques can overlap, and thus, hide this communication with\nindependent computations, techniques such as Tensor Parallelism (TP) inherently\nserialize communication with model execution. One approach to hide this\nserialized communication is to interleave it with the producer operation (of\nthe communicated data) in a fine-grained manner. However, this fine-grained\ninterleaving of communication and computation in software can be difficult.\nFurthermore, as with any concurrent execution, it requires compute and memory\nresources to be shared between computation and communication, causing resource\ncontention that reduces overlapping efficacy.\n  To overcome these challenges, we propose T3 which applies hardware-software\nco-design to transparently overlap serialized communication while minimizing\nresource contention with compute. T3 transparently fuses producer operations\nwith the subsequent communication via a simple configuration of the producer's\noutput address space and requires minor software changes. At the hardware\nlevel, T3 adds a lightweight track and trigger mechanism to orchestrate the\nproducer's compute, and communication. It further uses compute-enhanced\nmemories for communication's attendant compute. As a result, T3 reduces\nresource contention, and efficiently overlaps serialized communication with\ncomputation. For important Transformer models like T-NLG, T3 speeds up\ncommunication-heavy sublayers by 30% geomean (max 47%) and reduces data\nmovement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models\nscale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM\nand MT-NLG.",
        "pdf_link": "https://arxiv.org/pdf/2401.16677v1.pdf"
    },
    {
        "title": "Towards Language-Driven Video Inpainting via Multimodal Large Language Models",
        "authors": [
            "Jianzong Wu",
            "Xiangtai Li",
            "Chenyang Si",
            "Shangchen Zhou",
            "Jingkang Yang",
            "Jiangning Zhang",
            "Yining Li",
            "Kai Chen",
            "Yunhai Tong",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "published": "2024-01-18T18:59:13Z",
        "summary": "We introduce a new task -- language-driven video inpainting, which uses\nnatural language instructions to guide the inpainting process. This approach\novercomes the limitations of traditional video inpainting methods that depend\non manually labeled binary masks, a process often tedious and labor-intensive.\nWe present the Remove Objects from Videos by Instructions (ROVI) dataset,\ncontaining 5,650 videos and 9,091 inpainting results, to support training and\nevaluation for this task. We also propose a novel diffusion-based\nlanguage-driven video inpainting framework, the first end-to-end baseline for\nthis task, integrating Multimodal Large Language Models to understand and\nexecute complex language-based inpainting requests effectively. Our\ncomprehensive results showcase the dataset's versatility and the model's\neffectiveness in various language-instructed inpainting scenarios. We will make\ndatasets, code, and models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2401.10226v1.pdf"
    },
    {
        "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
        "authors": [
            "Bongsu Kang",
            "Jundong Kim",
            "Tae-Rim Yun",
            "Chang-Eop Kim"
        ],
        "published": "2024-01-20T14:59:43Z",
        "summary": "We propose a natural language prompt-based retrieval augmented generation\n(Prompt-RAG), a novel approach to enhance the performance of generative large\nlanguage models (LLMs) in niche domains. Conventional RAG methods mostly\nrequire vector embeddings, yet the suitability of generic LLM-based embedding\nrepresentations for specialized domains remains uncertain. To explore and\nexemplify this point, we compared vector embeddings from Korean Medicine (KM)\nand Conventional Medicine (CM) documents, finding that KM document embeddings\ncorrelated more with token overlaps and less with human-assessed document\nrelatedness, in contrast to CM embeddings. Prompt-RAG, distinct from\nconventional RAG models, operates without the need for embedding vectors. Its\nperformance was assessed through a Question-Answering (QA) chatbot application,\nwhere responses were evaluated for relevance, readability, and informativeness.\nThe results showed that Prompt-RAG outperformed existing models, including\nChatGPT and conventional vector embedding-based RAGs, in terms of relevance and\ninformativeness. Despite challenges like content structuring and response\nlatency, the advancements in LLMs are expected to encourage the use of\nPrompt-RAG, making it a promising tool for other domains in need of RAG\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2401.11246v1.pdf"
    },
    {
        "title": "Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data",
        "authors": [
            "Yinghao Zhu",
            "Zixiang Wang",
            "Junyi Gao",
            "Yuning Tong",
            "Jingkun An",
            "Weibin Liao",
            "Ewen M. Harrison",
            "Liantao Ma",
            "Chengwei Pan"
        ],
        "published": "2024-01-25T20:14:50Z",
        "summary": "The inherent complexity of structured longitudinal Electronic Health Records\n(EHR) data poses a significant challenge when integrated with Large Language\nModels (LLMs), which are traditionally tailored for natural language\nprocessing. Motivated by the urgent need for swift decision-making during new\ndisease outbreaks, where traditional predictive models often fail due to a lack\nof historical data, this research investigates the adaptability of LLMs, like\nGPT-4, to EHR data. We particularly focus on their zero-shot capabilities,\nwhich enable them to make predictions in scenarios in which they haven't been\nexplicitly trained. In response to the longitudinal, sparse, and\nknowledge-infused nature of EHR data, our prompting approach involves taking\ninto account specific EHR characteristics such as units and reference ranges,\nand employing an in-context learning strategy that aligns with clinical\ncontexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets\ndemonstrate that with our elaborately designed prompting framework, LLMs can\nimprove prediction performance in key tasks such as mortality, length-of-stay,\nand 30-day readmission by about 35\\%, surpassing ML models in few-shot\nsettings. Our research underscores the potential of LLMs in enhancing clinical\ndecision-making, especially in urgent healthcare situations like the outbreak\nof emerging diseases with no labeled data. The code is publicly available at\nhttps://github.com/yhzhu99/llm4healthcare for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.01713v2.pdf"
    },
    {
        "title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions",
        "authors": [
            "Daniel de S. Moraes",
            "Pedro T. C. Santos",
            "Polyana B. da Costa",
            "Matheus A. S. Pinto",
            "Ivan de J. P. Pinto",
            "\u00c1lvaro M. G. da Veiga",
            "Sergio Colcher",
            "Antonio J. G. Busson",
            "Rafael H. Rocha",
            "Rennan Gaio",
            "Rafael Miceli",
            "Gabriela Tourinho",
            "Marcos Rabaioli",
            "Leandro Santos",
            "Fellipe Marques",
            "David Favaro"
        ],
        "published": "2024-01-08T00:27:16Z",
        "summary": "This work presents an unsupervised method for automatically constructing and\nexpanding topic taxonomies using instruction-based fine-tuned LLMs (Large\nLanguage Models). We apply topic modeling and keyword extraction techniques to\ncreate initial topic taxonomies and LLMs to post-process the resulting terms\nand create a hierarchy. To expand an existing taxonomy with new terms, we use\nzero-shot prompting to find out where to add new nodes, which, to our\nknowledge, is the first work to present such an approach to taxonomy tasks. We\nuse the resulting taxonomies to assign tags that characterize merchants from a\nretail bank dataset. To evaluate our work, we asked 12 volunteers to answer a\ntwo-part form in which we first assessed the quality of the taxonomies created\nand then the tags assigned to merchants based on that taxonomy. The evaluation\nrevealed a coherence rate exceeding 90% for the chosen taxonomies. The\ntaxonomies' expansion with LLMs also showed exciting results for parent node\nprediction, with an f1-score above 70% in our taxonomies.",
        "pdf_link": "https://arxiv.org/pdf/2401.06790v2.pdf"
    },
    {
        "title": "Training microrobots to swim by a large language model",
        "authors": [
            "Zhuoqun Xu",
            "Lailai Zhu"
        ],
        "published": "2024-01-21T12:18:59Z",
        "summary": "Machine learning and artificial intelligence have recently represented a\npopular paradigm for designing and optimizing robotic systems across various\nscales. Recent studies have showcased the innovative application of large\nlanguage models (LLMs) in industrial control [1] and in directing legged\nwalking robots [2]. In this study, we utilize an LLM, GPT-4, to train two\nprototypical microrobots for swimming in viscous fluids. Adopting a few-shot\nlearning approach, we develop a minimal, unified prompt composed of only five\nsentences. The same concise prompt successfully guides two distinct articulated\nmicrorobots -- the three-link swimmer and the three-sphere swimmer -- in\nmastering their signature strokes. These strokes, initially conceptualized by\nphysicists, are now effectively interpreted and applied by the LLM, enabling\nthe microrobots to circumvent the physical constraints inherent to\nmicro-locomotion. Remarkably, our LLM-based decision-making strategy\nsubstantially surpasses a traditional reinforcement learning method in terms of\ntraining speed. We discuss the nuanced aspects of prompt design, particularly\nemphasizing the reduction of monetary expenses of using GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.00044v1.pdf"
    },
    {
        "title": "Navigating the OverKill in Large Language Models",
        "authors": [
            "Chenyu Shi",
            "Xiao Wang",
            "Qiming Ge",
            "Songyang Gao",
            "Xianjun Yang",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang",
            "Xun Zhao",
            "Dahua Lin"
        ],
        "published": "2024-01-31T07:26:47Z",
        "summary": "Large language models are meticulously aligned to be both helpful and\nharmless. However, recent research points to a potential overkill which means\nmodels may refuse to answer benign queries. In this paper, we investigate the\nfactors for overkill by exploring how models handle and determine the safety of\nqueries. Our findings reveal the presence of shortcuts within models, leading\nto an over-attention of harmful words like 'kill' and prompts emphasizing\nsafety will exacerbate overkill. Based on these insights, we introduce\nSelf-Contrastive Decoding (Self-CD), a training-free and model-agnostic\nstrategy, to alleviate this phenomenon. We first extract such over-attention by\namplifying the difference in the model's output distributions when responding\nto system prompts that either include or omit an emphasis on safety. Then we\ndetermine the final next-token predictions by downplaying the over-attention\nfrom the model via contrastive decoding. Empirical results indicate that our\nmethod has achieved an average reduction of the refusal rate by 20\\% while\nhaving almost no impact on safety.",
        "pdf_link": "https://arxiv.org/pdf/2401.17633v1.pdf"
    },
    {
        "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
        "authors": [
            "Jiaheng Liu",
            "Zhiqi Bai",
            "Yuanxing Zhang",
            "Chenchen Zhang",
            "Yu Zhang",
            "Ge Zhang",
            "Jiakai Wang",
            "Haoran Que",
            "Yukang Chen",
            "Wenbo Su",
            "Tiezheng Ge",
            "Jie Fu",
            "Wenhu Chen",
            "Bo Zheng"
        ],
        "published": "2024-01-13T02:11:20Z",
        "summary": "Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06951v3.pdf"
    },
    {
        "title": "LoMA: Lossless Compressed Memory Attention",
        "authors": [
            "Yumeng Wang",
            "Zhenyang Xiao"
        ],
        "published": "2024-01-16T09:18:46Z",
        "summary": "Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.",
        "pdf_link": "https://arxiv.org/pdf/2401.09486v2.pdf"
    },
    {
        "title": "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models",
        "authors": [
            "Dingning Liu",
            "Xiaoshui Huang",
            "Yuenan Hou",
            "Zhihui Wang",
            "Zhenfei Yin",
            "Yongshun Gong",
            "Peng Gao",
            "Wanli Ouyang"
        ],
        "published": "2024-01-09T06:20:23Z",
        "summary": "In this paper, we introduce Uni3D-LLM, a unified framework that leverages a\nLarge Language Model (LLM) to integrate tasks of 3D perception, generation, and\nediting within point cloud scenes. This framework empowers users to\neffortlessly generate and modify objects at specified locations within a scene,\nguided by the versatility of natural language descriptions. Uni3D-LLM harnesses\nthe expressive power of natural language to allow for precise command over the\ngeneration and editing of 3D objects, thereby significantly enhancing\noperational flexibility and controllability. By mapping point cloud into the\nunified representation space, Uni3D-LLM achieves cross-application\nfunctionality, enabling the seamless execution of a wide array of tasks,\nranging from the accurate instantiation of 3D objects to the diverse\nrequirements of interactive design. Through a comprehensive suite of rigorous\nexperiments, the efficacy of Uni3D-LLM in the comprehension, generation, and\nediting of point cloud has been validated. Additionally, we have assessed the\nimpact of integrating a point cloud perception module on the generation and\nediting processes, confirming the substantial potential of our approach for\npractical applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.03327v1.pdf"
    },
    {
        "title": "Can AI Assistants Know What They Don't Know?",
        "authors": [
            "Qinyuan Cheng",
            "Tianxiang Sun",
            "Xiangyang Liu",
            "Wenwei Zhang",
            "Zhangyue Yin",
            "Shimin Li",
            "Linyang Li",
            "Zhengfu He",
            "Kai Chen",
            "Xipeng Qiu"
        ],
        "published": "2024-01-24T07:34:55Z",
        "summary": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.",
        "pdf_link": "https://arxiv.org/pdf/2401.13275v2.pdf"
    },
    {
        "title": "CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs",
        "authors": [
            "Daoan Zhang",
            "Junming Yang",
            "Hanjia Lyu",
            "Zijian Jin",
            "Yuan Yao",
            "Mingkai Chen",
            "Jiebo Luo"
        ],
        "published": "2024-01-05T00:26:07Z",
        "summary": "When exploring the development of Artificial General Intelligence (AGI), a\ncritical task for these models involves interpreting and processing information\nfrom multiple image inputs. However, Large Multimodal Models (LMMs) encounter\ntwo issues in such scenarios: (1) a lack of fine-grained perception, and (2) a\ntendency to blend information across multiple images. We first extensively\ninvestigate the capability of LMMs to perceive fine-grained visual details when\ndealing with multiple input images. The research focuses on two aspects: first,\nimage-to-image matching (to evaluate whether LMMs can effectively reason and\npair relevant images), and second, multi-image-to-text matching (to assess\nwhether LMMs can accurately capture and summarize detailed image information).\nWe conduct evaluations on a range of both open-source and closed-source large\nmodels, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model\nperformance, we further develop a Contrastive Chain-of-Thought (CoCoT)\nprompting approach based on multi-input multimodal models. This method requires\nLMMs to compare the similarities and differences among multiple image inputs,\nand then guide the models to answer detailed questions about multi-image inputs\nbased on the identified similarities and differences. Our experimental results\nshowcase CoCoT's proficiency in enhancing the multi-image comprehension\ncapabilities of large multimodal models.",
        "pdf_link": "https://arxiv.org/pdf/2401.02582v1.pdf"
    },
    {
        "title": "When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",
        "authors": [
            "Abdenour Hadid",
            "Tanujit Chakraborty",
            "Daniel Busby"
        ],
        "published": "2024-01-25T12:03:50Z",
        "summary": "Generative Artificial Intelligence (GAI) represents an emerging field that\npromises the creation of synthetic data and outputs in different modalities.\nGAI has recently shown impressive results across a large spectrum of\napplications ranging from biology, medicine, education, legislation, computer\nscience, and finance. As one strives for enhanced safety, efficiency, and\nsustainability, generative AI indeed emerges as a key differentiator and\npromises a paradigm shift in the field. This paper explores the potential\napplications of generative AI and large language models in geoscience. The\nrecent developments in the field of machine learning and deep learning have\nenabled the generative model's utility for tackling diverse prediction\nproblems, simulation, and multi-criteria decision-making challenges related to\ngeoscience and Earth system dynamics. This survey discusses several GAI models\nthat have been used in geoscience comprising generative adversarial networks\n(GANs), physics-informed neural networks (PINNs), and generative pre-trained\ntransformer (GPT)-based structures. These tools have helped the geoscience\ncommunity in several applications, including (but not limited to) data\ngeneration/augmentation, super-resolution, panchromatic sharpening, haze\nremoval, restoration, and land surface changing. Some challenges still remain\nsuch as ensuring physical interpretation, nefarious use cases, and\ntrustworthiness. Beyond that, GAI models show promises to the geoscience\ncommunity, especially with the support to climate change, urban science,\natmospheric science, marine science, and planetary science through their\nextraordinary ability to data-driven modeling and uncertainty quantification.",
        "pdf_link": "https://arxiv.org/pdf/2402.03349v1.pdf"
    },
    {
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Chunyang Li",
            "Haochen Shi",
            "Wenxuan Ding",
            "Baixuan Xu",
            "Zhaowei Wang",
            "Jiaxin Bai",
            "Xin Liu",
            "Jiayang Cheng",
            "Chunkit Chan",
            "Yangqiu Song"
        ],
        "published": "2024-01-14T13:24:30Z",
        "summary": "The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.",
        "pdf_link": "https://arxiv.org/pdf/2401.07286v1.pdf"
    },
    {
        "title": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters",
        "authors": [
            "Shiwei Liu",
            "Guanchen Tao",
            "Yifei Zou",
            "Derek Chow",
            "Zichen Fan",
            "Kauna Lei",
            "Bangfei Pan",
            "Dennis Sylvester",
            "Gregory Kielian",
            "Mehdi Saligane"
        ],
        "published": "2024-01-31T17:52:52Z",
        "summary": "The self-attention mechanism sets transformer-based large language model\n(LLM) apart from the convolutional and recurrent neural networks. Despite the\nperformance improvement, achieving real-time LLM inference on silicon is\nchallenging due to the extensively used Softmax in self-attention. Apart from\nthe non-linearity, the low arithmetic intensity greatly reduces the processing\nparallelism, which becomes the bottleneck especially when dealing with a longer\ncontext. To address this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design as an efficient Softmax alternative. ConSmax\nemploys differentiable normalization parameters to remove the maximum searching\nand denominator summation in Softmax. It allows for massive parallelization\nwhile performing the critical tasks of Softmax. In addition, a scalable ConSmax\nhardware utilizing a bitwidth-split look-up table (LUT) can produce lossless\nnon-linear operation and support mix-precision computing. It further\nfacilitates efficient LLM inference. Experimental results show that ConSmax\nachieves a minuscule power consumption of 0.43 mW and area of 0.001 mm2 at\n1-GHz working frequency and 22-nm CMOS technology. Compared to state-of-the-art\nSoftmax hardware, ConSmax results in 14.5x energy and 14.0x area savings with a\ncomparable accuracy on a GPT-2 model and the WikiText103 dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.10930v2.pdf"
    },
    {
        "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption",
        "authors": [
            "Dehao Tao",
            "Feng Huang",
            "Yongfeng Huang",
            "Minghu Jiang"
        ],
        "published": "2024-01-24T13:36:50Z",
        "summary": "In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. While\nintegrating knowledge graphs with LLMs has been explored, existing approaches\ntreat LLMs as primary decision-makers, imposing high demands on their\ncapabilities. This is particularly unsuitable for LLMs with lower computational\ncosts and relatively poorer performance. In this paper, we introduce a\nClue-Guided Path Exploration framework (CGPE) that efficiently merges a\nknowledge base with an LLM, placing less stringent requirements on the model's\ncapabilities. Inspired by the method humans use to manually retrieve knowledge,\nCGPE employs information from the question as clues to systematically explore\nthe required knowledge path within the knowledge base. Experiments on\nopen-source datasets reveal that CGPE outperforms previous methods and is\nhighly applicable to LLMs with fewer parameters. In some instances, even\nChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.\nFurthermore, the results indicate a minimal invocation frequency of CGPE on\nLLMs, suggesting reduced computational overhead. For organizations and\nindividuals facing constraints in computational resources, our research offers\nsignificant practical value.",
        "pdf_link": "https://arxiv.org/pdf/2401.13444v1.pdf"
    },
    {
        "title": "Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis",
        "authors": [
            "Richard Roberson",
            "Gowtham Kaki",
            "Ashutosh Trivedi"
        ],
        "published": "2024-01-22T22:05:42Z",
        "summary": "This study investigates various approaches to using Large Language Models\n(LLMs) for Text-to-SQL program synthesis, focusing on the outcomes and insights\nderived. Employing the popular Text-to-SQL dataset, spider, the goal was to\ninput a natural language question along with the database schema and output the\ncorrect SQL SELECT query. The initial approach was to fine-tune a local and\nopen-source model to generate the SELECT query. After QLoRa fine-tuning\nWizardLM's WizardCoder-15B model on the spider dataset, the execution accuracy\nfor generated queries rose to a high of 61%. With the second approach, using\nthe fine-tuned gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error\ncorrection), the execution accuracy reached a high of 82.1%. Of all the\nincorrect queries, most can be categorized into a seven different categories of\nwhat went wrong: selecting the wrong columns or wrong order of columns,\ngrouping by the wrong column, predicting the wrong values in conditionals,\nusing different aggregates than the ground truth, extra or too few JOIN\nclauses, inconsistencies in the Spider dataset, and lastly completely incorrect\nquery structure. Most if not all of the queries fall into these categories and\nit is insightful to understanding where the faults still lie with LLM program\nsynthesis and where they can be improved.",
        "pdf_link": "https://arxiv.org/pdf/2401.12379v1.pdf"
    },
    {
        "title": "TeleChat Technical Report",
        "authors": [
            "Zhongjiang He",
            "Zihan Wang",
            "Xinzhang Liu",
            "Shixuan Liu",
            "Yitong Yao",
            "Yuyao Huang",
            "Xuelong Li",
            "Yongxiang Li",
            "Zhonghao Che",
            "Zhaoxi Zhang",
            "Yan Wang",
            "Xin Wang",
            "Luwen Pu",
            "Huinan Xu",
            "Ruiyu Fang",
            "Yu Zhao",
            "Jie Zhang",
            "Xiaomeng Huang",
            "Zhilong Lu",
            "Jiaxin Peng",
            "Wenjun Zheng",
            "Shiquan Wang",
            "Bingkai Yang",
            "Xuewei he",
            "Zhuoru Jiang",
            "Qiyi Xie",
            "Yanhan Zhang",
            "Zhongqiu Li",
            "Lingling Shi",
            "Weiwei Fu",
            "Yin Zhang",
            "Zilu Huang",
            "Sishi Xiong",
            "Yuxiang Zhang",
            "Chao Wang",
            "Shuangyong Song"
        ],
        "published": "2024-01-08T10:43:19Z",
        "summary": "In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.",
        "pdf_link": "https://arxiv.org/pdf/2401.03804v2.pdf"
    },
    {
        "title": "Physio: An LLM-Based Physiotherapy Advisor",
        "authors": [
            "R\u00faben Almeida",
            "Hugo Sousa",
            "Lu\u00eds F. Cunha",
            "Nuno Guimar\u00e3es",
            "Ricardo Campos",
            "Al\u00edpio Jorge"
        ],
        "published": "2024-01-03T16:42:13Z",
        "summary": "The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.",
        "pdf_link": "https://arxiv.org/pdf/2401.01825v1.pdf"
    },
    {
        "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens",
        "authors": [
            "Jiacheng Liu",
            "Sewon Min",
            "Luke Zettlemoyer",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2024-01-30T19:03:49Z",
        "summary": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2401.17377v3.pdf"
    },
    {
        "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
        "authors": [
            "Yiqi Wang",
            "Wentao Chen",
            "Xiaotian Han",
            "Xudong Lin",
            "Haiteng Zhao",
            "Yongfei Liu",
            "Bohan Zhai",
            "Jianbo Yuan",
            "Quanzeng You",
            "Hongxia Yang"
        ],
        "published": "2024-01-10T15:29:21Z",
        "summary": "Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence\n(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent\nadvancements in Large Language Models (LLMs), along with the emerging field of\nMultimodal Large Language Models (MLLMs), have demonstrated impressive\ncapabilities across a wide range of multimodal tasks and applications.\nParticularly, various MLLMs, each with distinct model architectures, training\ndata, and training stages, have been evaluated across a broad range of MLLM\nbenchmarks. These studies have, to varying degrees, revealed different aspects\nof the current capabilities of MLLMs. However, the reasoning abilities of MLLMs\nhave not been systematically investigated. In this survey, we comprehensively\nreview the existing evaluation protocols of multimodal reasoning, categorize\nand illustrate the frontiers of MLLMs, introduce recent trends in applications\nof MLLMs on reasoning-intensive tasks, and finally discuss current practices\nand future directions. We believe our survey establishes a solid base and sheds\nlight on this important topic, multimodal reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2401.06805v2.pdf"
    },
    {
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "authors": [
            "Andreas Opedal",
            "Alessandro Stolfo",
            "Haruki Shirakami",
            "Ying Jiao",
            "Ryan Cotterell",
            "Bernhard Sch\u00f6lkopf",
            "Abulhair Saparov",
            "Mrinmaya Sachan"
        ],
        "published": "2024-01-31T18:48:20Z",
        "summary": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\nwe study the biases of LLMs in relation to those known in children when solving\narithmetic word problems. Surveying the learning science literature, we posit\nthat the problem-solving process can be split into three distinct steps: text\ncomprehension, solution planning and solution execution. We construct tests for\neach one in order to understand which parts of this process can be faithfully\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\nproblems for each of these tests, using a neuro-symbolic method that enables\nfine-grained control over the problem features. We find evidence that LLMs,\nwith and without instruction-tuning, exhibit human-like biases in both the\ntext-comprehension and the solution-planning steps of the solving process, but\nnot during the final step which relies on the problem's arithmetic expressions\n(solution execution).",
        "pdf_link": "https://arxiv.org/pdf/2401.18070v1.pdf"
    },
    {
        "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
        "authors": [
            "Wenwen Li",
            "Chia-Yu Hsu",
            "Sizhe Wang",
            "Yezhou Yang",
            "Hyunho Lee",
            "Anna Liljedahl",
            "Chandi Witharana",
            "Yili Yang",
            "Brendan M. Rogers",
            "Samantha T. Arundel",
            "Matthew B. Jones",
            "Kenton McHenry",
            "Patricia Solis"
        ],
        "published": "2024-01-16T19:10:09Z",
        "summary": "This paper assesses trending AI foundation models, especially emerging\ncomputer vision foundation models and their performance in natural landscape\nfeature segmentation. While the term foundation model has quickly garnered\ninterest from the geospatial domain, its definition remains vague. Hence, this\npaper will first introduce AI foundation models and their defining\ncharacteristics. Built upon the tremendous success achieved by Large Language\nModels (LLMs) as the foundation models for language tasks, this paper discusses\nthe challenges of building foundation models for geospatial artificial\nintelligence (GeoAI) vision tasks. To evaluate the performance of large AI\nvision models, especially Meta's Segment Anything Model (SAM), we implemented\ndifferent instance segmentation pipelines that minimize the changes to SAM to\nleverage its power as a foundation model. A series of prompt strategies was\ndeveloped to test SAM's performance regarding its theoretical upper bound of\npredictive accuracy, zero-shot performance, and domain adaptability through\nfine-tuning. The analysis used two permafrost feature datasets, ice-wedge\npolygons and retrogressive thaw slumps because (1) these landform features are\nmore challenging to segment than manmade features due to their complicated\nformation mechanisms, diverse forms, and vague boundaries; (2) their presence\nand changes are important indicators for Arctic warming and climate change. The\nresults show that although promising, SAM still has room for improvement to\nsupport AI-augmented terrain mapping. The spatial and domain generalizability\nof this finding is further validated using a more general dataset EuroCrop for\nagricultural field mapping. Finally, we discuss future research directions that\nstrengthen SAM's applicability in challenging geospatial domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.08787v1.pdf"
    },
    {
        "title": "Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",
        "authors": [
            "Shadeeb Hossain",
            "Aayush Gohil",
            "Yizhou Wang"
        ],
        "published": "2024-01-18T20:14:10Z",
        "summary": "This paper discusses the feasibility of using Large Language Models LLM for\ncode generation with a particular application in designing an RISC. The paper\nalso reviews the associated steps such as parsing, tokenization, encoding,\nattention mechanism, sampling the tokens and iterations during code generation.\nThe generated code for the RISC components is verified through testbenches and\nhardware implementation on a FPGA board. Four metric parameters Correct output\non the first iteration, Number of errors embedded in the code, Number of trials\nrequired to achieve the code and Failure to generate the code after three\niterations, are used to compare the efficiency of using LLM in programming. In\nall the cases, the generated code had significant errors and human intervention\nwas always required to fix the bugs. LLM can therefore be used to complement a\nprogrammer code design.",
        "pdf_link": "https://arxiv.org/pdf/2401.10364v1.pdf"
    },
    {
        "title": "Cross-target Stance Detection by Exploiting Target Analytical Perspectives",
        "authors": [
            "Daijun Ding",
            "Rong Chen",
            "Liwen Jing",
            "Bowen Zhang",
            "Xu Huang",
            "Li Dong",
            "Xiaowen Zhao",
            "Ge Song"
        ],
        "published": "2024-01-03T14:28:55Z",
        "summary": "Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2401.01761v2.pdf"
    },
    {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "published": "2024-01-10T09:49:10Z",
        "summary": "Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.",
        "pdf_link": "https://arxiv.org/pdf/2401.05033v1.pdf"
    },
    {
        "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
        "authors": [
            "Junjie Ye",
            "Guanyu Li",
            "Songyang Gao",
            "Caishuang Huang",
            "Yilong Wu",
            "Sixian Li",
            "Xiaoran Fan",
            "Shihan Dou",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-01-01T12:49:36Z",
        "summary": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.",
        "pdf_link": "https://arxiv.org/pdf/2401.00741v2.pdf"
    },
    {
        "title": "Hierarchical Continual Reinforcement Learning via Large Language Model",
        "authors": [
            "Chaofan Pan",
            "Xin Yang",
            "Hao Wang",
            "Wei Wei",
            "Tianrui Li"
        ],
        "published": "2024-01-25T03:06:51Z",
        "summary": "The ability to learn continuously in dynamic environments is a crucial\nrequirement for reinforcement learning (RL) agents applying in the real world.\nDespite the progress in continual reinforcement learning (CRL), existing\nmethods often suffer from insufficient knowledge transfer, particularly when\nthe tasks are diverse. To address this challenge, we propose a new framework,\nHierarchical Continual reinforcement learning via large language model\n(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core\norchestrates a twolayer structure: high-level policy formulation by a large\nlanguage model (LLM), which represents agenerates a sequence of goals, and\nlow-level policy learning that closely aligns with goal-oriented RL practices,\nproducing the agent's actions in response to the goals set forth. The framework\nemploys feedback to iteratively adjust and verify highlevel policies, storing\nthem along with low-level policies within a skill library. When encountering a\nnew task, Hi-Core retrieves relevant experience from this library to help to\nlearning. Through experiments on Minigrid, Hi-Core has demonstrated its\neffectiveness in handling diverse CRL tasks, which outperforms popular\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2401.15098v2.pdf"
    },
    {
        "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study",
        "authors": [
            "Hala Abdelkader",
            "Mohamed Abdelrazek",
            "Scott Barnett",
            "Jean-Guy Schneider",
            "Priya Rani",
            "Rajesh Vasa"
        ],
        "published": "2024-01-12T11:27:15Z",
        "summary": "Machine learning (ML), especially with the emergence of large language models\n(LLMs), has significantly transformed various industries. However, the\ntransition from ML model prototyping to production use within software systems\npresents several challenges. These challenges primarily revolve around ensuring\nsafety, security, and transparency, subsequently influencing the overall\nrobustness and trustworthiness of ML models. In this paper, we introduce\nML-On-Rails, a protocol designed to safeguard ML models, establish a\nwell-defined endpoint interface for different ML tasks, and clear communication\nbetween ML providers and ML consumers (software engineers). ML-On-Rails\nenhances the robustness of ML models via incorporating detection capabilities\nto identify unique challenges specific to production ML. We evaluated the\nML-On-Rails protocol through a real-world case study of the MoveReminder\napplication. Through this evaluation, we emphasize the importance of\nsafeguarding ML models in production.",
        "pdf_link": "https://arxiv.org/pdf/2401.06513v1.pdf"
    },
    {
        "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods",
        "authors": [
            "Yu Sun",
            "Keyu Chen",
            "Shujie Wang",
            "Qipeng Guo",
            "Hang Yan",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Dahua Lin"
        ],
        "published": "2024-01-26T13:55:32Z",
        "summary": "Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities.",
        "pdf_link": "https://arxiv.org/pdf/2401.14869v1.pdf"
    },
    {
        "title": "Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",
        "authors": [
            "Yunshi Lan",
            "Xinyuan Li",
            "Hanyue Du",
            "Xuesong Lu",
            "Ming Gao",
            "Weining Qian",
            "Aoying Zhou"
        ],
        "published": "2024-01-15T07:48:42Z",
        "summary": "Natural Language Processing (NLP) aims to analyze text or speech via\ntechniques in the computer science field. It serves the applications in domains\nof healthcare, commerce, education and so on. Particularly, NLP has been widely\napplied to the education domain and its applications have enormous potential to\nhelp teaching and learning. In this survey, we review recent advances in NLP\nwith the focus on solving problems relevant to the education domain. In detail,\nwe begin with introducing the related background and the real-world scenarios\nin education where NLP techniques could contribute. Then, we present a taxonomy\nof NLP in the education domain and highlight typical NLP applications including\nquestion answering, question construction, automated assessment, and error\ncorrection. Next, we illustrate the task definition, challenges, and\ncorresponding cutting-edge techniques based on the above taxonomy. In\nparticular, LLM-involved methods are included for discussion due to the wide\nusage of LLMs in diverse NLP applications. After that, we showcase some\noff-the-shelf demonstrations in this domain. At last, we conclude with six\npromising directions for future research, including more datasets in education\ndomain, controllable usage of LLMs, intervention of difficulty-level control,\ninterpretable educational NLP, methods with adaptive learning, and integrated\nsystems for education. We organize all relevant datasets and papers in the\nopen-available Github Link for better\nreview~\\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",
        "pdf_link": "https://arxiv.org/pdf/2401.07518v3.pdf"
    },
    {
        "title": "Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models",
        "authors": [
            "Xin He",
            "Longhui Wei",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "published": "2024-01-06T02:02:34Z",
        "summary": "Multimodal Large Language Models (MLLMs) are experiencing rapid growth,\nyielding a plethora of noteworthy contributions in recent months. The\nprevailing trend involves adopting data-driven methodologies, wherein diverse\ninstruction-following datasets are collected. However, a prevailing challenge\npersists in these approaches, specifically in relation to the limited visual\nperception ability, as CLIP-like encoders employed for extracting visual\ninformation from inputs. Though these encoders are pre-trained on billions of\nimage-text pairs, they still grapple with the information loss dilemma, given\nthat textual captions only partially capture the contents depicted in images.\nTo address this limitation, this paper proposes to improve the visual\nperception ability of MLLMs through a mixture-of-experts knowledge enhancement\nmechanism. Specifically, we introduce a novel method that incorporates\nmulti-task encoders and visual tools into the existing MLLMs training and\ninference pipeline, aiming to provide a more comprehensive and accurate\nsummarization of visual inputs. Extensive experiments have evaluated its\neffectiveness of advancing MLLMs, showcasing improved visual perception\nachieved through the integration of visual experts.",
        "pdf_link": "https://arxiv.org/pdf/2401.03105v2.pdf"
    },
    {
        "title": "ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation",
        "authors": [
            "Bhabesh Mali",
            "Karthik Maddala",
            "Sweeya Reddy",
            "Vatsal Gupta",
            "Chandan Karfa",
            "Ramesh Karri"
        ],
        "published": "2024-01-31T12:41:27Z",
        "summary": "System Verilog Assertion (SVA) formulation -- a critical yet complex task is\na prerequisite in the Formal Property Verification (FPV) process.\nTraditionally, SVA formulation involves expert-driven interpretation of\nspecifications, which is timeconsuming and prone to human error. However,\nLLM-informed automatic assertion generation is gaining interest. We designeda\nnovel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA\nassertions from natural language specifications. ChIRAAG constitutes the\nsystematic breakdown of design specifications into a standardized format,\nfurther generating assertions from formatted specifications using LLM.\nFurthermore, we developed testbenches to verify/validate the LLM-generated\nassertions. Automatic feedback of log files from the simulation tool to the LLM\nensures that the framework can generate correc SVAs automatically. Only 33% of\nLLM-generated raw assertions had errors. Our results on OpenTitan designs shows\nthat LLMs can streamline and assist engineers in the assertion generation\nprocess, reshaping verification workflows.",
        "pdf_link": "https://arxiv.org/pdf/2402.00093v2.pdf"
    },
    {
        "title": "DeepEdit: Knowledge Editing as Decoding with Constraints",
        "authors": [
            "Yiwei Wang",
            "Muhao Chen",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2024-01-19T03:48:27Z",
        "summary": "We propose a new perspective of knowledge editing (KE) for large language\nmodels (LLMs) that treats it as a constrained decoding problem. We design\ndecoding constraints to regulate LLMs, ensuring coherence between reasoning\nsteps when incorporating new knowledge. To enforce these constraints, we\nutilize a depth-first search to adaptively substitute new knowledge for the\nLLMs' original reasoning steps, greedily seeking the optimal path of multi-hop\nreasoning with new knowledge. From this vantage, we propose DEEPEDIT:\nDepth-first Search-based Decoding for Knowledge Editing. DEEPEDIT improves the\nKE of LLMs by enhancing the conciseness, coherence, pertinence, and\nreceptiveness of reasoning with new knowledge. DEEPEDIT is flexibly applicable\nto any black-box LLM without requiring access to model parameters or token-wise\ndistributions. In addition to DEEPEDIT, we propose two new KE benchmarks:\nMQuAKE-2002 and MQuAKE-hard, which are designed to provide more precise and\nchallenging assessments of KE approaches. Qualitatively, DEEPEDIT enables LLMs\nto produce more succinct reasoning outputs in accordance with new knowledge.\nQuantitatively, it yields significant improvements on multiple KE benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.10471v2.pdf"
    },
    {
        "title": "Reinforcement learning for question answering in programming domain using public community scoring as a human feedback",
        "authors": [
            "Alexey Gorbatovski",
            "Sergey Kovalchuk"
        ],
        "published": "2024-01-19T18:49:36Z",
        "summary": "In this study, we investigate the enhancement of the GPT Neo 125M performance\nin Community Question Answering (CQA) with a focus on programming, through the\nintegration of Reinforcement Learning from Human Feedback (RLHF) and the\nutilization of scores from Stack Overflow. Two distinct reward model training\nstrategies are employed for fine-tuning with Proximal Policy Optimization\n(PPO). Notably, the improvements in performance achieved through this method\nare comparable to those of GPT Neo 2.7B parameter variant. Additionally, an\nauxiliary scoring mechanism is introduced, which demonstrates the limitations\nof conventional linguistic metrics in evaluating responses in the programming\ndomain. Through accurate analysis, this paper looks at the divergence between\ntraditional linguistic metrics and our human-preferences-based reward model,\nunderscoring the imperative for domain-specific evaluation methods. By\nelucidating the complexities involved in applying RLHF to programming CQA and\naccentuating the significance of context-aware evaluation, this study\ncontributes to the ongoing efforts in refining Large Language Models through\nfocused human feedback.",
        "pdf_link": "https://arxiv.org/pdf/2401.10882v1.pdf"
    },
    {
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "authors": [
            "Alexandre Alcoforado",
            "Thomas Palmeira Ferraz",
            "Lucas Hideki Okamura",
            "Israel Campos Fama",
            "Arnold Moya Lavado",
            "B\u00e1rbara Dias Bueno",
            "Bruno Veloso",
            "Anna Helena Reali Costa"
        ],
        "published": "2024-01-24T04:57:32Z",
        "summary": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.13229v1.pdf"
    },
    {
        "title": "Can Large Language Models Replace Economic Choice Prediction Labs?",
        "authors": [
            "Eilam Shapira",
            "Omer Madmon",
            "Roi Reichart",
            "Moshe Tennenholtz"
        ],
        "published": "2024-01-30T20:49:47Z",
        "summary": "Economic choice prediction is an essential challenging task, often\nconstrained by the difficulties in acquiring human choice data. Indeed,\nexperimental economics studies had focused mostly on simple choice settings.\nThe AI community has recently contributed to that effort in two ways:\nconsidering whether LLMs can substitute for humans in the above-mentioned\nsimple choice prediction settings, and the study through ML lens of more\nelaborated but still rigorous experimental economics settings, employing\nincomplete information, repetitive play, and natural language communication,\nnotably language-based persuasion games. This leaves us with a major\ninspiration: can LLMs be used to fully simulate the economic environment and\ngenerate data for efficient human choice prediction, substituting for the\nelaborated economic lab studies? We pioneer the study of this subject,\ndemonstrating its feasibility. In particular, we show that a model trained\nsolely on LLM-generated data can effectively predict human behavior in a\nlanguage-based persuasion game, and can even outperform models trained on\nactual human data.",
        "pdf_link": "https://arxiv.org/pdf/2401.17435v3.pdf"
    },
    {
        "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
        "authors": [
            "Ran Elgedawy",
            "Sudarshan Srinivasan",
            "Ioana Danciu"
        ],
        "published": "2024-01-19T14:50:22Z",
        "summary": "Electronic health records (EHRs) house crucial patient data in clinical\nnotes. As these notes grow in volume and complexity, manual extraction becomes\nchallenging. This work introduces a natural language interface using large\nlanguage models (LLMs) for dynamic question-answering on clinical notes. Our\nchatbot, powered by Langchain and transformer-based LLMs, allows users to query\nin natural language, receiving relevant answers from clinical notes.\nExperiments, utilizing various embedding models and advanced LLMs, show Wizard\nVicuna's superior accuracy, albeit with high compute demands. Model\noptimization, including weight quantization, improves latency by approximately\n48 times. Promising results indicate potential, yet challenges such as model\nhallucinations and limited diverse medical case evaluations remain. Addressing\nthese gaps is crucial for unlocking the value in clinical notes and advancing\nAI-driven clinical decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2401.10733v1.pdf"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": [
            "Andy Zhou",
            "Bo Li",
            "Haohan Wang"
        ],
        "published": "2024-01-30T18:56:08Z",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on benign use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4, GUARD, from 92% to 6%.",
        "pdf_link": "https://arxiv.org/pdf/2401.17263v2.pdf"
    },
    {
        "title": "TQCompressor: improving tensor decomposition methods in neural networks via permutations",
        "authors": [
            "V. Abronin",
            "A. Naumov",
            "D. Mazur",
            "D. Bystrov",
            "K. Tsarova",
            "Ar. Melnikov",
            "I. Oseledets",
            "S. Dolgov",
            "R. Brasher",
            "M. Perelshtein"
        ],
        "published": "2024-01-29T18:07:56Z",
        "summary": "We introduce TQCompressor, a novel method for neural network model\ncompression with improved tensor decompositions. We explore the challenges\nposed by the computational and storage demands of pre-trained language models\nin NLP tasks and propose a permutation-based enhancement to Kronecker\ndecomposition. This enhancement makes it possible to reduce loss in model\nexpressivity which is usually associated with factorization. We demonstrate\nthis method applied to the GPT-2$_{small}$. The result of the compression is\nTQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in\nthe GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further\nenhance the performance of the TQCompressedGPT-2 through a training strategy\ninvolving multi-step knowledge distillation, using only a 3.1% of the\nOpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative\nevaluations, marking an advancement in the efficient and effective deployment\nof models in resource-constrained environments.",
        "pdf_link": "https://arxiv.org/pdf/2401.16367v1.pdf"
    },
    {
        "title": "Security Code Review by LLMs: A Deep Dive into Responses",
        "authors": [
            "Jiaxin Yu",
            "Peng Liang",
            "Yujia Fu",
            "Amjed Tahir",
            "Mojtaba Shahin",
            "Chong Wang",
            "Yangxiao Cai"
        ],
        "published": "2024-01-29T17:13:44Z",
        "summary": "Security code review aims to combine automated tools and manual efforts to\ndetect security defects during development. The rapid development of Large\nLanguage Models (LLMs) has shown promising potential in software development,\nas well as opening up new possibilities in automated security code review. To\nexplore the challenges of applying LLMs in practical code review for security\ndefect detection, this study compared the detection performance of three\nstate-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on\n549 code files that contain security defects from real-world code reviews.\nThrough analyzing 82 responses generated by the best-performing LLM-prompt\ncombination based on 100 randomly selected code files, we extracted and\ncategorized quality problems present in these responses into 5 themes and 16\ncategories. Our results indicate that the responses produced by LLMs often\nsuffer from verbosity, vagueness, and incompleteness, highlighting the\nnecessity to enhance their conciseness, understandability, and compliance to\nsecurity defect detection. This work reveals the deficiencies of LLM-generated\nresponses in security code review and paves the way for future optimization of\nLLMs towards this task.",
        "pdf_link": "https://arxiv.org/pdf/2401.16310v1.pdf"
    },
    {
        "title": "Evolving Code with A Large Language Model",
        "authors": [
            "Erik Hemberg",
            "Stephen Moskal",
            "Una-May O'Reilly"
        ],
        "published": "2024-01-13T15:57:54Z",
        "summary": "Algorithms that use Large Language Models (LLMs) to evolve code arrived on\nthe Genetic Programming (GP) scene very recently. We present LLM GP, a\nformalized LLM-based evolutionary algorithm designed to evolve code. Like GP,\nit uses evolutionary operators, but its designs and implementations of those\noperators radically differ from GP's because they enlist an LLM, using\nprompting and the LLM's pre-trained pattern matching and sequence completion\ncapability. We also present a demonstration-level variant of LLM GP and share\nits code. By addressing algorithms that range from the formal to hands-on, we\ncover design and LLM-usage considerations as well as the scientific challenges\nthat arise when using an LLM for genetic programming.",
        "pdf_link": "https://arxiv.org/pdf/2401.07102v1.pdf"
    },
    {
        "title": "Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models",
        "authors": [
            "James Prather",
            "Paul Denny",
            "Juho Leinonen",
            "David H. Smith IV",
            "Brent N. Reeves",
            "Stephen MacNeil",
            "Brett A. Becker",
            "Andrew Luxton-Reilly",
            "Thezyrie Amarouche",
            "Bailey Kimmel"
        ],
        "published": "2024-01-19T15:32:46Z",
        "summary": "Large Language Models (LLMs) have upended decades of pedagogy in computing\neducation. Students previously learned to code through \\textit{writing} many\nsmall problems with less emphasis on code reading and comprehension. Recent\nresearch has shown that free code generation tools powered by LLMs can solve\nintroductory programming problems presented in natural language with ease. In\nthis paper, we propose a new way to teach programming with Prompt Problems.\nStudents receive a problem visually, indicating how input should be transformed\nto output, and must translate that to a prompt for an LLM to decipher. The\nproblem is considered correct when the code that is generated by the student\nprompt can pass all test cases. In this paper we present the design of this\ntool, discuss student interactions with it as they learn, and provide insights\ninto this new class of programming problems as well as the design tools that\nintegrate LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.10759v1.pdf"
    },
    {
        "title": "Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance",
        "authors": [
            "Chau Nguyen",
            "Le-Minh Nguyen"
        ],
        "published": "2024-01-31T15:04:01Z",
        "summary": "The objective of legal text entailment is to ascertain whether the assertions\nin a legal query logically follow from the information provided in one or\nmultiple legal articles. ChatGPT, a large language model, is robust in many\nnatural language processing tasks, including legal text entailment: when we set\nthe temperature = 0 (the ChatGPT answers are deterministic) and prompt the\nmodel, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms\nthe previous SOTA of 67.89%. On the other hand, if the temperature is larger\nthan zero, ChatGPT answers are not deterministic, leading to inconsistent\nanswers and fluctuating results. We propose to leverage label models (a\nfundamental component of weak supervision techniques) to integrate the\nprovisional answers by ChatGPT into consolidated labels. By that way, we treat\nChatGPT provisional answers as noisy predictions which can be consolidated by\nlabel models. The experimental results demonstrate that this approach can\nattain an accuracy of 76.15%, marking a significant improvement of 8.26% over\nthe prior state-of-the-art benchmark. Additionally, we perform an analysis of\nthe instances where ChatGPT produces incorrect answers, then we classify the\nerrors, offering insights that could guide potential enhancements for future\nresearch endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2401.17897v1.pdf"
    },
    {
        "title": "AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media",
        "authors": [
            "Alessandro Gambetti",
            "Qiwei Han"
        ],
        "published": "2024-01-16T20:57:36Z",
        "summary": "Online reviews in the form of user-generated content (UGC) significantly\nimpact consumer decision-making. However, the pervasive issue of not only human\nfake content but also machine-generated content challenges UGC's reliability.\nRecent advances in Large Language Models (LLMs) may pave the way to fabricate\nindistinguishable fake generated content at a much lower cost. Leveraging\nOpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a\nmulti-modal dataset of 20,144 restaurant review-image pairs divided into\nauthentic and machine-generated. We explore unimodal and multimodal detection\nmodels, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from\nreadability and photographic theories to score reviews and images,\nrespectively, demonstrating their utility as hand-crafted features in scalable\nand interpretable detection models, with comparable performance. The paper\ncontributes by open-sourcing the dataset and releasing fake review detectors,\nrecommending its use in unimodal and multimodal fake review detection tasks,\nand evaluating linguistic and visual features in synthetic versus authentic\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2401.08825v1.pdf"
    },
    {
        "title": "ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning",
        "authors": [
            "A. Ghafarollahi",
            "M. J. Buehler"
        ],
        "published": "2024-01-27T20:19:49Z",
        "summary": "Designing de novo proteins beyond those found in nature holds significant\npromise for advancements in both scientific and engineering applications.\nCurrent methodologies for protein design often rely on AI-based models, such as\nsurrogate models that address end-to-end problems by linking protein structure\nto material properties or vice versa. However, these models frequently focus on\nspecific material objectives or structural properties, limiting their\nflexibility when incorporating out-of-domain knowledge into the design process\nor comprehensive data analysis is required. In this study, we introduce\nProtAgents, a platform for de novo protein design based on Large Language\nModels (LLMs), where multiple AI agents with distinct capabilities\ncollaboratively address complex tasks within a dynamic environment. The\nversatility in agent development allows for expertise in diverse domains,\nincluding knowledge retrieval, protein structure analysis, physics-based\nsimulations, and results analysis. The dynamic collaboration between agents,\nempowered by LLMs, provides a versatile approach to tackling protein design and\nanalysis problems, as demonstrated through diverse examples in this study. The\nproblems of interest encompass designing new proteins, analyzing protein\nstructures and obtaining new first-principles data -- natural vibrational\nfrequencies -- via physics simulations. The concerted effort of the system\nallows for powerful automated and synergistic design of de novo proteins with\ntargeted mechanical properties. The flexibility in designing the agents, on one\nhand, and their capacity in autonomous collaboration through the dynamic\nLLM-based multi-agent environment on the other hand, unleashes great potentials\nof LLMs in addressing multi-objective materials problems and opens up new\navenues for autonomous materials discovery and design.",
        "pdf_link": "https://arxiv.org/pdf/2402.04268v1.pdf"
    },
    {
        "title": "EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge",
        "authors": [
            "Xuyang Zhao",
            "Qibin Zhao",
            "Toshihisa Tanaka"
        ],
        "published": "2024-01-11T13:39:00Z",
        "summary": "With large training datasets and massive amounts of computing sources, large\nlanguage models (LLMs) achieve remarkable performance in comprehensive and\ngenerative ability. Based on those powerful LLMs, the model fine-tuned with\ndomain-specific datasets posseses more specialized knowledge and thus is more\npractical like medical LLMs. However, the existing fine-tuned medical LLMs are\nlimited to general medical knowledge with English language. For\ndisease-specific problems, the model's response is inaccurate and sometimes\neven completely irrelevant, especially when using a language other than\nEnglish. In this work, we focus on the particular disease of Epilepsy with\nJapanese language and introduce a customized LLM termed as EpilepsyLLM. Our\nmodel is trained from the pre-trained LLM by fine-tuning technique using\ndatasets from the epilepsy domain. The datasets contain knowledge of basic\ninformation about disease, common treatment methods and drugs, and important\nnotes in life and work. The experimental results demonstrate that EpilepsyLLM\ncan provide more reliable and specialized medical knowledge responses.",
        "pdf_link": "https://arxiv.org/pdf/2401.05908v1.pdf"
    },
    {
        "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
        "authors": [
            "Zhicheng Dou",
            "Yuchen Guo",
            "Ching-Chun Chang",
            "Huy H. Nguyen",
            "Isao Echizen"
        ],
        "published": "2024-01-16T01:58:36Z",
        "summary": "The emergence of large language models (LLMs), such as Generative Pre-trained\nTransformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and\nbroader community. While these models offer numerous advantages in terms of\nrevolutionizing work and study methods, they have also garnered significant\nattention due to their potential negative consequences. One example is\ngenerating academic reports or papers with little to no human contribution.\nConsequently, researchers have focused on developing detectors to address the\nmisuse of LLMs. However, most existing methods prioritize achieving higher\naccuracy on restricted datasets, neglecting the crucial aspect of\ngeneralizability. This limitation hinders their practical application in\nreal-life scenarios where reliability is paramount. In this paper, we present a\ncomprehensive analysis of the impact of prompts on the text generated by LLMs\nand highlight the potential lack of robustness in one of the current\nstate-of-the-art GPT detectors. To mitigate these issues concerning the misuse\nof LLMs in academic writing, we propose a reference-based Siamese detector\nnamed Synthetic-Siamese which takes a pair of texts, one as the inquiry and the\nother as the reference. Our method effectively addresses the lack of robustness\nof previous detectors (OpenAI detector and DetectGPT) and significantly\nimproves the baseline performances in realistic academic writing scenarios by\napproximately 67% to 95%.",
        "pdf_link": "https://arxiv.org/pdf/2401.08046v1.pdf"
    },
    {
        "title": "How well can large language models explain business processes?",
        "authors": [
            "Dirk Fahland",
            "Fabiana Fournier",
            "Lior Limonad",
            "Inna Skarbovsky",
            "Ava J. E. Swevels"
        ],
        "published": "2024-01-23T15:29:26Z",
        "summary": "Large Language Models (LLMs) are likely to play a prominent role in future\nAI-augmented business process management systems (ABPMSs) catering\nfunctionalities across all system lifecycle stages. One such system's\nfunctionality is Situation-Aware eXplainability (SAX), which relates to\ngenerating causally sound and yet human-interpretable explanations that take\ninto account the process context in which the explained condition occurred. In\nthis paper, we present the SAX4BPM framework developed to generate SAX\nexplanations. The SAX4BPM suite consists of a set of services and a central\nknowledge repository. The functionality of these services is to elicit the\nvarious knowledge ingredients that underlie SAX explanations. A key innovative\ncomponent among these ingredients is the causal process execution view. In this\nwork, we integrate the framework with an LLM to leverage its power to\nsynthesize the various input ingredients for the sake of improved SAX\nexplanations. Since the use of LLMs for SAX is also accompanied by a certain\ndegree of doubt related to its capacity to adequately fulfill SAX along with\nits tendency for hallucination and lack of inherent capacity to reason, we\npursued a methodological evaluation of the quality of the generated\nexplanations. To this aim, we developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
        "pdf_link": "https://arxiv.org/pdf/2401.12846v1.pdf"
    },
    {
        "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "authors": [
            "Coleman Hooper",
            "Sehoon Kim",
            "Hiva Mohammadzadeh",
            "Michael W. Mahoney",
            "Yakun Sophia Shao",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "published": "2024-01-31T18:58:14Z",
        "summary": "LLMs are seeing growing use for applications such as document analysis and\nsummarization which require large context windows, and with these large context\nwindows KV cache activations surface as the dominant contributor to memory\nconsumption during inference. Quantization is a promising approach for\ncompressing KV cache activations; however, existing solutions fail to represent\nactivations accurately in ultra-low precisions, such as sub-4-bit. In this\nwork, we present KVQuant, which addresses this problem by incorporating novel\nmethods for quantizing cached KV activations, including: (i) Per-Channel Key\nQuantization, where we adjust the dimension along which we quantize the Key\nactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,\nwhere we quantize Key activations before the rotary positional embedding to\nmitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,\nwhere we derive per-layer sensitivity-weighted non-uniform datatypes that\nbetter represent the distributions; (iv) Per-Vector Dense-and-Sparse\nQuantization, where we isolate outliers separately for each vector to minimize\nskews in quantization ranges; and (v) Q-Norm, where we normalize quantization\ncentroids in order to mitigate distribution shift, providing additional\nbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,\nand Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit\nquantization on both Wikitext-2 and C4, outperforming existing approaches. Our\nmethod enables serving the LLaMA-7B model with a context length of up to 1\nmillion on a single A100-80GB GPU and up to 10 million on an 8-GPU system.",
        "pdf_link": "https://arxiv.org/pdf/2401.18079v3.pdf"
    },
    {
        "title": "QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners",
        "authors": [
            "Rui Xiao",
            "Lu Han",
            "Xiaoying Zhou",
            "Jiong Wang",
            "Na Zong",
            "Pengyu Zhang"
        ],
        "published": "2024-01-30T13:11:23Z",
        "summary": "In online learning platforms, particularly in rapidly growing computer\nprogramming courses, addressing the thousands of students' learning queries\nrequires considerable human cost. The creation of intelligent assistant large\nlanguage models (LLMs) tailored for programming education necessitates distinct\ndata support. However, in real application scenarios, the data resources for\ntraining such LLMs are relatively scarce. Therefore, to address the data\nscarcity in intelligent educational systems for programming, this paper\nproposes a new Chinese question-and-answer dataset for Python learners. To\nensure the authenticity and reliability of the sources of the questions, we\ncollected questions from actual student questions and categorized them\naccording to various dimensions such as the type of questions and the type of\nlearners. This annotation principle is designed to enhance the effectiveness\nand quality of online programming education, providing a solid data foundation\nfor developing the programming teaching assists (TA). Furthermore, we conducted\ncomprehensive evaluations of various LLMs proficient in processing and\ngenerating Chinese content, highlighting the potential limitations of general\nLLMs as intelligent teaching assistants in computer programming courses.",
        "pdf_link": "https://arxiv.org/pdf/2402.07913v2.pdf"
    },
    {
        "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
        "authors": [
            "Li Sun",
            "Liuan Wang",
            "Jun Sun",
            "Takayuki Okatani"
        ],
        "published": "2024-01-18T10:18:48Z",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the comprehension of multimedia content, bringing\ntogether diverse modalities such as text, images, and videos. However, a\ncritical challenge faced by these models, especially when processing video\ninputs, is the occurrence of hallucinations - erroneous perceptions or\ninterpretations, particularly at the event level. This study introduces an\ninnovative method to address event-level hallucinations in MLLMs, focusing on\nspecific temporal understanding in video content. Our approach leverages a\nnovel framework that extracts and utilizes event-specific information from both\nthe event query and the provided video to refine MLLMs' response. We propose a\nunique mechanism that decomposes on-demand event queries into iconic actions.\nSubsequently, we employ models like CLIP and BLIP2 to predict specific\ntimestamps for event occurrences. Our evaluation, conducted using the\nCharades-STA dataset, demonstrates a significant reduction in temporal\nhallucinations and an improvement in the quality of event-related responses.\nThis research not only provides a new perspective in addressing a critical\nlimitation of MLLMs but also contributes a quantitatively measurable method for\nevaluating MLLMs in the context of temporal-related questions.",
        "pdf_link": "https://arxiv.org/pdf/2401.09861v1.pdf"
    },
    {
        "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "authors": [
            "Janice Ahn",
            "Rishu Verma",
            "Renze Lou",
            "Di Liu",
            "Rui Zhang",
            "Wenpeng Yin"
        ],
        "published": "2024-01-31T20:26:32Z",
        "summary": "Mathematical reasoning serves as a cornerstone for assessing the fundamental\ncognitive capabilities of human intelligence. In recent times, there has been a\nnotable surge in the development of Large Language Models (LLMs) geared towards\nthe automated resolution of mathematical problems. However, the landscape of\nmathematical problem types is vast and varied, with LLM-oriented techniques\nundergoing evaluation across diverse datasets and settings. This diversity\nmakes it challenging to discern the true advancements and obstacles within this\nburgeoning field. This survey endeavors to address four pivotal dimensions: i)\na comprehensive exploration of the various mathematical problems and their\ncorresponding datasets that have been investigated; ii) an examination of the\nspectrum of LLM-oriented techniques that have been proposed for mathematical\nproblem-solving; iii) an overview of factors and concerns affecting LLMs in\nsolving math; and iv) an elucidation of the persisting challenges within this\ndomain. To the best of our knowledge, this survey stands as one of the first\nextensive examinations of the landscape of LLMs in the realm of mathematics,\nproviding a holistic perspective on the current state, accomplishments, and\nfuture challenges in this rapidly evolving field.",
        "pdf_link": "https://arxiv.org/pdf/2402.00157v3.pdf"
    },
    {
        "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering",
        "authors": [
            "Pierre Erbacher",
            "Louis Falissar",
            "Vincent Guigue",
            "Laure Soulier"
        ],
        "published": "2024-01-03T15:12:42Z",
        "summary": "While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.",
        "pdf_link": "https://arxiv.org/pdf/2401.01780v1.pdf"
    },
    {
        "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
        "authors": [
            "Qiang Zhang",
            "Keyang Ding",
            "Tianwen Lyv",
            "Xinda Wang",
            "Qingyu Yin",
            "Yiwen Zhang",
            "Jing Yu",
            "Yuhao Wang",
            "Xiaotong Li",
            "Zhuoyi Xiang",
            "Xiang Zhuang",
            "Zeyuan Wang",
            "Ming Qin",
            "Mengyao Zhang",
            "Jinlu Zhang",
            "Jiyu Cui",
            "Renjun Xu",
            "Hongyang Chen",
            "Xiaohui Fan",
            "Huabin Xing",
            "Huajun Chen"
        ],
        "published": "2024-01-26T05:33:34Z",
        "summary": "Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.14656v1.pdf"
    },
    {
        "title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
        "authors": [
            "Feng Jiang",
            "Kuang Wang",
            "Haizhou Li"
        ],
        "published": "2024-01-17T11:50:53Z",
        "summary": "In the contemporary information era, significantly accelerated by the advent\nof Large-scale Language Models, the proliferation of scientific literature is\nreaching unprecedented levels. Researchers urgently require efficient tools for\nreading and summarizing academic papers, uncovering significant scientific\nliterature, and employing diverse interpretative methodologies. To address this\nburgeoning demand, the role of automated scientific literature interpretation\nsystems has become paramount. However, prevailing models, both commercial and\nopen-source, confront notable challenges: they often overlook multimodal data,\ngrapple with summarizing over-length texts, and lack diverse user interfaces.\nIn response, we introduce an open-source multi-modal automated academic paper\ninterpretation system (MMAPIS) with three-step process stages, incorporating\nLLMs to augment its functionality. Our system first employs the hybrid modality\npreprocessing and alignment module to extract plain text, and tables or figures\nfrom documents separately. It then aligns this information based on the section\nnames they belong to, ensuring that data with identical section names are\ncategorized under the same section. Following this, we introduce a hierarchical\ndiscourse-aware summarization method. It utilizes the extracted section names\nto divide the article into shorter text segments, facilitating specific\nsummarizations both within and between sections via LLMs with specific prompts.\nFinally, we have designed four types of diversified user interfaces, including\npaper recommendation, multimodal Q\\&A, audio broadcasting, and interpretation\nblog, which can be widely applied across various scenarios. Our qualitative and\nquantitative evaluations underscore the system's superiority, especially in\nscientific summarization, where it outperforms solutions relying solely on\nGPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2401.09150v1.pdf"
    },
    {
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "authors": [
            "Scott Barnett",
            "Stefanus Kurniawan",
            "Srikanth Thudumu",
            "Zach Brannelly",
            "Mohamed Abdelrazek"
        ],
        "published": "2024-01-11T12:04:11Z",
        "summary": "Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2401.05856v1.pdf"
    },
    {
        "title": "Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task",
        "authors": [
            "Gabriel Lino Garcia",
            "Pedro Henrique Paiola",
            "Luis Henrique Morelli",
            "Giovani Candido",
            "Arnaldo C\u00e2ndido J\u00fanior",
            "Danilo Samuel Jodas",
            "Luis C. S. Afonso",
            "Ivan Rizzo Guilherme",
            "Bruno Elias Penteado",
            "Jo\u00e3o Paulo Papa"
        ],
        "published": "2024-01-05T17:15:01Z",
        "summary": "Large Language Models (LLMs) are increasingly bringing advances to Natural\nLanguage Processing. However, low-resource languages, those lacking extensive\nprominence in datasets for various NLP tasks, or where existing datasets are\nnot as substantial, such as Portuguese, already obtain several benefits from\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\nnormally struggle to respond to prompts in Portuguese satisfactorily,\npresenting, for example, code switching in their responses. This work proposes\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\nversions: 7B and 13B. We evaluate the performance of this model in\nclassification tasks using the zero-shot approach with in-context learning, and\ncompare it with other LLMs. Our main contribution is to bring an LLM with\nsatisfactory results in the Portuguese language, as well as to provide a model\nthat is free for research or commercial purposes.",
        "pdf_link": "https://arxiv.org/pdf/2401.02909v1.pdf"
    },
    {
        "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
        "authors": [
            "Cheonsu Jeong"
        ],
        "published": "2024-01-01T06:22:04Z",
        "summary": "Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.",
        "pdf_link": "https://arxiv.org/pdf/2401.02981v2.pdf"
    },
    {
        "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation",
        "authors": [
            "Kohei Uehara",
            "Nabarun Goswami",
            "Hanqin Wang",
            "Toshiaki Baba",
            "Kohtaro Tanaka",
            "Tomohiro Hashimoto",
            "Kai Wang",
            "Rei Ito",
            "Takagi Naoya",
            "Ryo Umagami",
            "Yingyi Wen",
            "Tanachai Anakewat",
            "Tatsuya Harada"
        ],
        "published": "2024-01-18T14:21:56Z",
        "summary": "The increasing demand for intelligent systems capable of interpreting and\nreasoning about visual content requires the development of Large Multi-Modal\nModels (LMMs) that are not only accurate but also have explicit reasoning\ncapabilities. This paper presents a novel approach to imbue an LMM with the\nability to conduct explicit reasoning based on visual content and textual\ninstructions. We introduce a system that can ask a question to acquire\nnecessary knowledge, thereby enhancing the robustness and explicability of the\nreasoning process. Our method comprises the development of a novel dataset\ngenerated by a Large Language Model (LLM), designed to promote chain-of-thought\nreasoning combined with a question-asking mechanism. We designed an LMM, which\nhas high capabilities on region awareness to address the intricate requirements\nof image-text alignment. The model undergoes a three-stage training phase,\nstarting with large-scale image-text alignment using a large-scale datasets,\nfollowed by instruction tuning, and fine-tuning with a focus on\nchain-of-thought reasoning. The results demonstrate a stride toward a more\nrobust, accurate, and interpretable LMM, capable of reasoning explicitly and\nseeking information proactively when confronted with ambiguous visual input.",
        "pdf_link": "https://arxiv.org/pdf/2401.10005v1.pdf"
    },
    {
        "title": "LLMs for Robotic Object Disambiguation",
        "authors": [
            "Connie Jiang",
            "Yiqing Xu",
            "David Hsu"
        ],
        "published": "2024-01-07T04:46:23Z",
        "summary": "The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.",
        "pdf_link": "https://arxiv.org/pdf/2401.03388v1.pdf"
    },
    {
        "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
        "authors": [
            "Jing Yu Koh",
            "Robert Lo",
            "Lawrence Jang",
            "Vikram Duvvur",
            "Ming Chong Lim",
            "Po-Yu Huang",
            "Graham Neubig",
            "Shuyan Zhou",
            "Ruslan Salakhutdinov",
            "Daniel Fried"
        ],
        "published": "2024-01-24T18:35:21Z",
        "summary": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.",
        "pdf_link": "https://arxiv.org/pdf/2401.13649v1.pdf"
    },
    {
        "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models",
        "authors": [
            "Zhengxin Zhang",
            "Dan Zhao",
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Qing Li",
            "Yong Jiang",
            "Zhihao Jia"
        ],
        "published": "2024-01-13T21:00:21Z",
        "summary": "Finetuning large language models (LLMs) has been empirically effective on a\nvariety of downstream tasks. Existing approaches to finetuning an LLM either\nfocus on parameter-efficient finetuning, which only updates a small number of\ntrainable parameters, or attempt to reduce the memory footprint during the\ntraining phase of the finetuning. Typically, the memory footprint during\nfinetuning stems from three contributors: model weights, optimizer states, and\nintermediate activations. However, existing works still require considerable\nmemory and none can simultaneously mitigate memory footprint for all three\nsources. In this paper, we present Quantized Side Tuing (QST), which enables\nmemory-efficient and fast finetuning of LLMs by operating through a dual-stage\nprocess. First, QST quantizes an LLM's model weights into 4-bit to reduce the\nmemory footprint of the LLM's original weights; QST also introduces a side\nnetwork separated from the LLM, which utilizes the hidden states of the LLM to\nmake task-specific predictions. Using a separate side network avoids performing\nbackpropagation through the LLM, thus reducing the memory requirement of the\nintermediate activations. Furthermore, QST leverages several low-rank adaptors\nand gradient-free downsample modules to significantly reduce the trainable\nparameters, so as to save the memory footprint of the optimizer states.\nExperiments show that QST can reduce the total memory footprint by up to 2.3\n$\\times$ and speed up the finetuning process by up to 3 $\\times$ while\nachieving competent performance compared with the state-of-the-art. When it\ncomes to full finetuning, QST can reduce the total memory footprint up to 7\n$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2401.07159v1.pdf"
    },
    {
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "authors": [
            "Varshini Reddy",
            "Rik Koncel-Kedziorski",
            "Viet Dac Lai",
            "Michael Krumdick",
            "Charles Lovering",
            "Chris Tanner"
        ],
        "published": "2024-01-12T22:19:22Z",
        "summary": "For large language models (LLMs) to be effective in the financial domain --\nwhere each decision can have a significant impact -- it is necessary to\ninvestigate realistic tasks and data. Financial professionals often interact\nwith documents that are hundreds of pages long, but most financial research\ndatasets only deal with short excerpts from these documents. To address this,\nwe introduce a long-document financial QA task. We augment 7,437 questions from\nthe existing FinQA dataset with the full-document context, extending the\naverage context length from under 700 words in FinQA to 123k words in DocFinQA.\nWe conduct extensive experiments over retrieval-based QA pipelines and\nlong-context language models. DocFinQA proves a significant challenge for even\nstate-of-the-art systems. We also provide a case-study on the longest documents\nin DocFinQA and find that models particularly struggle on these documents.\nAddressing these challenges may have a wide reaching impact across applications\nwhere specificity and long-range contexts are critical, like gene sequences and\nlegal document contract analysis.",
        "pdf_link": "https://arxiv.org/pdf/2401.06915v2.pdf"
    },
    {
        "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
        "authors": [
            "Mauricio Rivera",
            "Jean-Fran\u00e7ois Godbout",
            "Reihaneh Rabbany",
            "Kellin Pelrine"
        ],
        "published": "2024-01-13T16:36:58Z",
        "summary": "Large Language Models have emerged as prime candidates to tackle\nmisinformation mitigation. However, existing approaches struggle with\nhallucinations and overconfident predictions. We propose an uncertainty\nquantification framework that leverages both direct confidence elicitation and\nsampled-based consistency methods to provide better calibration for NLP\nmisinformation mitigation solutions. We first investigate the calibration of\nsample-based consistency methods that exploit distinct features of consistency\nacross sample sizes and stochastic levels. Next, we evaluate the performance\nand distributional shift of a robust numeric verbalization prompt across single\nvs. two-step confidence elicitation procedure. We also compare the performance\nof the same prompt with different versions of GPT and different numerical\nscales. Finally, we combine the sample-based consistency and verbalized methods\nto propose a hybrid framework that yields a better uncertainty estimation for\nGPT models. Overall, our work proposes novel uncertainty quantification methods\nthat will improve the reliability of Large Language Models in misinformation\nmitigation applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.08694v2.pdf"
    },
    {
        "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark",
        "authors": [
            "Ge Zhang",
            "Xinrun Du",
            "Bei Chen",
            "Yiming Liang",
            "Tongxu Luo",
            "Tianyu Zheng",
            "Kang Zhu",
            "Yuyang Cheng",
            "Chunpu Xu",
            "Shuyue Guo",
            "Haoran Zhang",
            "Xingwei Qu",
            "Junjie Wang",
            "Ruibin Yuan",
            "Yizhi Li",
            "Zekun Wang",
            "Yudong Liu",
            "Yu-Hsuan Tsai",
            "Fengji Zhang",
            "Chenghua Lin",
            "Wenhao Huang",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "published": "2024-01-22T13:34:34Z",
        "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.",
        "pdf_link": "https://arxiv.org/pdf/2401.11944v2.pdf"
    },
    {
        "title": "Prompting open-source and commercial language models for grammatical error correction of English learner text",
        "authors": [
            "Christopher Davis",
            "Andrew Caines",
            "\u00d8istein Andersen",
            "Shiva Taslimipoor",
            "Helen Yannakoudakis",
            "Zheng Yuan",
            "Christopher Bryant",
            "Marek Rei",
            "Paula Buttery"
        ],
        "published": "2024-01-15T14:19:47Z",
        "summary": "Thanks to recent advances in generative AI, we are able to prompt large\nlanguage models (LLMs) to produce texts which are fluent and grammatical. In\naddition, it has been shown that we can elicit attempts at grammatical error\ncorrection (GEC) from LLMs when prompted with ungrammatical input sentences. We\nevaluate how well LLMs can perform at GEC by measuring their performance on\nestablished benchmark datasets. We go beyond previous studies, which only\nexamined GPT* models on a selection of English GEC datasets, by evaluating\nseven open-source and three commercial LLMs on four established GEC benchmarks.\nWe investigate model performance and report results against individual error\ntypes. Our results indicate that LLMs do not always outperform supervised\nEnglish GEC models except in specific contexts -- namely commercial LLMs on\nbenchmarks annotated with fluency corrections as opposed to minimal edits. We\nfind that several open-source models outperform commercial ones on minimal edit\nbenchmarks, and that in some settings zero-shot prompting is just as\ncompetitive as few-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2401.07702v1.pdf"
    },
    {
        "title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis",
        "authors": [
            "Chao Zhang",
            "Yuren Mao",
            "Yijiang Fan",
            "Yu Mi",
            "Yunjun Gao",
            "Lu Chen",
            "Dongfang Lou",
            "Jinshu Lin"
        ],
        "published": "2024-01-19T05:48:07Z",
        "summary": "Text-to-SQL, which provides zero-code interface for operating relational\ndatabases, has gained much attention in financial analysis; because, financial\nprofessionals may not well-skilled in SQL programming. However, until now,\nthere is no practical Text-to-SQL benchmark dataset for financial analysis, and\nexisting Text-to-SQL methods have not considered the unique characteristics of\ndatabases in financial applications, such as commonly existing wide tables. To\naddress these issues, we collect a practical Text-to-SQL benchmark dataset and\npropose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL\nframework for financial analysis. The benchmark dataset, BULL, is collected\nfrom the practical financial analysis business of Hundsun Technologies Inc.,\nincluding databases for fund, stock, and macro economy. Besides, the proposed\nLLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for\nfinancial Text-to-SQL from the perspectives of prompt construction,\nparameter-efficient fine-tuning and output calibration. Extensive experimental\nresults on BULL demonstrate that FinSQL achieves the state-of-the-art\nText-to-SQL performance at a small cost; furthermore, FinSQL can bring up to\n36.64% performance improvement in scenarios requiring few-shot cross-database\nmodel transfer.",
        "pdf_link": "https://arxiv.org/pdf/2401.10506v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education",
        "authors": [
            "Vahid Ashrafimoghari",
            "Necdet G\u00fcrkan",
            "Jordan W. Suchow"
        ],
        "published": "2024-01-02T03:54:50Z",
        "summary": "The rapid evolution of artificial intelligence (AI), especially in the domain\nof Large Language Models (LLMs) and generative AI, has opened new avenues for\napplication across various fields, yet its role in business education remains\nunderexplored. This study introduces the first benchmark to assess the\nperformance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and\nGPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models\n(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission\nprocess for graduate business programs. Our analysis shows that most LLMs\noutperform human candidates, with GPT-4 Turbo not only outperforming the other\nmodels but also surpassing the average scores of graduate students at top\nbusiness schools. Through a case study, this research examines GPT-4 Turbo's\nability to explain answers, evaluate responses, identify errors, tailor\ninstructions, and generate alternative scenarios. The latest LLM versions,\nGPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in\nreasoning tasks compared to their predecessors, underscoring their potential\nfor complex problem-solving. While AI's promise in education, assessment, and\ntutoring is clear, challenges remain. Our study not only sheds light on LLMs'\nacademic potential but also emphasizes the need for careful development and\napplication of AI in education. As AI technology advances, it is imperative to\nestablish frameworks and protocols for AI interaction, verify the accuracy of\nAI-generated content, ensure worldwide access for diverse learners, and create\nan educational environment where AI supports human expertise. This research\nsets the stage for further exploration into the responsible use of AI to enrich\neducational experiences and improve exam preparation and assessment methods.",
        "pdf_link": "https://arxiv.org/pdf/2401.02985v1.pdf"
    },
    {
        "title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation",
        "authors": [
            "Zhenyu Wang",
            "Enze Xie",
            "Aoxue Li",
            "Zhongdao Wang",
            "Xihui Liu",
            "Zhenguo Li"
        ],
        "published": "2024-01-28T16:18:39Z",
        "summary": "Despite significant advancements in text-to-image models for generating\nhigh-quality images, these methods still struggle to ensure the controllability\nof text prompts over images in the context of complex text prompts, especially\nwhen it comes to retaining object attributes and relationships. In this paper,\nwe propose CompAgent, a training-free approach for compositional text-to-image\ngeneration, with a large language model (LLM) agent as its core. The\nfundamental idea underlying CompAgent is premised on a divide-and-conquer\nmethodology. Given a complex text prompt containing multiple concepts including\nobjects, attributes, and relationships, the LLM agent initially decomposes it,\nwhich entails the extraction of individual objects, their associated\nattributes, and the prediction of a coherent scene layout. These individual\nobjects can then be independently conquered. Subsequently, the agent performs\nreasoning by analyzing the text, plans and employs the tools to compose these\nisolated objects. The verification and human feedback mechanism is finally\nincorporated into our agent to further correct the potential attribute errors\nand refine the generated images. Guided by the LLM agent, we propose a\ntuning-free multi-concept customization model and a layout-to-image generation\nmodel as the tools for concept composition, and a local image editing method as\nthe tool to interact with the agent for verification. The scene layout controls\nthe image generation process among these tools to prevent confusion among\nmultiple objects. Extensive experiments demonstrate the superiority of our\napproach for compositional text-to-image generation: CompAgent achieves more\nthan 10\\% improvement on T2I-CompBench, a comprehensive benchmark for\nopen-world compositional T2I generation. The extension to various related tasks\nalso illustrates the flexibility of our CompAgent for potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.15688v2.pdf"
    },
    {
        "title": "Are self-explanations from Large Language Models faithful?",
        "authors": [
            "Andreas Madsen",
            "Sarath Chandar",
            "Siva Reddy"
        ],
        "published": "2024-01-15T19:39:15Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, importance measure, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2,\nimportance measures for Mistral, and redaction for Falcon 40B.",
        "pdf_link": "https://arxiv.org/pdf/2401.07927v3.pdf"
    },
    {
        "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "published": "2024-01-27T11:41:48Z",
        "summary": "Retrieval-augmented generation (RAG) augments large language models (LLM) by\nretrieving relevant knowledge, showing promising potential in mitigating LLM\nhallucinations and enhancing response quality, thereby facilitating the great\nadoption of LLMs in practice. However, we find that existing RAG systems are\ninadequate in answering multi-hop queries, which require retrieving and\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-hop queries, their ground-truth\nanswers, and the associated supporting evidence. We detail the procedure of\nbuilding the dataset, utilizing an English news article dataset as the\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\nMultiHop-RAG in two experiments. The first experiment compares different\nembedding models for retrieving evidence for multi-hop queries. In the second\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\nqueries given the evidence. Both experiments reveal that existing RAG methods\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\nMultiHop-RAG will be a valuable resource for the community in developing\neffective RAG systems, thereby facilitating greater adoption of LLMs in\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\nhttps://github.com/yixuantt/MultiHop-RAG/.",
        "pdf_link": "https://arxiv.org/pdf/2401.15391v1.pdf"
    },
    {
        "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models",
        "authors": [
            "Selim Sandal",
            "Ismail Akturk"
        ],
        "published": "2024-01-12T17:41:38Z",
        "summary": "The design and optimization of hardware have traditionally been\nresource-intensive, demanding considerable expertise and dependence on\nestablished design automation tools. This paper discusses the possibility of\nexploiting large language models to streamline the code generation process in\nhardware design. In contrast to earlier studies, this paper aims to use large\nlanguage models that accepts high-level design specifications through a single\nprompt to generate corresponding Register-Transfer Level (RTL) code. The\nability to use large language models on RTL code generation not only expedites\ndesign iteration cycles but also facilitates the exploration of design spaces\nthat have computational challenges for conventional techniques. Through our\nevaluation, we demonstrate the shortcoming of existing attention mechanisms,\nand present the abilities of language models to produce functional, optimized,\nand industry-standard compliant RTL code when a novel attention mechanism is\nused. These findings underscore the expanding role of large language models in\nshaping the future landscape of architectural exploration and automation in\nhardware design.",
        "pdf_link": "https://arxiv.org/pdf/2401.08683v1.pdf"
    },
    {
        "title": "Tuning Language Models by Proxy",
        "authors": [
            "Alisa Liu",
            "Xiaochuang Han",
            "Yizhong Wang",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "published": "2024-01-16T18:49:55Z",
        "summary": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.",
        "pdf_link": "https://arxiv.org/pdf/2401.08565v2.pdf"
    },
    {
        "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning",
        "authors": [
            "Zheqi He",
            "Xinya Wu",
            "Pengfei Zhou",
            "Richeng Xuan",
            "Guang Liu",
            "Xi Yang",
            "Qiannan Zhu",
            "Hua Huang"
        ],
        "published": "2024-01-25T08:22:10Z",
        "summary": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose a rigorous evaluation strategy called ShiftCheck for assessing\nmultiple-choice questions. The strategy aims to reduce position bias, minimize\nthe influence of randomness on correctness, and perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.14011v2.pdf"
    },
    {
        "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "authors": [
            "Mouxiang Chen",
            "Hao Tian",
            "Zhongxin Liu",
            "Xiaoxue Ren",
            "Jianling Sun"
        ],
        "published": "2024-01-15T18:04:29Z",
        "summary": "While existing code large language models (code LLMs) exhibit impressive\ncapabilities in code generation, their autoregressive sequential generation\ninherently lacks reversibility. This limitation hinders them from timely\ncorrecting previous missing statements during coding as humans do, often\nleading to error propagation and suboptimal performance. We introduce\nJumpCoder, a novel modelagnostic framework that enables online modification and\nnon-sequential generation to augment the code LLMs. The key idea behind\nJumpCoder is to insert new code into the currently generated code when\nnecessary during generation, which is achieved through an auxiliary infilling\nmodel that works in tandem with the code LLM. Since identifying the best infill\nposition beforehand is intractable, we adopt an infill-first, judge-later\nstrategy, which experiments with filling at the $k$ most critical positions\nfollowing the generation of each line, and uses an Abstract Syntax Tree (AST)\nparser alongside the Generation Model Scoring to effectively judge the validity\nof each potential infill. Extensive experiments using six state-of-the-art code\nLLMs across multiple benchmarks consistently indicate significant improvements\nover all baselines. Notably, JumpCoder assists code LLMs in achieving up to a\n3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the\nmultilingual HumanEval benchmarks. Our code is public at\nhttps://github.com/Keytoyze/JumpCoder.",
        "pdf_link": "https://arxiv.org/pdf/2401.07870v1.pdf"
    },
    {
        "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
        "authors": [
            "Zaibin Zhang",
            "Yongting Zhang",
            "Lijun Li",
            "Hongzhi Gao",
            "Lijun Wang",
            "Huchuan Lu",
            "Feng Zhao",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-01-22T12:11:55Z",
        "summary": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit\nprofound capabilities in collective intelligence. However, the potential misuse\nof this intelligence for malicious purposes presents significant risks. To\ndate, comprehensive research on the safety issues associated with multi-agent\nsystems remains limited. In this paper, we explore these concerns through the\ninnovative lens of agent psychology, revealing that the dark psychological\nstates of agents constitute a significant threat to safety. To tackle these\nconcerns, we propose a comprehensive framework (PsySafe) grounded in agent\npsychology, focusing on three key areas: firstly, identifying how dark\npersonality traits in agents can lead to risky behaviors; secondly, evaluating\nthe safety of multi-agent systems from the psychological and behavioral\nperspectives, and thirdly, devising effective strategies to mitigate these\nrisks. Our experiments reveal several intriguing phenomena, such as the\ncollective dangerous behaviors among agents, agents' self-reflection when\nengaging in dangerous behavior, and the correlation between agents'\npsychological assessments and dangerous behaviors. We anticipate that our\nframework and observations will provide valuable insights for further research\ninto the safety of multi-agent systems. We will make our data and code publicly\naccessible at https://github.com/AI4Good24/PsySafe.",
        "pdf_link": "https://arxiv.org/pdf/2401.11880v2.pdf"
    },
    {
        "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
        "authors": [
            "Yuanchun Li",
            "Hao Wen",
            "Weijun Wang",
            "Xiangyu Li",
            "Yizhen Yuan",
            "Guohong Liu",
            "Jiacheng Liu",
            "Wenxing Xu",
            "Xiang Wang",
            "Yi Sun",
            "Rui Kong",
            "Yile Wang",
            "Hanfei Geng",
            "Jian Luan",
            "Xuefeng Jin",
            "Zilong Ye",
            "Guanjing Xiong",
            "Fan Zhang",
            "Xiang Li",
            "Mengwei Xu",
            "Zhijun Li",
            "Peng Li",
            "Yang Liu",
            "Ya-Qin Zhang",
            "Yunxin Liu"
        ],
        "published": "2024-01-10T09:25:45Z",
        "summary": "Since the advent of personal computing devices, intelligent personal\nassistants (IPAs) have been one of the key technologies that researchers and\nengineers have focused on, aiming to help users efficiently obtain information\nand execute tasks, and provide users with more intelligent, convenient, and\nrich interaction experiences. With the development of smartphones and IoT,\ncomputing and sensing devices have become ubiquitous, greatly expanding the\nboundaries of IPAs. However, due to the lack of capabilities such as user\nintent understanding, task planning, tool using, and personal data management\netc., existing IPAs still have limited practicality and scalability. Recently,\nthe emergence of foundation models, represented by large language models\n(LLMs), brings new opportunities for the development of IPAs. With the powerful\nsemantic understanding and reasoning capabilities, LLM can enable intelligent\nagents to solve complex problems autonomously. In this paper, we focus on\nPersonal LLM Agents, which are LLM-based agents that are deeply integrated with\npersonal data and personal devices and used for personal assistance. We\nenvision that Personal LLM Agents will become a major software paradigm for\nend-users in the upcoming era. To realize this vision, we take the first step\nto discuss several important questions about Personal LLM Agents, including\ntheir architecture, capability, efficiency and security. We start by\nsummarizing the key components and design choices in the architecture of\nPersonal LLM Agents, followed by an in-depth analysis of the opinions collected\nfrom domain experts. Next, we discuss several key challenges to achieve\nintelligent, efficient and secure Personal LLM Agents, followed by a\ncomprehensive survey of representative solutions to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2401.05459v1.pdf"
    },
    {
        "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
        "authors": [
            "Chengbo Yuan",
            "Chuan Wen",
            "Tong Zhang",
            "Yang Gao"
        ],
        "published": "2024-01-21T09:39:11Z",
        "summary": "We address the challenge of acquiring real-world manipulation skills with a\nscalable framework.Inspired by the success of large-scale auto-regressive\nprediction in Large Language Models (LLMs), we hold the belief that identifying\nan appropriate prediction target capable of leveraging large-scale datasets is\ncrucial for achieving efficient and universal learning. Therefore, we propose\nto utilize flow, which represents the future trajectories of 3D points on\nobjects of interest, as an ideal prediction target in robot learning. To\nexploit scalable data resources, we turn our attention to cross-embodiment\ndatasets. We develop, for the first time, a language-conditioned prediction\nmodel directly from large-scale RGBD human video datasets. Our predicted flow\noffers actionable geometric and physics guidance, thus facilitating stable\nzero-shot skill transfer in real-world scenarios.We deploy our method with a\npolicy based on closed-loop flow prediction. Remarkably, without any additional\ntraining, our method achieves an impressive 81% success rate in human-to-robot\nskill transfer, covering 18 tasks in 6 scenes. Our framework features the\nfollowing benefits: (1) scalability: leveraging cross-embodiment data\nresources; (2) universality: multiple object categories, including rigid,\narticulated, and soft bodies; (3) stable skill transfer: providing actionable\nguidance with a small inference domain-gap. These lead to a new pathway towards\nscalable general robot learning. Data, code, and model weights will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2401.11439v1.pdf"
    },
    {
        "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks",
        "authors": [
            "Ping Guo",
            "Fei Liu",
            "Xi Lin",
            "Qingchuan Zhao",
            "Qingfu Zhang"
        ],
        "published": "2024-01-27T07:57:20Z",
        "summary": "In the rapidly evolving field of machine learning, adversarial attacks\npresent a significant challenge to model robustness and security.\nDecision-based attacks, which only require feedback on the decision of a model\nrather than detailed probabilities or scores, are particularly insidious and\ndifficult to defend against. This work introduces L-AutoDA (Large Language\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\nleveraging the generative capabilities of Large Language Models (LLMs) to\nautomate the design of these attacks. By iteratively interacting with LLMs in\nan evolutionary framework, L-AutoDA automatically designs competitive attack\nalgorithms efficiently without much human effort. We demonstrate the efficacy\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\nmethods in both success rate and computational efficiency. Our findings\nunderscore the potential of language models as tools for adversarial attack\ngeneration and highlight new avenues for the development of robust AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.15335v1.pdf"
    },
    {
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "authors": [
            "Yuncheng Hua",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "published": "2024-01-29T09:07:40Z",
        "summary": "In this work, we aim to develop LLM agents to mitigate social norm violations\nin negotiations in a multi-agent setting. We simulate real-world negotiations\nby letting two large Language Models (LLMs) play the roles of two negotiators\nin each conversation. A third LLM acts as a remediation agent to rewrite\nutterances violating norms for improving negotiation outcomes. As it is a novel\ntask, no manually constructed data is available. To address this limitation, we\nintroduce a value impact based In-Context Learning (ICL) method to identify\nhigh-quality ICL examples for the LLM-based remediation agents, where the value\nimpact function measures the quality of negotiation outcomes. We show the\nconnection of this method to policy learning and provide rich empirical\nevidence to demonstrate its effectiveness in negotiations across three\ndifferent topics: product sale, housing price, and salary negotiation. The\nsource code and the generated dataset will be publicly available upon\nacceptance.",
        "pdf_link": "https://arxiv.org/pdf/2402.01737v1.pdf"
    },
    {
        "title": "Pre-trained Large Language Models for Financial Sentiment Analysis",
        "authors": [
            "Wei Luo",
            "Dihong Gong"
        ],
        "published": "2024-01-10T15:27:41Z",
        "summary": "Financial sentiment analysis refers to classifying financial text contents\ninto sentiment categories (e.g. positive, negative, and neutral). In this\npaper, we focus on the classification of financial news title, which is a\nchallenging task due to a lack of large amount of training samples. To overcome\nthis difficulty, we propose to adapt the pretrained large language models\n(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge\namount of text corpora,have an advantage in text understanding and can be\neffectively adapted to domain-specific task while requiring very few amount of\ntraining samples. In particular, we adapt the open-source Llama2-7B model\n(2023) with the supervised fine-tuning (SFT) technique [4]. Experimental\nevaluation shows that even with the 7B model (which is relatively small for\nLLMs), our approach significantly outperforms the previous state-of-the-art\nalgorithms.",
        "pdf_link": "https://arxiv.org/pdf/2401.05215v1.pdf"
    },
    {
        "title": "LightHouse: A Survey of AGI Hallucination",
        "authors": [
            "Feng Wang"
        ],
        "published": "2024-01-08T03:52:40Z",
        "summary": "With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.",
        "pdf_link": "https://arxiv.org/pdf/2401.06792v2.pdf"
    },
    {
        "title": "A Vision Check-up for Language Models",
        "authors": [
            "Pratyusha Sharma",
            "Tamar Rott Shaham",
            "Manel Baradad",
            "Stephanie Fu",
            "Adrian Rodriguez-Munoz",
            "Shivam Duggal",
            "Phillip Isola",
            "Antonio Torralba"
        ],
        "published": "2024-01-03T18:09:33Z",
        "summary": "What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01862v1.pdf"
    },
    {
        "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
        "authors": [
            "Xiyao Wang",
            "Yuhang Zhou",
            "Xiaoyu Liu",
            "Hongjin Lu",
            "Yuancheng Xu",
            "Feihong He",
            "Jaehong Yoon",
            "Taixi Lu",
            "Gedas Bertasius",
            "Mohit Bansal",
            "Huaxiu Yao",
            "Furong Huang"
        ],
        "published": "2024-01-19T07:10:13Z",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.",
        "pdf_link": "https://arxiv.org/pdf/2401.10529v2.pdf"
    },
    {
        "title": "Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data",
        "authors": [
            "Yubin Kim",
            "Xuhai Xu",
            "Daniel McDuff",
            "Cynthia Breazeal",
            "Hae Won Park"
        ],
        "published": "2024-01-12T19:40:11Z",
        "summary": "Large language models (LLMs) are capable of many natural language tasks, yet\nthey are far from perfect. In health applications, grounding and interpreting\ndomain-specific and non-linguistic data is important. This paper investigates\nthe capacity of LLMs to deliver multi-modal health predictions based on\ncontextual information (e.g. user demographics, health knowledge) and\nphysiological data (e.g. resting heart rate, sleep minutes). We present a\ncomprehensive evaluation of eight state-of-the-art LLMs with diverse prompting\nand fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps,\nGLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer\nhealth prediction tasks in mental health, activity, metabolic, sleep, and\ncardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable\nperformance to larger models (GPT-3.5 and GPT-4), achieving the best\nperformance in 5 out of 13 tasks. Ablation studies highlight the effectiveness\nof context enhancement strategies, and generalization capability of the\nfine-tuned models across training datasets and the size of training samples.\nNotably, we observe that our context enhancement can yield up to 23.8%\nimprovement in performance. While constructing contextually rich prompts\n(combining user context, health knowledge and temporal information) exhibits\nsynergistic improvement, the inclusion of health knowledge context in prompts\nsignificantly enhances overall performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.06866v1.pdf"
    },
    {
        "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension",
        "authors": [
            "Ting Jiang",
            "Shaohan Huang",
            "Shengyue Luo",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang",
            "Deqing Wang",
            "Fuzhen Zhuang"
        ],
        "published": "2024-01-14T13:11:31Z",
        "summary": "To enhance the domain-specific capabilities of large language models,\ncontinued pre-training on a domain-specific corpus is a prevalent method.\nRecent work demonstrates that adapting models using reading comprehension data\nformatted by regex-based patterns can significantly improve performance on\ndomain-specific tasks. However, regex-based patterns are incapable of parsing\nraw corpora using domain-specific knowledge. Furthermore, the question and\nanswer pairs are extracted directly from the corpus in predefined formats\noffers limited context. To address this limitation, we improve reading\ncomprehension via LLM and clustering. LLM focuses on leveraging domain\nknowledge within the corpus to refine comprehension stage, while clustering\nsupplies relevant knowledge by extending the context to enrich reading stage.\nAdditionally, our method incorporates parameter-efficient fine-tuning to\nimprove the efficiency of domain adaptation. In comparison to AdaptLLM, our\nmethod achieves an improvement exceeding 5% in domain-specific tasks. Our code\nwill available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://arxiv.org/pdf/2401.07284v2.pdf"
    },
    {
        "title": "Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access",
        "authors": [
            "Saibo Geng",
            "Berkay D\u00f6ner",
            "Chris Wendler",
            "Martin Josifoski",
            "Robert West"
        ],
        "published": "2024-01-18T13:31:24Z",
        "summary": "Constrained decoding, a technique for enforcing constraints on language model\noutputs, offers a way to control text generation without retraining or\narchitectural modifications. Its application is, however, typically restricted\nto models that give users access to next-token distributions (usually via\nsoftmax logits), which poses a limitation with blackbox large language models\n(LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a\nnovel approach to constrained decoding for blackbox LLMs, which operates\nwithout access to the logits of the blackbox LLM. SGCD utilizes a locally\nhosted auxiliary model to refine the output of an unconstrained blackbox LLM,\neffectively treating this initial output as a \"sketch\" for further elaboration.\nThis approach is complementary to traditional logit-based techniques and\nenables the application of constrained decoding in settings where full model\ntransparency is unavailable. We demonstrate the efficacy of SGCD through\nexperiments in closed information extraction and constituency parsing, showing\nhow it enhances the utility and flexibility of blackbox LLMs for complex NLP\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.09967v1.pdf"
    },
    {
        "title": "Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models",
        "authors": [
            "J. E. Eicher",
            "R. F. Irgoli\u010d"
        ],
        "published": "2024-01-29T15:43:23Z",
        "summary": "Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have\nbecome instrumental in interpreting and executing semantic-based tasks.\nUnfortunately, these models' inherent biases, akin to human cognitive biases,\nadversely affect their performance. Particularly affected is object selection\nfrom lists; a fundamental operation in digital navigation and decision-making.\nThis research critically examines these biases and quantifies the effects on a\nrepresentative list selection task. To explore these biases, we conducted a\nseries of controlled experiments, manipulating temperature, list length, object\nidentity, object type, prompt complexity, and model. This enabled us to isolate\nand measure the influence of the biases on selection behavior. Our findings\nshow that bias structure is strongly dependent on the model, with object type\nmodulating the magnitude of the effect. With a strong primacy effect, causing\nthe first objects in a list to be disproportionately represented in outputs.\nFurthermore the usage of guard rails, a prompt engineering method of ensuring a\nresponse structure, can increase bias and decrease instruction adherence when\ncombined with a selection task. The bias is ablated when the guard rail step is\nseparated from the list sampling step, lowering the complexity of each\nindividual task. The implications of this research are two-fold, practically\nproviding a guide for designing unbiased LLM applications and theoretically\nsuggesting that LLMs experience a form of cognitive load compensated for by\nincreasing bias.",
        "pdf_link": "https://arxiv.org/pdf/2402.01740v2.pdf"
    },
    {
        "title": "Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",
        "authors": [
            "Xiaoding Lu",
            "Zongyi Liu",
            "Adian Liusie",
            "Vyas Raina",
            "Vineet Mudupalli",
            "Yuwen Zhang",
            "William Beauchamp"
        ],
        "published": "2024-01-04T07:45:49Z",
        "summary": "In conversational AI research, there's a noticeable trend towards developing\nmodels with a larger number of parameters, exemplified by models like ChatGPT.\nWhile these expansive models tend to generate increasingly better chat\nresponses, they demand significant computational resources and memory. This\nstudy explores a pertinent question: Can a combination of smaller models\ncollaboratively achieve comparable or enhanced performance relative to a\nsingular large model? We introduce an approach termed \"blending\", a\nstraightforward yet effective method of integrating multiple chat AIs. Our\nempirical evidence suggests that when specific smaller models are\nsynergistically blended, they can potentially outperform or match the\ncapabilities of much larger counterparts. For instance, integrating just three\nmodels of moderate size (6B/13B paramaeters) can rival or even surpass the\nperformance metrics of a substantially larger model like ChatGPT (175B+\nparamaters). This hypothesis is rigorously tested using A/B testing\nmethodologies with a large user base on the Chai research platform over a span\nof thirty days. The findings underscore the potential of the \"blending\"\nstrategy as a viable approach for enhancing chat AI efficacy without a\ncorresponding surge in computational demands.",
        "pdf_link": "https://arxiv.org/pdf/2401.02994v3.pdf"
    },
    {
        "title": "A match made in consistency heaven: when large language models meet evolutionary algorithms",
        "authors": [
            "Wang Chao",
            "Jiaxuan Zhao",
            "Licheng Jiao",
            "Lingling Li",
            "Fang Liu",
            "Shuyuan Yang"
        ],
        "published": "2024-01-19T05:58:30Z",
        "summary": "Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text sequence generation and evolution, this\npaper illustrates the strong consistency of LLMs and EAs, which includes\nmultiple one-to-one key characteristics: token embedding and genotype-phenotype\nmapping, position encoding and fitness shaping, position embedding and\nselection, attention and crossover, feed-forward neural network and mutation,\nmodel training and parameter update, and multi-task learning and\nmulti-objective optimization. Based on this consistency perspective, existing\ncoupling studies are analyzed, including evolutionary fine-tuning and\nLLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap\nfor future research in coupling LLMs and EAs, while highlighting key challenges\nalong the way. The consistency not only reveals the evolution mechanism behind\nLLMs but also facilitates the development of evolved artificial agents that\napproach or surpass biological organisms.",
        "pdf_link": "https://arxiv.org/pdf/2401.10510v1.pdf"
    },
    {
        "title": "Towards Goal-oriented Large Language Model Prompting: A Survey",
        "authors": [
            "Haochen Li",
            "Jonathan Leung",
            "Zhiqi Shen"
        ],
        "published": "2024-01-25T09:47:55Z",
        "summary": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks in which prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not as an overview of current prompt engineering\nmethods, aims to highlight the limitation of designing prompts while holding an\nanthropomorphic assumption that expects LLMs to think like humans. From our\nreview of 35 representative studies, we demonstrate that a goal-oriented prompt\nformulation, which guides LLMs to follow established human logical thinking,\nsignificantly improves the performance of LLMs. Furthermore, We introduce a\nnovel taxonomy that categorizes goal-oriented prompting methods into five\ninterconnected stages and we demonstrate the broad applicability of our\nframework by summarizing ten applicable tasks. With four future directions\nproposed, we hope to further emphasize and promote goal-oriented prompt\nengineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.14043v1.pdf"
    },
    {
        "title": "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study",
        "authors": [
            "Shangding Gu"
        ],
        "published": "2024-01-12T14:35:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities for\nreinforcement learning (RL) models, such as planning and reasoning\ncapabilities. However, the problems of LLMs and RL model collaboration still\nneed to be solved. In this study, we employ a teacher-student learning\nframework to tackle these problems, specifically by offering feedback for LLMs\nusing RL models and providing high-level information for RL models with LLMs in\na cooperative multi-agent setting. Within this framework, the LLM acts as a\nteacher, while the RL model acts as a student. The two agents cooperatively\nassist each other through a process of recursive help, such as \"I help you help\nI help.\" The LLM agent supplies abstract information to the RL agent, enabling\nefficient exploration and policy improvement. In turn, the RL agent offers\nfeedback to the LLM agent, providing valuable, real-time information that helps\ngenerate more useful tokens. This bi-directional feedback loop promotes\noptimization, exploration, and mutual improvement for both agents, enabling\nthem to accomplish increasingly challenging tasks. Remarkably, we propose a\npractical algorithm to address the problem and conduct empirical experiments to\nevaluate the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2401.06603v1.pdf"
    },
    {
        "title": "Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis",
        "authors": [
            "Yanhong Peng",
            "Ceng Zhang",
            "Chenlong Hu",
            "Zebing Mao"
        ],
        "published": "2024-01-21T14:10:27Z",
        "summary": "This paper presents an innovative approach to integrating Large Language\nModels (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for\nprecise color synthesis in automation systems. We propose a novel framework\nthat employs fine-tuned LLMs to interpret natural language commands and convert\nthem into specific operational instructions for EHD pump control. This approach\naims to enhance user interaction with complex hardware systems, making it more\nintuitive and efficient. The methodology involves four key steps: fine-tuning\nthe language model with a dataset of color specifications and corresponding\nArduino code, developing a natural language processing interface, translating\nuser inputs into executable Arduino code, and controlling EHD pumps for\naccurate color mixing. Conceptual experiment results, based on theoretical\nassumptions, indicate a high potential for accurate color synthesis, efficient\nlanguage model interpretation, and reliable EHD pump operation. This research\nextends the application of LLMs beyond text-based tasks, demonstrating their\npotential in industrial automation and control systems. While highlighting the\nlimitations and the need for real-world testing, this study opens new avenues\nfor AI applications in physical system control and sets a foundation for future\nadvancements in AI-driven automation technologies.",
        "pdf_link": "https://arxiv.org/pdf/2401.11500v1.pdf"
    },
    {
        "title": "AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents",
        "authors": [
            "Yuanzhi Liang",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "published": "2024-01-12T11:18:00Z",
        "summary": "Large Language Models (LLMs) have demonstrated their ability to replicate\nhuman behaviors across a wide range of scenarios. However, their capability in\nhandling complex, multi-character social interactions has yet to be fully\nexplored, primarily due to the absence of robust, quantitative evaluation\nmethods. This gap has slowed the development of agents proficient in more\nnuanced interactions beyond simple exchanges, for example, small talk. To\naddress this challenge, we introduce the Multi-Agent Interaction Evaluation\nFramework (AntEval), encompassing a novel interaction framework and evaluation\nmethods. The interaction framework aims to foster an complex interaction\nenvironment that bolsters information exchange and intention expression within\nsocial interactions. Furthermore, we introduce evaluation methods, including\ntwo metrics: Information Exchanging Precision (IEP) and Interaction\nExpressiveness Gap (IEG), designed for the quantitative and objective\nassessment of agents' interaction competencies. Our findings highlight the\nutility of these evaluative methods and show significant potential for\nimproving LLMs' ability to construct agents that interact in a more natural\nmanner with human-like intricacy.",
        "pdf_link": "https://arxiv.org/pdf/2401.06509v3.pdf"
    },
    {
        "title": "Adaptive Text Watermark for Large Language Models",
        "authors": [
            "Yepeng Liu",
            "Yuheng Bu"
        ],
        "published": "2024-01-25T03:57:12Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.",
        "pdf_link": "https://arxiv.org/pdf/2401.13927v1.pdf"
    },
    {
        "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "published": "2024-01-26T18:59:01Z",
        "summary": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
        "pdf_link": "https://arxiv.org/pdf/2401.15077v2.pdf"
    },
    {
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "authors": [
            "Haoran Xu",
            "Amr Sharaf",
            "Yunmo Chen",
            "Weiting Tan",
            "Lingfeng Shen",
            "Benjamin Van Durme",
            "Kenton Murray",
            "Young Jin Kim"
        ],
        "published": "2024-01-16T15:04:51Z",
        "summary": "Moderate-sized large language models (LLMs) -- those with 7B or 13B\nparameters -- exhibit promising machine translation (MT) performance. However,\neven the top-performing 13B LLM-based translation models, like ALMA, does not\nmatch the performance of state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such as GPT-4. In this study, we bridge\nthis performance gap. We first assess the shortcomings of supervised\nfine-tuning for LLMs in the MT task, emphasizing the quality issues present in\nthe reference data, despite being human-generated. Then, in contrast to SFT\nwhich mimics reference translations, we introduce Contrastive Preference\nOptimization (CPO), a novel approach that trains models to avoid generating\nadequate but not perfect translations. Applying CPO to ALMA models with only\n22K parallel sentences and 12M parameters yields significant improvements. The\nresulting model, called ALMA-R, can match or exceed the performance of the WMT\ncompetition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.",
        "pdf_link": "https://arxiv.org/pdf/2401.08417v3.pdf"
    },
    {
        "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
        "authors": [
            "Xiaoran Fan",
            "Tao Ji",
            "Changhao Jiang",
            "Shuo Li",
            "Senjie Jin",
            "Sirui Song",
            "Junke Wang",
            "Boyang Hong",
            "Lu Chen",
            "Guodong Zheng",
            "Ming Zhang",
            "Caishuang Huang",
            "Rui Zheng",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Shihan Dou",
            "Junjie Ye",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "published": "2024-01-30T18:09:11Z",
        "summary": "Current large vision-language models (VLMs) often encounter challenges such\nas insufficient capabilities of a single visual component and excessively long\nvisual tokens. These issues can limit the model's effectiveness in accurately\ninterpreting complex visual information and over-lengthy contextual\ninformation. Addressing these challenges is crucial for enhancing the\nperformance and applicability of VLMs. This paper proposes the use of ensemble\nexperts technique to synergizes the capabilities of individual visual encoders,\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\nThis technique introduces a fusion network to unify the processing of outputs\nfrom different visual experts, while bridging the gap between image encoders\nand pre-trained LLMs. In addition, we explore different positional encoding\nschemes to alleviate the waste of positional encoding caused by lengthy image\nfeature sequences, effectively addressing the issue of position overflow and\nlength limitations. For instance, in our implementation, this technique\nsignificantly reduces the positional occupancy in models like SAM, from a\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\nExperimental results demonstrate that VLMs with multiple experts exhibit\nconsistently superior performance over isolated visual encoders and mark a\nsignificant performance boost as more experts are integrated. We have\nopen-sourced the training code used in this report. All of these resources can\nbe found on our project website.",
        "pdf_link": "https://arxiv.org/pdf/2401.17221v1.pdf"
    },
    {
        "title": "Identifying and Analyzing Task-Encoding Tokens in Large Language Models",
        "authors": [
            "Yu Bai",
            "Heyan Huang",
            "Cesare Spinoso-Di Piano",
            "Marc-Antoine Rondeau",
            "Sanxing Chen",
            "Yang Gao",
            "Jackie Chi Kit Cheung"
        ],
        "published": "2024-01-20T20:55:21Z",
        "summary": "In-context learning (ICL) has become an effective solution for few-shot\nlearning in natural language processing. However, our understanding of ICL's\nworking mechanisms is limited, specifically regarding how models learn to\nperform tasks from ICL demonstrations. For example, unexpectedly large changes\nin performance can arise from small changes in the prompt, leaving prompt\ndesign a largely empirical endeavour. In this paper, we investigate this\nproblem by identifying and analyzing task-encoding tokens on whose\nrepresentations the task performance depends. Using experiments that ablate the\nrepresentations of different token types, we find that template and stopword\ntokens are the most prone to be task-encoding. In addition, we demonstrate\nexperimentally that lexical meaning, repetition, and text formatting are the\nmain distinguishing characteristics of these tokens. Our work sheds light on\nhow large language models (LLMs) learn to perform a task from demonstrations,\ndeepens our understanding of the varied roles different types of tokens play in\nLLMs, and provides insights for avoiding instability from improperly utilizing\ntask-encoding tokens.",
        "pdf_link": "https://arxiv.org/pdf/2401.11323v2.pdf"
    },
    {
        "title": "Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM",
        "authors": [
            "Gabriel Ryan",
            "Siddhartha Jain",
            "Mingyue Shang",
            "Shiqi Wang",
            "Xiaofei Ma",
            "Murali Krishna Ramanathan",
            "Baishakhi Ray"
        ],
        "published": "2024-01-31T18:21:49Z",
        "summary": "Testing plays a pivotal role in ensuring software quality, yet conventional\nSearch Based Software Testing (SBST) methods often struggle with complex\nsoftware units, achieving suboptimal test coverage. Recent works using large\nlanguage models (LLMs) for test generation have focused on improving generation\nquality through optimizing the test generation context and correcting errors in\nmodel outputs, but use fixed prompting strategies that prompt the model to\ngenerate tests without additional guidance. As a result LLM-generated\ntestsuites still suffer from low coverage. In this paper, we present SymPrompt,\na code-aware prompting strategy for LLMs in test generation. SymPrompt's\napproach is based on recent work that demonstrates LLMs can solve more complex\nlogical problems when prompted to reason about the problem in a multi-step\nfashion. We apply this methodology to test generation by deconstructing the\ntestsuite generation process into a multi-stage sequence, each of which is\ndriven by a specific prompt aligned with the execution paths of the method\nunder test, and exposing relevant type and dependency focal context to the\nmodel. Our approach enables pretrained LLMs to generate more complete test\ncases without any additional training. We implement SymPrompt using the\nTreeSitter parsing framework and evaluate on a benchmark challenging methods\nfrom open source Python projects. SymPrompt enhances correct test generations\nby a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably,\nwhen applied to GPT-4, SymPrompt improves coverage by over 2x compared to\nbaseline prompting strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.00097v2.pdf"
    },
    {
        "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
        "authors": [
            "Elias Stengel-Eskin",
            "Archiki Prasad",
            "Mohit Bansal"
        ],
        "published": "2024-01-29T18:45:30Z",
        "summary": "While large language models (LLMs) are increasingly being used for program\nsynthesis, they lack the global view needed to develop useful abstractions;\nthey generally predict programs one at a time, often repeating the same\nfunctionality. Generating redundant code from scratch is both inefficient and\nerror-prone. To address this, we propose Refactoring for Generalizable\nAbstraction Learning (ReGAL), a gradient-free method for learning a library of\nreusable functions via code refactorization, i.e. restructuring code without\nchanging its execution output. ReGAL learns from a small set of existing\nprograms, iteratively verifying and refining its abstractions via execution. We\nfind that the shared function libraries discovered by ReGAL make programs\neasier to predict across diverse domains. On three datasets (LOGO graphics\ngeneration, Date reasoning, and TextCraft, a Minecraft-based text game), both\nopen-source and proprietary LLMs improve in accuracy when predicting programs\nwith ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy\nincreases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on\nTextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals\nReGAL's abstractions encapsulate frequently-used subroutines as well as\nenvironment dynamics.",
        "pdf_link": "https://arxiv.org/pdf/2401.16467v1.pdf"
    },
    {
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "published": "2024-01-08T02:22:04Z",
        "summary": "With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2401.03630v2.pdf"
    },
    {
        "title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do",
        "authors": [
            "William Kidder",
            "Jason D'Cruz",
            "Kush R. Varshney"
        ],
        "published": "2024-01-25T21:30:06Z",
        "summary": "Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.",
        "pdf_link": "https://arxiv.org/pdf/2401.14523v1.pdf"
    },
    {
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "authors": [
            "Shengbang Tong",
            "Zhuang Liu",
            "Yuexiang Zhai",
            "Yi Ma",
            "Yann LeCun",
            "Saining Xie"
        ],
        "published": "2024-01-11T18:58:36Z",
        "summary": "Is vision good enough for language? Recent advancements in multimodal models\nprimarily stem from the powerful reasoning abilities of large language models\n(LLMs). However, the visual component typically depends only on the\ninstance-level contrastive language-image pre-training (CLIP). Our research\nreveals that the visual capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings. To understand the roots of these errors, we\nexplore the gap between the visual embedding space of CLIP and vision-only\nself-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP\nperceives as similar despite their clear visual differences. With these pairs,\nwe construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes\nareas where state-of-the-art systems, including GPT-4V, struggle with\nstraightforward questions across nine basic visual patterns, often providing\nincorrect answers and hallucinated explanations. We further evaluate various\nCLIP-based vision-and-language models and found a notable correlation between\nvisual patterns that challenge CLIP models and those problematic for multimodal\nLLMs. As an initial effort to address these issues, we propose a Mixture of\nFeatures (MoF) approach, demonstrating that integrating vision self-supervised\nlearning features with MLLMs can significantly enhance their visual grounding\ncapabilities. Together, our research suggests visual representation learning\nremains an open challenge, and accurate visual grounding is crucial for future\nsuccessful multimodal systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.06209v1.pdf"
    },
    {
        "title": "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",
        "authors": [
            "Claudio Novelli",
            "Federico Casolari",
            "Philipp Hacker",
            "Giorgio Spedicato",
            "Luciano Floridi"
        ],
        "published": "2024-01-14T19:16:29Z",
        "summary": "The advent of Generative AI, particularly through Large Language Models\n(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI\nlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,\nthereby broadening their application scope. However, the complexity and\nemergent autonomy of these models introduce challenges in predictability and\nlegal compliance. This paper delves into the legal and regulatory implications\nof Generative AI and LLMs in the European Union context, analyzing aspects of\nliability, privacy, intellectual property, and cybersecurity. It critically\nexamines the adequacy of the existing and proposed EU legislation, including\nthe Artificial Intelligence Act (AIA) draft, in addressing the unique\nchallenges posed by Generative AI in general and LLMs in particular. The paper\nidentifies potential gaps and shortcomings in the legislative framework and\nproposes recommendations to ensure the safe and compliant deployment of\ngenerative models, ensuring they align with the EU's evolving digital landscape\nand legal standards.",
        "pdf_link": "https://arxiv.org/pdf/2401.07348v4.pdf"
    },
    {
        "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain",
        "authors": [
            "Bingchao Wang"
        ],
        "published": "2024-01-10T02:59:49Z",
        "summary": "Recently, various Large Language Models (LLMs) evaluation datasets have\nemerged, but most of them have issues with distorted rankings and difficulty in\nmodel capabilities analysis. Addressing these concerns, this paper introduces\nANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\nKeypoint categorization standard for the first time, each question in ANGO can\ncorrespond to multiple keypoints, effectively enhancing interpretability of\nevaluation results. Base on performance of real humans, we build a quantifiable\nquestion difficulty standard and divide ANGO questions into 9 difficulty\nlevels, which provide more precise guidance for model training. To minimize\ndata leakage impact and fully leverage ANGO's innovative features, we have\nengineered exclusive sampling strategies and a new evaluation framework that\nsupport swift testset iteration. Our experiments demonstrate that ANGO poses a\nstronger challenge to models and reveals more details in evaluation result\ncompared to existing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.04898v2.pdf"
    },
    {
        "title": "Detecting mental disorder on social media: a ChatGPT-augmented explainable approach",
        "authors": [
            "Loris Belcastro",
            "Riccardo Cantini",
            "Fabrizio Marozzo",
            "Domenico Talia",
            "Paolo Trunfio"
        ],
        "published": "2024-01-30T22:22:55Z",
        "summary": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.",
        "pdf_link": "https://arxiv.org/pdf/2401.17477v1.pdf"
    },
    {
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
        "authors": [
            "Haonan Wang",
            "Qianli Shen",
            "Yao Tong",
            "Yang Zhang",
            "Kenji Kawaguchi"
        ],
        "published": "2024-01-07T08:37:29Z",
        "summary": "The commercialization of diffusion models, renowned for their ability to\ngenerate high-quality images that are often indistinguishable from real ones,\nbrings forth potential copyright concerns. Although attempts have been made to\nimpede unauthorized access to copyrighted material during training and to\nsubsequently prevent DMs from generating copyrighted images, the effectiveness\nof these solutions remains unverified. This study explores the vulnerabilities\nassociated with copyright protection in DMs by introducing a backdoor data\npoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.\nOur attack method operates without requiring access to or control over the\ndiffusion model's training or fine-tuning processes; it merely involves the\ninsertion of poisoning data into the clean training dataset. This data,\ncomprising poisoning images equipped with prompts, is generated by leveraging\nthe powerful capabilities of multimodal large language models and text-guided\nimage inpainting techniques. Our experimental results and analysis confirm the\nmethod's effectiveness. By integrating a minor portion of\nnon-copyright-infringing stealthy poisoning data into the clean\ndataset-rendering it free from suspicion-we can prompt the finetuned diffusion\nmodels to produce copyrighted content when activated by specific trigger\nprompts. These findings underline potential pitfalls in the prevailing\ncopyright protection strategies and underscore the necessity for increased\nscrutiny and preventative measures against the misuse of DMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04136v1.pdf"
    },
    {
        "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
        "authors": [
            "Bowen Zhao",
            "Hannaneh Hajishirzi",
            "Qingqing Cao"
        ],
        "published": "2024-01-22T18:39:40Z",
        "summary": "Fine-tuning and inference with large Language Models (LM) are generally known\nto be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces\ntraining memory by updating a small number of LM parameters but does not\nimprove inference efficiency. Structured pruning improves LM inference\nefficiency by removing consistent parameter blocks, yet often increases\ntraining memory and time. To improve both training and inference efficiency, we\nintroduce APT that adaptively prunes and tunes parameters for the LMs. At the\nearly stage of fine-tuning, APT dynamically adds salient tuning parameters for\nfast and accurate convergence while discarding unimportant parameters for\nefficiency. Compared to baselines, our experiments show that APT maintains up\nto 98% task performance when pruning RoBERTa and T5 models with 40% parameters\nleft while keeping 86.4% LLaMA models' performance with 70% parameters\nremained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces\nlarge LMs memory training footprint by up to 70%.",
        "pdf_link": "https://arxiv.org/pdf/2401.12200v1.pdf"
    },
    {
        "title": "PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models",
        "authors": [
            "Jiaxuan Li",
            "Minxi Yang",
            "Dahua Gao",
            "Wenlong Xu",
            "Guangming Shi"
        ],
        "published": "2024-01-30T06:55:17Z",
        "summary": "Current communication technologies face limitations in terms of theoretical\ncapacity, spectrum availability, and power resources. Pragmatic communication,\nleveraging terminal intelligence for selective data transmission, offers\nresource conservation. Existing research lacks universal intention resolution\ntools, limiting applicability to specific tasks. This paper proposes an image\npragmatic communication framework based on a Pragmatic Agent for Communication\nEfficiency (PACE) using Large Language Models (LLM). In this framework, PACE\nsequentially performs semantic perception, intention resolution, and\nintention-oriented coding. To ensure the effective utilization of LLM in\ncommunication, a knowledge base is designed to supplement the necessary\nknowledge, dedicated prompts are introduced to facilitate understanding of\npragmatic communication scenarios and task requirements, and a chain of thought\nis designed to assist in making reasonable trade-offs between transmission\nefficiency and cost. For experimental validation, this paper constructs an\nimage pragmatic communication dataset along with corresponding evaluation\nstandards. Simulation results indicate that the proposed method outperforms\ntraditional and non-LLM-based pragmatic communication in terms of transmission\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.01750v1.pdf"
    },
    {
        "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
        "authors": [
            "Li Lucy",
            "Suchin Gururangan",
            "Luca Soldaini",
            "Emma Strubell",
            "David Bamman",
            "Lauren Klein",
            "Jesse Dodge"
        ],
        "published": "2024-01-12T07:10:10Z",
        "summary": "Large language models' (LLMs) abilities are drawn from their pretraining\ndata, and model development begins with data curation. However, decisions\naround what data is retained or removed during this initial stage is\nunder-scrutinized. In our work, we ground web text, which is a popular\npretraining data source, to its social and geographic contexts. We create a new\ndataset of 10.3 million self-descriptions of website creators, and extract\ninformation about who they are and where they are from: their topical\ninterests, social roles, and geographic affiliations. Then, we conduct the\nfirst study investigating how ten \"quality\" and English language identification\n(langID) filters affect webpages that vary along these social dimensions. Our\nexperiments illuminate a range of implicit preferences in data curation: we\nshow that some quality classifiers act like topical domain filters, and langID\ncan overlook English content from some regions of the world. Overall, we hope\nthat our work will encourage a new line of research on pretraining data\ncuration practices and its social implications.",
        "pdf_link": "https://arxiv.org/pdf/2401.06408v2.pdf"
    },
    {
        "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
        "authors": [
            "Jason Toy",
            "Josh MacAdam",
            "Phil Tabor"
        ],
        "published": "2024-01-09T15:00:47Z",
        "summary": "Recent advances in Large Language Models (LLMs) have shown impressive\ncapabilities in various applications, yet LLMs face challenges such as limited\ncontext windows and difficulties in generalization. In this paper, we introduce\na metacognition module for generative agents, enabling them to observe their\nown thought processes and actions. This metacognitive approach, designed to\nemulate System 1 and System 2 cognitive processes, allows agents to\nsignificantly enhance their performance by modifying their strategy. We tested\nthe metacognition module on a variety of scenarios, including a situation where\ngenerative agents must survive a zombie apocalypse, and observe that our system\noutperform others, while agents adapt and improve their strategies to complete\ntasks over time.",
        "pdf_link": "https://arxiv.org/pdf/2401.10910v2.pdf"
    },
    {
        "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "authors": [
            "Ling Yang",
            "Zhaochen Yu",
            "Chenlin Meng",
            "Minkai Xu",
            "Stefano Ermon",
            "Bin Cui"
        ],
        "published": "2024-01-22T06:16:29Z",
        "summary": "Diffusion models have exhibit exceptional performance in text-to-image\ngeneration and editing. However, existing methods often face challenges when\nhandling complex text prompts that involve multiple objects with multiple\nattributes and relationships. In this paper, we propose a brand new\ntraining-free text-to-image generation/editing framework, namely Recaption,\nPlan and Generate (RPG), harnessing the powerful chain-of-thought reasoning\nability of multimodal LLMs to enhance the compositionality of text-to-image\ndiffusion models. Our approach employs the MLLM as a global planner to\ndecompose the process of generating complex images into multiple simpler\ngeneration tasks within subregions. We propose complementary regional diffusion\nto enable region-wise compositional generation. Furthermore, we integrate\ntext-guided image generation and editing within the proposed RPG in a\nclosed-loop fashion, thereby enhancing generalization ability. Extensive\nexperiments demonstrate our RPG outperforms state-of-the-art text-to-image\ndiffusion models, including DALL-E 3 and SDXL, particularly in multi-category\nobject composition and text-image semantic alignment. Notably, our RPG\nframework exhibits wide compatibility with various MLLM architectures (e.g.,\nMiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available\nat: https://github.com/YangLing0818/RPG-DiffusionMaster",
        "pdf_link": "https://arxiv.org/pdf/2401.11708v2.pdf"
    },
    {
        "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across Time",
        "authors": [
            "Lyle Muller",
            "Patricia S. Churchland",
            "Terrence J. Sejnowski"
        ],
        "published": "2024-01-25T16:01:49Z",
        "summary": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence into a long\n\"encoding vector\" - that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity, traveling across single cortical regions or across\nmultiple regions at the whole-brain scale, could implement a similar encoding\nprinciple. By encapsulating recent input history into a single spatial pattern\nat each moment in time, cortical waves may enable temporal context to be\nextracted from sequences of sensory inputs, the same computational principle\nused in transformers.",
        "pdf_link": "https://arxiv.org/pdf/2401.14267v1.pdf"
    },
    {
        "title": "\u03b4-CAUSAL: Exploring Defeasibility in Causal Reasoning",
        "authors": [
            "Shaobo Cui",
            "Lazar Milikic",
            "Yiyang Feng",
            "Mete Ismayilzada",
            "Debjit Paul",
            "Antoine Bosselut",
            "Boi Faltings"
        ],
        "published": "2024-01-06T10:08:33Z",
        "summary": "Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present {\\delta}-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n{\\delta}-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin {\\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by {\\delta}-CAUSAL.",
        "pdf_link": "https://arxiv.org/pdf/2401.03183v1.pdf"
    },
    {
        "title": "Arrows of Time for Large Language Models",
        "authors": [
            "Vassilis Papadopoulos",
            "J\u00e9r\u00e9mie Wenger",
            "Cl\u00e9ment Hongler"
        ],
        "published": "2024-01-30T23:46:35Z",
        "summary": "We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models through the angle of time directionality. We empirically find a\ntime asymmetry exhibited by such models in their ability to model natural\nlanguage: a difference in the average log-perplexity when trying to predict the\nnext token versus when trying to predict the previous one. This difference is\nat the same time subtle and very consistent across various modalities\n(language, model size, training time, ...). Theoretically, this is surprising:\nfrom an information-theoretic point of view, there should be no such\ndifference. We provide a theoretical framework to explain how such an asymmetry\ncan appear from sparsity and computational complexity considerations, and\noutline a number of perspectives opened by our results.",
        "pdf_link": "https://arxiv.org/pdf/2401.17505v2.pdf"
    },
    {
        "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
        "authors": [
            "Adrian Theuma",
            "Ehsan Shareghi"
        ],
        "published": "2024-01-27T07:08:37Z",
        "summary": "Large language models (LLMs) have exhibited an array of reasoning\ncapabilities but face challenges like error propagation and hallucination,\nparticularly in specialised areas like finance, where data is heterogeneous,\nand precision is paramount. We explore the potential of language model\naugmentation with external tools to mitigate these limitations and offload\ncertain reasoning steps to external tools that are more suited for the task,\ninstead of solely depending on the LLM's inherent abilities. More concretely,\nusing financial domain question-answering datasets, we apply supervised\nfine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and\n'task solver'. The 'task router' dynamically directs a question to either be\nanswered internally by the LLM or externally via the right tool from the tool\nset. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%\nand 5.06% over the base model and SFT-only baselines, respectively, and is\nhighly competitive with strong GPT-3.5 results. To the best of our knowledge,\nour work is the first that investigates tool augmentation of language models\nfor the finance domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.15328v2.pdf"
    },
    {
        "title": "Leveraging Print Debugging to Improve Code Generation in Large Language Models",
        "authors": [
            "Xueyu Hu",
            "Kun Kuang",
            "Jiankai Sun",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "published": "2024-01-10T18:37:59Z",
        "summary": "Large language models (LLMs) have made significant progress in code\ngeneration tasks, but their performance in tackling programming problems with\ncomplex data structures and algorithms remains suboptimal. To address this\nissue, we propose an in-context learning approach that guides LLMs to debug by\nusing a \"print debugging\" method, which involves inserting print statements to\ntrace and analysing logs for fixing the bug. We collect a Leetcode problem\ndataset and evaluate our method using the Leetcode online judging system.\nExperiments with GPT-4 demonstrate the effectiveness of our approach,\noutperforming rubber duck debugging in easy and medium-level Leetcode problems\nby 1.5% and 17.9%.",
        "pdf_link": "https://arxiv.org/pdf/2401.05319v1.pdf"
    },
    {
        "title": "Under the Surface: Tracking the Artifactuality of LLM-Generated Data",
        "authors": [
            "Debarati Das",
            "Karin De Langis",
            "Anna Martin-Boyle",
            "Jaehyung Kim",
            "Minhwa Lee",
            "Zae Myung Kim",
            "Shirley Anugrah Hayati",
            "Risako Owan",
            "Bin Hu",
            "Ritik Parkar",
            "Ryan Koo",
            "Jonginn Park",
            "Aahan Tyagi",
            "Libby Ferland",
            "Sanjali Roy",
            "Vincent Liu",
            "Dongyeop Kang"
        ],
        "published": "2024-01-26T07:53:27Z",
        "summary": "This work delves into the expanding role of large language models (LLMs) in\ngenerating artificial data. LLMs are increasingly employed to create a variety\nof outputs, including annotations, preferences, instruction prompts, simulated\ndialogues, and free text. As these forms of LLM-generated data often intersect\nin their application, they exert mutual influence on each other and raise\nsignificant concerns about the quality and diversity of the artificial data\nincorporated into training cycles, leading to an artificial data ecosystem. To\nthe best of our knowledge, this is the first study to aggregate various types\nof LLM-generated text data, from more tightly constrained data like \"task\nlabels\" to more lightly constrained \"free-form text\". We then stress test the\nquality and implications of LLM-generated artificial data, comparing it with\nhuman data across various existing benchmarks. Despite artificial data's\ncapability to match human performance, this paper reveals significant hidden\ndisparities, especially in complex tasks where LLMs often miss the nuanced\nunderstanding of intrinsic human-generated content. This study critically\nexamines diverse LLM-generated data and emphasizes the need for ethical\npractices in data creation and when using LLMs. It highlights the LLMs'\nshortcomings in replicating human traits and behaviors, underscoring the\nimportance of addressing biases and artifacts produced in LLM-generated content\nfor future research and development. All data and code are available on our\nproject page.",
        "pdf_link": "https://arxiv.org/pdf/2401.14698v2.pdf"
    },
    {
        "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes",
        "authors": [
            "Darren Liu",
            "Cheng Ding",
            "Delgersuren Bold",
            "Monique Bouvier",
            "Jiaying Lu",
            "Benjamin Shickel",
            "Craig S. Jabaley",
            "Wenhui Zhang",
            "Soojin Park",
            "Michael J. Young",
            "Mark S. Wainwright",
            "Gilles Clermont",
            "Parisa Rashidi",
            "Eric S. Rosenthal",
            "Laurie Dimisko",
            "Ran Xiao",
            "Joo Heung Yoon",
            "Carl Yang",
            "Xiao Hu"
        ],
        "published": "2024-01-24T16:52:37Z",
        "summary": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.13588v1.pdf"
    },
    {
        "title": "Improving Classification Performance With Human Feedback: Label a few, we label the rest",
        "authors": [
            "Natan Vidra",
            "Thomas Clifford",
            "Katherine Jijo",
            "Eden Chung",
            "Liang Zhang"
        ],
        "published": "2024-01-17T19:13:05Z",
        "summary": "In the realm of artificial intelligence, where a vast majority of data is\nunstructured, obtaining substantial amounts of labeled data to train supervised\nmachine learning models poses a significant challenge. To address this, we\ndelve into few-shot and active learning, where are goal is to improve AI models\nwith human feedback on a few labeled examples. This paper focuses on\nunderstanding how a continuous feedback loop can refine models, thereby\nenhancing their accuracy, recall, and precision through incremental human\ninput. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and\nSetFit, we aim to analyze the efficacy of using a limited number of labeled\nexamples to substantially improve model accuracy. We benchmark this approach on\nthe Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to\nprove that with just a few labeled examples, we are able to surpass the\naccuracy of zero shot large language models to provide enhanced text\nclassification performance. We demonstrate that rather than needing to manually\nlabel millions of rows of data, we just need to label a few and the model can\neffectively predict the rest.",
        "pdf_link": "https://arxiv.org/pdf/2401.09555v1.pdf"
    },
    {
        "title": "Entity Recognition from Colloquial Text",
        "authors": [
            "Tamara Babaian",
            "Jennifer Xu"
        ],
        "published": "2024-01-09T23:52:32Z",
        "summary": "Extraction of concepts and entities of interest from non-formal texts such as\nsocial media posts and informal communication is an important capability for\ndecision support systems in many domains, including healthcare, customer\nrelationship management, and others. Despite the recent advances in training\nlarge language models for a variety of natural language processing tasks, the\ndeveloped models and techniques have mainly focused on formal texts and do not\nperform as well on colloquial data, which is characterized by a number of\ndistinct challenges. In our research, we focus on the healthcare domain and\ninvestigate the problem of symptom recognition from colloquial texts by\ndesigning and evaluating several training strategies for BERT-based model\nfine-tuning. These strategies are distinguished by the choice of the base\nmodel, the training corpora, and application of term perturbations in the\ntraining data. The best-performing models trained using these strategies\noutperform the state-of-the-art specialized symptom recognizer by a large\nmargin. Through a series of experiments, we have found specific patterns of\nmodel behavior associated with the training strategies we designed. We present\ndesign principles for training strategies for effective entity recognition in\ncolloquial texts based on our findings.",
        "pdf_link": "https://arxiv.org/pdf/2401.04853v1.pdf"
    },
    {
        "title": "Generalist embedding models are better at short-context clinical semantic search than specialized embedding models",
        "authors": [
            "Jean-Baptiste Excoffier",
            "Tom Roehr",
            "Alexei Figueroa",
            "Jens-Michalis Papaioannou",
            "Keno Bressem",
            "Matthieu Ortala"
        ],
        "published": "2024-01-03T19:03:32Z",
        "summary": "The increasing use of tools and solutions based on Large Language Models\n(LLMs) for various tasks in the medical domain has become a prominent trend.\nTheir use in this highly critical and sensitive domain has thus raised\nimportant questions about their robustness, especially in response to\nvariations in input, and the reliability of the generated outputs. This study\naddresses these questions by constructing a textual dataset based on the\nICD-10-CM code descriptions, widely used in US hospitals and containing many\nclinical terms, and their easily reproducible rephrasing. We then benchmarked\nexisting embedding models, either generalist or specialized in the clinical\ndomain, in a semantic search task where the goal was to correctly match the\nrephrased text to the original description. Our results showed that generalist\nmodels performed better than clinical models, suggesting that existing clinical\nspecialized models are more sensitive to small changes in input that confuse\nthem. The highlighted problem of specialized models may be due to the fact that\nthey have not been trained on sufficient data, and in particular on datasets\nthat are not diverse enough to have a reliable global language understanding,\nwhich is still necessary for accurate handling of medical documents.",
        "pdf_link": "https://arxiv.org/pdf/2401.01943v2.pdf"
    },
    {
        "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
        "authors": [
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zhengwen Zhang",
            "Xiangrui Meng",
            "Sirui Hong",
            "Wenhao Li",
            "Zihao Wang",
            "Zekai Wang",
            "Feng Yin",
            "Junhua Zhao",
            "Xiuqiang He"
        ],
        "published": "2024-01-07T09:08:24Z",
        "summary": "Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.",
        "pdf_link": "https://arxiv.org/pdf/2401.03428v1.pdf"
    },
    {
        "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
        "authors": [
            "Haritz Puerto",
            "Martin Tutek",
            "Somak Aditya",
            "Xiaodan Zhu",
            "Iryna Gurevych"
        ],
        "published": "2024-01-18T15:32:24Z",
        "summary": "Reasoning is a fundamental component of language understanding. Recent\nprompting techniques, such as chain of thought, have consistently improved\nLLMs' performance on various reasoning tasks. Nevertheless, there is still\nlittle understanding of what triggers reasoning abilities in LLMs in the\ninference stage. In this paper, we introduce code prompting, a chain of prompts\nthat transforms a natural language problem into code and directly prompts the\nLLM using the generated code without resorting to external code execution. We\nhypothesize that code prompts can elicit certain reasoning capabilities of LLMs\ntrained on text and code and utilize the proposed method to improve conditional\nreasoning, the ability to infer different conclusions depending on the\nfulfillment of certain conditions. We find that code prompting exhibits a\nhigh-performance boost for multiple LLMs (up to 22.52 percentage points on GPT\n3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional\nreasoning datasets. We then conduct comprehensive experiments to understand how\ncode prompts trigger reasoning abilities and which capabilities are elicited in\nthe underlying models. Our analysis of GPT 3.5 reveals that the code formatting\nof the input problem is essential for performance improvement. Furthermore,\ncode prompts improve sample efficiency of in-context learning and facilitate\nstate tracking of variables or entities.",
        "pdf_link": "https://arxiv.org/pdf/2401.10065v2.pdf"
    },
    {
        "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Yuze He",
            "Ji Qi",
            "Lei Hou",
            "Jie Tang",
            "Yuxiao Dong",
            "Juanzi Li"
        ],
        "published": "2024-01-31T18:29:39Z",
        "summary": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
        "pdf_link": "https://arxiv.org/pdf/2401.18058v1.pdf"
    },
    {
        "title": "CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs",
        "authors": [
            "Majeed Kazemitabaar",
            "Runlong Ye",
            "Xiaoning Wang",
            "Austin Z. Henley",
            "Paul Denny",
            "Michelle Craig",
            "Tovi Grossman"
        ],
        "published": "2024-01-20T20:14:42Z",
        "summary": "Timely, personalized feedback is essential for students learning programming.\nLLM-powered tools like ChatGPT offer instant support, but reveal direct answers\nwith code, which may hinder deep conceptual engagement. We developed CodeAid,\nan LLM-powered programming assistant delivering helpful, technically correct\nresponses, without revealing code solutions. CodeAid answers conceptual\nquestions, generates pseudo-code with line-by-line explanations, and annotates\nstudent's incorrect code with fix suggestions. We deployed CodeAid in a\nprogramming class of 700 students for a 12-week semester. A thematic analysis\nof 8,000 usages of CodeAid was performed, further enriched by weekly surveys,\nand 22 student interviews. We then interviewed eight programming educators to\ngain further insights. Our findings reveal four design considerations for\nfuture educational AI assistants: D1) exploiting AI's unique benefits; D2)\nsimplifying query formulation while promoting cognitive engagement; D3)\navoiding direct responses while encouraging motivated learning; and D4)\nmaintaining transparency and control for students to asses and steer AI\nresponses.",
        "pdf_link": "https://arxiv.org/pdf/2401.11314v2.pdf"
    },
    {
        "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
        "authors": [
            "Fu Li",
            "Xueying Wang",
            "Bin Li",
            "Yunlong Wu",
            "Yanzhen Wang",
            "Xiaodong Yi"
        ],
        "published": "2024-01-16T03:28:29Z",
        "summary": "This paper presents an innovative exploration of the application potential of\nlarge language models (LLM) in addressing the challenging task of automatically\ngenerating behavior trees (BTs) for complex tasks. The conventional manual BT\ngeneration method is inefficient and heavily reliant on domain expertise. On\nthe other hand, existing automatic BT generation technologies encounter\nbottlenecks related to task complexity, model adaptability, and reliability. In\norder to overcome these challenges, we propose a novel methodology that\nleverages the robust representation and reasoning abilities of LLMs. The core\ncontribution of this paper lies in the design of a BT generation framework\nbased on LLM, which encompasses the entire process, from data synthesis and\nmodel training to application developing and data verification. Synthetic data\nis introduced to train the BT generation model (BTGen model), enhancing its\nunderstanding and adaptability to various complex tasks, thereby significantly\nimproving its overall performance. In order to ensure the effectiveness and\nexecutability of the generated BTs, we emphasize the importance of data\nverification and introduce a multilevel verification strategy. Additionally, we\nexplore a range of agent design and development schemes with LLM as the central\nelement. We hope that the work in this paper may provide a reference for the\nresearchers who are interested in BT generation based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.08089v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on Controllable Generation under Diversified Instructions",
        "authors": [
            "Yihan Chen",
            "Benfeng Xu",
            "Quan Wang",
            "Yi Liu",
            "Zhendong Mao"
        ],
        "published": "2024-01-01T07:35:31Z",
        "summary": "While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2401.00690v1.pdf"
    },
    {
        "title": "An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search",
        "authors": [
            "Fei Liu",
            "Xialiang Tong",
            "Mingxuan Yuan",
            "Xi Lin",
            "Fu Luo",
            "Zhenkun Wang",
            "Zhichao Lu",
            "Qingfu Zhang"
        ],
        "published": "2024-01-04T04:11:59Z",
        "summary": "It is often very tedious for human experts to design efficient algorithms.\nRecently, we have proposed a novel Algorithm Evolution using Large Language\nModel (AEL) framework for automatic algorithm design. AEL combines the power of\na large language model and the paradigm of evolutionary computation to design,\ncombine, and modify algorithms automatically. In this paper, we use AEL to\ndesign the guide algorithm for guided local search (GLS) to solve the\nwell-known traveling salesman problem (TSP). AEL automatically evolves elite\nGLS algorithms in two days, with minimal human effort and no model training.\nExperimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show\nthat AEL-designed GLS outperforms state-of-the-art human-designed GLS with the\nsame iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap\non TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in\nautomatic algorithm design.",
        "pdf_link": "https://arxiv.org/pdf/2401.02051v1.pdf"
    },
    {
        "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
        "authors": [
            "Eva Vanmassenhove"
        ],
        "published": "2024-01-18T14:34:49Z",
        "summary": "This chapter examines the role of Machine Translation in perpetuating gender\nbias, highlighting the challenges posed by cross-linguistic settings and\nstatistical dependencies. A comprehensive overview of relevant existing work\nrelated to gender bias in both conventional Neural Machine Translation\napproaches and Generative Pretrained Transformer models employed as Machine\nTranslation systems is provided. Through an experiment using ChatGPT (based on\nGPT-3.5) in an English-Italian translation context, we further assess ChatGPT's\ncurrent capacity to address gender bias. The findings emphasize the ongoing\nneed for advancements in mitigating bias in Machine Translation systems and\nunderscore the importance of fostering fairness and inclusivity in language\ntechnologies.",
        "pdf_link": "https://arxiv.org/pdf/2401.10016v1.pdf"
    },
    {
        "title": "Pheme: Efficient and Conversational Speech Generation",
        "authors": [
            "Pawe\u0142 Budzianowski",
            "Taras Sereda",
            "Tomasz Cichy",
            "Ivan Vuli\u0107"
        ],
        "published": "2024-01-05T14:47:20Z",
        "summary": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.",
        "pdf_link": "https://arxiv.org/pdf/2401.02839v1.pdf"
    },
    {
        "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications",
        "authors": [
            "Xuchen Suo"
        ],
        "published": "2024-01-15T11:44:18Z",
        "summary": "The critical challenge of prompt injection attacks in Large Language Models\n(LLMs) integrated applications, a growing concern in the Artificial\nIntelligence (AI) field. Such attacks, which manipulate LLMs through natural\nlanguage inputs, pose a significant threat to the security of these\napplications. Traditional defense strategies, including output and input\nfiltering, as well as delimiter use, have proven inadequate. This paper\nintroduces the 'Signed-Prompt' method as a novel solution. The study involves\nsigning sensitive instructions within command segments by authorized users,\nenabling the LLM to discern trusted instruction sources. The paper presents a\ncomprehensive analysis of prompt injection attack patterns, followed by a\ndetailed explanation of the Signed-Prompt concept, including its basic\narchitecture and implementation through both prompt engineering and fine-tuning\nof LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method,\nshowing substantial resistance to various types of prompt injection attacks,\nthus validating its potential as a robust defense strategy in AI security.",
        "pdf_link": "https://arxiv.org/pdf/2401.07612v1.pdf"
    },
    {
        "title": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "authors": [
            "Uri Shaham",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty",
            "Matan Eyal"
        ],
        "published": "2024-01-03T17:48:10Z",
        "summary": "As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages\nfrom the pre-training corpus. We first show that many languages transfer some\ninstruction-following capabilities to other languages from even monolingual\ntuning. Furthermore, we find that only 40 multilingual examples integrated in\nan English tuning set substantially improve multilingual instruction-following,\nboth in seen and unseen languages during tuning. In general, we observe that\nmodels tuned on multilingual mixtures exhibit comparable or superior\nperformance in multiple languages compared to monolingually tuned models,\ndespite training on 10x fewer examples in those languages. Finally, we find\nthat diversifying the instruction tuning set with even just 2-4 languages\nsignificantly improves cross-lingual generalization. Our results suggest that\nbuilding massively multilingual instruction-tuned models can be done with only\na very small set of multilingual instruction-responses.",
        "pdf_link": "https://arxiv.org/pdf/2401.01854v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models for NLG Evaluation: A Survey",
        "authors": [
            "Zhen Li",
            "Xiaohan Xu",
            "Tao Shen",
            "Can Xu",
            "Jia-Chen Gu",
            "Chongyang Tao"
        ],
        "published": "2024-01-13T15:59:09Z",
        "summary": "In the rapidly evolving domain of Natural Language Generation (NLG)\nevaluation, introducing Large Language Models (LLMs) has opened new avenues for\nassessing generated content quality, e.g., coherence, creativity, and context\nrelevance. This survey aims to provide a thorough overview of leveraging LLMs\nfor NLG evaluation, a burgeoning area that lacks a systematic analysis. We\npropose a coherent taxonomy for organizing existing LLM-based evaluation\nmetrics, offering a structured framework to understand and compare these\nmethods. Our detailed exploration includes critically assessing various\nLLM-based methodologies, as well as comparing their strengths and limitations\nin evaluating NLG outputs. By discussing unresolved challenges, including bias,\nrobustness, domain-specificity, and unified evaluation, this survey seeks to\noffer insights to researchers and advocate for fairer and more advanced NLG\nevaluation techniques.",
        "pdf_link": "https://arxiv.org/pdf/2401.07103v1.pdf"
    },
    {
        "title": "Prompting Large Vision-Language Models for Compositional Reasoning",
        "authors": [
            "Timothy Ossowski",
            "Ming Jiang",
            "Junjie Hu"
        ],
        "published": "2024-01-20T22:04:28Z",
        "summary": "Vision-language models such as CLIP have shown impressive capabilities in\nencoding texts and images into aligned embeddings, enabling the retrieval of\nmultimodal data in a shared embedding space. However, these embedding-based\nmodels still face challenges in effectively matching images and texts with\nsimilar visio-linguistic compositionality, as evidenced by their performance on\nthe recent Winoground dataset. In this paper, we argue that this limitation\nstems from two factors: the use of single vector representations for complex\nmultimodal data, and the absence of step-by-step reasoning in these\nembedding-based methods. To address this issue, we make an exploratory step\nusing a novel generative method that prompts large vision-language models\n(e.g., GPT-4) to depict images and perform compositional reasoning. Our method\noutperforms other embedding-based methods on the Winoground dataset, and\nobtains further improvement of up to 10% accuracy when enhanced with the\noptimal description.",
        "pdf_link": "https://arxiv.org/pdf/2401.11337v1.pdf"
    },
    {
        "title": "Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example",
        "authors": [
            "Wei Xia",
            "Shaoguang Mao",
            "Chanjing Zheng"
        ],
        "published": "2024-01-07T07:13:50Z",
        "summary": "Large language models have demonstrated exceptional capabilities in tasks\ninvolving natural language generation, reasoning, and comprehension. This study\naims to construct prompts and comments grounded in the diverse scoring criteria\ndelineated within the official TOEFL guide. The primary objective is to assess\nthe capabilities and constraints of ChatGPT, a prominent representative of\nlarge language models, within the context of automated essay scoring. The\nprevailing methodologies for automated essay scoring involve the utilization of\ndeep neural networks, statistical machine learning techniques, and fine-tuning\npre-trained models. However, these techniques face challenges when applied to\ndifferent contexts or subjects, primarily due to their substantial data\nrequirements and limited adaptability to small sample sizes. In contrast, this\nstudy employs ChatGPT to conduct an automated evaluation of English essays,\neven with a small sample size, employing an experimental approach. The\nempirical findings indicate that ChatGPT can provide operational functionality\nfor automated essay scoring, although the results exhibit a regression effect.\nIt is imperative to underscore that the effective design and implementation of\nChatGPT prompts necessitate a profound domain expertise and technical\nproficiency, as these prompts are subject to specific threshold criteria.\nKeywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent\nWriting Task",
        "pdf_link": "https://arxiv.org/pdf/2401.03401v1.pdf"
    },
    {
        "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
        "authors": [
            "Jie Qin",
            "Jie Wu",
            "Weifeng Chen",
            "Yuxi Ren",
            "Huixia Li",
            "Hefeng Wu",
            "Xuefeng Xiao",
            "Rui Wang",
            "Shilei Wen"
        ],
        "published": "2024-01-18T15:30:58Z",
        "summary": "Diffusion models have opened up new avenues for the field of image\ngeneration, resulting in the proliferation of high-quality models shared on\nopen-source platforms. However, a major challenge persists in current\ntext-to-image systems are often unable to handle diverse inputs, or are limited\nto single model results. Current unified attempts often fall into two\northogonal aspects: i) parse Diverse Prompts in input stage; ii) activate\nexpert model to output. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models (LLM) to offer a unified\ngeneration system capable of seamlessly accommodating various types of prompts\nand integrating domain-expert models. DiffusionGPT constructs domain-specific\nTrees for various generative models based on prior knowledge. When provided\nwith an input, the LLM parses the prompt and employs the Trees-of-Thought to\nguide the selection of an appropriate model, thereby relaxing input constraints\nand ensuring exceptional performance across diverse domains. Moreover, we\nintroduce Advantage Databases, where the Tree-of-Thought is enriched with human\nfeedback, aligning the model selection process with human preferences. Through\nextensive experiments and comparisons, we demonstrate the effectiveness of\nDiffusionGPT, showcasing its potential for pushing the boundaries of image\nsynthesis in diverse domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.10061v1.pdf"
    },
    {
        "title": "INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges",
        "authors": [
            "Jayr Pereira",
            "Andre Assumpcao",
            "Julio Trecenti",
            "Luiz Airosa",
            "Caio Lente",
            "Jhonatan Cl\u00e9to",
            "Guilherme Dobins",
            "Rodrigo Nogueira",
            "Luis Mitchell",
            "Roberto Lotufo"
        ],
        "published": "2024-01-10T17:13:28Z",
        "summary": "This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia\nArtificial), a groundbreaking system designed to integrate Large Language\nModels (LLMs) into the operational framework of Brazilian Federal Court of\nAccounts (TCU). The system automates various stages of case analysis, including\nbasic information extraction, admissibility examination, Periculum in mora and\nFumus boni iuris analyses, and recommendations generation. Through a series of\nexperiments, we demonstrate INACIA's potential in extracting relevant\ninformation from case documents, evaluating its legal plausibility, and\nformulating propositions for judicial decision-making. Utilizing a validation\ndataset alongside LLMs, our evaluation methodology presents a novel approach to\nassessing system performance, correlating highly with human judgment. These\nresults underscore INACIA's potential in complex legal task handling while also\nacknowledging the current limitations. This study discusses possible\nimprovements and the broader implications of applying AI in legal contexts,\nsuggesting that INACIA represents a significant step towards integrating AI in\nlegal systems globally, albeit with cautious optimism grounded in the empirical\nfindings.",
        "pdf_link": "https://arxiv.org/pdf/2401.05273v3.pdf"
    },
    {
        "title": "Cross-lingual Editing in Multilingual Language Models",
        "authors": [
            "Himanshu Beniwal",
            "Kowsik Nandagopan D",
            "Mayank Singh"
        ],
        "published": "2024-01-19T06:54:39Z",
        "summary": "The training of large language models (LLMs) necessitates substantial data\nand computational resources, and updating outdated LLMs entails significant\nefforts and resources. While numerous model editing techniques (METs) have\nemerged to efficiently update model outputs without retraining, their\neffectiveness in multilingual LLMs, where knowledge is stored in diverse\nlanguages, remains an underexplored research area. This research paper\nintroduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a\nfact is edited in one language, and the subsequent update propagation is\nobserved across other languages. To investigate the XME paradigm, we conducted\nexperiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:\n\\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi,\nGujarati, and Bengali). The results reveal notable performance limitations of\nstate-of-the-art METs under the XME setting, mainly when the languages involved\nbelong to two distinct script families. These findings highlight the need for\nfurther research and development of XME techniques to address these challenges.\nFor more comprehensive information, the dataset used in this research and the\nassociated code are publicly available at the following\nURL\\url{https://github.com/lingo-iitgn/XME}.",
        "pdf_link": "https://arxiv.org/pdf/2401.10521v2.pdf"
    },
    {
        "title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models",
        "authors": [
            "Wai-Chung Kwan",
            "Xingshan Zeng",
            "Yuxin Jiang",
            "Yufei Wang",
            "Liangyou Li",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Kam-Fai Wong"
        ],
        "published": "2024-01-30T04:50:28Z",
        "summary": "Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.",
        "pdf_link": "https://arxiv.org/pdf/2401.16745v1.pdf"
    },
    {
        "title": "Scaling Sparse Fine-Tuning to Large Language Models",
        "authors": [
            "Alan Ansell",
            "Ivan Vuli\u0107",
            "Hannah Sterz",
            "Anna Korhonen",
            "Edoardo M. Ponti"
        ],
        "published": "2024-01-29T18:43:49Z",
        "summary": "Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with\ninstructions or human feedback) due to their sheer number of parameters. A\nfamily of parameter-efficient sparse fine-tuning methods have proven promising\nin terms of performance but their memory requirements increase proportionally\nto the size of the LLMs. In this work, we scale sparse fine-tuning to\nstate-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse\nfine-tuning method which, for a desired density level, maintains an array of\nparameter indices and the deltas of these parameters relative to their\npretrained values. It iterates over: (a) updating the active deltas, (b)\npruning indices (based on the change of magnitude of their deltas) and (c)\nregrowth of indices. For regrowth, we explore two criteria based on either the\naccumulated gradients of a few candidate parameters or their approximate\nmomenta estimated using the efficient SM3 optimizer. We experiment with\ninstruction-tuning of LLMs on standard dataset mixtures, finding that SpIEL is\noften superior to popular parameter-efficient fine-tuning methods like LoRA\n(low-rank adaptation) in terms of performance and comparable in terms of run\ntime. We additionally show that SpIEL is compatible with both quantization and\nefficient optimizers, to facilitate scaling to ever-larger model sizes. We\nrelease the code for SpIEL at https://github.com/AlanAnsell/peft and for the\ninstruction-tuning experiments at https://github.com/ducdauge/sft-llm.",
        "pdf_link": "https://arxiv.org/pdf/2401.16405v2.pdf"
    },
    {
        "title": "Wordflow: Social Prompt Engineering for Large Language Models",
        "authors": [
            "Zijie J. Wang",
            "Aishwarya Chakravarthy",
            "David Munechika",
            "Duen Horng Chau"
        ],
        "published": "2024-01-25T18:58:11Z",
        "summary": "Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.",
        "pdf_link": "https://arxiv.org/pdf/2401.14447v1.pdf"
    },
    {
        "title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models",
        "authors": [
            "Shuai Wang",
            "Liang Ding",
            "Li Shen",
            "Yong Luo",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2024-01-12T15:21:36Z",
        "summary": "Advancing automated programming necessitates robust and comprehensive code\ngeneration benchmarks, yet current evaluation frameworks largely neglect\nobject-oriented programming (OOP) in favor of functional programming (FP),\ne.g., HumanEval and MBPP. To address this, our study introduces a pioneering\nOOP-focused benchmark, featuring 431 Python programs that encompass essential\nOOP concepts and features like classes and encapsulation methods. We propose a\nnovel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k\nmeasures. Our evaluation of 23 leading large language models (LLMs), including\nboth general and code-specialized models, reveals three key insights: 1) pass@o\noffers a more relevant and comprehensive assessment for OOP code generation; 2)\nDespite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP\ncompared to models like ChatGPT; 3) The poor performance of all advanced LLMs\non our OOP benchmark highlights a critical need for improvements in this field.\nOur benchmark and scripts are publicly released at:\nhttps://github.com/alphadl/OOP-eval.",
        "pdf_link": "https://arxiv.org/pdf/2401.06628v2.pdf"
    },
    {
        "title": "Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability",
        "authors": [
            "Md Sadman Sakib",
            "Yu Sun"
        ],
        "published": "2024-01-15T18:01:59Z",
        "summary": "The inherent probabilistic nature of Large Language Models (LLMs) introduces\nan element of unpredictability, raising concerns about potential discrepancies\nin their output. This paper introduces an innovative approach aims to generate\ncorrect and optimal robotic task plans for diverse real-world demands and\nscenarios. LLMs have been used to generate task plans, but they are unreliable\nand may contain wrong, questionable, or high-cost steps. The proposed approach\nuses LLM to generate a number of task plans as trees and amalgamates them into\na graph by removing questionable paths. Then an optimal task tree can be\nretrieved to circumvent questionable and high-cost nodes, thereby improving\nplanning accuracy and execution efficiency. The approach is further improved by\nincorporating a large knowledge network. Leveraging GPT-4 further, the\nhigh-level task plan is converted into a low-level Planning Domain Definition\nLanguage (PDDL) plan executable by a robot. Evaluation results highlight the\nsuperior accuracy and efficiency of our approach compared to previous\nmethodologies in the field of task planning.",
        "pdf_link": "https://arxiv.org/pdf/2401.07868v1.pdf"
    },
    {
        "title": "De-identification is not always enough",
        "authors": [
            "Atiquer Rahman Sarkar",
            "Yao-Shun Chuang",
            "Noman Mohammed",
            "Xiaoqian Jiang"
        ],
        "published": "2024-01-31T21:14:01Z",
        "summary": "For sharing privacy-sensitive data, de-identification is commonly regarded as\nadequate for safeguarding privacy. Synthetic data is also being considered as a\nprivacy-preserving alternative. Recent successes with numerical and tabular\ndata generative models and the breakthroughs in large generative language\nmodels raise the question of whether synthetically generated clinical notes\ncould be a viable alternative to real notes for research purposes. In this\nwork, we demonstrated that (i) de-identification of real clinical notes does\nnot protect records against a membership inference attack, (ii) proposed a\nnovel approach to generate synthetic clinical notes using the current\nstate-of-the-art large language models, (iii) evaluated the performance of the\nsynthetically generated notes in a clinical domain task, and (iv) proposed a\nway to mount a membership inference attack where the target model is trained\nwith synthetic data. We observed that when synthetically generated notes\nclosely match the performance of real data, they also exhibit similar privacy\nconcerns to the real data. Whether other approaches to synthetically generated\nclinical notes could offer better trade-offs and become a better alternative to\nsensitive real notes warrants further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2402.00179v1.pdf"
    },
    {
        "title": "Transformers are Multi-State RNNs",
        "authors": [
            "Matanel Oren",
            "Michael Hassid",
            "Yossi Adi",
            "Roy Schwartz"
        ],
        "published": "2024-01-11T18:35:26Z",
        "summary": "Transformers are considered conceptually different compared to the previous\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\nIn this work, we demonstrate that decoder-only transformers can in fact be\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\nhidden state size. We further show that pretrained transformers can be\nconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of their\nhidden state. We observe that several existing transformers cache compression\ntechniques can be framed as such conversion policies, and introduce a novel\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\nseveral long range tasks indicate that TOVA outperforms all other baseline\npolicies, while being nearly on par with the full (infinite) model, and using\nin some cases only $\\frac{1}{8}$ of the original cache size. Our results\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\nalso lay out the option of mitigating one of their most painful computational\nbottlenecks - the size of their cache memory. We publicly release our code at\nhttps://github.com/schwartz-lab-NLP/TOVA.",
        "pdf_link": "https://arxiv.org/pdf/2401.06104v1.pdf"
    },
    {
        "title": "DevEval: Evaluating Code Generation in Practical Software Projects",
        "authors": [
            "Jia Li",
            "Ge Li",
            "Yunfei Zhao",
            "Yongmin Li",
            "Zhi Jin",
            "Hao Zhu",
            "Huanyu Liu",
            "Kaibo Liu",
            "Lecheng Wang",
            "Zheng Fang",
            "Lanshen Wang",
            "Jiazheng Ding",
            "Xuanming Zhang",
            "Yihong Dong",
            "Yuqi Zhu",
            "Bin Gu",
            "Mengfei Yang"
        ],
        "published": "2024-01-12T06:51:30Z",
        "summary": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.",
        "pdf_link": "https://arxiv.org/pdf/2401.06401v4.pdf"
    },
    {
        "title": "Question Translation Training for Better Multilingual Reasoning",
        "authors": [
            "Wenhao Zhu",
            "Shujian Huang",
            "Fei Yuan",
            "Shuaijie She",
            "Jiajun Chen",
            "Alexandra Birch"
        ],
        "published": "2024-01-15T16:39:10Z",
        "summary": "Large language models show compelling performance on reasoning tasks but they\ntend to perform much worse in languages other than English. This is\nunsurprising given that their training data largely consists of English text\nand instructions. A typical solution is to translate instruction data into all\nlanguages of interest, and then train on the resulting multilingual data, which\nis called translate-training. This approach not only incurs high cost, but also\nresults in poorly translated data due to the non-standard formatting of\nmathematical chain-of-thought. In this paper, we explore the benefits of\nquestion alignment, where we train the model to translate reasoning questions\ninto English by finetuning on X-English parallel question data. In this way we\nperform targeted, in-domain language alignment which makes best use of English\ninstruction data to unlock the LLMs' multilingual reasoning abilities.\nExperimental results on LLaMA2-13B show that question alignment leads to\nconsistent improvements over the translate-training approach: an average\nimprovement of 11.3% and 16.1% accuracy across ten languages on the MGSM and\nMSVAMP multilingual reasoning benchmarks. The project will be available at:\nhttps://github.com/NJUNLP/QAlign.",
        "pdf_link": "https://arxiv.org/pdf/2401.07817v2.pdf"
    },
    {
        "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT",
        "authors": [
            "Azmain Kabir",
            "Shaowei Wang",
            "Yuan Tian",
            "Tse-Hsun",
            "Chen",
            "Muhammad Asaduzzaman",
            "Wenbin Zhang"
        ],
        "published": "2024-01-25T16:10:33Z",
        "summary": "Technical question and answering (Q&A) sites such as Stack Overflow have\nbecome an important source for software developers to seek knowledge. However,\ncode snippets on Q&A sites are usually uncompilable and semantically incomplete\nfor compilation due to unresolved types and missing dependent libraries, which\nraises the obstacle for users to reuse or analyze Q&A code snippets. Prior\napproaches either are not designed for synthesizing compilable code or suffer\nfrom a low compilation success rate. To address this problem, we propose ZS4C,\na lightweight approach to perform zero-shot synthesis of compilable code from\nincomplete code snippets using Large Language Model (LLM). ZS4C operates in two\nstages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify\nmissing import statements for a given code snippet, leveraging our designed\ntask-specific prompt template. In the second stage, ZS4C fixes compilation\nerrors caused by incorrect import statements and syntax errors through\ncollaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C\non a widely used benchmark called StatType-SO against the SOTA approach SnR.\nCompared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a\n39.3% improvement. On average, ZS4C can infer more accurate import statements\nthan SnR, with an improvement of 6.6% in the F1.",
        "pdf_link": "https://arxiv.org/pdf/2401.14279v1.pdf"
    },
    {
        "title": "From Prompt Engineering to Prompt Science With Human in the Loop",
        "authors": [
            "Chirag Shah"
        ],
        "published": "2024-01-01T01:37:36Z",
        "summary": "As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process.",
        "pdf_link": "https://arxiv.org/pdf/2401.04122v2.pdf"
    },
    {
        "title": "When Large Language Models Meet Vector Databases: A Survey",
        "authors": [
            "Zhi Jing",
            "Yongye Su",
            "Yikun Han",
            "Bo Yuan",
            "Haiyun Xu",
            "Chunjiang Liu",
            "Kehai Chen",
            "Min Zhang"
        ],
        "published": "2024-01-30T23:35:28Z",
        "summary": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.01763v2.pdf"
    },
    {
        "title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
        "authors": [
            "Minrui Xu",
            "Dusit Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shiwen Mao",
            "Zhu Han",
            "Dong In Kim",
            "Khaled B. Letaief"
        ],
        "published": "2024-01-15T15:20:59Z",
        "summary": "AI agents based on multimodal large language models (LLMs) are expected to\nrevolutionize human-computer interaction and offer more personalized assistant\nservices across various domains like healthcare, education, manufacturing, and\nentertainment. Deploying LLM agents in 6G networks enables users to access\npreviously expensive AI assistant services via mobile devices democratically,\nthereby reducing interaction latency and better preserving user privacy.\nNevertheless, the limited capacity of mobile devices constrains the\neffectiveness of deploying and executing local LLMs, which necessitates\noffloading complex tasks to global LLMs running on edge servers during\nlong-horizon interactions. In this article, we propose a split learning system\nfor LLM agents in 6G networks leveraging the collaboration between mobile\ndevices and edge servers, where multiple LLMs with different roles are\ndistributed across mobile devices and edge servers to perform user-agent\ninteractive tasks collaboratively. In the proposed system, LLM agents are split\ninto perception, grounding, and alignment modules, facilitating inter-module\ncommunications to meet extended user requirements on 6G network functions,\nincluding integrated sensing and communication, digital twins, and\ntask-oriented communications. Furthermore, we introduce a novel model caching\nalgorithm for LLMs within the proposed system to improve model utilization in\ncontext, thus reducing network costs of the collaborative mobile and edge LLM\nagents.",
        "pdf_link": "https://arxiv.org/pdf/2401.07764v2.pdf"
    },
    {
        "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "authors": [
            "Fuzhao Xue",
            "Zian Zheng",
            "Yao Fu",
            "Jinjie Ni",
            "Zangwei Zheng",
            "Wangchunshu Zhou",
            "Yang You"
        ],
        "published": "2024-01-29T12:05:02Z",
        "summary": "To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01739v2.pdf"
    },
    {
        "title": "Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches",
        "authors": [
            "Wannapon Suraworachet",
            "Jennifer Seon",
            "Mutlu Cukurova"
        ],
        "published": "2024-01-03T11:54:30Z",
        "summary": "Effective collaboration requires groups to strategically regulate themselves\nto overcome challenges. Research has shown that groups may fail to regulate due\nto differences in members' perceptions of challenges which may benefit from\nexternal support. In this study, we investigated the potential of leveraging\nthree distinct natural language processing models: an expert knowledge\nrule-based model, a supervised machine learning (ML) model and a Large Language\nmodel (LLM), in challenge detection and challenge dimension identification\n(cognitive, metacognitive, emotional and technical/other challenges) from\nstudent discourse, was investigated. The results show that the supervised ML\nand the LLM approaches performed considerably well in both tasks, in contrast\nto the rule-based approach, whose efficacy heavily relies on the engineered\nfeatures by experts. The paper provides an extensive discussion of the three\napproaches' performance for automated detection and support of students'\nchallenge moments in collaborative learning activities. It argues that,\nalthough LLMs provide many advantages, they are unlikely to be the panacea to\nissues of the detection and feedback provision of socially shared regulation of\nlearning due to their lack of reliability, as well as issues of validity\nevaluation, privacy and confabulation. We conclude the paper with a discussion\non additional considerations, including model transparency to explore feasible\nand meaningful analytical feedback for students and educators using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01692v1.pdf"
    },
    {
        "title": "Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization",
        "authors": [
            "Shuaimin Li",
            "Xuanang Chen",
            "Yuanfeng Song",
            "Yunze Song",
            "Chen Zhang"
        ],
        "published": "2024-01-29T10:23:47Z",
        "summary": "Data visualization (DV) systems are increasingly recognized for their\nprofound capability to uncover insights from vast datasets, gaining attention\nacross both industry and academia. Crafting data queries is an essential\nprocess within certain declarative visualization languages (DVLs, e.g.,\nVega-Lite, EChart.). The evolution of natural language processing (NLP)\ntechnologies has streamlined the use of natural language interfaces to\nvisualize tabular data, offering a more accessible and intuitive user\nexperience. However, current methods for converting natural language questions\ninto data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite\nutilizing complex neural network architectures, still fall short of\nexpectations and have great room for improvement.\n  Large language models (LLMs) such as ChatGPT and GPT-4, have established new\nbenchmarks in a variety of NLP tasks, fundamentally altering the landscape of\nthe field. Inspired by these advancements, we introduce a novel framework,\nPrompt4Vis, leveraging LLMs and in-context learning to enhance the performance\nof generating data visualization from natural language. Prompt4Vis comprises\ntwo key components: (1) a multi-objective example mining module, designed to\nfind out the truly effective examples that strengthen the LLM's in-context\nlearning capabilities for text-to-vis; (2) a schema filtering module, which is\nproposed to simplify the schema of the database. Extensive experiments through\n5-fold cross-validation on the NVBench dataset demonstrate the superiority of\nPrompt4Vis, which notably surpasses the state-of-the-art (SOTA) RGVisNet by\napproximately 35.9% and 71.3% on dev and test sets, respectively. To the best\nof our knowledge, Prompt4Vis is the first work that introduces in-context\nlearning into the text-to-vis for generating data visualization queries.",
        "pdf_link": "https://arxiv.org/pdf/2402.07909v1.pdf"
    },
    {
        "title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification",
        "authors": [
            "Jan Trienes",
            "Sebastian Joseph",
            "J\u00f6rg Schl\u00f6tterer",
            "Christin Seifert",
            "Kyle Lo",
            "Wei Xu",
            "Byron C. Wallace",
            "Junyi Jessy Li"
        ],
        "published": "2024-01-29T19:00:01Z",
        "summary": "Text simplification aims to make technical texts more accessible to laypeople\nbut often results in deletion of information and vagueness. This work proposes\nInfoLossQA, a framework to characterize and recover simplification-induced\ninformation loss in form of question-and-answer (QA) pairs. Building on the\ntheory of Question Under Discussion, the QA pairs are designed to help readers\ndeepen their knowledge of a text. We conduct a range of experiments with this\nframework. First, we collect a dataset of 1,000 linguist-curated QA pairs\nderived from 104 LLM simplifications of scientific abstracts of medical\nstudies. Our analyses of this data reveal that information loss occurs\nfrequently, and that the QA pairs give a high-level overview of what\ninformation was lost. Second, we devise two methods for this task: end-to-end\nprompting of open-source and commercial language models, and a natural language\ninference pipeline. With a novel evaluation framework considering the\ncorrectness of QA pairs and their linguistic suitability, our expert evaluation\nreveals that models struggle to reliably identify information loss and applying\nsimilar standards as humans at what constitutes information loss.",
        "pdf_link": "https://arxiv.org/pdf/2401.16475v1.pdf"
    },
    {
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers",
        "authors": [
            "Chen Zheng",
            "Ke Sun",
            "Da Tang",
            "Yukun Ma",
            "Yuyu Zhang",
            "Chenguang Xi",
            "Xun Zhou"
        ],
        "published": "2024-01-04T05:47:41Z",
        "summary": "The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2401.02072v1.pdf"
    },
    {
        "title": "Agent Alignment in Evolving Social Norms",
        "authors": [
            "Shimin Li",
            "Tianxiang Sun",
            "Qinyuan Cheng",
            "Xipeng Qiu"
        ],
        "published": "2024-01-09T15:44:44Z",
        "summary": "Agents based on Large Language Models (LLMs) are increasingly permeating\nvarious domains of human production and life, highlighting the importance of\naligning them with human values. The current alignment of AI systems primarily\nfocuses on passively aligning LLMs through human intervention. However, agents\npossess characteristics like receiving environmental feedback and\nself-evolution, rendering the LLM alignment methods inadequate. In response, we\npropose an evolutionary framework for agent evolution and alignment, named\nEvolutionaryAgent, which transforms agent alignment into a process of evolution\nand selection under the principle of survival of the fittest. In an environment\nwhere social norms continuously evolve, agents better adapted to the current\nsocial norms will have a higher probability of survival and proliferation,\nwhile those inadequately aligned dwindle over time. Experimental results\nassessing the agents from multiple perspectives in aligning with social norms\ndemonstrate that EvolutionaryAgent can align progressively better with the\nevolving social norms while maintaining its proficiency in general tasks.\nEffectiveness tests conducted on various open and closed-source LLMs as the\nfoundation for agents also prove the applicability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.04620v4.pdf"
    },
    {
        "title": "Security and Privacy Challenges of Large Language Models: A Survey",
        "authors": [
            "Badhan Chandra Das",
            "M. Hadi Amini",
            "Yanzhao Wu"
        ],
        "published": "2024-01-30T04:00:54Z",
        "summary": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2402.00888v1.pdf"
    },
    {
        "title": "Hallucination Benchmark in Medical Visual Question Answering",
        "authors": [
            "Jinge Wu",
            "Yunsoo Kim",
            "Honghan Wu"
        ],
        "published": "2024-01-11T10:52:17Z",
        "summary": "The recent success of large language and vision models (LLVMs) on vision\nquestion answering (VQA), particularly their applications in medicine\n(Med-VQA), has shown a great potential of realizing effective visual assistants\nfor healthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models' limitations and reveals the effectiveness\nof various prompting strategies.",
        "pdf_link": "https://arxiv.org/pdf/2401.05827v2.pdf"
    },
    {
        "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models",
        "authors": [
            "Diego Carraro",
            "Derek Bridge"
        ],
        "published": "2024-01-21T14:33:52Z",
        "summary": "It has long been recognized that it is not enough for a Recommender System\n(RS) to provide recommendations based only on their relevance to users. Among\nmany other criteria, the set of recommendations may need to be diverse in order\nto handle uncertainty and offer a meaningful choice. The literature reports\nmany ways of measuring diversity and ways of improving the diversity of a set\nof recommendations, most notably by re-ranking and selecting from a larger set\nof candidate recommendations. Driven by promising insights from the literature\non how to incorporate versatile Large Language Models (LLMs) into the RS\npipeline, in this paper, we show how LLMs can be used for diversity re-ranking.\n  We begin with an informal study that verifies that LLMs can be used for\nre-ranking tasks and do have some understanding of the concept of diversity.\nThen, we design a more rigorous methodology where LLMs are prompted to generate\na diverse ranking from a candidate ranking using various prompt templates with\ndifferent re-ranking instructions in a zero-shot fashion. We conduct\ncomprehensive experiments testing state-of-the-art conversational LLMs from the\nGPT and Llama families. We compare their re-ranking capabilities with random\nre-ranking and various traditional re-ranking methods from the literature (MMR,\nxQuAD and RxQuAD). We find that LLM-based re-ranking outperforms random\nre-ranking across all the metrics that we use but does not perform as well as\nthe traditional re-ranking methods. We gain insight into prompt design for this\ntask (e.g.\\ on the whole, it is better to prompt for diversity rather than a\nbalance of diversity and relevance). Given that no special knowledge\nengineering is needed, we conclude that LLM-based re-ranking is a promising\napproach, and we highlight directions for future research. We open-source the\ncode of our experiments for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2401.11506v1.pdf"
    },
    {
        "title": "Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs",
        "authors": [
            "Stepan Tytarenko",
            "Mohammad Ruhul Amin"
        ],
        "published": "2024-01-30T00:23:29Z",
        "summary": "Fine-tuning large pre-trained language models (LLMs) on particular datasets\nis a commonly employed strategy in Natural Language Processing (NLP)\nclassification tasks. However, this approach usually results in a loss of\nmodels generalizability. In this paper, we present a framework that allows for\nmaintaining generalizability, and enhances the performance on the downstream\ntask by utilizing task-specific context attribution. We show that a linear\ntransformation of the text representation from any transformer model using the\ntask-specific concept operator results in a projection onto the latent concept\nspace, referred to as context attribution in this paper. The specific concept\noperator is optimized during the supervised learning stage via novel loss\nfunctions. The proposed framework demonstrates that context attribution of the\ntext representation for each task objective can improve the capacity of the\ndiscriminator function and thus achieve better performance for the\nclassification task. Experimental results on three datasets, namely HateXplain,\nIMDB reviews, and Social Media Attributions, illustrate that the proposed model\nattains superior accuracy and generalizability. Specifically, for the\nnon-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in\naccuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,\nfine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and\nF1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT\nfine-tuned on the IMDB dataset in conjunction with the proposed model improves\nthe F1-score on the HateXplain dataset by 7%. For the Social Media Attributions\ndataset of YouTube comments, we observe 5.2% increase in F1-metric. The\nproposed framework is implemented with PyTorch and provided open-source on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2401.16638v1.pdf"
    },
    {
        "title": "Knowledge Verification to Nip Hallucination in the Bud",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Leyang Cui",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2024-01-19T15:39:49Z",
        "summary": "While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as \\emph{hallucination}. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/KCA}.",
        "pdf_link": "https://arxiv.org/pdf/2401.10768v3.pdf"
    },
    {
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages",
        "authors": [
            "Zhuoyuan Mao",
            "Yen Yu"
        ],
        "published": "2024-01-11T10:28:17Z",
        "summary": "This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.",
        "pdf_link": "https://arxiv.org/pdf/2401.05811v1.pdf"
    },
    {
        "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "authors": [
            "Yuqiang Sun",
            "Daoyuan Wu",
            "Yue Xue",
            "Han Liu",
            "Wei Ma",
            "Lyuye Zhang",
            "Miaolei Shi",
            "Yang Liu"
        ],
        "published": "2024-01-29T14:32:27Z",
        "summary": "Large language models (LLMs) have demonstrated significant potential for many\ndownstream tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent attempts to use LLMs for vulnerability\ndetection are still preliminary, as they lack an in-depth understanding of a\nsubject LLM's vulnerability reasoning capability -- whether it originates from\nthe model itself or from external assistance, such as invoking tool support and\nretrieving vulnerability knowledge. In this paper, we aim to decouple LLMs'\nvulnerability reasoning capability from their other capabilities, including the\nability to actively seek additional information (e.g., via function calling in\nSOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based\nmatching and retrieval), and follow instructions to output structured results.\nTo this end, we propose a unified evaluation framework named LLM4Vuln, which\nseparates LLMs' vulnerability reasoning from their other capabilities and\nevaluates how LLMs' vulnerability reasoning could be enhanced when combined\nwith the enhancement of other capabilities. To demonstrate the effectiveness of\nLLM4Vuln, we have designed controlled experiments using 75 ground-truth smart\ncontract vulnerabilities, which were extensively audited as high-risk on\nCode4rena from August to November 2023, and tested them in 4,950 different\nscenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama).\nOur results not only reveal ten findings regarding the varying effects of\nknowledge enhancement, context supplementation, prompt schemes, and models but\nalso enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty\nprograms with over 1,000 USD being awarded.",
        "pdf_link": "https://arxiv.org/pdf/2401.16185v1.pdf"
    },
    {
        "title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models",
        "authors": [
            "Wendi Cui",
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Lopez Damien",
            "Kamalika Das",
            "Bradley Malin",
            "Sricharan Kumar"
        ],
        "published": "2024-01-04T08:34:16Z",
        "summary": "Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2401.02132v1.pdf"
    },
    {
        "title": "Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness",
        "authors": [
            "Samaneh Shafee",
            "Alysson Bessani",
            "Pedro M. Ferreira"
        ],
        "published": "2024-01-26T13:15:24Z",
        "summary": "Knowledge sharing about emerging threats is crucial in the rapidly advancing\nfield of cybersecurity and forms the foundation of Cyber Threat Intelligence\n(CTI). In this context, Large Language Models are becoming increasingly\nsignificant in the field of cybersecurity, presenting a wide range of\nopportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,\nStanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary\nclassification and Named Entity Recognition (NER) tasks performed using Open\nSource INTelligence (OSINT). We utilize well-established data collected in\nprevious research from Twitter to assess the competitiveness of these chatbots\nwhen compared to specialized models trained for those tasks. In binary\nclassification experiments, Chatbot GPT-4 as a commercial model achieved an\nacceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1\nscore of 0.90. However, concerning cybersecurity entity recognition, all\nevaluated chatbots have limitations and are less effective. This study\ndemonstrates the capability of chatbots for OSINT binary classification and\nshows that they require further improvement in NER to effectively replace\nspecially trained models. Our results shed light on the limitations of the LLM\nchatbots when compared to specialized models, and can help researchers improve\nchatbots technology with the objective to reduce the required effort to\nintegrate machine learning in OSINT-based CTI tools.",
        "pdf_link": "https://arxiv.org/pdf/2401.15127v2.pdf"
    },
    {
        "title": "Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values",
        "authors": [
            "Jon Chun",
            "Katherine Elkins"
        ],
        "published": "2024-01-09T14:57:30Z",
        "summary": "With the rise of individual and collaborative networks of autonomous agents,\nAI is deployed in more key reasoning and decision-making roles. For this\nreason, ethics-based audits play a pivotal role in the rapidly growing fields\nof AI safety and regulation. This paper undertakes an ethics-based audit to\nprobe the 8 leading commercial and open-source Large Language Models including\nGPT-4. We assess explicability and trustworthiness by a) establishing how well\ndifferent models engage in moral reasoning and b) comparing normative values\nunderlying models as ethical frameworks. We employ an experimental,\nevidence-based approach that challenges the models with ethical dilemmas in\norder to probe human-AI alignment. The ethical scenarios are designed to\nrequire a decision in which the particulars of the situation may or may not\nnecessitate deviating from normative ethical principles. A sophisticated\nethical framework was consistently elicited in one model, GPT-4. Nonetheless,\ntroubling findings include underlying normative frameworks with clear bias\ntowards particular cultural norms. Many models also exhibit disturbing\nauthoritarian tendencies. Code is available at\nhttps://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.",
        "pdf_link": "https://arxiv.org/pdf/2402.01651v1.pdf"
    },
    {
        "title": "Towards Optimizing the Costs of LLM Usage",
        "authors": [
            "Shivanshu Shekhar",
            "Tanishq Dubey",
            "Koyel Mukherjee",
            "Apoorv Saxena",
            "Atharv Tyagi",
            "Nishanth Kotla"
        ],
        "published": "2024-01-29T16:36:31Z",
        "summary": "Generative AI and LLMs in particular are heavily used nowadays for various\ndocument processing tasks such as question answering and summarization.\nHowever, different LLMs come with different capabilities for different tasks as\nwell as with different costs, tokenization, and latency. In fact, enterprises\nare already incurring huge costs of operating or using LLMs for their\nrespective use cases.\n  In this work, we propose optimizing the usage costs of LLMs by estimating\ntheir output quality (without actually invoking the LLMs), and then solving an\noptimization routine for the LLM selection to either keep costs under a budget,\nor minimize the costs, in a quality and latency aware manner. We propose a\nmodel to predict the output quality of LLMs on document processing tasks like\nsummarization, followed by an LP rounding algorithm to optimize the selection\nof LLMs. We study optimization problems trading off the quality and costs, both\ntheoretically and empirically. We further propose a sentence simplification\nmodel for reducing the number of tokens in a controlled manner. Additionally,\nwe propose several deterministic heuristics for reducing tokens in a quality\naware manner, and study the related optimization problem of applying the\nheuristics optimizing the quality and cost trade-off. We perform extensive\nempirical validation of our methods on not only enterprise datasets but also on\nopen-source datasets, annotated by us, and show that we perform much better\ncompared to closest baselines. Our methods reduce costs by 40%- 90% while\nimproving quality by 4%-7%. We will release the annotated open source datasets\nto the community for further research and exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.01742v1.pdf"
    },
    {
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
        "authors": [
            "Zilong Wang",
            "Hao Zhang",
            "Chun-Liang Li",
            "Julian Martin Eisenschlos",
            "Vincent Perot",
            "Zifeng Wang",
            "Lesly Miculicich",
            "Yasuhisa Fujii",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published": "2024-01-09T07:46:26Z",
        "summary": "Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.",
        "pdf_link": "https://arxiv.org/pdf/2401.04398v2.pdf"
    },
    {
        "title": "Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets",
        "authors": [
            "Nikita Moghe",
            "Arnisa Fazla",
            "Chantal Amrhein",
            "Tom Kocmi",
            "Mark Steedman",
            "Alexandra Birch",
            "Rico Sennrich",
            "Liane Guillou"
        ],
        "published": "2024-01-29T17:17:42Z",
        "summary": "Recent machine translation (MT) metrics calibrate their effectiveness by\ncorrelating with human judgement but without any insights about their behaviour\nacross different error types. Challenge sets are used to probe specific\ndimensions of metric behaviour but there are very few such datasets and they\neither focus on a limited number of phenomena or a limited number of language\npairs. We introduce ACES, a contrastive challenge set spanning 146 language\npairs, aimed at discovering whether metrics can identify 68 translation\naccuracy errors. These phenomena range from simple alterations at the\nword/character level to more complex errors based on discourse and real-world\nknowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics\nsubmitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric\nperformance, assess their incremental performance over successive campaigns,\nand measure their sensitivity to a range of linguistic phenomena. We also\ninvestigate claims that Large Language Models (LLMs) are effective as MT\nevaluators by evaluating on ACES. Our results demonstrate that different metric\nfamilies struggle with different phenomena and that LLM-based methods fail to\ndemonstrate reliable performance. Our analyses indicate that most metrics\nignore the source sentence, tend to prefer surface-level overlap and end up\nincorporating properties of base models which are not always beneficial. We\nexpand ACES to include error span annotations, denoted as SPAN-ACES and we use\nthis dataset to evaluate span-based error metrics showing these metrics also\nneed considerable improvement. Finally, we provide a set of recommendations for\nbuilding better MT metrics, including focusing on error labels instead of\nscores, ensembling, designing strategies to explicitly focus on the source\nsentence, focusing on semantic content and choosing the right base model for\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2401.16313v1.pdf"
    },
    {
        "title": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications",
        "authors": [
            "Yuhang Zhou",
            "Paiheng Xu",
            "Xiyao Wang",
            "Xuan Lu",
            "Ge Gao",
            "Wei Ai"
        ],
        "published": "2024-01-22T06:02:39Z",
        "summary": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01681v2.pdf"
    },
    {
        "title": "Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine",
        "authors": [
            "Qiao Jin",
            "Fangyuan Chen",
            "Yiliang Zhou",
            "Ziyang Xu",
            "Justin M. Cheung",
            "Robert Chen",
            "Ronald M. Summers",
            "Justin F. Rousseau",
            "Peiyun Ni",
            "Marc J Landsman",
            "Sally L. Baxter",
            "Subhi J. Al'Aref",
            "Yijia Li",
            "Michael F. Chiang",
            "Yifan Peng",
            "Zhiyong Lu"
        ],
        "published": "2024-01-16T14:41:20Z",
        "summary": "Recent studies indicate that Generative Pre-trained Transformer 4 with Vision\n(GPT-4V) outperforms human physicians in medical challenge tasks. However,\nthese evaluations primarily focused on the accuracy of multi-choice questions\nalone. Our study extends the current scope by conducting a comprehensive\nanalysis of GPT-4V's rationales of image comprehension, recall of medical\nknowledge, and step-by-step multimodal reasoning when solving New England\nJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test\nthe knowledge and diagnostic capabilities of medical professionals. Evaluation\nresults confirmed that GPT-4V outperforms human physicians regarding\nmulti-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in\ncases where physicians incorrectly answer, with over 80% accuracy. However, we\ndiscovered that GPT-4V frequently presents flawed rationales in cases where it\nmakes the correct final choices (27.3%), most prominent in image comprehension\n(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our\nfindings emphasize the necessity for further in-depth evaluations of its\nrationales before integrating such models into clinical workflows.",
        "pdf_link": "https://arxiv.org/pdf/2401.08396v2.pdf"
    },
    {
        "title": "ChatGraph: Chat with Your Graphs",
        "authors": [
            "Yun Peng",
            "Sen Lin",
            "Qian Chen",
            "Lyu Xu",
            "Xiaojun Ren",
            "Yafei Li",
            "Jianliang Xu"
        ],
        "published": "2024-01-23T11:29:19Z",
        "summary": "Graph analysis is fundamental in real-world applications. Traditional\napproaches rely on SPARQL-like languages or clicking-and-dragging interfaces to\ninteract with graph data. However, these methods either require users to\npossess high programming skills or support only a limited range of graph\nanalysis functionalities. To address the limitations, we propose a large\nlanguage model (LLM)-based framework called ChatGraph. With ChatGraph, users\ncan interact with graphs through natural language, making it easier to use and\nmore flexible than traditional approaches. The core of ChatGraph lies in\ngenerating chains of graph analysis APIs based on the understanding of the\ntexts and graphs inputted in the user prompts. To achieve this, ChatGraph\nconsists of three main modules: an API retrieval module that searches for\nrelevant APIs, a graph-aware LLM module that enables the LLM to comprehend\ngraphs, and an API chain-oriented finetuning module that guides the LLM in\ngenerating API chains.",
        "pdf_link": "https://arxiv.org/pdf/2401.12672v1.pdf"
    },
    {
        "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
        "authors": [
            "Caiqi Zhang",
            "Zhijiang Guo",
            "Andreas Vlachos"
        ],
        "published": "2024-01-27T20:26:03Z",
        "summary": "This paper investigates the potential benefits of language-specific\nfact-checking models, focusing on the case of Chinese. We first demonstrate the\nlimitations of translation-based methods and multilingual large language models\n(e.g., GPT-4), highlighting the need for language-specific systems. We further\npropose a Chinese fact-checking system that can better retrieve evidence from a\ndocument by incorporating context information. To better analyze token-level\nbiases in different systems, we construct an adversarial dataset based on the\nCHEF dataset, where each instance has large word overlap with the original one\nbut holds the opposite veracity label. Experimental results on the CHEF dataset\nand our adversarial dataset show that our proposed method outperforms\ntranslation-based methods and multilingual LLMs and is more robust toward\nbiases, while there is still large room for improvement, emphasizing the\nimportance of language-specific fact-checking systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.15498v2.pdf"
    },
    {
        "title": "On Prompt-Driven Safeguarding for Large Language Models",
        "authors": [
            "Chujie Zheng",
            "Fan Yin",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Kai-Wei Chang",
            "Minlie Huang",
            "Nanyun Peng"
        ],
        "published": "2024-01-31T17:28:24Z",
        "summary": "Prepending model inputs with safety prompts is a common practice for\nsafeguarding large language models (LLMs) from complying with queries that\ncontain harmful intents. However, the working mechanisms of safety prompts have\nnot been revealed yet, which hinders the potential for automatically optimizing\nthem to improve LLM safety. To this end, we investigate the impact of safety\nprompts from the perspective of model representations. We find that in models'\nrepresentation space, harmful and harmless queries can be largely\ndistinguished, but this is not noticeably enhanced by safety prompts. Instead,\nthe queries' representations are moved by safety prompts in similar directions\nwhere models become more prone to refusal (i.e., refusing to provide\nassistance) even when the queries are harmless. Inspired by these findings, we\npropose a method called DRO (Directed Representation Optimization) for\nautomatic safety prompt optimization. It treats safety prompts as continuous,\ntrainable embeddings and learns to move the representations of harmful/harmless\nqueries along/opposite the direction in which the model's refusal probability\nincreases. Experiments with eight LLMs on out-of-domain benchmarks demonstrate\nthat DRO remarkably improves the safeguarding performance of human-crafted\nsafety prompts and outperforms strong baselines, without compromising the\ngeneral model capability.",
        "pdf_link": "https://arxiv.org/pdf/2401.18018v2.pdf"
    },
    {
        "title": "Large language model empowered participatory urban planning",
        "authors": [
            "Zhilun Zhou",
            "Yuming Lin",
            "Yong Li"
        ],
        "published": "2024-01-24T10:50:01Z",
        "summary": "Participatory urban planning is the mainstream of modern urban planning and\ninvolves the active engagement of different stakeholders. However, the\ntraditional participatory paradigm encounters challenges in time and manpower,\nwhile the generative planning tools fail to provide adjustable and inclusive\nsolutions. This research introduces an innovative urban planning approach\nintegrating Large Language Models (LLMs) within the participatory process. The\nframework, based on the crafted LLM agent, consists of role-play, collaborative\ngeneration, and feedback iteration, solving a community-level land-use task\ncatering to 1000 distinct interests. Empirical experiments in diverse urban\ncommunities exhibit LLM's adaptability and effectiveness across varied planning\nscenarios. The results were evaluated on four metrics, surpassing human experts\nin satisfaction and inclusion, and rivaling state-of-the-art reinforcement\nlearning methods in service and ecology. Further analysis shows the advantage\nof LLM agents in providing adjustable and inclusive solutions with natural\nlanguage reasoning and strong scalability. While implementing the recent\nadvancements in emulating human behavior for planning, this work envisions both\nplanners and citizens benefiting from low-cost, efficient LLM agents, which is\ncrucial for enhancing participation and realizing participatory urban planning.",
        "pdf_link": "https://arxiv.org/pdf/2402.01698v1.pdf"
    },
    {
        "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
        "authors": [
            "Xiaolong Jin",
            "Zhuo Zhang",
            "Xiangyu Zhang"
        ],
        "published": "2024-01-25T02:57:40Z",
        "summary": "Large Language Model (LLM) alignment aims to ensure that LLM outputs match\nwith human values. Researchers have demonstrated the severity of alignment\nproblems with a large spectrum of jailbreak techniques that can induce LLMs to\nproduce malicious content during conversations. Finding the corresponding\njailbreaking prompts usually requires substantial human intelligence or\ncomputation resources. In this paper, we report that LLMs have different levels\nof alignment in various contexts. As such, by systematically constructing many\ncontexts, called worlds, leveraging a Domain Specific Language describing\npossible worlds (e.g., time, location, characters, actions and languages) and\nthe corresponding compiler, we can cost-effectively expose latent alignment\nissues. Given the low cost of our method, we are able to conduct a large scale\nstudy regarding LLM alignment issues in different worlds. Our results show that\nour method outperforms the-state-of-the-art jailbreaking techniques on both\neffectiveness and efficiency. In addition, our results indicate that existing\nLLMs are extremely vulnerable to nesting worlds and programming language\nworlds. They imply that existing alignment training focuses on the real-world\nand is lacking in various (virtual) worlds where LLMs can be exploited.",
        "pdf_link": "https://arxiv.org/pdf/2402.01706v1.pdf"
    },
    {
        "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer",
        "authors": [
            "Zhen Tao",
            "Dinghao Xi",
            "Zhiyu Li",
            "Liumin Tang",
            "Wei Xu"
        ],
        "published": "2024-01-11T07:18:46Z",
        "summary": "Text style transfer is increasingly prominent in online entertainment and\nsocial media. However, existing research mainly concentrates on style transfer\nwithin individual English sentences, while ignoring the complexity of long\nChinese texts, which limits the wider applicability of style transfer in\ndigital media realm. To bridge this gap, we propose a Chinese Article-style\nTransfer framework (CAT-LLM), leveraging the capabilities of Large Language\nModels (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition\n(TSD) module aimed at comprehensively analyzing text features in articles,\nprompting LLMs to efficiently transfer Chinese article-style. The TSD module\nintegrates a series of machine learning algorithms to analyze article-style\nfrom both words and sentences levels, thereby aiding LLMs thoroughly grasp the\ntarget style without compromising the integrity of the original text. In\naddition, this module supports dynamic expansion of internal style trees,\nshowcasing robust compatibility and allowing flexible optimization in\nsubsequent research. Moreover, we select five Chinese articles with distinct\nstyles and create five parallel datasets using ChatGPT, enhancing the models'\nperformance evaluation accuracy and establishing a novel paradigm for\nevaluating subsequent research on article-style transfer. Extensive\nexperimental results affirm that CAT-LLM outperforms current research in terms\nof transfer accuracy and content preservation, and has remarkable applicability\nto various types of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.05707v1.pdf"
    },
    {
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "authors": [
            "Mingdao Liu",
            "Aohan Zeng",
            "Bowen Wang",
            "Peng Zhang",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "published": "2024-01-12T18:50:36Z",
        "summary": "The massive adoption of large language models (LLMs) demands efficient\ndeployment strategies. However, the auto-regressive decoding process, which is\nfundamental to how most LLMs generate text, poses challenges to achieve\nefficient serving. In this work, we introduce a parallel auto-regressive\ngeneration method. By instruct-tuning on general domain data that contains\nhierarchical structures, we enable LLMs to independently plan their generation\nprocess and perform auto-parallel auto-regressive (APAR) generation,\nsignificantly reducing the number of generation steps. APAR alone can achieve\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\nand attention computation during generation. This leads to a throughput\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\ncompared to state-of-the-art serving frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06761v1.pdf"
    },
    {
        "title": "Large Language Models Are Neurosymbolic Reasoners",
        "authors": [
            "Meng Fang",
            "Shilong Deng",
            "Yudi Zhang",
            "Zijing Shi",
            "Ling Chen",
            "Mykola Pechenizkiy",
            "Jun Wang"
        ],
        "published": "2024-01-17T16:57:19Z",
        "summary": "A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.09334v1.pdf"
    },
    {
        "title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
        "authors": [
            "Xu Huang",
            "Zhirui Zhang",
            "Xiang Geng",
            "Yichao Du",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published": "2024-01-12T13:23:21Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable results in the machine\ntranslation evaluation task, yet there remains a gap in knowledge regarding how\nthey utilize the provided data to conduct evaluations. This study aims to\nexplore how LLMs leverage source and reference information in evaluating\ntranslations, with the ultimate goal of better understanding the working\nmechanism of LLMs. To this end, we design the controlled experiments across\nvarious input modes and model types, and employ both coarse-grained and\nfine-grained prompts to discern the utility of source versus reference\ninformation. Surprisingly, we find that reference information significantly\nenhances the evaluation accuracy, while source information sometimes is\ncounterproductive, indicating a lack of cross-lingual capability when using\nLLMs to evaluate translations. We further conduct a meta-evaluation for\ntranslation error detection of LLMs, observing a similar phenomenon. These\nfindings also suggest a potential research direction for LLMs that fully\nexploits the cross-lingual capability of LLMs to achieve better performance in\nmachine translation evaluation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06568v1.pdf"
    },
    {
        "title": "Detection of Machine-Generated Text: Literature Survey",
        "authors": [
            "Dmytro Valiaiev"
        ],
        "published": "2024-01-02T01:44:15Z",
        "summary": "Since language models produce fake text quickly and easily, there is an\noversupply of such content in the public domain. The degree of sophistication\nand writing style has reached a point where differentiating between human\nauthored and machine-generated content is nearly impossible. As a result, works\ngenerated by language models rather than human authors have gained significant\nmedia attention and stirred controversy.Concerns regarding the possible\ninfluence of advanced language models on society have also arisen, needing a\nfuller knowledge of these processes. Natural language generation (NLG) and\ngenerative pre-trained transformer (GPT) models have revolutionized a variety\nof sectors: the scope not only permeated throughout journalism and customer\nservice but also reached academia. To mitigate the hazardous implications that\nmay arise from the use of these models, preventative measures must be\nimplemented, such as providing human agents with the capacity to distinguish\nbetween artificially made and human composed texts utilizing automated systems\nand possibly reverse-engineered language models. Furthermore, to ensure a\nbalanced and responsible approach, it is critical to have a full grasp of the\nsocio-technological ramifications of these breakthroughs. This literature\nsurvey aims to compile and synthesize accomplishments and developments in the\naforementioned work, while also identifying future prospects. It also gives an\noverview of machine-generated text trends and explores the larger societal\nimplications. Ultimately, this survey intends to contribute to the development\nof robust and effective approaches for resolving the issues connected with the\nusage and detection of machine-generated text by exploring the interplay\nbetween the capabilities of language models and their possible implications.",
        "pdf_link": "https://arxiv.org/pdf/2402.01642v1.pdf"
    },
    {
        "title": "Extreme Compression of Large Language Models via Additive Quantization",
        "authors": [
            "Vage Egiazarian",
            "Andrei Panferov",
            "Denis Kuznedelev",
            "Elias Frantar",
            "Artem Babenko",
            "Dan Alistarh"
        ],
        "published": "2024-01-11T18:54:44Z",
        "summary": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards quantization techniques for such models enabling execution on end-user\ndevices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression--defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter, from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our work builds on top of Additive Quantization, a classic\nalgorithm from the MCQ family, and adapts it to the quantization of language\nmodels. The resulting algorithm advances the state-of-the-art in LLM\ncompression, outperforming all recently-proposed techniques in terms of\naccuracy at a given compression budget. For instance, when compressing Llama 2\nmodels to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93\nperplexity (a 1.29 improvement relative to the best prior work, and 1.81 points\nfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B\nmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release our\nimplementation of Additive Quantization for Language Models AQLM as a baseline\nto facilitate future research in LLM quantization.",
        "pdf_link": "https://arxiv.org/pdf/2401.06118v2.pdf"
    },
    {
        "title": "The Neglected Tails of Vision-Language Models",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Tian Liu",
            "Xiangjue Dong",
            "Yanan Li",
            "Deva Ramanan",
            "James Caverlee",
            "Shu Kong"
        ],
        "published": "2024-01-23T01:25:00Z",
        "summary": "Vision-language models (VLMs) excel in zero-shot recognition but their\nperformance varies greatly across different visual concepts. For example,\nalthough CLIP achieves impressive accuracy on ImageNet (60-80%), its\nperformance drops below 10% for more than ten concepts like night snake,\npresumably due to their limited presence in the pretraining data. However,\nmeasuring the frequency of concepts in VLMs' large-scale datasets is\nchallenging. We address this by using large language models (LLMs) to count the\nnumber of pretraining texts that contain synonyms of these concepts. Our\nanalysis confirms that popular datasets, such as LAION, exhibit a long-tailed\nconcept distribution, yielding biased performance in VLMs. We also find that\ndownstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and\ntext-to-image models (e.g., Stable Diffusion), often fail to recognize or\ngenerate images of rare concepts identified by our method. To mitigate the\nimbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented\nLearning (REAL). First, instead of prompting VLMs using the original class\nnames, REAL uses their most frequent synonyms found in pretraining texts. This\nsimple change already outperforms costly human-engineered and LLM-enriched\nprompts over nine benchmark datasets. Second, REAL trains a linear classifier\non a small yet balanced set of pretraining data retrieved using concept\nsynonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage\nand 10,000x less training time!",
        "pdf_link": "https://arxiv.org/pdf/2401.12425v2.pdf"
    },
    {
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "authors": [
            "Pratyush Maini",
            "Zhili Feng",
            "Avi Schwarzschild",
            "Zachary C. Lipton",
            "J. Zico Kolter"
        ],
        "published": "2024-01-11T18:57:12Z",
        "summary": "Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.",
        "pdf_link": "https://arxiv.org/pdf/2401.06121v1.pdf"
    },
    {
        "title": "An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks",
        "authors": [
            "Chen Yang",
            "Peng Liang",
            "Zinan Ma"
        ],
        "published": "2024-01-08T03:50:03Z",
        "summary": "Stakeholders constantly make assumptions in the development of deep learning\n(DL) frameworks. These assumptions are related to various types of software\nartifacts (e.g., requirements, design decisions, and technical debt) and can\nturn out to be invalid, leading to system failures. Existing approaches and\ntools for assumption management usually depend on manual identification of\nassumptions. However, assumptions are scattered in various sources (e.g., code\ncomments, commits, pull requests, and issues) of DL framework development, and\nmanually identifying assumptions has high costs (e.g., time and resources). To\novercome the issues of manually identifying assumptions in DL framework\ndevelopment, we constructed a new and largest dataset (i.e., AssuEval) of\nassumptions collected from the TensorFlow and Keras repositories on GitHub;\nexplored the performance of seven traditional machine learning models (e.g.,\nSupport Vector Machine, Classification and Regression Trees), a popular DL\nmodel (i.e., ALBERT), and a large language model (i.e., ChatGPT) of identifying\nassumptions on the AssuEval dataset. The experiment results show that: ALBERT\nachieves the best performance (f1-score: 0.9584) of identifying assumptions on\nthe AssuEval dataset, which is much better than the other models (the 2nd best\nf1-score is 0.6211, achieved by ChatGPT). Though ChatGPT is the most popular\nlarge language model, we do not recommend using it to identify assumptions in\nDL framework development because of its low performance on the task.\nFine-tuning ChatGPT specifically for assumption identification could improve\nthe performance. This study provides researchers with the largest dataset of\nassumptions for further research (e.g., assumption classification, evaluation,\nand reasoning) and helps practitioners better understand assumptions and how to\nmanage them in their projects.",
        "pdf_link": "https://arxiv.org/pdf/2401.03653v3.pdf"
    },
    {
        "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
        "authors": [
            "Yiwei Qin",
            "Kaiqiang Song",
            "Yebowen Hu",
            "Wenlin Yao",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Xuansheng Wu",
            "Fei Liu",
            "Pengfei Liu",
            "Dong Yu"
        ],
        "published": "2024-01-07T23:01:56Z",
        "summary": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2401.03601v1.pdf"
    },
    {
        "title": "AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment",
        "authors": [
            "Sida Peng",
            "Wojciech Swiatek",
            "Allen Gao",
            "Paul Cullivan",
            "Haoge Chang"
        ],
        "published": "2024-01-19T05:54:35Z",
        "summary": "In recent years, generative AI has undergone major advancements,\ndemonstrating significant promise in augmenting human productivity. Notably,\nlarge language models (LLM), with ChatGPT-4 as an example, have drawn\nconsiderable attention. Numerous articles have examined the impact of LLM-based\ntools on human productivity in lab settings and designed tasks or in\nobservational studies. Despite recent advances, field experiments applying\nLLM-based tools in realistic settings are limited. This paper presents the\nfindings of a field randomized controlled trial assessing the effectiveness of\nLLM-based tools in providing unmonitored support services for information\nretrieval.",
        "pdf_link": "https://arxiv.org/pdf/2401.10956v1.pdf"
    },
    {
        "title": "Towards Conversational Diagnostic AI",
        "authors": [
            "Tao Tu",
            "Anil Palepu",
            "Mike Schaekermann",
            "Khaled Saab",
            "Jan Freyberg",
            "Ryutaro Tanno",
            "Amy Wang",
            "Brenna Li",
            "Mohamed Amin",
            "Nenad Tomasev",
            "Shekoofeh Azizi",
            "Karan Singhal",
            "Yong Cheng",
            "Le Hou",
            "Albert Webson",
            "Kavita Kulkarni",
            "S Sara Mahdavi",
            "Christopher Semturs",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2024-01-11T04:25:06Z",
        "summary": "At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.",
        "pdf_link": "https://arxiv.org/pdf/2401.05654v1.pdf"
    },
    {
        "title": "Extending LLMs' Context Window with 100 Samples",
        "authors": [
            "Yikai Zhang",
            "Junlong Li",
            "Pengfei Liu"
        ],
        "published": "2024-01-13T07:57:01Z",
        "summary": "Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.",
        "pdf_link": "https://arxiv.org/pdf/2401.07004v1.pdf"
    },
    {
        "title": "MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline",
        "authors": [
            "Minpeng Liao",
            "Wei Luo",
            "Chengxi Li",
            "Jing Wu",
            "Kai Fan"
        ],
        "published": "2024-01-16T08:08:01Z",
        "summary": "Large language models (LLMs) have seen considerable advancements in natural\nlanguage understanding tasks, yet there remains a gap to bridge before\nattaining true artificial general intelligence, especially concerning\nshortcomings in mathematical reasoning capabilities. We postulate that the\ninherent nature of LLM training, which focuses on predicting probabilities of\nnext token, presents challenges in effectively modeling mathematical reasoning\nthat demands exact calculations, both from data-driven and theoretical\nstandpoints. In this paper, we address this challenge by enriching the data\nlandscape and introducing a novel math dataset, enhanced with a capability to\nutilize a Python code interpreter. This dataset is derived from GSM8K and MATH\nand has been further refined through a combination of GPT-4 annotations, human\nreview, and self-training processes, where the errors in the original GSM8K\ntraining set have been fixed. Additionally, we propose a tentative, easily\nreplicable protocol for the fine-tuning of math-specific LLMs, which has led to\na significant improvement in the performance of a 7B-parameter LLM on the GSM8K\nand MATH datasets. We are committed to advancing the field of mathematical\nreasoning in LLMs and, to that end, we have made source code for data\ngeneration / training / inference, and the model checkpoints publicly available\nat \\url{https://github.com/MARIO-Math-Reasoning/MARIO}. We hope this will\nfacilitate further research and development within the community.",
        "pdf_link": "https://arxiv.org/pdf/2401.08190v3.pdf"
    },
    {
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "authors": [
            "Tianyu Cui",
            "Yanling Wang",
            "Chuanpu Fu",
            "Yong Xiao",
            "Sijia Li",
            "Xinhao Deng",
            "Yunpeng Liu",
            "Qinglin Zhang",
            "Ziyi Qiu",
            "Peiyang Li",
            "Zhixing Tan",
            "Junwu Xiong",
            "Xinyu Kong",
            "Zujie Wen",
            "Ke Xu",
            "Qi Li"
        ],
        "published": "2024-01-11T09:29:56Z",
        "summary": "Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.05778v1.pdf"
    },
    {
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
        "authors": [
            "Saleh Ashkboos",
            "Maximilian L. Croci",
            "Marcelo Gennari do Nascimento",
            "Torsten Hoefler",
            "James Hensman"
        ],
        "published": "2024-01-26T17:35:45Z",
        "summary": "Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression",
        "pdf_link": "https://arxiv.org/pdf/2401.15024v2.pdf"
    },
    {
        "title": "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",
        "authors": [
            "Shijue Huang",
            "Wanjun Zhong",
            "Jianqiao Lu",
            "Qi Zhu",
            "Jiahui Gao",
            "Weiwen Liu",
            "Yutai Hou",
            "Xingshan Zeng",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Ruifeng Xu",
            "Qun Liu"
        ],
        "published": "2024-01-30T16:52:56Z",
        "summary": "The recent trend of using Large Language Models (LLMs) as tool agents in\nreal-world applications underscores the necessity for comprehensive evaluations\nof their capabilities, particularly in complex scenarios involving planning,\ncreating, and using tools. However, existing benchmarks typically focus on\nsimple synthesized queries that do not reflect real-world complexity, thereby\noffering limited perspectives in evaluating tool utilization. To address this\nissue, we present UltraTool, a novel benchmark designed to improve and evaluate\nLLMs' ability in tool utilization within real-world scenarios. UltraTool\nfocuses on the entire process of using tools - from planning and creating to\napplying them in complex tasks. It emphasizes real-world complexities,\ndemanding accurate, multi-step planning for effective problem-solving. A key\nfeature of UltraTool is its independent evaluation of planning with natural\nlanguage, which happens before tool usage and simplifies the task solving by\nmapping out the intermediate steps. Thus, unlike previous work, it eliminates\nthe restriction of pre-defined toolset. Through extensive experiments on\nvarious LLMs, we offer novel insights into the evaluation of capabilities of\nLLMs in tool utilization, thereby contributing a fresh perspective to this\nrapidly evolving field. The benchmark is publicly available at\nhttps://github.com/JoeYing1019/UltraTool.",
        "pdf_link": "https://arxiv.org/pdf/2401.17167v2.pdf"
    },
    {
        "title": "Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection",
        "authors": [
            "Haoxin Liu",
            "Wenli Zhang",
            "Jiaheng Xie",
            "Buomsoo Kim",
            "Zhu Zhang",
            "Yidong Chai"
        ],
        "published": "2024-01-16T13:54:43Z",
        "summary": "This study harnesses state-of-the-art AI technology for chronic disease\nmanagement, specifically in detecting various mental disorders through\nuser-generated textual content. Existing studies typically rely on fully\nsupervised machine learning, which presents challenges such as the\nlabor-intensive manual process of annotating extensive training data for each\ndisease and the need to design specialized deep learning architectures for each\nproblem. To address such challenges, we propose a novel framework that\nleverages advanced AI techniques, including large language models and\nmulti-prompt engineering. Specifically, we address two key technical challenges\nin data-driven chronic disease management: (1) developing personalized prompts\nto represent each user's uniqueness and (2) incorporating medical knowledge\ninto prompts to provide context for chronic disease detection, instruct\nlearning objectives, and operationalize prediction goals. We evaluate our\nmethod using four mental disorders, which are prevalent chronic diseases\nworldwide, as research cases. On the depression detection task, our method (F1\n= 0.975~0.978) significantly outperforms traditional supervised learning\nparadigms, including feature engineering (F1 = 0.760) and architecture\nengineering (F1 = 0.756). Meanwhile, our approach demonstrates success in\nfew-shot learning, i.e., requiring only a minimal number of training examples\nto detect chronic diseases based on user-generated textual content (i.e., only\n2, 10, or 100 subjects). Moreover, our method can be generalized to other\nmental disorder detection tasks, including anorexia, pathological gambling, and\nself-harm (F1 = 0.919~0.978).",
        "pdf_link": "https://arxiv.org/pdf/2401.12988v1.pdf"
    },
    {
        "title": "Video Anomaly Detection and Explanation via Large Language Models",
        "authors": [
            "Hui Lv",
            "Qianru Sun"
        ],
        "published": "2024-01-11T07:09:44Z",
        "summary": "Video Anomaly Detection (VAD) aims to localize abnormal events on the\ntimeline of long-range surveillance videos. Anomaly-scoring-based methods have\nbeen prevailing for years but suffer from the high complexity of thresholding\nand low explanability of detection results. In this paper, we conduct pioneer\nresearch on equipping video-based large language models (VLLMs) in the\nframework of VAD, making the VAD model free from thresholds and able to explain\nthe reasons for the detected anomalies. We introduce a novel network module\nLong-Term Context (LTC) to mitigate the incapability of VLLMs in long-range\ncontext modeling. We design a three-phase training method to improve the\nefficiency of fine-tuning VLLMs by substantially minimizing the requirements\nfor VAD data and lowering the costs of annotating instruction-tuning data. Our\ntrained model achieves the top performance on the anomaly videos of the\nUCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%,\nrespectively. More impressively, our approach can provide textual explanations\nfor detected anomalies.",
        "pdf_link": "https://arxiv.org/pdf/2401.05702v1.pdf"
    },
    {
        "title": "Finetuning Large Language Models for Vulnerability Detection",
        "authors": [
            "Alexey Shestov",
            "Rodion Levichev",
            "Ravil Mussabayev",
            "Evgeny Maslov",
            "Anton Cheshkov",
            "Pavel Zadorozhny"
        ],
        "published": "2024-01-30T13:46:49Z",
        "summary": "This paper presents the results of finetuning large language models (LLMs)\nfor the task of detecting vulnerabilities in source code. We leverage\nWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and\nadapt it for vulnerability detection through further finetuning. To accelerate\ntraining, we modify WizardCoder's training procedure, also we investigate\noptimal training regimes. For the imbalanced dataset with many more negative\nexamples than positive, we also explore different techniques to improve\nclassification performance. The finetuned WizardCoder model achieves\nimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerability\ndatasets over CodeBERT-like model, demonstrating the effectiveness of adapting\npretrained LLMs for vulnerability detection in source code. The key\ncontributions are finetuning the state-of-the-art code LLM, WizardCoder,\nincreasing its training speed without the performance harm, optimizing the\ntraining procedure and regimes, handling class imbalance, and improving\nperformance on difficult vulnerability detection datasets. This demonstrates\nthe potential for transfer learning by finetuning large pretrained language\nmodels for specialized source code analysis tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.17010v4.pdf"
    },
    {
        "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
        "authors": [
            "Silin Gao",
            "Jane Dwivedi-Yu",
            "Ping Yu",
            "Xiaoqing Ellen Tan",
            "Ramakanth Pasunuru",
            "Olga Golovneva",
            "Koustuv Sinha",
            "Asli Celikyilmaz",
            "Antoine Bosselut",
            "Tianlu Wang"
        ],
        "published": "2024-01-30T21:53:30Z",
        "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.17464v2.pdf"
    },
    {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "authors": [
            "Gantavya Bhatt",
            "Yifang Chen",
            "Arnav M. Das",
            "Jifan Zhang",
            "Sang T. Truong",
            "Stephen Mussmann",
            "Yinglun Zhu",
            "Jeffrey Bilmes",
            "Simon S. Du",
            "Kevin Jamieson",
            "Jordan T. Ash",
            "Robert D. Nowak"
        ],
        "published": "2024-01-12T16:56:54Z",
        "summary": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "pdf_link": "https://arxiv.org/pdf/2401.06692v1.pdf"
    },
    {
        "title": "Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models",
        "authors": [
            "Minbyul Jeong",
            "Jiwoong Sohn",
            "Mujeen Sung",
            "Jaewoo Kang"
        ],
        "published": "2024-01-27T02:29:42Z",
        "summary": "Recent proprietary large language models (LLMs), such as GPT-4, have achieved\na milestone in tackling diverse challenges in the biomedical domain, ranging\nfrom multiple-choice questions to long-form generations. To address challenges\nthat still cannot be handled with the encoded knowledge of LLMs, various\nretrieval-augmented generation (RAG) methods have been developed by searching\ndocuments from the knowledge corpus and appending them unconditionally or\nselectively to the input of LLMs for generation. However, when applying\nexisting methods to different domain-specific problems, poor generalization\nbecomes apparent, leading to fetching incorrect documents or making inaccurate\njudgments. In this paper, we introduce Self-BioRAG, a framework reliable for\nbiomedical text that specializes in generating explanations, retrieving\ndomain-specific documents, and self-reflecting generated responses. We utilize\n84k filtered biomedical instruction sets to train Self-BioRAG that can assess\nits generated explanations with customized reflective tokens. Our work proves\nthat domain-specific components, such as a retriever, domain-related document\ncorpus, and instruction sets are necessary for adhering to domain-related\ninstructions. Using three major medical question-answering benchmark datasets,\nexperimental results of Self-BioRAG demonstrate significant performance gains\nby achieving a 7.2% absolute improvement on average over the state-of-the-art\nopen-foundation model with a parameter size of 7B or less. Overall, we analyze\nthat Self-BioRAG finds the clues in the question, retrieves relevant documents\nif needed, and understands how to answer with information from retrieved\ndocuments and encoded knowledge as a medical expert does. We release our data\nand code for training our framework components and model weights (7B and 13B)\nto enhance capabilities in biomedical and clinical domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.15269v2.pdf"
    },
    {
        "title": "Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding",
        "authors": [
            "Jie Tian",
            "Jixin Hou",
            "Zihao Wu",
            "Peng Shu",
            "Zhengliang Liu",
            "Yujie Xiang",
            "Beikang Gu",
            "Nicholas Filla",
            "Yiwei Li",
            "Ning Liu",
            "Xianyan Chen",
            "Keke Tang",
            "Tianming Liu",
            "Xianqiao Wang"
        ],
        "published": "2024-01-13T19:19:04Z",
        "summary": "This study is a pioneering endeavor to investigate the capabilities of Large\nLanguage Models (LLMs) in addressing conceptual questions within the domain of\nmechanical engineering with a focus on mechanics. Our examination involves a\nmanually crafted exam encompassing 126 multiple-choice questions, spanning\nvarious aspects of mechanics courses, including Fluid Mechanics, Mechanical\nVibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of\nElasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),\nChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against\nengineering faculties and students with or without mechanical engineering\nbackground. The findings reveal GPT-4's superior performance over the other two\nLLMs and human cohorts in answering questions across various mechanics topics,\nexcept for Continuum Mechanics. This signals the potential future improvements\nfor GPT models in handling symbolic calculations and tensor analyses. The\nperformances of LLMs were all significantly improved with explanations prompted\nprior to direct responses, underscoring the crucial role of prompt engineering.\nInterestingly, GPT-3.5 demonstrates improved performance with prompts covering\na broader domain, while GPT-4 excels with prompts focusing on specific\nsubjects. Finally, GPT-4 exhibits notable advancements in mitigating input\nbias, as evidenced by guessing preferences for humans. This study unveils the\nsubstantial potential of LLMs as highly knowledgeable assistants in both\nmechanical pedagogy and scientific research.",
        "pdf_link": "https://arxiv.org/pdf/2401.12983v1.pdf"
    },
    {
        "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
        "authors": [
            "Linzheng Chai",
            "Jian Yang",
            "Tao Sun",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Xiannian Liang",
            "Jiaqi Bai",
            "Tongliang Li",
            "Qiyao Peng",
            "Zhoujun Li"
        ],
        "published": "2024-01-13T10:53:53Z",
        "summary": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit\nreasoning in large language models and improve a variety of downstream tasks.\nCoT mainly demonstrates excellent performance in English, but its usage in\nlow-resource languages is constrained due to poor language generalization. To\nbridge the gap among different languages, we propose a cross-lingual\ninstruction fine-tuning framework (xCOT) to transfer knowledge from\nhigh-resource languages to low-resource languages. Specifically, the\nmultilingual instruction training data (xCOT-INSTRUCT) is created to encourage\nthe semantic alignment of multiple languages. We introduce cross-lingual\nin-context few-shot learning (xICL)) to accelerate multilingual agreement in\ninstruction tuning, where some fragments of source languages in examples are\nrandomly substituted by their counterpart translations of target languages.\nDuring multilingual instruction tuning, we adopt the randomly online CoT\nstrategy to enhance the multilingual reasoning ability of the large language\nmodel by first translating the query to another language and then answering in\nEnglish. To further facilitate the language transfer, we leverage the\nhigh-resource CoT to supervise the training of low-resource languages with\ncross-lingual distillation. Experimental results on previous benchmarks\ndemonstrate the superior performance of xCoT in reducing the gap among\ndifferent languages, highlighting its potential to reduce the cross-lingual\ngap.",
        "pdf_link": "https://arxiv.org/pdf/2401.07037v1.pdf"
    },
    {
        "title": "Generative Large Language Models are autonomous practitioners of evidence-based medicine",
        "authors": [
            "Akhil Vaid",
            "Joshua Lampert",
            "Juhee Lee",
            "Ashwin Sawant",
            "Donald Apakama",
            "Ankit Sakhuja",
            "Ali Soroush",
            "Denise Lee",
            "Isotta Landi",
            "Nicole Bussola",
            "Ismail Nabeel",
            "Robbie Freeman",
            "Patricia Kovatch",
            "Brendan Carr",
            "Benjamin Glicksberg",
            "Edgar Argulian",
            "Stamatios Lerakis",
            "Monica Kraft",
            "Alexander Charney",
            "Girish Nadkarni"
        ],
        "published": "2024-01-05T15:09:57Z",
        "summary": "Background: Evidence-based medicine (EBM) is fundamental to modern clinical\npractice, requiring clinicians to continually update their knowledge and apply\nthe best clinical evidence in patient care. The practice of EBM faces\nchallenges due to rapid advancements in medical research, leading to\ninformation overload for clinicians. The integration of artificial intelligence\n(AI), specifically Generative Large Language Models (LLMs), offers a promising\nsolution towards managing this complexity.\n  Methods: This study involved the curation of real-world clinical cases across\nvarious specialties, converting them into .json files for analysis. LLMs,\nincluding proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and\nopen-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models\nwere equipped with tools to retrieve information from case files and make\nclinical decisions similar to how clinicians must operate in the real world.\nModel performance was evaluated based on correctness of final answer, judicious\nuse of tools, conformity to guidelines, and resistance to hallucinations.\n  Results: GPT-4 was most capable of autonomous operation in a clinical\nsetting, being generally more effective in ordering relevant investigations and\nconforming to clinical guidelines. Limitations were observed in terms of model\nability to handle complex guidelines and diagnostic nuances. Retrieval\nAugmented Generation made recommendations more tailored to patients and\nhealthcare systems.\n  Conclusions: LLMs can be made to function as autonomous practitioners of\nevidence-based medicine. Their ability to utilize tooling can be harnessed to\ninteract with the infrastructure of a real-world healthcare system and perform\nthe tasks of patient management in a guideline directed manner. Prompt\nengineering may help to further enhance this potential and transform healthcare\nfor the clinician and the patient.",
        "pdf_link": "https://arxiv.org/pdf/2401.02851v1.pdf"
    },
    {
        "title": "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis",
        "authors": [
            "Zecheng Tang",
            "Chenfei Wu",
            "Zekai Zhang",
            "Mingheng Ni",
            "Shengming Yin",
            "Yu Liu",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Zicheng Liu",
            "Juntao Li",
            "Nan Duan"
        ],
        "published": "2024-01-30T15:20:26Z",
        "summary": "To leverage LLMs for visual synthesis, traditional methods convert raster\nimage information into discrete grid tokens through specialized visual modules,\nwhile disrupting the model's ability to capture the true semantic\nrepresentation of visual scenes. This paper posits that an alternative\nrepresentation of images, vector graphics, can effectively surmount this\nlimitation by enabling a more natural and semantically coherent segmentation of\nthe image information. Thus, we introduce StrokeNUWA, a pioneering work\nexploring a better visual representation ''stroke tokens'' on vector graphics,\nwhich is inherently visual semantics rich, naturally compatible with LLMs, and\nhighly compressed. Equipped with stroke tokens, StrokeNUWA can significantly\nsurpass traditional LLM-based and optimization-based methods across various\nmetrics in the vector graphic generation task. Besides, StrokeNUWA achieves up\nto a 94x speedup in inference over the speed of prior methods with an\nexceptional SVG code compression ratio of 6.9%.",
        "pdf_link": "https://arxiv.org/pdf/2401.17093v1.pdf"
    },
    {
        "title": "YODA: Teacher-Student Progressive Learning for Language Models",
        "authors": [
            "Jianqiao Lu",
            "Wanjun Zhong",
            "Yufei Wang",
            "Zhijiang Guo",
            "Qi Zhu",
            "Wenyong Huang",
            "Yanlin Wang",
            "Fei Mi",
            "Baojun Wang",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2024-01-28T14:32:15Z",
        "summary": "Although large language models (LLMs) have demonstrated adeptness in a range\nof tasks, they still lag behind human learning efficiency. This disparity is\noften linked to the inherent human capacity to learn from basic examples,\ngradually generalize and handle more complex problems, and refine their skills\nwith continuous feedback. Inspired by this, this paper introduces YODA, a novel\nteacher-student progressive learning framework that emulates the\nteacher-student education process to improve the efficacy of model fine-tuning.\nThe framework operates on an interactive \\textit{basic-generalized-harder}\nloop. The teacher agent provides tailored feedback on the student's answers,\nand systematically organizes the education process. This process unfolds by\nteaching the student basic examples, reinforcing understanding through\ngeneralized questions, and then enhancing learning by posing questions with\nprogressively enhanced complexity. With the teacher's guidance, the student\nlearns to iteratively refine its answer with feedback, and forms a robust and\ncomprehensive understanding of the posed questions. The systematic procedural\ndata, which reflects the progressive learning process of humans, is then\nutilized for model training. Taking math reasoning as a testbed, experiments\nshow that training LLaMA2 with data from YODA improves SFT with significant\nperformance gain (+17.01\\% on GSM8K and +9.98\\% on MATH). In addition, we find\nthat training with curriculum learning further improves learning robustness.",
        "pdf_link": "https://arxiv.org/pdf/2401.15670v1.pdf"
    },
    {
        "title": "ChemDFM: Dialogue Foundation Model for Chemistry",
        "authors": [
            "Zihan Zhao",
            "Da Ma",
            "Lu Chen",
            "Liangtai Sun",
            "Zihao Li",
            "Hongshen Xu",
            "Zichen Zhu",
            "Su Zhu",
            "Shuai Fan",
            "Guodong Shen",
            "Xin Chen",
            "Kai Yu"
        ],
        "published": "2024-01-26T12:45:55Z",
        "summary": "Large language models (LLMs) have established great success in the general\ndomain of natural language processing. Their emerging task generalization and\nfree-form dialogue capabilities can greatly help to design Chemical General\nIntelligence (CGI) to assist real-world research in chemistry. However, the\nexistence of specialized language and knowledge in the field of chemistry, such\nas the highly informative SMILES notation, hinders the performance of\ngeneral-domain LLMs in chemistry. To this end, we develop ChemDFM, the first\nLLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,\ntextbooks, and instructions as well as various data from the general domain.\nTherefore, it can store, understand, and reason over chemical knowledge and\nlanguages while still possessing advanced free-form language comprehension\ncapabilities. Extensive quantitative evaluation shows that ChemDFM can\nsignificantly outperform the representative open-sourced LLMs. Moreover,\nChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite\nthe significant size difference. Further qualitative evaluations demonstrate\nthe efficiency and effectiveness of ChemDFM in real-world research scenarios.\nWe will open-source the ChemDFM model soon.",
        "pdf_link": "https://arxiv.org/pdf/2401.14818v1.pdf"
    },
    {
        "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
        "authors": [
            "Yanda Chen",
            "Chandan Singh",
            "Xiaodong Liu",
            "Simiao Zuo",
            "Bin Yu",
            "He He",
            "Jianfeng Gao"
        ],
        "published": "2024-01-25T07:04:30Z",
        "summary": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .",
        "pdf_link": "https://arxiv.org/pdf/2401.13986v1.pdf"
    },
    {
        "title": "Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study",
        "authors": [
            "Ahmadul Karim Chowdhury",
            "Md. Saidur Rahman Sujon",
            "Md. Shirajus Salekin Shafi",
            "Tasin Ahmmad",
            "Sifat Ahmed",
            "Khan Md Hasib",
            "Faisal Muhammad Shah"
        ],
        "published": "2024-01-14T15:15:58Z",
        "summary": "In an era where the silent struggle of underdiagnosed depression pervades\nglobally, our research delves into the crucial link between mental health and\nsocial media. This work focuses on early detection of depression, particularly\nin extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our\nproposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning\nmodels(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT,\nSahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into\n\"Depressive\" and \"Non-Depressive\" segments, translated into Bengali by native\nspeakers with expertise in mental health, resulting in the creation of the\nBengali Social Media Depressive Dataset (BSMDD). Our work provides full\narchitecture details for each model and a methodical way to assess their\nperformance in Bengali depressive text categorization using zero-shot and\nfew-shot learning techniques. Our work demonstrates the superiority of\nSahajBERT and Bi-LSTM with FastText embeddings in their respective domains also\ntackles explainability issues with transformer models and emphasizes the\neffectiveness of LLMs, especially DepGPT, demonstrating flexibility and\ncompetence in a range of learning contexts. According to the experiment\nresults, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in\nzero-shot and few-shot scenarios but also every other model, achieving a\nnear-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and\nexceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B\nshow relatively poorer effectiveness in zero-shot and few-shot situations. The\nwork emphasizes the effectiveness and flexibility of LLMs in a variety of\nlinguistic circumstances, providing insightful information about the complex\nfield of depression detection models.",
        "pdf_link": "https://arxiv.org/pdf/2401.07310v1.pdf"
    },
    {
        "title": "Large Language Models Can Learn Temporal Reasoning",
        "authors": [
            "Siheng Xiong",
            "Ali Payani",
            "Ramana Kompella",
            "Faramarz Fekri"
        ],
        "published": "2024-01-12T19:00:26Z",
        "summary": "While large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they are not without their flaws and inaccuracies. Recent studies\nhave introduced various methods to mitigate these limitations. Temporal\nreasoning (TR), in particular, presents a significant challenge for LLMs due to\nits reliance on diverse temporal expressions and intricate contextual details.\nIn this paper, we propose TG-LLM, a new framework towards language-based TR. To\nbe specific, we first teach LLM to translate the context into a temporal graph\n(TG). A synthetic dataset, which is fully controllable and requires minimal\nsupervision, is constructed for fine-tuning on this graph translation task. We\nconfirm in experiments that the capability of TG extraction learned on our\ndataset can be transferred to other TR tasks and benchmarks. On top of that, we\nguide LLM to perform symbolic reasoning over the TG via Chain of Thoughts\n(CoTs) bootstrapping and special data augmentation strategies. We observe that\nCoTs with symbolic reasoning bring more consistent and reliable results than\nthose using free-form text.",
        "pdf_link": "https://arxiv.org/pdf/2401.06853v2.pdf"
    },
    {
        "title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
        "authors": [
            "Ron Sun"
        ],
        "published": "2024-01-19T01:14:45Z",
        "summary": "The paper discusses what is needed to address the limitations of current\nLLM-centered AI systems. The paper argues that incorporating insights from\nhuman cognition and psychology, as embodied by a computational cognitive\narchitecture, can help develop systems that are more capable, more reliable,\nand more human-like. It emphasizes the importance of the dual-process\narchitecture and the hybrid neuro-symbolic approach in addressing the\nlimitations of current LLMs. In the opposite direction, the paper also\nhighlights the need for an overhaul of computational cognitive architectures to\nbetter reflect advances in AI and computing technology. Overall, the paper\nadvocates for a multidisciplinary, mutually beneficial approach towards\ndeveloping better models both for AI and for understanding the human mind.",
        "pdf_link": "https://arxiv.org/pdf/2401.10444v1.pdf"
    },
    {
        "title": "\"You tell me\": A Dataset of GPT-4-Based Behaviour Change Support Conversations",
        "authors": [
            "Selina Meyer",
            "David Elsweiler"
        ],
        "published": "2024-01-29T13:54:48Z",
        "summary": "Conversational agents are increasingly used to address emotional needs on top\nof information needs. One use case of increasing interest are counselling-style\nmental health and behaviour change interventions, with large language model\n(LLM)-based approaches becoming more popular. Research in this context so far\nhas been largely system-focused, foregoing the aspect of user behaviour and the\nimpact this can have on LLM-generated texts. To address this issue, we share a\ndataset containing text-based user interactions related to behaviour change\nwith two GPT-4-based conversational agents collected in a preregistered user\nstudy. This dataset includes conversation data, user language analysis,\nperception measures, and user feedback for LLM-generated turns, and can offer\nvaluable insights to inform the design of such systems based on real\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2401.16167v2.pdf"
    },
    {
        "title": "Automated Fact-Checking of Climate Change Claims with Large Language Models",
        "authors": [
            "Markus Leippold",
            "Saeid Ashraf Vaghefi",
            "Dominik Stammbach",
            "Veruska Muccione",
            "Julia Bingler",
            "Jingwei Ni",
            "Chiara Colesanti-Senni",
            "Tobias Wekhof",
            "Tobias Schimanski",
            "Glen Gostlow",
            "Tingyu Yu",
            "Juerg Luterbacher",
            "Christian Huggel"
        ],
        "published": "2024-01-23T08:49:23Z",
        "summary": "This paper presents Climinator, a novel AI-based tool designed to automate\nthe fact-checking of climate change claims. Utilizing an array of Large\nLanguage Models (LLMs) informed by authoritative sources like the IPCC reports\nand peer-reviewed scientific literature, Climinator employs an innovative\nMediator-Advocate framework. This design allows Climinator to effectively\nsynthesize varying scientific perspectives, leading to robust, evidence-based\nevaluations. Our model demonstrates remarkable accuracy when testing claims\ncollected from Climate Feedback and Skeptical Science. Notably, when\nintegrating an advocate with a climate science denial perspective in our\nframework, Climinator's iterative debate process reliably converges towards\nscientific consensus, underscoring its adeptness at reconciling diverse\nviewpoints into science-based, factual conclusions. While our research is\nsubject to certain limitations and necessitates careful interpretation, our\napproach holds significant potential. We hope to stimulate further research and\nencourage exploring its applicability in other contexts, including political\nfact-checking and legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.12566v1.pdf"
    },
    {
        "title": "LLsM: Generative Linguistic Steganography with Large Language Model",
        "authors": [
            "Yihao Wang",
            "Ruiqi Song",
            "Ru Zhang",
            "Jianyi Liu",
            "Lingxiao Li"
        ],
        "published": "2024-01-28T13:21:44Z",
        "summary": "Linguistic Steganography (LS) tasks aim to generate steganographic text\n(stego) based on secret information. Only authorized recipients can perceive\nthe existence of the stegos and extract secrets, thereby preserving privacy.\nHowever, existing LS methods do not consider the controllable generation of\nstegos containing specific discourses such as style, genre, and theme. And they\nare difficult to simulate high-quality natural texts. As a result, the stegos\nare easily perceived and detectable, compromising covert communication. This\npaper proposes the LLsM, the first LS work with the Large Language Model (LLM).\nRegarding open-source LLMs, we reconstruct the token generator of LLM to the\n\"stego generator\" so that it can control the generation of stego based on the\nsecret. In this \"stego generator\", the candidate pool is encoded by range\ncoding, and the adjustment factor for the interval length is also given. The\nsecret determines the interval, thereby determining the next token. This better\nsimulates the distribution of natural texts and controls the adjustment of the\nembedding rate. In addition, we preliminarily built an LLsM-c architecture for\nclosed-source LLMs. It encodes discourse to obtain high-quality prompts\ncontaining discourse based on secrets, and generates pure natural texts\ncontaining discourse. Experiments show that LLsM performs superior to prevalent\nLS and related-task baselines regarding various kinds of concealment and\nanti-steganalysis. LLsM's MAUVE surpasses baselines by 60%-80% and\nanti-steganalysis exceeds baselines by 20%-30%. Notably, LLsM can also generate\nlonger stegos with high quality, showing its advantages in understanding and\ncoherence.",
        "pdf_link": "https://arxiv.org/pdf/2401.15656v3.pdf"
    },
    {
        "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
        "authors": [
            "Zi Yang",
            "Nan Hua"
        ],
        "published": "2024-01-10T02:20:48Z",
        "summary": "As LLMs have become capable of processing more complex types of inputs,\nresearchers have recently studied how to efficiently and affordably process\npossibly arbitrarily long sequences. One effective approach is to use a FIFO\nmemory to store keys and values of an attention sublayer from past chunks to\nallow subsequent queries to attend. However, this approach requires a large\nmemory and/or takes into the consideration the specific LM architecture.\nMoreover, due to the causal nature between the key-values in prior context and\nthe queries at present, this approach cannot be extended to bidirectional\nattention such as in an encoder-decoder or PrefixLM decoder-only architecture.\nIn this paper, we propose to use eviction policies, such as LRA and LFA, to\nreduce the memory size and adapt to various architectures, and we also propose\nthe Attendre layer, a wait-to-attend mechanism by retrieving the key-value\nmemory (K/V memory) with evicted queries in the query memory (Q memory). As a\nfirst step, we evaluate this method in the context length extension setup using\nthe TriviaQA reading comprehension task, and show the effectiveness of the\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2401.04881v1.pdf"
    },
    {
        "title": "Small Language Model Can Self-correct",
        "authors": [
            "Haixia Han",
            "Jiaqing Liang",
            "Jie Shi",
            "Qianyu He",
            "Yanghua Xiao"
        ],
        "published": "2024-01-14T14:29:07Z",
        "summary": "Generative Language Models (LMs) such as ChatGPT have exhibited remarkable\nperformance across various downstream tasks. Nevertheless, one of their most\nprominent drawbacks is generating inaccurate or false information with a\nconfident tone. Previous studies have devised sophisticated pipelines and\nprompts to induce large LMs to exhibit the capability for self-correction.\nHowever, large LMs are explicitly prompted to verify and modify its answers\nseparately rather than completing all steps spontaneously like humans.\nMoreover, these complex prompts are extremely challenging for small LMs to\nfollow. In this paper, we introduce the \\underline{I}ntrinsic\n\\underline{S}elf-\\underline{C}orrection (ISC) in generative language models,\naiming to correct the initial output of LMs in a self-triggered manner, even\nfor those small LMs with 6 billion parameters. Specifically, we devise a\npipeline for constructing self-correction data and propose Partial Answer\nMasking (PAM), aiming to endow the model with the capability for intrinsic\nself-correction through fine-tuning. We conduct experiments using LMs with\nparameters sizes ranging from 6 billion to 13 billion in two tasks, including\ncommonsense reasoning and factual knowledge reasoning. Our experiments\ndemonstrate that the outputs generated using ISC outperform those generated\nwithout self-correction. We believe that the output quality of even small LMs\ncan be further improved by empowering them with the ability to intrinsic\nself-correct.",
        "pdf_link": "https://arxiv.org/pdf/2401.07301v1.pdf"
    },
    {
        "title": "Conditional and Modal Reasoning in Large Language Models",
        "authors": [
            "Wesley H. Holliday",
            "Matthew Mandelkern"
        ],
        "published": "2024-01-30T16:56:54Z",
        "summary": "The reasoning abilities of large language models (LLMs) are the topic of a\ngrowing body of research in artificial intelligence and cognitive science. In\nthis paper, we probe the extent to which a dozen LLMs are able to distinguish\nlogically correct inferences from logically fallacious ones. We focus on\ninference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob\nhas a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must\nhave a king'). These inference patterns have been of special interest to\nlogicians, philosophers, and linguists, since they plausibly play a central\nrole in human reasoning. Assessing LLMs on these inference patterns is thus\nhighly relevant to the question of how much the reasoning abilities of LLMs\nmatch those of humans. Among the LLMs we tested, all but GPT-4 often make basic\nmistakes with conditionals. Moreover, even GPT-4 displays logically\ninconsistent judgments across inference patterns involving epistemic modals.",
        "pdf_link": "https://arxiv.org/pdf/2401.17169v1.pdf"
    },
    {
        "title": "PRE: A Peer Review Based Large Language Model Evaluator",
        "authors": [
            "Zhumin Chu",
            "Qingyao Ai",
            "Yiteng Tu",
            "Haitao Li",
            "Yiqun Liu"
        ],
        "published": "2024-01-28T12:33:14Z",
        "summary": "The impressive performance of large language models (LLMs) has attracted\nconsiderable attention from the academic and industrial communities. Besides\nhow to construct and train LLMs, how to effectively evaluate and compare the\ncapacity of LLMs has also been well recognized as an important yet difficult\nproblem. Existing paradigms rely on either human annotators or model-based\nevaluators to evaluate the performance of LLMs on different tasks. However,\nthese paradigms often suffer from high cost, low generalizability, and\ninherited biases in practice, which make them incapable of supporting the\nsustainable development of LLMs in long term. In order to address these issues,\ninspired by the peer review systems widely used in academic publication\nprocess, we propose a novel framework that can automatically evaluate LLMs\nthrough a peer-review process. Specifically, for the evaluation of a specific\ntask, we first construct a small qualification exam to select \"reviewers\" from\na couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written\nby different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to\nrate or compare the submissions. The final ranking of evaluatee LLMs is\ngenerated based on the results provided by all reviewers. We conducted\nextensive experiments on text summarization tasks with eleven LLMs including\nGPT-4. The results demonstrate the existence of biasness when evaluating using\na single LLM. Also, our PRE model outperforms all the baselines, illustrating\nthe effectiveness of the peer review mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2401.15641v1.pdf"
    },
    {
        "title": "Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?",
        "authors": [
            "Nir Fulman",
            "Abdulkadir Memduho\u011flu",
            "Alexander Zipf"
        ],
        "published": "2024-01-08T20:08:04Z",
        "summary": "We present a benchmark for assessing the capability of Large Language Models\n(LLMs) to discern intercardinal directions between geographic locations and\napply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark\nspecifically evaluates whether LLMs exhibit a hierarchical spatial bias similar\nto humans, where judgments about individual locations' spatial relationships\nare influenced by the perceived relationships of the larger groups that contain\nthem. To investigate this, we formulated 14 questions focusing on well-known\nAmerican cities. Seven questions were designed to challenge the LLMs with\nscenarios potentially influenced by the orientation of larger geographical\nunits, such as states or countries, while the remaining seven targeted\nlocations less susceptible to such hierarchical categorization. Among the\ntested models, GPT-4 exhibited superior performance with 55.3% accuracy,\nfollowed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed\nsignificantly reduced accuracy on tasks with suspected hierarchical bias. For\nexample, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on\nothers. Despite these inaccuracies, the models identified the nearest cardinal\ndirection in most cases, suggesting associative learning, embodying human-like\nmisconceptions. We discuss the potential of text-based data representing\ngeographic relationships directly to improve the spatial reasoning capabilities\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04218v1.pdf"
    },
    {
        "title": "LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging",
        "authors": [
            "Weimin Fu",
            "Kaichen Yang",
            "Raj Gautam Dutta",
            "Xiaolong Guo",
            "Gang Qu"
        ],
        "published": "2024-01-28T19:45:25Z",
        "summary": "This paper presents LLM4SecHW, a novel framework for hardware debugging that\nleverages domain specific Large Language Model (LLM). Despite the success of\nLLMs in automating various software development tasks, their application in the\nhardware security domain has been limited due to the constraints of commercial\nLLMs and the scarcity of domain specific data. To address these challenges, we\npropose a unique approach to compile a dataset of open source hardware design\ndefects and their remediation steps, utilizing version control data. This\ndataset provides a substantial foundation for training machine learning models\nfor hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this\ndataset, enabling the identification and rectification of bugs in hardware\ndesigns. This pioneering approach offers a reference workflow for the\napplication of fine tuning domain specific LLMs in other research areas. We\nevaluate the performance of our proposed system on various open source hardware\ndesigns, demonstrating its efficacy in accurately identifying and correcting\ndefects. Our work brings a new perspective on automating the quality control\nprocess in hardware design.",
        "pdf_link": "https://arxiv.org/pdf/2401.16448v1.pdf"
    },
    {
        "title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning",
        "authors": [
            "Shangqing Xu",
            "Chao Zhang"
        ],
        "published": "2024-01-12T00:11:24Z",
        "summary": "In-context learning with large language models (LLMs) excels at adapting to\nvarious tasks rapidly. However, its success hinges on carefully selecting\ndemonstrations, which remains an obstacle in practice. Current approaches to\nthis problem either rely on hard-to-acquire external supervision or require\nfrequent interactions with LLMs, resulting in high costs. We propose a new\nmethod called In-Context Reflection (ICR) to overcome these challenges. ICR\nstrategically selects demonstrations to reduce the discrepancy between the\nLLM's outputs and the actual input-output mappings. Specifically, ICR starts\nwith a random set of initial demonstrations, then iteratively refines it. In\neach step, it analyzes a pool of candidate examples and identifies the ones\nmost likely to challenge the LLM's current understanding, measured by a new\nmetric called misconfidence. These most confusing examples are then selected to\nreplace the less informative demonstrations in the current set. Our\ncomprehensive evaluation across five diverse datasets encompassing 13 subtasks\nshows the efficacy of ICR. Compared to existing methods, ICR achieves an\naverage performance boost of 4%, while demonstrating remarkable cross-task\ngeneralization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2401.06301v1.pdf"
    },
    {
        "title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring",
        "authors": [
            "Hasan Abu-Rasheed",
            "Mohamad Hussam Abdulsalam",
            "Christian Weber",
            "Madjid Fathi"
        ],
        "published": "2024-01-16T17:31:35Z",
        "summary": "Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.",
        "pdf_link": "https://arxiv.org/pdf/2401.08517v3.pdf"
    },
    {
        "title": "On Detecting Cherry-picking in News Coverage Using Large Language Models",
        "authors": [
            "Israa Jaradat",
            "Haiqi Zhang",
            "Chengkai Li"
        ],
        "published": "2024-01-11T04:03:35Z",
        "summary": "Cherry-picking refers to the deliberate selection of evidence or facts that\nfavor a particular viewpoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually identifying instances of\ncherry-picked statements in news stories can be challenging, particularly when\nthe opposing viewpoint's story is absent. This study introduces Cherry, an\ninnovative approach for automatically detecting cherry-picked statements in\nnews articles by finding missing important statements in the target news story.\nCherry utilizes the analysis of news coverage from multiple sources to identify\ninstances of cherry-picking. Our approach relies on language models that\nconsider contextual information from other news sources to classify statements\nbased on their importance to the event covered in the target news story.\nFurthermore, this research introduces a novel dataset specifically designed for\ncherry-picking detection, which was used to train and evaluate the performance\nof the models. Our best performing model achieves an F-1 score of about %89 in\ndetecting important statements when tested on unseen set of news stories.\nMoreover, results show the importance incorporating external knowledge from\nalternative unbiased narratives when assessing a statement's importance.",
        "pdf_link": "https://arxiv.org/pdf/2401.05650v1.pdf"
    },
    {
        "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
        "authors": [
            "Geetanjali Bihani",
            "Julia Taylor Rayz"
        ],
        "published": "2024-01-17T21:55:15Z",
        "summary": "The advent of large language models (LLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found that LLMs often resort to shortcuts when performing tasks,\ncreating an illusion of enhanced performance while lacking generalizability in\ntheir decision rules. This phenomenon introduces challenges in accurately\nassessing natural language understanding in LLMs. Our paper provides a concise\nsurvey of relevant research in this area and puts forth a perspective on the\nimplications of shortcut learning in the evaluation of language models,\nspecifically for NLU tasks. This paper urges more research efforts to be put\ntowards deepening our comprehension of shortcut learning, contributing to the\ndevelopment of more robust language models, and raising the standards of NLU\nevaluation in real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2401.09615v2.pdf"
    },
    {
        "title": "SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition",
        "authors": [
            "Yihan Wu",
            "Soumi Maiti",
            "Yifan Peng",
            "Wangyou Zhang",
            "Chenda Li",
            "Yuyue Wang",
            "Xihua Wang",
            "Shinji Watanabe",
            "Ruihua Song"
        ],
        "published": "2024-01-31T18:06:29Z",
        "summary": "Recent advancements in language models have significantly enhanced\nperformance in multiple speech-related tasks. Existing speech language models\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\na single model. However, this design omits the intrinsic connections between\ndifferent speech tasks, which can potentially boost the performance of each\ntask. In this work, we propose a novel decoder-only speech language model,\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\nrecognition, speech language modeling, and text language modeling --\nSpeechComposer can easily extend to more speech tasks via compositions of\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\nunification of prompt tokens also makes it possible for knowledge sharing among\ndifferent speech tasks in a more structured manner. Experimental results\ndemonstrate that our proposed SpeechComposer can improve the performance of\nboth primary tasks and composite tasks, showing the effectiveness of the shared\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\nand even better performance than the baselines which are expert models designed\nfor single tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.18045v1.pdf"
    },
    {
        "title": "Generating Zero-shot Abstractive Explanations for Rumour Verification",
        "authors": [
            "Iman Munire Bilal",
            "Preslav Nakov",
            "Rob Procter",
            "Maria Liakata"
        ],
        "published": "2024-01-23T12:29:37Z",
        "summary": "The task of rumour verification in social media concerns assessing the\nveracity of a claim on the basis of conversation threads that result from it.\nWhile previous work has focused on predicting a veracity label, here we\nreformulate the task to generate model-centric free-text explanations of a\nrumour's veracity. The approach is model agnostic in that it generalises to any\nmodel. Here we propose a novel GNN-based rumour verification model. We follow a\nzero-shot approach by first applying post-hoc explainability methods to score\nthe most important posts within a thread and then we use these posts to\ngenerate informative explanations using opinion-guided summarisation. To\nevaluate the informativeness of the explanatory summaries, we exploit the\nfew-shot learning capabilities of a large language model (LLM). Our experiments\nshow that LLMs can have similar agreement to humans in evaluating summaries.\nImportantly, we show explanatory abstractive summaries are more informative and\nbetter reflect the predicted rumour veracity than just using the highest\nranking posts in the thread.",
        "pdf_link": "https://arxiv.org/pdf/2401.12713v3.pdf"
    },
    {
        "title": "Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia",
        "authors": [
            "Balamurali B T",
            "Jer-Ming Chen"
        ],
        "published": "2024-01-30T07:55:43Z",
        "summary": "Large language models (LLMs) find increasing applications in many fields.\nHere, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in\ntheir current form, as publicly available - for their ability to recognize\nAlzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual\ninput derived from spontaneous speech recordings. Zero-shot learning approach\nis used at two levels of independent queries, with the second query\n(chain-of-thought prompting) eliciting more detailed than the first. Each LLM\nchatbot's performance is evaluated on the prediction generated in terms of\naccuracy, sensitivity, specificity, precision and F1 score. LLM chatbots\ngenerated three-class outcome (\"AD\", \"CN\", or \"Unsure\"). When positively\nidentifying AD, Bard produced highest true-positives (89% recall) and highest\nF1 score (71%), but tended to misidentify CN as AD, with high confidence (low\n\"Unsure\" rates); for positively identifying CN, GPT-4 resulted in the highest\ntrue-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance\n(moderate \"Unsure\" rates). Overall, three LLM chatbots identify AD vs CN\nsurpassing chance-levels but do not currently satisfy clinical application.",
        "pdf_link": "https://arxiv.org/pdf/2402.01751v1.pdf"
    },
    {
        "title": "Detecting Multimedia Generated by Large AI Models: A Survey",
        "authors": [
            "Li Lin",
            "Neeraj Gupta",
            "Yue Zhang",
            "Hainan Ren",
            "Chun-Hao Liu",
            "Feng Ding",
            "Xin Wang",
            "Xin Li",
            "Luisa Verdoliva",
            "Shu Hu"
        ],
        "published": "2024-01-22T15:08:19Z",
        "summary": "The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, and online\ndetection tools to provide a valuable resource for researchers and\npractitioners in this field. Furthermore, we identify current challenges in\ndetection and propose directions for future research that address unexplored,\nongoing, and emerging issues in detecting multimedia generated by LAIMs. Our\naim for this survey is to fill an academic gap and contribute to global AI\nsecurity efforts, helping to ensure the integrity of information in the digital\nrealm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.",
        "pdf_link": "https://arxiv.org/pdf/2402.00045v3.pdf"
    },
    {
        "title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics",
        "authors": [
            "Edward Y. Chang"
        ],
        "published": "2024-01-19T07:16:21Z",
        "summary": "Large language models (LLMs), while promising, face criticisms for biases,\nhallucinations, and a lack of reasoning capability. This paper introduces\nSocraSynth, a multi-LLM agent reasoning platform developed to mitigate these\nissues. SocraSynth utilizes conditional statistics and systematic context\nenhancement through continuous arguments, alongside adjustable debate\ncontentiousness levels. The platform typically involves a human moderator and\ntwo LLM agents representing opposing viewpoints on a given subject. SocraSynth\noperates in two main phases: knowledge generation and reasoning evaluation. In\nthe knowledge generation phase, the moderator defines the debate topic and\ncontentiousness level, prompting the agents to formulate supporting arguments\nfor their respective stances. The reasoning evaluation phase then employs\nSocratic reasoning and formal logic principles to appraise the quality of the\narguments presented. The dialogue concludes with the moderator adjusting the\ncontentiousness from confrontational to collaborative, gathering final,\nconciliatory remarks to aid in human reasoning and decision-making. Through\ncase studies in three distinct application domains, this paper showcases\nSocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,\ncomprehensive assessment, and enhanced collaboration. This underscores the\nvalue of multi-agent interactions in leveraging LLMs for advanced knowledge\nextraction and decision-making support.",
        "pdf_link": "https://arxiv.org/pdf/2402.06634v1.pdf"
    },
    {
        "title": "Critical Data Size of Language Models from a Grokking Perspective",
        "authors": [
            "Xuekai Zhu",
            "Yao Fu",
            "Bowen Zhou",
            "Zhouhan Lin"
        ],
        "published": "2024-01-19T03:24:36Z",
        "summary": "We explore the critical data size in language models, a threshold that marks\na fundamental shift from quick memorization to slow generalization. We\nformalize the phase transition under the grokking configuration into the Data\nEfficiency Hypothesis and identify data insufficiency, sufficiency, and surplus\nregimes in language models training dynamics. We develop a grokking\nconfiguration to reproduce grokking on simplistic language models stably by\nrescaling initialization and weight decay. We show that generalization occurs\nonly when language models reach a critical size. We analyze grokking across\nsample-wise and model-wise, verifying the proposed data efficiency hypothesis.\nOur experiments reveal smoother phase transitions occurring at the critical\ndataset size for language datasets. As the model size increases, this critical\npoint also becomes larger, indicating that larger models require more data. Our\nresults deepen the understanding of language model training, offering a novel\nperspective on the role of data in the learning mechanism of language models.",
        "pdf_link": "https://arxiv.org/pdf/2401.10463v2.pdf"
    },
    {
        "title": "Veagle: Advancements in Multimodal Representation Learning",
        "authors": [
            "Rajat Chawla",
            "Arkajit Datta",
            "Tushar Verma",
            "Adarsh Jha",
            "Anmol Gautam",
            "Ayush Vatsal",
            "Sukrit Chaterjee",
            "Mukunda NS",
            "Ishaan Bhola"
        ],
        "published": "2024-01-18T12:45:25Z",
        "summary": "Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08773v1.pdf"
    },
    {
        "title": "Enhanced Automated Code Vulnerability Repair using Large Language Models",
        "authors": [
            "David de-Fitero-Dominguez",
            "Eva Garcia-Lopez",
            "Antonio Garcia-Cabot",
            "Jose-Javier Martinez-Herraiz"
        ],
        "published": "2024-01-08T09:01:29Z",
        "summary": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.",
        "pdf_link": "https://arxiv.org/pdf/2401.03741v1.pdf"
    },
    {
        "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
        "authors": [
            "Jennifer Chien",
            "David Danks"
        ],
        "published": "2024-01-25T00:54:10Z",
        "summary": "Algorithmic harms are commonly categorized as either allocative or\nrepresentational. This study specifically addresses the latter, focusing on an\nexamination of current definitions of representational harms to discern what is\nincluded and what is not. This analysis motivates our expansion beyond\nbehavioral definitions to encompass harms to cognitive and affective states.\nThe paper outlines high-level requirements for measurement: identifying the\nnecessary expertise to implement this approach and illustrating it through a\ncase study. Our work highlights the unique vulnerabilities of large language\nmodels to perpetrating representational harms, particularly when these harms go\nunmeasured and unmitigated. The work concludes by presenting proposed\nmitigations and delineating when to employ them. The overarching aim of this\nresearch is to establish a framework for broadening the definition of\nrepresentational harms and to translate insights from fairness research into\npractical measurement and mitigation praxis.",
        "pdf_link": "https://arxiv.org/pdf/2402.01705v1.pdf"
    },
    {
        "title": "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance",
        "authors": [
            "Abel Salinas",
            "Fred Morstatter"
        ],
        "published": "2024-01-08T08:28:08Z",
        "summary": "Large Language Models (LLMs) are regularly being used to label data across\nmany domains and for myriad tasks. By simply asking the LLM for an answer, or\n``prompting,'' practitioners are able to use LLMs to quickly get a response for\nan arbitrary task. This prompting is done through a series of decisions by the\npractitioner, from simple wording of the prompt, to requesting the output in a\ncertain data format, to jailbreaking in the case of prompts that address more\nsensitive topics. In this work, we ask: do variations in the way a prompt is\nconstructed change the ultimate decision of the LLM? We answer this using a\nseries of prompt variations across a variety of text classification tasks. We\nfind that even the smallest of perturbations, such as adding a space at the end\nof a prompt, can cause the LLM to change its answer. Further, we find that\nrequesting responses in XML and commonly used jailbreaks can have cataclysmic\neffects on the data labeled by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.03729v3.pdf"
    },
    {
        "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
        "authors": [
            "Shengzhe Xu",
            "Christo Kurisummoottil Thomas",
            "Omar Hashash",
            "Nikhil Muralidhar",
            "Walid Saad",
            "Naren Ramakrishnan"
        ],
        "published": "2024-01-30T00:21:41Z",
        "summary": "Large language models (LLMs) and foundation models have been recently touted\nas a game-changer for 6G systems. However, recent efforts on LLMs for wireless\nnetworks are limited to a direct application of existing language models that\nwere designed for natural language processing (NLP) applications. To address\nthis challenge and create wireless-centric foundation models, this paper\npresents a comprehensive vision on how to design universal foundation models\nthat are tailored towards the deployment of artificial intelligence (AI)-native\nnetworks. Diverging from NLP-based foundation models, the proposed framework\npromotes the design of large multi-modal models (LMMs) fostered by three key\ncapabilities: 1) processing of multi-modal sensing data, 2) grounding of\nphysical symbol representations in real-world wireless systems using causal\nreasoning and retrieval-augmented generation (RAG), and 3) enabling\ninstructibility from the wireless environment feedback to facilitate dynamic\nnetwork adaptation thanks to logical and mathematical reasoning facilitated by\nneuro-symbolic AI. In essence, these properties enable the proposed LMM\nframework to build universal capabilities that cater to various cross-layer\nnetworking tasks and alignment of intents across different domains. Preliminary\nresults from experimental evaluation demonstrate the efficacy of grounding\nusing RAG in LMMs, and showcase the alignment of LMMs with wireless system\ndesigns. Furthermore, the enhanced rationale exhibited in the responses to\nmathematical questions by LMMs, compared to vanilla LLMs, demonstrates the\nlogical and mathematical reasoning capabilities inherent in LMMs. Building on\nthose results, we present a sequel of open questions and challenges for LMMs.\nWe then conclude with a set of recommendations that ignite the path towards\nLMM-empowered AI-native systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.01748v2.pdf"
    },
    {
        "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
        "authors": [
            "Renjie Pi",
            "Tianyang Han",
            "Yueqi Xie",
            "Rui Pan",
            "Qing Lian",
            "Hanze Dong",
            "Jipeng Zhang",
            "Tong Zhang"
        ],
        "published": "2024-01-05T17:05:42Z",
        "summary": "The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.",
        "pdf_link": "https://arxiv.org/pdf/2401.02906v2.pdf"
    },
    {
        "title": "TP-Aware Dequantization",
        "authors": [
            "Adnan Hoque",
            "Mudhakar Srivatsa",
            "Chih-Chieh Yang",
            "Raghu Ganti"
        ],
        "published": "2024-01-15T08:01:40Z",
        "summary": "In this paper, we present a novel method that reduces model inference latency\nduring distributed deployment of Large Language Models (LLMs). Our contribution\nis an optimized inference deployment scheme that address the current\nlimitations of state-of-the-art quantization kernels when used in conjunction\nwith Tensor Parallel (TP). Our method preserves data locality in GPU memory\naccess patterns and exploits a priori knowledge of TP to reduce global\ncommunication. We demonstrate an up to 1.81x speedup over existing methods for\nLlama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer\nproblem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.04925v1.pdf"
    },
    {
        "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review",
        "authors": [
            "Luoma Ke",
            "Song Tong",
            "Peng Cheng",
            "Kaiping Peng"
        ],
        "published": "2024-01-03T03:01:29Z",
        "summary": "This paper explores the frontiers of large language models (LLMs) in\npsychology applications. Psychology has undergone several theoretical changes,\nand the current use of Artificial Intelligence (AI) and Machine Learning,\nparticularly LLMs, promises to open up new research directions. We provide a\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\nresearch. It discusses the impact of LLMs across various branches of\npsychology, including cognitive and behavioral, clinical and counseling,\neducational and developmental, and social and cultural psychology, highlighting\ntheir potential to simulate aspects of human cognition and behavior. The paper\ndelves into the capabilities of these models to emulate human-like text\ngeneration, offering innovative tools for literature review, hypothesis\ngeneration, experimental design, experimental subjects, data analysis, academic\nwriting, and peer review in psychology. While LLMs are essential in advancing\nresearch methodologies in psychology, the paper also cautions about their\ntechnical and ethical challenges. There are issues like data privacy, the\nethical implications of using LLMs in psychological research, and the need for\na deeper understanding of these models' limitations. Researchers should\nresponsibly use LLMs in psychological studies, adhering to ethical standards\nand considering the potential consequences of deploying these technologies in\nsensitive areas. Overall, the article provides a comprehensive overview of the\ncurrent state of LLMs in psychology, exploring potential benefits and\nchallenges. It serves as a call to action for researchers to leverage LLMs'\nadvantages responsibly while addressing associated risks.",
        "pdf_link": "https://arxiv.org/pdf/2401.01519v3.pdf"
    },
    {
        "title": "Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging",
        "authors": [
            "Jai Prakash Veerla",
            "Poojitha Thota",
            "Partha Sai Guttikonda",
            "Shirin Nilizadeh",
            "Jacob M. Luber"
        ],
        "published": "2024-01-04T22:49:15Z",
        "summary": "In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.",
        "pdf_link": "https://arxiv.org/pdf/2401.02565v2.pdf"
    },
    {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published": "2024-01-02T17:56:30Z",
        "summary": "As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01313v3.pdf"
    },
    {
        "title": "GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries",
        "authors": [
            "Yuyuan Feng",
            "Guosheng Hu",
            "Zhihong Zhang"
        ],
        "published": "2024-01-30T14:47:15Z",
        "summary": "State of health (SOH) is a crucial indicator for assessing the degradation\nlevel of batteries that cannot be measured directly but requires estimation.\nAccurate SOH estimation enhances detection, control, and feedback for Li-ion\nbatteries, allowing for safe and efficient energy management and guiding the\ndevelopment of new-generation batteries. Despite the significant progress in\ndata-driven SOH estimation, the time and resource-consuming degradation\nexperiments for generating lifelong training data pose a challenge in\nestablishing one large model capable of handling diverse types of Li-ion\nbatteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity.\nHence, this paper utilizes the strong generalization capability of large\nlanguage model (LLM) to proposes a novel framework for adaptable SOH estimation\nacross diverse batteries. To match the real scenario where unlabeled data\nsequentially arrives in use with distribution shifts, the proposed model is\nmodified by a test-time training technique to ensure estimation accuracy even\nat the battery's end of life. The validation results demonstrate that the\nproposed framework achieves state-of-the-art accuracy on four widely recognized\ndatasets collected from 62 batteries. Furthermore, we analyze the theoretical\nchallenges of cross-battery estimation and provide a quantitative explanation\nof the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2402.00068v1.pdf"
    },
    {
        "title": "Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code",
        "authors": [
            "Beiqi Zhang",
            "Peng Liang",
            "Qiong Feng",
            "Yujia Fu",
            "Zengyang Li"
        ],
        "published": "2024-01-25T13:39:54Z",
        "summary": "As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released on\nSeptember 2023, functions as an interactive tool aims at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot's\nability to fix the code smells it generates. To this end, we built a dataset\ncomprising 102 code smells in Copilot-generated Python code. Our aim is to\nfirst explore the occurrence of code smells in Copilot-generated Python code\nand then evaluate the effectiveness of Copilot in fixing these code smells\nemploying different prompts. The results show that 8 out of 10 types of Python\nsmells can be detected in Copilot-generated Python code, among which\nMultiply-Nested Container is the most common one. For these code smells,\nCopilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing\nPython code smells generated by Copilot itself. Besides, the effectiveness of\nCopilot Chat in fixing these smells can be improved with the provision of more\ndetailed prompts. However, using Copilot Chat to fix these smells might\nintroduce new code smells.",
        "pdf_link": "https://arxiv.org/pdf/2401.14176v1.pdf"
    },
    {
        "title": "\"Which LLM should I use?\": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students",
        "authors": [
            "Vibhor Agarwal",
            "Madhav Krishan Garg",
            "Sahiti Dharmavaram",
            "Dhruv Kumar"
        ],
        "published": "2024-01-22T15:11:36Z",
        "summary": "This study evaluates the effectiveness of various large language models\n(LLMs) in performing tasks common among undergraduate computer science\nstudents. Although a number of research studies in the computing education\ncommunity have explored the possibility of using LLMs for a variety of tasks,\nthere is a lack of comprehensive research comparing different LLMs and\nevaluating which LLMs are most effective for different tasks. Our research\nsystematically assesses some of the publicly available LLMs such as Google\nBard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse\ntasks commonly encountered by undergraduate computer science students in India.\nThese tasks include code explanation and documentation, solving class\nassignments, technical interview preparation, learning new concepts and\nframeworks, and email writing. Evaluation for these tasks was carried out by\npre-final year and final year undergraduate computer science students and\nprovides insights into the models' strengths and limitations. This study aims\nto guide students as well as instructors in selecting suitable LLMs for any\nspecific task and offers valuable insights on how LLMs can be used\nconstructively by students and instructors.",
        "pdf_link": "https://arxiv.org/pdf/2402.01687v2.pdf"
    },
    {
        "title": "The Reasoning Under Uncertainty Trap: A Structural AI Risk",
        "authors": [
            "Toby D. Pilditch"
        ],
        "published": "2024-01-29T17:16:57Z",
        "summary": "This report examines a novel risk associated with current (and projected) AI\ntools. Making effective decisions about future actions requires us to reason\nunder uncertainty (RUU), and doing so is essential to many critical real world\nproblems. Overfaced by this challenge, there is growing demand for AI tools\nlike LLMs to assist decision-makers. Having evidenced this demand and the\nincentives behind it, we expose a growing risk: we 1) do not currently\nsufficiently understand LLM capabilities in this regard, and 2) have no\nguarantees of performance given fundamental computational explosiveness and\ndeep uncertainty constraints on accuracy. This report provides an exposition of\nwhat makes RUU so challenging for both humans and machines, and relates these\ndifficulties to prospective AI timelines and capabilities. Having established\nthis current potential misuse risk, we go on to expose how this seemingly\nadditive risk (more misuse additively contributed to potential harm) in fact\nhas multiplicative properties. Specifically, we detail how this misuse risk\nconnects to a wider network of underlying structural risks (e.g., shifting\nincentives, limited transparency, and feedback loops) to produce non-linear\nharms. We go on to provide a solutions roadmap that targets multiple leverage\npoints in the structure of the problem. This includes recommendations for all\ninvolved actors (prospective users, developers, and policy-makers) and enfolds\ninsights from areas including Decision-making Under Deep Uncertainty and\ncomplex systems theory. We argue this report serves not only to raise awareness\n(and subsequently mitigate/correct) of a current, novel AI risk, but also\nawareness of the underlying class of structural risks by illustrating how their\ninterconnected nature poses twin-dangers of camouflaging their presence, whilst\namplifying their potential effects.",
        "pdf_link": "https://arxiv.org/pdf/2402.01743v1.pdf"
    }
]