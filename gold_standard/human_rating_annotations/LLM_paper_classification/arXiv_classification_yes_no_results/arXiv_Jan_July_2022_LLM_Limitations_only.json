[
    {
        "title": "GitHub Copilot AI pair programmer: Asset or Liability?",
        "authors": [
            "Arghavan Moradi Dakhel",
            "Vahid Majdinasab",
            "Amin Nikanjam",
            "Foutse Khomh",
            "Michel C. Desmarais",
            "Zhen Ming",
            "Jiang"
        ],
        "published": "2022-06-30T15:00:03Z",
        "summary": "Automatic program synthesis is a long-lasting dream in software engineering.\nRecently, a promising Deep Learning (DL) based solution, called Copilot, has\nbeen proposed by OpenAI and Microsoft as an industrial product. Although some\nstudies evaluate the correctness of Copilot solutions and report its issues,\nmore empirical evaluations are necessary to understand how developers can\nbenefit from it effectively. In this paper, we study the capabilities of\nCopilot in two different programming tasks: (i) generating (and reproducing)\ncorrect and efficient solutions for fundamental algorithmic problems, and (ii)\ncomparing Copilot's proposed solutions with those of human programmers on a set\nof programming tasks. For the former, we assess the performance and\nfunctionality of Copilot in solving selected fundamental problems in computer\nscience, like sorting and implementing data structures. In the latter, a\ndataset of programming problems with human-provided solutions is used. The\nresults show that Copilot is capable of providing solutions for almost all\nfundamental algorithmic problems, however, some solutions are buggy and\nnon-reproducible. Moreover, Copilot has some difficulties in combining multiple\nmethods to generate a solution. Comparing Copilot to humans, our results show\nthat the correct ratio of humans' solutions is greater than Copilot's\nsuggestions, while the buggy solutions generated by Copilot require less effort\nto be repaired.",
        "pdf_link": "https://arxiv.org/pdf/2206.15331v2.pdf"
    },
    {
        "title": "Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding",
        "authors": [
            "Connor Holmes",
            "Minjia Zhang",
            "Yuxiong He",
            "Bo Wu"
        ],
        "published": "2022-06-30T04:33:50Z",
        "summary": "In recent years, large pre-trained Transformer networks have demonstrated\ndramatic improvements in many natural language understanding tasks. However,\nthe huge size of these models brings significant challenges to their\nfine-tuning and online deployment due to latency and cost constraints. New\nhardware supporting both N:M semi-structured sparsity and low-precision integer\ncomputation is a promising solution to boost DNN model serving efficiency.\nHowever, there have been very few studies that systematically investigate to\nwhat extent pre-trained Transformer networks benefit from the combination of\nthese techniques, as well as how to best compress each component of the\nTransformer. We propose a flexible compression framework NxMiFormer that\nperforms simultaneous sparsification and quantization using ADMM and STE-based\nQAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm\nthat identifies promising heterogeneous compression configurations that meet a\ncompression ratio constraint. When evaluated across the GLUE suite of NLU\nbenchmarks, our approach can achieve up to 93% compression of the encoders of a\nBERT model while retaining 98.2% of the original model accuracy and taking full\nadvantage of the hardware's capabilities. Heterogeneous configurations found\nthe by the search heuristic maintain 99.5% of the baseline accuracy while still\ncompressing the model by 87.5%.",
        "pdf_link": "https://arxiv.org/pdf/2206.15014v1.pdf"
    },
    {
        "title": "Solving Quantitative Reasoning Problems with Language Models",
        "authors": [
            "Aitor Lewkowycz",
            "Anders Andreassen",
            "David Dohan",
            "Ethan Dyer",
            "Henryk Michalewski",
            "Vinay Ramasesh",
            "Ambrose Slone",
            "Cem Anil",
            "Imanol Schlag",
            "Theo Gutman-Solo",
            "Yuhuai Wu",
            "Behnam Neyshabur",
            "Guy Gur-Ari",
            "Vedant Misra"
        ],
        "published": "2022-06-29T18:54:49Z",
        "summary": "Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.",
        "pdf_link": "https://arxiv.org/pdf/2206.14858v2.pdf"
    },
    {
        "title": "Knowledge Distillation of Transformer-based Language Models Revisited",
        "authors": [
            "Chengqiang Lu",
            "Jianwei Zhang",
            "Yunfei Chu",
            "Zhengyu Chen",
            "Jingren Zhou",
            "Fei Wu",
            "Haiqing Chen",
            "Hongxia Yang"
        ],
        "published": "2022-06-29T02:16:56Z",
        "summary": "In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2206.14366v3.pdf"
    },
    {
        "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
        "authors": [
            "Pu Wang",
            "Hugo Van hamme"
        ],
        "published": "2022-06-28T23:08:32Z",
        "summary": "End-to-end spoken language understanding (SLU) systems benefit from\npretraining on large corpora, followed by fine-tuning on application-specific\ndata. The resulting models are too large for on-edge applications. For\ninstance, BERT-based systems contain over 110M parameters. Observing the model\nis overparameterized, we propose lean transformer structure where the dimension\nof the attention mechanism is automatically reduced using group sparsity. We\npropose a variant where the learned attention subspace is transferred to an\nattention bottleneck layer. In a low-resource setting and without pre-training,\nthe resulting compact SLU model achieves accuracies competitive with\npre-trained large models.",
        "pdf_link": "https://arxiv.org/pdf/2206.14318v1.pdf"
    },
    {
        "title": "CC-Riddle: A Question Answering Dataset of Chinese Character Riddles",
        "authors": [
            "Fan Xu",
            "Yunxiang Zhang",
            "Xiaojun Wan"
        ],
        "published": "2022-06-28T06:23:13Z",
        "summary": "The Chinese character riddle is a unique form of cultural entertainment\nspecific to the Chinese language. It typically comprises two parts: the riddle\ndescription and the solution. The solution to the riddle is a single character,\nwhile the riddle description primarily describes the glyph of the solution,\noccasionally supplemented with its explanation and pronunciation. Solving\nChinese character riddles is a challenging task that demands understanding of\ncharacter glyph, general knowledge, and a grasp of figurative language. In this\npaper, we construct a \\textbf{C}hinese \\textbf{C}haracter riddle dataset named\nCC-Riddle, which covers the majority of common simplified Chinese characters.\nThe construction process is a combination of web crawling, language model\ngeneration and manual filtering. In generation stage, we input the Chinese\nphonetic alphabet, glyph and meaning of the solution character into the\ngeneration model, which then produces multiple riddle descriptions. The\ngenerated riddles are then manually filtered and the final CC-Riddle dataset is\ncomposed of both human-written riddles and these filtered, generated riddles.\nIn order to assess the performance of language models on the task of solving\ncharacter riddles, we use retrieval-based, generative and multiple-choice QA\nstrategies to test three language models: BERT, ChatGPT and ChatGLM. The test\nresults reveal that current language models still struggle to solve Chinese\ncharacter riddles. CC-Riddle is publicly available at\n\\url{https://github.com/pku0xff/CC-Riddle}.",
        "pdf_link": "https://arxiv.org/pdf/2206.13778v2.pdf"
    },
    {
        "title": "Flexible text generation for counterfactual fairness probing",
        "authors": [
            "Zee Fryer",
            "Vera Axelrod",
            "Ben Packer",
            "Alex Beutel",
            "Jilin Chen",
            "Kellie Webster"
        ],
        "published": "2022-06-28T05:07:20Z",
        "summary": "A common approach for testing fairness issues in text-based classifiers is\nthrough the use of counterfactuals: does the classifier output change if a\nsensitive attribute in the input is changed? Existing counterfactual generation\nmethods typically rely on wordlists or templates, producing simple\ncounterfactuals that don't take into account grammar, context, or subtle\nsensitive attribute references, and could miss issues that the wordlist\ncreators had not considered. In this paper, we introduce a task for generating\ncounterfactuals that overcomes these shortcomings, and demonstrate how large\nlanguage models (LLMs) can be leveraged to make progress on this task. We show\nthat this LLM-based method can produce complex counterfactuals that existing\nmethods cannot, comparing the performance of various counterfactual generation\nmethods on the Civil Comments dataset and showing their value in evaluating a\ntoxicity classifier.",
        "pdf_link": "https://arxiv.org/pdf/2206.13757v1.pdf"
    },
    {
        "title": "NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts",
        "authors": [
            "Supriti Vijay",
            "Aman Priyanshu"
        ],
        "published": "2022-06-28T03:22:55Z",
        "summary": "With increasing applications in areas such as biomedical information\nextraction pipelines and social media analytics, Named Entity Recognition (NER)\nhas become an indispensable tool for knowledge extraction. However, with the\ngradual shift in language structure and vocabulary, NERs are plagued with\ndistribution shifts, making them redundant or not as profitable without\nre-training. Re-training NERs based on Large Language Models (LLMs) from\nscratch over newly acquired data poses economic disadvantages. In contrast,\nre-training only with newly acquired data will result in Catastrophic\nForgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a\npipeline for training NERs with LLM bases by incorporating the concept of\nElastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As\nwe believe our work has implications to be utilized in the pipeline of\ncontinual learning and NER, we open-source our code as well as provide the\nfine-tuning library of the same name NERDA-Con at\nhttps://github.com/SupritiVijay/NERDA-Con and\nhttps://pypi.org/project/NERDA-Con/.",
        "pdf_link": "https://arxiv.org/pdf/2206.14607v1.pdf"
    },
    {
        "title": "A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages",
        "authors": [
            "Akhter Al Amin",
            "Kazi Sinthia Kabir"
        ],
        "published": "2022-06-23T21:57:08Z",
        "summary": "Language models (LM) are becoming prevalent in many language-based\napplication spaces globally. Although these LMs are improving our day-to-day\ninteractions with digital products, concerns remain whether open-ended\nlanguages or text generated from these models reveal any biases toward a\nspecific group of people, thereby risking the usability of a certain product.\nThere is a need to identify whether these models possess bias to improve the\nfairness in these models. This gap motivates our ongoing work, where we\nmeasured the two aspects of bias in GPT-3 generated text through a disability\nlens.",
        "pdf_link": "https://arxiv.org/pdf/2206.11993v1.pdf"
    },
    {
        "title": "BERT Rankers are Brittle: a Study using Adversarial Document Perturbations",
        "authors": [
            "Yumeng Wang",
            "Lijun Lyu",
            "Avishek Anand"
        ],
        "published": "2022-06-23T14:16:48Z",
        "summary": "Contextual ranking models based on BERT are now well established for a wide\nrange of passage and document ranking tasks. However, the robustness of\nBERT-based ranking models under adversarial inputs is under-explored. In this\npaper, we argue that BERT-rankers are not immune to adversarial attacks\ntargeting retrieved documents given a query. Firstly, we propose algorithms for\nadversarial perturbation of both highly relevant and non-relevant documents\nusing gradient-based optimization methods. The aim of our algorithms is to\nadd/replace a small number of tokens to a highly relevant or non-relevant\ndocument to cause a large rank demotion or promotion. Our experiments show that\na small number of tokens can already result in a large change in the rank of a\ndocument. Moreover, we find that BERT-rankers heavily rely on the document\nstart/head for relevance prediction, making the initial part of the document\nmore susceptible to adversarial attacks. More interestingly, we find a small\nset of recurring adversarial words that when added to documents result in\nsuccessful rank demotion/promotion of any relevant/non-relevant document\nrespectively. Finally, our adversarial tokens also show particular topic\npreferences within and across datasets, exposing potential biases from BERT\npre-training or downstream datasets.",
        "pdf_link": "https://arxiv.org/pdf/2206.11724v1.pdf"
    },
    {
        "title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models",
        "authors": [
            "Virginia K. Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2022-06-23T05:30:47Z",
        "summary": "This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.",
        "pdf_link": "https://arxiv.org/pdf/2206.11484v2.pdf"
    },
    {
        "title": "Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities",
        "authors": [
            "Zejiang Shen",
            "Kyle Lo",
            "Lauren Yu",
            "Nathan Dahlberg",
            "Margo Schlanger",
            "Doug Downey"
        ],
        "published": "2022-06-22T07:26:55Z",
        "summary": "With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2206.10883v3.pdf"
    },
    {
        "title": "Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information",
        "authors": [
            "Tomasz Limisiewicz",
            "David Mareček"
        ],
        "published": "2022-06-21T21:38:25Z",
        "summary": "The representations in large language models contain multiple types of gender\ninformation. We focus on two types of such signals in English texts: factual\ngender information, which is a grammatical or semantic property, and gender\nbias, which is the correlation between a word and specific gender. We can\ndisentangle the model's embeddings and identify components encoding both types\nof information with probing. We aim to diminish the stereotypical bias in the\nrepresentations while preserving the factual gender signal. Our filtering\nmethod shows that it is possible to decrease the bias of gender-neutral\nprofession names without significant deterioration of language modeling\ncapabilities. The findings can be applied to language generation to mitigate\nreliance on stereotypes while preserving gender agreement in coreferences.",
        "pdf_link": "https://arxiv.org/pdf/2206.10744v1.pdf"
    },
    {
        "title": "Using cognitive psychology to understand GPT-3",
        "authors": [
            "Marcel Binz",
            "Eric Schulz"
        ],
        "published": "2022-06-21T20:06:03Z",
        "summary": "We study GPT-3, a recent large language model, using tools from cognitive\npsychology. More specifically, we assess GPT-3's decision-making, information\nsearch, deliberation, and causal reasoning abilities on a battery of canonical\nexperiments from the literature. We find that much of GPT-3's behavior is\nimpressive: it solves vignette-based tasks similarly or better than human\nsubjects, is able to make decent decisions from descriptions, outperforms\nhumans in a multi-armed bandit task, and shows signatures of model-based\nreinforcement learning. Yet we also find that small perturbations to\nvignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures\nof directed exploration, and that it fails miserably in a causal reasoning\ntask. These results enrich our understanding of current large language models\nand pave the way for future investigations using tools from cognitive\npsychology to study increasingly capable and opaque artificial agents.",
        "pdf_link": "https://arxiv.org/pdf/2206.14576v1.pdf"
    },
    {
        "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Alberto Olmo",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2022-06-21T16:15:27Z",
        "summary": "Generating plans of action, and reasoning about change have long been\nconsidered a core competence of intelligent agents. It is thus no surprise that\nevaluating the planning and reasoning capabilities of large language models\n(LLMs) has become a hot topic of research. Most claims about LLM planning\ncapabilities are however based on common sense tasks-where it becomes hard to\ntell whether LLMs are planning or merely retrieving from their vast world\nknowledge. There is a strong need for systematic and extensible planning\nbenchmarks with sufficient diversity to evaluate whether LLMs have innate\nplanning capabilities. Motivated by this, we propose PlanBench, an extensible\nbenchmark suite based on the kinds of domains used in the automated planning\ncommunity, especially in the International Planning Competition, to test the\ncapabilities of LLMs in planning or reasoning about actions and change.\nPlanBench provides sufficient diversity in both the task domains and the\nspecific planning capabilities. Our studies also show that on many critical\ncapabilities-including plan generation-LLM performance falls quite short, even\nwith the SOTA models. PlanBench can thus function as a useful marker of\nprogress of LLMs in planning and reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2206.10498v4.pdf"
    },
    {
        "title": "TabText: A Flexible and Contextual Approach to Tabular Data Representation",
        "authors": [
            "Kimberly Villalobos Carballo",
            "Liangyuan Na",
            "Yu Ma",
            "Léonard Boussioux",
            "Cynthia Zeng",
            "Luis R. Soenksen",
            "Dimitris Bertsimas"
        ],
        "published": "2022-06-21T13:28:57Z",
        "summary": "Tabular data is essential for applying machine learning tasks across various\nindustries. However, traditional data processing methods do not fully utilize\nall the information available in the tables, ignoring important contextual\ninformation such as column header descriptions. In addition, pre-processing\ndata into a tabular format can remain a labor-intensive bottleneck in model\ndevelopment. This work introduces TabText, a processing and feature extraction\nframework that extracts contextual information from tabular data structures.\nTabText addresses processing difficulties by converting the content into\nlanguage and utilizing pre-trained large language models (LLMs). We evaluate\nour framework on nine healthcare prediction tasks ranging from patient\ndischarge, ICU admission, and mortality. We show that 1) applying our TabText\nframework enables the generation of high-performing and simple machine learning\nbaseline models with minimal data pre-processing, and 2) augmenting\npre-processed tabular data with TabText representations improves the average\nand worst-case AUC performance of standard machine learning models by as much\nas 6%.",
        "pdf_link": "https://arxiv.org/pdf/2206.10381v4.pdf"
    },
    {
        "title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
        "authors": [
            "Yarden Tal",
            "Inbal Magar",
            "Roy Schwartz"
        ],
        "published": "2022-06-20T15:52:40Z",
        "summary": "The size of pretrained models is increasing, and so is their performance on a\nvariety of NLP tasks. However, as their memorization capacity grows, they might\npick up more social biases. In this work, we examine the connection between\nmodel size and its gender bias (specifically, occupational gender bias). We\nmeasure bias in three masked language model families (RoBERTa, DeBERTa, and T5)\nin two setups: directly using prompt based method, and using a downstream task\n(Winogender). We find on the one hand that larger models receive higher bias\nscores on the former task, but when evaluated on the latter, they make fewer\ngender errors. To examine these potentially conflicting results, we carefully\ninvestigate the behavior of the different models on Winogender. We find that\nwhile larger models outperform smaller ones, the probability that their\nmistakes are caused by gender bias is higher. Moreover, we find that the\nproportion of stereotypical errors compared to anti-stereotypical ones grows\nwith the model size. Our findings highlight the potential risks that can arise\nfrom increasing model size.",
        "pdf_link": "https://arxiv.org/pdf/2206.09860v1.pdf"
    },
    {
        "title": "Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data",
        "authors": [
            "Tian Li",
            "Xiang Chen",
            "Zhen Dong",
            "Weijiang Yu",
            "Yijun Yan",
            "Kurt Keutzer",
            "Shanghang Zhang"
        ],
        "published": "2022-06-20T06:38:51Z",
        "summary": "Domain adaptive text classification is a challenging problem for the\nlarge-scale pretrained language models because they often require expensive\nadditional labeled data to adapt to new domains. Existing works usually fails\nto leverage the implicit relationships among words across domains. In this\npaper, we propose a novel method, called Domain Adaptation with Structured\nKnowledge (DASK), to enhance domain adaptation by exploiting word-level\nsemantic relationships. DASK first builds a knowledge graph to capture the\nrelationship between pivot terms (domain-independent words) and non-pivot terms\nin the target domain. Then during training, DASK injects pivot-related\nknowledge graph information into source domain texts. For the downstream task,\nthese knowledge-injected texts are fed into a BERT variant capable of\nprocessing knowledge-injected textual data. Thanks to the knowledge injection,\nour model learns domain-invariant features for non-pivots according to their\nrelationships with pivots. DASK ensures the pivots to have domain-invariant\nbehaviors by dynamically inferring via the polarity scores of candidate pivots\nduring training with pseudo-labels. We validate DASK on a wide range of\ncross-domain sentiment classification tasks and observe up to 2.9% absolute\nperformance improvement over baselines for 20 different domain pairs. Code will\nbe made available at https://github.com/hikaru-nara/DASK.",
        "pdf_link": "https://arxiv.org/pdf/2206.09591v1.pdf"
    },
    {
        "title": "Local Slot Attention for Vision-and-Language Navigation",
        "authors": [
            "Yifeng Zhuang",
            "Qiang Sun",
            "Yanwei Fu",
            "Lifeng Chen",
            "Xiangyang Xue"
        ],
        "published": "2022-06-17T09:21:26Z",
        "summary": "Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\n  Recently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\n  However, there exist two problems in current transformer-based models.\n  1) The models process each view independently without taking the integrity of\nthe objects into account.\n  2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\n  To address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2206.08645v2.pdf"
    },
    {
        "title": "Methods for Estimating and Improving Robustness of Language Models",
        "authors": [
            "Michal Štefánik"
        ],
        "published": "2022-06-16T21:02:53Z",
        "summary": "Despite their outstanding performance, large language models (LLMs) suffer\nnotorious flaws related to their preference for simple, surface-level textual\nrelations over full semantic complexity of the problem. This proposal\ninvestigates a common denominator of this problem in their weak ability to\ngeneralise outside of the training domain. We survey diverse research\ndirections providing estimations of model generalisation ability and find that\nincorporating some of these measures in the training objectives leads to\nenhanced distributional robustness of neural models. Based on these findings,\nwe present future research directions towards enhancing the robustness of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2206.08446v1.pdf"
    },
    {
        "title": "Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models",
        "authors": [
            "Maribeth Rauh",
            "John Mellor",
            "Jonathan Uesato",
            "Po-Sen Huang",
            "Johannes Welbl",
            "Laura Weidinger",
            "Sumanth Dathathri",
            "Amelia Glaese",
            "Geoffrey Irving",
            "Iason Gabriel",
            "William Isaac",
            "Lisa Anne Hendricks"
        ],
        "published": "2022-06-16T17:28:01Z",
        "summary": "Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2206.08325v2.pdf"
    },
    {
        "title": "Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task",
        "authors": [
            "Mikhail Lepekhin",
            "Serge Sharoff"
        ],
        "published": "2022-06-15T09:59:05Z",
        "summary": "Genre identification is a subclass of non-topical text classification. The\nmain difference between this task and topical classification is that genres,\nunlike topics, usually do not correspond to simple keywords, and thus they need\nto be defined in terms of their functions in communication. Neural models based\non pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA\nresults in many NLP tasks, including non-topical classification. However, in\nmany cases, their downstream application to very large corpora, such as those\nextracted from social media, can lead to unreliable results because of dataset\nshifts, when some raw texts do not match the profile of the training set. To\nmitigate this problem, we experiment with individual models as well as with\ntheir ensembles. To evaluate the robustness of all models we use a prediction\nconfidence metric, which estimates the reliability of a prediction in the\nabsence of a gold standard label. We can evaluate robustness via the confidence\ngap between the correctly classified texts and the misclassified ones on a\nlabeled test corpus, higher gaps make it easier to improve our confidence that\nour classifier made the right decision. Our results show that for all of the\nclassifiers tested in this study, there is a confidence gap, but for the\nensembles, the gap is bigger, meaning that ensembles are more robust than their\nindividual models.",
        "pdf_link": "https://arxiv.org/pdf/2206.07427v1.pdf"
    },
    {
        "title": "SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features",
        "authors": [
            "Juri Opitz",
            "Anette Frank"
        ],
        "published": "2022-06-14T17:37:18Z",
        "summary": "Models based on large-pretrained language models, such as S(entence)BERT,\nprovide effective and efficient sentence embeddings that show high correlation\nto human similarity ratings, but lack interpretability. On the other hand,\ngraph metrics for graph-based meaning representations (e.g., Abstract Meaning\nRepresentation, AMR) can make explicit the semantic aspects in which two\nsentences are similar. However, such metrics tend to be slow, rely on parsers,\nand do not reach state-of-the-art performance when rating sentence similarity.\n  In this work, we aim at the best of both worlds, by learning to induce\n$S$emantically $S$tructured $S$entence BERT embeddings (S$^3$BERT). Our\nS$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize\nvarious semantic sentence features (e.g., semantic roles, negation, or\nquantification). We show how to i) learn a decomposition of the sentence\nembeddings into semantic features, through approximation of a suite of\ninterpretable AMR graph metrics, and how to ii) preserve the overall power of\nthe neural embeddings by controlling the decomposition learning process with a\nsecond objective that enforces consistency with the similarity ratings of an\nSBERT teacher model. In our experimental studies, we show that our approach\noffers interpretability -- while fully preserving the effectiveness and\nefficiency of the neural sentence embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2206.07023v2.pdf"
    },
    {
        "title": "Memory-Based Model Editing at Scale",
        "authors": [
            "Eric Mitchell",
            "Charles Lin",
            "Antoine Bosselut",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2022-06-13T23:40:34Z",
        "summary": "Even the largest neural networks make errors, and once-correct predictions\ncan become invalid as the world changes. Model editors make local updates to\nthe behavior of base (pre-trained) models to inject updated knowledge or\ncorrect undesirable behaviors. Existing model editors have shown promise, but\nalso suffer from insufficient expressiveness: they struggle to accurately model\nan edit's intended scope (examples affected by the edit), leading to inaccurate\npredictions for test inputs loosely related to the edit, and they often fail\naltogether after many edits. As a higher-capacity alternative, we propose\nSemi-Parametric Editing with a Retrieval-Augmented Counterfactual Model\n(SERAC), which stores edits in an explicit memory and learns to reason over\nthem to modulate the base model's predictions as needed. To enable more\nrigorous evaluation of model editors, we introduce three challenging language\nmodel editing problems based on question answering, fact-checking, and dialogue\ngeneration. We find that only SERAC achieves high performance on all three\nproblems, consistently outperforming existing approaches to model editing by a\nsignificant margin. Code, data, and additional project information will be made\navailable at https://sites.google.com/view/serac-editing.",
        "pdf_link": "https://arxiv.org/pdf/2206.06520v1.pdf"
    },
    {
        "title": "Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models",
        "authors": [
            "Han Liu",
            "Bingning Wang",
            "Ting Yao",
            "Haijin Liang",
            "Jianjin Xu",
            "Xiaolin Hu"
        ],
        "published": "2022-06-11T12:52:32Z",
        "summary": "Large-scale pre-trained language models have achieved great success on\nnatural language generation tasks. However, it is difficult to control the\npre-trained language models to generate sentences with the desired attribute\nsuch as topic and sentiment, etc. Recently, Bayesian Controllable Language\nModels (BCLMs) have been shown to be efficient in controllable language\ngeneration. Rather than fine-tuning the parameters of pre-trained language\nmodels, BCLMs use external discriminators to guide the generation of\npre-trained language models. However, the mismatch between training and\ninference of BCLMs limits the performance of the models. To address the\nproblem, in this work we propose a \"Gemini Discriminator\" for controllable\nlanguage generation which alleviates the mismatch problem with a small\ncomputational cost. We tested our method on two controllable language\ngeneration tasks: sentiment control and topic control. On both tasks, our\nmethod reached achieved new state-of-the-art results in automatic and human\nevaluations.",
        "pdf_link": "https://arxiv.org/pdf/2206.05519v1.pdf"
    },
    {
        "title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "authors": [
            "Jesse Dodge",
            "Taylor Prewitt",
            "Remi Tachet Des Combes",
            "Erika Odmark",
            "Roy Schwartz",
            "Emma Strubell",
            "Alexandra Sasha Luccioni",
            "Noah A. Smith",
            "Nicole DeCario",
            "Will Buchanan"
        ],
        "published": "2022-06-10T17:04:04Z",
        "summary": "By providing unprecedented access to computational resources, cloud computing\nhas enabled rapid growth in technologies such as machine learning, the\ncomputational demands of which incur a high energy cost and a commensurate\ncarbon footprint. As a result, recent scholarship has called for better\nestimates of the greenhouse gas impact of AI: data scientists today do not have\neasy or reliable access to measurements of this information, precluding\ndevelopment of actionable tactics. Cloud providers presenting information about\nsoftware carbon intensity to users is a fundamental stepping stone towards\nminimizing emissions. In this paper, we provide a framework for measuring\nsoftware carbon intensity, and propose to measure operational carbon emissions\nby using location-based and time-specific marginal emissions data per energy\nunit. We provide measurements of operational software carbon intensity for a\nset of modern models for natural language processing and computer vision, and a\nwide range of model sizes, including pretraining of a 6.1 billion parameter\nlanguage model. We then evaluate a suite of approaches for reducing emissions\non the Microsoft Azure cloud compute platform: using cloud instances in\ndifferent geographic regions, using cloud instances at different times of day,\nand dynamically pausing cloud instances when the marginal carbon intensity is\nabove a certain threshold. We confirm previous results that the geographic\nregion of the data center plays a significant role in the carbon intensity for\na given cloud instance, and find that choosing an appropriate region can have\nthe largest operational emissions reduction impact. We also show that the time\nof day has notable impact on operational software carbon intensity. Finally, we\nconclude with recommendations for how machine learning practitioners can use\nsoftware carbon intensity information to reduce environmental impact.",
        "pdf_link": "https://arxiv.org/pdf/2206.05229v1.pdf"
    },
    {
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
        "authors": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R. Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adrià Garriga-Alonso",
            "Agnieszka Kluska",
            "Aitor Lewkowycz",
            "Akshat Agarwal",
            "Alethea Power",
            "Alex Ray",
            "Alex Warstadt",
            "Alexander W. Kocurek",
            "Ali Safaya",
            "Ali Tazarv",
            "Alice Xiang",
            "Alicia Parrish",
            "Allen Nie",
            "Aman Hussain",
            "Amanda Askell",
            "Amanda Dsouza",
            "Ambrose Slone",
            "Ameet Rahane",
            "Anantharaman S. Iyer",
            "Anders Andreassen",
            "Andrea Madotto",
            "Andrea Santilli",
            "Andreas Stuhlmüller",
            "Andrew Dai",
            "Andrew La",
            "Andrew Lampinen",
            "Andy Zou",
            "Angela Jiang",
            "Angelica Chen",
            "Anh Vuong",
            "Animesh Gupta",
            "Anna Gottardi",
            "Antonio Norelli",
            "Anu Venkatesh",
            "Arash Gholamidavoodi",
            "Arfa Tabassum",
            "Arul Menezes",
            "Arun Kirubarajan",
            "Asher Mullokandov",
            "Ashish Sabharwal",
            "Austin Herrick",
            "Avia Efrat",
            "Aykut Erdem",
            "Ayla Karakaş",
            "B. Ryan Roberts",
            "Bao Sheng Loe",
            "Barret Zoph",
            "Bartłomiej Bojanowski",
            "Batuhan Özyurt",
            "Behnam Hedayatnia",
            "Behnam Neyshabur",
            "Benjamin Inden",
            "Benno Stein",
            "Berk Ekmekci",
            "Bill Yuchen Lin",
            "Blake Howald",
            "Bryan Orinion",
            "Cameron Diao",
            "Cameron Dour",
            "Catherine Stinson",
            "Cedrick Argueta",
            "César Ferri Ramírez",
            "Chandan Singh",
            "Charles Rathkopf",
            "Chenlin Meng",
            "Chitta Baral",
            "Chiyu Wu",
            "Chris Callison-Burch",
            "Chris Waites",
            "Christian Voigt",
            "Christopher D. Manning",
            "Christopher Potts",
            "Cindy Ramirez",
            "Clara E. Rivera",
            "Clemencia Siro",
            "Colin Raffel",
            "Courtney Ashcraft",
            "Cristina Garbacea",
            "Damien Sileo",
            "Dan Garrette",
            "Dan Hendrycks",
            "Dan Kilman",
            "Dan Roth",
            "Daniel Freeman",
            "Daniel Khashabi",
            "Daniel Levy",
            "Daniel Moseguí González",
            "Danielle Perszyk",
            "Danny Hernandez",
            "Danqi Chen",
            "Daphne Ippolito",
            "Dar Gilboa",
            "David Dohan",
            "David Drakard",
            "David Jurgens",
            "Debajyoti Datta",
            "Deep Ganguli",
            "Denis Emelin",
            "Denis Kleyko",
            "Deniz Yuret",
            "Derek Chen",
            "Derek Tam",
            "Dieuwke Hupkes",
            "Diganta Misra",
            "Dilyar Buzan",
            "Dimitri Coelho Mollo",
            "Diyi Yang",
            "Dong-Ho Lee",
            "Dylan Schrader",
            "Ekaterina Shutova",
            "Ekin Dogus Cubuk",
            "Elad Segal",
            "Eleanor Hagerman",
            "Elizabeth Barnes",
            "Elizabeth Donoway",
            "Ellie Pavlick",
            "Emanuele Rodola",
            "Emma Lam",
            "Eric Chu",
            "Eric Tang",
            "Erkut Erdem",
            "Ernie Chang",
            "Ethan A. Chi",
            "Ethan Dyer",
            "Ethan Jerzak",
            "Ethan Kim",
            "Eunice Engefu Manyasi",
            "Evgenii Zheltonozhskii",
            "Fanyue Xia",
            "Fatemeh Siar",
            "Fernando Martínez-Plumed",
            "Francesca Happé",
            "Francois Chollet",
            "Frieda Rong",
            "Gaurav Mishra",
            "Genta Indra Winata",
            "Gerard de Melo",
            "Germán Kruszewski",
            "Giambattista Parascandolo",
            "Giorgio Mariani",
            "Gloria Wang",
            "Gonzalo Jaimovitch-López",
            "Gregor Betz",
            "Guy Gur-Ari",
            "Hana Galijasevic",
            "Hannah Kim",
            "Hannah Rashkin",
            "Hannaneh Hajishirzi",
            "Harsh Mehta",
            "Hayden Bogar",
            "Henry Shevlin",
            "Hinrich Schütze",
            "Hiromu Yakura",
            "Hongming Zhang",
            "Hugh Mee Wong",
            "Ian Ng",
            "Isaac Noble",
            "Jaap Jumelet",
            "Jack Geissinger",
            "Jackson Kernion",
            "Jacob Hilton",
            "Jaehoon Lee",
            "Jaime Fernández Fisac",
            "James B. Simon",
            "James Koppel",
            "James Zheng",
            "James Zou",
            "Jan Kocoń",
            "Jana Thompson",
            "Janelle Wingfield",
            "Jared Kaplan",
            "Jarema Radom",
            "Jascha Sohl-Dickstein",
            "Jason Phang",
            "Jason Wei",
            "Jason Yosinski",
            "Jekaterina Novikova",
            "Jelle Bosscher",
            "Jennifer Marsh",
            "Jeremy Kim",
            "Jeroen Taal",
            "Jesse Engel",
            "Jesujoba Alabi",
            "Jiacheng Xu",
            "Jiaming Song",
            "Jillian Tang",
            "Joan Waweru",
            "John Burden",
            "John Miller",
            "John U. Balis",
            "Jonathan Batchelder",
            "Jonathan Berant",
            "Jörg Frohberg",
            "Jos Rozen",
            "Jose Hernandez-Orallo",
            "Joseph Boudeman",
            "Joseph Guerr",
            "Joseph Jones",
            "Joshua B. Tenenbaum",
            "Joshua S. Rule",
            "Joyce Chua",
            "Kamil Kanclerz",
            "Karen Livescu",
            "Karl Krauth",
            "Karthik Gopalakrishnan",
            "Katerina Ignatyeva",
            "Katja Markert",
            "Kaustubh D. Dhole",
            "Kevin Gimpel",
            "Kevin Omondi",
            "Kory Mathewson",
            "Kristen Chiafullo",
            "Ksenia Shkaruta",
            "Kumar Shridhar",
            "Kyle McDonell",
            "Kyle Richardson",
            "Laria Reynolds",
            "Leo Gao",
            "Li Zhang",
            "Liam Dugan",
            "Lianhui Qin",
            "Lidia Contreras-Ochando",
            "Louis-Philippe Morency",
            "Luca Moschella",
            "Lucas Lam",
            "Lucy Noble",
            "Ludwig Schmidt",
            "Luheng He",
            "Luis Oliveros Colón",
            "Luke Metz",
            "Lütfi Kerem Şenel",
            "Maarten Bosma",
            "Maarten Sap",
            "Maartje ter Hoeve",
            "Maheen Farooqi",
            "Manaal Faruqui",
            "Mantas Mazeika",
            "Marco Baturan",
            "Marco Marelli",
            "Marco Maru",
            "Maria Jose Ramírez Quintana",
            "Marie Tolkiehn",
            "Mario Giulianelli",
            "Martha Lewis",
            "Martin Potthast",
            "Matthew L. Leavitt",
            "Matthias Hagen",
            "Mátyás Schubert",
            "Medina Orduna Baitemirova",
            "Melody Arnaud",
            "Melvin McElrath",
            "Michael A. Yee",
            "Michael Cohen",
            "Michael Gu",
            "Michael Ivanitskiy",
            "Michael Starritt",
            "Michael Strube",
            "Michał Swędrowski",
            "Michele Bevilacqua",
            "Michihiro Yasunaga",
            "Mihir Kale",
            "Mike Cain",
            "Mimee Xu",
            "Mirac Suzgun",
            "Mitch Walker",
            "Mo Tiwari",
            "Mohit Bansal",
            "Moin Aminnaseri",
            "Mor Geva",
            "Mozhdeh Gheini",
            "Mukund Varma T",
            "Nanyun Peng",
            "Nathan A. Chi",
            "Nayeon Lee",
            "Neta Gur-Ari Krakover",
            "Nicholas Cameron",
            "Nicholas Roberts",
            "Nick Doiron",
            "Nicole Martinez",
            "Nikita Nangia",
            "Niklas Deckers",
            "Niklas Muennighoff",
            "Nitish Shirish Keskar",
            "Niveditha S. Iyer",
            "Noah Constant",
            "Noah Fiedel",
            "Nuan Wen",
            "Oliver Zhang",
            "Omar Agha",
            "Omar Elbaghdadi",
            "Omer Levy",
            "Owain Evans",
            "Pablo Antonio Moreno Casares",
            "Parth Doshi",
            "Pascale Fung",
            "Paul Pu Liang",
            "Paul Vicol",
            "Pegah Alipoormolabashi",
            "Peiyuan Liao",
            "Percy Liang",
            "Peter Chang",
            "Peter Eckersley",
            "Phu Mon Htut",
            "Pinyu Hwang",
            "Piotr Miłkowski",
            "Piyush Patil",
            "Pouya Pezeshkpour",
            "Priti Oli",
            "Qiaozhu Mei",
            "Qing Lyu",
            "Qinlang Chen",
            "Rabin Banjade",
            "Rachel Etta Rudolph",
            "Raefer Gabriel",
            "Rahel Habacker",
            "Ramon Risco",
            "Raphaël Millière",
            "Rhythm Garg",
            "Richard Barnes",
            "Rif A. Saurous",
            "Riku Arakawa",
            "Robbe Raymaekers",
            "Robert Frank",
            "Rohan Sikand",
            "Roman Novak",
            "Roman Sitelew",
            "Ronan LeBras",
            "Rosanne Liu",
            "Rowan Jacobs",
            "Rui Zhang",
            "Ruslan Salakhutdinov",
            "Ryan Chi",
            "Ryan Lee",
            "Ryan Stovall",
            "Ryan Teehan",
            "Rylan Yang",
            "Sahib Singh",
            "Saif M. Mohammad",
            "Sajant Anand",
            "Sam Dillavou",
            "Sam Shleifer",
            "Sam Wiseman",
            "Samuel Gruetter",
            "Samuel R. Bowman",
            "Samuel S. Schoenholz",
            "Sanghyun Han",
            "Sanjeev Kwatra",
            "Sarah A. Rous",
            "Sarik Ghazarian",
            "Sayan Ghosh",
            "Sean Casey",
            "Sebastian Bischoff",
            "Sebastian Gehrmann",
            "Sebastian Schuster",
            "Sepideh Sadeghi",
            "Shadi Hamdan",
            "Sharon Zhou",
            "Shashank Srivastava",
            "Sherry Shi",
            "Shikhar Singh",
            "Shima Asaadi",
            "Shixiang Shane Gu",
            "Shubh Pachchigar",
            "Shubham Toshniwal",
            "Shyam Upadhyay",
            "Shyamolima",
            "Debnath",
            "Siamak Shakeri",
            "Simon Thormeyer",
            "Simone Melzi",
            "Siva Reddy",
            "Sneha Priscilla Makini",
            "Soo-Hwan Lee",
            "Spencer Torene",
            "Sriharsha Hatwar",
            "Stanislas Dehaene",
            "Stefan Divic",
            "Stefano Ermon",
            "Stella Biderman",
            "Stephanie Lin",
            "Stephen Prasad",
            "Steven T. Piantadosi",
            "Stuart M. Shieber",
            "Summer Misherghi",
            "Svetlana Kiritchenko",
            "Swaroop Mishra",
            "Tal Linzen",
            "Tal Schuster",
            "Tao Li",
            "Tao Yu",
            "Tariq Ali",
            "Tatsu Hashimoto",
            "Te-Lin Wu",
            "Théo Desbordes",
            "Theodore Rothschild",
            "Thomas Phan",
            "Tianle Wang",
            "Tiberius Nkinyili",
            "Timo Schick",
            "Timofei Kornev",
            "Titus Tunduny",
            "Tobias Gerstenberg",
            "Trenton Chang",
            "Trishala Neeraj",
            "Tushar Khot",
            "Tyler Shultz",
            "Uri Shaham",
            "Vedant Misra",
            "Vera Demberg",
            "Victoria Nyamai",
            "Vikas Raunak",
            "Vinay Ramasesh",
            "Vinay Uday Prabhu",
            "Vishakh Padmakumar",
            "Vivek Srikumar",
            "William Fedus",
            "William Saunders",
            "William Zhang",
            "Wout Vossen",
            "Xiang Ren",
            "Xiaoyu Tong",
            "Xinran Zhao",
            "Xinyi Wu",
            "Xudong Shen",
            "Yadollah Yaghoobzadeh",
            "Yair Lakretz",
            "Yangqiu Song",
            "Yasaman Bahri",
            "Yejin Choi",
            "Yichi Yang",
            "Yiding Hao",
            "Yifu Chen",
            "Yonatan Belinkov",
            "Yu Hou",
            "Yufang Hou",
            "Yuntao Bai",
            "Zachary Seid",
            "Zhuoye Zhao",
            "Zijian Wang",
            "Zijie J. Wang",
            "Zirui Wang",
            "Ziyi Wu"
        ],
        "published": "2022-06-09T17:05:34Z",
        "summary": "Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.",
        "pdf_link": "https://arxiv.org/pdf/2206.04615v3.pdf"
    },
    {
        "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
        "authors": [
            "Yujie Lu",
            "Weixi Feng",
            "Wanrong Zhu",
            "Wenda Xu",
            "Xin Eric Wang",
            "Miguel Eckstein",
            "William Yang Wang"
        ],
        "published": "2022-06-06T22:09:52Z",
        "summary": "Procedural planning aims to implement complex high-level goals by\ndecomposition into sequential simpler low-level steps. Although procedural\nplanning is a basic skill set for humans in daily life, it remains a challenge\nfor large language models (LLMs) that lack a deep understanding of the\ncause-effect relations in procedures. Previous methods require manual exemplars\nto acquire procedural planning knowledge from LLMs in the zero-shot setting.\nHowever, such elicited pre-trained knowledge in LLMs induces spurious\ncorrelations between goals and steps, which impair the model generalization to\nunseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\nPLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\ncommonsense-infused prompting. To mitigate spurious goal-step correlations, we\nuse symbolic program executors on the latent procedural representations to\nformalize prompts from commonsense knowledge bases as a causal intervention\ntoward the Structural Causal Model. Both automatic and human evaluations on\nWikiHow and RobotHow show the superiority of PLAN on procedural planning\nwithout further training or manual exemplars.",
        "pdf_link": "https://arxiv.org/pdf/2206.02928v6.pdf"
    },
    {
        "title": "Domain-specific Language Pre-training for Dialogue Comprehension on Clinical Inquiry-Answering Conversations",
        "authors": [
            "Zhengyuan Liu",
            "Pavitra Krishnaswamy",
            "Nancy F. Chen"
        ],
        "published": "2022-06-06T08:45:03Z",
        "summary": "There is growing interest in the automated extraction of relevant information\nfrom clinical dialogues. However, it is difficult to collect and construct\nlarge annotated resources for clinical dialogue tasks. Recent developments in\nnatural language processing suggest that large-scale pre-trained language\nbackbones could be leveraged for such machine comprehension and information\nextraction tasks. Yet, due to the gap between pre-training and downstream\nclinical domains, it remains challenging to exploit the generic backbones for\ndomain-specific applications. Therefore, in this work, we propose a\ndomain-specific language pre-training, to improve performance on downstream\ntasks like dialogue comprehension. Aside from the common token-level masking\npre-training method, according to the nature of human conversations and\ninteractive flow of multi-topic inquiry-answering dialogues, we further propose\nsample generation strategies with speaker and utterance manipulation. The\nconversational pre-training guides the language backbone to reconstruct the\nutterances coherently based on the remaining context, thus bridging the gap\nbetween general and specific domains. Experiments are conducted on a clinical\nconversation dataset for symptom checking, where nurses inquire and discuss\nsymptom information with patients. We empirically show that the neural model\nwith our proposed approach brings improvement in the dialogue comprehension\ntask, and can achieve favorable results in the low resource training scenario.",
        "pdf_link": "https://arxiv.org/pdf/2206.02428v1.pdf"
    },
    {
        "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier",
        "authors": [
            "Yifei Li",
            "Zeqi Lin",
            "Shizhuo Zhang",
            "Qiang Fu",
            "Bei Chen",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2022-06-06T03:38:36Z",
        "summary": "Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "pdf_link": "https://arxiv.org/pdf/2206.02336v3.pdf"
    },
    {
        "title": "Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models",
        "authors": [
            "Daniil Moskovskiy",
            "Daryna Dementieva",
            "Alexander Panchenko"
        ],
        "published": "2022-06-05T20:02:30Z",
        "summary": "Detoxification is a task of generating text in polite style while preserving\nmeaning and fluency of the original toxic text. Existing detoxification methods\nare designed to work in one exact language. This work investigates multilingual\nand cross-lingual detoxification and the behavior of large multilingual models\nlike in this setting. Unlike previous works we aim to make large language\nmodels able to perform detoxification without direct fine-tuning in given\nlanguage. Experiments show that multilingual models are capable of performing\nmultilingual style transfer. However, models are not able to perform\ncross-lingual detoxification and direct fine-tuning on exact language is\ninevitable.",
        "pdf_link": "https://arxiv.org/pdf/2206.02252v1.pdf"
    },
    {
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
        "authors": [
            "Charlie Snell",
            "Ilya Kostrikov",
            "Yi Su",
            "Mengjiao Yang",
            "Sergey Levine"
        ],
        "published": "2022-06-05T18:38:42Z",
        "summary": "Large language models distill broad knowledge from text corpora. However,\nthey can be inconsistent when it comes to completing user specified tasks. This\nissue can be addressed by finetuning such models via supervised learning on\ncurated datasets, or via reinforcement learning. In this work, we propose a\nnovel offline RL method, implicit language Q-learning (ILQL), designed for use\non language models, that combines both the flexible utility maximization\nframework of RL algorithms with the ability of supervised learning to leverage\npreviously collected data, as well as its simplicity and stability. Our method\nemploys a combination of value conservatism alongside an implicit dataset\nsupport constraint in learning value functions, which are then used to guide\nlanguage model generations towards maximizing user-specified utility functions.\nIn addition to empirically validating ILQL, we present a detailed empirical\nanalysis of situations where offline RL can be useful in natural language\ngeneration settings, demonstrating how it can be a more effective utility\noptimizer than prior approaches for end-to-end dialogue, and how it can\neffectively optimize high variance reward functions based on subjective\njudgement, such as whether to label a comment as toxic or not.",
        "pdf_link": "https://arxiv.org/pdf/2206.11871v2.pdf"
    },
    {
        "title": "Fault-Aware Neural Code Rankers",
        "authors": [
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Mei Yang",
            "Andres Codas",
            "Mark Encarnación",
            "Shuvendu K Lahiri",
            "Madanlal Musuvathi",
            "Jianfeng Gao"
        ],
        "published": "2022-06-04T22:01:05Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\ngenerate code for various programming tasks. In many instances, LLMs can\ngenerate a correct program for a task when given numerous trials. Consequently,\na recent trend is to do large scale sampling of programs using a model and then\nfiltering/ranking the programs based on the program execution on a small number\nof known unit tests to select one candidate solution. However, these approaches\nassume that the unit tests are given and assume the ability to safely execute\nthe generated programs (which can do arbitrary dangerous operations such as\nfile manipulations). Both of the above assumptions are impractical in\nreal-world software development. In this paper, we propose CodeRanker, a neural\nranker that can predict the correctness of a sampled program without executing\nit. Our CodeRanker is fault-aware i.e., it is trained to predict different\nkinds of execution information such as predicting the exact compile/runtime\nerror type (e.g., an IndexError or a TypeError). We show that CodeRanker can\nsignificantly increase the pass@1 accuracy of various code generation models\n(including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets.",
        "pdf_link": "https://arxiv.org/pdf/2206.03865v2.pdf"
    },
    {
        "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient",
        "authors": [
            "Xiaoxia Wu",
            "Zhewei Yao",
            "Minjia Zhang",
            "Conglong Li",
            "Yuxiong He"
        ],
        "published": "2022-06-04T00:19:45Z",
        "summary": "Extreme compression, particularly ultra-low bit precision (binary/ternary)\nquantization, has been proposed to fit large NLP models on resource-constraint\ndevices. However, to preserve the accuracy for such aggressive compression\nschemes, cutting-edge methods usually introduce complicated compression\npipelines, e.g., multi-stage expensive knowledge distillation with extensive\nhyperparameter tuning. Also, they oftentimes focus less on smaller transformer\nmodels that have already been heavily compressed via knowledge distillation and\nlack a systematic study to show the effectiveness of their methods. In this\npaper, we perform a very comprehensive systematic study to measure the impact\nof many key hyperparameters and training strategies from previous works. As a\nresult, we find out that previous baselines for ultra-low bit precision\nquantization are significantly under-trained. Based on our study, we propose a\nsimple yet effective compression pipeline for extreme compression, named XTC.\nXTC demonstrates that (1) we can skip the pre-training knowledge distillation\nto obtain a 5-layer BERT while achieving better performance than previous\nstate-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization\nplus layer reduction is able to reduce the model size by 50x, resulting in new\nstate-of-the-art results on GLUE tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.01859v1.pdf"
    },
    {
        "title": "Differentially Private Model Compression",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Arturs Backurs",
            "Huseyin A Inan",
            "Lukas Wutschitz",
            "Janardhan Kulkarni"
        ],
        "published": "2022-06-03T22:04:36Z",
        "summary": "Recent papers have shown that large pre-trained language models (LLMs) such\nas BERT, GPT-2 can be fine-tuned on private data to achieve performance\ncomparable to non-private models for many downstream Natural Language\nProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.\nThe inference cost of these models -- which consist of hundreds of millions of\nparameters -- however, can be prohibitively large. Hence, often in practice,\nLLMs are compressed before they are deployed in specific applications. In this\npaper, we initiate the study of differentially private model compression and\npropose frameworks for achieving 50% sparsity levels while maintaining nearly\nfull performance. We demonstrate these ideas on standard GLUE benchmarks using\nBERT models, setting benchmarks for future research on this topic.",
        "pdf_link": "https://arxiv.org/pdf/2206.01838v1.pdf"
    },
    {
        "title": "Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models",
        "authors": [
            "Kaiji Lu",
            "Anupam Datta"
        ],
        "published": "2022-06-01T02:30:12Z",
        "summary": "Previous works show that deep NLP models are not always conceptually sound:\nthey do not always learn the correct linguistic concepts. Specifically, they\ncan be insensitive to word order. In order to systematically evaluate models\nfor their conceptual soundness with respect to word order, we introduce a new\nexplanation method for sequential data: Order-sensitive Shapley Values (OSV).\nWe conduct an extensive empirical evaluation to validate the method and surface\nhow well various deep NLP models learn word order. Using synthetic data, we\nfirst show that OSV is more faithful in explaining model behavior than\ngradient-based methods. Second, applying to the HANS dataset, we discover that\nthe BERT-based NLI model uses only the word occurrences without word orders.\nAlthough simple data augmentation improves accuracy on HANS, OSV shows that the\naugmented model does not fundamentally improve the model's learning of order.\nThird, we discover that not all sentiment analysis models learn negation\nproperly: some fail to capture the correct syntax of the negation construct.\nFinally, we show that pretrained language models such as BERT may rely on the\nabsolute positions of subject words to learn long-range Subject-Verb Agreement.\nWith each NLP task, we also demonstrate how OSV can be leveraged to generate\nadversarial examples.",
        "pdf_link": "https://arxiv.org/pdf/2206.00192v1.pdf"
    },
    {
        "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
        "authors": [
            "Yinlam Chow",
            "Aza Tulepbergenov",
            "Ofir Nachum",
            "MoonKyung Ryu",
            "Mohammad Ghavamzadeh",
            "Craig Boutilier"
        ],
        "published": "2022-05-31T19:00:41Z",
        "summary": "Despite recent advancements in language models (LMs), their application to\ndialogue management (DM) problems and ability to carry on rich conversations\nremain a challenge. We use reinforcement learning (RL) to develop a dialogue\nagent that avoids being short-sighted (outputting generic utterances) and\nmaximizes overall user satisfaction. Most existing RL approaches to DM train\nthe agent at the word-level, and thus, have to deal with a combinatorially\ncomplex action space even for a medium-size vocabulary. As a result, they\nstruggle to produce a successful and engaging dialogue even if they are\nwarm-started with a pre-trained LM. To address this issue, we develop a\nRL-based DM using a novel mixture of expert language model (MoE-LM) that\nconsists of (i) a LM capable of learning diverse semantics for conversation\nhistories, (ii) a number of {\\em specialized} LMs (or experts) capable of\ngenerating utterances corresponding to a particular attribute or personality,\nand (iii) a RL-based DM that performs dialogue planning with the utterances\ngenerated by the experts. Our MoE approach provides greater flexibility to\ngenerate sensible utterances with different intents and allows RL to focus on\nconversational-level DM. We compare it with SOTA baselines on open-domain\ndialogues and demonstrate its effectiveness both in terms of the diversity and\nsensibility of the generated utterances and the overall DM performance.",
        "pdf_link": "https://arxiv.org/pdf/2206.00059v1.pdf"
    },
    {
        "title": "Automatic Short Math Answer Grading via In-context Meta-learning",
        "authors": [
            "Mengxue Zhang",
            "Sami Baral",
            "Neil Heffernan",
            "Andrew Lan"
        ],
        "published": "2022-05-30T16:26:02Z",
        "summary": "Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.",
        "pdf_link": "https://arxiv.org/pdf/2205.15219v3.pdf"
    },
    {
        "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
        "authors": [
            "Wenyi Hong",
            "Ming Ding",
            "Wendi Zheng",
            "Xinghan Liu",
            "Jie Tang"
        ],
        "published": "2022-05-29T19:02:15Z",
        "summary": "Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2205.15868v1.pdf"
    },
    {
        "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
        "authors": [
            "Pascal Notin",
            "Mafalda Dias",
            "Jonathan Frazer",
            "Javier Marchena-Hurtado",
            "Aidan Gomez",
            "Debora S. Marks",
            "Yarin Gal"
        ],
        "published": "2022-05-27T04:51:15Z",
        "summary": "The ability to accurately model the fitness landscape of protein sequences is\ncritical to a wide range of applications, from quantifying the effects of human\nvariants on disease likelihood, to predicting immune-escape mutations in\nviruses and designing novel biotherapeutic proteins. Deep generative models of\nprotein sequences trained on multiple sequence alignments have been the most\nsuccessful approaches so far to address these tasks. The performance of these\nmethods is however contingent on the availability of sufficiently deep and\ndiverse alignments for reliable training. Their potential scope is thus limited\nby the fact many protein families are hard, if not impossible, to align. Large\nlanguage models trained on massive quantities of non-aligned protein sequences\nfrom diverse families address these problems and show potential to eventually\nbridge the performance gap. We introduce Tranception, a novel transformer\narchitecture leveraging autoregressive predictions and retrieval of homologous\nsequences at inference to achieve state-of-the-art fitness prediction\nperformance. Given its markedly higher performance on multiple mutants,\nrobustness to shallow alignments and ability to score indels, our approach\noffers significant gain of scope over existing approaches. To enable more\nrigorous model testing across a broader range of protein families, we develop\nProteinGym -- an extensive set of multiplexed assays of variant effects,\nsubstantially increasing both the number and diversity of assays compared to\nexisting benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2205.13760v1.pdf"
    },
    {
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning",
        "authors": [
            "Ximing Lu",
            "Sean Welleck",
            "Jack Hessel",
            "Liwei Jiang",
            "Lianhui Qin",
            "Peter West",
            "Prithviraj Ammanabrolu",
            "Yejin Choi"
        ],
        "published": "2022-05-26T21:11:51Z",
        "summary": "Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.",
        "pdf_link": "https://arxiv.org/pdf/2205.13636v2.pdf"
    },
    {
        "title": "Differentially Private Decoding in Large Language Models",
        "authors": [
            "Jimit Majmudar",
            "Christophe Dupuy",
            "Charith Peris",
            "Sami Smaili",
            "Rahul Gupta",
            "Richard Zemel"
        ],
        "published": "2022-05-26T20:50:58Z",
        "summary": "Recent large-scale natural language processing (NLP) systems use a\npre-trained Large Language Model (LLM) on massive and diverse corpora as a\nheadstart. In practice, the pre-trained model is adapted to a wide array of\ntasks via fine-tuning on task-specific datasets. LLMs, while effective, have\nbeen shown to memorize instances of training data thereby potentially revealing\nprivate information processed during pre-training. The potential leakage might\nfurther propagate to the downstream tasks for which LLMs are fine-tuned. On the\nother hand, privacy-preserving algorithms usually involve retraining from\nscratch, which is prohibitively expensive for LLMs. In this work, we propose a\nsimple, easy to interpret, and computationally lightweight perturbation\nmechanism to be applied to an already trained model at the decoding stage. Our\nperturbation mechanism is model-agnostic and can be used in conjunction with\nany LLM. We provide theoretical analysis showing that the proposed mechanism is\ndifferentially private, and experimental results showing a privacy-utility\ntrade-off.",
        "pdf_link": "https://arxiv.org/pdf/2205.13621v2.pdf"
    },
    {
        "title": "BiT: Robustly Binarized Multi-distilled Transformer",
        "authors": [
            "Zechun Liu",
            "Barlas Oguz",
            "Aasish Pappu",
            "Lin Xiao",
            "Scott Yih",
            "Meng Li",
            "Raghuraman Krishnamoorthi",
            "Yashar Mehdad"
        ],
        "published": "2022-05-25T19:01:54Z",
        "summary": "Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however, is\ntechnically challenging from an optimization perspective. In this work, we\nidentify a series of improvements that enables binary transformers at a much\nhigher accuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.\nCode and models are available at: https://github.com/facebookresearch/bit.",
        "pdf_link": "https://arxiv.org/pdf/2205.13016v2.pdf"
    },
    {
        "title": "Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling",
        "authors": [
            "Kaitao Song",
            "Yichong Leng",
            "Xu Tan",
            "Yicheng Zou",
            "Tao Qin",
            "Dongsheng Li"
        ],
        "published": "2022-05-25T18:00:09Z",
        "summary": "Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.",
        "pdf_link": "https://arxiv.org/pdf/2205.12986v4.pdf"
    },
    {
        "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
        "authors": [
            "Liyan Tang",
            "Tanya Goyal",
            "Alexander R. Fabbri",
            "Philippe Laban",
            "Jiacheng Xu",
            "Semih Yavuz",
            "Wojciech Kryściński",
            "Justin F. Rousseau",
            "Greg Durrett"
        ],
        "published": "2022-05-25T15:26:48Z",
        "summary": "The propensity of abstractive summarization models to make factual errors has\nbeen studied extensively, including design of metrics to detect factual errors\nand annotation of errors in current systems' outputs. However, the\never-evolving nature of summarization systems, metrics, and annotated\nbenchmarks makes factuality evaluation a moving target, and drawing clear\ncomparisons among metrics has become increasingly difficult. In this work, we\naggregate factuality error annotations from nine existing datasets and stratify\nthem according to the underlying summarization model. We compare performance of\nstate-of-the-art factuality metrics, including recent ChatGPT-based metrics, on\nthis stratified benchmark and show that their performance varies significantly\nacross different types of summarization models. Critically, our analysis shows\nthat much of the recent improvement in the factuality detection space has been\non summaries from older (pre-Transformer) models instead of more relevant\nrecent summarization models. We further perform a finer-grained analysis per\nerror-type and find similar performance variance across error types for\ndifferent factuality metrics. Our results show that no one metric is superior\nin all settings or for all error types, and we provide recommendations for best\npractices given these insights.",
        "pdf_link": "https://arxiv.org/pdf/2205.12854v2.pdf"
    },
    {
        "title": "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation",
        "authors": [
            "Ao Liu",
            "Haoyu Dong",
            "Naoaki Okazaki",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022-05-25T11:55:54Z",
        "summary": "Logical table-to-text generation is a task that involves generating logically\nfaithful sentences from tables, which requires models to derive logical level\nfacts from table records via logical inference. It raises a new challenge on\nthe logical-level content planning of table-to-text models. However, directly\nlearning the logical inference knowledge from table-text pairs is very\ndifficult for neural models because of the ambiguity of natural language and\nthe scarcity of parallel data. Hence even large-scale pre-trained language\nmodels present low logical fidelity on logical table-to-text. In this work, we\npropose a PLOG (Pretrained Logical Form Generator) framework to improve the\ngeneration fidelity. Specifically, PLOG is first pretrained on a\ntable-to-logic-form generation (table-to-logic) task, then finetuned on\ndownstream table-to-text tasks. The formal definition of logical forms enables\nus to collect large amount of accurate logical forms from tables without human\nannotation. In addition, PLOG can learn logical inference from table-logic\npairs much more definitely than from table-text pairs. To evaluate our model,\nwe further collect a controlled logical table-to-text dataset CONTLOG based on\nan existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms\nstrong baselines by a large margin on the logical fidelity, demonstrating the\neffectiveness of table-to-logic pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2205.12697v2.pdf"
    },
    {
        "title": "Large Language Models are Few-Shot Clinical Information Extractors",
        "authors": [
            "Monica Agrawal",
            "Stefan Hegselmann",
            "Hunter Lang",
            "Yoon Kim",
            "David Sontag"
        ],
        "published": "2022-05-25T11:49:58Z",
        "summary": "A long-running goal of the clinical NLP community is the extraction of\nimportant variables trapped in clinical notes. However, roadblocks have\nincluded dataset shift from the general domain and a lack of public clinical\ncorpora and annotations. In this work, we show that large language models, such\nas InstructGPT, perform well at zero- and few-shot information extraction from\nclinical text despite not being trained specifically for the clinical domain.\nWhereas text classification and generation performance have already been\nstudied extensively in such models, here we additionally demonstrate how to\nleverage them to tackle a diverse set of NLP tasks which require more\nstructured outputs, including span identification, token-level sequence\nclassification, and relation extraction. Further, due to the dearth of\navailable data to evaluate these systems, we introduce new datasets for\nbenchmarking few-shot clinical information extraction based on a manual\nre-annotation of the CASI dataset for new tasks. On the clinical extraction\ntasks we studied, the GPT-3 systems significantly outperform existing zero- and\nfew-shot baselines.",
        "pdf_link": "https://arxiv.org/pdf/2205.12689v2.pdf"
    },
    {
        "title": "RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning",
        "authors": [
            "Soumya Sanyal",
            "Zeyi Liao",
            "Xiang Ren"
        ],
        "published": "2022-05-25T09:23:50Z",
        "summary": "Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in English natural\nlanguage. While the progress is promising, it is currently unclear if these\nmodels indeed perform logical reasoning by understanding the underlying logical\nsemantics in the language. To this end, we propose RobustLR, a suite of\nevaluation datasets that evaluate the robustness of these models to minimal\nlogical edits in rulebases and some standard logical equivalence conditions. In\nour experiments with RoBERTa and T5, we find that the models trained in prior\nworks do not perform consistently on the different perturbations in RobustLR,\nthus showing that the models are not robust to the proposed logical\nperturbations. Further, we find that the models find it especially hard to\nlearn logical negation and disjunction operators. Overall, using our evaluation\nsets, we demonstrate some shortcomings of the deductive reasoning-based\nlanguage models, which can eventually help towards designing better models for\nlogical reasoning over natural language. All the datasets and code base have\nbeen made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2205.12598v2.pdf"
    },
    {
        "title": "Perturbation Augmentation for Fairer NLP",
        "authors": [
            "Rebecca Qian",
            "Candace Ross",
            "Jude Fernandes",
            "Eric Smith",
            "Douwe Kiela",
            "Adina Williams"
        ],
        "published": "2022-05-25T09:00:29Z",
        "summary": "Unwanted and often harmful social biases are becoming ever more salient in\nNLP research, affecting both models and datasets. In this work, we ask whether\ntraining on demographically perturbed data leads to fairer language models. We\ncollect a large dataset of human annotated text perturbations and train a\nneural perturbation model, which we show outperforms heuristic alternatives. We\nfind that (i) language models (LMs) pre-trained on demographically perturbed\ncorpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE\ndatasets exhibit less demographic bias on downstream tasks, and (iii) fairness\nimprovements do not come at the expense of performance on downstream tasks.\nLastly, we discuss outstanding questions about how best to evaluate the\n(un)fairness of large language models. We hope that this exploration of neural\ndemographic perturbation will help drive more improvement towards fairer NLP.",
        "pdf_link": "https://arxiv.org/pdf/2205.12586v2.pdf"
    },
    {
        "title": "Gradient-Based Constrained Sampling from Language Models",
        "authors": [
            "Sachin Kumar",
            "Biswajit Paria",
            "Yulia Tsvetkov"
        ],
        "published": "2022-05-25T08:09:03Z",
        "summary": "Large pretrained language models generate fluent text but are notoriously\nhard to controllably sample from. In this work, we study constrained sampling\nfrom such language models: generating text that satisfies user-defined\nconstraints, while maintaining fluency and the model's performance in a\ndownstream task. We propose MuCoLa -- a sampling procedure that combines the\nlog-likelihood of the language model with arbitrary (differentiable)\nconstraints in a single energy function, and then generates samples in a\nnon-autoregressive manner. Specifically, it initializes the entire output\nsequence with noise and follows a Markov chain defined by Langevin Dynamics\nusing the gradients of the energy function. We evaluate MuCoLa on text\ngeneration with soft and hard constraints as well as their combinations\nobtaining significant improvements over competitive baselines for toxicity\navoidance, sentiment control, and keyword-guided generation.",
        "pdf_link": "https://arxiv.org/pdf/2205.12558v2.pdf"
    },
    {
        "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
        "authors": [
            "Mingkai Deng",
            "Jianyu Wang",
            "Cheng-Ping Hsieh",
            "Yihan Wang",
            "Han Guo",
            "Tianmin Shu",
            "Meng Song",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "published": "2022-05-25T07:50:31Z",
        "summary": "Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by \"enumeration\n(e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.",
        "pdf_link": "https://arxiv.org/pdf/2205.12548v3.pdf"
    },
    {
        "title": "Memorization in NLP Fine-tuning Methods",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Archit Uniyal",
            "Tianhao Wang",
            "David Evans",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022-05-25T05:49:31Z",
        "summary": "Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.",
        "pdf_link": "https://arxiv.org/pdf/2205.12506v2.pdf"
    },
    {
        "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?",
        "authors": [
            "Ivan Kobyzev",
            "Aref Jafari",
            "Mehdi Rezagholizadeh",
            "Tianda Li",
            "Alan Do-Omri",
            "Peng Lu",
            "Pascal Poupart",
            "Ali Ghodsi"
        ],
        "published": "2022-05-25T01:26:31Z",
        "summary": "Knowledge Distillation (KD) is a prominent neural model compression technique\nthat heavily relies on teacher network predictions to guide the training of a\nstudent model. Considering the ever-growing size of pre-trained language models\n(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is\nevident that in KD, deploying the teacher network during training adds to the\nmemory and computational requirements of training. In the computer vision\nliterature, the necessity of the teacher network is put under scrutiny by\nshowing that KD is a label regularization technique that can be replaced with\nlighter teacher-free variants such as the label-smoothing technique. However,\nto the best of our knowledge, this issue is not investigated in NLP. Therefore,\nthis work concerns studying different label regularization techniques and\nwhether we actually need them to improve the fine-tuning of smaller PLM\nnetworks on downstream tasks. In this regard, we did a comprehensive set of\nexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600\ndistinct trials and ran each configuration five times. This investigation led\nto a surprising observation that KD and other label regularization techniques\ndo not play any meaningful role over regular fine-tuning when the student model\nis pre-trained. We further explore this phenomenon in different settings of NLP\nand computer vision tasks and demonstrate that pre-training itself acts as a\nkind of regularization, and additional label regularization is unnecessary.",
        "pdf_link": "https://arxiv.org/pdf/2205.12428v2.pdf"
    },
    {
        "title": "Fine-tuned Language Models are Continual Learners",
        "authors": [
            "Thomas Scialom",
            "Tuhin Chakrabarty",
            "Smaranda Muresan"
        ],
        "published": "2022-05-24T22:53:34Z",
        "summary": "Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.",
        "pdf_link": "https://arxiv.org/pdf/2205.12393v4.pdf"
    },
    {
        "title": "Toxicity Detection with Generative Prompt-based Inference",
        "authors": [
            "Yau-Shian Wang",
            "Yingshan Chang"
        ],
        "published": "2022-05-24T22:44:43Z",
        "summary": "Due to the subtleness, implicity, and different possible interpretations\nperceived by different people, detecting undesirable content from text is a\nnuanced difficulty. It is a long-known risk that language models (LMs), once\ntrained on corpus containing undesirable content, have the power to manifest\nbiases and toxicity. However, recent studies imply that, as a remedy, LMs are\nalso capable of identifying toxic content without additional fine-tuning.\nPrompt-methods have been shown to effectively harvest this surprising\nself-diagnosing capability. However, existing prompt-based methods usually\nspecify an instruction to a language model in a discriminative way. In this\nwork, we explore the generative variant of zero-shot prompt-based toxicity\ndetection with comprehensive trials on prompt engineering. We evaluate on three\ndatasets with toxicity labels annotated on social media posts. Our analysis\nhighlights the strengths of our generative classification approach both\nquantitatively and qualitatively. Interesting aspects of self-diagnosis and its\nethical implications are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2205.12390v1.pdf"
    },
    {
        "title": "Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint",
        "authors": [
            "Heng-Yi Wu",
            "Jingqing Zhang",
            "Julia Ive",
            "Tong Li",
            "Vibhor Gupta",
            "Bingyuan Chen",
            "Yike Guo"
        ],
        "published": "2022-05-24T21:10:57Z",
        "summary": "Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.",
        "pdf_link": "https://arxiv.org/pdf/2205.12368v2.pdf"
    },
    {
        "title": "Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing",
        "authors": [
            "Linlu Qiu",
            "Peter Shaw",
            "Panupong Pasupat",
            "Tianze Shi",
            "Jonathan Herzig",
            "Emily Pitler",
            "Fei Sha",
            "Kristina Toutanova"
        ],
        "published": "2022-05-24T17:57:39Z",
        "summary": "Despite their strong performance on many tasks, pre-trained language models\nhave been shown to struggle on out-of-distribution compositional\ngeneralization. Meanwhile, recent work has shown considerable improvements on\nmany NLP tasks from model scaling. Can scaling up model size also improve\ncompositional generalization in semantic parsing? We evaluate encoder-decoder\nmodels up to 11B parameters and decoder-only models up to 540B parameters, and\ncompare model scaling curves for three different methods for applying a\npre-trained language model to a new task: fine-tuning all parameters, prompt\ntuning, and in-context learning. We observe that fine-tuning generally has flat\nor negative scaling curves on out-of-distribution compositional generalization\nin semantic parsing evaluations. In-context learning has positive scaling\ncurves, but is generally outperformed by much smaller fine-tuned models.\nPrompt-tuning can outperform fine-tuning, suggesting further potential\nimprovements from scaling as it exhibits a more positive scaling curve.\nAdditionally, we identify several error trends that vary with model scale. For\nexample, larger models are generally better at modeling the syntax of the\noutput space, but are also more prone to certain types of overfitting. Overall,\nour study highlights limitations of current techniques for effectively\nleveraging model scale for compositional generalization, while our analysis\nalso suggests promising directions for future work.",
        "pdf_link": "https://arxiv.org/pdf/2205.12253v2.pdf"
    },
    {
        "title": "PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation",
        "authors": [
            "Aitor Ormazabal",
            "Mikel Artetxe",
            "Manex Agirrezabal",
            "Aitor Soroa",
            "Eneko Agirre"
        ],
        "published": "2022-05-24T17:09:55Z",
        "summary": "Formal verse poetry imposes strict constraints on the meter and rhyme scheme\nof poems. Most prior work on generating this type of poetry uses existing poems\nfor supervision, which are difficult to obtain for most languages and poetic\nforms. In this work, we propose an unsupervised approach to generate poems\nfollowing any given meter and rhyme scheme, without requiring any poetic text\nfor training. Our method works by splitting a regular, non-poetic corpus into\nphrases, prepending control codes that describe the length and end rhyme of\neach phrase, and training a transformer language model in the augmented corpus.\nDuring inference, we build control codes for the desired meter and rhyme\nscheme, and condition our language model on them to generate formal verse\npoetry. Experiments in Spanish and Basque show that our approach is able to\ngenerate valid poems, which are often comparable in quality to those written by\nhumans.",
        "pdf_link": "https://arxiv.org/pdf/2205.12206v2.pdf"
    },
    {
        "title": "Word-order typology in Multilingual BERT: A case study in subordinate-clause detection",
        "authors": [
            "Dmitry Nikolaev",
            "Sebastian Padó"
        ],
        "published": "2022-05-24T11:35:39Z",
        "summary": "The capabilities and limitations of BERT and similar models are still unclear\nwhen it comes to learning syntactic abstractions, in particular across\nlanguages. In this paper, we use the task of subordinate-clause detection\nwithin and across languages to probe these properties. We show that this task\nis deceptively simple, with easy gains offset by a long tail of harder cases,\nand that BERT's zero-shot performance is dominated by word-order effects,\nmirroring the SVO/VSO/SOV typology.",
        "pdf_link": "https://arxiv.org/pdf/2205.11987v1.pdf"
    },
    {
        "title": "The Authenticity Gap in Human Evaluation",
        "authors": [
            "Kawin Ethayarajh",
            "Dan Jurafsky"
        ],
        "published": "2022-05-24T09:51:27Z",
        "summary": "Human ratings are the gold standard in NLG evaluation. The standard protocol\nis to collect ratings of generated text, average across annotators, and rank\nNLG systems by their average scores. However, little consideration has been\ngiven as to whether this approach faithfully captures human preferences.\nAnalyzing this standard protocol through the lens of utility theory in\neconomics, we identify the implicit assumptions it makes about annotators.\nThese assumptions are often violated in practice, in which case annotator\nratings cease to reflect their preferences. The most egregious violations come\nfrom using Likert scales, which provably reverse the direction of the true\npreference in certain cases. We suggest improvements to the standard protocol\nto make it more theoretically sound, but even in its improved form, it cannot\nbe used to evaluate open-ended tasks like story generation. For the latter, we\npropose a new human evaluation protocol called $\\textit{system-level\nprobabilistic assessment}$ (SPA). When human evaluation of stories is done with\nSPA, we can recover the ordering of GPT-3 models by size, with statistically\nsignificant results. However, when human evaluation is done with the standard\nprotocol, less than half of the expected preferences can be recovered (e.g.,\nthere is no significant difference between $\\texttt{curie}$ and\n$\\texttt{davinci}$, despite using a highly powered test).",
        "pdf_link": "https://arxiv.org/pdf/2205.11930v2.pdf"
    },
    {
        "title": "Large Language Models are Zero-Shot Reasoners",
        "authors": [
            "Takeshi Kojima",
            "Shixiang Shane Gu",
            "Machel Reid",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "published": "2022-05-24T09:22:26Z",
        "summary": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.",
        "pdf_link": "https://arxiv.org/pdf/2205.11916v4.pdf"
    },
    {
        "title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
        "authors": [
            "Afra Feyza Akyürek",
            "Sejin Paik",
            "Muhammed Yusuf Kocyigit",
            "Seda Akbiyik",
            "Şerife Leman Runyun",
            "Derry Wijaya"
        ],
        "published": "2022-05-23T20:01:20Z",
        "summary": "Large language models trained on a mixture of NLP tasks that are converted\ninto a text-to-text format using prompts, can generalize into novel forms of\nlanguage and handle novel tasks. A large body of work within prompt engineering\nattempts to understand the effects of input forms and prompts in achieving\nsuperior performance. We consider an alternative measure and inquire whether\nthe way in which an input is encoded affects social biases promoted in outputs.\nIn this paper, we study T0, a large-scale multi-task text-to-text language\nmodel trained using prompt-based learning. We consider two different forms of\nsemantically equivalent inputs: question-answer format and premise-hypothesis\nformat. We use an existing bias benchmark for the former BBQ and create the\nfirst bias benchmark in natural language inference BBNLI with hand-written\nhypotheses while also converting each benchmark into the other form. The\nresults on two benchmarks suggest that given two different formulations of\nessentially the same input, T0 conspicuously acts more biased in question\nanswering form, which is seen during training, compared to premise-hypothesis\nform which is unlike its training examples. Code and data are released under\nhttps://github.com/feyzaakyurek/bbnli.",
        "pdf_link": "https://arxiv.org/pdf/2205.11605v1.pdf"
    },
    {
        "title": "Challenges in Measuring Bias via Open-Ended Language Generation",
        "authors": [
            "Afra Feyza Akyürek",
            "Muhammed Yusuf Kocyigit",
            "Sejin Paik",
            "Derry Wijaya"
        ],
        "published": "2022-05-23T19:57:15Z",
        "summary": "Researchers have devised numerous ways to quantify social biases vested in\npretrained language models. As some language models are capable of generating\ncoherent completions given a set of textual prompts, several prompting datasets\nhave been proposed to measure biases between social groups -- posing language\ngeneration as a way of identifying biases. In this opinion paper, we analyze\nhow specific choices of prompt sets, metrics, automatic tools and sampling\nstrategies affect bias results. We find out that the practice of measuring\nbiases through text completion is prone to yielding contradicting results under\ndifferent experiment settings. We additionally provide recommendations for\nreporting biases in open-ended language generation for a more complete outlook\nof biases exhibited by a given language model. Code to reproduce the results is\nreleased under https://github.com/feyzaakyurek/bias-textgen.",
        "pdf_link": "https://arxiv.org/pdf/2205.11601v1.pdf"
    },
    {
        "title": "On the Paradox of Learning to Reason from Data",
        "authors": [
            "Honghua Zhang",
            "Liunian Harold Li",
            "Tao Meng",
            "Kai-Wei Chang",
            "Guy Van den Broeck"
        ],
        "published": "2022-05-23T17:56:48Z",
        "summary": "Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be\ntrained end-to-end to solve logical reasoning problems presented in natural\nlanguage? We attempt to answer this question in a confined problem space where\nthere exists a set of parameters that perfectly simulates logical reasoning. We\nmake observations that seem to contradict each other: BERT attains near-perfect\naccuracy on in-distribution test examples while failing to generalize to other\ndata distributions over the exact same problem space. Our study provides an\nexplanation for this paradox: instead of learning to emulate the correct\nreasoning function, BERT has in fact learned statistical features that\ninherently exist in logical reasoning problems. We also show that it is\ninfeasible to jointly remove statistical features from data, illustrating the\ndifficulty of learning to reason in general. Our result naturally extends to\nother neural models and unveils the fundamental difference between learning to\nreason and learning to achieve high performance on NLP benchmarks using\nstatistical features.",
        "pdf_link": "https://arxiv.org/pdf/2205.11502v2.pdf"
    },
    {
        "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Aleksandr Drozd",
            "Felice Dell'Orletta"
        ],
        "published": "2022-05-23T15:19:09Z",
        "summary": "While Transformer-based language models are generally very robust to pruning,\nthere is the recently discovered outlier phenomenon: disabling only 48 out of\n110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We\nreplicate the original evidence for the outlier phenomenon and we link it to\nthe geometry of the embedding space. We find that in both BERT and RoBERTa the\nmagnitude of hidden state coefficients corresponding to outlier dimensions\ncorrelates with the frequency of encoded tokens in pre-training data, and it\nalso contributes to the \"vertical\" self-attention pattern enabling the model to\nfocus on the special tokens. This explains the drop in performance from\ndisabling the outliers, and it suggests that to decrease anisotropicity in\nfuture models we need pre-training schemas that would better take into account\nthe skewed token distributions.",
        "pdf_link": "https://arxiv.org/pdf/2205.11380v3.pdf"
    },
    {
        "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements",
        "authors": [
            "Conrad Borchers",
            "Dalia Sara Gala",
            "Benjamin Gilburt",
            "Eduard Oravkin",
            "Wilfried Bounsi",
            "Yuki M. Asano",
            "Hannah Rose Kirk"
        ],
        "published": "2022-05-23T15:05:27Z",
        "summary": "The growing capability and availability of generative language models has\nenabled a wide range of new downstream tasks. Academic research has identified,\nquantified and mitigated biases present in language models but is rarely\ntailored to downstream tasks where wider impact on individuals and society can\nbe felt. In this work, we leverage one popular generative language model,\nGPT-3, with the goal of writing unbiased and realistic job advertisements. We\nfirst assess the bias and realism of zero-shot generated advertisements and\ncompare them to real-world advertisements. We then evaluate prompt-engineering\nand fine-tuning as debiasing methods. We find that prompt-engineering with\ndiversity-encouraging prompts gives no significant improvement to bias, nor\nrealism. Conversely, fine-tuning, especially on unbiased real advertisements,\ncan improve realism and reduce bias.",
        "pdf_link": "https://arxiv.org/pdf/2205.11374v1.pdf"
    },
    {
        "title": "RL with KL penalties is better viewed as Bayesian inference",
        "authors": [
            "Tomasz Korbak",
            "Ethan Perez",
            "Christopher L Buckley"
        ],
        "published": "2022-05-23T12:47:13Z",
        "summary": "Reinforcement learning (RL) is frequently employed in fine-tuning large\nlanguage models (LMs), such as GPT-3, to penalize them for undesirable features\nof generated sequences, such as offensiveness, social bias, harmfulness or\nfalsehood. The RL formulation involves treating the LM as a policy and updating\nit to maximise the expected value of a reward function which captures human\npreferences, such as non-offensiveness. In this paper, we analyze challenges\nassociated with treating a language model as an RL policy and show how avoiding\nthose challenges requires moving beyond the RL paradigm. We start by observing\nthat the standard RL approach is flawed as an objective for fine-tuning LMs\nbecause it leads to distribution collapse: turning the LM into a degenerate\ndistribution. Then, we analyze KL-regularised RL, a widely used recipe for\nfine-tuning LMs, which additionally constrains the fine-tuned LM to stay close\nto its original distribution in terms of Kullback-Leibler (KL) divergence. We\nshow that KL-regularised RL is equivalent to variational inference:\napproximating a Bayesian posterior which specifies how to update a prior LM to\nconform with evidence provided by the reward function. We argue that this\nBayesian inference view of KL-regularised RL is more insightful than the\ntypically employed RL perspective. The Bayesian inference view explains how\nKL-regularised RL avoids the distribution collapse problem and offers a\nfirst-principles derivation for its objective. While this objective happens to\nbe equivalent to RL (with a particular choice of parametric reward), there\nexist other objectives for fine-tuning LMs which are no longer equivalent to\nRL. That observation leads to a more general point: RL is not an adequate\nformal framework for problems such as fine-tuning language models. These\nproblems are best viewed as Bayesian inference: approximating a pre-defined\ntarget distribution.",
        "pdf_link": "https://arxiv.org/pdf/2205.11275v2.pdf"
    },
    {
        "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning",
        "authors": [
            "Yuchao Li",
            "Fuli Luo",
            "Chuanqi Tan",
            "Mengdi Wang",
            "Songfang Huang",
            "Shen Li",
            "Junjie Bai"
        ],
        "published": "2022-05-23T02:43:45Z",
        "summary": "With the dramatically increased number of parameters in language models,\nsparsity methods have received ever-increasing research focus to compress and\naccelerate the models. While most research focuses on how to accurately retain\nappropriate weights while maintaining the performance of the compressed model,\nthere are challenges in the computational overhead and memory footprint of\nsparse training when compressing large-scale language models. To address this\nproblem, we propose a Parameter-efficient Sparse Training (PST) method to\nreduce the number of trainable parameters during sparse-aware training in\ndownstream tasks. Specifically, we first combine the data-free and data-driven\ncriteria to efficiently and accurately measure the importance of weights. Then\nwe investigate the intrinsic redundancy of data-driven weight importance and\nderive two obvious characteristics i.e., low-rankness and structuredness. Based\non that, two groups of small matrices are introduced to compute the data-driven\nimportance of weights, instead of using the original large importance score\nmatrix, which therefore makes the sparse training resource-efficient and\nparameter-efficient. Experiments with diverse networks (i.e., BERT, RoBERTa and\nGPT-2) on dozens of datasets demonstrate PST performs on par or better than\nprevious sparsity methods, despite only training a small number of parameters.\nFor instance, compared with previous sparsity methods, our PST only requires\n1.5% trainable parameters to achieve comparable performance on BERT.",
        "pdf_link": "https://arxiv.org/pdf/2205.11005v1.pdf"
    },
    {
        "title": "Improving Short Text Classification With Augmented Data Using GPT-3",
        "authors": [
            "Salvador Balkus",
            "Donghui Yan"
        ],
        "published": "2022-05-23T01:10:38Z",
        "summary": "GPT-3 is a large-scale natural language model developed by OpenAI that can\nperform many different tasks, including topic classification. Although\nresearchers claim that it requires only a small number of in-context examples\nto learn a task, in practice GPT-3 requires these training examples to be\neither of exceptional quality or a higher quantity than easily created by hand.\nTo address this issue, this study teaches GPT-3 to classify whether a question\nis related to data science by augmenting a small training set with additional\nexamples generated by GPT-3 itself. This study compares two classifiers: the\nGPT-3 Classification Endpoint with augmented examples, and the GPT-3 Completion\nEndpoint with an optimal training set chosen using a genetic algorithm. We find\nthat while the augmented Completion Endpoint achieves upwards of 80 percent\nvalidation accuracy, using the augmented Classification Endpoint yields more\nconsistent accuracy on unseen examples. In this way, giving large-scale machine\nlearning models like GPT-3 the ability to propose their own additional training\nexamples can result in improved classification performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.10981v1.pdf"
    },
    {
        "title": "Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers",
        "authors": [
            "Albert Q. Jiang",
            "Wenda Li",
            "Szymon Tworkowski",
            "Konrad Czechowski",
            "Tomasz Odrzygóźdź",
            "Piotr Miłoś",
            "Yuhuai Wu",
            "Mateja Jamnik"
        ],
        "published": "2022-05-22T18:03:03Z",
        "summary": "In theorem proving, the task of selecting useful premises from a large\nlibrary to unlock the proof of a given conjecture is crucially important. This\npresents a challenge for all theorem provers, especially the ones based on\nlanguage models, due to their relative inability to reason over huge volumes of\npremises in text form. This paper introduces Thor, a framework integrating\nlanguage models and automated theorem provers to overcome this difficulty. In\nThor, a class of methods called hammers that leverage the power of automated\ntheorem provers are used for premise selection, while all other tasks are\ndesignated to language models. Thor increases a language model's success rate\non the PISA dataset from $39\\%$ to $57\\%$, while solving $8.2\\%$ of problems\nneither language models nor automated theorem provers are able to solve on\ntheir own. Furthermore, with a significantly smaller computational budget, Thor\ncan achieve a success rate on the MiniF2F dataset that is on par with the best\nexisting methods. Thor can be instantiated for the majority of popular\ninteractive theorem provers via a straightforward protocol we provide.",
        "pdf_link": "https://arxiv.org/pdf/2205.10893v1.pdf"
    },
    {
        "title": "Scaling Laws and Interpretability of Learning from Repeated Data",
        "authors": [
            "Danny Hernandez",
            "Tom Brown",
            "Tom Conerly",
            "Nova DasSarma",
            "Dawn Drain",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Tom Henighan",
            "Tristan Hume",
            "Scott Johnston",
            "Ben Mann",
            "Chris Olah",
            "Catherine Olsson",
            "Dario Amodei",
            "Nicholas Joseph",
            "Jared Kaplan",
            "Sam McCandlish"
        ],
        "published": "2022-05-21T02:14:27Z",
        "summary": "Recent large language models have been trained on vast datasets, but also\noften on repeated data, either intentionally for the purpose of upweighting\nhigher quality data, or unintentionally because data deduplication is not\nperfect and the model is exposed to repeated data at the sentence, paragraph,\nor document level. Some works have reported substantial negative performance\neffects of this repeated data. In this paper we attempt to study repeated data\nsystematically and to understand its effects mechanistically. To do this, we\ntrain a family of models where most of the data is unique but a small fraction\nof it is repeated many times. We find a strong double descent phenomenon, in\nwhich repeated data can lead test loss to increase midway through training. A\npredictable range of repetition frequency leads to surprisingly severe\ndegradation in performance. For instance, performance of an 800M parameter\nmodel can be degraded to that of a 2x smaller model (400M params) by repeating\n0.1% of the data 100 times, despite the other 90% of the training tokens\nremaining unique. We suspect there is a range in the middle where the data can\nbe memorized and doing so consumes a large fraction of the model's capacity,\nand this may be where the peak of degradation occurs. Finally, we connect these\nobservations to recent mechanistic interpretability work - attempting to\nreverse engineer the detailed computations performed by the model - by showing\nthat data repetition disproportionately damages copying and internal structures\nassociated with generalization, such as induction heads, providing a possible\nmechanism for the shift from generalization to memorization. Taken together,\nthese results provide a hypothesis for why repeating a relatively small\nfraction of data in large language models could lead to disproportionately\nlarge harms to performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.10487v1.pdf"
    },
    {
        "title": "Automated Scoring for Reading Comprehension via In-context BERT Tuning",
        "authors": [
            "Nigel Fernandez",
            "Aritra Ghosh",
            "Naiming Liu",
            "Zichao Wang",
            "Benoît Choffin",
            "Richard Baraniuk",
            "Andrew Lan"
        ],
        "published": "2022-05-19T21:16:15Z",
        "summary": "Automated scoring of open-ended student responses has the potential to\nsignificantly reduce human grader effort. Recent advances in automated scoring\noften leverage textual representations based on pre-trained language models\nsuch as BERT and GPT as input to scoring models. Most existing approaches train\na separate model for each item/question, which is suitable for scenarios such\nas essay scoring where items can be quite different from one another. However,\nthese approaches have two limitations: 1) they fail to leverage item linkage\nfor scenarios such as reading comprehension where multiple items may share a\nreading passage; 2) they are not scalable since storing one model per item\nbecomes difficult when models have a large number of parameters. In this paper,\nwe report our (grand prize-winning) solution to the National Assessment of\nEducation Progress (NAEP) automated scoring challenge for reading\ncomprehension. Our approach, in-context BERT fine-tuning, produces a single\nshared scoring model for all items with a carefully-designed input structure to\nprovide contextual information on each item. We demonstrate the effectiveness\nof our approach via local evaluations using the training dataset provided by\nthe challenge. We also discuss the biases, common error types, and limitations\nof our approach.",
        "pdf_link": "https://arxiv.org/pdf/2205.09864v2.pdf"
    },
    {
        "title": "Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation",
        "authors": [
            "Samhita Honnavalli",
            "Aesha Parekh",
            "Lily Ou",
            "Sophie Groenwold",
            "Sharon Levy",
            "Vicente Ordonez",
            "William Yang Wang"
        ],
        "published": "2022-05-19T20:05:02Z",
        "summary": "Women are often perceived as junior to their male counterparts, even within\nthe same job titles. While there has been significant progress in the\nevaluation of gender bias in natural language processing (NLP), existing\nstudies seldom investigate how biases toward gender groups change when\ncompounded with other societal biases. In this work, we investigate how\nseniority impacts the degree of gender bias exhibited in pretrained neural\ngeneration models by introducing a novel framework for probing compound bias.\nWe contribute a benchmark robustness-testing dataset spanning two domains, U.S.\nsenatorship and professorship, created using a distant-supervision method. Our\ndataset includes human-written text with underlying ground truth and paired\ncounterfactuals. We then examine GPT-2 perplexity and the frequency of gendered\nlanguage in generated text. Our results show that GPT-2 amplifies bias by\nconsidering women as junior and men as senior more often than the ground truth\nin both domains. These results suggest that NLP applications built using GPT-2\nmay harm women in professional capacities.",
        "pdf_link": "https://arxiv.org/pdf/2205.09830v1.pdf"
    },
    {
        "title": "Overcoming Language Disparity in Online Content Classification with Multimodal Learning",
        "authors": [
            "Gaurav Verma",
            "Rohit Mujumdar",
            "Zijie J. Wang",
            "Munmun De Choudhury",
            "Srijan Kumar"
        ],
        "published": "2022-05-19T17:56:02Z",
        "summary": "Advances in Natural Language Processing (NLP) have revolutionized the way\nresearchers and practitioners address crucial societal problems. Large language\nmodels are now the standard to develop state-of-the-art solutions for text\ndetection and classification tasks. However, the development of advanced\ncomputational techniques and resources is disproportionately focused on the\nEnglish language, sidelining a majority of the languages spoken globally. While\nexisting research has developed better multilingual and monolingual language\nmodels to bridge this language disparity between English and non-English\nlanguages, we explore the promise of incorporating the information contained in\nimages via multimodal machine learning. Our comparative analyses on three\ndetection tasks focusing on crisis information, fake news, and emotion\nrecognition, as well as five high-resource non-English languages, demonstrate\nthat: (a) detection frameworks based on pre-trained large language models like\nBERT and multilingual-BERT systematically perform better on the English\nlanguage compared against non-English languages, and (b) including images via\nmultimodal learning bridges this performance gap. We situate our findings with\nrespect to existing work on the pitfalls of large language models, and discuss\ntheir theoretical and practical implications. Resources for this paper are\navailable at https://multimodality-language-disparity.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2205.09744v1.pdf"
    },
    {
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "authors": [
            "Antonia Creswell",
            "Murray Shanahan",
            "Irina Higgins"
        ],
        "published": "2022-05-19T17:25:28Z",
        "summary": "Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.",
        "pdf_link": "https://arxiv.org/pdf/2205.09712v1.pdf"
    },
    {
        "title": "Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",
        "authors": [
            "Joseph McDonald",
            "Baolin Li",
            "Nathan Frey",
            "Devesh Tiwari",
            "Vijay Gadepally",
            "Siddharth Samsi"
        ],
        "published": "2022-05-19T16:03:55Z",
        "summary": "The energy requirements of current natural language processing models\ncontinue to grow at a rapid, unsustainable pace. Recent works highlighting this\nproblem conclude there is an urgent need for methods that reduce the energy\nneeds of NLP and machine learning more broadly. In this article, we investigate\ntechniques that can be used to reduce the energy consumption of common NLP\napplications. In particular, we focus on techniques to measure energy usage and\ndifferent hardware and datacenter-oriented settings that can be tuned to reduce\nenergy consumption for training and inference for language models. We\ncharacterize the impact of these settings on metrics such as computational\nperformance and energy consumption through experiments conducted on a high\nperformance computing system as well as popular cloud computing platforms.\nThese techniques can lead to significant reduction in energy consumption when\ntraining language models or their use for inference. For example,\npower-capping, which limits the maximum power a GPU can consume, can enable a\n15\\% decrease in energy usage with marginal increase in overall computation\ntime when training a transformer-based language model.",
        "pdf_link": "https://arxiv.org/pdf/2205.09646v1.pdf"
    },
    {
        "title": "Are Prompt-based Models Clueless?",
        "authors": [
            "Pride Kavumba",
            "Ryo Takahashi",
            "Yusuke Oda"
        ],
        "published": "2022-05-19T02:47:58Z",
        "summary": "Finetuning large pre-trained language models with a task-specific head has\nadvanced the state-of-the-art on many natural language understanding\nbenchmarks. However, models with a task-specific head require a lot of training\ndata, making them susceptible to learning and exploiting dataset-specific\nsuperficial cues that do not generalize to other datasets. Prompting has\nreduced the data requirement by reusing the language model head and formatting\nthe task input to match the pre-training objective. Therefore, it is expected\nthat few-shot prompt-based models do not exploit superficial cues. This paper\npresents an empirical examination of whether few-shot prompt-based models also\nexploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,\nHANS, and COPA has revealed that prompt-based models also exploit superficial\ncues. While the models perform well on instances with superficial cues, they\noften underperform or only marginally outperform random accuracy on instances\nwithout superficial cues.",
        "pdf_link": "https://arxiv.org/pdf/2205.09295v2.pdf"
    },
    {
        "title": "Transformer-based Program Synthesis for Low-Data Environments",
        "authors": [
            "Jack Roper"
        ],
        "published": "2022-05-18T23:33:33Z",
        "summary": "Recent advancements in large pre-trained transformer models (GPT2/3, T5) have\nfound use in program synthesis to generate programs that satisfy a set of\ninput/output examples. However, these models perform poorly on long-horizon and\nlow-data tasks, and often don't seem to understand the semantics of the\nlanguages they generate. We investigate an approach that tackles both of these\nissues, by using attributed context-free-grammars of programming languages to\ngenerate programs, and then analyzing generated programs so that they can be\nannotated with compile and runtime attributes, such as types, so that\ninformation about the program can be remembered during long-horizon generation.\nWe firstly find that synthesized datasets can be made efficiently and can\nprovide transformer models with enough data in order to perform well on some\nsynthesis tasks. We also find that giving models access to program attributes\nis especially effective in low-data environments, and tends improve the quality\nand reduce errors of transformer-generated programs.",
        "pdf_link": "https://arxiv.org/pdf/2205.09246v1.pdf"
    },
    {
        "title": "The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues",
        "authors": [
            "Anaïs Tack",
            "Chris Piech"
        ],
        "published": "2022-05-16T09:36:30Z",
        "summary": "How can we test whether state-of-the-art generative models, such as Blender\nand GPT-3, are good AI teachers, capable of replying to a student in an\neducational dialogue? Designing an AI teacher test is challenging: although\nevaluation methods are much-needed, there is no off-the-shelf solution to\nmeasuring pedagogical ability. This paper reports on a first attempt at an AI\nteacher test. We built a solution around the insight that you can run\nconversational agents in parallel to human teachers in real-world dialogues,\nsimulate how different agents would respond to a student, and compare these\ncounterpart responses in terms of three abilities: speak like a teacher,\nunderstand a student, help a student. Our method builds on the reliability of\ncomparative judgments in education and uses a probabilistic model and Bayesian\nsampling to infer estimates of pedagogical ability. We find that, even though\nconversational agents (Blender in particular) perform well on conversational\nuptake, they are quantifiably worse than real teachers on several pedagogical\ndimensions, especially with regard to helpfulness (Blender: {\\Delta} ability =\n-0.75; GPT-3: {\\Delta} ability = -0.93).",
        "pdf_link": "https://arxiv.org/pdf/2205.07540v1.pdf"
    },
    {
        "title": "What GPT Knows About Who is Who",
        "authors": [
            "Xiaohan Yang",
            "Eduardo Peynetti",
            "Vasco Meerman",
            "Chris Tanner"
        ],
        "published": "2022-05-16T00:59:37Z",
        "summary": "Coreference resolution -- which is a crucial task for understanding discourse\nand language at large -- has yet to witness widespread benefits from large\nlanguage models (LLMs). Moreover, coreference resolution systems largely rely\non supervised labels, which are highly expensive and difficult to annotate,\nthus making it ripe for prompt engineering. In this paper, we introduce a\nQA-based prompt-engineering method and discern \\textit{generative}, pre-trained\nLLMs' abilities and limitations toward the task of coreference resolution. Our\nexperiments show that GPT-2 and GPT-Neo can return valid answers, but that\ntheir capabilities to identify coreferent mentions are limited and\nprompt-sensitive, leading to inconsistent results.",
        "pdf_link": "https://arxiv.org/pdf/2205.07407v1.pdf"
    },
    {
        "title": "Transkimmer: Transformer Learns to Layer-wise Skim",
        "authors": [
            "Yue Guan",
            "Zhengyi Li",
            "Jingwen Leng",
            "Zhouhan Lin",
            "Minyi Guo"
        ],
        "published": "2022-05-15T16:23:30Z",
        "summary": "Transformer architecture has become the de-facto model for many machine\nlearning tasks from natural language processing and computer vision. As such,\nimproving its computational efficiency becomes paramount. One of the major\ncomputational inefficiency of Transformer-based models is that they spend the\nidentical amount of computation throughout all layers. Prior works have\nproposed to augment the Transformer model with the capability of skimming\ntokens to improve its computational efficiency. However, they suffer from not\nhaving effectual and end-to-end optimization of the discrete skimming\npredictor. To address the above limitations, we propose the Transkimmer\narchitecture, which learns to identify hidden state tokens that are not\nrequired by each layer. The skimmed tokens are then forwarded directly to the\nfinal output, thus reducing the computation of the successive layers. The key\nidea in Transkimmer is to add a parameterized predictor before each layer that\nlearns to make the skimming decision. We also propose to adopt\nreparameterization trick and add skim loss for the end-to-end training of\nTranskimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark\ncompared with vanilla BERT-base baseline with less than 1% accuracy\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2205.07324v1.pdf"
    },
    {
        "title": "Discovering Latent Concepts Learned in BERT",
        "authors": [
            "Fahim Dalvi",
            "Abdul Rafae Khan",
            "Firoj Alam",
            "Nadir Durrani",
            "Jia Xu",
            "Hassan Sajjad"
        ],
        "published": "2022-05-15T09:45:34Z",
        "summary": "A large number of studies that analyze deep neural network models and their\nability to encode various linguistic and non-linguistic concepts provide an\ninterpretation of the inner mechanics of these models. The scope of the\nanalyses is limited to pre-defined concepts that reinforce the traditional\nlinguistic knowledge and do not reflect on how novel concepts are learned by\nthe model. We address this limitation by discovering and analyzing latent\nconcepts learned in neural network models in an unsupervised fashion and\nprovide interpretations from the model's perspective. In this work, we study:\ni) what latent concepts exist in the pre-trained BERT model, ii) how the\ndiscovered latent concepts align or diverge from classical linguistic hierarchy\nand iii) how the latent concepts evolve across layers. Our findings show: i) a\nmodel learns novel concepts (e.g. animal categories and demographic groups),\nwhich do not strictly adhere to any pre-defined categorization (e.g. POS,\nsemantic tags), ii) several latent concepts are based on multiple properties\nwhich may include semantics, syntax, and morphology, iii) the lower layers in\nthe model dominate in learning shallow lexical concepts while the higher layers\nlearn semantic relations and iv) the discovered latent concepts highlight\npotential biases learned in the model. We also release a novel BERT ConceptNet\ndataset (BCN) consisting of 174 concept labels and 1M annotated instances.",
        "pdf_link": "https://arxiv.org/pdf/2205.07237v1.pdf"
    },
    {
        "title": "PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain",
        "authors": [
            "Thiago Santos",
            "Amara Tariq",
            "Susmita Das",
            "Kavyasree Vayalpati",
            "Geoffrey H. Smith",
            "Hari Trivedi",
            "Imon Banerjee"
        ],
        "published": "2022-05-13T20:42:07Z",
        "summary": "Pathology text mining is a challenging task given the reporting variability\nand constant new findings in cancer sub-type definitions. However, successful\ntext mining of a large pathology database can play a critical role to advance\n'big data' cancer research like similarity-based treatment selection, case\nidentification, prognostication, surveillance, clinical trial screening, risk\nstratification, and many others. While there is a growing interest in\ndeveloping language models for more specific clinical domains, no\npathology-specific language space exist to support the rapid data-mining\ndevelopment in pathology space. In literature, a few approaches fine-tuned\ngeneral transformer models on specialized corpora while maintaining the\noriginal tokenizer, but in fields requiring specialized terminology, these\nmodels often fail to perform adequately. We propose PathologyBERT - a\npre-trained masked language model which was trained on 347,173 histopathology\nspecimen reports and publicly released in the Huggingface repository. Our\ncomprehensive experiments demonstrate that pre-training of transformer model on\npathology corpora yields performance improvements on Natural Language\nUnderstanding (NLU) and Breast Cancer Diagnose Classification when compared to\nnonspecific language models.",
        "pdf_link": "https://arxiv.org/pdf/2205.06885v1.pdf"
    },
    {
        "title": "Towards Answering Open-ended Ethical Quandary Questions",
        "authors": [
            "Yejin Bang",
            "Nayeon Lee",
            "Tiezheng Yu",
            "Leila Khalatbari",
            "Yan Xu",
            "Samuel Cahyawijaya",
            "Dan Su",
            "Bryan Wilie",
            "Romain Barraud",
            "Elham J. Barezi",
            "Andrea Madotto",
            "Hayden Kee",
            "Pascale Fung"
        ],
        "published": "2022-05-12T09:52:59Z",
        "summary": "Considerable advancements have been made in various NLP tasks based on the\nimpressive power of large language models (LLMs) and many NLP applications are\ndeployed in our daily lives. In this work, we challenge the capability of LLMs\nwith the new task of Ethical Quandary Generative Question Answering. Ethical\nquandary questions are more challenging to address because multiple conflicting\nanswers may exist to a single quandary. We explore the current capability of\nLLMs in providing an answer with a deliberative exchange of different\nperspectives to an ethical quandary, in the approach of Socratic philosophy,\ninstead of providing a closed answer like an oracle. We propose a model that\nsearches for different ethical principles applicable to the ethical quandary\nand generates an answer conditioned on the chosen principles through\nprompt-based few-shot learning. We also discuss the remaining challenges and\nethical issues involved in this task and suggest the direction toward\ndeveloping responsible NLP systems by incorporating human values explicitly.",
        "pdf_link": "https://arxiv.org/pdf/2205.05989v3.pdf"
    },
    {
        "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
        "authors": [
            "Katherine M. Collins",
            "Catherine Wong",
            "Jiahai Feng",
            "Megan Wei",
            "Joshua B. Tenenbaum"
        ],
        "published": "2022-05-11T18:14:33Z",
        "summary": "Human language offers a powerful window into our thoughts -- we tell stories,\ngive explanations, and express our beliefs and goals through words. Abundant\nevidence also suggests that language plays a developmental role in structuring\nour learning. Here, we ask: how much of human-like thinking can be captured by\nlearning statistical patterns in language alone? We first contribute a new\nchallenge benchmark for comparing humans and distributional large language\nmodels (LLMs). Our benchmark contains two problem-solving domains (planning and\nexplanation generation) and is designed to require generalization to new,\nout-of-distribution problems expressed in language. We find that humans are far\nmore robust than LLMs on this benchmark. Next, we propose a hybrid\nParse-and-Solve model, which augments distributional LLMs with a structured\nsymbolic reasoning module. We find that this model shows more robust adaptation\nto out-of-distribution planning problems, demonstrating the promise of hybrid\nAI models for more human-like reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2205.05718v1.pdf"
    },
    {
        "title": "Clinical Prompt Learning with Frozen Language Models",
        "authors": [
            "Niall Taylor",
            "Yi Zhang",
            "Dan Joyce",
            "Alejo Nevado-Holgado",
            "Andrey Kormilitzin"
        ],
        "published": "2022-05-11T14:25:13Z",
        "summary": "Prompt learning is a new paradigm in the Natural Language Processing (NLP)\nfield which has shown impressive performance on a number of natural language\ntasks with common benchmarking text datasets in full, few-shot, and zero-shot\ntrain-evaluation setups. Recently, it has even been observed that large but\nfrozen pre-trained language models (PLMs) with prompt learning outperform\nsmaller but fine-tuned models. However, as with many recent NLP trends, the\nperformance of even the largest PLMs such as GPT-3 do not perform well on\nspecialized domains (e.g. medical text), and the common practice to achieve\nState of the Art (SoTA) results still consists of pre-training and fine-tuning\nthe PLMs on downstream tasks. The reliance on fine-tuning large PLMs is\nproblematic in clinical settings where data is often held in non-GPU\nenvironments, and more resource efficient methods of training specialized\ndomain models is crucial. We investigated the viability of prompt learning on\nclinically meaningful decision tasks and directly compared with more\ntraditional fine-tuning methods. Results are partially in line with the prompt\nlearning literature, with prompt learning able to match or improve on\ntraditional fine-tuning with substantially fewer trainable parameters and\nrequiring less training data. We argue that prompt learning therefore provides\nlower computational resource costs applicable to clinical settings, that can\nserve as an alternative to fine-tuning ever increasing in size PLMs.\nComplementary code to reproduce experiments presented in this work can be found\nat: https://github.com/NtaylorOX/Public_Clinical_Prompt.",
        "pdf_link": "https://arxiv.org/pdf/2205.05535v1.pdf"
    },
    {
        "title": "Towards the Generation of Musical Explanations with GPT-3",
        "authors": [
            "Stephen James Krol",
            "Maria Teresa Llano",
            "Jon McCormack"
        ],
        "published": "2022-05-11T13:04:54Z",
        "summary": "Open AI's language model, GPT-3, has shown great potential for many NLP\ntasks, with applications in many different domains. In this work we carry out a\nfirst study on GPT-3's capability to communicate musical decisions through\ntextual explanations when prompted with a textual representation of a piece of\nmusic. Enabling a dialogue in human-AI music partnerships is an important step\ntowards more engaging and creative human-AI interactions. Our results show that\nGPT-3 lacks the necessary intelligence to really understand musical decisions.\nA major barrier to reach a better performance is the lack of data that includes\nexplanations of the creative process carried out by artists for musical pieces.\nWe believe such a resource would aid the understanding and collaboration with\nAI music systems.",
        "pdf_link": "https://arxiv.org/pdf/2206.08264v1.pdf"
    },
    {
        "title": "Query-Based Keyphrase Extraction from Long Documents",
        "authors": [
            "Martin Docekal",
            "Pavel Smrz"
        ],
        "published": "2022-05-11T10:29:30Z",
        "summary": "Transformer-based architectures in natural language processing force input\nsize limits that can be problematic when long documents need to be processed.\nThis paper overcomes this issue for keyphrase extraction by chunking the long\ndocuments while keeping a global context as a query defining the topic for\nwhich relevant keyphrases should be extracted. The developed system employs a\npre-trained BERT model and adapts it to estimate the probability that a given\ntext span forms a keyphrase. We experimented using various context sizes on two\npopular datasets, Inspec and SemEval, and a large novel dataset. The presented\nresults show that a shorter context with a query overcomes a longer one without\nthe query on long documents.",
        "pdf_link": "https://arxiv.org/pdf/2205.05391v1.pdf"
    },
    {
        "title": "Towards Unified Prompt Tuning for Few-shot Text Classification",
        "authors": [
            "Jianing Wang",
            "Chengyu Wang",
            "Fuli Luo",
            "Chuanqi Tan",
            "Minghui Qiu",
            "Fei Yang",
            "Qiuhui Shi",
            "Songfang Huang",
            "Ming Gao"
        ],
        "published": "2022-05-11T07:40:45Z",
        "summary": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language\nModels (PLMs) on few-shot text classification by employing task-specific\nprompts. Yet, PLMs are unfamiliar with prompt-style expressions during\npre-training, which limits the few-shot learning performance on downstream\ntasks. It would be desirable if the models can acquire some prompting knowledge\nbefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning\n(UPT) framework, leading to better few-shot text classification for BERT-style\nmodels by explicitly capturing prompting semantics from non-target NLP\ndatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for\njoint prompt learning across different NLP tasks, forcing PLMs to capture\ntask-invariant prompting knowledge. We further design a self-supervised task\nnamed Knowledge-enhanced Selective Masked Language Modeling to improve the\nPLM's generalization abilities for accurate adaptation to previously unseen\ntasks. After multi-task learning across multiple tasks, the PLM can be better\nprompt-tuned towards any dissimilar target tasks in low-resourced settings.\nExperiments over a variety of NLP tasks show that UPT consistently outperforms\nstate-of-the-arts for prompt-based fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2205.05313v1.pdf"
    },
    {
        "title": "Reducing Activation Recomputation in Large Transformer Models",
        "authors": [
            "Vijay Korthikanti",
            "Jared Casper",
            "Sangkug Lym",
            "Lawrence McAfee",
            "Michael Andersch",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2022-05-10T22:40:17Z",
        "summary": "Training large transformer models is one of the most important computational\nchallenges of modern AI. In this paper, we show how to significantly accelerate\ntraining of large transformer models by reducing activation recomputation.\nActivation recomputation is commonly used to work around memory capacity\nconstraints. Rather than storing activations for backpropagation, they are\ntraditionally recomputed, which saves memory but adds redundant compute. In\nthis work, we show most of this redundant compute is unnecessary because we can\nreduce memory consumption sufficiently without it. We present two novel yet\nvery simple techniques: sequence parallelism and selective activation\nrecomputation. In conjunction with tensor parallelism, these techniques almost\neliminate the need to recompute activations. We evaluate our approach on\nlanguage models up to one trillion parameters in scale and show that our method\nreduces activation memory by 5x, while reducing execution time overhead from\nactivation recomputation by over 90%. For example, when training a 530B\nparameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops\nUtilization of 54.2%, which is 29% faster than the 42.1% we achieve using\nrecomputation. Our implementation will be available in both Megatron-LM and\nNeMo-Megatron.",
        "pdf_link": "https://arxiv.org/pdf/2205.05198v1.pdf"
    },
    {
        "title": "Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words",
        "authors": [
            "Kaitlyn Zhou",
            "Kawin Ethayarajh",
            "Dallas Card",
            "Dan Jurafsky"
        ],
        "published": "2022-05-10T18:00:06Z",
        "summary": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,\nQA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in\nwhich word similarities estimated by cosine over BERT embeddings are\nunderstated and trace this effect to training data frequency. We find that\nrelative to human judgements, cosine similarity underestimates the similarity\nof frequent words with other instances of the same word or other words across\ncontexts, even after controlling for polysemy and other factors. We conjecture\nthat this underestimation of similarity for high frequency words is due to\ndifferences in the representational geometry of high and low frequency words\nand provide a formal argument for the two-dimensional case.",
        "pdf_link": "https://arxiv.org/pdf/2205.05092v1.pdf"
    },
    {
        "title": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence",
        "authors": [
            "Myeongjun Jang",
            "Frank Mtumbuka",
            "Thomas Lukasiewicz"
        ],
        "published": "2022-05-08T08:37:36Z",
        "summary": "The logical negation property (LNP), which implies generating different\npredictions for semantically opposite inputs, is an important property that a\ntrustworthy language model must satisfy. However, much recent evidence shows\nthat large-size pre-trained language models (PLMs) do not satisfy this\nproperty. In this paper, we perform experiments using probing tasks to assess\nPLM's LNP understanding. Unlike previous studies that only examined negation\nexpressions, we expand the boundary of the investigation to lexical semantics.\nThrough experiments, we observe that PLMs violate the LNP frequently. To\nalleviate the issue, we propose a novel intermediate training task, names\nmeaning-matching, designed to directly learn a meaning-text correspondence,\ninstead of relying on the distributional hypothesis. Through multiple\nexperiments, we find that the task enables PLMs to learn lexical semantic\ninformation. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm\nthat it is a safe intermediate task that guarantees a similar or better\nperformance of downstream tasks. Finally, we observe that our proposed approach\noutperforms our previous counterparts despite its time and resource efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2205.03815v1.pdf"
    },
    {
        "title": "Context-Aware Abbreviation Expansion Using Large Language Models",
        "authors": [
            "Shanqing Cai",
            "Subhashini Venugopalan",
            "Katrin Tomanek",
            "Ajit Narayanan",
            "Meredith Ringel Morris",
            "Michael P. Brenner"
        ],
        "published": "2022-05-08T03:02:53Z",
        "summary": "Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.",
        "pdf_link": "https://arxiv.org/pdf/2205.03767v3.pdf"
    },
    {
        "title": "Vector Representations of Idioms in Conversational Systems",
        "authors": [
            "Tosin Adewumi",
            "Foteini Liwicki",
            "Marcus Liwicki"
        ],
        "published": "2022-05-07T14:50:05Z",
        "summary": "We demonstrate, in this study, that an open-domain conversational system\ntrained on idioms or figurative language generates more fitting responses to\nprompts containing idioms. Idioms are part of everyday speech in many\nlanguages, across many cultures, but they pose a great challenge for many\nNatural Language Processing (NLP) systems that involve tasks such as\nInformation Retrieval (IR) and Machine Translation (MT), besides conversational\nAI. We utilize the Potential Idiomatic Expression (PIE)-English idioms corpus\nfor the two tasks that we investigate: classification and conversation\ngeneration. We achieve state-of-the-art (SoTA) result of 98% macro F1 score on\nthe classification task by using the SoTA T5 model. We experiment with three\ninstances of the SoTA dialogue model, Dialogue Generative Pre-trained\nTransformer (DialoGPT), for conversation generation. Their performances are\nevaluated using the automatic metric perplexity and human evaluation. The\nresults show that the model trained on the idiom corpus generates more fitting\nresponses to prompts containing idioms 71.9% of the time, compared to a similar\nmodel not trained on the idioms corpus. We contribute the model checkpoint/demo\nand code on the HuggingFace hub for public access.",
        "pdf_link": "https://arxiv.org/pdf/2205.03666v1.pdf"
    },
    {
        "title": "When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",
        "authors": [
            "Sebastian Schuster",
            "Tal Linzen"
        ],
        "published": "2022-05-06T20:49:27Z",
        "summary": "Understanding longer narratives or participating in conversations requires\ntracking of discourse entities that have been mentioned. Indefinite noun\nphrases (NPs), such as 'a dog', frequently introduce discourse entities but\nthis behavior is modulated by sentential operators such as negation. For\nexample, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse\nentity due to the presence of negation. In this work, we adapt the\npsycholinguistic assessment of language models paradigm to higher-level\nlinguistic phenomena and introduce an English evaluation suite that targets the\nknowledge of the interactions between sentential operators and indefinite NPs.\nWe use this evaluation suite for a fine-grained investigation of the entity\ntracking abilities of the Transformer-based models GPT-2 and GPT-3. We find\nthat while the models are to a certain extent sensitive to the interactions we\ninvestigate, they are all challenged by the presence of multiple NPs and their\nbehavior is not systematic, which suggests that even models at the scale of\nGPT-3 do not fully acquire basic entity tracking abilities.",
        "pdf_link": "https://arxiv.org/pdf/2205.03472v1.pdf"
    },
    {
        "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
        "authors": [
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2022-05-06T17:57:58Z",
        "summary": "Does prompting a large language model (LLM) like GPT-3 with explanations\nimprove in-context learning? We study this question on two NLP tasks that\ninvolve reasoning over text, namely question answering and natural language\ninference. We test the performance of four LLMs on three textual reasoning\ndatasets using prompts that include explanations in multiple different styles.\nFor these tasks, we find that including explanations in the prompts for OPT,\nGPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to\nmoderate accuracy improvements over standard few-show learning. However,\ntext-davinci-002 is able to benefit more substantially.\n  We further show that explanations generated by the LLMs may not entail the\nmodels' predictions nor be factually grounded in the input, even on simple\ntasks with extractive explanations. However, these flawed explanations can\nstill be useful as a way to verify LLMs' predictions post-hoc. Through analysis\nin our three settings, we show that explanations judged by humans to be\ngood--logically consistent with the input and the prediction--more likely\ncooccur with accurate predictions. Following these observations, we train\ncalibrators using automatically extracted scores that assess the reliability of\nexplanations, allowing us to improve performance post-hoc across all of our\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2205.03401v2.pdf"
    },
    {
        "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers",
        "authors": [
            "Ninareh Mehrabi",
            "Ahmad Beirami",
            "Fred Morstatter",
            "Aram Galstyan"
        ],
        "published": "2022-05-05T01:48:39Z",
        "summary": "Warning: this paper contains content that maybe offensive or upsetting.\nRecent research in Natural Language Processing (NLP) has advanced the\ndevelopment of various toxicity detection models with the intention of\nidentifying and mitigating toxic language from existing systems. Despite the\nabundance of research in this area, less attention has been given to\nadversarial attacks that force the system to generate toxic language and the\ndefense against them. Existing work to generate such attacks is either based on\nhuman-generated attacks which is costly and not scalable or, in case of\nautomatic attacks, the attack vector does not conform to human-like language,\nwhich can be detected using a language model loss. In this work, we propose\nattacks against conversational agents that are imperceptible, i.e., they fit\nthe conversation in terms of coherency, relevancy, and fluency, while they are\neffective and scalable, i.e., they can automatically trigger the system into\ngenerating toxic language. We then propose a defense mechanism against such\nattacks which not only mitigates the attack but also attempts to maintain the\nconversational flow. Through automatic and human evaluations, we show that our\ndefense is effective at avoiding toxic language generation even against\nimperceptible toxicity triggers while the generated language fits the\nconversation in terms of coherency and relevancy. Lastly, we establish the\ngeneralizability of such a defense mechanism on language generation models\nbeyond conversational agents.",
        "pdf_link": "https://arxiv.org/pdf/2205.02392v1.pdf"
    },
    {
        "title": "Provably Confidential Language Modelling",
        "authors": [
            "Xuandong Zhao",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "published": "2022-05-04T02:33:45Z",
        "summary": "Large language models are shown to memorize privacy information such as\nsocial security numbers in training data. Given the sheer scale of the training\ncorpus, it is challenging to screen and filter these privacy data, either\nmanually or automatically. In this paper, we propose Confidentially Redacted\nTraining (CRT), a method to train language generation models while protecting\nthe confidential segments. We borrow ideas from differential privacy (which\nsolves a related but distinct problem) and show that our method is able to\nprovably prevent unintended memorization by randomizing parts of the training\nprocess. Moreover, we show that redaction with an approximately correct\nscreening policy amplifies the confidentiality guarantee. We implement the\nmethod for both LSTM and GPT language models. Our experimental results show\nthat the models trained by CRT obtain almost the same perplexity while\npreserving strong confidentiality.",
        "pdf_link": "https://arxiv.org/pdf/2205.01863v2.pdf"
    },
    {
        "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology",
        "authors": [
            "Yacine Jernite",
            "Huu Nguyen",
            "Stella Biderman",
            "Anna Rogers",
            "Maraim Masoud",
            "Valentin Danchev",
            "Samson Tan",
            "Alexandra Sasha Luccioni",
            "Nishant Subramani",
            "Gérard Dupont",
            "Jesse Dodge",
            "Kyle Lo",
            "Zeerak Talat",
            "Isaac Johnson",
            "Dragomir Radev",
            "Somaieh Nikpoor",
            "Jörg Frohberg",
            "Aaron Gokaslan",
            "Peter Henderson",
            "Rishi Bommasani",
            "Margaret Mitchell"
        ],
        "published": "2022-05-04T00:44:35Z",
        "summary": "The recent emergence and adoption of Machine Learning technology, and\nspecifically of Large Language Models, has drawn attention to the need for\nsystematic and transparent management of language data. This work proposes an\napproach to global language data governance that attempts to organize data\nmanagement amongst stakeholders, values, and rights. Our proposal is informed\nby prior work on distributed governance that accounts for human values and\ngrounded by an international research collaboration that brings together\nresearchers and practitioners from 60 countries. The framework we present is a\nmulti-party international governance structure focused on language data, and\nincorporating technical and organizational tools needed to support its work.",
        "pdf_link": "https://arxiv.org/pdf/2206.03216v2.pdf"
    },
    {
        "title": "Efficient Fine-Tuning of BERT Models on the Edge",
        "authors": [
            "Danilo Vucetic",
            "Mohammadreza Tayaranian",
            "Maryam Ziaeefard",
            "James J. Clark",
            "Brett H. Meyer",
            "Warren J. Gross"
        ],
        "published": "2022-05-03T14:51:53Z",
        "summary": "Resource-constrained devices are increasingly the deployment targets of\nmachine learning applications. Static models, however, do not always suffice\nfor dynamic environments. On-device training of models allows for quick\nadaptability to new scenarios. With the increasing size of deep neural\nnetworks, as noted with the likes of BERT and other natural language processing\nmodels, comes increased resource requirements, namely memory, computation,\nenergy, and time. Furthermore, training is far more resource intensive than\ninference. Resource-constrained on-device learning is thus doubly difficult,\nespecially with large BERT-like models. By reducing the memory usage of\nfine-tuning, pre-trained BERT models can become efficient enough to fine-tune\non resource-constrained devices. We propose Freeze And Reconfigure (FAR), a\nmemory-efficient training regime for BERT-like models that reduces the memory\nusage of activation maps during fine-tuning by avoiding unnecessary parameter\nupdates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset\nby 30%, and time spent on memory operations by 47%. More broadly, reductions in\nmetric performance on the GLUE and SQuAD datasets are around 1% on average.",
        "pdf_link": "https://arxiv.org/pdf/2205.01541v1.pdf"
    },
    {
        "title": "SemAttack: Natural Textual Attacks via Different Semantic Spaces",
        "authors": [
            "Boxin Wang",
            "Chejian Xu",
            "Xiangyu Liu",
            "Yu Cheng",
            "Bo Li"
        ],
        "published": "2022-05-03T03:44:03Z",
        "summary": "Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.",
        "pdf_link": "https://arxiv.org/pdf/2205.01287v3.pdf"
    },
    {
        "title": "Improving Students' Academic Performance with AI and Semantic Technologies",
        "authors": [
            "Yixin Cheng"
        ],
        "published": "2022-05-02T06:11:24Z",
        "summary": "Artificial intelligence and semantic technologies are evolving and have been\napplied in various research areas, including the education domain. Higher\nEducation institutions strive to improve students' academic performance. Early\nintervention to at-risk students and a reasonable curriculum is vital for\nstudents' success. Prior research opted for deploying traditional machine\nlearning models to predict students' performance. In terms of curriculum\nsemantic analysis, after conducting a comprehensive systematic review regarding\nthe use of semantic technologies in the Computer Science curriculum, a major\nfinding of the study is that technologies used to measure similarity have\nlimitations in terms of accuracy and ambiguity in the representation of\nconcepts, courses, etc. To fill these gaps, in this study, three\nimplementations were developed, that is, to predict students' performance using\nmarks from the previous semester, to model a course representation in a\nsemantic way and compute the similarity, and to identify the prerequisite\nbetween two similar courses. Regarding performance prediction, we used the\ncombination of Genetic Algorithm and Long-Short Term Memory (LSTM) on a dataset\nfrom a Brazilian university containing 248730 records. As for similarity\nmeasurement, we deployed BERT to encode the sentences and used cosine\nsimilarity to obtain the distance between courses. With respect to prerequisite\nidentification, TextRazor was applied to extract concepts from course\ndescription, followed by employing SemRefD to measure the degree of\nprerequisite between two concepts. The outcomes of this study can be summarized\nas: (i) a breakthrough result improves Manrique's work by 2.5% in terms of\naccuracy in dropout prediction; (ii) uncover the similarity between courses\nbased on course description; (iii) identify the prerequisite over three\ncompulsory courses of School of Computing at ANU.",
        "pdf_link": "https://arxiv.org/pdf/2206.03213v2.pdf"
    },
    {
        "title": "Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning",
        "authors": [
            "Angelo Ziletti",
            "Alan Akbik",
            "Christoph Berns",
            "Thomas Herold",
            "Marion Legler",
            "Martina Viell"
        ],
        "published": "2022-05-01T22:49:28Z",
        "summary": "Medical coding (MC) is an essential pre-requisite for reliable data retrieval\nand reporting. Given a free-text reported term (RT) such as \"pain of right\nthigh to the knee\", the task is to identify the matching lowest-level term\n(LLT) - in this case \"unilateral leg pain\" - from a very large and continuously\ngrowing repository of standardized medical terms. However, automating this task\nis challenging due to a large number of LLT codes (as of writing over 80,000),\nlimited availability of training data for long tail/emerging classes, and the\ngeneral high accuracy demands of the medical domain. With this paper, we\nintroduce the MC task, discuss its challenges, and present a novel approach\ncalled xTARS that combines traditional BERT-based classification with a recent\nzero/few-shot learning approach (TARS). We present extensive experiments that\nshow that our combined approach outperforms strong baselines, especially in the\nfew-shot regime. The approach is developed and deployed at Bayer, live since\nNovember 2021. As we believe our approach potentially promising beyond MC, and\nto ensure reproducibility, we release the code to the research community.",
        "pdf_link": "https://arxiv.org/pdf/2206.02662v1.pdf"
    },
    {
        "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",
        "authors": [
            "Ehud Karpas",
            "Omri Abend",
            "Yonatan Belinkov",
            "Barak Lenz",
            "Opher Lieber",
            "Nir Ratner",
            "Yoav Shoham",
            "Hofit Bata",
            "Yoav Levine",
            "Kevin Leyton-Brown",
            "Dor Muhlgay",
            "Noam Rozen",
            "Erez Schwartz",
            "Gal Shachaf",
            "Shai Shalev-Shwartz",
            "Amnon Shashua",
            "Moshe Tenenholtz"
        ],
        "published": "2022-05-01T11:01:28Z",
        "summary": "Huge language models (LMs) have ushered in a new era for AI, serving as a\ngateway to natural-language-based knowledge tasks. Although an essential\nelement of modern AI, LMs are also inherently limited in a number of ways. We\ndiscuss these limitations and how they can be avoided by adopting a systems\napproach. Conceptualizing the challenge as one that involves knowledge and\nreasoning in addition to linguistic processing, we define a flexible\narchitecture with multiple neural models, complemented by discrete knowledge\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'\nMRKL system implementation.",
        "pdf_link": "https://arxiv.org/pdf/2205.00445v1.pdf"
    },
    {
        "title": "Training Language Models with Language Feedback",
        "authors": [
            "Jérémy Scheurer",
            "Jon Ander Campos",
            "Jun Shern Chan",
            "Angelica Chen",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2022-04-29T15:06:58Z",
        "summary": "Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level summarization\nability.",
        "pdf_link": "https://arxiv.org/pdf/2204.14146v4.pdf"
    },
    {
        "title": "Inferring Implicit Relations in Complex Questions with Language Models",
        "authors": [
            "Uri Katz",
            "Mor Geva",
            "Jonathan Berant"
        ],
        "published": "2022-04-28T21:00:54Z",
        "summary": "A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the challenge in\nimplicit reasoning questions does not stem from the need to plan a reasoning\nstrategy alone, but to do it while also retrieving and reasoning over relevant\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2204.13778v2.pdf"
    },
    {
        "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
        "authors": [
            "Seongjin Shin",
            "Sang-Woo Lee",
            "Hwijeen Ahn",
            "Sungdong Kim",
            "HyoungSeok Kim",
            "Boseop Kim",
            "Kyunghyun Cho",
            "Gichang Lee",
            "Woomyoung Park",
            "Jung-Woo Ha",
            "Nako Sung"
        ],
        "published": "2022-04-28T13:59:54Z",
        "summary": "Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.13509v2.pdf"
    },
    {
        "title": "An End-to-End Dialogue Summarization System for Sales Calls",
        "authors": [
            "Abedelkadir Asi",
            "Song Wang",
            "Roy Eisenstadt",
            "Dean Geckt",
            "Yarin Kuper",
            "Yi Mao",
            "Royi Ronen"
        ],
        "published": "2022-04-27T14:02:50Z",
        "summary": "Summarizing sales calls is a routine task performed manually by salespeople.\nWe present a production system which combines generative models fine-tuned for\ncustomer-agent setting, with a human-in-the-loop user experience for an\ninteractive summary curation process. We address challenging aspects of\ndialogue summarization task in a real-world setting including long input\ndialogues, content validation, lack of labeled data and quality evaluation. We\nshow how GPT-3 can be leveraged as an offline data labeler to handle training\ndata scarcity and accommodate privacy constraints in an industrial setting.\nExperiments show significant improvements by our models in tackling the\nsummarization and content validation tasks on public datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.12951v2.pdf"
    },
    {
        "title": "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval",
        "authors": [
            "Yuying Ge",
            "Yixiao Ge",
            "Xihui Liu",
            "Alex Jinpeng Wang",
            "Jianping Wu",
            "Ying Shan",
            "Xiaohu Qie",
            "Ping Luo"
        ],
        "published": "2022-04-26T16:06:31Z",
        "summary": "Dominant pre-training work for video-text retrieval mainly adopt the\n\"dual-encoder\" architectures to enable efficient retrieval, where two separate\nencoders are used to contrast global video and text representations, but ignore\ndetailed local semantics. The recent success of image BERT pre-training with\nmasked visual modeling that promotes the learning of local visual context,\nmotivates a possible solution to address the above limitation. In this work, we\nfor the first time investigate masked visual modeling in video-text\npre-training with the \"dual-encoder\" architecture. We perform Masked visual\nmodeling with Injected LanguagE Semantics (MILES) by employing an extra\nsnapshot video encoder as an evolving \"tokenizer\" to produce reconstruction\ntargets for masked video patch prediction. Given the corrupted video, the video\nencoder is trained to recover text-aligned features of the masked patches via\nreasoning with the visible regions along the spatial and temporal dimensions,\nwhich enhances the discriminativeness of local visual features and the\nfine-grained cross-modality alignment. Our method outperforms state-of-the-art\nmethods for text-to-video retrieval on four datasets with both zero-shot and\nfine-tune evaluation protocols. Our approach also surpasses the baseline models\nsignificantly on zero-shot action recognition, which can be cast as\nvideo-to-text retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2204.12408v1.pdf"
    },
    {
        "title": "You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas",
        "authors": [
            "Haoran Li",
            "Yangqiu Song",
            "Lixin Fan"
        ],
        "published": "2022-04-26T09:36:18Z",
        "summary": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large\npretrained language models. Despite the huge progress, privacy concerns have\narisen recently: training data of large language models can be extracted via\nmodel inversion attacks. On the other hand, the datasets used for training\nchatbots contain many private conversations between two individuals. In this\nwork, we further investigate the privacy leakage of the hidden states of\nchatbots trained by language modeling which has not been well studied yet. We\nshow that speakers' personas can be inferred through a simple neural network\nwith high accuracy. To this end, we propose effective defense objectives to\nprotect persona leakage from hidden states. We conduct extensive experiments to\ndemonstrate that our proposed defense objectives can greatly reduce the attack\naccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve\nlanguage models' powerful generation ability.",
        "pdf_link": "https://arxiv.org/pdf/2205.10228v1.pdf"
    },
    {
        "title": "Pretraining Chinese BERT for Detecting Word Insertion and Deletion Errors",
        "authors": [
            "Cong Zhou",
            "Yong Dai",
            "Duyu Tang",
            "Enbo Zhao",
            "Zhangyin Feng",
            "Li Kuang",
            "Shuming Shi"
        ],
        "published": "2022-04-26T03:19:36Z",
        "summary": "Chinese BERT models achieve remarkable progress in dealing with grammatical\nerrors of word substitution. However, they fail to handle word insertion and\ndeletion because BERT assumes the existence of a word at each position. To\naddress this, we present a simple and effective Chinese pretrained model. The\nbasic idea is to enable the model to determine whether a word exists at a\nparticular position. We achieve this by introducing a special token\n\\texttt{[null]}, the prediction of which stands for the non-existence of a\nword. In the training stage, we design pretraining tasks such that the model\nlearns to predict \\texttt{[null]} and real words jointly given the surrounding\ncontext. In the inference stage, the model readily detects whether a word\nshould be inserted or deleted with the standard masked language modeling\nfunction. We further create an evaluation dataset to foster research on word\ninsertion and deletion. It includes human-annotated corrections for 7,726\nerroneous sentences. Results show that existing Chinese BERT performs poorly on\ndetecting insertion and deletion errors. Our approach significantly improves\nthe F1 scores from 24.1\\% to 78.1\\% for word insertion and from 26.5\\% to\n68.5\\% for word deletion, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2204.12052v1.pdf"
    },
    {
        "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
        "authors": [
            "Kai Hui",
            "Honglei Zhuang",
            "Tao Chen",
            "Zhen Qin",
            "Jing Lu",
            "Dara Bahri",
            "Ji Ma",
            "Jai Prakash Gupta",
            "Cicero Nogueira dos Santos",
            "Yi Tay",
            "Don Metzler"
        ],
        "published": "2022-04-25T06:26:29Z",
        "summary": "State-of-the-art neural models typically encode document-query pairs using\ncross-attention for re-ranking. To this end, models generally utilize an\nencoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach.\nThese paradigms, however, are not without flaws, i.e., running the model on all\nquery-document pairs at inference-time incurs a significant computational cost.\nThis paper proposes a new training and inference paradigm for re-ranking. We\npropose to finetune a pretrained encoder-decoder model using in the form of\ndocument to query generation. Subsequently, we show that this encoder-decoder\narchitecture can be decomposed into a decoder-only language model during\ninference. This results in significant inference time speedups since the\ndecoder-only architecture only needs to learn to interpret static encoder\nembeddings during inference. Our experiments show that this new paradigm\nachieves results that are comparable to the more expensive cross-attention\nranking approaches while being up to 6.8X faster. We believe this work paves\nthe way for more efficient neural rankers that leverage large pretrained\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2204.11458v1.pdf"
    },
    {
        "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
        "authors": [
            "Stephanie C. Y. Chan",
            "Adam Santoro",
            "Andrew K. Lampinen",
            "Jane X. Wang",
            "Aaditya Singh",
            "Pierre H. Richemond",
            "Jay McClelland",
            "Felix Hill"
        ],
        "published": "2022-04-22T16:10:50Z",
        "summary": "Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2205.05055v6.pdf"
    },
    {
        "title": "KALA: Knowledge-Augmented Language Model Adaptation",
        "authors": [
            "Minki Kang",
            "Jinheon Baek",
            "Sung Ju Hwang"
        ],
        "published": "2022-04-22T08:11:59Z",
        "summary": "Pre-trained language models (PLMs) have achieved remarkable success on\nvarious natural language understanding tasks. Simple fine-tuning of PLMs, on\nthe other hand, might be suboptimal for domain-specific tasks because they\ncannot possibly cover knowledge from all domains. While adaptive pre-training\nof PLMs can help them obtain domain-specific knowledge, it requires a large\ntraining cost. Moreover, adaptive pre-training can harm the PLM's performance\non the downstream task by causing catastrophic forgetting of its general\nknowledge. To overcome such limitations of adaptive pre-training for PLM\nadaption, we propose a novel domain adaption framework for PLMs coined as\nKnowledge-Augmented Language model Adaptation (KALA), which modulates the\nintermediate hidden representations of PLMs with domain knowledge, consisting\nof entities and their relational facts. We validate the performance of our KALA\non question answering and named entity recognition tasks on multiple datasets\nacross various domains. The results show that, despite being computationally\nefficient, our KALA largely outperforms adaptive pre-training. Code is\navailable at: https://github.com/Nardien/KALA/.",
        "pdf_link": "https://arxiv.org/pdf/2204.10555v2.pdf"
    },
    {
        "title": "Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing",
        "authors": [
            "Benedikt Boecking",
            "Naoto Usuyama",
            "Shruthi Bannur",
            "Daniel C. Castro",
            "Anton Schwaighofer",
            "Stephanie Hyland",
            "Maria Wetscherek",
            "Tristan Naumann",
            "Aditya Nori",
            "Javier Alvarez-Valle",
            "Hoifung Poon",
            "Ozan Oktay"
        ],
        "published": "2022-04-21T00:04:35Z",
        "summary": "Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.",
        "pdf_link": "https://arxiv.org/pdf/2204.09817v4.pdf"
    },
    {
        "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models",
        "authors": [
            "Richard Plant",
            "Valerio Giuffrida",
            "Dimitra Gkatzia"
        ],
        "published": "2022-04-20T11:12:53Z",
        "summary": "Large scale adoption of large language models has introduced a new era of\nconvenient knowledge transfer for a slew of natural language processing tasks.\nHowever, these models also run the risk of undermining user trust by exposing\nunwanted information about the data subjects, which may be extracted by a\nmalicious party, e.g. through adversarial attacks. We present an empirical\ninvestigation into the extent of the personal information encoded into\npre-trained representations by a range of popular models, and we show a\npositive correlation between the complexity of a model, the amount of data used\nin pre-training, and data leakage. In this paper, we present the first wide\ncoverage evaluation and comparison of some of the most popular\nprivacy-preserving algorithms, on a large, multi-lingual dataset on sentiment\nanalysis annotated with demographic information (location, age and gender). The\nresults show since larger and more complex models are more prone to leaking\nprivate information, use of privacy-preserving methods is highly desirable. We\nalso find that highly privacy-preserving technologies like differential privacy\n(DP) can have serious model utility effects, which can be ameliorated using\nhybrid or metric-DP techniques.",
        "pdf_link": "https://arxiv.org/pdf/2204.09391v1.pdf"
    },
    {
        "title": "Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification",
        "authors": [
            "Dawei Zhu",
            "Michael A. Hedderich",
            "Fangzhou Zhai",
            "David Ifeoluwa Adelani",
            "Dietrich Klakow"
        ],
        "published": "2022-04-20T10:24:19Z",
        "summary": "Incorrect labels in training data occur when human annotators make mistakes\nor when the data is generated via weak or distant supervision. It has been\nshown that complex noise-handling techniques - by modeling, cleaning or\nfiltering the noisy instances - are required to prevent models from fitting\nthis label noise. However, we show in this work that, for text classification\ntasks with modern NLP models like BERT, over a variety of noise types, existing\nnoisehandling methods do not always improve its performance, and may even\ndeteriorate it, suggesting the need for further investigation. We also back our\nobservations with a comprehensive analysis.",
        "pdf_link": "https://arxiv.org/pdf/2204.09371v1.pdf"
    },
    {
        "title": "What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment",
        "authors": [
            "Matthew Finlayson",
            "Kyle Richardson",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "published": "2022-04-19T22:11:47Z",
        "summary": "The instruction learning paradigm -- where a model learns to perform new\ntasks from task descriptions alone -- has become popular in general-purpose\nmodel research. The capabilities of large transformer models as instruction\nlearners, however, remain poorly understood. We use a controlled synthetic\nenvironment to characterize such capabilities. Specifically, we use the task of\ndeciding whether a given string matches a regular expression (viewed as an\ninstruction) to identify properties of tasks, instructions, and instances that\nmake instruction learning challenging. For instance, we find that our model, a\nfine-tuned T5-based text2text transformer, struggles with large regular\nlanguages, suggesting that less precise instructions are challenging for\nmodels. Additionally, instruction executions that require tracking longer\ncontexts of prior steps are also more difficult. We use our findings to\nsystematically construct a challenging instruction learning dataset, which we\ncall Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to\ncorrectly interpret only 65.6% of test instructions (with at least 90%\naccuracy), and 11%-24% of the instructions in out-of-distribution\ngeneralization settings. We propose Hard RegSet as a challenging instruction\nlearning task, and a controlled environment for studying instruction learning.",
        "pdf_link": "https://arxiv.org/pdf/2204.09148v2.pdf"
    },
    {
        "title": "Impact of Tokenization on Language Models: An Analysis for Turkish",
        "authors": [
            "Cagri Toraman",
            "Eyup Halit Yilmaz",
            "Furkan Şahinuç",
            "Oguzhan Ozcelik"
        ],
        "published": "2022-04-19T12:01:46Z",
        "summary": "Tokenization is an important text preprocessing step to prepare input tokens\nfor deep language models. WordPiece and BPE are de facto methods employed by\nimportant models, such as BERT and GPT. However, the impact of tokenization can\nbe different for morphologically rich languages, such as Turkic languages,\nwhere many words can be generated by adding prefixes and suffixes. We compare\nfive tokenizers at different granularity levels, i.e. their outputs vary from\nsmallest pieces of characters to the surface form of words, including a\nMorphological-level tokenizer. We train these tokenizers and pretrain\nmedium-sized language models using RoBERTa pretraining procedure on the Turkish\nsplit of the OSCAR corpus. We then fine-tune our models on six downstream\ntasks. Our experiments, supported by statistical tests, reveal that\nMorphological-level tokenizer has challenging performance with de facto\ntokenizers. Furthermore, we find that increasing the vocabulary size improves\nthe performance of Morphological and Word-level tokenizers more than that of de\nfacto tokenizers. The ratio of the number of vocabulary parameters to the total\nnumber of model parameters can be empirically chosen as 20% for de facto\ntokenizers and 40% for other tokenizers to obtain a reasonable trade-off\nbetween model size and performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.08832v1.pdf"
    },
    {
        "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks",
        "authors": [
            "Ziyang Luo",
            "Yadong Xi",
            "Jing Ma",
            "Zhiwei Yang",
            "Xiaoxi Mao",
            "Changjie Fan",
            "Rongsheng Zhang"
        ],
        "published": "2022-04-19T06:12:48Z",
        "summary": "Since 2017, the Transformer-based models play critical roles in various\ndownstream Natural Language Processing tasks. However, a common limitation of\nthe attention mechanism utilized in Transformer Encoder is that it cannot\nautomatically capture the information of word order, so explicit position\nembeddings are generally required to be fed into the target model. In contrast,\nTransformer Decoder with the causal attention masks is naturally sensitive to\nthe word order. In this work, we focus on improving the position encoding\nability of BERT with the causal attention masks. Furthermore, we propose a new\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\nExperimental results show that (1) the causal attention mask is effective for\nBERT on the language understanding tasks; (2) our DecBERT model without\nposition embeddings achieve comparable performance on the GLUE benchmark; and\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\nachieves better overall performance than the baseline systems when pre-training\nwith the same amount of computational resources.",
        "pdf_link": "https://arxiv.org/pdf/2204.08688v1.pdf"
    },
    {
        "title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
        "authors": [
            "Charlie Snell",
            "Mengjiao Yang",
            "Justin Fu",
            "Yi Su",
            "Sergey Levine"
        ],
        "published": "2022-04-18T17:23:11Z",
        "summary": "Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.10198v2.pdf"
    },
    {
        "title": "L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and BERT Language Models",
        "authors": [
            "Ravindra Nayak",
            "Raviraj Joshi"
        ],
        "published": "2022-04-18T16:49:59Z",
        "summary": "Code-switching occurs when more than one language is mixed in a given\nsentence or a conversation. This phenomenon is more prominent on social media\nplatforms and its adoption is increasing over time. Therefore code-mixed NLP\nhas been extensively studied in the literature. As pre-trained\ntransformer-based architectures are gaining popularity, we observe that real\ncode-mixing data are scarce to pre-train large language models. We present\nL3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in\na Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from\nTwitter. We further present HingBERT, HingMBERT, HingRoBERTa, and HingGPT. The\nBERT models have been pre-trained on codemixed HingCorpus using masked language\nmodelling objectives. We show the effectiveness of these BERT models on the\nsubsequent downstream tasks like code-mixed sentiment analysis, POS tagging,\nNER, and LID from the GLUECoS benchmark. The HingGPT is a GPT2 based generative\ntransformer model capable of generating full tweets. We also release\nL3Cube-HingLID Corpus, the largest code-mixed Hindi-English language\nidentification(LID) dataset and HingBERT-LID, a production-quality LID model to\nfacilitate capturing of more code-mixed data using the process outlined in this\nwork. The dataset and models are available at\nhttps://github.com/l3cube-pune/code-mixed-nlp .",
        "pdf_link": "https://arxiv.org/pdf/2204.08398v1.pdf"
    },
    {
        "title": "Pathologies of Pre-trained Language Models in Few-shot Fine-tuning",
        "authors": [
            "Hanjie Chen",
            "Guoqing Zheng",
            "Ahmed Hassan Awadallah",
            "Yangfeng Ji"
        ],
        "published": "2022-04-17T15:55:18Z",
        "summary": "Although adapting pre-trained language models with few examples has shown\npromising performance on text classification, there is a lack of understanding\nof where the performance gain comes from. In this work, we propose to answer\nthis question by interpreting the adaptation behavior using post-hoc\nexplanations from model predictions. By modeling feature statistics of\nexplanations, we discover that (1) without fine-tuning, pre-trained models\n(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although\nfew-shot fine-tuning can mitigate the prediction bias and demonstrate promising\nprediction performance, our analysis shows models gain performance improvement\nby capturing non-task-related features (e.g. stop words) or shallow data\npatterns (e.g. lexical overlaps). These observations alert that pursuing model\nperformance with fewer examples may incur pathological prediction behavior,\nwhich requires further sanity check on model predictions and careful design in\nmodel evaluations in few-shot fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2204.08039v1.pdf"
    },
    {
        "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
        "authors": [
            "Weiyan Shi",
            "Ryan Shea",
            "Si Chen",
            "Chiyuan Zhang",
            "Ruoxi Jia",
            "Zhou Yu"
        ],
        "published": "2022-04-15T22:36:55Z",
        "summary": "Protecting large language models from privacy leakage is becoming\nincreasingly crucial with their wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical notion with provable privacy\nguarantees for machine learning models, to those models remains challenging due\nto the trade-off between model utility and privacy loss. Utilizing the fact\nthat sensitive information in language data tends to be sparse, Shi et al.\n(2021) formalized a DP notion extension called Selective Differential Privacy\n(SDP) to protect only the sensitive tokens defined by a policy function.\nHowever, their algorithm only works for RNN-based models. In this paper, we\ndevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models. Our method is easy to\nimplement: it first fine-tunes the model with redacted in-domain data, and then\nfine-tunes it again with the original in-domain data using a private training\nmechanism. Furthermore, we study the scenario of imperfect implementation of\npolicy functions that misses sensitive tokens and develop systematic methods to\nhandle it. Experiments show that our method achieves strong utility compared to\nprevious baselines. We also analyze the SDP privacy guarantee empirically with\nthe canary insertion attack.",
        "pdf_link": "https://arxiv.org/pdf/2204.07667v3.pdf"
    },
    {
        "title": "CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing",
        "authors": [
            "Chen Liang",
            "Pengcheng He",
            "Yelong Shen",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2022-04-13T19:54:51Z",
        "summary": "Model ensemble is a popular approach to produce a low-variance and\nwell-generalized model. However, it induces large memory and inference costs,\nwhich are often not affordable for real-world deployment. Existing work has\nresorted to sharing weights among models. However, when increasing the\nproportion of the shared weights, the resulting models tend to be similar, and\nthe benefits of using model ensemble diminish. To retain ensemble benefits\nwhile maintaining a low memory cost, we propose a consistency-regularized\nensemble learning approach based on perturbed models, named CAMERO.\nSpecifically, we share the weights of bottom layers across all models and apply\ndifferent perturbations to the hidden representations for different models,\nwhich can effectively promote the model diversity. Meanwhile, we apply a\nprediction consistency regularizer across the perturbed models to control the\nvariance due to the model diversity. Our experiments using large language\nmodels demonstrate that CAMERO significantly improves the generalization\nperformance of the ensemble model. Specifically, CAMERO outperforms the\nstandard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a\nsignificantly smaller model size (114.2M vs. 880.6M).",
        "pdf_link": "https://arxiv.org/pdf/2204.06625v2.pdf"
    },
    {
        "title": "Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems",
        "authors": [
            "Hong Liu",
            "Yucheng Cai",
            "Zhijian Ou",
            "Yi Huang",
            "Junlan Feng"
        ],
        "published": "2022-04-13T15:21:34Z",
        "summary": "Recently, Transformer based pretrained language models (PLMs), such as GPT2\nand T5, have been leveraged to build generative task-oriented dialog (TOD)\nsystems. A drawback of existing PLM-based models is their non-Markov\narchitectures across turns, i.e., the whole history is used as the conditioning\ninput at each turn. First, this brings inefficiencies in memory and\ncomputation. Furthermore, using the whole history increases model complexity\nand may hurt the training efficiency, especially when facing small amounts of\nlabeled training data (the low-resource setting). In this paper, motivated by\nthe observation that dialog states could be viewed as Markov states, we propose\nto build Markovian Generative Architectures (MGA) over PLM backbones for\nefficient TOD systems. Experiments on MultiWOZ2.1 show that in the\nrich-resource setting, the proposed Markov models reduce memory and time costs\nwithout performance degradation; in the low-resource setting, the training\nefficiency of the Markov models is more significant.",
        "pdf_link": "https://arxiv.org/pdf/2204.06452v2.pdf"
    },
    {
        "title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao"
        ],
        "published": "2022-04-13T10:32:03Z",
        "summary": "In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nour experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.",
        "pdf_link": "https://arxiv.org/pdf/2204.06283v2.pdf"
    },
    {
        "title": "Impossible Triangle: What's Next for Pre-trained Language Models?",
        "authors": [
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2022-04-13T01:28:18Z",
        "summary": "Recent development of large-scale pre-trained language models (PLM) have\nsignificantly improved the capability of models in various NLP tasks, in terms\nof performance after task-specific fine-tuning and zero-shot / few-shot\nlearning. However, many of such models come with a dauntingly huge size that\nfew institutions can afford to pre-train, fine-tune or even deploy, while\nmoderate-sized models usually lack strong generalized few-shot learning\ncapabilities. In this paper, we first elaborate the current obstacles of using\nPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)\nstate-of-the-art few-shot learning capability, and 3) state-of-the-art\nfine-tuning capability. We argue that all existing PLM models lack one or more\nproperties from the Impossible Triangle. To remedy these missing properties of\nPLMs, various techniques have been proposed, such as knowledge distillation,\ndata augmentation and prompt learning, which inevitably brings additional work\nto the application of PLMs in real scenarios. We then offer insights into\nfuture research directions of PLMs to achieve the Impossible Triangle, and\nbreak down the task into several key phases.",
        "pdf_link": "https://arxiv.org/pdf/2204.06130v2.pdf"
    },
    {
        "title": "MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages",
        "authors": [
            "Gokul Karthik Kumar",
            "Abhishek Singh Gehlot",
            "Sahal Shaji Mullappilly",
            "Karthik Nandakumar"
        ],
        "published": "2022-04-12T13:52:54Z",
        "summary": "Accuracy of English-language Question Answering (QA) systems has improved\nsignificantly in recent years with the advent of Transformer-based models\n(e.g., BERT). These models are pre-trained in a self-supervised fashion with a\nlarge English text corpus and further fine-tuned with a massive English QA\ndataset (e.g., SQuAD). However, QA datasets on such a scale are not available\nfor most of the other languages. Multi-lingual BERT-based models (mBERT) are\noften used to transfer knowledge from high-resource languages to low-resource\nlanguages. Since these models are pre-trained with huge text corpora containing\nmultiple languages, they typically learn language-agnostic embeddings for\ntokens from different languages. However, directly training an mBERT-based QA\nsystem for low-resource languages is challenging due to the paucity of training\ndata. In this work, we augment the QA samples of the target language using\ntranslation and transliteration into other languages and use the augmented data\nto fine-tune an mBERT-based QA model, which is already pre-trained in English.\nExperiments on the Google ChAII dataset show that fine-tuning the mBERT model\nwith translations from the same language family boosts the question-answering\nperformance, whereas the performance degrades in the case of cross-language\nfamilies. We further show that introducing a contrastive loss between the\ntranslated question-context feature pairs during the fine-tuning process,\nprevents such degradation with cross-lingual family translations and leads to\nmarginal improvement. The code for this work is available at\nhttps://github.com/gokulkarthik/mucot.",
        "pdf_link": "https://arxiv.org/pdf/2204.05814v1.pdf"
    },
    {
        "title": "Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change",
        "authors": [
            "Mario Giulianelli",
            "Andrey Kutuzov",
            "Lidia Pivovarova"
        ],
        "published": "2022-04-12T11:20:42Z",
        "summary": "Morphological and syntactic changes in word usage (as captured, e.g., by\ngrammatical profiles) have been shown to be good predictors of a word's meaning\nchange. In this work, we explore whether large pre-trained contextualised\nlanguage models, a common tool for lexical semantic change detection, are\nsensitive to such morphosyntactic changes. To this end, we first compare the\nperformance of grammatical profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, covering 7 languages, and then combine\nthe two approaches in ensembles to assess their complementarity. Our results\nshow that ensembling grammatical profiles with XLM-R improves semantic change\ndetection performance for most datasets and languages. This indicates that\nlanguage models do not fully cover the fine-grained morphological and syntactic\nsignals that are explicitly represented in grammatical profiles.\n  An interesting exception are the test sets where the time spans under\nanalysis are much longer than the time gap between them (for example,\ncentury-long spans with a one-year gap between them). Morphosyntactic change is\nslow so grammatical profiles do not detect in such cases. In contrast, language\nmodels, thanks to their access to lexical information, are able to detect fast\ntopical changes.",
        "pdf_link": "https://arxiv.org/pdf/2204.05717v1.pdf"
    },
    {
        "title": "Uniform Complexity for Text Generation",
        "authors": [
            "Joseph Marvin Imperial",
            "Harish Tayyar Madabushi"
        ],
        "published": "2022-04-11T15:19:47Z",
        "summary": "Large language models (LLMs) have shown promising results in a wide array of\ngenerative NLP tasks, such as summarization and machine translation. In the\ncontext of narrative generation, however, existing models still do not capture\nfactors that contribute to producing consistent text. For instance, it is\nlogical that a piece of text or a story should be uniformly readable throughout\nand that this form of complexity should be controllable. As such, if the\ncomplexity of an input text prompt is rated first-grade reading level in the\nFlesch Reading Ease test, then the generated text continuing the plot should\nalso be within this range of complexity. With this in mind, we introduce\nUniform Complexity for Text Generation (UCTG), a new benchmark test which\nraises the challenge of making generative models observe uniform linguistic\nproperties with respect to prompts. We experiment with over 150+ linguistically\nand cognitively motivated features for evaluating text complexity in humans and\ngenerative models. From our results, we find that models such as GPT-2 struggle\nto preserve the complexity of input prompts used in its generations, even if\nfinetuned with professionally written texts.",
        "pdf_link": "https://arxiv.org/pdf/2204.05185v3.pdf"
    },
    {
        "title": "Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts",
        "authors": [
            "Saadullah Amin",
            "Noon Pokaratsiri Goldstein",
            "Morgan Kelly Wixted",
            "Alejandro García-Rudolph",
            "Catalina Martínez-Costa",
            "Günter Neumann"
        ],
        "published": "2022-04-10T21:46:52Z",
        "summary": "Despite the advances in digital healthcare systems offering curated\nstructured knowledge, much of the critical information still lies in large\nvolumes of unlabeled and unstructured clinical texts. These texts, which often\ncontain protected health information (PHI), are exposed to information\nextraction tools for downstream applications, risking patient identification.\nExisting works in de-identification rely on using large-scale annotated corpora\nin English, which often are not suitable in real-world multilingual settings.\nPre-trained language models (LM) have shown great potential for cross-lingual\ntransfer in low-resource settings. In this work, we empirically show the\nfew-shot cross-lingual transfer property of LMs for named entity recognition\n(NER) and apply it to solve a low-resource and real-world challenge of\ncode-mixed (Spanish-Catalan) clinical notes de-identification in the stroke\ndomain. We annotate a gold evaluation dataset to assess few-shot setting\nperformance where we only use a few hundred labeled examples for training. Our\nmodel improves the zero-shot F1-score from 73.7% to 91.2% on the gold\nevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)\nfrom the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual\ntarget corpus. When generalized to an out-of-sample test set, the best model\nachieves a human-evaluation F1-score of 97.2%.",
        "pdf_link": "https://arxiv.org/pdf/2204.04775v1.pdf"
    },
    {
        "title": "Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model",
        "authors": [
            "Usman Naseem",
            "Byoung Chan Lee",
            "Matloob Khushi",
            "Jinman Kim",
            "Adam G. Dunn"
        ],
        "published": "2022-04-09T18:01:18Z",
        "summary": "A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.04521v1.pdf"
    },
    {
        "title": "Contextual Representation Learning beyond Masked Language Modeling",
        "authors": [
            "Zhiyi Fu",
            "Wangchunshu Zhou",
            "Jingjing Xu",
            "Hao Zhou",
            "Lei Li"
        ],
        "published": "2022-04-08T16:18:06Z",
        "summary": "How do masked language models (MLMs) such as BERT learn contextual\nrepresentations? In this work, we analyze the learning dynamics of MLMs. We\nfind that MLMs adopt sampled embeddings as anchors to estimate and inject\ncontextual semantics to representations, which limits the efficiency and\neffectiveness of MLMs. To address these issues, we propose TACO, a simple yet\neffective representation learning approach to directly model global semantics.\nTACO extracts and aligns contextual semantics hidden in contextualized\nrepresentations to encourage models to attend global semantics when generating\ncontextualized representations. Experiments on the GLUE benchmark show that\nTACO achieves up to 5x speedup and up to 1.2 points average improvement over\nexisting MLMs. The code is available at https://github.com/FUZHIYI/TACO.",
        "pdf_link": "https://arxiv.org/pdf/2204.04163v1.pdf"
    },
    {
        "title": "Fair and Argumentative Language Modeling for Computational Argumentation",
        "authors": [
            "Carolin Holtermann",
            "Anne Lauscher",
            "Simone Paolo Ponzetto"
        ],
        "published": "2022-04-08T12:23:46Z",
        "summary": "Although much work in NLP has focused on measuring and mitigating\nstereotypical bias in semantic spaces, research addressing bias in\ncomputational argumentation is still in its infancy. In this paper, we address\nthis research gap and conduct a thorough investigation of bias in argumentative\nlanguage models. To this end, we introduce ABBA, a novel resource for bias\nmeasurement specifically tailored to argumentation. We employ our resource to\nassess the effect of argumentative fine-tuning and debiasing on the intrinsic\nbias found in transformer-based language models using a lightweight\nadapter-based approach that is more sustainable and parameter-efficient than\nfull fine-tuning. Finally, we analyze the potential impact of language model\ndebiasing on the performance in argument quality prediction, a downstream task\nof computational argumentation. Our results show that we are able to\nsuccessfully and sustainably remove bias in general and argumentative language\nmodels while preserving (and sometimes improving) model performance in\ndownstream tasks. We make all experimental code and data available at\nhttps://github.com/umanlp/FairArgumentativeLM.",
        "pdf_link": "https://arxiv.org/pdf/2204.04026v1.pdf"
    },
    {
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yutao Xie",
            "Sheng Yu"
        ],
        "published": "2022-04-08T08:07:42Z",
        "summary": "Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.03905v2.pdf"
    },
    {
        "title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
        "authors": [
            "Wanyu Du",
            "Zae Myung Kim",
            "Vipul Raheja",
            "Dhruv Kumar",
            "Dongyeop Kang"
        ],
        "published": "2022-04-07T18:33:10Z",
        "summary": "Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.",
        "pdf_link": "https://arxiv.org/pdf/2204.03685v2.pdf"
    },
    {
        "title": "Testing the limits of natural language models for predicting human language judgments",
        "authors": [
            "Tal Golan",
            "Matthew Siegelman",
            "Nikolaus Kriegeskorte",
            "Christopher Baldassano"
        ],
        "published": "2022-04-07T17:12:57Z",
        "summary": "Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.",
        "pdf_link": "https://arxiv.org/pdf/2204.03592v3.pdf"
    },
    {
        "title": "BERTuit: Understanding Spanish language in Twitter through a native transformer",
        "authors": [
            "Javier Huertas-Tato",
            "Alejandro Martin",
            "David Camacho"
        ],
        "published": "2022-04-07T14:28:51Z",
        "summary": "The appearance of complex attention-based language models such as BERT,\nRoberta or GPT-3 has allowed to address highly complex tasks in a plethora of\nscenarios. However, when applied to specific domains, these models encounter\nconsiderable difficulties. This is the case of Social Networks such as Twitter,\nan ever-changing stream of information written with informal and complex\nlanguage, where each message requires careful evaluation to be understood even\nby humans given the important role that context plays. Addressing tasks in this\ndomain through Natural Language Processing involves severe challenges. When\npowerful state-of-the-art multilingual language models are applied to this\nscenario, language specific nuances use to get lost in translation. To face\nthese challenges we present \\textbf{BERTuit}, the larger transformer proposed\nso far for Spanish language, pre-trained on a massive dataset of 230M Spanish\ntweets using RoBERTa optimization. Our motivation is to provide a powerful\nresource to better understand Spanish Twitter and to be used on applications\nfocused on this social network, with special emphasis on solutions devoted to\ntackle the spreading of misinformation in this platform. BERTuit is evaluated\non several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very\ncompetitive multilingual transformers. The utility of our approach is shown\nwith applications, in this case: a zero-shot methodology to visualize groups of\nhoaxes and profiling authors spreading disinformation.\n  Misinformation spreads wildly on platforms such as Twitter in languages other\nthan English, meaning performance of transformers may suffer when transferred\noutside English speaking communities.",
        "pdf_link": "https://arxiv.org/pdf/2204.03465v2.pdf"
    },
    {
        "title": "Knowledge Infused Decoding",
        "authors": [
            "Ruibo Liu",
            "Guoqing Zheng",
            "Shashank Gupta",
            "Radhika Gaonkar",
            "Chongyang Gao",
            "Soroush Vosoughi",
            "Milad Shokouhi",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2022-04-06T20:58:32Z",
        "summary": "Pre-trained language models (LMs) have been shown to memorize a substantial\namount of knowledge from the pre-training corpora; however, they are still\nlimited in recalling factually correct knowledge given a certain context.\nHence, they tend to suffer from counterfactual or hallucinatory generation when\nused in knowledge-intensive natural language generation (NLG) tasks. Recent\nremedies to this problem focus on modifying either the pre-training or task\nfine-tuning objectives to incorporate knowledge, which normally require\nadditional costly training or architecture modification of LMs for practical\napplications. We present Knowledge Infused Decoding (KID) -- a novel decoding\nalgorithm for generative LMs, which dynamically infuses external knowledge into\neach step of the LM decoding. Specifically, we maintain a local knowledge\nmemory based on the current context, interacting with a dynamically created\nexternal knowledge trie, and continuously update the local memory as a\nknowledge-aware constraint to guide decoding via reinforcement learning. On six\ndiverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)\narmed with KID outperform many task-optimized state-of-the-art models, and show\nparticularly strong performance in few-shot scenarios over seven related\nknowledge-infusion techniques. Human evaluation confirms KID's ability to\ngenerate more relevant and factual language for the input context when compared\nwith multiple baselines. Finally, KID also alleviates exposure bias and\nprovides stable generation quality when generating longer sequences. Code for\nKID is available at https://github.com/microsoft/KID.",
        "pdf_link": "https://arxiv.org/pdf/2204.03084v1.pdf"
    },
    {
        "title": "DAGAM: Data Augmentation with Generation And Modification",
        "authors": [
            "Byeong-Cheol Jo",
            "Tak-Sung Heo",
            "Yeongjoon Park",
            "Yongmin Yoo",
            "Won Ik Cho",
            "Kyungsun Kim"
        ],
        "published": "2022-04-06T07:20:45Z",
        "summary": "Text classification is a representative downstream task of natural language\nprocessing, and has exhibited excellent performance since the advent of\npre-trained language models based on Transformer architecture. However, in\npre-trained language models, under-fitting often occurs due to the size of the\nmodel being very large compared to the amount of available training data. Along\nwith significant importance of data collection in modern machine learning\nparadigm, studies have been actively conducted for natural language data\naugmentation. In light of this, we introduce three data augmentation schemes\nthat help reduce underfitting problems of large-scale language models.\nPrimarily we use a generation model for data augmentation, which is defined as\nData Augmentation with Generation (DAG). Next, we augment data using text\nmodification techniques such as corruption and word order change (Data\nAugmentation with Modification, DAM). Finally, we propose Data Augmentation\nwith Generation And Modification (DAGAM), which combines DAG and DAM techniques\nfor a boosted performance. We conduct data augmentation for six benchmark\ndatasets of text classification task, and verify the usefulness of DAG, DAM,\nand DAGAM through BERT-based fine-tuning and evaluation, deriving better\nresults compared to the performance with original datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.02633v1.pdf"
    },
    {
        "title": "LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition",
        "authors": [
            "Rishab Sharma",
            "Fuxiang Chen",
            "Fatemeh Fard"
        ],
        "published": "2022-04-05T20:53:06Z",
        "summary": "Code comment generation is the task of generating a high-level natural\nlanguage description for a given code method or function. Although researchers\nhave been studying multiple ways to generate code comments automatically,\nprevious work mainly considers representing a code token in its entirety\nsemantics form only (e.g., a language model is used to learn the semantics of a\ncode token), and additional code properties such as the tree structure of a\ncode are included as an auxiliary input to the model. There are two\nlimitations: 1) Learning the code token in its entirety form may not be able to\ncapture information succinctly in source code, and 2) The code token does not\ncontain additional syntactic information, inherently important in programming\nlanguages.\n  In this paper, we present LAnguage Model and Named Entity Recognition\n(LAMNER), a code comment generator capable of encoding code constructs\neffectively and capturing the structural property of a code token. A\ncharacter-level language model is used to learn the semantic representation to\nencode a code token. For the structural property of a token, a Named Entity\nRecognition model is trained to learn the different types of code tokens. These\nrepresentations are then fed into an encoder-decoder architecture to generate\ncode comments. We evaluate the generated comments from LAMNER and other\nbaselines on a popular Java dataset with four commonly used metrics. Our\nresults show that LAMNER is effective and improves over the best baseline model\nin BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,\n18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,\nwe fused LAMNER's code representation with the baseline models, and the fused\nmodels consistently showed improvement over the non-fused models. The human\nevaluation further shows that LAMNER produces high-quality code comments.",
        "pdf_link": "https://arxiv.org/pdf/2204.09654v1.pdf"
    },
    {
        "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
        "authors": [
            "Robert Litschko",
            "Ivan Vulić",
            "Goran Glavaš"
        ],
        "published": "2022-04-05T15:44:27Z",
        "summary": "State-of-the-art neural (re)rankers are notoriously data-hungry which --\ngiven the lack of large-scale training data in languages other than English --\nmakes them rarely used in multilingual and cross-lingual retrieval settings.\nCurrent approaches therefore commonly transfer rankers trained on English data\nto other languages and cross-lingual setups by means of multilingual encoders:\nthey fine-tune all parameters of pretrained massively multilingual Transformers\n(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy\nthem in the target language(s). In this work, we show that two\nparameter-efficient approaches to cross-lingual transfer, namely Sparse\nFine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more\neffective zero-shot transfer to multilingual and cross-lingual retrieval tasks.\nWe first train language adapters (or SFTMs) via Masked Language Modelling and\nthen train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping\nall other parameters fixed. At inference, this modular design allows us to\ncompose the ranker by applying the (re)ranking adapter (or SFTM) trained with\nsource language data together with the language adapter (or SFTM) of a target\nlanguage. We carry out a large scale evaluation on the CLEF-2003 and HC4\nbenchmarks and additionally, as another contribution, extend the former with\nqueries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed\nparameter-efficient methods outperform standard zero-shot transfer with full\nMMT fine-tuning, while being more modular and reducing training times. The\ngains are particularly pronounced for low-resource languages, where our\napproaches also substantially outperform the competitive machine\ntranslation-based rankers.",
        "pdf_link": "https://arxiv.org/pdf/2204.02292v2.pdf"
    },
    {
        "title": "Data Augmentation for Intent Classification with Off-the-shelf Large Language Models",
        "authors": [
            "Gaurav Sahu",
            "Pau Rodriguez",
            "Issam H. Laradji",
            "Parmida Atighehchian",
            "David Vazquez",
            "Dzmitry Bahdanau"
        ],
        "published": "2022-04-05T03:29:26Z",
        "summary": "Data augmentation is a widely employed technique to alleviate the problem of\ndata scarcity. In this work, we propose a prompting-based approach to generate\nlabelled training data for intent classification with off-the-shelf language\nmodels (LMs) such as GPT-3. An advantage of this method is that no\ntask-specific LM-fine-tuning for data generation is required; hence the method\nrequires no hyper-parameter tuning and is applicable even when the available\ntraining data is very scarce. We evaluate the proposed method in a few-shot\nsetting on four diverse intent classification tasks. We find that GPT-generated\ndata significantly boosts the performance of intent classifiers when intents in\nconsideration are sufficiently distinct from each other. In tasks with\nsemantically close intents, we observe that the generated data is less helpful.\nOur analysis shows that this is because GPT often generates utterances that\nbelong to a closely-related intent instead of the desired one. We present\npreliminary evidence that a prompting-based GPT classifier could be helpful in\nfiltering the generated data to enhance its quality.",
        "pdf_link": "https://arxiv.org/pdf/2204.01959v1.pdf"
    },
    {
        "title": "Applying Automatic Text Summarization for Fake News Detection",
        "authors": [
            "Philipp Hartl",
            "Udo Kruschwitz"
        ],
        "published": "2022-04-04T21:00:55Z",
        "summary": "The distribution of fake news is not a new but a rapidly growing problem. The\nshift to news consumption via social media has been one of the drivers for the\nspread of misleading and deliberately wrong information, as in addition to it\nof easy use there is rarely any veracity monitoring. Due to the harmful effects\nof such fake news on society, the detection of these has become increasingly\nimportant. We present an approach to the problem that combines the power of\ntransformer-based language models while simultaneously addressing one of their\ninherent problems. Our framework, CMTR-BERT, combines multiple text\nrepresentations, with the goal of circumventing sequential limits and related\nloss of information the underlying transformer architecture typically suffers\nfrom. Additionally, it enables the incorporation of contextual information.\nExtensive experiments on two very different, publicly available datasets\ndemonstrates that our approach is able to set new state-of-the-art performance\nbenchmarks. Apart from the benefit of using automatic text summarization\ntechniques we also find that the incorporation of contextual information\ncontributes to performance gains.",
        "pdf_link": "https://arxiv.org/pdf/2204.01841v1.pdf"
    },
    {
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
        "authors": [
            "Michael Ahn",
            "Anthony Brohan",
            "Noah Brown",
            "Yevgen Chebotar",
            "Omar Cortes",
            "Byron David",
            "Chelsea Finn",
            "Chuyuan Fu",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Alex Herzog",
            "Daniel Ho",
            "Jasmine Hsu",
            "Julian Ibarz",
            "Brian Ichter",
            "Alex Irpan",
            "Eric Jang",
            "Rosario Jauregui Ruano",
            "Kyle Jeffrey",
            "Sally Jesmonth",
            "Nikhil J Joshi",
            "Ryan Julian",
            "Dmitry Kalashnikov",
            "Yuheng Kuang",
            "Kuang-Huei Lee",
            "Sergey Levine",
            "Yao Lu",
            "Linda Luu",
            "Carolina Parada",
            "Peter Pastor",
            "Jornell Quiambao",
            "Kanishka Rao",
            "Jarek Rettinghouse",
            "Diego Reyes",
            "Pierre Sermanet",
            "Nicolas Sievers",
            "Clayton Tan",
            "Alexander Toshev",
            "Vincent Vanhoucke",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Sichun Xu",
            "Mengyuan Yan",
            "Andy Zeng"
        ],
        "published": "2022-04-04T17:57:11Z",
        "summary": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2204.01691v2.pdf"
    },
    {
        "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
        "authors": [
            "Pei Ke",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022-04-02T13:42:49Z",
        "summary": "Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.",
        "pdf_link": "https://arxiv.org/pdf/2204.00862v2.pdf"
    },
    {
        "title": "CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos",
        "authors": [
            "Shengyao Zhuang",
            "Guido Zuccon"
        ],
        "published": "2022-04-01T23:02:50Z",
        "summary": "Current dense retrievers are not robust to out-of-domain and outlier queries,\ni.e. their effectiveness on these queries is much poorer than what one would\nexpect. In this paper, we consider a specific instance of such queries: queries\nthat contain typos. We show that a small character level perturbation in\nqueries (as caused by typos) highly impacts the effectiveness of dense\nretrievers. We then demonstrate that the root cause of this resides in the\ninput tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\nqueries with typos, while still being as performant as previous methods on\nqueries without typos. For this, we use CharacterBERT as the backbone encoder\nand an efficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers. Code, experimental results and dataset\nare made available at https://github.com/ielab/CharacterBERT-DR.",
        "pdf_link": "https://arxiv.org/pdf/2204.00716v2.pdf"
    },
    {
        "title": "Evaluation of Fake News Detection with Knowledge-Enhanced Language Models",
        "authors": [
            "Chenxi Whitehouse",
            "Tillman Weyde",
            "Pranava Madhyastha",
            "Nikos Komninos"
        ],
        "published": "2022-04-01T14:14:46Z",
        "summary": "Recent advances in fake news detection have exploited the success of\nlarge-scale pre-trained language models (PLMs). The predominant\nstate-of-the-art approaches are based on fine-tuning PLMs on labelled fake news\ndatasets. However, large-scale PLMs are generally not trained on structured\nfactual data and hence may not possess priors that are grounded in factually\naccurate knowledge. The use of existing knowledge bases (KBs) with rich\nhuman-curated factual information has thus the potential to make fake news\ndetection more effective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detection. We study several\nstate-of-the-art approaches for knowledge integration, mostly using Wikidata as\nKB, on two popular fake news datasets - LIAR, a politics-based dataset, and\nCOVID-19, a dataset of messages posted on social media relating to the COVID-19\npandemic. Our experiments show that knowledge-enhanced models can significantly\nimprove fake news detection on LIAR where the KB is relevant and up-to-date.\nThe mixed results on COVID-19 highlight the reliance on stylistic features and\nthe importance of domain-specific and current KBs.",
        "pdf_link": "https://arxiv.org/pdf/2204.00458v2.pdf"
    },
    {
        "title": "Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?",
        "authors": [
            "Marina Sedinkina",
            "Martin Schmitt",
            "Hinrich Schütze"
        ],
        "published": "2022-03-31T09:59:08Z",
        "summary": "The practical success of much of NLP depends on the availability of training\ndata. However, in real-world scenarios, training data is often scarce, not\nleast because many application domains are restricted and specific. In this\nwork, we compare different methods to handle this problem and provide\nguidelines for building NLP applications when there is only a small amount of\nlabeled training data available for a specific domain. While transfer learning\nwith pre-trained language models outperforms other methods across tasks,\nalternatives do not perform much worse while requiring much less computational\neffort, thus significantly reducing monetary and environmental cost. We examine\nthe performance tradeoffs of several such alternatives, including models that\ncan be trained up to 175K times faster and do not require a single GPU.",
        "pdf_link": "https://arxiv.org/pdf/2203.16926v1.pdf"
    },
    {
        "title": "Leveraging pre-trained language models for conversational information seeking from text",
        "authors": [
            "Patrizio Bellan",
            "Mauro Dragoni",
            "Chiara Ghidini"
        ],
        "published": "2022-03-31T09:00:46Z",
        "summary": "Recent advances in Natural Language Processing, and in particular on the\nconstruction of very large pre-trained language representation models, is\nopening up new perspectives on the construction of conversational information\nseeking (CIS) systems. In this paper we investigate the usage of in-context\nlearning and pre-trained language representation models to address the problem\nof information extraction from process description documents, in an incremental\nquestion and answering oriented fashion. In particular we investigate the usage\nof the native GPT-3 (Generative Pre-trained Transformer 3) model, together with\ntwo in-context learning customizations that inject conceptual definitions and a\nlimited number of samples in a few shot-learning fashion. The results highlight\nthe potential of the approach and the usefulness of the in-context learning\ncustomizations, which can substantially contribute to address the \"training\ndata challenge\" of deep learning based NLP techniques the BPM field. It also\nhighlight the challenge posed by control flow relations for which further\ntraining needs to be devised.",
        "pdf_link": "https://arxiv.org/pdf/2204.03542v1.pdf"
    },
    {
        "title": "Reproducibility Issues for BERT-based Evaluation Metrics",
        "authors": [
            "Yanran Chen",
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022-03-30T20:35:37Z",
        "summary": "Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).",
        "pdf_link": "https://arxiv.org/pdf/2204.00004v3.pdf"
    },
    {
        "title": "Training Compute-Optimal Large Language Models",
        "authors": [
            "Jordan Hoffmann",
            "Sebastian Borgeaud",
            "Arthur Mensch",
            "Elena Buchatskaya",
            "Trevor Cai",
            "Eliza Rutherford",
            "Diego de Las Casas",
            "Lisa Anne Hendricks",
            "Johannes Welbl",
            "Aidan Clark",
            "Tom Hennigan",
            "Eric Noland",
            "Katie Millican",
            "George van den Driessche",
            "Bogdan Damoc",
            "Aurelia Guy",
            "Simon Osindero",
            "Karen Simonyan",
            "Erich Elsen",
            "Jack W. Rae",
            "Oriol Vinyals",
            "Laurent Sifre"
        ],
        "published": "2022-03-29T13:38:03Z",
        "summary": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "pdf_link": "https://arxiv.org/pdf/2203.15556v1.pdf"
    },
    {
        "title": "GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models",
        "authors": [
            "Changye Li",
            "David Knopman",
            "Weizhe Xu",
            "Trevor Cohen",
            "Serguei Pakhomov"
        ],
        "published": "2022-03-25T00:25:42Z",
        "summary": "Deep learning (DL) techniques involving fine-tuning large numbers of model\nparameters have delivered impressive performance on the task of discriminating\nbetween language produced by cognitively healthy individuals, and those with\nAlzheimer's disease (AD). However, questions remain about their ability to\ngeneralize beyond the small reference sets that are publicly available for\nresearch. As an alternative to fitting model parameters directly, we propose a\nnovel method by which a Transformer DL model (GPT-2) pre-trained on general\nEnglish text is paired with an artificially degraded version of itself (GPT-D),\nto compute the ratio between these two models' \\textit{perplexities} on\nlanguage from cognitively healthy and impaired individuals. This technique\napproaches state-of-the-art performance on text data from a widely used \"Cookie\nTheft\" picture description task, and unlike established alternatives also\ngeneralizes well to spontaneous conversations. Furthermore, GPT-D generates\ntext with characteristics known to be associated with AD, demonstrating the\ninduction of dementia-related linguistic anomalies. Our study is a step toward\nbetter understanding of the relationships between the inner workings of\ngenerative neural language models, the language that they produce, and the\ndeleterious effects of dementia on human speech and language characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2203.13397v1.pdf"
    },
    {
        "title": "Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",
        "authors": [
            "Hanjie Chen",
            "Yangfeng Ji"
        ],
        "published": "2022-03-23T20:04:14Z",
        "summary": "Neural language models show vulnerability to adversarial examples which are\nsemantically similar to their original counterparts with a few words replaced\nby their synonyms. A common way to improve model robustness is adversarial\ntraining which follows two steps-collecting adversarial examples by attacking a\ntarget model, and fine-tuning the model on the augmented dataset with these\nadversarial examples. The objective of traditional adversarial training is to\nmake a model produce the same correct predictions on an original/adversarial\nexample pair. However, the consistency between model decision-makings on two\nsimilar texts is ignored. We argue that a robust model should behave\nconsistently on original/adversarial example pairs, that is making the same\npredictions (what) based on the same reasons (how) which can be reflected by\nconsistent interpretations. In this work, we propose a novel feature-level\nadversarial training method named FLAT. FLAT aims at improving model robustness\nin terms of both predictions and interpretations. FLAT incorporates variational\nword masks in neural networks to learn global word importance and play as a\nbottleneck teaching the model to make predictions based on important words.\nFLAT explicitly shoots at the vulnerability problem caused by the mismatch\nbetween model understandings on the replaced words and their synonyms in\noriginal/adversarial example pairs by regularizing the corresponding global\nword importance scores. Experiments show the effectiveness of FLAT in improving\nthe robustness with respect to both predictions and interpretations of four\nneural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks\non four text classification tasks. The models trained via FLAT also show better\nrobustness than baseline models on unforeseen adversarial examples across\ndifferent attacks.",
        "pdf_link": "https://arxiv.org/pdf/2203.12709v1.pdf"
    },
    {
        "title": "BERT-ASC: Implicit Aspect Representation Learning through Auxiliary-Sentence Construction for Sentiment Analysis",
        "authors": [
            "Murtadha Ahmed",
            "Shengfeng Pan",
            "Jianlin Su",
            "Xinxin Cao",
            "Wenze Zhang",
            "Bo Wen",
            "Yunfeng Liu"
        ],
        "published": "2022-03-22T13:12:27Z",
        "summary": "Aspect-based sentiment analysis (ABSA) task aim at associating a piece of\ntext with a set of aspects and meanwhile infer their respective sentimental\npolarities. The state-of-the-art approaches are built upon fine-tuning of\nvarious pre-trained language models. They commonly attempt to learn\naspect-specific representation from the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples are\navailable. However, high-quality labeled examples may not be readily available\nin real-world scenarios. In this paper, we propose to jointly address aspect\ncategorization and aspect-based sentiment subtasks in a unified framework.\nSpecifically, we first introduce a simple but effective mechanism to construct\nan auxiliary-sentence for the implicit aspect based on the semantic information\nin the corpus. Then, we encourage BERT to learn the aspect-specific\nrepresentation in response to the automatically constructed auxiliary-sentence\ninstead of the aspect itself. Finally, we empirically evaluate the performance\nof the proposed solution by a comparative study on real benchmark datasets for\nboth ABSA and Targeted-ABSA tasks. Our extensive experiments show that it\nconsistently achieves state-of-the-art performance in terms of aspect\ncategorization and aspect-based sentiment across all datasets and the\nimprovement margins are considerable. The code of BERT-ASC is available in\nGitHub: https://github.com/amurtadha/BERT-ASC.",
        "pdf_link": "https://arxiv.org/pdf/2203.11702v2.pdf"
    },
    {
        "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
        "authors": [
            "Zheng Li",
            "Zijian Wang",
            "Ming Tan",
            "Ramesh Nallapati",
            "Parminder Bhatia",
            "Andrew Arnold",
            "Bing Xiang",
            "Dan Roth"
        ],
        "published": "2022-03-21T18:04:25Z",
        "summary": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve\nstate-of-the-art performance on many generative NLP tasks. However, such models\npose a great challenge in resource-constrained scenarios owing to their large\nmemory requirements and high latency. To alleviate this issue, we propose to\njointly distill and quantize the model, where knowledge is transferred from the\nfull-precision teacher model to the quantized and distilled low-precision\nstudent model. Empirical analyses show that, despite the challenging nature of\ngenerative tasks, we were able to achieve a 16.5x model footprint compression\nratio with little performance drop relative to the full-precision counterparts\non multiple summarization and QA datasets. We further pushed the limit of\ncompression ratio to 27.7x and presented the performance-efficiency trade-off\nfor generative tasks using pre-trained models. To the best of our knowledge,\nthis is the first work aiming to effectively distill and quantize\nsequence-to-sequence pre-trained models for language generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.11239v1.pdf"
    },
    {
        "title": "Teaching language models to support answers with verified quotes",
        "authors": [
            "Jacob Menick",
            "Maja Trebacz",
            "Vladimir Mikulik",
            "John Aslanides",
            "Francis Song",
            "Martin Chadwick",
            "Mia Glaese",
            "Susannah Young",
            "Lucy Campbell-Gillingham",
            "Geoffrey Irving",
            "Nat McAleese"
        ],
        "published": "2022-03-21T17:26:29Z",
        "summary": "Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.",
        "pdf_link": "https://arxiv.org/pdf/2203.11147v1.pdf"
    },
    {
        "title": "Mitigating Gender Bias in Machine Translation through Adversarial Learning",
        "authors": [
            "Eve Fleisig",
            "Christiane Fellbaum"
        ],
        "published": "2022-03-20T23:35:09Z",
        "summary": "Machine translation and other NLP systems often contain significant biases\nregarding sensitive attributes, such as gender or race, that worsen system\nperformance and perpetuate harmful stereotypes. Recent preliminary research\nsuggests that adversarial learning can be used as part of a model-agnostic bias\nmitigation method that requires no data modifications. However, adapting this\nstrategy for machine translation and other modern NLP domains requires (1)\nrestructuring training objectives in the context of fine-tuning pretrained\nlarge language models and (2) developing measures for gender or other protected\nvariables for tasks in which these attributes must be deduced from the data\nitself.\n  We present an adversarial learning framework that addresses these challenges\nto mitigate gender bias in seq2seq machine translation. Our framework improves\nthe disparity in translation quality for sentences with male vs. female\nentities by 86% for English-German translation and 91% for English-French\ntranslation, with minimal effect on translation quality. The results suggest\nthat adversarial learning is a promising technique for mitigating gender bias\nin machine translation.",
        "pdf_link": "https://arxiv.org/pdf/2203.10675v1.pdf"
    },
    {
        "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
        "authors": [
            "Jiayi Wang",
            "Rongzhou Bao",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022-03-19T14:06:46Z",
        "summary": "Recently, the problem of robustness of pre-trained language models (PrLMs)\nhas received increasing research interest. Latest studies on adversarial\nattacks achieve high attack success rates against PrLMs, claiming that PrLMs\nare not robust. However, we find that the adversarial samples that PrLMs fail\nare mostly non-natural and do not appear in reality. We question the validity\nof current evaluation of robustness of PrLMs based on these non-natural\nadversarial samples and propose an anomaly detector to evaluate the robustness\nof PrLMs with more natural adversarial samples. We also investigate two\napplications of the anomaly detector: (1) In data augmentation, we employ the\nanomaly detector to force generating augmented data that are distinguished as\nnon-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply\nthe anomaly detector to a defense framework to enhance the robustness of PrLMs.\nIt can be used to defend all types of attacks and achieves higher accuracy on\nboth adversarial samples and compliant samples than other defense frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2203.11199v1.pdf"
    },
    {
        "title": "Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators",
        "authors": [
            "Keenan Jones",
            "Jason R. C. Nurse",
            "Shujun Li"
        ],
        "published": "2022-03-18T09:19:14Z",
        "summary": "Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.",
        "pdf_link": "https://arxiv.org/pdf/2203.09813v1.pdf"
    },
    {
        "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
        "authors": [
            "Giuseppe Attanasio",
            "Debora Nozza",
            "Dirk Hovy",
            "Elena Baralis"
        ],
        "published": "2022-03-17T09:29:50Z",
        "summary": "Natural Language Processing (NLP) models risk overfitting to specific terms\nin the training data, thereby reducing their performance, fairness, and\ngeneralizability. E.g., neural hate speech detection models are strongly\ninfluenced by identity terms like gay, or women, resulting in false positives,\nsevere unintended bias, and lower performance. Most mitigation techniques use\nlists of identity terms or samples from the target domain during training.\nHowever, this approach requires a-priori knowledge and introduces further bias\nif important terms are neglected. Instead, we propose a knowledge-free\nEntropy-based Attention Regularization (EAR) to discourage overfitting to\ntraining-specific terms. An additional objective function penalizes tokens with\nlow self-attention entropy. We fine-tune BERT via EAR: the resulting model\nmatches or exceeds state-of-the-art performance for hate speech classification\nand bias metrics on three benchmark corpora in English and Italian. EAR also\nreveals overfitting terms, i.e., terms most likely to induce bias, to help\nidentify their effect on the model, task, and predictions.",
        "pdf_link": "https://arxiv.org/pdf/2203.09192v1.pdf"
    },
    {
        "title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT",
        "authors": [
            "Jing Zhao",
            "Yifan Wang",
            "Junwei Bao",
            "Youzheng Wu",
            "Xiaodong He"
        ],
        "published": "2022-03-17T03:33:47Z",
        "summary": "Transformer-based pre-trained models, such as BERT, have shown extraordinary\nsuccess in achieving state-of-the-art results in many natural language\nprocessing applications. However, deploying these models can be prohibitively\ncostly, as the standard self-attention mechanism of the Transformer suffers\nfrom quadratic computational cost in the input sequence length. To confront\nthis, we propose FCA, a fine- and coarse-granularity hybrid self-attention that\nreduces the computation cost through progressively shortening the computational\nsequence length in self-attention. Specifically, FCA conducts an\nattention-based scoring strategy to determine the informativeness of tokens at\neach layer. Then, the informative tokens serve as the fine-granularity\ncomputing units in self-attention and the uninformative tokens are replaced\nwith one or several clusters as the coarse-granularity computing units in\nself-attention. Experiments on GLUE and RACE datasets show that BERT with FCA\nachieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We\nshow that FCA offers a significantly better trade-off between accuracy and\nFLOPs compared to prior methods.",
        "pdf_link": "https://arxiv.org/pdf/2203.09055v1.pdf"
    },
    {
        "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation",
        "authors": [
            "Zihan Liu",
            "Mostofa Patwary",
            "Ryan Prenger",
            "Shrimai Prabhumoye",
            "Wei Ping",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2022-03-16T16:53:43Z",
        "summary": "Existing knowledge-grounded dialogue systems typically use finetuned versions\nof a pretrained language model (LM) and large-scale knowledge bases. These\nmodels typically fail to generalize on topics outside of the knowledge base,\nand require maintaining separate potentially large checkpoints each time\nfinetuning is needed. In this paper, we aim to address these limitations by\nleveraging the inherent knowledge stored in the pretrained LM as well as its\npowerful generation ability. We propose a multi-stage prompting approach to\ngenerate knowledgeable responses from a single pretrained LM. We first prompt\nthe LM to generate knowledge based on the dialogue context. Then, we further\nprompt it to generate responses based on the dialogue context and the\npreviously generated knowledge. Results show that our knowledge generator\noutperforms the state-of-the-art retrieval-based model by 5.8% when combining\nknowledge relevance and correctness. In addition, our multi-stage prompting\noutperforms the finetuning-based dialogue model in terms of response\nknowledgeability and engagement by up to 10% and 5%, respectively. Furthermore,\nwe scale our model up to 530 billion parameters and show that larger LMs\nimprove the generation correctness score by up to 10%, and response relevance,\nknowledgeability and engagement by up to 10%. Our code is available at:\nhttps://github.com/NVIDIA/Megatron-LM.",
        "pdf_link": "https://arxiv.org/pdf/2203.08745v1.pdf"
    },
    {
        "title": "Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again",
        "authors": [
            "Bernal Jiménez Gutiérrez",
            "Nikolas McNeal",
            "Clay Washington",
            "You Chen",
            "Lang Li",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2022-03-16T05:56:08Z",
        "summary": "The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for application\ndomains such as biomedicine, which feature high and diverse demands of language\ntechnologies but also high data annotation costs. In this paper, we present the\nfirst systematic and comprehensive study to compare the few-shot performance of\nGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on\ntwo highly representative biomedical information extraction tasks, named entity\nrecognition and relation extraction. We follow the true few-shot setting to\navoid overestimating models' few-shot performance by model selection over a\nlarge validation set. We also optimize GPT-3's performance with known\ntechniques such as contextual calibration and dynamic in-context example\nretrieval. However, our results show that GPT-3 still significantly\nunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3\nin-context learning also yields smaller gains in accuracy when more training\ndata becomes available. Our in-depth analyses further reveal issues of the\nin-context learning setting that may be detrimental to information extraction\ntasks in general. Given the high cost of experimenting with GPT-3, we hope our\nstudy provides guidance for biomedical researchers and practitioners towards\nmore promising directions such as fine-tuning small PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2203.08410v3.pdf"
    },
    {
        "title": "Data Contamination: From Memorization to Exploitation",
        "authors": [
            "Inbal Magar",
            "Roy Schwartz"
        ],
        "published": "2022-03-15T20:37:16Z",
        "summary": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "pdf_link": "https://arxiv.org/pdf/2203.08242v1.pdf"
    },
    {
        "title": "Representation Learning for Resource-Constrained Keyphrase Generation",
        "authors": [
            "Di Wu",
            "Wasi Uddin Ahmad",
            "Sunipa Dev",
            "Kai-Wei Chang"
        ],
        "published": "2022-03-15T17:48:04Z",
        "summary": "State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with limited\nannotated data. To overcome this challenge, we design a data-oriented approach\nthat first identifies salient information using retrieval-based corpus-level\nstatistics, and then learns a task-specific intermediate representation based\non a pre-trained language model using large-scale unlabeled documents. We\nintroduce salient span recovery and salient span prediction as denoising\ntraining objectives that condense the intra-article and inter-article knowledge\nessential for keyphrase generation. Through experiments on multiple keyphrase\ngeneration benchmarks, we show the effectiveness of the proposed approach for\nfacilitating low-resource keyphrase generation and zero-shot domain adaptation.\nOur method especially benefits the generation of absent keyphrases, approaching\nthe performance of models trained with large training sets.",
        "pdf_link": "https://arxiv.org/pdf/2203.08118v3.pdf"
    },
    {
        "title": "Training a Tokenizer for Free with Private Federated Learning",
        "authors": [
            "Eugene Bagdasaryan",
            "Congzheng Song",
            "Rogier van Dalen",
            "Matt Seigel",
            "Áine Cahill"
        ],
        "published": "2022-03-15T14:29:39Z",
        "summary": "Federated learning with differential privacy, i.e. private federated learning\n(PFL), makes it possible to train models on private data distributed across\nusers' devices without harming privacy. PFL is efficient for models, such as\nneural networks, that have a fixed number of parameters, and thus a\nfixed-dimensional gradient vector. Such models include neural-net language\nmodels, but not tokenizers, the topic of this work. Training a tokenizer\nrequires frequencies of words from an unlimited vocabulary, and existing\nmethods for finding an unlimited vocabulary need a separate privacy budget.\n  A workaround is to train the tokenizer on publicly available data. However,\nin this paper we first show that a tokenizer trained on mismatched data results\nin worse model performance compared to a privacy-violating \"oracle\" tokenizer\nthat accesses user data, with perplexity increasing by 20%. We also show that\nsub-word tokenizers are better suited to the federated context than word-level\nones, since they can encode new words, though with more tokens per word.\n  Second, we propose a novel method to obtain a tokenizer without using any\nadditional privacy budget. During private federated learning of the language\nmodel, we sample from the model, train a new tokenizer on the sampled\nsequences, and update the model embeddings. We then continue private federated\nlearning, and obtain performance within 1% of the \"oracle\" tokenizer. Since\nthis process trains the tokenizer only indirectly on private data, we can use\nthe \"postprocessing guarantee\" of differential privacy and thus use no\nadditional privacy budget.",
        "pdf_link": "https://arxiv.org/pdf/2203.09943v1.pdf"
    },
    {
        "title": "The Ghost in the Machine has an American accent: value conflict in GPT-3",
        "authors": [
            "Rebecca L Johnson",
            "Giada Pistilli",
            "Natalia Menédez-González",
            "Leslye Denisse Dias Duran",
            "Enrico Panai",
            "Julija Kalpokiene",
            "Donald Jay Bertulfo"
        ],
        "published": "2022-03-15T11:06:54Z",
        "summary": "The alignment problem in the context of large language models must consider\nthe plurality of human values in our world. Whilst there are many resonant and\noverlapping values amongst the world's cultures, there are also many\nconflicting, yet equally valid, values. It is important to observe which\ncultural values a model exhibits, particularly when there is a value conflict\nbetween input prompts and generated outputs. We discuss how the co-creation of\nlanguage and cultural value impacts large language models (LLMs). We explore\nthe constitution of the training data for GPT-3 and compare that to the world's\nlanguage and internet access demographics, as well as to reported statistical\nprofiles of dominant values in some Nation-states. We stress tested GPT-3 with\na range of value-rich texts representing several languages and nations;\nincluding some with values orthogonal to dominant US public opinion as reported\nby the World Values Survey. We observed when values embedded in the input text\nwere mutated in the generated outputs and noted when these conflicting values\nwere more aligned with reported dominant US values. Our discussion of these\nresults uses a moral value pluralism (MVP) lens to better understand these\nvalue mutations. Finally, we provide recommendations for how our work may\ncontribute to other current work in the field.",
        "pdf_link": "https://arxiv.org/pdf/2203.07785v1.pdf"
    },
    {
        "title": "Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation",
        "authors": [
            "Xuandong Zhao",
            "Zhiguo Yu",
            "Ming Wu",
            "Lei Li"
        ],
        "published": "2022-03-15T07:05:43Z",
        "summary": "How to learn highly compact yet effective sentence representation?\nPre-trained language models have been effective in many NLP tasks. However,\nthese models are often huge and produce large sentence embeddings. Moreover,\nthere is a big performance gap between large and small models. In this paper,\nwe propose Homomorphic Projective Distillation (HPD) to learn compressed\nsentence embeddings. Our method augments a small Transformer encoder model with\nlearnable projection layers to produce compact representations while mimicking\na large pre-trained language model to retain the sentence representation\nquality. We evaluate our method with different model sizes on both semantic\ntextual similarity (STS) and semantic retrieval (SR) tasks. Experiments show\nthat our method achieves 2.7-4.5 points performance gain on STS tasks compared\nwith previous best representations of the same size. In SR tasks, our method\nimproves retrieval speed (8.2$\\times$) and memory usage (8.0$\\times$) compared\nwith state-of-the-art large models.",
        "pdf_link": "https://arxiv.org/pdf/2203.07687v1.pdf"
    },
    {
        "title": "Do Language Models Plagiarize?",
        "authors": [
            "Jooyoung Lee",
            "Thai Le",
            "Jinghui Chen",
            "Dongwon Lee"
        ],
        "published": "2022-03-15T03:11:11Z",
        "summary": "Past literature has illustrated that language models (LMs) often memorize\nparts of training instances and reproduce them in natural language generation\n(NLG) processes. However, it is unclear to what extent LMs \"reuse\" a training\ncorpus. For instance, models can generate paraphrased sentences that are\ncontextually similar to training samples. In this work, therefore, we study\nthree types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2\ngenerated texts, in comparison to its training data, and further analyze the\nplagiarism patterns of fine-tuned LMs with domain-specific corpora which are\nextensively used in practice. Our results suggest that (1) three types of\nplagiarism widely exist in LMs beyond memorization, (2) both size and decoding\nmethods of LMs are strongly associated with the degrees of plagiarism they\nexhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus\nsimilarity and homogeneity. Given that a majority of LMs' training data is\nscraped from the Web without informing content owners, their reiteration of\nwords, phrases, and even core ideas from training sets into generated texts has\nethical implications. Their patterns are likely to exacerbate as both the size\nof LMs and their training data increase, raising concerns about\nindiscriminately pursuing larger models with larger training corpora.\nPlagiarized content can also contain individuals' personal and sensitive\ninformation. These findings overall cast doubt on the practicality of current\nLMs in mission-critical writing tasks and urge more discussions around the\nobserved phenomena. Data and source code are available at\nhttps://github.com/Brit7777/LM-plagiarism.",
        "pdf_link": "https://arxiv.org/pdf/2203.07618v2.pdf"
    },
    {
        "title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
        "authors": [
            "Woojeong Jin",
            "Dong-Ho Lee",
            "Chenguang Zhu",
            "Jay Pujara",
            "Xiang Ren"
        ],
        "published": "2022-03-14T22:02:40Z",
        "summary": "Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.",
        "pdf_link": "https://arxiv.org/pdf/2203.07519v2.pdf"
    },
    {
        "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
        "authors": [
            "Archiki Prasad",
            "Peter Hase",
            "Xiang Zhou",
            "Mohit Bansal"
        ],
        "published": "2022-03-14T16:54:46Z",
        "summary": "Providing natural language instructions in prompts is a useful new paradigm\nfor improving task performance of large language models in a zero-shot setting.\nRecent work has aimed to improve such prompts via manual rewriting or\ngradient-based tuning. However, manual rewriting is time-consuming and requires\nsubjective interpretation, while gradient-based tuning can be extremely\ncomputationally demanding for large models and may not be feasible for\nAPI-based models. In this work, we introduce Gradient-free Instructional Prompt\nSearch (GrIPS), a gradient-free, edit-based search approach for improving task\ninstructions for large language models. GrIPS takes in instructions designed\nfor humans and automatically returns an improved, edited prompt, while allowing\nfor API-based tuning. With InstructGPT models, GrIPS improves the average task\nperformance by up to 4.30 percentage points on eight classification tasks from\nthe Natural Instructions dataset (with similar improvements for OPT, BLOOM, and\nFLAN-T5). We see improvements for both instruction-only prompts and instruction\n+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and\npurely example-based prompts while controlling for the available compute and\ndata budget. Further, performance of GrIPS is comparable to select\ngradient-based tuning approaches. Qualitatively, we show our edits can simplify\ninstructions and at times make them incoherent but nonetheless improve\naccuracy. Our code is available at: https://github.com/archiki/GrIPS",
        "pdf_link": "https://arxiv.org/pdf/2203.07281v2.pdf"
    },
    {
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
        "authors": [
            "Eldar Kurtic",
            "Daniel Campos",
            "Tuan Nguyen",
            "Elias Frantar",
            "Mark Kurtz",
            "Benjamin Fineran",
            "Michael Goin",
            "Dan Alistarh"
        ],
        "published": "2022-03-14T16:40:31Z",
        "summary": "Transformer-based language models have become a key building block for\nnatural language processing. While these models are extremely accurate, they\ncan be too large and computationally intensive to run on standard deployments.\nA variety of compression methods, including distillation, quantization,\nstructured and unstructured pruning are known to decrease model size and\nincrease inference speed, with low accuracy loss. In this context, this paper's\ncontributions are two-fold. We perform an in-depth study of the\naccuracy-compression trade-off for unstructured weight pruning of BERT models.\nWe introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight\npruning method based on approximate second-order information, which we show to\nyield state-of-the-art results in both stages of language tasks: pre-training\nand fine-tuning. Specifically, oBERT extends existing work on unstructured\nsecond-order pruning by allowing for pruning blocks of weights, and by being\napplicable at the BERT scale. Second, we investigate the impact of this pruning\nmethod when compounding compression approaches to obtain highly compressed but\naccurate models for deployment on edge devices. These models significantly push\nboundaries of the current state-of-the-art sparse BERT models with respect to\nall metrics: model size, inference speed and task accuracy. For example,\nrelative to the dense BERT-base, we obtain 10x model size compression (in MB)\nwith < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and\n29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated\nwith Transformers and SparseML, is available at\nhttps://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.",
        "pdf_link": "https://arxiv.org/pdf/2203.07259v3.pdf"
    },
    {
        "title": "Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study",
        "authors": [
            "Roelien C. Timmer",
            "David Liebowitz",
            "Surya Nepal",
            "Salil S. Kanhere"
        ],
        "published": "2022-03-14T00:17:34Z",
        "summary": "Each and every organisation releases information in a variety of forms\nranging from annual reports to legal proceedings. Such documents may contain\nsensitive information and releasing them openly may lead to the leakage of\nconfidential information. Detection of sentences that contain sensitive\ninformation in documents can help organisations prevent the leakage of valuable\nconfidential information. This is especially challenging when such sentences\ncontain a substantial amount of information or are paraphrased versions of\nknown sensitive content. Current approaches to sensitive information detection\nin such complex settings are based on keyword-based approaches or standard\nmachine learning models. In this paper, we wish to explore whether pre-trained\ntransformer models are well suited to detect complex sensitive information.\nPre-trained transformers are typically trained on an enormous amount of text\nand therefore readily learn grammar, structure and other linguistic features,\nmaking them particularly attractive for this task. Through our experiments on\nthe Monsanto trial data set, we observe that the fine-tuned Bidirectional\nEncoder Representations from Transformers (BERT) transformer model performs\nbetter than traditional models. We experimented with four different categories\nof documents in the Monsanto dataset and observed that BERT achieves better F2\nscores by 24.13\\% to 65.79\\% for GHOST, 30.14\\% to 54.88\\% for TOXIC, 39.22\\%\nfor CHEMI, 53.57\\% for REGUL compared to existing sensitive information\ndetection models.",
        "pdf_link": "https://arxiv.org/pdf/2203.06793v1.pdf"
    },
    {
        "title": "Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice",
        "authors": [
            "Andreas Grivas",
            "Nikolay Bogoychev",
            "Adam Lopez"
        ],
        "published": "2022-03-12T15:34:54Z",
        "summary": "Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.",
        "pdf_link": "https://arxiv.org/pdf/2203.06462v2.pdf"
    },
    {
        "title": "When classifying grammatical role, BERT doesn't care about word order... except when it matters",
        "authors": [
            "Isabel Papadimitriou",
            "Richard Futrell",
            "Kyle Mahowald"
        ],
        "published": "2022-03-11T19:00:15Z",
        "summary": "Because meaning can often be inferred from lexical semantics alone, word\norder is often a redundant cue in natural language. For example, the words\nchopped, chef, and onion are more likely used to convey \"The chef chopped the\nonion,\" not \"The onion chopped the chef.\" Recent work has shown large language\nmodels to be surprisingly word order invariant, but crucially has largely\nconsidered natural prototypical inputs, where compositional meaning mostly\nmatches lexical expectations. To overcome this confound, we probe grammatical\nrole representation in English BERT and GPT-2, on instances where lexical\nexpectations are not sufficient, and word order knowledge is necessary for\ncorrect classification. Such non-prototypical instances are naturally occurring\nEnglish sentences with inanimate subjects or animate objects, or sentences\nwhere we systematically swap the arguments to make sentences like \"The onion\nchopped the chef\". We find that, while early layer embeddings are largely\nlexical, word order is in fact crucial in defining the later-layer\nrepresentations of words in semantically non-prototypical positions. Our\nexperiments isolate the effect of word order on the contextualization process,\nand highlight how models use context in the uncommon, but critical, instances\nwhere it matters.",
        "pdf_link": "https://arxiv.org/pdf/2203.06204v1.pdf"
    },
    {
        "title": "Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers",
        "authors": [
            "Sahar Sadrizadeh",
            "Ljiljana Dolamic",
            "Pascal Frossard"
        ],
        "published": "2022-03-11T14:37:41Z",
        "summary": "Recently, it has been shown that, in spite of the significant performance of\ndeep neural networks in different fields, those are vulnerable to adversarial\nexamples. In this paper, we propose a gradient-based adversarial attack against\ntransformer-based text classifiers. The adversarial perturbation in our method\nis imposed to be block-sparse so that the resultant adversarial example differs\nfrom the original sentence in only a few words. Due to the discrete nature of\ntextual data, we perform gradient projection to find the minimizer of our\nproposed optimization problem. Experimental results demonstrate that, while our\nadversarial attack maintains the semantics of the sentence, it can reduce the\naccuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and\nYelp Reviews). Furthermore, the block-sparsity constraint of the proposed\noptimization problem results in small perturbations in the adversarial example.",
        "pdf_link": "https://arxiv.org/pdf/2203.05948v1.pdf"
    },
    {
        "title": "Contextualized Sensorimotor Norms: multi-dimensional measures of sensorimotor strength for ambiguous English words, in context",
        "authors": [
            "Sean Trott",
            "Benjamin Bergen"
        ],
        "published": "2022-03-10T21:23:00Z",
        "summary": "Most large language models are trained on linguistic input alone, yet humans\nappear to ground their understanding of words in sensorimotor experience. A\nnatural solution is to augment LM representations with human judgments of a\nword's sensorimotor associations (e.g., the Lancaster Sensorimotor Norms), but\nthis raises another challenge: most words are ambiguous, and judgments of words\nin isolation fail to account for this multiplicity of meaning (e.g., \"wooden\ntable\" vs. \"data table\"). We attempted to address this problem by building a\nnew lexical resource of contextualized sensorimotor judgments for 112 English\nwords, each rated in four different contexts (448 sentences total). We show\nthat these ratings encode overlapping but distinct information from the\nLancaster Sensorimotor Norms, and that they also predict other measures of\ninterest (e.g., relatedness), above and beyond measures derived from BERT.\nBeyond shedding light on theoretical questions, we suggest that these ratings\ncould be of use as a \"challenge set\" for researchers building grounded language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2203.05648v1.pdf"
    },
    {
        "title": "Speciesist Language and Nonhuman Animal Bias in English Masked Language Models",
        "authors": [
            "Masashi Takeshita",
            "Rafal Rzepka",
            "Kenji Araki"
        ],
        "published": "2022-03-10T03:32:29Z",
        "summary": "Various existing studies have analyzed what social biases are inherited by\nNLP models. These biases may directly or indirectly harm people, therefore\nprevious studies have focused only on human attributes. However, until recently\nno research on social biases in NLP regarding nonhumans existed. In this paper,\nwe analyze biases to nonhuman animals, i.e. speciesist bias, inherent in\nEnglish Masked Language Models such as BERT. We analyzed speciesist bias\nagainst 46 animal names using template-based and corpus-extracted sentences\ncontaining speciesist (or non-speciesist) language. We found that pre-trained\nmasked language models tend to associate harmful words with nonhuman animals\nand have a bias toward using speciesist language for some nonhuman animal\nnames. Our code for reproducing the experiments will be made available on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2203.05140v3.pdf"
    },
    {
        "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition",
        "authors": [
            "W. Ronny Huang",
            "Cal Peyser",
            "Tara N. Sainath",
            "Ruoming Pang",
            "Trevor Strohman",
            "Shankar Kumar"
        ],
        "published": "2022-03-09T19:20:03Z",
        "summary": "Language model fusion helps smart assistants recognize words which are rare\nin acoustic data but abundant in text-only corpora (typed search logs).\nHowever, such corpora have properties that hinder downstream performance,\nincluding being (1) too large, (2) beset with domain-mismatched content, and\n(3) heavy-headed rather than heavy-tailed (excessively many duplicate search\nqueries such as \"weather\"). We show that three simple strategies for selecting\nlanguage modeling data can dramatically improve rare-word recognition without\nharming overall performance. First, to address the heavy-headedness, we\ndownsample the data according to a soft log function, which tunably reduces\nhigh frequency (head) sentences. Second, to encourage rare-word exposure, we\nexplicitly filter for words rare in the acoustic data. Finally, we tackle\ndomain-mismatch via perplexity-based contrastive selection, filtering for\nexamples matched to the target domain. We down-select a large corpus of web\nsearch queries by a factor of 53x and achieve better LM perplexities than\nwithout down-selection. When shallow-fused with a state-of-the-art, production\nspeech engine, our LM achieves WER reductions of up to 24% relative on\nrare-word sentences (without changing overall WER) compared to a baseline LM\ntrained on the raw corpus. These gains are further validated through favorable\nside-by-side evaluations on live voice search traffic.",
        "pdf_link": "https://arxiv.org/pdf/2203.05008v2.pdf"
    },
    {
        "title": "Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing",
        "authors": [
            "Sonish Sivarajkumar",
            "Thomas Yu CHow Tam",
            "Haneef Ahamed Mohammad",
            "Samual Viggiano",
            "David Oniani",
            "Shyam Visweswaran",
            "Yanshan Wang"
        ],
        "published": "2022-03-08T21:20:19Z",
        "summary": "Alzheimer's Disease (AD) is the most common form of dementia in the United\nStates. Sleep is one of the lifestyle-related factors that has been shown\ncritical for optimal cognitive function in old age. However, there is a lack of\nresearch studying the association between sleep and AD incidence. A major\nbottleneck for conducting such research is that the traditional way to acquire\nsleep information is time-consuming, inefficient, non-scalable, and limited to\npatients' subjective experience. A gold standard dataset is created from manual\nannotation of 570 randomly sampled clinical note documents from the adSLEEP, a\ncorpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved\nfrom the University of Pittsburgh Medical Center (UPMC). We developed a\nrule-based Natural Language Processing (NLP) algorithm, machine learning\nmodels, and Large Language Model(LLM)-based NLP algorithms to automate the\nextraction of sleep-related concepts, including snoring, napping, sleep\nproblem, bad sleep quality, daytime sleepiness, night wakings, and sleep\nduration, from the gold standard dataset. Rule-based NLP algorithm achieved the\nbest performance of F1 across all sleep-related concepts. In terms of Positive\nPredictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime\nsleepiness and sleep duration, machine learning models: 0.95 and for napping,\n0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning\nachieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for\nsleep duration. The results show that the rule-based NLP algorithm consistently\nachieved the best performance for all sleep concepts. This study focused on the\nclinical notes of patients with AD, but could be extended to general sleep\ninformation extraction for other diseases.",
        "pdf_link": "https://arxiv.org/pdf/2204.09601v2.pdf"
    },
    {
        "title": "What Did You Say? Task-Oriented Dialog Datasets Are Not Conversational!?",
        "authors": [
            "Alice Shoshana Jakobovits",
            "Francesco Piccinno",
            "Yasemin Altun"
        ],
        "published": "2022-03-07T14:26:23Z",
        "summary": "High-quality datasets for task-oriented dialog are crucial for the\ndevelopment of virtual assistants. Yet three of the most relevant large scale\ndialog datasets suffer from one common flaw: the dialog state update can be\ntracked, to a great extent, by a model that only considers the current user\nutterance, ignoring the dialog history. In this work, we outline a taxonomy of\nconversational and contextual effects, which we use to examine MultiWOZ, SGD\nand SMCalFlow, among the most recent and widely used task-oriented dialog\ndatasets. We analyze the datasets in a model-independent fashion and\ncorroborate these findings experimentally using a strong text-to-text baseline\n(T5). We find that less than 4% of MultiWOZ's turns and 10% of SGD's turns are\nconversational, while SMCalFlow is not conversational at all in its current\nrelease: its dialog state tracking task can be reduced to single exchange\nsemantic parsing. We conclude by outlining desiderata for truly conversational\ndialog datasets.",
        "pdf_link": "https://arxiv.org/pdf/2203.03431v1.pdf"
    },
    {
        "title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models",
        "authors": [
            "Shengnan An",
            "Yifei Li",
            "Zeqi Lin",
            "Qian Liu",
            "Bei Chen",
            "Qiang Fu",
            "Weizhu Chen",
            "Nanning Zheng",
            "Jian-Guang Lou"
        ],
        "published": "2022-03-07T05:04:32Z",
        "summary": "Recently the prompt-tuning paradigm has attracted significant attention. By\nonly tuning continuous prompts with a frozen pre-trained language model (PLM),\nprompt-tuning takes a step towards deploying a shared frozen PLM to serve\nnumerous downstream tasks. Although prompt-tuning shows good performance on\ncertain natural language understanding (NLU) tasks, its effectiveness on\nnatural language generation (NLG) tasks is still under-explored. In this paper,\nwe argue that one of the factors hindering the development of prompt-tuning on\nNLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different\nfrom the pretraining corpus). For example, our preliminary exploration reveals\na large performance gap between prompt-tuning and fine-tuning when unfamiliar\ninputs occur frequently in NLG tasks. This motivates us to propose\ninput-tuning, which fine-tunes both the continuous prompts and the input\nrepresentations, leading to a more effective way to adapt unfamiliar inputs to\nfrozen PLMs. Our proposed input-tuning is conceptually simple and empirically\npowerful. Experimental results on seven NLG tasks demonstrate that input-tuning\nis significantly and consistently better than prompt-tuning. Furthermore, on\nthree of these tasks, input-tuning can achieve a comparable or even better\nperformance than fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2203.03131v1.pdf"
    },
    {
        "title": "Training language models to follow instructions with human feedback",
        "authors": [
            "Long Ouyang",
            "Jeff Wu",
            "Xu Jiang",
            "Diogo Almeida",
            "Carroll L. Wainwright",
            "Pamela Mishkin",
            "Chong Zhang",
            "Sandhini Agarwal",
            "Katarina Slama",
            "Alex Ray",
            "John Schulman",
            "Jacob Hilton",
            "Fraser Kelton",
            "Luke Miller",
            "Maddie Simens",
            "Amanda Askell",
            "Peter Welinder",
            "Paul Christiano",
            "Jan Leike",
            "Ryan Lowe"
        ],
        "published": "2022-03-04T07:04:42Z",
        "summary": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.",
        "pdf_link": "https://arxiv.org/pdf/2203.02155v1.pdf"
    },
    {
        "title": "LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models",
        "authors": [
            "Mojan Javaheripi",
            "Gustavo H. de Rosa",
            "Subhabrata Mukherjee",
            "Shital Shah",
            "Tomasz L. Religa",
            "Caio C. T. Mendes",
            "Sebastien Bubeck",
            "Farinaz Koushanfar",
            "Debadeepta Dey"
        ],
        "published": "2022-03-04T02:10:43Z",
        "summary": "The Transformer architecture is ubiquitously used as the building block of\nlarge-scale autoregressive language models. However, finding architectures with\nthe optimal trade-off between task performance (perplexity) and hardware\nconstraints like peak memory utilization and latency is non-trivial. This is\nexacerbated by the proliferation of various hardware. We leverage the somewhat\nsurprising empirical observation that the number of decoder parameters in\nautoregressive Transformers has a high rank correlation with task performance,\nirrespective of the architecture topology. This observation organically induces\na simple Neural Architecture Search (NAS) algorithm that uses decoder\nparameters as a proxy for perplexity without need for any model training. The\nsearch phase of our training-free algorithm, dubbed Lightweight Transformer\nSearch (LTS), can be run directly on target devices since it does not require\nGPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of\nperplexity versus any hardware performance cost. We evaluate LTS on diverse\ndevices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer\nbackbones: GPT-2 and Transformer-XL. Results show that the perplexity of\n16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster\nruntime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero\nand one-shot settings, LTS Pareto-frontier models achieve higher average\naccuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x\nlower latency. LTS extracts the Pareto-frontier in under 3 hours while running\non a commodity laptop. We effectively remove the carbon footprint of hundreds\nof GPU hours of training during search, offering a strong simple baseline for\nfuture NAS methods in autoregressive language modeling.",
        "pdf_link": "https://arxiv.org/pdf/2203.02094v2.pdf"
    },
    {
        "title": "BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification",
        "authors": [
            "Yuanhong Chen",
            "Fengbei Liu",
            "Hu Wang",
            "Chong Wang",
            "Yu Tian",
            "Yuyuan Liu",
            "Gustavo Carneiro"
        ],
        "published": "2022-03-03T08:04:59Z",
        "summary": "Deep learning methods have shown outstanding classification accuracy in\nmedical imaging problems, which is largely attributed to the availability of\nlarge-scale datasets manually annotated with clean labels. However, given the\nhigh cost of such manual annotation, new medical imaging classification\nproblems may need to rely on machine-generated noisy labels extracted from\nradiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been\nmodelled from datasets with noisy labels, but their training procedure is in\ngeneral not robust to noisy-label samples, leading to sub-optimal models.\nFurthermore, CXR datasets are mostly multi-label, so current noisy-label\nlearning methods designed for multi-class problems cannot be easily adapted. In\nthis paper, we propose a new method designed for the noisy multi-label CXR\nlearning, which detects and smoothly re-labels samples from the dataset, which\nis then used to train common multi-label classifiers. The proposed method\noptimises a bag of multi-label descriptors (BoMD) to promote their similarity\nwith the semantic descriptors produced by BERT models from the multi-label\nimage annotation. Our experiments on diverse noisy multi-label training sets\nand clean testing sets show that our model has state-of-the-art accuracy and\nrobustness in many CXR multi-label classification benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2203.01937v5.pdf"
    },
    {
        "title": "Logical Fallacy Detection",
        "authors": [
            "Zhijing Jin",
            "Abhinav Lalwani",
            "Tejas Vaidhya",
            "Xiaoyu Shen",
            "Yiwen Ding",
            "Zhiheng Lyu",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Bernhard Schölkopf"
        ],
        "published": "2022-02-28T13:18:26Z",
        "summary": "Reasoning is central to human intelligence. However, fallacious arguments are\ncommon, and some exacerbate problems such as spreading misinformation about\nclimate change. In this paper, we propose the task of logical fallacy\ndetection, and provide a new dataset (Logic) of logical fallacies generally\nfound in text, together with an additional challenge set for detecting logical\nfallacies in climate change claims (LogicClimate). Detecting logical fallacies\nis a hard problem as the model must understand the underlying logical structure\nof the argument. We find that existing pretrained large language models perform\npoorly on this task. In contrast, we show that a simple structure-aware\nclassifier outperforms the best language model by 5.46% on Logic and 4.51% on\nLogicClimate. We encourage future work to explore this task as (a) it can serve\nas a new reasoning challenge for language models, and (b) it can have potential\napplications in tackling the spread of misinformation. Our dataset and code are\navailable at https://github.com/causalNLP/logical-fallacy",
        "pdf_link": "https://arxiv.org/pdf/2202.13758v3.pdf"
    },
    {
        "title": "Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation",
        "authors": [
            "Chulun Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Min Zhang",
            "Hongji Wang",
            "Jinsong Su"
        ],
        "published": "2022-02-28T10:24:22Z",
        "summary": "Most dominant neural machine translation (NMT) models are restricted to make\npredictions only according to the local context of preceding words in a\nleft-to-right manner. Although many previous studies try to incorporate global\ninformation into NMT models, there still exist limitations on how to\neffectively exploit bidirectional global context. In this paper, we propose a\nConfidence Based Bidirectional Global Context Aware (CBBGCA) training framework\nfor NMT, where the NMT model is jointly trained with an auxiliary conditional\nmasked language model (CMLM). The training consists of two stages: (1)\nmulti-task joint training; (2) confidence based knowledge distillation. At the\nfirst stage, by sharing encoder parameters, the NMT model is additionally\nsupervised by the signal from the CMLM decoder that contains bidirectional\nglobal contexts. Moreover, at the second stage, using the CMLM as teacher, we\nfurther pertinently incorporate bidirectional global context to the NMT model\non its unconfidently-predicted target words via knowledge distillation.\nExperimental results show that our proposed CBBGCA training framework\nsignificantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on\nthree large-scale translation datasets, namely WMT'14 English-to-German, WMT'19\nChinese-to-English and WMT'14 English-to-French, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2202.13663v3.pdf"
    },
    {
        "title": "A Systematic Evaluation of Large Language Models of Code",
        "authors": [
            "Frank F. Xu",
            "Uri Alon",
            "Graham Neubig",
            "Vincent J. Hellendoorn"
        ],
        "published": "2022-02-26T15:53:55Z",
        "summary": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.",
        "pdf_link": "https://arxiv.org/pdf/2202.13169v3.pdf"
    },
    {
        "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
        "authors": [
            "Chujie Zheng",
            "Sahand Sabour",
            "Jiaxin Wen",
            "Zheng Zhang",
            "Minlie Huang"
        ],
        "published": "2022-02-26T03:17:08Z",
        "summary": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.13047v3.pdf"
    },
    {
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "authors": [
            "Sewon Min",
            "Xinxi Lyu",
            "Ari Holtzman",
            "Mikel Artetxe",
            "Mike Lewis",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022-02-25T17:25:19Z",
        "summary": "Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.",
        "pdf_link": "https://arxiv.org/pdf/2202.12837v2.pdf"
    },
    {
        "title": "TrimBERT: Tailoring BERT for Trade-offs",
        "authors": [
            "Sharath Nittur Sridhar",
            "Anthony Sarah",
            "Sairam Sundaresan"
        ],
        "published": "2022-02-24T23:06:29Z",
        "summary": "Models based on BERT have been extremely successful in solving a variety of\nnatural language processing (NLP) tasks. Unfortunately, many of these large\nmodels require a great deal of computational resources and/or time for\npre-training and fine-tuning which limits wider adoptability. While\nself-attention layers have been well-studied, a strong justification for\ninclusion of the intermediate layers which follow them remains missing in the\nliterature. In this work, we show that reducing the number of intermediate\nlayers in BERT-Base results in minimal fine-tuning accuracy loss of downstream\ntasks while significantly decreasing model size and training time. We further\nmitigate two key bottlenecks, by replacing all softmax operations in the\nself-attention layers with a computationally simpler alternative and removing\nhalf of all layernorm operations. This further decreases the training time\nwhile maintaining a high level of fine-tuning accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2202.12411v1.pdf"
    },
    {
        "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
        "authors": [
            "Zhengxuan Wu",
            "Alex Tamkin",
            "Isabel Papadimitriou"
        ],
        "published": "2022-02-24T19:00:39Z",
        "summary": "When we transfer a pretrained language model to a new language, there are\nmany axes of variation that change at once. To disentangle the impact of\ndifferent factors like syntactic similarity and vocabulary similarity, we\npropose a set of controlled transfer studies: we systematically transform the\nlanguage of the GLUE benchmark, altering one axis of crosslingual variation at\na time, and then measure the resulting drops in a pretrained model's downstream\nperformance. We find that models can largely recover from syntactic-style\nshifts, but cannot recover from vocabulary misalignment and embedding matrix\nre-initialization, even with continued pretraining on 15 million tokens. %On\nthe other hand, transferring to a dataset with an unaligned vocabulary is\nextremely hard to recover from in the low-data regime. Moreover, good-quality\ntokenizers in the transfer language do not make vocabulary alignment easier.\nOur experiments provide insights into the factors of cross-lingual transfer\nthat researchers should most focus on when designing language transfer\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2202.12312v2.pdf"
    },
    {
        "title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
        "authors": [
            "Erik Jones",
            "Jacob Steinhardt"
        ],
        "published": "2022-02-24T18:58:52Z",
        "summary": "Large language models generate complex, open-ended outputs: instead of\noutputting a class label they write summaries, generate dialogue, or produce\nworking code. In order to asses the reliability of these open-ended generation\nsystems, we aim to identify qualitative categories of erroneous behavior,\nbeyond identifying individual errors. To hypothesize and test for such\nqualitative errors, we draw inspiration from human cognitive biases --\nsystematic patterns of deviation from rational judgement. Specifically, we use\ncognitive biases as motivation to (i) generate hypotheses for problems that\nmodels may have, and (ii) develop experiments that elicit these problems. Using\ncode generation as a case study, we find that OpenAI's Codex errs predictably\nbased on how the input prompt is framed, adjusts outputs towards anchors, and\nis biased towards outputs that mimic frequent training examples. We then use\nour framework to elicit high-impact errors such as incorrectly deleting files.\nOur results indicate that experimental methodology from cognitive science can\nhelp characterize how machine learning systems behave.",
        "pdf_link": "https://arxiv.org/pdf/2202.12299v2.pdf"
    },
    {
        "title": "Ask2Mask: Guided Data Selection for Masked Speech Modeling",
        "authors": [
            "Murali Karthick Baskar",
            "Andrew Rosenberg",
            "Bhuvana Ramabhadran",
            "Yu Zhang",
            "Pedro Moreno"
        ],
        "published": "2022-02-24T17:34:54Z",
        "summary": "Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn\nrepresentations over speech frames which are randomly masked within an\nutterance. While these methods improve performance of Automatic Speech\nRecognition (ASR) systems, they have one major limitation. They treat all\nunsupervised speech samples with equal weight, which hinders learning as not\nall samples have relevant information to learn meaningful representations. In\nthis work, we address this limitation. We propose ask2mask (ATM), a novel\napproach to focus on specific samples during MSM pre-training. ATM employs an\nexternal ASR model or \\textit{scorer} to weight unsupervised input samples in\ntwo different ways: 1) A fine-grained data selection is performed by masking\nover the highly confident input frames as chosen by the scorer. This allows the\nmodel to learn meaningful representations. 2) ATM is further extended to focus\nat utterance-level by weighting the final MSM loss with the utterance-level\nconfidence score. We conduct fine-tuning experiments on two well-benchmarked\ncorpora: LibriSpeech (matching the pre-training data) and Commonvoice,\nTED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results\nsubstantiate the efficacy of ATM on significantly improving the recognition\nperformance under mismatched conditions (up to 11.6\\% relative over published\nresults and upto 4.46\\% relative over our internal baseline) while still\nyielding modest improvements under matched conditions.",
        "pdf_link": "https://arxiv.org/pdf/2202.12719v1.pdf"
    },
    {
        "title": "Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals",
        "authors": [
            "Thilo Hagendorff",
            "Leonie Bossert",
            "Tse Yip Fai",
            "Peter Singer"
        ],
        "published": "2022-02-22T12:23:21Z",
        "summary": "Massive efforts are made to reduce biases in both data and algorithms in\norder to render AI applications fair. These efforts are propelled by various\nhigh-profile cases where biased algorithmic decision-making caused harm to\nwomen, people of color, minorities, etc. However, the AI fairness field still\nsuccumbs to a blind spot, namely its insensitivity to discrimination against\nanimals. This paper is the first to describe the 'speciesist bias' and\ninvestigate it in several different AI systems. Speciesist biases are learned\nand solidified by AI applications when they are trained on datasets in which\nspeciesist patterns prevail. These patterns can be found in image recognition\nsystems, large language models, and recommender systems. Therefore, AI\ntechnologies currently play a significant role in perpetuating and normalizing\nviolence against animals. This can only be changed when AI fairness frameworks\nwiden their scope and include mitigation measures for speciesist biases. This\npaper addresses the AI community in this regard and stresses the influence AI\nsystems can have on either increasing or reducing the violence that is\ninflicted on animals, and especially on farmed animals.",
        "pdf_link": "https://arxiv.org/pdf/2202.10848v1.pdf"
    },
    {
        "title": "Adaptive Discounting of Implicit Language Models in RNN-Transducers",
        "authors": [
            "Vinit Unni",
            "Shreya Khare",
            "Ashish Mittal",
            "Preethi Jyothi",
            "Sunita Sarawagi",
            "Samarth Bharadwaj"
        ],
        "published": "2022-02-21T08:44:56Z",
        "summary": "RNN-Transducer (RNN-T) models have become synonymous with streaming\nend-to-end ASR systems. While they perform competitively on a number of\nevaluation categories, rare words pose a serious challenge to RNN-T models. One\nmain reason for the degradation in performance on rare words is that the\nlanguage model (LM) internal to RNN-Ts can become overconfident and lead to\nhallucinated predictions that are acoustically inconsistent with the underlying\nspeech. To address this issue, we propose a lightweight adaptive LM discounting\ntechnique AdaptLMD, that can be used with any RNN-T architecture without\nrequiring any external resources or additional parameters. AdaptLMD uses a\ntwo-pronged approach: 1) Randomly mask the prediction network output to\nencourage the RNN-T to not be overly reliant on it's outputs. 2) Dynamically\nchoose when to discount the implicit LM (ILM) based on rarity of recently\npredicted tokens and divergence between ILM and implicit acoustic model (IAM)\nscores. Comparing AdaptLMD to a competitive RNN-T baseline, we obtain up to 4%\nand 14% relative reductions in overall WER and rare word PER, respectively, on\na conversational, code-mixed Hindi-English ASR task.",
        "pdf_link": "https://arxiv.org/pdf/2203.02317v1.pdf"
    },
    {
        "title": "GPT-based Open-Ended Knowledge Tracing",
        "authors": [
            "Naiming Liu",
            "Zichao Wang",
            "Richard G. Baraniuk",
            "Andrew Lan"
        ],
        "published": "2022-02-21T02:33:34Z",
        "summary": "In education applications, knowledge tracing refers to the problem of\nestimating students' time-varying concept/skill mastery level from their past\nresponses to questions and predicting their future performance. One key\nlimitation of most existing knowledge tracing methods is that they treat\nstudent responses to questions as binary-valued, i.e., whether they are correct\nor incorrect. Response correctness analysis/prediction ignores important\ninformation on student knowledge contained in the exact content of the\nresponses, especially for open-ended questions. In this paper, we conduct the\nfirst exploration into open-ended knowledge tracing (OKT) by studying the new\ntask of predicting students' exact open-ended responses to questions. Our work\nis grounded in the domain of computer science education with programming\nquestions. We develop an initial solution to the OKT problem, a student\nknowledge-guided code generation approach, that combines program synthesis\nmethods using language models with student knowledge tracing methods. We also\nconduct a series of quantitative and qualitative experiments on a real-world\nstudent code dataset to validate OKT and demonstrate its promise in educational\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2203.03716v4.pdf"
    },
    {
        "title": "Reward Modeling for Mitigating Toxicity in Transformer-based Language Models",
        "authors": [
            "Farshid Faal",
            "Ketra Schmitt",
            "Jia Yuan Yu"
        ],
        "published": "2022-02-19T19:26:22Z",
        "summary": "Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.",
        "pdf_link": "https://arxiv.org/pdf/2202.09662v6.pdf"
    },
    {
        "title": "SGPT: GPT Sentence Embeddings for Semantic Search",
        "authors": [
            "Niklas Muennighoff"
        ],
        "published": "2022-02-17T21:35:56Z",
        "summary": "Decoder transformers have continued increasing in scale reaching hundreds of\nbillions of parameters. Due to their scale the same decoder sets\nstate-of-the-art results on various language tasks via prompting or\nfine-tuning. Yet, these large foundation models remain unusable for the related\nfields of semantic search and sentence embeddings. This prevents possibly new\nstate-of-the-art results and forces organizations to train and maintain\nseparate models. To this end, we propose SGPT to use decoders for sentence\nembeddings and semantic search via prompting or fine-tuning. At 5.8 billion\nparameters SGPT improves on the previously best sentence embeddings by a margin\nof 7% and outperforms a concurrent method with 175 billion parameters as\nmeasured on the BEIR search benchmark. Code, models and result files are freely\navailable at https://github.com/Muennighoff/sgpt.",
        "pdf_link": "https://arxiv.org/pdf/2202.08904v5.pdf"
    },
    {
        "title": "Information Extraction in Low-Resource Scenarios: Survey and Perspective",
        "authors": [
            "Shumin Deng",
            "Yubo Ma",
            "Ningyu Zhang",
            "Yixin Cao",
            "Bryan Hooi"
        ],
        "published": "2022-02-16T13:44:00Z",
        "summary": "Information Extraction (IE) seeks to derive structured information from\nunstructured texts, often facing challenges in low-resource scenarios due to\ndata scarcity and unseen classes. This paper presents a review of neural\napproaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based}\nperspectives, systematically categorizing them into a fine-grained taxonomy.\nThen we conduct empirical study on LLM-based methods compared with previous\nstate-of-the-art models, and discover that (1) well-tuned LMs are still\npredominant; (2) tuning open-resource LLMs and ICL with GPT family is promising\nin general; (3) the optimal LLM-based technical solution for low-resource IE\ncan be task-dependent. In addition, we discuss low-resource IE with LLMs,\nhighlight promising applications, and outline potential research directions.\nThis survey aims to foster understanding of this field, inspire new ideas, and\nencourage widespread applications in both academia and industry.",
        "pdf_link": "https://arxiv.org/pdf/2202.08063v5.pdf"
    },
    {
        "title": "Should You Mask 15% in Masked Language Modeling?",
        "authors": [
            "Alexander Wettig",
            "Tianyu Gao",
            "Zexuan Zhong",
            "Danqi Chen"
        ],
        "published": "2022-02-16T11:42:34Z",
        "summary": "Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT's 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2202.08005v3.pdf"
    },
    {
        "title": "Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers",
        "authors": [
            "Yotaro Kubo",
            "Shigeki Karita",
            "Michiel Bacchiani"
        ],
        "published": "2022-02-16T07:02:24Z",
        "summary": "End-to-end speech recognition is a promising technology for enabling compact\nautomatic speech recognition (ASR) systems since it can unify the acoustic and\nlanguage model into a single neural network. However, as a drawback, training\nof end-to-end speech recognizers always requires transcribed utterances. Since\nend-to-end models are also known to be severely data hungry, this constraint is\ncrucial especially because obtaining transcribed utterances is costly and can\npossibly be impractical or impossible. This paper proposes a method for\nalleviating this issue by transferring knowledge from a language model neural\nnetwork that can be pretrained with text-only data. Specifically, this paper\nattempts to transfer semantic knowledge acquired in embedding vectors of\nlarge-scale language models. Since embedding vectors can be assumed as implicit\nrepresentations of linguistic information such as part-of-speech, intent, and\nso on, those are also expected to be useful modeling cues for ASR decoders.\nThis paper extends two types of ASR decoders, attention-based decoders and\nneural transducers, by modifying training loss functions to include embedding\nprediction terms. The proposed systems were shown to be effective for error\nrate reduction without incurring extra computational costs in the decoding\nphase.",
        "pdf_link": "https://arxiv.org/pdf/2202.07894v1.pdf"
    },
    {
        "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications",
        "authors": [
            "Jun Xia",
            "Yanqiao Zhu",
            "Yuanqi Du",
            "Stan Z. Li"
        ],
        "published": "2022-02-16T07:00:52Z",
        "summary": "Pretrained Language Models (PLMs) such as BERT have revolutionized the\nlandscape of Natural Language Processing (NLP). Inspired by their\nproliferation, tremendous efforts have been devoted to Pretrained Graph Models\n(PGMs). Owing to the powerful model architectures of PGMs, abundant knowledge\nfrom massive labeled and unlabeled graph data can be captured. The knowledge\nimplicitly encoded in model parameters can benefit various downstream tasks and\nhelp to alleviate several fundamental issues of learning on graphs. In this\npaper, we provide the first comprehensive survey for PGMs. We firstly present\nthe limitations of graph representation learning and thus introduce the\nmotivation for graph pre-training. Then, we systematically categorize existing\nPGMs based on a taxonomy from four different perspectives. Next, we present the\napplications of PGMs in social recommendation and drug discovery. Finally, we\noutline several promising research directions that can serve as a guideline for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2202.07893v2.pdf"
    },
    {
        "title": "Predicting on the Edge: Identifying Where a Larger Model Does Better",
        "authors": [
            "Taman Narayan",
            "Heinrich Jiang",
            "Sen Zhao",
            "Sanjiv Kumar"
        ],
        "published": "2022-02-15T18:53:14Z",
        "summary": "Much effort has been devoted to making large and more accurate models, but\nrelatively little has been put into understanding which examples are benefiting\nfrom the added complexity. In this paper, we demonstrate and analyze the\nsurprisingly tight link between a model's predictive uncertainty on individual\nexamples and the likelihood that larger models will improve prediction on them.\nThrough extensive numerical studies on the T5 encoder-decoder architecture, we\nshow that large models have the largest improvement on examples where the small\nmodel is most uncertain. On more certain examples, even those where the small\nmodel is not particularly accurate, large models are often unable to improve at\nall, and can even perform worse than the smaller model. Based on these\nfindings, we show that a switcher model which defers examples to a larger model\nwhen a small model is uncertain can achieve striking improvements in\nperformance and resource usage. We also explore committee-based uncertainty\nmetrics that can be more effective but less practical.",
        "pdf_link": "https://arxiv.org/pdf/2202.07652v1.pdf"
    },
    {
        "title": "Quantifying Memorization Across Neural Language Models",
        "authors": [
            "Nicholas Carlini",
            "Daphne Ippolito",
            "Matthew Jagielski",
            "Katherine Lee",
            "Florian Tramer",
            "Chiyuan Zhang"
        ],
        "published": "2022-02-15T18:48:31Z",
        "summary": "Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.",
        "pdf_link": "https://arxiv.org/pdf/2202.07646v3.pdf"
    },
    {
        "title": "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
        "authors": [
            "Patrick Schramowski",
            "Christopher Tauchmann",
            "Kristian Kersting"
        ],
        "published": "2022-02-14T13:00:31Z",
        "summary": "Large datasets underlying much of current machine learning raise serious\nissues concerning inappropriate content such as offensive, insulting,\nthreatening, or might otherwise cause anxiety. This calls for increased dataset\ndocumentation, e.g., using datasheets. They, among other topics, encourage to\nreflect on the composition of the datasets. So far, this documentation,\nhowever, is done manually and therefore can be tedious and error-prone,\nespecially for large image datasets. Here we ask the arguably \"circular\"\nquestion of whether a machine can help us reflect on inappropriate content,\nanswering Question 16 in Datasheets. To this end, we propose to use the\ninformation stored in pre-trained transformer models to assist us in the\ndocumentation process. Specifically, prompt-tuning based on a dataset of\nsocio-moral values steers CLIP to identify potentially inappropriate content,\ntherefore reducing human labor. We then document the inappropriate images found\nusing word clouds, based on captions generated using a vision-language model.\nThe documentations of two popular, large-scale computer vision datasets --\nImageNet and OpenImages -- produced this way suggest that machines can indeed\nhelp dataset creators to answer Question 16 on inappropriate image content.",
        "pdf_link": "https://arxiv.org/pdf/2202.06675v2.pdf"
    },
    {
        "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models",
        "authors": [
            "Nikhil Kandpal",
            "Eric Wallace",
            "Colin Raffel"
        ],
        "published": "2022-02-14T08:20:15Z",
        "summary": "Past work has shown that large language models are susceptible to privacy\nattacks, where adversaries generate sequences from a trained model and detect\nwhich sequences are memorized from the training set. In this work, we show that\nthe success of these attacks is largely due to duplication in commonly used\nweb-scraped training sets. We first show that the rate at which language models\nregenerate training sequences is superlinearly related to a sequence's count in\nthe training set. For instance, a sequence that is present 10 times in the\ntraining data is on average generated ~1000 times more often than a sequence\nthat is present only once. We next show that existing methods for detecting\nmemorized sequences have near-chance accuracy on non-duplicated training\nsequences. Finally, we find that after applying methods to deduplicate training\ndata, language models are considerably more secure against these types of\nprivacy attacks. Taken together, our results motivate an increased focus on\ndeduplication in privacy-sensitive applications and a reevaluation of the\npracticality of existing privacy attacks.",
        "pdf_link": "https://arxiv.org/pdf/2202.06539v3.pdf"
    },
    {
        "title": "Assessment of contextualised representations in detecting outcome phrases in clinical trials",
        "authors": [
            "Micheal Abaho",
            "Danushka Bollegala",
            "Paula R Williamson",
            "Susanna Dodd"
        ],
        "published": "2022-02-13T15:08:00Z",
        "summary": "Automating the recognition of outcomes reported in clinical trials using\nmachine learning has a huge potential of speeding up access to evidence\nnecessary in healthcare decision-making. Prior research has however\nacknowledged inadequate training corpora as a challenge for the Outcome\ndetection (OD) task. Additionally, several contextualized representations like\nBERT and ELMO have achieved unparalleled success in detecting various diseases,\ngenes, proteins, and chemicals, however, the same cannot be emphatically stated\nfor outcomes, because these models have been relatively under-tested and\nstudied for the OD task. We introduce \"EBM-COMET\", a dataset in which 300\nPubMed abstracts are expertly annotated for clinical outcomes. Unlike prior\nrelated datasets that use arbitrary outcome classifications, we use labels from\na taxonomy recently published to standardize outcome classifications. To\nextract outcomes, we fine-tune a variety of pre-trained contextualized\nrepresentations, additionally, we use frozen contextualized and\ncontext-independent representations in our custom neural model augmented with\nclinically informed Part-Of-Speech embeddings and a cost-sensitive loss\nfunction. We adopt strict evaluation for the trained models by rewarding them\nfor correctly identifying full outcome phrases rather than words within the\nentities i.e. given an outcome \"systolic blood pressure\", the models are\nrewarded a classification score only when they predict all 3 words in sequence,\notherwise, they are not rewarded. We observe our best model (BioBERT) achieve\n81.5\\% F1, 81.3\\% sensitivity and 98.0\\% specificity. We reach a consensus on\nwhich contextualized representations are best suited for detecting outcomes\nfrom clinical-trial abstracts. Furthermore, our best model outperforms scores\npublished on the original EBM-NLP dataset leader-board scores.",
        "pdf_link": "https://arxiv.org/pdf/2203.03547v2.pdf"
    },
    {
        "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification",
        "authors": [
            "Xinjie Lin",
            "Gang Xiong",
            "Gaopeng Gou",
            "Zhen Li",
            "Junzheng Shi",
            "Jing Yu"
        ],
        "published": "2022-02-13T14:54:48Z",
        "summary": "Encrypted traffic classification requires discriminative and robust traffic\nrepresentation captured from content-invisible and imbalanced traffic data for\naccurate classification, which is challenging but indispensable to achieve\nnetwork security and network management. The major limitation of existing\nsolutions is that they highly rely on the deep features, which are overly\ndependent on data size and hard to generalize on unseen data. How to leverage\nthe open-domain unlabeled traffic data to learn representation with strong\ngeneralization ability remains a key challenge. In this paper,we propose a new\ntraffic representation model called Encrypted Traffic Bidirectional Encoder\nRepresentations from Transformer (ET-BERT), which pre-trains deep\ncontextualized datagram-level representation from large-scale unlabeled data.\nThe pre-trained model can be fine-tuned on a small number of task-specific\nlabeled data and achieves state-of-the-art performance across five encrypted\ntraffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2%\n(4.4% absolute improvement), ISCX-VPN-Service to 98.9% (5.2% absolute\nimprovement), Cross-Platform (Android) to 92.5% (5.4% absolute improvement),\nCSTNET-TLS 1.3 to 97.4% (10.0% absolute improvement). Notably, we provide\nexplanation of the empirically powerful pre-training model by analyzing the\nrandomness of ciphers. It gives us insights in understanding the boundary of\nclassification ability over encrypted traffic. The code is available at:\nhttps://github.com/linwhitehat/ET-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2202.06335v2.pdf"
    },
    {
        "title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models",
        "authors": [
            "Yanchen Liu",
            "Timo Schick",
            "Hinrich Schütze"
        ],
        "published": "2022-02-12T19:50:59Z",
        "summary": "Due to the high costs associated with finetuning large language models,\nvarious recent works propose to adapt them to specific tasks without any\nparameter updates through in-context learning. Unfortunately, for in-context\nlearning there is currently no way to leverage unlabeled data, which is often\nmuch easier to obtain in large quantities than labeled examples. In this work,\nwe therefore investigate ways to make use of unlabeled examples to improve the\nzero-shot performance of pretrained language models without any finetuning: We\nintroduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies\nexamples by retrieving semantically similar unlabeled examples, assigning\nlabels to them in a zero-shot fashion, and then using them for in-context\nlearning. We also propose bag-of-contexts priming, a new priming strategy that\nis more suitable for our setting and enables the usage of more examples than\nfit into the context window.",
        "pdf_link": "https://arxiv.org/pdf/2202.06133v1.pdf"
    },
    {
        "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
        "authors": [
            "Yucheng Lu",
            "Conglong Li",
            "Minjia Zhang",
            "Christopher De Sa",
            "Yuxiong He"
        ],
        "published": "2022-02-12T08:02:23Z",
        "summary": "1-bit gradient compression and local steps are two representative techniques\nthat enable drastic communication reduction in distributed SGD. Their benefits,\nhowever, remain an open question on Adam-based large model pre-training (e.g.\nBERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes\nslow convergence even when 1-bit compression or local steps are individually\napplied. To alleviate this limitation, we propose 0/1 Adam that linearizes each\nAdam step via approximating its optimizer states using their stale estimates\nand linear correlation. 0/1 Adam performs an Adam-like step to preserve the\nadaptivity, while its linearity allows utilizing 1-bit compression and local\nsteps simultaneously for wall-clock time speed up. We provide convergence\nguarantee for 0/1 Adam on smooth non-convex objectives. On various large-scale\nbenchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we\ndemonstrate on up to 128 GPUs that 0/1 Adam is able to reduce up to 87% of data\nvolume, 54% of communication rounds, and achieve up to 2$\\times$ higher\ntraining throughput and end-to-end training time reduction compared to the\nstate-of-the-art baseline 1-bit Adam; while enjoying the same statistical\nconvergence speed and end task model accuracy on GLUE dataset and ImageNet\nvalidation set.",
        "pdf_link": "https://arxiv.org/pdf/2202.06009v3.pdf"
    },
    {
        "title": "What Does it Mean for a Language Model to Preserve Privacy?",
        "authors": [
            "Hannah Brown",
            "Katherine Lee",
            "Fatemehsadat Mireshghallah",
            "Reza Shokri",
            "Florian Tramèr"
        ],
        "published": "2022-02-11T09:18:27Z",
        "summary": "Natural language reflects our private lives and identities, making its\nprivacy concerns as broad as those of real life. Language models lack the\nability to understand the context and sensitivity of text, and tend to memorize\nphrases present in their training sets. An adversary can exploit this tendency\nto extract training data. Depending on the nature of the content and the\ncontext in which this data was collected, this could violate expectations of\nprivacy. Thus there is a growing interest in techniques for training language\nmodels that preserve privacy. In this paper, we discuss the mismatch between\nthe narrow assumptions made by popular data protection techniques (data\nsanitization and differential privacy), and the broadness of natural language\nand of privacy as a social norm. We argue that existing protection methods\ncannot guarantee a generic and meaningful notion of privacy for language\nmodels. We conclude that language models should be trained on text data which\nwas explicitly produced for public use.",
        "pdf_link": "https://arxiv.org/pdf/2202.05520v2.pdf"
    },
    {
        "title": "Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "published": "2022-02-09T17:26:08Z",
        "summary": "Topic models have been the prominent tools for automatic topic discovery from\ntext corpora. Despite their effectiveness, topic models suffer from several\nlimitations including the inability of modeling word ordering information in\ndocuments, the difficulty of incorporating external linguistic knowledge, and\nthe lack of both accurate and efficient inference methods for approximating the\nintractable posterior. Recently, pretrained language models (PLMs) have brought\nastonishing performance improvements to a wide variety of tasks due to their\nsuperior representations of text. Interestingly, there have not been standard\napproaches to deploy PLMs for topic discovery as better alternatives to topic\nmodels. In this paper, we begin by analyzing the challenges of using PLM\nrepresentations for topic discovery, and then propose a joint latent space\nlearning and clustering framework built upon PLM embeddings. In the latent\nspace, topic-word and document-topic distributions are jointly modeled so that\nthe discovered topics can be interpreted by coherent and distinctive terms and\nmeanwhile serve as meaningful summaries of the documents. Our model effectively\nleverages the strong representation power and superb linguistic features\nbrought by PLMs for topic discovery, and is conceptually simpler than topic\nmodels. On two benchmark datasets in different domains, our model generates\nsignificantly more coherent and diverse topics than strong topic models, and\noffers better topic-wise document representations, based on both automatic and\nhuman evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2202.04582v1.pdf"
    },
    {
        "title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models",
        "authors": [
            "Boxin Wang",
            "Wei Ping",
            "Chaowei Xiao",
            "Peng Xu",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bo Li",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "published": "2022-02-08T22:10:40Z",
        "summary": "Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2202.04173v3.pdf"
    },
    {
        "title": "Logical Reasoning for Task Oriented Dialogue Systems",
        "authors": [
            "Sajjad Beygi",
            "Maryam Fazel-Zarandi",
            "Alessandra Cervone",
            "Prakash Krishnan",
            "Siddhartha Reddy Jonnalagadda"
        ],
        "published": "2022-02-08T21:46:27Z",
        "summary": "In recent years, large pretrained models have been used in dialogue systems\nto improve successful task completion rates. However, lack of reasoning\ncapabilities of dialogue platforms make it difficult to provide relevant and\nfluent responses, unless the designers of a conversational experience spend a\nconsiderable amount of time implementing these capabilities in external rule\nbased modules. In this work, we propose a novel method to fine-tune pretrained\ntransformer models such as Roberta and T5. to reason over a set of facts in a\ngiven dialogue context. Our method includes a synthetic data generation\nmechanism which helps the model learn logical relations, such as comparison\nbetween list of numerical values, inverse relations (and negation), inclusion\nand exclusion for categorical attributes, and application of a combination of\nattributes over both numerical and categorical values, and spoken form for\nnumerical values, without need for additional training dataset. We show that\nthe transformer based model can perform logical reasoning to answer questions\nwhen the dialogue context contains all the required information, otherwise it\nis able to extract appropriate constraints to pass to downstream components\n(e.g. a knowledge base) when partial information is available. We observe that\ntransformer based models such as UnifiedQA-T5 can be fine-tuned to perform\nlogical reasoning (such as numerical and categorical attributes' comparison)\nover attributes that been seen in training time (e.g., accuracy of 90\\%+ for\ncomparison of smaller than $k_{\\max}$=5 values over heldout test dataset).",
        "pdf_link": "https://arxiv.org/pdf/2202.04161v1.pdf"
    },
    {
        "title": "Survey of Hallucination in Natural Language Generation",
        "authors": [
            "Ziwei Ji",
            "Nayeon Lee",
            "Rita Frieske",
            "Tiezheng Yu",
            "Dan Su",
            "Yan Xu",
            "Etsuko Ishii",
            "Yejin Bang",
            "Delong Chen",
            "Ho Shu Chan",
            "Wenliang Dai",
            "Andrea Madotto",
            "Pascale Fung"
        ],
        "published": "2022-02-08T03:55:01Z",
        "summary": "Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.",
        "pdf_link": "https://arxiv.org/pdf/2202.03629v6.pdf"
    },
    {
        "title": "Do Language Models Learn Position-Role Mappings?",
        "authors": [
            "Jackson Petty",
            "Michael Wilson",
            "Robert Frank"
        ],
        "published": "2022-02-08T02:50:53Z",
        "summary": "How is knowledge of position-role mappings in natural language learned? We\nexplore this question in a computational setting, testing whether a variety of\nwell-performing pertained language models (BERT, RoBERTa, and DistilBERT)\nexhibit knowledge of these mappings, and whether this knowledge persists across\nalternations in syntactic, structural, and lexical alternations. In Experiment\n1, we show that these neural models do indeed recognize distinctions between\ntheme and recipient roles in ditransitive constructions, and that these\ndistinct patterns are shared across construction type. We strengthen this\nfinding in Experiment 2 by showing that fine-tuning these language models on\nnovel theme- and recipient-like tokens in one paradigm allows the models to\nmake correct predictions about their placement in other paradigms, suggesting\nthat the knowledge of these mappings is shared rather than independently\nlearned. We do, however, observe some limitations of this generalization when\ntasks involve constructions with novel ditransitive verbs, hinting at a degree\nof lexical specificity which underlies model performance.",
        "pdf_link": "https://arxiv.org/pdf/2202.03611v1.pdf"
    },
    {
        "title": "Fine-Tuning Approach for Arabic Offensive Language Detection System: BERT-Based Model",
        "authors": [
            "Fatemah Husain",
            "Ozlem Uzuner"
        ],
        "published": "2022-02-07T17:26:35Z",
        "summary": "The problem of online offensive language limits the health and security of\nonline users. It is essential to apply the latest state-of-the-art techniques\nin developing a system to detect online offensive language and to ensure social\njustice to the online communities. Our study investigates the effects of\nfine-tuning across several Arabic offensive language datasets. We develop\nmultiple classifiers that use four datasets individually and in combination in\norder to gain knowledge about online Arabic offensive content and classify\nusers comments accordingly. Our results demonstrate the limited effects of\ntransfer learning on the classifiers performance, particularly for highly\ndialectal comments.",
        "pdf_link": "https://arxiv.org/pdf/2203.03542v1.pdf"
    },
    {
        "title": "Multilingual Hate Speech and Offensive Content Detection using Modified Cross-entropy Loss",
        "authors": [
            "Arka Mitra",
            "Priyanshu Sankhala"
        ],
        "published": "2022-02-05T20:31:40Z",
        "summary": "The number of increased social media users has led to a lot of people\nmisusing these platforms to spread offensive content and use hate speech.\nManual tracking the vast amount of posts is impractical so it is necessary to\ndevise automated methods to identify them quickly. Large language models are\ntrained on a lot of data and they also make use of contextual embeddings. We\nfine-tune the large language models to help in our task. The data is also quite\nunbalanced; so we used a modified cross-entropy loss to tackle the issue. We\nobserved that using a model which is fine-tuned in hindi corpora performs\nbetter. Our team (HNLP) achieved the macro F1-scores of 0.808, 0.639 in English\nSubtask A and English Subtask B respectively. For Hindi Subtask A, Hindi\nSubtask B our team achieved macro F1-scores of 0.737, 0.443 respectively in\nHASOC 2021.",
        "pdf_link": "https://arxiv.org/pdf/2202.02635v1.pdf"
    },
    {
        "title": "JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning",
        "authors": [
            "Yash Jakhotiya",
            "Vaibhav Kumar",
            "Ashwin Pathak",
            "Raj Shah"
        ],
        "published": "2022-02-04T21:17:41Z",
        "summary": "Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.",
        "pdf_link": "https://arxiv.org/pdf/2202.02394v6.pdf"
    },
    {
        "title": "Pop Quiz! Can a Large Language Model Help With Reverse Engineering?",
        "authors": [
            "Hammond Pearce",
            "Benjamin Tan",
            "Prashanth Krishnamurthy",
            "Farshad Khorrami",
            "Ramesh Karri",
            "Brendan Dolan-Gavitt"
        ],
        "published": "2022-02-02T17:09:15Z",
        "summary": "Large language models (such as OpenAI's Codex) have demonstrated impressive\nzero-shot multi-task capabilities in the software domain, including code\nexplanation. In this work, we examine if this ability can be used to help with\nreverse engineering. Specifically, we investigate prompting Codex to identify\nthe purpose, capabilities, and important variable names or values from code,\neven when the code is produced through decompilation. Alongside an examination\nof the model's responses in answering open-ended questions, we devise a\ntrue/false quiz framework to characterize the performance of the language\nmodel. We present an extensive quantitative analysis of the measured\nperformance of the language model on a set of program purpose identification\nand information extraction tasks: of the 136,260 questions we posed, it\nanswered 72,754 correctly. A key takeaway is that while promising, LLMs are not\nyet ready for zero-shot reverse engineering.",
        "pdf_link": "https://arxiv.org/pdf/2202.01142v1.pdf"
    },
    {
        "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records",
        "authors": [
            "Xi Yang",
            "Aokun Chen",
            "Nima PourNejatian",
            "Hoo Chang Shin",
            "Kaleb E Smith",
            "Christopher Parisien",
            "Colin Compas",
            "Cheryl Martin",
            "Mona G Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Christopher A Harle",
            "Gloria Lipori",
            "Duane A Mitchell",
            "William R Hogan",
            "Elizabeth A Shenkman",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2022-02-02T14:28:51Z",
        "summary": "There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using >90 billion words of text (including >82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.",
        "pdf_link": "https://arxiv.org/pdf/2203.03540v3.pdf"
    },
    {
        "title": "What Has Been Enhanced in my Knowledge-Enhanced Language Model?",
        "authors": [
            "Yifan Hou",
            "Guoji Fu",
            "Mrinmaya Sachan"
        ],
        "published": "2022-02-02T11:23:36Z",
        "summary": "Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.",
        "pdf_link": "https://arxiv.org/pdf/2202.00964v7.pdf"
    },
    {
        "title": "Co-training Improves Prompt-based Learning for Large Language Models",
        "authors": [
            "Hunter Lang",
            "Monica Agrawal",
            "Yoon Kim",
            "David Sontag"
        ],
        "published": "2022-02-02T00:48:26Z",
        "summary": "We demonstrate that co-training (Blum & Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.",
        "pdf_link": "https://arxiv.org/pdf/2202.00828v1.pdf"
    },
    {
        "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation",
        "authors": [
            "Biao Zhang",
            "Behrooz Ghorbani",
            "Ankur Bapna",
            "Yong Cheng",
            "Xavier Garcia",
            "Jonathan Shen",
            "Orhan Firat"
        ],
        "published": "2022-02-01T16:20:15Z",
        "summary": "Natural language understanding and generation models follow one of the two\ndominant architectural paradigms: language models (LMs) that process\nconcatenated sequences in a single stack of layers, and encoder-decoder models\n(EncDec) that utilize separate layer stacks for input and output processing. In\nmachine translation, EncDec has long been the favoured approach, but with few\nstudies investigating the performance of LMs. In this work, we thoroughly\nexamine the role of several architectural design choices on the performance of\nLMs on bilingual, (massively) multilingual and zero-shot translation tasks,\nunder systematic variations of data conditions and model sizes. Our results\nshow that: (i) Different LMs have different scaling properties, where\narchitectural differences often have a significant impact on model performance\nat small scales, but the performance gap narrows as the number of parameters\nincreases, (ii) Several design choices, including causal masking and\nlanguage-modeling objectives for the source sequence, have detrimental effects\non translation quality, and (iii) When paired with full-visible masking for\nsource sequences, LMs could perform on par with EncDec on supervised bilingual\nand multilingual translation tasks, and improve greatly on zero-shot directions\nby facilitating the reduction of off-target translations.",
        "pdf_link": "https://arxiv.org/pdf/2202.00528v3.pdf"
    },
    {
        "title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise",
        "authors": [
            "Minjia Zhang",
            "Niranjan Uma Naresh",
            "Yuxiong He"
        ],
        "published": "2022-01-29T01:47:01Z",
        "summary": "In recent years, large pre-trained Transformer-based language models have led\nto dramatic improvements in many natural language understanding tasks. To train\nthese models with increasing sizes, many neural network practitioners attempt\nto increase the batch sizes in order to leverage multiple GPUs to improve\ntraining speed. However, increasing the batch size often makes the optimization\nmore difficult, leading to slow convergence or poor generalization that can\nrequire orders of magnitude more training time to achieve the same model\nquality. In this paper, we explore the steepness of the loss landscape of\nlarge-batch optimization for adapting pre-trained Transformer-based language\nmodels to domain-specific tasks and find that it tends to be highly complex and\nirregular, posing challenges to generalization on downstream tasks.\n  To tackle this challenge, we propose ScaLA, a novel and efficient method to\naccelerate the adaptation speed of pre-trained transformer networks. Different\nfrom prior methods, we take a sequential game-theoretic approach by adding\nlightweight adversarial noise into large-batch optimization, which\nsignificantly improves adaptation speed while preserving model generalization.\nExperiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups\nover the baseline for GLUE on BERT-base and RoBERTa-large, while achieving\ncomparable and sometimes higher accuracy than the state-of-the-art large-batch\noptimization methods. Finally, we also address the theoretical aspect of\nlarge-batch optimization with adversarial noise and provide a theoretical\nconvergence rate analysis for ScaLA using techniques for analyzing non-convex\nsaddle-point problems.",
        "pdf_link": "https://arxiv.org/pdf/2201.12469v1.pdf"
    },
    {
        "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences",
        "authors": [
            "Yikuan Li",
            "Ramsey M. Wehbe",
            "Faraz S. Ahmad",
            "Hanyin Wang",
            "Yuan Luo"
        ],
        "published": "2022-01-27T22:51:58Z",
        "summary": "Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].",
        "pdf_link": "https://arxiv.org/pdf/2201.11838v3.pdf"
    },
    {
        "title": "Going Extreme: Comparative Analysis of Hate Speech in Parler and Gab",
        "authors": [
            "Abraham Israeli",
            "Oren Tsur"
        ],
        "published": "2022-01-27T19:29:17Z",
        "summary": "Social platforms such as Gab and Parler, branded as `free-speech' networks,\nhave seen a significant growth of their user base in recent years. This\npopularity is mainly attributed to the stricter moderation enforced by\nmainstream platforms such as Twitter, Facebook, and Reddit. In this work we\nprovide the first large scale analysis of hate-speech on Parler.\n  We experiment with an array of algorithms for hate-speech detection,\ndemonstrating limitations of transfer learning in that domain, given the\nillusive and ever changing nature of the ways hate-speech is delivered. In\norder to improve classification accuracy we annotated 10K Parler posts, which\nwe use to fine-tune a BERT classifier. Classification of individual posts is\nthen leveraged for the classification of millions of users via label\npropagation over the social network. Classifying users by their propensity to\ndisseminate hate, we find that hate mongers make 16.1\\% of Parler active users,\nand that they have distinct characteristics comparing to other user groups. We\nfind that hate mongers are more active, more central and express distinct\nlevels of sentiment and convey a distinct array of emotions like anger and\nsadness. We further complement our analysis by comparing the trends discovered\nin Parler and those found in Gab.\n  To the best of our knowledge, this is among the first works to analyze hate\nspeech in Parler in a quantitative manner and on the user level, and the first\nannotated dataset to be made available to the community.",
        "pdf_link": "https://arxiv.org/pdf/2201.11770v1.pdf"
    },
    {
        "title": "Synchromesh: Reliable code generation from pre-trained language models",
        "authors": [
            "Gabriel Poesia",
            "Oleksandr Polozov",
            "Vu Le",
            "Ashish Tiwari",
            "Gustavo Soares",
            "Christopher Meek",
            "Sumit Gulwani"
        ],
        "published": "2022-01-26T22:57:44Z",
        "summary": "Large pre-trained language models have been used to generate code,providing a\nflexible interface for synthesizing programs from natural language\nspecifications. However, they often violate syntactic and semantic rules of\ntheir output language, limiting their practical usability. In this paper, we\npropose Synchromesh: a framework for substantially improving the reliability of\npre-trained models for code generation. Synchromesh comprises two components.\nFirst, it retrieves few-shot examples from a training bank using Target\nSimilarity Tuning (TST), a novel method for semantic example selection. TST\nlearns to recognize utterances that describe similar target programs despite\ndifferences in surface natural language features. Then, Synchromesh feeds the\nexamples to a pre-trained language model and samples programs using Constrained\nSemantic Decoding (CSD): a general framework for constraining the output to a\nset of valid programs in the target language. CSD leverages constraints on\npartial outputs to sample complete correct programs, and needs neither\nre-training nor fine-tuning of the language model. We evaluate our methods by\nsynthesizing code from natural language descriptions using GPT-3 and Codex in\nthree real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow\nprograms. These domains showcase rich constraints that CSD is able to enforce,\nincluding syntax, scope, typing rules, and contextual logic. We observe\nsubstantial complementary gains from CSD and TST in prediction accuracy and in\neffectively preventing run-time errors.",
        "pdf_link": "https://arxiv.org/pdf/2201.11227v1.pdf"
    },
    {
        "title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence",
        "authors": [
            "Wei Zhao",
            "Michael Strube",
            "Steffen Eger"
        ],
        "published": "2022-01-26T20:28:26Z",
        "summary": "Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\nare weak in recognizing coherence, and thus are not reliable in a way to spot\nthe discourse-level improvements of those text generation systems. In this\nwork, we introduce DiscoScore, a parametrized discourse metric, which uses BERT\nto model discourse coherence from different perspectives, driven by Centering\ntheory. Our experiments encompass 16 non-discourse and discourse metrics,\nincluding DiscoScore and popular coherence models, evaluated on summarization\nand document-level machine translation (MT). We find that (i) the majority of\nBERT-based metrics correlate much worse with human rated coherence than early\ndiscourse metrics, invented a decade ago; (ii) the recent state-of-the-art\nBARTScore is weak when operated at system level -- which is particularly\nproblematic as systems are typically compared in this manner. DiscoScore, in\ncontrast, achieves strong system-level correlation with human ratings, not only\nin coherence but also in factual consistency and other aspects, and surpasses\nBARTScore by over 10 correlation points on average. Further, aiming to\nunderstand DiscoScore, we provide justifications to the importance of discourse\ncoherence for evaluation metrics, and explain the superiority of one variant\nover another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.",
        "pdf_link": "https://arxiv.org/pdf/2201.11176v4.pdf"
    },
    {
        "title": "Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs",
        "authors": [
            "Anna Filighera",
            "Sebastian Ochs",
            "Tim Steuer",
            "Thomas Tregel"
        ],
        "published": "2022-01-20T17:34:33Z",
        "summary": "Automatic grading models are valued for the time and effort saved during the\ninstruction of large student bodies. Especially with the increasing\ndigitization of education and interest in large-scale standardized testing, the\npopularity of automatic grading has risen to the point where commercial\nsolutions are widely available and used. However, for short answer formats,\nautomatic grading is challenging due to natural language ambiguity and\nversatility. While automatic short answer grading models are beginning to\ncompare to human performance on some datasets, their robustness, especially to\nadversarially manipulated data, is questionable. Exploitable vulnerabilities in\ngrading models can have far-reaching consequences ranging from cheating\nstudents receiving undeserved credit to undermining automatic grading\naltogether - even when most predictions are valid. In this paper, we devise a\nblack-box adversarial attack tailored to the educational short answer grading\nscenario to investigate the grading models' robustness. In our attack, we\ninsert adjectives and adverbs into natural places of incorrect student answers,\nfooling the model into predicting them as correct. We observed a loss of\nprediction accuracy between 10 and 22 percentage points using the\nstate-of-the-art models BERT and T5. While our attack made answers appear less\nnatural to humans in our experiments, it did not significantly increase the\ngraders' suspicions of cheating. Based on our experiments, we provide\nrecommendations for utilizing automatic grading systems more safely in\npractice.",
        "pdf_link": "https://arxiv.org/pdf/2201.08318v2.pdf"
    },
    {
        "title": "AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees",
        "authors": [
            "Rong Liang",
            "Tiehua Zhang",
            "Yujie Lu",
            "Yuze Liu",
            "Zhen Huang",
            "Xin Chen"
        ],
        "published": "2022-01-20T03:27:26Z",
        "summary": "Using the pre-trained language models to understand source codes has\nattracted increasing attention from financial institutions owing to the great\npotential to uncover financial risks. However, there are several challenges in\napplying these language models to solve programming language-related problems\ndirectly. For instance, the shift of domain knowledge between natural language\n(NL) and programming language (PL) requires understanding the semantic and\nsyntactic information from the data from different perspectives. To this end,\nwe propose the AstBERT model, a pre-trained PL model aiming to better\nunderstand the financial codes using the abstract syntax tree (AST).\nSpecifically, we collect a sheer number of source codes (both Java and Python)\nfrom the Alipay code repository and incorporate both syntactic and semantic\ncode knowledge into our model through the help of code parsers, in which AST\ninformation of the source codes can be interpreted and integrated. We evaluate\nthe performance of the proposed model on three tasks, including code question\nanswering, code clone detection and code refinement. Experiment results show\nthat our AstBERT achieves promising performance on three different downstream\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.07984v4.pdf"
    },
    {
        "title": "Unveiling Project-Specific Bias in Neural Code Models",
        "authors": [
            "Zhiming Li",
            "Yanzhou Li",
            "Tianlin Li",
            "Mengnan Du",
            "Bozhi Wu",
            "Yushi Cao",
            "Junzhe Jiang",
            "Yang Liu"
        ],
        "published": "2022-01-19T02:09:48Z",
        "summary": "Deep learning has introduced significant improvements in many software\nanalysis tasks. Although the Large Language Models (LLMs) based neural code\nmodels demonstrate commendable performance when trained and tested within the\nintra-project independent and identically distributed (IID) setting, they often\nstruggle to generalize effectively to real-world inter-project\nout-of-distribution (OOD) data. In this work, we show that this phenomenon is\ncaused by the heavy reliance on project-specific shortcuts for prediction\ninstead of ground-truth evidence. We propose a Cond-Idf measurement to\ninterpret this behavior, which quantifies the relatedness of a token with a\nlabel and its project-specificness. The strong correlation between model\nbehavior and the proposed measurement indicates that without proper\nregularization, models tend to leverage spurious statistical cues for\nprediction. Equipped with these observations, we propose a novel bias\nmitigation mechanism that regularizes the model's learning behavior by\nleveraging latent logic relations among samples. Experimental results on two\nrepresentative program analysis tasks indicate that our mitigation framework\ncan improve both inter-project OOD generalization and adversarial robustness,\nwhile not sacrificing accuracy on intra-project IID data.",
        "pdf_link": "https://arxiv.org/pdf/2201.07381v2.pdf"
    },
    {
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "authors": [
            "Wenlong Huang",
            "Pieter Abbeel",
            "Deepak Pathak",
            "Igor Mordatch"
        ],
        "published": "2022-01-18T18:59:45Z",
        "summary": "Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner",
        "pdf_link": "https://arxiv.org/pdf/2201.07207v2.pdf"
    },
    {
        "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities",
        "authors": [
            "Mina Lee",
            "Percy Liang",
            "Qian Yang"
        ],
        "published": "2022-01-18T07:51:57Z",
        "summary": "Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs' generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3's\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3's language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing \"collaborator\" under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs' promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.",
        "pdf_link": "https://arxiv.org/pdf/2201.06796v2.pdf"
    },
    {
        "title": "Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records",
        "authors": [
            "Shuai Niu",
            "Qing Yin",
            "Yunya Song",
            "Yike Guo",
            "Xian Yang"
        ],
        "published": "2022-01-18T07:21:20Z",
        "summary": "Disease risk prediction has attracted increasing attention in the field of\nmodern healthcare, especially with the latest advances in artificial\nintelligence (AI). Electronic health records (EHRs), which contain\nheterogeneous patient information, are widely used in disease risk prediction\ntasks. One challenge of applying AI models for risk prediction lies in\ngenerating interpretable evidence to support the prediction results while\nretaining the prediction ability. In order to address this problem, we propose\nthe method of jointly embedding words and labels whereby attention modules\nlearn the weights of words from medical notes according to their relevance to\nthe names of risk prediction labels. This approach boosts interpretability by\nemploying an attention mechanism and including the names of prediction tasks in\nthe model. However, its application is only limited to the handling of textual\ninputs such as medical notes. In this paper, we propose a label dependent\nattention model LDAM to 1) improve the interpretability by exploiting\nClinical-BERT (a biomedical language model pre-trained on a large clinical\ncorpus) to encode biomedically meaningful features and labels jointly; 2)\nextend the idea of joint embedding to the processing of time-series data, and\ndevelop a multi-modal learning framework for integrating heterogeneous\ninformation from medical notes and time-series health status indicators. To\ndemonstrate our method, we apply LDAM to the MIMIC-III dataset to predict\ndifferent disease risks. We evaluate our method both quantitatively and\nqualitatively. Specifically, the predictive power of LDAM will be shown, and\ncase studies will be carried out to illustrate its interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2201.06779v1.pdf"
    },
    {
        "title": "Hierarchical Neural Network Approaches for Long Document Classification",
        "authors": [
            "Snehal Khandve",
            "Vedangi Wagh",
            "Apurva Wani",
            "Isha Joshi",
            "Raviraj Joshi"
        ],
        "published": "2022-01-18T07:17:40Z",
        "summary": "Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.",
        "pdf_link": "https://arxiv.org/pdf/2201.06774v1.pdf"
    },
    {
        "title": "Unintended Bias in Language Model-driven Conversational Recommendation",
        "authors": [
            "Tianshu Shen",
            "Jiaru Li",
            "Mohamed Reda Bouadjenek",
            "Zheda Mai",
            "Scott Sanner"
        ],
        "published": "2022-01-17T05:50:14Z",
        "summary": "Conversational Recommendation Systems (CRSs) have recently started to\nleverage pretrained language models (LM) such as BERT for their ability to\nsemantically interpret a wide range of preference statement variations.\nHowever, pretrained LMs are well-known to be prone to intrinsic biases in their\ntraining data, which may be exacerbated by biases embedded in domain-specific\nlanguage data(e.g., user reviews) used to fine-tune LMs for CRSs. We study a\nrecently introduced LM-driven recommendation backbone (termed LMRec) of a CRS\nto investigate how unintended bias i.e., language variations such as name\nreferences or indirect indicators of sexual orientation or location that should\nnot affect recommendations manifests in significantly shifted price and\ncategory distributions of restaurant recommendations. The alarming results we\nobserve strongly indicate that LMRec has learned to reinforce harmful\nstereotypes through its recommendations. For example, offhand mention of names\nassociated with the black community significantly lowers the price distribution\nof recommended restaurants, while offhand mentions of common male-associated\nnames lead to an increase in recommended alcohol-serving establishments. These\nand many related results presented in this work raise a red flag that advances\nin the language handling capability of LM-drivenCRSs do not come without\nsignificant challenges related to mitigating unintended bias in future deployed\nCRS assistants with a potential reach of hundreds of millions of end-users.",
        "pdf_link": "https://arxiv.org/pdf/2201.06224v2.pdf"
    },
    {
        "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Peter Clark",
            "Yiming Yang"
        ],
        "published": "2022-01-16T10:11:37Z",
        "summary": "Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homophone, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. Code, data, and\ninstructions to implement MEMPROMPT for a new task at\nhttps://www.memprompt.com/.",
        "pdf_link": "https://arxiv.org/pdf/2201.06009v7.pdf"
    },
    {
        "title": "The Dark Side of the Language: Pre-trained Transformers in the DarkNet",
        "authors": [
            "Leonardo Ranaldi",
            "Aria Nourbakhsh",
            "Arianna Patrizi",
            "Elena Sofia Ruzzetti",
            "Dario Onorati",
            "Francesca Fallucchi",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2022-01-14T16:04:09Z",
        "summary": "Pre-trained Transformers are challenging human performances in many NLP\ntasks. The massive datasets used for pre-training seem to be the key to their\nsuccess on existing tasks. In this paper, we explore how a range of pre-trained\nNatural Language Understanding models perform on definitely unseen sentences\nprovided by classification tasks over a DarkNet corpus. Surprisingly, results\nshow that syntactic and lexical neural networks perform on par with pre-trained\nTransformers even after fine-tuning. Only after what we call extreme domain\nadaptation, that is, retraining with the masked language model task on all the\nnovel corpus, pre-trained Transformers reach their standard high results. This\nsuggests that huge pre-training corpora may give Transformers unexpected help\nsince they are exposed to many of the possible sentences.",
        "pdf_link": "https://arxiv.org/pdf/2201.05613v3.pdf"
    },
    {
        "title": "A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models",
        "authors": [
            "Hanqing Zhang",
            "Haolin Song",
            "Shaoyu Li",
            "Ming Zhou",
            "Dawei Song"
        ],
        "published": "2022-01-14T08:32:20Z",
        "summary": "Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that better meet the specific constraints\nin practical applications. In recent years, methods using large-scale\npre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the limited level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks that require different types of controlled\nconstraints. In this paper, we present a systematic critical review on the\ncommon tasks, main approaches, and evaluation methods in this area. Finally, we\ndiscuss the challenges that the field is facing, and put forward various\npromising future directions. To the best of our knowledge, this is the first\nsurvey paper to summarize the state-of-the-art CTG techniques from the\nperspective of Transformer-based PLMs. We hope it can help researchers and\npractitioners in the related fields to quickly track the academic and\ntechnological frontier, providing them with a landscape of the area and a\nroadmap for future research.",
        "pdf_link": "https://arxiv.org/pdf/2201.05337v5.pdf"
    },
    {
        "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification",
        "authors": [
            "Alon Talmor",
            "Ori Yoran",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yoav Goldberg",
            "Yejin Choi",
            "Jonathan Berant"
        ],
        "published": "2022-01-14T06:49:15Z",
        "summary": "Constructing benchmarks that test the abilities of modern natural language\nunderstanding models is difficult - pre-trained language models exploit\nartifacts in benchmarks to achieve human parity, but still fail on adversarial\nexamples and make errors that demonstrate a lack of common sense. In this work,\nwe propose gamification as a framework for data construction. The goal of\nplayers in the game is to compose questions that mislead a rival AI while using\nspecific phrases for extra points. The game environment leads to enhanced user\nengagement and simultaneously gives the game designer control over the\ncollected data, allowing us to collect high-quality data at scale. Using our\nmethod we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and\ndemonstrate its difficulty for models that are orders-of-magnitude larger than\nthe AI used in the game itself. Our best baseline, the T5-based Unicorn with\n11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3\n(52.9%) in a few-shot inference setup. Both score well below human performance\nwhich is at 94.1%.",
        "pdf_link": "https://arxiv.org/pdf/2201.05320v1.pdf"
    },
    {
        "title": "Quantifying Robustness to Adversarial Word Substitutions",
        "authors": [
            "Yuting Yang",
            "Pei Huang",
            "FeiFei Ma",
            "Juan Cao",
            "Meishan Zhang",
            "Jian Zhang",
            "Jintao Li"
        ],
        "published": "2022-01-11T08:18:39Z",
        "summary": "Deep-learning-based NLP models are found to be vulnerable to word\nsubstitution perturbations. Before they are widely adopted, the fundamental\nissues of robustness need to be addressed. Along this line, we propose a formal\nframework to evaluate word-level robustness. First, to study safe regions for a\nmodel, we introduce robustness radius which is the boundary where the model can\nresist any perturbation. As calculating the maximum robustness radius is\ncomputationally hard, we estimate its upper and lower bound. We repurpose\nattack methods as ways of seeking upper bound and design a pseudo-dynamic\nprogramming algorithm for a tighter upper bound. Then verification method is\nutilized for a lower bound. Further, for evaluating the robustness of regions\noutside a safe radius, we reexamine robustness from another view:\nquantification. A robustness metric with a rigorous statistical guarantee is\nintroduced to measure the quantification of adversarial examples, which\nindicates the model's susceptibility to perturbations outside the safe radius.\nThe metric helps us figure out why state-of-the-art models like BERT can be\neasily fooled by a few word substitutions, but generalize well in the presence\nof real-world noises.",
        "pdf_link": "https://arxiv.org/pdf/2201.03829v1.pdf"
    },
    {
        "title": "Latency Adjustable Transformer Encoder for Language Understanding",
        "authors": [
            "Sajjad Kachuee",
            "Mohammad Sharifkhani"
        ],
        "published": "2022-01-10T13:04:39Z",
        "summary": "Adjusting the latency, power, and accuracy of natural language understanding\nmodels is a desirable objective of an efficient architecture. This paper\nproposes an efficient Transformer architecture that adjusts the inference\ncomputational cost adaptively with a desired inference latency speedup. In\nfine-tuning phase, the proposed method detects less important hidden sequence\nelements (word-vectors) and eliminates them in each encoder layer using a\nproposed Attention Context Contribution (ACC) metric. After the fine-tuning\nphase, with the novel offline-tuning property, the inference latency of the\nmodel can be adjusted in a wide range of inference speedup selections without\nany further training. The proposed method is applied to the BERT-base and GPT-2\nmodels for evaluation. Extensive experiments show that most of the word-vectors\nin higher Transformer layers have less contribution to the subsequent layers;\nhence, they can be eliminated to improve the inference latency. Experimental\nresults on extensive sentiment analysis, classification, text generation tasks\nand regression benchmarks like GLUE showed that the method is effective in\nvarious datasets with minimal impact on global context. The proposed method\nmathematically and experimentally improves the inference latency of BERT-base\nand GPT-2 by up to 4.8 and 3.72 times with less than 0.75% accuracy drop and\npassable perplexity on average. The suggested approach posits that in Large\nLanguage Models (LLMs), although the complete network is necessary for\ntraining, it can be truncated during the fine-tuning phase.",
        "pdf_link": "https://arxiv.org/pdf/2201.03327v7.pdf"
    },
    {
        "title": "Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow",
        "authors": [
            "Maarten Sap",
            "Anna Jafarpour",
            "Yejin Choi",
            "Noah A. Smith",
            "James W. Pennebaker",
            "Eric Horvitz"
        ],
        "published": "2022-01-07T20:10:47Z",
        "summary": "Lifelong experiences and learned knowledge lead to shared expectations about\nhow common situations tend to unfold. Such knowledge of narrative event flow\nenables people to weave together a story. However, comparable computational\ntools to evaluate the flow of events in narratives are limited. We quantify the\ndifferences between autobiographical and imagined stories by introducing\nsequentiality, a measure of narrative flow of events, drawing probabilistic\ninferences from a cutting-edge large language model (GPT-3). Sequentiality\ncaptures the flow of a narrative by comparing the probability of a sentence\nwith and without its preceding story context. We applied our measure to study\nthousands of diary-like stories, collected from crowdworkers about either a\nrecent remembered experience or an imagined story on the same topic. The\nresults show that imagined stories have higher sequentiality than\nautobiographical stories and that the sequentiality of autobiographical stories\nincreases when the memories are retold several months later. In pursuit of\ndeeper understandings of how sequentiality measures the flow of narratives, we\nexplore proportions of major and minor events in story sentences, as annotated\nby crowdworkers. We find that lower sequentiality is associated with higher\nproportions of major events. The methods and results highlight opportunities to\nuse cutting-edge computational analyses, such as sequentiality, on large\ncorpora of matched imagined and autobiographical stories to investigate the\ninfluences of memory and reasoning on language generation processes.",
        "pdf_link": "https://arxiv.org/pdf/2201.02662v2.pdf"
    },
    {
        "title": "Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model",
        "authors": [
            "Jinchuan Tian",
            "Jianwei Yu",
            "Chao Weng",
            "Yuexian Zou",
            "Dong Yu"
        ],
        "published": "2022-01-06T10:04:56Z",
        "summary": "Despite the rapid progress of end-to-end (E2E) automatic speech recognition\n(ASR), it has been shown that incorporating external language models (LMs) into\nthe decoding can further improve the recognition performance of E2E ASR\nsystems. To align with the modeling units adopted in E2E ASR systems,\nsubword-level (e.g., characters, BPE) LMs are usually used to cooperate with\ncurrent E2E ASR systems. However, the use of subword-level LMs will ignore the\nword-level information, which may limit the strength of the external LMs in E2E\nASR. Although several methods have been proposed to incorporate word-level\nexternal LMs in E2E ASR, these methods are mainly designed for languages with\nclear word boundaries such as English and cannot be directly applied to\nlanguages like Mandarin, in which each character sequence can have multiple\ncorresponding word sequences. To this end, we propose a novel decoding\nalgorithm where a word-level lattice is constructed on-the-fly to consider all\npossible word sequences for each partial hypothesis. Then, the LM score of the\nhypothesis is obtained by intersecting the generated lattice with an external\nword N-gram LM. The proposed method is examined on both Attention-based\nEncoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments\nsuggest that our method consistently outperforms subword-level LMs, including\nN-gram LM and neural network LM. We achieve state-of-the-art results on both\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by\n14.8% relatively on a 21K-hour Mandarin dataset.",
        "pdf_link": "https://arxiv.org/pdf/2201.01995v1.pdf"
    },
    {
        "title": "Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models",
        "authors": [
            "Diana Kim",
            "Ahmed Elgammal",
            "Marian Mazzone"
        ],
        "published": "2022-01-05T21:03:29Z",
        "summary": "We present a machine learning system that can quantify fine art paintings\nwith a set of visual elements and principles of art. This formal analysis is\nfundamental for understanding art, but developing such a system is challenging.\nPaintings have high visual complexities, but it is also difficult to collect\nenough training data with direct labels. To resolve these practical\nlimitations, we introduce a novel mechanism, called proxy learning, which\nlearns visual concepts in paintings though their general relation to styles.\nThis framework does not require any visual annotation, but only uses style\nlabels and a general relationship between visual concepts and style. In this\npaper, we propose a novel proxy model and reformulate four pre-existing methods\nin the context of proxy learning. Through quantitative and qualitative\ncomparison, we evaluate these methods and compare their effectiveness in\nquantifying the artistic visual concepts, where the general relationship is\nestimated by language models; GloVe or BERT. The language modeling is a\npractical and scalable solution requiring no labeling, but it is inevitably\nimperfect. We demonstrate how the new proxy model is robust to the\nimperfection, while the other models are sensitively affected by it.",
        "pdf_link": "https://arxiv.org/pdf/2201.01819v1.pdf"
    },
    {
        "title": "Submix: Practical Private Prediction for Large-Scale Language Models",
        "authors": [
            "Antonio Ginart",
            "Laurens van der Maaten",
            "James Zou",
            "Chuan Guo"
        ],
        "published": "2022-01-04T04:23:38Z",
        "summary": "Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.",
        "pdf_link": "https://arxiv.org/pdf/2201.00971v1.pdf"
    },
    {
        "title": "On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations",
        "authors": [
            "Aamir Miyajiwala",
            "Arnav Ladkat",
            "Samiksha Jagadale",
            "Raviraj Joshi"
        ],
        "published": "2022-01-02T08:33:49Z",
        "summary": "Text classification is a fundamental Natural Language Processing task that\nhas a wide variety of applications, where deep learning approaches have\nproduced state-of-the-art results. While these models have been heavily\ncriticized for their black-box nature, their robustness to slight perturbations\nin input text has been a matter of concern. In this work, we carry out a\ndata-focused study evaluating the impact of systematic practical perturbations\non the performance of the deep learning based text classification models like\nCNN, LSTM, and BERT-based algorithms. The perturbations are induced by the\naddition and removal of unwanted tokens like punctuation and stop-words that\nare minimally associated with the final performance of the model. We show that\nthese deep learning approaches including BERT are sensitive to such legitimate\ninput perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,\nand tweet_eval. We observe that BERT is more susceptible to the removal of\ntokens as compared to the addition of tokens. Moreover, LSTM is slightly more\nsensitive to input perturbations as compared to CNN based model. The work also\nserves as a practical guide to assessing the impact of discrepancies in\ntrain-test conditions on the final performance of models.",
        "pdf_link": "https://arxiv.org/pdf/2201.00318v2.pdf"
    }
]