[
    {
        "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
        "authors": [
            "B. A. Levinstein",
            "Daniel A. Herrmann"
        ],
        "published": "2023-06-30T23:44:51Z",
        "summary": "We consider the questions of whether or not large language models (LLMs) have\nbeliefs, and, if they do, how we might measure them. First, we evaluate two\nexisting approaches, one due to Azaria and Mitchell (2023) and the other to\nBurns et al. (2022). We provide empirical results that show that these methods\nfail to generalize in very basic ways. We then argue that, even if LLMs have\nbeliefs, these methods are unlikely to be successful for conceptual reasons.\nThus, there is still no lie-detector for LLMs. After describing our empirical\nresults we take a step back and consider whether or not we should expect LLMs\nto have something like beliefs in the first place. We consider some recent\narguments aiming to show that LLMs cannot have beliefs. We show that these\narguments are misguided. We provide a more productive framing of questions\nsurrounding the status of beliefs in LLMs, and highlight the empirical nature\nof the problem. We conclude by suggesting some concrete paths for future work.",
        "pdf_link": "https://arxiv.org/pdf/2307.00175v1.pdf"
    },
    {
        "title": "Large Language Models (GPT) for automating feedback on programming assignments",
        "authors": [
            "Maciej Pankiewicz",
            "Ryan S. Baker"
        ],
        "published": "2023-06-30T21:57:40Z",
        "summary": "Addressing the challenge of generating personalized feedback for programming\nassignments is demanding due to several factors, like the complexity of code\nsyntax or different ways to correctly solve a task. In this experimental study,\nwe automated the process of feedback generation by employing OpenAI's GPT-3.5\nmodel to generate personalized hints for students solving programming\nassignments on an automated assessment platform. Students rated the usefulness\nof GPT-generated hints positively. The experimental group (with GPT hints\nenabled) relied less on the platform's regular feedback but performed better in\nterms of percentage of successful submissions across consecutive attempts for\ntasks, where GPT hints were enabled. For tasks where the GPT feedback was made\nunavailable, the experimental group needed significantly less time to solve\nassignments. Furthermore, when GPT hints were unavailable, students in the\nexperimental condition were initially less likely to solve the assignment\ncorrectly. This suggests potential over-reliance on GPT-generated feedback.\nHowever, students in the experimental condition were able to correct reasonably\nrapidly, reaching the same percentage correct after seven submission attempts.\nThe availability of GPT hints did not significantly impact students' affective\nstate.",
        "pdf_link": "https://arxiv.org/pdf/2307.00150v1.pdf"
    },
    {
        "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
        "authors": [
            "Aaron Mueller",
            "Kanika Narang",
            "Lambert Mathias",
            "Qifan Wang",
            "Hamed Firooz"
        ],
        "published": "2023-06-30T20:16:22Z",
        "summary": "Large language models show impressive results on few-shot NLP tasks. However,\nthese models are memory and computation-intensive. Meta-training allows one to\nleverage smaller models for few-shot generalization in a domain-general and\ntask-agnostic manner; however, these methods alone results in models that may\nnot have sufficient parameterization or knowledge to adapt quickly to a large\nvariety of tasks. To overcome this issue, we propose meta-training with\ndemonstration retrieval, where we use a dense passage retriever to retrieve\nsemantically similar labeled demonstrations to each example for more varied\nsupervision. By separating external knowledge from model parameters, we can use\nmeta-training to train parameter-efficient models that generalize well on a\nlarger variety of tasks. We construct a meta-training set from UnifiedQA and\nCrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our\nknowledge, our work is the first to combine retrieval with meta-training, to\nuse DPR models to retrieve demonstrations, and to leverage demonstrations from\nmany tasks simultaneously, rather than randomly sampling demonstrations from\nthe training set of the target task. Our approach outperforms a variety of\ntargeted parameter-efficient and retrieval-augmented few-shot methods on QA,\nNLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our\napproach can be meta-trained and fine-tuned quickly on a single GPU.",
        "pdf_link": "https://arxiv.org/pdf/2307.00119v1.pdf"
    },
    {
        "title": "Ticket-BERT: Labeling Incident Management Tickets with Language Models",
        "authors": [
            "Zhexiong Liu",
            "Cris Benge",
            "Siduo Jiang"
        ],
        "published": "2023-06-30T19:48:25Z",
        "summary": "An essential aspect of prioritizing incident tickets for resolution is\nefficiently labeling tickets with fine-grained categories. However, ticket data\nis often complex and poses several unique challenges for modern machine\nlearning methods: (1) tickets are created and updated either by machines with\npre-defined algorithms or by engineers with domain expertise that share\ndifferent protocols, (2) tickets receive frequent revisions that update ticket\nstatus by modifying all or parts of ticket descriptions, and (3) ticket\nlabeling is time-sensitive and requires knowledge updates and new labels per\nthe rapid software and hardware improvement lifecycle. To handle these issues,\nwe introduce Ticket- BERT which trains a simple yet robust language model for\nlabeling tickets using our proposed ticket datasets. Experiments demonstrate\nthe superiority of Ticket-BERT over baselines and state-of-the-art text\nclassifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT\nwith an active learning cycle and deploy it on the Microsoft IcM system, which\nenables the model to quickly finetune on newly-collected tickets with a few\nannotations.",
        "pdf_link": "https://arxiv.org/pdf/2307.00108v1.pdf"
    },
    {
        "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
        "authors": [
            "Harnoor Dhingra",
            "Preetiha Jayashanker",
            "Sayali Moghe",
            "Emma Strubell"
        ],
        "published": "2023-06-30T19:39:01Z",
        "summary": "Large Language Models (LLMs) are trained primarily on minimally processed web\ntext, which exhibits the same wide range of social biases held by the humans\nwho created that content. Consequently, text generated by LLMs can\ninadvertently perpetuate stereotypes towards marginalized groups, like the\nLGBTQIA+ community. In this paper, we perform a comparative study of how LLMs\ngenerate text describing people with different sexual identities. Analyzing\nbias in the text generated by an LLM using regard score shows measurable bias\nagainst queer people. We then show that a post-hoc method based on\nchain-of-thought prompting using SHAP analysis can increase the regard of the\nsentence, representing a promising approach towards debiasing the output of\nLLMs in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2307.00101v1.pdf"
    },
    {
        "title": "Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Pei Zhang",
            "Baosong Yang",
            "Rui Wang"
        ],
        "published": "2023-06-30T17:38:10Z",
        "summary": "Neural-symbolic methods have demonstrated efficiency in enhancing the\nreasoning abilities of large language models (LLMs). However, existing methods\nmainly rely on syntactically mapping natural languages to complete formal\nlanguages like Python and SQL. Those methods require that reasoning tasks be\nconvertible into programs, which cater to the computer execution mindset and\ndeviate from human reasoning habits. To broaden symbolic methods' applicability\nand adaptability in the real world, we propose the Meta-Reasoning from a\nlinguistic perspective. This method empowers LLMs to deconstruct\nreasoning-independent semantic information into generic symbolic\nrepresentations, thereby efficiently capturing more generalized reasoning\nknowledge. We conduct extensive experiments on more than ten datasets\nencompassing conventional reasoning tasks like arithmetic, symbolic, and\nlogical reasoning, and the more complex interactive reasoning tasks like\ntheory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning\nsignificantly enhances in-context reasoning accuracy, learning efficiency,\nout-of-domain generalization, and output stability compared to the\nChain-of-Thought technique. Code and data are publicly available at\n\\url{https://github.com/Alsace08/Meta-Reasoning}.",
        "pdf_link": "https://arxiv.org/pdf/2306.17820v3.pdf"
    },
    {
        "title": "Biomedical Language Models are Robust to Sub-optimal Tokenization",
        "authors": [
            "Bernal Jim\u00e9nez Guti\u00e9rrez",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-06-30T13:35:24Z",
        "summary": "As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.",
        "pdf_link": "https://arxiv.org/pdf/2306.17649v3.pdf"
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
        "authors": [
            "Zhen Qin",
            "Rolf Jagerman",
            "Kai Hui",
            "Honglei Zhuang",
            "Junru Wu",
            "Le Yan",
            "Jiaming Shen",
            "Tianqi Liu",
            "Jialu Liu",
            "Donald Metzler",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "published": "2023-06-30T11:32:25Z",
        "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.",
        "pdf_link": "https://arxiv.org/pdf/2306.17563v2.pdf"
    },
    {
        "title": "Preference Ranking Optimization for Human Alignment",
        "authors": [
            "Feifan Song",
            "Bowen Yu",
            "Minghao Li",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li",
            "Houfeng Wang"
        ],
        "published": "2023-06-30T09:07:37Z",
        "summary": "Large language models (LLMs) often contain misleading content, emphasizing\nthe need to align them with human values to ensure secure AI systems.\nReinforcement learning from human feedback (RLHF) has been employed to achieve\nthis alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits\ncomplexity, instability, and sensitivity to hyperparameters in contrast to SFT.\n(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise\ncontrast, thus lacking contrasts from a macro perspective. In this paper, we\npropose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to\ndirectly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast\nto accommodate preference rankings of any length. By iteratively contrasting\ncandidates, PRO instructs the LLM to prioritize the best response while\nprogressively ranking the rest responses. In this manner, PRO effectively\ntransforms human alignment into aligning the probability ranking of n responses\ngenerated by LLM with the preference ranking of humans towards these responses.\nExperiments have shown that PRO outperforms baseline algorithms, achieving\ncomparable results to ChatGPT and human responses through automatic-based,\nreward-based, GPT-4, and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2306.17492v2.pdf"
    },
    {
        "title": "Provable Robust Watermarking for AI-Generated Text",
        "authors": [
            "Xuandong Zhao",
            "Prabhanjan Ananth",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "published": "2023-06-30T07:24:32Z",
        "summary": "We study the problem of watermarking large language models (LLMs) generated\ntext -- one of the most promising approaches for addressing the safety\nchallenges of LLM usage. In this paper, we propose a rigorous theoretical\nframework to quantify the effectiveness and robustness of LLM watermarks. We\npropose a robust and high-quality watermark method, Unigram-Watermark, by\nextending an existing approach with a simplified fixed grouping strategy. We\nprove that our watermark method enjoys guaranteed generation quality,\ncorrectness in watermark detection, and is robust against text editing and\nparaphrasing. Experiments on three varying LLMs and two datasets verify that\nour Unigram-Watermark achieves superior detection accuracy and comparable\ngeneration quality in perplexity, thus promoting the responsible use of LLMs.\nCode is available at https://github.com/XuandongZhao/Unigram-Watermark.",
        "pdf_link": "https://arxiv.org/pdf/2306.17439v2.pdf"
    },
    {
        "title": "LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection",
        "authors": [
            "Zijian Cai",
            "Zhaoxuan Tan",
            "Zhenyu Lei",
            "Zifeng Zhu",
            "Hongrui Wang",
            "Qinghua Zheng",
            "Minnan Luo"
        ],
        "published": "2023-06-30T05:50:26Z",
        "summary": "As malicious actors employ increasingly advanced and widespread bots to\ndisseminate misinformation and manipulate public opinion, the detection of\nTwitter bots has become a crucial task. Though graph-based Twitter bot\ndetection methods achieve state-of-the-art performance, we find that their\ninference depends on the neighbor users multi-hop away from the targets, and\nfetching neighbors is time-consuming and may introduce bias. At the same time,\nwe find that after finetuning on Twitter bot detection, pretrained language\nmodels achieve competitive performance and do not require a graph structure\nduring deployment. Inspired by this finding, we propose a novel bot detection\nframework LMBot that distills the knowledge of graph neural networks (GNNs)\ninto language models (LMs) for graph-less deployment in Twitter bot detection\nto combat the challenge of data dependency. Moreover, LMBot is compatible with\ngraph-based and graph-less datasets. Specifically, we first represent each user\nas a textual sequence and feed them into the LM for domain adaptation. For\ngraph-based datasets, the output of LMs provides input features for the GNN,\nenabling it to optimize for bot detection and distill knowledge back to the LM\nin an iterative, mutually enhancing process. Armed with the LM, we can perform\ngraph-less inference, which resolves the graph data dependency and sampling\nbias issues. For datasets without graph structure, we simply replace the GNN\nwith an MLP, which has also shown strong performance. Our experiments\ndemonstrate that LMBot achieves state-of-the-art performance on four Twitter\nbot detection benchmarks. Extensive studies also show that LMBot is more\nrobust, versatile, and efficient compared to graph-based Twitter bot detection\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2306.17408v3.pdf"
    },
    {
        "title": "SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization",
        "authors": [
            "Yash Mathur",
            "Sanketh Rangreji",
            "Raghav Kapoor",
            "Medha Palavalli",
            "Amanda Bertsch",
            "Matthew R. Gormley"
        ],
        "published": "2023-06-30T03:14:04Z",
        "summary": "Medical dialogue summarization is challenging due to the unstructured nature\nof medical conversations, the use of medical terminology in gold summaries, and\nthe need to identify key information across multiple symptom sets. We present a\nnovel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA\n2023 Shared Task. Our approach for section-wise summarization (Task A) is a\ntwo-stage process of selecting semantically similar dialogues and using the\ntop-k similar dialogues as in-context examples for GPT-4. For full-note\nsummarization (Task B), we use a similar solution with k=1. We achieved 3rd\nplace in Task A (2nd among all teams), 4th place in Task B Division Wise\nSummarization (2nd among all teams), 15th place in Task A Section Header\nClassification (9th among all teams), and 8th place among all teams in Task B.\nOur results highlight the effectiveness of few-shot prompting for this task,\nthough we also identify several weaknesses of prompting-based approaches. We\ncompare GPT-4 performance with several finetuned baselines. We find that GPT-4\nsummaries are more abstractive and shorter. We make our code publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2306.17384v1.pdf"
    },
    {
        "title": "Modeling Parallel Programs using Large Language Models",
        "authors": [
            "Daniel Nichols",
            "Aniruddha Marathe",
            "Harshitha Menon",
            "Todd Gamblin",
            "Abhinav Bhatele"
        ],
        "published": "2023-06-29T19:44:55Z",
        "summary": "Parallel software codes in high performance computing (HPC) continue to grow\nin complexity and scale as we enter the exascale era. A diverse set of emerging\nhardware and programming paradigms make developing, optimizing, and maintaining\nparallel software burdensome for developers. One way to alleviate some of these\nburdens is with automated development and analysis tools. Such tools can\nperform complex and/or remedial tasks for developers that increase their\nproductivity and decrease the chance for error. So far, such tools for code\ndevelopment and performance analysis have been limited in the complexity of\ntasks they can perform. However, with recent advancements in language modeling,\nand the wealth of code related data that is now available online, these tools\nhave started to utilize predictive language models to automate more complex\ntasks. In this paper, we show how large language models (LLMs) can be applied\nto tasks specific to high performance and scientific codes. We train LLMs using\ncode and performance data that is specific to parallel codes. We compare\nseveral recent LLMs on HPC related tasks and introduce a new model, HPC-Coder,\ntrained on parallel code. In our experiments we show that this model can\nauto-complete HPC functions where general models cannot, decorate for loops\nwith OpenMP pragmas, and model performance changes in two scientific\napplication repositories.",
        "pdf_link": "https://arxiv.org/pdf/2306.17281v1.pdf"
    },
    {
        "title": "DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios",
        "authors": [
            "Vinicius G. Goecks",
            "Nicholas R. Waytowich"
        ],
        "published": "2023-06-29T19:24:19Z",
        "summary": "The development of plans of action in disaster response scenarios is a\ntime-consuming process. Large Language Models (LLMs) offer a powerful solution\nto expedite this process through in-context learning. This study presents\nDisasterResponseGPT, an algorithm that leverages LLMs to generate valid plans\nof action quickly by incorporating disaster response and planning guidelines in\nthe initial prompt. In DisasterResponseGPT, users input the scenario\ndescription and receive a plan of action as output. The proposed method\ngenerates multiple plans within seconds, which can be further refined following\nthe user's feedback. Preliminary results indicate that the plans of action\ndeveloped by DisasterResponseGPT are comparable to human-generated ones while\noffering greater ease of modification in real-time. This approach has the\npotential to revolutionize disaster response operations by enabling rapid\nupdates and adjustments during the plan's execution.",
        "pdf_link": "https://arxiv.org/pdf/2306.17271v1.pdf"
    },
    {
        "title": "Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations",
        "authors": [
            "Xuansheng Wu",
            "Huachi Zhou",
            "Yucheng Shi",
            "Wenlin Yao",
            "Xiao Huang",
            "Ninghao Liu"
        ],
        "published": "2023-06-29T18:50:12Z",
        "summary": "Recommendation systems help users find matched items based on their previous\nbehaviors. Personalized recommendation becomes challenging in the absence of\nhistorical user-item interactions, a practical problem for startups known as\nthe system cold-start recommendation. While existing research addresses\ncold-start issues for either users or items, we still lack solutions for system\ncold-start scenarios. To tackle the problem, we propose PromptRec, a simple but\neffective approach based on in-context learning of language models, where we\ntransform the recommendation task into the sentiment analysis task on natural\nlanguage containing user and item profiles. However, this naive approach\nheavily relies on the strong in-context learning ability emerged from large\nlanguage models, which could suffer from significant latency for online\nrecommendations. To solve the challenge, we propose to enhance small language\nmodels for recommender systems with a data-centric pipeline, which consists of:\n(1) constructing a refined corpus for model pre-training; (2) constructing a\ndecomposed prompt template via prompt pre-training. They correspond to the\ndevelopment of training data and inference data, respectively. The pipeline is\nsupported by a theoretical framework that formalizes the connection between\nin-context recommendation and language modeling. To evaluate our approach, we\nintroduce a cold-start recommendation benchmark, and the results demonstrate\nthat the enhanced small language models can achieve comparable cold-start\nrecommendation performance to that of large models with only $17\\%$ of the\ninference time. To the best of our knowledge, this is the first study to tackle\nthe system cold-start recommendation problem. We believe our findings will\nprovide valuable insights for future works. The benchmark and implementations\nare available at https://github.com/JacksonWuxs/PromptRec.",
        "pdf_link": "https://arxiv.org/pdf/2306.17256v5.pdf"
    },
    {
        "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
        "authors": [
            "Tung Phung",
            "Victor-Alexandru P\u0103durean",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Tobias Kohn",
            "Rupak Majumdar",
            "Adish Singla",
            "Gustavo Soares"
        ],
        "published": "2023-06-29T17:57:40Z",
        "summary": "Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.",
        "pdf_link": "https://arxiv.org/pdf/2306.17156v3.pdf"
    },
    {
        "title": "Concept-Oriented Deep Learning with Large Language Models",
        "authors": [
            "Daniel T. Chang"
        ],
        "published": "2023-06-29T16:47:11Z",
        "summary": "Large Language Models (LLMs) have been successfully used in many\nnatural-language tasks and applications including text generation and AI\nchatbots. They also are a promising new technology for concept-oriented deep\nlearning (CODL). However, the prerequisite is that LLMs understand concepts and\nensure conceptual consistency. We discuss these in this paper, as well as major\nuses of LLMs for CODL including concept extraction from text, concept graph\nextraction from text, and concept learning. Human knowledge consists of both\nsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only\nLLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal\nLLMs, on the other hand, are capable of representing the full range (conceptual\nand sensory) of human knowledge. We discuss conceptual understanding in\nvisual-language LLMs, the most important multimodal LLMs, and major uses of\nthem for CODL including concept extraction from image, concept graph extraction\nfrom image, and concept learning. While uses of LLMs for CODL are valuable\nstandalone, they are particularly valuable as part of LLM applications such as\nAI chatbots.",
        "pdf_link": "https://arxiv.org/pdf/2306.17089v2.pdf"
    },
    {
        "title": "UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Avijit Mitra",
            "Samuel Osebe",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023-06-29T13:30:41Z",
        "summary": "This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023\nshared task for Task-A and Task-C. We focus especially on Task-C and propose a\nnovel LLMs cooperation system named a doctor-patient loop to generate\nhigh-quality conversation data sets. The experiment results demonstrate that\nour approaches yield reasonable performance as evaluated by automatic metrics\nsuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we\nconducted a comparative analysis between our proposed method and ChatGPT and\nGPT-4. This analysis also investigates the potential of utilizing cooperation\nLLMs to generate high-quality datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.16931v1.pdf"
    },
    {
        "title": "From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data",
        "authors": [
            "Taiyu Ban",
            "Lyvzhou Chen",
            "Xiangyu Wang",
            "Huanhuan Chen"
        ],
        "published": "2023-06-29T12:48:00Z",
        "summary": "Large Language Models (LLMs) exhibit exceptional abilities for causal\nanalysis between concepts in numerous societally impactful domains, including\nmedicine, science, and law. Recent research on LLM performance in various\ncausal discovery and inference tasks has given rise to a new ladder in the\nclassical three-stage framework of causality. In this paper, we advance the\ncurrent research of LLM-driven causal discovery by proposing a novel framework\nthat combines knowledge-based LLM causal analysis with data-driven causal\nstructure learning. To make LLM more than a query tool and to leverage its\npower in discovering natural and new laws of causality, we integrate the\nvaluable LLM expertise on existing causal mechanisms into statistical analysis\nof objective data to build a novel and practical baseline for causal structure\nlearning.\n  We introduce a universal set of prompts designed to extract causal graphs\nfrom given variables and assess the influence of LLM prior causality on\nrecovering causal structures from data. We demonstrate the significant\nenhancement of LLM expertise on the quality of recovered causal structures from\ndata, while also identifying critical challenges and issues, along with\npotential approaches to address them. As a pioneering study, this paper aims to\nemphasize the new frontier that LLMs are opening for classical causal discovery\nand inference, and to encourage the widespread adoption of LLM capabilities in\ndata-driven causal analysis.",
        "pdf_link": "https://arxiv.org/pdf/2306.16902v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
        "authors": [
            "Joshua Maynez",
            "Priyanka Agrawal",
            "Sebastian Gehrmann"
        ],
        "published": "2023-06-29T08:59:40Z",
        "summary": "Pre-trained large language models (PLMs) underlie most new developments in\nnatural language processing. They have shifted the field from\napplication-specific model pipelines to a single model that is adapted to a\nwide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside\ntechniques like few-shot learning, have additionally shifted the output\nmodality to generation instead of classification or regression. Despite their\nubiquitous use, the generation quality of language models is rarely evaluated\nwhen these models are introduced. Additionally, it is unclear how existing\ngeneration tasks--while they can be used to compare systems at a high\nlevel--relate to the real world use cases for which people have been adopting\nthem. In this work, we discuss how to adapt existing application-specific\ngeneration benchmarks to PLMs and provide an in-depth, empirical study of the\nlimitations and capabilities of PLMs in natural language generation tasks along\ndimensions such as scale, architecture, input and output language. Our results\nshow that PLMs differ in their applicability to different data regimes and\ntheir generalization to multiple languages and inform which PLMs to use for a\ngiven generation task setup. We share best practices to be taken into\nconsideration when benchmarking generation capabilities during the development\nof upcoming PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.16793v1.pdf"
    },
    {
        "title": "Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game",
        "authors": [
            "Huy A. Nguyen",
            "Hayden Stec",
            "Xinying Hou",
            "Sarah Di",
            "Bruce M. McLaren"
        ],
        "published": "2023-06-29T02:28:09Z",
        "summary": "While open-ended self-explanations have been shown to promote robust learning\nin multiple studies, they pose significant challenges to automated grading and\nfeedback in technology-enhanced learning, due to the unconstrained nature of\nthe students' input. Our work investigates whether recent advances in Large\nLanguage Models, and in particular ChatGPT, can address this issue. Using\ndecimal exercises and student data from a prior study of the learning game\nDecimal Point, with more than 5,000 open-ended self-explanation responses, we\ninvestigate ChatGPT's capability in (1) solving the in-game exercises, (2)\ndetermining the correctness of students' answers, and (3) providing meaningful\nfeedback to incorrect answers. Our results showed that ChatGPT can respond well\nto conceptual questions, but struggled with decimal place values and number\nline problems. In addition, it was able to accurately assess the correctness of\n75% of the students' answers and generated generally high-quality feedback,\nsimilar to human instructors. We conclude with a discussion of ChatGPT's\nstrengths and weaknesses and suggest several venues for extending its use cases\nin digital teaching and learning.",
        "pdf_link": "https://arxiv.org/pdf/2306.16639v1.pdf"
    },
    {
        "title": "A negation detection assessment of GPTs: analysis with the xNot360 dataset",
        "authors": [
            "Ha Thanh Nguyen",
            "Randy Goebel",
            "Francesca Toni",
            "Kostas Stathis",
            "Ken Satoh"
        ],
        "published": "2023-06-29T02:27:48Z",
        "summary": "Negation is a fundamental aspect of natural language, playing a critical role\nin communication and comprehension. Our study assesses the negation detection\nperformance of Generative Pre-trained Transformer (GPT) models, specifically\nGPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in\nnatural language using a zero-shot prediction approach applied to our custom\nxNot360 dataset. Our approach examines sentence pairs labeled to indicate\nwhether the second sentence negates the first. Our findings expose a\nconsiderable performance disparity among the GPT models, with GPT-4 surpassing\nits counterparts and GPT-3.5 displaying a marked performance reduction. The\noverall proficiency of the GPT models in negation detection remains relatively\nmodest, indicating that this task pushes the boundaries of their natural\nlanguage understanding capabilities. We not only highlight the constraints of\nGPT models in handling negation but also emphasize the importance of logical\nreliability in high-stakes domains such as healthcare, science, and law.",
        "pdf_link": "https://arxiv.org/pdf/2306.16638v1.pdf"
    },
    {
        "title": "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?",
        "authors": [
            "Tianwen Wei",
            "Jian Luan",
            "Wei Liu",
            "Shuang Dong",
            "Bin Wang"
        ],
        "published": "2023-06-29T02:19:50Z",
        "summary": "We present the Chinese Elementary School Math Word Problems (CMATH) dataset,\ncomprising 1.7k elementary school-level math word problems with detailed\nannotations, source from actual Chinese workbooks and exams. This dataset aims\nto provide a benchmark tool for assessing the following question: to what grade\nlevel of elementary school math do the abilities of popular large language\nmodels (LLMs) correspond? We evaluate a variety of popular LLMs, including both\ncommercial and open-source options, and discover that only GPT-4 achieves\nsuccess (accuracy $\\geq$ 60\\%) across all six elementary school grades, while\nother models falter at different grade levels. Furthermore, we assess the\nrobustness of several top-performing LLMs by augmenting the original problems\nin the CMATH dataset with distracting information. Our findings reveal that\nGPT-4 is able to maintains robustness, while other model fail. We anticipate\nthat our study will expose limitations in LLMs' arithmetic and reasoning\ncapabilities, and promote their ongoing development and advancement.",
        "pdf_link": "https://arxiv.org/pdf/2306.16636v1.pdf"
    },
    {
        "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
        "authors": [
            "Theodore Zhao",
            "Mu Wei",
            "J. Samuel Preston",
            "Hoifung Poon"
        ],
        "published": "2023-06-28T21:11:15Z",
        "summary": "Generative Large language models (LLMs) have demonstrated remarkable\ncapabilities for a wide range of applications, but reducing ungrounded or\nerroneous responses remains a major growth area. Unlike task-specific models,\nthere lack an effective method to calibrate the confidence level of LLM\nresponses to indicate potential errors and facilitate human-in-the-loop\nverification. An important source of calibration stems from expert-stipulated\nprogrammatic supervision, which is often available at low cost but has its own\nlimitations such as noise and coverage. In this paper, we introduce a Pareto\noptimal self-supervision framework that can leverage available programmatic\nsupervision to systematically calibrate LLM responses by producing a risk score\nfor every LLM response, without any additional manual efforts. This is\naccomplished by learning a harmonizer model to align with LLM output as well as\nother weak supervision sources. The model assigns higher risk scores to more\nuncertain LLM responses and facilitate error correction. Experiments on\nstandard relation extraction and classification tasks in biomedical and general\ndomains demonstrate that the proposed risk score is highly correlated with the\nactual LLM error rate. By using a dynamic prompting strategy based on the risk\nscore, we observed significant accuracy improvement for off-the-shelf LLMs,\nboosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model\nand GPT-4 results past SOTA supervised results on challenging evaluation\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.16564v3.pdf"
    },
    {
        "title": "Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023",
        "authors": [
            "Daoji Huang",
            "Otmar Hilliges",
            "Luc Van Gool",
            "Xi Wang"
        ],
        "published": "2023-06-28T20:33:52Z",
        "summary": "We present Palm, a solution to the Long-Term Action Anticipation (LTA) task\nutilizing vision-language and large language models. Given an input video with\nannotated action periods, the LTA task aims to predict possible future actions.\nWe hypothesize that an optimal solution should capture the interdependency\nbetween past and future actions, and be able to infer future actions based on\nthe structure and dependency encoded in the past actions. Large language models\nhave demonstrated remarkable commonsense-based reasoning ability. Inspired by\nthat, Palm chains an image captioning model and a large language model. It\npredicts future actions based on frame descriptions and action labels extracted\nfrom the input videos. Our method outperforms other participants in the EGO4D\nLTA challenge and achieves the best performance in terms of action prediction.\nOur code is available at https://github.com/DanDoge/Palm",
        "pdf_link": "https://arxiv.org/pdf/2306.16545v1.pdf"
    },
    {
        "title": "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language",
        "authors": [
            "William Berrios",
            "Gautam Mittal",
            "Tristan Thrush",
            "Douwe Kiela",
            "Amanpreet Singh"
        ],
        "published": "2023-06-28T17:57:10Z",
        "summary": "We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.",
        "pdf_link": "https://arxiv.org/pdf/2306.16410v1.pdf"
    },
    {
        "title": "On the Exploitability of Instruction Tuning",
        "authors": [
            "Manli Shu",
            "Jiongxiao Wang",
            "Chen Zhu",
            "Jonas Geiping",
            "Chaowei Xiao",
            "Tom Goldstein"
        ],
        "published": "2023-06-28T17:54:04Z",
        "summary": "Instruction tuning is an effective technique to align large language models\n(LLMs) with human intents. In this work, we investigate how an adversary can\nexploit instruction tuning by injecting specific instruction-following examples\ninto the training data that intentionally changes the model's behavior. For\nexample, an adversary can achieve content injection by injecting training\nexamples that mention target content and eliciting such behavior from\ndownstream models. To achieve this goal, we propose \\textit{AutoPoison}, an\nautomated data poisoning pipeline. It naturally and coherently incorporates\nversatile attack goals into poisoned data with the help of an oracle LLM. We\nshowcase two example attacks: content injection and over-refusal attacks, each\naiming to induce a specific exploitable behavior. We quantify and benchmark the\nstrength and the stealthiness of our data poisoning scheme. Our results show\nthat AutoPoison allows an adversary to change a model's behavior by poisoning\nonly a small fraction of data while maintaining a high level of stealthiness in\nthe poisoned examples. We hope our work sheds light on how data quality affects\nthe behavior of instruction-tuned models and raises awareness of the importance\nof data quality for responsible deployments of LLMs. Code is available at\n\\url{https://github.com/azshue/AutoPoison}.",
        "pdf_link": "https://arxiv.org/pdf/2306.17194v2.pdf"
    },
    {
        "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
        "authors": [
            "Esin Durmus",
            "Karina Nyugen",
            "Thomas I. Liao",
            "Nicholas Schiefer",
            "Amanda Askell",
            "Anton Bakhtin",
            "Carol Chen",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Nicholas Joseph",
            "Liane Lovitt",
            "Sam McCandlish",
            "Orowa Sikder",
            "Alex Tamkin",
            "Janel Thamkul",
            "Jared Kaplan",
            "Jack Clark",
            "Deep Ganguli"
        ],
        "published": "2023-06-28T17:31:53Z",
        "summary": "Large language models (LLMs) may not equitably represent diverse global\nperspectives on societal issues. In this paper, we develop a quantitative\nframework to evaluate whose opinions model-generated responses are more similar\nto. We first build a dataset, GlobalOpinionQA, comprised of questions and\nanswers from cross-national surveys designed to capture diverse opinions on\nglobal issues across different countries. Next, we define a metric that\nquantifies the similarity between LLM-generated survey responses and human\nresponses, conditioned on country. With our framework, we run three experiments\non an LLM trained to be helpful, honest, and harmless with Constitutional AI.\nBy default, LLM responses tend to be more similar to the opinions of certain\npopulations, such as those from the USA, and some European and South American\ncountries, highlighting the potential for biases. When we prompt the model to\nconsider a particular country's perspective, responses shift to be more similar\nto the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes. When we translate GlobalOpinionQA questions to a target language,\nthe model's responses do not necessarily become the most similar to the\nopinions of speakers of those languages. We release our dataset for others to\nuse and build on. Our data is at\nhttps://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide\nan interactive visualization at https://llmglobalvalues.anthropic.com.",
        "pdf_link": "https://arxiv.org/pdf/2306.16388v1.pdf"
    },
    {
        "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
        "authors": [
            "Yiwen Shi",
            "Ping Ren",
            "Jing Wang",
            "Biao Han",
            "Taha ValizadehAslani",
            "Felix Agbavor",
            "Yi Zhang",
            "Meng Hu",
            "Liang Zhao",
            "Hualou Liang"
        ],
        "published": "2023-06-28T14:55:13Z",
        "summary": "Food effect summarization from New Drug Application (NDA) is an essential\ncomponent of product-specific guidance (PSG) development and assessment.\nHowever, manual summarization of food effect from extensive drug application\nreview documents is time-consuming, which arouses a need to develop automated\nmethods. Recent advances in large language models (LLMs) such as ChatGPT and\nGPT-4, have demonstrated great potential in improving the effectiveness of\nautomated text summarization, but its ability regarding the accuracy in\nsummarizing food effect for PSG assessment remains unclear. In this study, we\nintroduce a simple yet effective approach, iterative prompting, which allows\none to interact with ChatGPT or GPT-4 more effectively and efficiently through\nmulti-turn interaction. Specifically, we propose a three-turn iterative\nprompting approach to food effect summarization in which the keyword-focused\nand length-controlled prompts are respectively provided in consecutive turns to\nrefine the quality of the generated summary. We conduct a series of extensive\nevaluations, ranging from automated metrics to FDA professionals and even\nevaluation by GPT-4, on 100 NDA review documents selected over the past five\nyears. We observe that the summary quality is progressively improved throughout\nthe process. Moreover, we find that GPT-4 performs better than ChatGPT, as\nevaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).\nImportantly, all the FDA professionals unanimously rated that 85% of the\nsummaries generated by GPT-4 are factually consistent with the golden reference\nsummary, a finding further supported by GPT-4 rating of 72% consistency. These\nresults strongly suggest a great potential for GPT-4 to draft food effect\nsummaries that could be reviewed by FDA professionals, thereby improving the\nefficiency of PSG assessment cycle and promoting the generic drug product\ndevelopment.",
        "pdf_link": "https://arxiv.org/pdf/2306.16275v1.pdf"
    },
    {
        "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
        "authors": [
            "Yufei Huang",
            "Deyi Xiong"
        ],
        "published": "2023-06-28T14:14:44Z",
        "summary": "Holistically measuring societal biases of large language models is crucial\nfor detecting and reducing ethical risks in highly capable AI models. In this\nwork, we present a Chinese Bias Benchmark dataset that consists of over 100K\nquestions jointly constructed by human experts and generative language models,\ncovering stereotypes and societal biases in 14 social dimensions related to\nChinese culture and values. The curation process contains 4 essential steps:\nbias identification via extensive literature review, ambiguous context\ngeneration, AI-assisted disambiguous context generation, snd manual review \\&\nrecomposition. The testing instances in the dataset are automatically derived\nfrom 3K+ high-quality templates manually authored with stringent quality\ncontrol. The dataset exhibits wide coverage and high diversity. Extensive\nexperiments demonstrate the effectiveness of the dataset in detecting model\nbias, with all 10 publicly available Chinese large language models exhibiting\nstrong bias in certain categories. Additionally, we observe from our\nexperiments that fine-tuned models could, to a certain extent, heed\ninstructions and avoid generating outputs that are morally harmful in some\ntypes, in the way of \"moral self-correction\". Our dataset and results are\npublicly available at\n\\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},\noffering debiasing research opportunities to a widened community.",
        "pdf_link": "https://arxiv.org/pdf/2306.16244v1.pdf"
    },
    {
        "title": "Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition",
        "authors": [
            "Yuang Li",
            "Yu Wu",
            "Jinyu Li",
            "Shujie Liu"
        ],
        "published": "2023-06-28T08:29:00Z",
        "summary": "The integration of Language Models (LMs) has proven to be an effective way to\naddress domain shifts in speech recognition. However, these approaches usually\nrequire a significant amount of target domain text data for the training of\nLMs. Different from these methods, in this work, with only a domain-specific\ntext prompt, we propose two zero-shot ASR domain adaptation methods using\nLLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two\nways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR\nsystem with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an\nencoder-decoder based ASR system. Experiments show that, with only one domain\nprompt, both methods can effectively reduce word error rates (WER) on\nout-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep\nLLM-fusion has the advantage of better recall of entity and out-of-vocabulary\nwords.",
        "pdf_link": "https://arxiv.org/pdf/2306.16007v1.pdf"
    },
    {
        "title": "Query Understanding in the Age of Large Language Models",
        "authors": [
            "Avishek Anand",
            "Venktesh V",
            "Abhijit Anand",
            "Vinay Setty"
        ],
        "published": "2023-06-28T08:24:14Z",
        "summary": "Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.",
        "pdf_link": "https://arxiv.org/pdf/2306.16004v1.pdf"
    },
    {
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
        "authors": [
            "Yue Yu",
            "Yuchen Zhuang",
            "Jieyu Zhang",
            "Yu Meng",
            "Alexander Ratner",
            "Ranjay Krishna",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2023-06-28T03:31:31Z",
        "summary": "Large language models (LLMs) have been recently leveraged as training data\ngenerators for various natural language processing (NLP) tasks. While previous\nresearch has explored different approaches to training models using generated\ndata, they generally rely on simple class-conditional prompts, which may limit\nthe diversity of the generated data and inherit systematic biases of LLM. Thus,\nwe investigate training data generation with diversely attributed prompts\n(e.g., specifying attributes like length and style), which have the potential\nto yield diverse and attributed generated data. Our investigation focuses on\ndatasets with high cardinality and diverse domains, wherein we demonstrate that\nattributed prompts outperform simple class-conditional prompts in terms of the\nresulting model's performance. Additionally, we present a comprehensive\nempirical study on data generation encompassing vital aspects like bias,\ndiversity, and efficiency, and highlight three key observations: firstly,\nsynthetic datasets generated by simple prompts exhibit significant biases, such\nas regional bias; secondly, attribute diversity plays a pivotal role in\nenhancing model performance; lastly, attributed prompts achieve the performance\nof simple class-conditional prompts while utilizing only 5\\% of the querying\ncost of ChatGPT associated with the latter. The data and code are available on\n\\url{https://github.com/yueyu1030/AttrPrompt}.",
        "pdf_link": "https://arxiv.org/pdf/2306.15895v2.pdf"
    },
    {
        "title": "Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",
        "authors": [
            "Salmonn Talebi",
            "Elizabeth Tong",
            "Mohammad R. K. Mofrad"
        ],
        "published": "2023-06-28T03:03:51Z",
        "summary": "The use of large language models (LLMs) in healthcare is gaining popularity,\nbut their practicality and safety in clinical settings have not been thoroughly\nassessed. In high-stakes environments like medical settings, trust and safety\nare critical issues for LLMs. To address these concerns, we present an approach\nto evaluate the performance and trustworthiness of a GPT3.5 model for medical\nimage protocol assignment. We compare it with a fine-tuned BERT model and a\nradiologist. In addition, we have a radiologist review the GPT3.5 output to\nevaluate its decision-making process. Our evaluation dataset consists of 4,700\nphysician entries across 11 imaging protocol classes spanning the entire head.\nOur findings suggest that the GPT3.5 performance falls behind BERT and a\nradiologist. However, GPT3.5 outperforms BERT in its ability to explain its\ndecision, detect relevant word indicators, and model calibration. Furthermore,\nby analyzing the explanations of GPT3.5 for misclassifications, we reveal\nsystematic errors that need to be resolved to enhance its safety and\nsuitability for clinical use.",
        "pdf_link": "https://arxiv.org/pdf/2306.15887v1.pdf"
    },
    {
        "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
        "authors": [
            "Eric Nguyen",
            "Michael Poli",
            "Marjan Faizi",
            "Armin Thomas",
            "Callum Birch-Sykes",
            "Michael Wornow",
            "Aman Patel",
            "Clayton Rabideau",
            "Stefano Massaroli",
            "Yoshua Bengio",
            "Stefano Ermon",
            "Stephen A. Baccus",
            "Chris R\u00e9"
        ],
        "published": "2023-06-27T20:46:34Z",
        "summary": "Genomic (DNA) sequences encode an enormous amount of information for gene\nregulation and protein synthesis. Similar to natural language models,\nresearchers have proposed foundation models in genomics to learn generalizable\nfeatures from unlabeled genome data that can then be fine-tuned for downstream\ntasks such as identifying regulatory elements. Due to the quadratic scaling of\nattention, previous Transformer-based genomic models have used 512 to 4k tokens\nas context (<0.001% of the human genome), significantly limiting the modeling\nof long-range interactions in DNA. In addition, these methods rely on\ntokenizers or fixed k-mers to aggregate meaningful DNA units, losing single\nnucleotide resolution where subtle genetic variations can completely alter\nprotein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a\nlarge language model based on implicit convolutions was shown to match\nattention in quality while allowing longer context lengths and lower time\ncomplexity. Leveraging Hyena's new long-range capabilities, we present\nHyenaDNA, a genomic foundation model pretrained on the human reference genome\nwith context lengths of up to 1 million tokens at the single nucleotide-level -\nan up to 500x increase over previous dense attention-based models. HyenaDNA\nscales sub-quadratically in sequence length (training up to 160x faster than\nTransformer), uses single nucleotide tokens, and has full global context at\neach layer. We explore what longer context enables - including the first use of\nin-context learning in genomics. On fine-tuned benchmarks from the Nucleotide\nTransformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets\nusing a model with orders of magnitude less parameters and pretraining data. On\nthe GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by\n+10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.",
        "pdf_link": "https://arxiv.org/pdf/2306.15794v2.pdf"
    },
    {
        "title": "Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese",
        "authors": [
            "Maria Carolina Penteado",
            "F\u00e1bio Perez"
        ],
        "published": "2023-06-27T20:37:54Z",
        "summary": "We investigate the effectiveness of GPT-3.5 and GPT-4, two large language\nmodels, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese\nand compare their performance against Microsoft Word and Google Docs. We\nintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,\nSpelling, Internet, and Fast typing. Our results show that while GPT-4 has\nhigher recall than other methods, LLMs tend to have lower precision, leading to\novercorrection. This study demonstrates the potential of LLMs as practical GEC\ntools for Brazilian Portuguese and encourages further exploration of LLMs for\nnon-English languages and other educational settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.15788v2.pdf"
    },
    {
        "title": "Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost",
        "authors": [
            "Parikshit Bansal",
            "Amit Sharma"
        ],
        "published": "2023-06-27T19:29:55Z",
        "summary": "State-of-the-art supervised NLP models achieve high accuracy but are also\nsusceptible to failures on inputs from low-data regimes, such as domains that\nare not represented in training data. As an approximation to collecting\nground-truth labels for the specific domain, we study the use of large language\nmodels (LLMs) for annotating inputs and improving the generalization of NLP\nmodels. Specifically, given a budget for LLM annotations, we present an\nalgorithm for sampling the most informative inputs to annotate and retrain the\nNLP model. We find that popular active learning strategies such as\nuncertainty-based sampling do not work well. Instead, we propose a sampling\nstrategy based on the difference in prediction scores between the base model\nand the finetuned NLP model, utilizing the fact that most NLP models are\nfinetuned from a base model. Experiments with classification (semantic\nsimilarity) and ranking (semantic search) tasks show that our sampling strategy\nleads to significant gains in accuracy for both the training and target\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2306.15766v1.pdf"
    },
    {
        "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
        "authors": [
            "Zeyi Liu",
            "Arpit Bahety",
            "Shuran Song"
        ],
        "published": "2023-06-27T18:03:15Z",
        "summary": "The ability to detect and analyze failed executions automatically is crucial\nfor an explainable and robust robotic system. Recently, Large Language Models\n(LLMs) have demonstrated strong reasoning abilities on textual inputs. To\nleverage the power of LLMs for robot failure explanation, we introduce REFLECT,\na framework which queries LLM for failure reasoning based on a hierarchical\nsummary of robot past experiences generated from multisensory observations. The\nfailure explanation can further guide a language-based planner to correct the\nfailure and complete the task. To systematically evaluate the framework, we\ncreate the RoboFail dataset with a variety of tasks and failure scenarios. We\ndemonstrate that the LLM-based framework is able to generate informative\nfailure explanations that assist successful correction planning.",
        "pdf_link": "https://arxiv.org/pdf/2306.15724v4.pdf"
    },
    {
        "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models",
        "authors": [
            "Kaiyu Yang",
            "Aidan M. Swope",
            "Alex Gu",
            "Rahul Chalamala",
            "Peiyang Song",
            "Shixing Yu",
            "Saad Godil",
            "Ryan Prenger",
            "Anima Anandkumar"
        ],
        "published": "2023-06-27T17:05:32Z",
        "summary": "Large language models (LLMs) have shown promise in proving formal theorems\nusing proof assistants such as Lean. However, existing methods are difficult to\nreproduce or build on, due to private code, data, and large compute\nrequirements. This has created substantial barriers to research on machine\nlearning methods for theorem proving. This paper removes these barriers by\nintroducing LeanDojo: an open-source Lean playground consisting of toolkits,\ndata, models, and benchmarks. LeanDojo extracts data from Lean and enables\ninteraction with the proof environment programmatically. It contains\nfine-grained annotations of premises in proofs, providing valuable data for\npremise selection: a key bottleneck in theorem proving. Using this data, we\ndevelop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented\nwith retrieval for selecting premises from a vast math library. It is\ninexpensive and needs only one GPU week of training. Our retriever leverages\nLeanDojo's program analysis capability to identify accessible premises and hard\nnegative examples, which makes retrieval much more effective. Furthermore, we\nconstruct a new benchmark consisting of 98,734 theorems and proofs extracted\nfrom Lean's math library. It features challenging data split requiring the\nprover to generalize to theorems relying on novel premises that are never used\nin training. We use this benchmark for training and evaluation, and\nexperimental results demonstrate the effectiveness of ReProver over\nnon-retrieval baselines and GPT-4. We thus provide the first set of open-source\nLLM-based theorem provers without any proprietary datasets and release it under\na permissive MIT license to facilitate further research.",
        "pdf_link": "https://arxiv.org/pdf/2306.15626v2.pdf"
    },
    {
        "title": "Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",
        "authors": [
            "Jingwei Ni",
            "Julia Bingler",
            "Chiara Colesanti-Senni",
            "Mathias Kraus",
            "Glen Gostlow",
            "Tobias Schimanski",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Nicolas Webersinke",
            "Tobias Wekhof",
            "Tingyu Yu",
            "Markus Leippold"
        ],
        "published": "2023-06-27T14:46:47Z",
        "summary": "This paper introduces a novel approach to enhance Large Language Models\n(LLMs) with expert knowledge to automate the analysis of corporate\nsustainability reports by benchmarking them against the Task Force for\nClimate-Related Financial Disclosures (TCFD) recommendations. Corporate\nsustainability reports are crucial in assessing organizations' environmental\nand social risks and impacts. However, analyzing these reports' vast amounts of\ninformation makes human analysis often too costly. As a result, only a few\nentities worldwide have the resources to analyze these reports, which could\nlead to a lack of transparency. While AI-powered tools can automatically\nanalyze the data, they are prone to inaccuracies as they lack domain-specific\nexpertise. This paper introduces a novel approach to enhance LLMs with expert\nknowledge to automate the analysis of corporate sustainability reports. We\nchristen our tool CHATREPORT, and apply it in a first use case to assess\ncorporate climate risk disclosures following the TCFD recommendations.\nCHATREPORT results from collaborating with experts in climate science, finance,\neconomic policy, and computer science, demonstrating how domain experts can be\ninvolved in developing AI tools. We make our prompt templates, generated data,\nand scores available to the public to encourage transparency.",
        "pdf_link": "https://arxiv.org/pdf/2306.15518v2.pdf"
    },
    {
        "title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
        "authors": [
            "Sophie Jentzsch",
            "Cigdem Turan"
        ],
        "published": "2023-06-27T08:36:35Z",
        "summary": "Pretrained language models are publicly available and constantly finetuned\nfor various real-life applications. As they become capable of grasping complex\ncontextual information, harmful biases are likely increasingly intertwined with\nthose models. This paper analyses gender bias in BERT models with two main\ncontributions: First, a novel bias measure is introduced, defining biases as\nthe difference in sentiment valuation of female and male sample versions.\nSecond, we comprehensively analyse BERT's biases on the example of a realistic\nIMDB movie classifier. By systematically varying elements of the training\npipeline, we can conclude regarding their impact on the final model bias. Seven\ndifferent public BERT models in nine training conditions, i.e. 63 models in\ntotal, are compared. Almost all conditions yield significant gender biases.\nResults indicate that reflected biases stem from public BERT models rather than\ntask-specific data, emphasising the weight of responsible usage.",
        "pdf_link": "https://arxiv.org/pdf/2306.15298v1.pdf"
    },
    {
        "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
        "authors": [
            "Virginia K. Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2023-06-26T22:07:33Z",
        "summary": "We present WinoQueer: a benchmark specifically designed to measure whether\nlarge language models (LLMs) encode biases that are harmful to the LGBTQ+\ncommunity. The benchmark is community-sourced, via application of a novel\nmethod that generates a bias benchmark from a community survey. We apply our\nbenchmark to several popular LLMs and find that off-the-shelf models generally\ndo exhibit considerable anti-queer bias. Finally, we show that LLM bias against\na marginalized community can be somewhat mitigated by finetuning on data\nwritten about or by members of that community, and that social media text\nwritten by community members is more effective than news text written about the\ncommunity by non-members. Our method for community-in-the-loop benchmark\ndevelopment provides a blueprint for future researchers to develop\ncommunity-driven, harms-grounded LLM benchmarks for other marginalized\ncommunities.",
        "pdf_link": "https://arxiv.org/pdf/2306.15087v1.pdf"
    },
    {
        "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
        "authors": [
            "John Yang",
            "Akshara Prabhakar",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "published": "2023-06-26T17:59:50Z",
        "summary": "Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io",
        "pdf_link": "https://arxiv.org/pdf/2306.14898v3.pdf"
    },
    {
        "title": "Are aligned neural networks adversarially aligned?",
        "authors": [
            "Nicholas Carlini",
            "Milad Nasr",
            "Christopher A. Choquette-Choo",
            "Matthew Jagielski",
            "Irena Gao",
            "Anas Awadalla",
            "Pang Wei Koh",
            "Daphne Ippolito",
            "Katherine Lee",
            "Florian Tramer",
            "Ludwig Schmidt"
        ],
        "published": "2023-06-26T17:18:44Z",
        "summary": "Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study to what extent these models remain\naligned, even when interacting with an adversarial user who constructs\nworst-case inputs (adversarial examples). These inputs are designed to cause\nthe model to emit harmful content that would otherwise be prohibited. We show\nthat existing NLP-based optimization attacks are insufficiently powerful to\nreliably attack aligned text models: even when current NLP-based attacks fail,\nwe can find adversarial inputs with brute force. As a result, the failure of\ncurrent attacks should not be seen as proof that aligned text models remain\naligned under adversarial inputs.\n  However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models.",
        "pdf_link": "https://arxiv.org/pdf/2306.15447v1.pdf"
    },
    {
        "title": "Exploring the Robustness of Large Language Models for Solving Programming Problems",
        "authors": [
            "Atsushi Shirafuji",
            "Yutaka Watanobe",
            "Takumi Ito",
            "Makoto Morishita",
            "Yuki Nakamura",
            "Yusuke Oda",
            "Jun Suzuki"
        ],
        "published": "2023-06-26T10:48:50Z",
        "summary": "Using large language models (LLMs) for source code has recently gained\nattention. LLMs, such as Transformer-based models like Codex and ChatGPT, have\nbeen shown to be highly capable of solving a wide range of programming\nproblems. However, the extent to which LLMs understand problem descriptions and\ngenerate programs accordingly or just retrieve source code from the most\nrelevant problem in training data based on superficial cues has not been\ndiscovered yet. To explore this research question, we conduct experiments to\nunderstand the robustness of several popular LLMs, CodeGen and GPT-3.5 series\nmodels, capable of tackling code generation tasks in introductory programming\nproblems. Our experimental results show that CodeGen and Codex are sensitive to\nthe superficial modifications of problem descriptions and significantly impact\ncode generation performance. Furthermore, we observe that Codex relies on\nvariable names, as randomized variables decrease the solved rate significantly.\nHowever, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,\nshow higher robustness to superficial modifications and have an outstanding\ncapability for solving programming problems. This highlights the fact that\nslight modifications to the prompts given to the LLMs can greatly affect code\ngeneration performance, and careful formatting of prompts is essential for\nhigh-quality code generation, while the SOTA models are becoming more robust to\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2306.14583v1.pdf"
    },
    {
        "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference",
        "authors": [
            "Junyan Li",
            "Li Lyna Zhang",
            "Jiahang Xu",
            "Yujing Wang",
            "Shaoguang Yan",
            "Yunqing Xia",
            "Yuqing Yang",
            "Ting Cao",
            "Hao Sun",
            "Weiwei Deng",
            "Qi Zhang",
            "Mao Yang"
        ],
        "published": "2023-06-26T03:06:57Z",
        "summary": "Deploying pre-trained transformer models like BERT on downstream tasks in\nresource-constrained scenarios is challenging due to their high inference cost,\nwhich grows rapidly with input sequence length. In this work, we propose a\nconstraint-aware and ranking-distilled token pruning method ToP, which\nselectively removes unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while preserving accuracy.\nToP overcomes the limitation of inaccurate token importance ranking in the\nconventional self-attention mechanism through a ranking-distilled token\ndistillation technique, which distills effective token rankings from the final\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\na coarse-to-fine pruning approach that automatically selects the optimal subset\nof transformer layers and optimizes token pruning decisions within these layers\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\nand model compression methods with improved accuracy and speedups. ToP reduces\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\nand provides a real latency speedup of up to 7.4x on an Intel CPU.",
        "pdf_link": "https://arxiv.org/pdf/2306.14393v1.pdf"
    },
    {
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
        "authors": [
            "Yilun Zhao",
            "Chen Zhao",
            "Linyong Nan",
            "Zhenting Qi",
            "Wenlin Zhang",
            "Xiangru Tang",
            "Boyu Mi",
            "Dragomir Radev"
        ],
        "published": "2023-06-25T19:23:21Z",
        "summary": "Despite significant progress having been made in question answering on\ntabular data (Table QA), it's unclear whether, and to what extent existing\nTable QA models are robust to task-specific perturbations, e.g., replacing key\nquestion entities or shuffling table columns. To systematically study the\nrobustness of Table QA models, we propose a benchmark called RobuT, which\nbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and\nincludes human-annotated adversarial perturbations in terms of table header,\ntable content, and question. Our results indicate that both state-of-the-art\nTable QA models and large language models (e.g., GPT-3) with few-shot learning\nfalter in these adversarial sets. We propose to address this problem by using\nlarge language models to generate adversarial examples to enhance training,\nwhich significantly improves the robustness of Table QA models. Our data and\ncode is publicly available at https://github.com/yilunzhao/RobuT.",
        "pdf_link": "https://arxiv.org/pdf/2306.14321v1.pdf"
    },
    {
        "title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning",
        "authors": [
            "Xiao Ma",
            "Swaroop Mishra",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023-06-25T18:40:43Z",
        "summary": "Language models still struggle on moral reasoning, despite their impressive\nperformance in many other tasks. In particular, the Moral Scenarios task in\nMMLU (Multi-task Language Understanding) is among the worst performing tasks\nfor many language models, including GPT-3. In this work, we propose a new\nprompting framework, Thought Experiments, to teach language models to do better\nmoral reasoning using counterfactuals. Experiment results show that our\nframework elicits counterfactual questions and answers from the model, which in\nturn helps improve the accuracy on Moral Scenarios task by 9-16% compared to\nother zero-shot baselines. Interestingly, unlike math reasoning tasks,\nzero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and\neven reduces accuracy by around 4% compared to direct zero-shot. We further\nobserved that with minimal human supervision in the form of 5 few-shot\nexamples, the accuracy of the task can be improved to as much as 80%.",
        "pdf_link": "https://arxiv.org/pdf/2306.14308v1.pdf"
    },
    {
        "title": "Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction",
        "authors": [
            "Feng Chen",
            "Yujian Feng"
        ],
        "published": "2023-06-25T04:33:56Z",
        "summary": "Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction\n(MRE) necessitate the fundamental reasoning capacity for intricate linguistic\nand multimodal comprehension. In this study, we explore distilling the\nreasoning ability of large language models (LLMs) into a more compact student\nmodel by generating a \\textit{chain of thought} (CoT) -- a sequence of\nintermediate reasoning steps. Specifically, we commence by exemplifying the\nelicitation of such reasoning ability from LLMs through CoT prompts covering\nmulti-grain (noun, sentence, multimodality) and data-augmentation (style,\nentity, image) dimensions. Subsequently, we present a novel conditional prompt\ndistillation method to assimilate the commonsense reasoning ability from LLMs,\nthereby enhancing the utility of the student model in addressing text-only\ninputs without the requisite addition of image and CoT knowledge. Extensive\nexperiments reveal that our approach attains state-of-the-art accuracy and\nmanifests a plethora of advantages concerning interpretability, data\nefficiency, and cross-domain generalization on MNER and MRE datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.14122v3.pdf"
    },
    {
        "title": "Language models are weak learners",
        "authors": [
            "Hariharan Manikandan",
            "Yiding Jiang",
            "J Zico Kolter"
        ],
        "published": "2023-06-25T02:39:19Z",
        "summary": "A central notion in practical and theoretical machine learning is that of a\n$\\textit{weak learner}$, classifiers that achieve better-than-random\nperformance (on any given distribution over data), even by a small margin. Such\nweak learners form the practical basis for canonical machine learning methods\nsuch as boosting. In this work, we illustrate that prompt-based large language\nmodels can operate effectively as said weak learners. Specifically, we\nillustrate the use of a large language model (LLM) as a weak learner in a\nboosting algorithm applied to tabular data. We show that by providing (properly\nsampled according to the distribution of interest) text descriptions of tabular\ndata samples, LLMs can produce a summary of the samples that serves as a\ntemplate for classification and achieves the aim of acting as a weak learner on\nthis task. We incorporate these models into a boosting approach, which in some\nsettings can leverage the knowledge within the LLM to outperform traditional\ntree-based boosting. The model outperforms both few-shot learning and\noccasionally even more involved fine-tuning procedures, particularly for tasks\ninvolving small numbers of data points. The results illustrate the potential\nfor prompt-based LLMs to function not just as few-shot learners themselves, but\nas components of larger machine learning pipelines.",
        "pdf_link": "https://arxiv.org/pdf/2306.14101v1.pdf"
    },
    {
        "title": "Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models",
        "authors": [
            "Yinyu Lan",
            "Yanru Wu",
            "Wang Xu",
            "Weiqiang Feng",
            "Youhao Zhang"
        ],
        "published": "2023-06-25T02:24:30Z",
        "summary": "Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. The FinChina SA dataset is\npublicly available at https://github.com/YerayL/FinChina-SA",
        "pdf_link": "https://arxiv.org/pdf/2306.14096v5.pdf"
    },
    {
        "title": "Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles",
        "authors": [
            "Paul Tarau"
        ],
        "published": "2023-06-24T23:33:00Z",
        "summary": "We automate deep step-by step reasoning in an LLM dialog thread by\nrecursively exploring alternatives (OR-nodes) and expanding details (AND-nodes)\nup to a given depth. Starting from a single succinct task-specific initiator we\nsteer the automated dialog thread to stay focussed on the task by synthesizing\na prompt that summarizes the depth-first steps taken so far.\n  Our algorithm is derived from a simple recursive descent implementation of a\nHorn Clause interpreter, except that we accommodate our logic engine to fit the\nnatural language reasoning patterns LLMs have been trained on. Semantic\nsimilarity to ground-truth facts or oracle advice from another LLM instance is\nused to restrict the search space and validate the traces of justification\nsteps returned as answers. At the end, the unique minimal model of a generated\nHorn Clause program collects the results of the reasoning process.\n  As applications, we sketch implementations of consequence predictions, causal\nexplanations, recommendation systems and topic-focussed exploration of\nscientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2306.14077v1.pdf"
    },
    {
        "title": "On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions",
        "authors": [
            "Reza Fayyazi",
            "Shanchieh Jay Yang"
        ],
        "published": "2023-06-24T21:08:15Z",
        "summary": "The volume, variety, and velocity of change in vulnerabilities and exploits\nhave made incident threat analysis challenging with human expertise and\nexperience along. Tactics, Techniques, and Procedures (TTPs) are to describe\nhow and why attackers exploit vulnerabilities. However, a TTP description\nwritten by one security professional can be interpreted very differently by\nanother, leading to confusion in cybersecurity operations or even business,\npolicy, and legal decisions. Meanwhile, advancements in AI have led to the\nincreasing use of Natural Language Processing (NLP) algorithms to assist the\nvarious tasks in cyber operations. With the rise of Large Language Models\n(LLMs), NLP tasks have significantly improved because of the LLM's semantic\nunderstanding and scalability. This leads us to question how well LLMs can\ninterpret TTPs or general cyberattack descriptions to inform analysts of the\nintended purposes of cyberattacks. We propose to analyze and compare the direct\nuse of LLMs (e.g., GPT-3.5) versus supervised fine-tuning (SFT) of\nsmall-scale-LLMs (e.g., BERT) to study their capabilities in predicting ATT&CK\ntactics. Our results reveal that the small-scale-LLMs with SFT provide a more\nfocused and clearer differentiation between the ATT&CK tactics (if such\ndifferentiation exists). On the other hand, direct use of LLMs offer a broader\ninterpretation of cyberattack techniques. When treating more general cases,\ndespite the power of LLMs, inherent ambiguity exists and limits their\npredictive power. We then summarize the challenges and recommend research\ndirections on LLMs to treat the inherent ambiguity of TTP descriptions used in\nvarious cyber operations.",
        "pdf_link": "https://arxiv.org/pdf/2306.14062v2.pdf"
    },
    {
        "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-Step",
        "authors": [
            "Liunian Harold Li",
            "Jack Hessel",
            "Youngjae Yu",
            "Xiang Ren",
            "Kai-Wei Chang",
            "Yejin Choi"
        ],
        "published": "2023-06-24T20:15:07Z",
        "summary": "Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.",
        "pdf_link": "https://arxiv.org/pdf/2306.14050v1.pdf"
    },
    {
        "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "authors": [
            "Zhenyu Zhang",
            "Ying Sheng",
            "Tianyi Zhou",
            "Tianlong Chen",
            "Lianmin Zheng",
            "Ruisi Cai",
            "Zhao Song",
            "Yuandong Tian",
            "Christopher R\u00e9",
            "Clark Barrett",
            "Zhangyang Wang",
            "Beidi Chen"
        ],
        "published": "2023-06-24T20:11:14Z",
        "summary": "Large Language Models (LLMs), despite their recent impressive\naccomplishments, are notably cost-prohibitive to deploy, particularly for\napplications involving long-content generation, such as dialogue systems and\nstory writing. Often, a large amount of transient state information, referred\nto as the KV cache, is stored in GPU memory in addition to model parameters,\nscaling linearly with the sequence length and batch size. In this paper, we\nintroduce a novel approach for implementing the KV cache which significantly\nreduces its memory footprint. Our approach is based on the noteworthy\nobservation that a small portion of tokens contributes most of the value when\ncomputing attention scores. We call these tokens Heavy Hitters (H$_2$). Through\na comprehensive investigation, we find that (i) the emergence of H$_2$ is\nnatural and strongly correlates with the frequent co-occurrence of tokens in\nthe text, and (ii) removing them results in significant performance\ndegradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),\na KV cache eviction policy that dynamically retains a balance of recent and\nH$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular\nproblem and prove (under mild assumptions) a theoretical guarantee for our\nnovel eviction algorithm which could help guide future work. We validate the\naccuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of\ntasks. Our implementation of H$_2$O with 20% heavy hitters improves the\nthroughput over three leading inference systems DeepSpeed Zero-Inference,\nHugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and\n3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the\nlatency by up to 1.9$\\times$. The code is available at\nhttps://github.com/FMInference/H2O.",
        "pdf_link": "https://arxiv.org/pdf/2306.14048v3.pdf"
    },
    {
        "title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",
        "authors": [
            "Tanmay Chavan",
            "Omkar Gokhale",
            "Aditya Kane",
            "Shantanu Patankar",
            "Raviraj Joshi"
        ],
        "published": "2023-06-24T18:17:38Z",
        "summary": "The research on code-mixed data is limited due to the unavailability of\ndedicated code-mixed datasets and pre-trained language models. In this work, we\nfocus on the low-resource Indian language Marathi which lacks any prior work in\ncode-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English\n(Mr-En) corpus with 10 million social media sentences for pretraining. We also\nrelease L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models\npre-trained on MeCorpus. Furthermore, for benchmarking, we present three\nsupervised datasets MeHate, MeSent, and MeLID for downstream tasks like\ncode-mixed Mr-En hate speech detection, sentiment analysis, and language\nidentification respectively. These evaluation datasets individually consist of\nmanually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations\nshow that the models trained on this novel corpus significantly outperform the\nexisting state-of-the-art BERT models. This is the first work that presents\nartifacts for code-mixed Marathi research. All datasets and models are publicly\nreleased at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2306.14030v2.pdf"
    },
    {
        "title": "LLM-assisted Generation of Hardware Assertions",
        "authors": [
            "Rahul Kande",
            "Hammond Pearce",
            "Benjamin Tan",
            "Brendan Dolan-Gavitt",
            "Shailja Thakur",
            "Ramesh Karri",
            "Jeyavijayan Rajendran"
        ],
        "published": "2023-06-24T17:44:36Z",
        "summary": "The security of computer systems typically relies on a hardware root of\ntrust. As vulnerabilities in hardware can have severe implications on a system,\nthere is a need for techniques to support security verification activities.\nAssertion-based verification is a popular verification technique that involves\ncapturing design intent in a set of assertions that can be used in formal\nverification or testing-based checking. However, writing security-centric\nassertions is a challenging task. In this work, we investigate the use of\nemerging large language models (LLMs) for code generation in hardware assertion\ngeneration for security, where primarily natural language prompts, such as\nthose one would see as code comments in assertion files, are used to produce\nSystemVerilog assertions. We focus our attention on a popular LLM and\ncharacterize its ability to write assertions out of the box, given varying\nlevels of detail in the prompt. We design an evaluation framework that\ngenerates a variety of prompts, and we create a benchmark suite comprising\nreal-world hardware designs and corresponding golden reference assertions that\nwe want to generate with the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.14027v1.pdf"
    },
    {
        "title": "Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?",
        "authors": [
            "Jaromir Savelka",
            "Kevin D. Ashley",
            "Morgan A Gray",
            "Hannes Westermann",
            "Huihui Xu"
        ],
        "published": "2023-06-24T08:48:24Z",
        "summary": "We evaluated the capability of generative pre-trained transformers~(GPT-4) in\nanalysis of textual data in tasks that require highly specialized domain\nexpertise. Specifically, we focused on the task of analyzing court opinions to\ninterpret legal concepts. We found that GPT-4, prompted with annotation\nguidelines, performs on par with well-trained law student annotators. We\nobserved that, with a relatively minor decrease in performance, GPT-4 can\nperform batch predictions leading to significant cost reductions. However,\nemploying chain-of-thought prompting did not lead to noticeably improved\nperformance on this task. Further, we demonstrated how to analyze GPT-4's\npredictions to identify and mitigate deficiencies in annotation guidelines, and\nsubsequently improve the performance of the model. Finally, we observed that\nthe model is quite brittle, as small formatting related changes in the prompt\nhad a high impact on the predictions. These findings can be leveraged by\nresearchers and practitioners who engage in semantic/pragmatic annotations of\ntexts in the context of the tasks requiring highly specialized domain\nexpertise.",
        "pdf_link": "https://arxiv.org/pdf/2306.13906v1.pdf"
    },
    {
        "title": "IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations",
        "authors": [
            "Yuxin Zi",
            "Kaushik Roy",
            "Vignesh Narayanan",
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-06-24T05:02:34Z",
        "summary": "Large Language Models (LLMs) encode meanings of words in the form of\ndistributed semantics. Distributed semantics capture common statistical\npatterns among language tokens (words, phrases, and sentences) from large\namounts of data. LLMs perform exceedingly well across General Language\nUnderstanding Evaluation (GLUE) tasks designed to test a model's understanding\nof the meanings of the input tokens. However, recent studies have shown that\nLLMs tend to generate unintended, inconsistent, or wrong texts as outputs when\nprocessing inputs that were seen rarely during training, or inputs that are\nassociated with diverse contexts (e.g., well-known hallucination phenomenon in\nlanguage generation tasks). Crowdsourced and expert-curated knowledge graphs\nsuch as ConceptNet are designed to capture the meaning of words from a compact\nset of well-defined contexts. Thus LLMs may benefit from leveraging such\nknowledge contexts to reduce inconsistencies in outputs. We propose a novel\nensemble learning method, Interpretable Ensemble Representation Learning\n(IERL), that systematically combines LLM and crowdsourced knowledge\nrepresentations of input tokens. IERL has the distinct advantage of being\ninterpretable by design (when was the LLM context used vs. when was the\nknowledge context used?) over state-of-the-art (SOTA) methods, allowing\nscrutiny of the inputs in conjunction with the parameters of the model,\nfacilitating the analysis of models' inconsistent or irrelevant outputs.\nAlthough IERL is agnostic to the choice of LLM and crowdsourced knowledge, we\ndemonstrate our approach using BERT and ConceptNet. We report improved or\ncompetitive results with IERL across GLUE tasks over current SOTA methods and\nsignificantly enhanced model interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2306.13865v1.pdf"
    },
    {
        "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
        "authors": [
            "Alycia Lee",
            "Brando Miranda",
            "Sudharsan Sundar",
            "Sanmi Koyejo"
        ],
        "published": "2023-06-24T02:25:56Z",
        "summary": "Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.13840v2.pdf"
    },
    {
        "title": "Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models",
        "authors": [
            "Adel Elmahdy",
            "Ahmed Salem"
        ],
        "published": "2023-06-23T21:25:38Z",
        "summary": "Natural language processing (NLP) models have become increasingly popular in\nreal-world applications, such as text classification. However, they are\nvulnerable to privacy attacks, including data reconstruction attacks that aim\nto extract the data used to train the model. Most previous studies on data\nreconstruction attacks have focused on LLM, while classification models were\nassumed to be more secure. In this work, we propose a new targeted data\nreconstruction attack called the Mix And Match attack, which takes advantage of\nthe fact that most classification models are based on LLM. The Mix And Match\nattack uses the base model of the target model to generate candidate tokens and\nthen prunes them using the classification head. We extensively demonstrate the\neffectiveness of the attack using both random and organic canaries. This work\nhighlights the importance of considering the privacy risks associated with data\nreconstruction attacks in classification models and offers insights into\npossible leakages.",
        "pdf_link": "https://arxiv.org/pdf/2306.13789v1.pdf"
    },
    {
        "title": "LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding",
        "authors": [
            "Robert Chew",
            "John Bollenbacher",
            "Michael Wenger",
            "Jessica Speer",
            "Annice Kim"
        ],
        "published": "2023-06-23T20:57:32Z",
        "summary": "Deductive coding is a widely used qualitative research method for determining\nthe prevalence of themes across documents. While useful, deductive coding is\noften burdensome and time consuming since it requires researchers to read,\ninterpret, and reliably categorize a large body of unstructured text documents.\nLarge language models (LLMs), like ChatGPT, are a class of quickly evolving AI\ntools that can perform a range of natural language processing and reasoning\ntasks. In this study, we explore the use of LLMs to reduce the time it takes\nfor deductive coding while retaining the flexibility of a traditional content\nanalysis. We outline the proposed approach, called LLM-assisted content\nanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a\npublicly available deductive coding data set. Additionally, we conduct an\nempirical benchmark using LACA on 4 publicly available data sets to assess the\nbroader question of how well GPT-3.5 performs across a range of deductive\ncoding tasks. Overall, we find that GPT-3.5 can often perform deductive coding\nat levels of agreement comparable to human coders. Additionally, we demonstrate\nthat LACA can help refine prompts for deductive coding, identify codes for\nwhich an LLM is randomly guessing, and help assess when to use LLMs vs. human\ncoders for deductive coding. We conclude with several implications for future\npractice of deductive coding and related research methods.",
        "pdf_link": "https://arxiv.org/pdf/2306.14924v1.pdf"
    },
    {
        "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
        "authors": [
            "Neel Jain",
            "Khalid Saifullah",
            "Yuxin Wen",
            "John Kirchenbauer",
            "Manli Shu",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2023-06-23T17:59:09Z",
        "summary": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2306.13651v2.pdf"
    },
    {
        "title": "Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation",
        "authors": [
            "Massimiliano Patacchiola",
            "Mingfei Sun",
            "Katja Hofmann",
            "Richard E. Turner"
        ],
        "published": "2023-06-23T15:29:15Z",
        "summary": "In this paper we explore few-shot imitation learning for control problems,\nwhich involves learning to imitate a target policy by accessing a limited set\nof offline rollouts. This setting has been relatively under-explored despite\nits relevance to robotics and control applications. State-of-the-art methods\ndeveloped to tackle few-shot imitation rely on meta-learning, which is\nexpensive to train as it requires access to a distribution over tasks (rollouts\nfrom many target policies and variations of the base environment). Given this\nlimitation we investigate an alternative approach, fine-tuning, a family of\nmethods that pretrain on a single dataset and then fine-tune on unseen\ndomain-specific data. Recent work has shown that fine-tuners outperform\nmeta-learners in few-shot image classification tasks, especially when the data\nis out-of-domain. Here we evaluate to what extent this is true for control\nproblems, proposing a simple yet effective baseline which relies on two stages:\n(i) training a base policy online via reinforcement learning (e.g. Soft\nActor-Critic) on a single base environment, (ii) fine-tuning the base policy\nvia behavioral cloning on a few offline rollouts of the target policy. Despite\nits simplicity this baseline is competitive with meta-learning methods on a\nvariety of conditions and is able to imitate target policies trained on unseen\nvariations of the original environment. Importantly, the proposed approach is\npractical and easy to implement, as it does not need any complex meta-training\nprotocol. As a further contribution, we release an open source dataset called\niMuJoCo (iMitation MuJoCo) consisting of 154 variants of popular OpenAI-Gym\nMuJoCo environments with associated pretrained target policies and rollouts,\nwhich can be used by the community to study few-shot imitation learning and\noffline reinforcement learning.",
        "pdf_link": "https://arxiv.org/pdf/2306.13554v1.pdf"
    },
    {
        "title": "Knowledge-Infused Self Attention Transformers",
        "authors": [
            "Kaushik Roy",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-06-23T13:55:01Z",
        "summary": "Transformer-based language models have achieved impressive success in various\nnatural language processing tasks due to their ability to capture complex\ndependencies and contextual information using self-attention mechanisms.\nHowever, they are not without limitations. These limitations include\nhallucinations, where they produce incorrect outputs with high confidence, and\nalignment issues, where they generate unhelpful and unsafe outputs for human\nusers. These limitations stem from the absence of implicit and missing context\nin the data alone. To address this, researchers have explored augmenting these\nmodels with external knowledge from knowledge graphs to provide the necessary\nadditional context. However, the ad-hoc nature of existing methods makes it\ndifficult to properly analyze the effects of knowledge infusion on the many\nmoving parts or components of a transformer. This paper introduces a systematic\nmethod for infusing knowledge into different components of a transformer-based\nmodel. A modular framework is proposed to identify specific components within\nthe transformer architecture, such as the self-attention mechanism, encoder\nlayers, or the input embedding layer, where knowledge infusion can be applied.\nAdditionally, extensive experiments are conducted on the General Language\nUnderstanding Evaluation (GLUE) benchmark tasks, and the findings are reported.\nThis systematic approach aims to facilitate more principled approaches to\nincorporating knowledge into language model architectures.",
        "pdf_link": "https://arxiv.org/pdf/2306.13501v1.pdf"
    },
    {
        "title": "Efficient Online Processing with Deep Neural Networks",
        "authors": [
            "Lukas Hedegaard"
        ],
        "published": "2023-06-23T12:29:44Z",
        "summary": "The capabilities and adoption of deep neural networks (DNNs) grow at an\nexhilarating pace: Vision models accurately classify human actions in videos\nand identify cancerous tissue in medical scans as precisely than human experts;\nlarge language models answer wide-ranging questions, generate code, and write\nprose, becoming the topic of everyday dinner-table conversations. Even though\ntheir uses are exhilarating, the continually increasing model sizes and\ncomputational complexities have a dark side. The economic cost and negative\nenvironmental externalities of training and serving models is in evident\ndisharmony with financial viability and climate action goals.\n  Instead of pursuing yet another increase in predictive performance, this\ndissertation is dedicated to the improvement of neural network efficiency.\nSpecifically, a core contribution addresses the efficiency aspects during\nonline inference. Here, the concept of Continual Inference Networks (CINs) is\nproposed and explored across four publications. CINs extend prior\nstate-of-the-art methods developed for offline processing of spatio-temporal\ndata and reuse their pre-trained weights, improving their online processing\nefficiency by an order of magnitude. These advances are attained through a\nbottom-up computational reorganization and judicious architectural\nmodifications. The benefit to online inference is demonstrated by reformulating\nseveral widely used network architectures into CINs, including 3D CNNs,\nST-GCNs, and Transformer Encoders. An orthogonal contribution tackles the\nconcurrent adaptation and computational acceleration of a large source model\ninto multiple lightweight derived models. Drawing on fusible adapter networks\nand structured pruning, Structured Pruning Adapters achieve superior predictive\naccuracy under aggressive pruning using significantly fewer learned weights\ncompared to fine-tuning with pruning.",
        "pdf_link": "https://arxiv.org/pdf/2306.13474v1.pdf"
    },
    {
        "title": "Product Information Extraction using ChatGPT",
        "authors": [
            "Alexander Brinkmann",
            "Roee Shraga",
            "Reng Chiz Der",
            "Christian Bizer"
        ],
        "published": "2023-06-23T09:30:01Z",
        "summary": "Structured product data in the form of attribute/value pairs is the\nfoundation of many e-commerce applications such as faceted product search,\nproduct comparison, and product recommendation. Product offers often only\ncontain textual descriptions of the product attributes in the form of titles or\nfree text. Hence, extracting attribute/value pairs from textual product\ndescriptions is an essential enabler for e-commerce applications. In order to\nexcel, state-of-the-art product information extraction methods require large\nquantities of task-specific training data. The methods also struggle with\ngeneralizing to out-of-distribution attributes and attribute values that were\nnot a part of the training data. Due to being pre-trained on huge amounts of\ntext as well as due to emergent effects resulting from the model size, Large\nLanguage Models like ChatGPT have the potential to address both of these\nshortcomings. This paper explores the potential of ChatGPT for extracting\nattribute/value pairs from product descriptions. We experiment with different\nzero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a\nperformance similar to a pre-trained language model but requires much smaller\namounts of training data and computation for fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.14921v1.pdf"
    },
    {
        "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
        "authors": [
            "Yuchen Zhuang",
            "Yue Yu",
            "Kuan Wang",
            "Haotian Sun",
            "Chao Zhang"
        ],
        "published": "2023-06-23T05:43:28Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nvarious NLP tasks, but they still suffer from challenges such as hallucination\nand weak numerical reasoning. To overcome these challenges, external tools can\nbe used to enhance LLMs' question-answering abilities. However, current\nevaluation methods do not distinguish between questions that can be answered\nusing LLMs' internal knowledge and those that require external information\nthrough tool use. To address this issue, we introduce a new dataset called\nToolQA, which is designed to faithfully evaluate LLMs' ability to use external\ntools for question answering. Our development of ToolQA involved a scalable,\nautomated process for dataset curation, along with 13 specialized tools\ndesigned for interaction with external knowledge in order to answer questions.\nImportantly, we strive to minimize the overlap between our benchmark data and\nLLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use\nreasoning abilities. We conducted an in-depth diagnosis of existing tool-use\nLLMs to highlight their strengths, weaknesses, and potential improvements. Our\nfindings set a new benchmark for evaluating LLMs and suggest new directions for\nfuture advancements. Our data and code are freely available to the broader\nscientific community on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2306.13304v1.pdf"
    },
    {
        "title": "Correcting discount-factor mismatch in on-policy policy gradient methods",
        "authors": [
            "Fengdi Che",
            "Gautham Vasan",
            "A. Rupam Mahmood"
        ],
        "published": "2023-06-23T04:10:58Z",
        "summary": "The policy gradient theorem gives a convenient form of the policy gradient in\nterms of three factors: an action value, a gradient of the action likelihood,\nand a state distribution involving discounting called the \\emph{discounted\nstationary distribution}. But commonly used on-policy methods based on the\npolicy gradient theorem ignores the discount factor in the state distribution,\nwhich is technically incorrect and may even cause degenerate learning behavior\nin some environments. An existing solution corrects this discrepancy by using\n$\\gamma^t$ as a factor in the gradient estimate. However, this solution is not\nwidely adopted and does not work well in tasks where the later states are\nsimilar to earlier states. We introduce a novel distribution correction to\naccount for the discounted stationary distribution that can be plugged into\nmany existing gradient estimators. Our correction circumvents the performance\ndegradation associated with the $\\gamma^t$ correction with a lower variance.\nImportantly, compared to the uncorrected estimators, our algorithm provides\nimproved state emphasis to evade suboptimal policies in certain environments\nand consistently matches or exceeds the original performance on several OpenAI\ngym and DeepMind suite benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2306.13284v1.pdf"
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Peter Henderson",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "published": "2023-06-22T22:13:03Z",
        "summary": "Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.",
        "pdf_link": "https://arxiv.org/pdf/2306.13213v2.pdf"
    },
    {
        "title": "Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation",
        "authors": [
            "Yuetian Chen",
            "Bowen Shi",
            "Mei Si"
        ],
        "published": "2023-06-22T20:38:52Z",
        "summary": "Artificial intelligence has made significant progress in natural language\nprocessing, with models like GPT-3 demonstrating impressive capabilities.\nHowever, these models still have limitations when it comes to complex tasks\nthat require an understanding of the user, such as mastering human comedy\nwriting strategies. This paper explores humor generation using GPT-3 by\nmodeling human comedy writing theory and leveraging step-by-step thinking\ninstructions. In addition, we explore the role of cognitive distance in\ncreating humor.",
        "pdf_link": "https://arxiv.org/pdf/2306.13195v1.pdf"
    },
    {
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
        "authors": [
            "Miao Xiong",
            "Zhiyuan Hu",
            "Xinyang Lu",
            "Yifei Li",
            "Jie Fu",
            "Junxian He",
            "Bryan Hooi"
        ],
        "published": "2023-06-22T17:31:44Z",
        "summary": "Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.13063v2.pdf"
    },
    {
        "title": "Towards Explainable Evaluation Metrics for Machine Translation",
        "authors": [
            "Christoph Leiter",
            "Piyawat Lertvittayakumjorn",
            "Marina Fomicheva",
            "Wei Zhao",
            "Yang Gao",
            "Steffen Eger"
        ],
        "published": "2023-06-22T17:07:57Z",
        "summary": "Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics for machine translation (for example, COMET or BERTScore)\nare based on black-box large language models. They often achieve strong\ncorrelations with human judgments, but recent research indicates that the\nlower-quality classical metrics remain dominant, one of the potential reasons\nbeing that their decision processes are more transparent. To foster more\nwidespread acceptance of novel high-quality metrics, explainability thus\nbecomes crucial. In this concept paper, we identify key properties as well as\nkey goals of explainable machine translation metrics and provide a\ncomprehensive synthesis of recent techniques, relating them to our established\ngoals and properties. In this context, we also discuss the latest\nstate-of-the-art approaches to explainable metrics based on generative models\nsuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generation\napproaches, including natural language explanations. We hope that our work can\nhelp catalyze and guide future research on explainable evaluation metrics and,\nmediately, also contribute to better and more transparent machine translation\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2306.13041v1.pdf"
    },
    {
        "title": "Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US",
        "authors": [
            "Jonathan H. Rystr\u00f8m"
        ],
        "published": "2023-06-22T15:56:50Z",
        "summary": "As generative language models are deployed in ever-wider contexts, concerns\nabout their political values have come to the forefront with critique from all\nparts of the political spectrum that the models are biased and lack neutrality.\nHowever, the question of what neutrality is and whether it is desirable remains\nunderexplored. In this paper, I examine neutrality through an audit of Delphi\n[arXiv:2110.07574], a large language model designed for crowdsourced ethics. I\nanalyse how Delphi responds to politically controversial questions compared to\ndifferent US political subgroups. I find that Delphi is poorly calibrated with\nrespect to confidence and exhibits a significant political skew. Based on these\nresults, I examine the question of neutrality from a data-feminist lens, in\nterms of how notions of neutrality shift power and further marginalise unheard\nvoices. These findings can hopefully contribute to a more reflexive debate\nabout the normative questions of alignment and what role we want generative\nmodels to play in society.",
        "pdf_link": "https://arxiv.org/pdf/2306.13000v1.pdf"
    },
    {
        "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
        "authors": [
            "Yelysei Bondarenko",
            "Markus Nagel",
            "Tijmen Blankevoort"
        ],
        "published": "2023-06-22T14:39:04Z",
        "summary": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
        "pdf_link": "https://arxiv.org/pdf/2306.12929v2.pdf"
    },
    {
        "title": "Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT",
        "authors": [
            "Stephen Yang"
        ],
        "published": "2023-06-22T13:21:20Z",
        "summary": "Analysis of innovation has been fundamentally limited by conventional\napproaches to broad, structural variables. This paper pushes the boundaries,\ntaking an LLM approach to patent analysis with the groundbreaking ChatGPT\ntechnology. OpenAI's state-of-the-art textual embedding accesses complex\ninformation about the quality and impact of each invention to power deep\nlearning predictive models. The nuanced embedding drives a 24% incremental\nimprovement in R-squared predicting patent value and clearly isolates the worst\nand best applications. These models enable a revision of the contemporary\nKogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median\ndeviation of 1.5 times, accounting for potential institutional predictions.\nFurthermore, the market fails to incorporate timely information about\napplications; a long-short portfolio based on predicted acceptance rates\nachieves significant abnormal returns of 3.3% annually. The models provide an\nopportunity to revolutionize startup and small-firm corporate policy vis-a-vis\npatenting.",
        "pdf_link": "https://arxiv.org/pdf/2307.01202v1.pdf"
    },
    {
        "title": "Generative Multimodal Entity Linking",
        "authors": [
            "Senbao Shi",
            "Zhenran Xu",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2023-06-22T07:57:19Z",
        "summary": "Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask. Our code is available at https://github.com/HITsz-TMG/GEMEL.",
        "pdf_link": "https://arxiv.org/pdf/2306.12725v4.pdf"
    },
    {
        "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models",
        "authors": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Xiao-Yang Liu"
        ],
        "published": "2023-06-22T03:56:38Z",
        "summary": "Sentiment analysis is a vital tool for uncovering insights from financial\narticles, news, and social media, shaping our understanding of market\nmovements. Despite the impressive capabilities of large language models (LLMs)\nin financial natural language processing (NLP), they still struggle with\naccurately interpreting numerical values and grasping financial context,\nlimiting their effectiveness in predicting financial sentiment. In this paper,\nwe introduce a simple yet effective instruction tuning approach to address\nthese issues. By transforming a small portion of supervised financial sentiment\nanalysis data into instruction data and fine-tuning a general-purpose LLM with\nthis method, we achieve remarkable advancements in financial sentiment\nanalysis. In the experiment, our approach outperforms state-of-the-art\nsupervised sentiment analysis models, as well as widely used LLMs like ChatGPT\nand LLaMAs, particularly in scenarios where numerical understanding and\ncontextual comprehension are vital.",
        "pdf_link": "https://arxiv.org/pdf/2306.12659v1.pdf"
    },
    {
        "title": "Identifying and Extracting Rare Disease Phenotypes with Large Language Models",
        "authors": [
            "Cathy Shyr",
            "Yan Hu",
            "Paul A. Harris",
            "Hua Xu"
        ],
        "published": "2023-06-22T03:52:12Z",
        "summary": "Rare diseases (RDs) are collectively common and affect 300 million people\nworldwide. Accurate phenotyping is critical for informing diagnosis and\ntreatment, but RD phenotypes are often embedded in unstructured text and\ntime-consuming to extract manually. While natural language processing (NLP)\nmodels can perform named entity recognition (NER) to automate extraction, a\nmajor bottleneck is the development of a large, annotated corpus for model\ntraining. Recently, prompt learning emerged as an NLP paradigm that can lead to\nmore generalizable results without any (zero-shot) or few labeled samples\n(few-shot). Despite growing interest in ChatGPT, a revolutionary large language\nmodel capable of following complex human prompts and generating high-quality\nresponses, none have studied its NER performance for RDs in the zero- and\nfew-shot settings. To this end, we engineered novel prompts aimed at extracting\nRD phenotypes and, to the best of our knowledge, are the first the establish a\nbenchmark for evaluating ChatGPT's performance in these settings. We compared\nits performance to the traditional fine-tuning approach and conducted an\nin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in\nhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the\nzero- and few-shot settings, respectively). Despite this, ChatGPT achieved\nsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)\nin the one-shot setting (F1 of 0.776 and 0.725). This suggests that with\nappropriate prompt engineering, ChatGPT has the potential to match or\noutperform fine-tuned language models for certain entity types with just one\nlabeled sample. While the proliferation of large language models may provide\nopportunities for supporting RD diagnosis and treatment, researchers and\nclinicians should critically evaluate model outputs and be well-informed of\ntheir limitations.",
        "pdf_link": "https://arxiv.org/pdf/2306.12656v1.pdf"
    },
    {
        "title": "FLAG: Finding Line Anomalies (in code) with Generative AI",
        "authors": [
            "Baleegh Ahmad",
            "Benjamin Tan",
            "Ramesh Karri",
            "Hammond Pearce"
        ],
        "published": "2023-06-22T03:04:56Z",
        "summary": "Code contains security and functional bugs. The process of identifying and\nlocalizing them is difficult and relies on human labor. In this work, we\npresent a novel approach (FLAG) to assist human debuggers. FLAG is based on the\nlexical capabilities of generative AI, specifically, Large Language Models\n(LLMs). Here, we input a code file then extract and regenerate each line within\nthat file for self-comparison. By comparing the original code with an\nLLM-generated alternative, we can flag notable differences as anomalies for\nfurther inspection, with features such as distance from comments and LLM\nconfidence also aiding this classification. This reduces the inspection search\nspace for the designer. Unlike other automated approaches in this area, FLAG is\nlanguage-agnostic, can work on incomplete (and even non-compiling) code and\nrequires no creation of security properties, functional tests or definition of\nrules. In this work, we explore the features that help LLMs in this\nclassification and evaluate the performance of FLAG on known bugs. We use 121\nbenchmarks across C, Python and Verilog; with each benchmark containing a known\nsecurity or functional weakness. We conduct the experiments using two state of\nthe art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approach\nmay be used by other models. FLAG can identify 101 of the defects and helps\nreduce the search space to 12-17% of source code.",
        "pdf_link": "https://arxiv.org/pdf/2306.12643v1.pdf"
    },
    {
        "title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
        "authors": [
            "Mike D'Arcy",
            "Alexis Ross",
            "Erin Bransom",
            "Bailey Kuehl",
            "Jonathan Bragg",
            "Tom Hope",
            "Doug Downey"
        ],
        "published": "2023-06-21T22:00:03Z",
        "summary": "Revising scientific papers based on peer feedback is a challenging task that\nrequires not only deep scientific knowledge and reasoning, but also the ability\nto recognize the implicit requests in high-level feedback and to choose the\nbest of many possible ways to update the manuscript in response. We introduce\nthis task for large language models and release ARIES, a dataset of review\ncomments and their corresponding paper edits, to enable training and evaluating\nmodels. We study two versions of the task: comment-edit alignment and edit\ngeneration, and evaluate several baselines, including GPT-4. We find that\nmodels struggle even to identify the edits that correspond to a comment,\nespecially in cases where the comment is phrased in an indirect way or where\nthe edit addresses the spirit of a comment but not the precise request. When\ntasked with generating edits, GPT-4 often succeeds in addressing comments on a\nsurface level, but it rigidly follows the wording of the feedback rather than\nthe underlying intent, and includes fewer technical details than human-written\nedits. We hope that our formalization, dataset, and analysis will form a\nfoundation for future work in this area.",
        "pdf_link": "https://arxiv.org/pdf/2306.12587v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases",
        "authors": [
            "Risako Ando",
            "Takanobu Morishita",
            "Hirohiko Abe",
            "Koji Mineshima",
            "Mitsuhiro Okada"
        ],
        "published": "2023-06-21T21:04:11Z",
        "summary": "This paper investigates whether current large language models exhibit biases\nin logical reasoning, similar to humans. Specifically, we focus on syllogistic\nreasoning, a well-studied form of inference in the cognitive science of human\ndeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,\noriginally designed for psychological experiments that assess human logical\nabilities in syllogistic reasoning. The dataset consists of syllogistic\ninferences in both English and Japanese. We examine three types of biases\nobserved in human syllogistic reasoning: belief biases, conversion errors, and\natmosphere effects. Our findings demonstrate that current large language models\nstruggle more with problems involving these three types of biases.",
        "pdf_link": "https://arxiv.org/pdf/2306.12567v1.pdf"
    },
    {
        "title": "FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair",
        "authors": [
            "Sakina Fatima",
            "Hadi Hemmati",
            "Lionel Briand"
        ],
        "published": "2023-06-21T19:34:16Z",
        "summary": "Flaky tests are problematic because they non-deterministically pass or fail\nfor the same software version under test, causing confusion and wasting\ndevelopment effort. While machine learning models have been used to predict\nflakiness and its root causes, there is much less work on providing support to\nfix the problem. To address this gap, in this paper, we focus on predicting the\ntype of fix that is required to remove flakiness and then repair the test code\non that basis. We do this for a subset of flaky test cases where the root cause\nof flakiness is in the test case itself and not in the production code. Our key\nidea is to guide the repair process with additional knowledge about the test's\nflakiness in the form of its predicted fix category. Thus, we first propose a\nframework that automatically generates labeled datasets for 13 fix categories\nand trains models to predict the fix category of a flaky test by analyzing the\ntest code only. Our experimental results using code models and few-shot\nlearning show that we can correctly predict most of the fix categories. To show\nthe usefulness of such fix category labels for automatically repairing\nflakiness, in addition to informing testers, we augment a Large Language Model\n(LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions.\nThe results show that our suggested fix category labels significantly enhance\nthe capability of GPT 3.5 Turbo, in generating fixes for flaky tests.",
        "pdf_link": "https://arxiv.org/pdf/2307.00012v2.pdf"
    },
    {
        "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
        "authors": [
            "Alessandro Sordoni",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Matheus Pereira",
            "Adam Trischler",
            "Ziang Xiao",
            "Arian Hosseini",
            "Friederike Niedtner",
            "Nicolas Le Roux"
        ],
        "published": "2023-06-21T18:45:56Z",
        "summary": "Large language models (LLMs) can be seen as atomic units of computation\nmapping sequences to a distribution over sequences. Thus, they can be seen as\nstochastic language layers in a language network, where the learnable\nparameters are the natural language prompts at each layer. By stacking two such\nlayers and feeding the output of one layer to the next, we obtain a Deep\nLanguage Network (DLN). We first show how to effectively perform prompt\noptimization for a 1-Layer language network (DLN-1). Then, we present an\nextension that applies to 2-layer DLNs (DLN-2), where two prompts must be\nlearned. The key idea is to consider the output of the first layer as a latent\nvariable, which requires inference, and prompts to be learned as the parameters\nof the generative distribution. We first test the effectiveness of DLN-1 in\nmultiple reasoning and natural language understanding tasks. Then, we show that\nDLN-2 can reach higher performance than a single layer, showing promise that we\nmight reach comparable performance to GPT-4, even when each LLM in the network\nis smaller and less powerful.",
        "pdf_link": "https://arxiv.org/pdf/2306.12509v2.pdf"
    },
    {
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "authors": [
            "Kanishk Gandhi",
            "Jan-Philipp Fr\u00e4nken",
            "Tobias Gerstenberg",
            "Noah D. Goodman"
        ],
        "published": "2023-06-21T16:42:15Z",
        "summary": "As Large Language Models (LLMs) become increasingly integrated into our\neveryday lives, understanding their ability to comprehend human mental states\nbecomes critical for ensuring effective interactions. However, despite the\nrecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of\nLLMs, the degree to which these models can align with human ToM remains a\nnuanced topic of exploration. This is primarily due to two distinct challenges:\n(1) the presence of inconsistent results from previous evaluations, and (2)\nconcerns surrounding the validity of existing evaluation methodologies. To\naddress these challenges, we present a novel framework for procedurally\ngenerating evaluations with LLMs by populating causal templates. Using our\nframework, we create a new social reasoning benchmark (BigToM) for LLMs which\nconsists of 25 controls and 5,000 model-written evaluations. We find that human\nparticipants rate the quality of our benchmark higher than previous\ncrowd-sourced evaluations and comparable to expert-written evaluations. Using\nBigToM, we evaluate the social reasoning capabilities of a variety of LLMs and\ncompare model performances with human performance. Our results suggest that\nGPT4 has ToM capabilities that mirror human inference patterns, though less\nreliable, while other LLMs struggle.",
        "pdf_link": "https://arxiv.org/pdf/2306.15448v2.pdf"
    },
    {
        "title": "Testing of Detection Tools for AI-Generated Text",
        "authors": [
            "Debora Weber-Wulff",
            "Alla Anohina-Naumeca",
            "Sonja Bjelobaba",
            "Tom\u00e1\u0161 Folt\u00fdnek",
            "Jean Guerrero-Dib",
            "Olumide Popoola",
            "Petr \u0160igut",
            "Lorna Waddington"
        ],
        "published": "2023-06-21T16:29:44Z",
        "summary": "Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.15666v2.pdf"
    },
    {
        "title": "GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": [
            "Philippe J. Giabbanelli"
        ],
        "published": "2023-06-21T15:42:36Z",
        "summary": "The disruptive technology provided by large-scale pre-trained language models\n(LLMs) such as ChatGPT or GPT-4 has received significant attention in several\napplication domains, often with an emphasis on high-level opportunities and\nconcerns. This paper is the first examination regarding the use of LLMs for\nscientific simulations. We focus on four modeling and simulation tasks, each\ntime assessing the expected benefits and limitations of LLMs while providing\npractical guidance for modelers regarding the steps involved. The first task is\ndevoted to explaining the structure of a conceptual model to promote the\nengagement of participants in the modeling process. The second task focuses on\nsummarizing simulation outputs, so that model users can identify a preferred\nscenario. The third task seeks to broaden accessibility to simulation platforms\nby conveying the insights of simulation visualizations via text. Finally, the\nlast task evokes the possibility of explaining simulation errors and providing\nguidance to resolve them.",
        "pdf_link": "https://arxiv.org/pdf/2306.13679v1.pdf"
    },
    {
        "title": "Solving and Generating NPR Sunday Puzzles with Large Language Models",
        "authors": [
            "Jingmiao Zhao",
            "Carolyn Jane Anderson"
        ],
        "published": "2023-06-21T13:23:48Z",
        "summary": "We explore the ability of large language models to solve and generate puzzles\nfrom the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15\nyears of on-air puzzles. We evaluate four large language models using PUZZLEQA,\nin both multiple choice and free response formats, and explore two prompt\nengineering techniques to improve free response performance: chain-of-thought\nreasoning and prompt summarization. We find that state-of-the-art large\nlanguage models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,\nachieves 50.2% loose accuracy. However, in our few-shot puzzle generation\nexperiment, we find no evidence that models can generate puzzles: GPT-3.5\ngenerates puzzles with answers that do not conform to the generated rules.\nPuzzle generation remains a challenging task for future work.",
        "pdf_link": "https://arxiv.org/pdf/2306.12255v1.pdf"
    },
    {
        "title": "Limits for Learning with Language Models",
        "authors": [
            "Nicholas Asher",
            "Swarnadeep Bhar",
            "Akshay Chaturvedi",
            "Julie Hunter",
            "Soumya Paul"
        ],
        "published": "2023-06-21T12:11:31Z",
        "summary": "With the advent of large language models (LLMs), the trend in NLP has been to\ntrain LLMs on vast amounts of data to solve diverse language understanding and\ngeneration tasks. The list of LLM successes is long and varied. Nevertheless,\nseveral recent papers provide empirical evidence that LLMs fail to capture\nimportant aspects of linguistic meaning. Focusing on universal quantification,\nwe provide a theoretical foundation for these empirical findings by proving\nthat LLMs cannot learn certain fundamental semantic properties including\nsemantic entailment and consistency as they are defined in formal semantics.\nMore generally, we show that LLMs are unable to learn concepts beyond the first\nlevel of the Borel Hierarchy, which imposes severe limits on the ability of\nLMs, both large and small, to capture many aspects of linguistic meaning. This\nmeans that LLMs will continue to operate without formal guarantees on tasks\nthat require entailments and deep linguistic understanding.",
        "pdf_link": "https://arxiv.org/pdf/2306.12213v1.pdf"
    },
    {
        "title": "Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks",
        "authors": [
            "Mohamad Ballout",
            "Ulf Krumnack",
            "Gunther Heidemann",
            "Kai-Uwe K\u00fchnberger"
        ],
        "published": "2023-06-21T11:48:07Z",
        "summary": "Investigating deep learning language models has always been a significant\nresearch area due to the ``black box\" nature of most advanced models. With the\nrecent advancements in pre-trained language models based on transformers and\ntheir increasing integration into daily life, addressing this issue has become\nmore pressing. In order to achieve an explainable AI model, it is essential to\ncomprehend the procedural steps involved and compare them with human thought\nprocesses. Thus, in this paper, we use simple, well-understood non-language\ntasks to explore these models' inner workings. Specifically, we apply a\npre-trained language model to constrained arithmetic problems with hierarchical\nstructure, to analyze their attention weight scores and hidden states. The\ninvestigation reveals promising results, with the model addressing hierarchical\nproblems in a moderately structured manner, similar to human problem-solving\nstrategies. Additionally, by inspecting the attention weights layer by layer,\nwe uncover an unconventional finding that layer 10, rather than the model's\nfinal layer, is the optimal layer to unfreeze for the least parameter-intensive\napproach to fine-tune the model. We support these findings with entropy\nanalysis and token embeddings similarity analysis. The attention analysis\nallows us to hypothesize that the model can generalize to longer sequences in\nListOps dataset, a conclusion later confirmed through testing on sequences\nlonger than those in the training set. Lastly, by utilizing a straightforward\ntask in which the model predicts the winner of a Tic Tac Toe game, we identify\nlimitations in attention analysis, particularly its inability to capture 2D\npatterns.",
        "pdf_link": "https://arxiv.org/pdf/2306.12198v1.pdf"
    },
    {
        "title": "OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue",
        "authors": [
            "Weihao Gao",
            "Zhuo Deng",
            "Zhiyuan Niu",
            "Fuju Rong",
            "Chucheng Chen",
            "Zheng Gong",
            "Wenze Zhang",
            "Daimin Xiao",
            "Fang Li",
            "Zhenjie Cao",
            "Zhaoyi Ma",
            "Wenbin Wei",
            "Lan Ma"
        ],
        "published": "2023-06-21T11:09:48Z",
        "summary": "Large multimodal language models (LMMs) have achieved significant success in\ngeneral domains. However, due to the significant differences between medical\nimages and text and general web content, the performance of LMMs in medical\nscenarios is limited. In ophthalmology, clinical diagnosis relies on multiple\nmodalities of medical images, but unfortunately, multimodal ophthalmic large\nlanguage models have not been explored to date. In this paper, we study and\nconstruct an ophthalmic large multimodal model. Firstly, we use fundus images\nas an entry point to build a disease assessment and diagnosis pipeline to\nachieve common ophthalmic disease diagnosis and lesion segmentation. Then, we\nestablish a new ophthalmic multimodal instruction-following and dialogue\nfine-tuning dataset based on disease-related knowledge data and publicly\navailable real-world medical dialogue. We introduce visual ability into the\nlarge language model to complete the ophthalmic large language and vision\nassistant (OphGLM). Our experimental results demonstrate that the OphGLM model\nperforms exceptionally well, and it has the potential to revolutionize clinical\napplications in ophthalmology. The dataset, code, and models will be made\npublicly available at https://github.com/ML-AILab/OphGLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.12174v2.pdf"
    },
    {
        "title": "Interactive Molecular Discovery with Natural Language",
        "authors": [
            "Zheni Zeng",
            "Bangchen Yin",
            "Shipeng Wang",
            "Jiarui Liu",
            "Cheng Yang",
            "Haishen Yao",
            "Xingzhi Sun",
            "Maosong Sun",
            "Guotong Xie",
            "Zhiyuan Liu"
        ],
        "published": "2023-06-21T02:05:48Z",
        "summary": "Natural language is expected to be a key medium for various human-machine\ninteractions in the era of large language models. When it comes to the\nbiochemistry field, a series of tasks around molecules (e.g., property\nprediction, molecule mining, etc.) are of great significance while having a\nhigh technical threshold. Bridging the molecule expressions in natural language\nand chemical language can not only hugely improve the interpretability and\nreduce the operation difficulty of these tasks, but also fuse the chemical\nknowledge scattered in complementary materials for a deeper comprehension of\nmolecules. Based on these benefits, we propose the conversational molecular\ndesign, a novel task adopting natural language for describing and editing\ntarget molecules. To better accomplish this task, we design ChatMol, a\nknowledgeable and versatile generative pre-trained model, enhanced by injecting\nexperimental property information, molecular spatial knowledge, and the\nassociations between natural and chemical languages into it. Several typical\nsolutions including large language models (e.g., ChatGPT) are evaluated,\nproving the challenge of conversational molecular design and the effectiveness\nof our knowledge enhancement method. Case observations and analysis are\nconducted to provide directions for further exploration of natural-language\ninteraction in molecular discovery.",
        "pdf_link": "https://arxiv.org/pdf/2306.11976v1.pdf"
    },
    {
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "authors": [
            "Christopher T. Small",
            "Ivan Vendrov",
            "Esin Durmus",
            "Hadjar Homaei",
            "Elizabeth Barry",
            "Julien Cornebise",
            "Ted Suzman",
            "Deep Ganguli",
            "Colin Megill"
        ],
        "published": "2023-06-20T22:52:51Z",
        "summary": "Polis is a platform that leverages machine intelligence to scale up\ndeliberative processes. In this paper, we explore the opportunities and risks\nassociated with applying Large Language Models (LLMs) towards challenges with\nfacilitating, moderating and summarizing the results of Polis engagements. In\nparticular, we demonstrate with pilot experiments using Anthropic's Claude that\nLLMs can indeed augment human intelligence to help more efficiently run Polis\nconversations. In particular, we find that summarization capabilities enable\ncategorically new methods with immense promise to empower the public in\ncollective meaning-making exercises. And notably, LLM context limitations have\na significant impact on insight and quality of these results.\n  However, these opportunities come with risks. We discuss some of these risks,\nas well as principles and techniques for characterizing and mitigating them,\nand the implications for other deliberative or political systems that may\nemploy LLMs. Finally, we conclude with several open future research directions\nfor augmenting tools like Polis with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.11932v1.pdf"
    },
    {
        "title": "Open-Domain Text Evaluation via Meta Distribution Modeling",
        "authors": [
            "Sidi Lu",
            "Asli Celikyilmaz",
            "Tianlu Wang",
            "Nanyun Peng"
        ],
        "published": "2023-06-20T20:37:54Z",
        "summary": "Recent advances in open-domain text generation models powered by large\npre-trained language models (LLMs) have achieved remarkable performance.\nHowever, evaluating and controlling these models for desired attributes remains\na challenge, as traditional reference-based metrics such as BLEU, ROUGE, and\nMETEOR are insufficient for open-ended generation tasks. Similarly, while\ntrainable discriminator-based evaluation metrics show promise, obtaining\nhigh-quality training data is a non-trivial task. In this paper, we introduce a\nnovel approach to evaluate open-domain generation - the Meta-Distribution\nMethods (MDM). Drawing on the correlation between the rising parameter counts\nand the improving performance of LLMs, MDM creates a mapping from the contrast\nof two probabilistic distributions -- one known to be superior to the other --\nto quality measures, which can be viewed as a distribution of distributions\ni.e. Meta-Distribution. We investigate MDM for open-domain text generation\nevaluation under two paradigms: 1) \\emph{Generative} MDM, which leverages the\nMeta-Distribution Methods to generate in-domain negative samples for training\ndiscriminator-based metrics; 2) \\emph{Discriminative} MDM, which directly uses\ndistribution discrepancies between two language models for evaluation. Our\nexperiments on multi-turn dialogue and factuality in abstractive summarization\ndemonstrate that MDMs correlate better with human judgment than existing\nautomatic evaluation metrics on both tasks, highlighting the strong performance\nand generalizability of such methods.",
        "pdf_link": "https://arxiv.org/pdf/2306.11879v1.pdf"
    },
    {
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
        "authors": [
            "Boxin Wang",
            "Weixin Chen",
            "Hengzhi Pei",
            "Chulin Xie",
            "Mintong Kang",
            "Chenhui Zhang",
            "Chejian Xu",
            "Zidi Xiong",
            "Ritik Dutta",
            "Rylan Schaeffer",
            "Sang T. Truong",
            "Simran Arora",
            "Mantas Mazeika",
            "Dan Hendrycks",
            "Zinan Lin",
            "Yu Cheng",
            "Sanmi Koyejo",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2023-06-20T17:24:23Z",
        "summary": "Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/ ; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .",
        "pdf_link": "https://arxiv.org/pdf/2306.11698v5.pdf"
    },
    {
        "title": "A Simple and Effective Pruning Approach for Large Language Models",
        "authors": [
            "Mingjie Sun",
            "Zhuang Liu",
            "Anna Bair",
            "J. Zico Kolter"
        ],
        "published": "2023-06-20T17:18:20Z",
        "summary": "As their size increases, Large Languages Models (LLMs) are natural candidates\nfor network pruning methods: approaches that drop a subset of network weights\nwhile striving to preserve performance. Existing methods, however, require\neither retraining, which is rarely affordable for billion-scale LLMs, or\nsolving a weight reconstruction problem reliant on second-order information,\nwhich may also be computationally expensive. In this paper, we introduce a\nnovel, straightforward yet effective pruning method, termed Wanda (Pruning by\nWeights and activations), designed to induce sparsity in pretrained LLMs.\nMotivated by the recent observation of emergent large magnitude features in\nLLMs, our approach prunes weights with the smallest magnitudes multiplied by\nthe corresponding input activations, on a per-output basis. Notably, Wanda\nrequires no retraining or weight update, and the pruned LLM can be used as is.\nWe conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2\nacross various language benchmarks. Wanda significantly outperforms the\nestablished baseline of magnitude pruning and performs competitively against\nrecent method involving intensive weight update. Code is available at\nhttps://github.com/locuslab/wanda.",
        "pdf_link": "https://arxiv.org/pdf/2306.11695v2.pdf"
    },
    {
        "title": "Towards Environmentally Equitable AI via Geographical Load Balancing",
        "authors": [
            "Pengfei Li",
            "Jianyi Yang",
            "Adam Wierman",
            "Shaolei Ren"
        ],
        "published": "2023-06-20T17:13:33Z",
        "summary": "Fueled by the soaring popularity of large language and foundation models, the\naccelerated growth of artificial intelligence (AI) models' enormous\nenvironmental footprint has come under increased scrutiny. While many\napproaches have been proposed to make AI more energy-efficient and\nenvironmentally friendly, environmental inequity -- the fact that AI's\nenvironmental footprint can be disproportionately higher in certain regions\nthan in others -- has emerged, raising social-ecological justice concerns. This\npaper takes a first step toward addressing AI's environmental inequity by\nbalancing its regional negative environmental impact. Concretely, we focus on\nthe carbon and water footprints of AI model inference and propose equity-aware\ngeographical load balancing (GLB) to explicitly address AI's environmental\nimpacts on the most disadvantaged regions. We run trace-based simulations by\nconsidering a set of 10 geographically-distributed data centers that serve\ninference requests for a large language AI model. The results demonstrate that\nexisting GLB approaches may amplify environmental inequity while our proposed\nequity-aware GLB can significantly reduce the regional disparity in terms of\ncarbon and water footprints.",
        "pdf_link": "https://arxiv.org/pdf/2307.05494v1.pdf"
    },
    {
        "title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion",
        "authors": [
            "Simone Bianco",
            "Luigi Celona",
            "Marco Donzella",
            "Paolo Napoletano"
        ],
        "published": "2023-06-20T15:13:02Z",
        "summary": "State-of-The-Art (SoTA) image captioning models often rely on the Microsoft\nCOCO (MS-COCO) dataset for training. This dataset contains annotations provided\nby human annotators, who typically produce captions averaging around ten\ntokens. However, this constraint presents a challenge in effectively capturing\ncomplex scenes and conveying detailed information. Furthermore, captioning\nmodels tend to exhibit bias towards the ``average'' caption, which captures\nonly the more general aspects. What would happen if we were able to\nautomatically generate longer captions, thereby making them more detailed?\nWould these captions, evaluated by humans, be more or less representative of\nthe image content compared to the original MS-COCO captions? In this paper, we\npresent a novel approach to address previous challenges by showcasing how\ncaptions generated from different SoTA models can be effectively fused,\nresulting in richer captions. Our proposed method leverages existing models\nfrom the literature, eliminating the need for additional training. Instead, it\nutilizes an image-text based metric to rank the captions generated by SoTA\nmodels for a given image. Subsequently, the top two captions are fused using a\nLarge Language Model (LLM). Experimental results demonstrate the effectiveness\nof our approach, as the captions generated by our model exhibit higher\nconsistency with human judgment when evaluated on the MS-COCO test set. By\ncombining the strengths of various SoTA models, our method enhances the quality\nand appeal of image captions, bridging the gap between automated systems and\nthe rich, informative nature of human-generated descriptions. This advance\nopens up new possibilities for generating captions that are more suitable for\nthe training of both vision-language and captioning models.",
        "pdf_link": "https://arxiv.org/pdf/2306.11593v1.pdf"
    },
    {
        "title": "Hallucination is the last thing you need",
        "authors": [
            "Shawn Curran",
            "Sam Lansley",
            "Oliver Bethell"
        ],
        "published": "2023-06-20T13:14:15Z",
        "summary": "The legal profession necessitates a multidimensional approach that involves\nsynthesizing an in-depth comprehension of a legal issue with insightful\ncommentary based on personal experience, combined with a comprehensive\nunderstanding of pertinent legislation, regulation, and case law, in order to\ndeliver an informed legal solution. The present offering with generative AI\npresents major obstacles in replicating this, as current models struggle to\nintegrate and navigate such a complex interplay of understanding, experience,\nand fact-checking procedures. It is noteworthy that where generative AI outputs\nunderstanding and experience, which reflect the aggregate of various subjective\nviews on similar topics, this often deflects the model's attention from the\ncrucial legal facts, thereby resulting in hallucination. Hence, this paper\ndelves into the feasibility of three independent LLMs, each focused on\nunderstanding, experience, and facts, synthesising as one single ensemble model\nto effectively counteract the current challenges posed by the existing\nmonolithic generative AI models. We introduce an idea of mutli-length\ntokenisation to protect key information assets like common law judgements, and\nfinally we interrogate the most advanced publicly available models for legal\nhallucination, with some interesting results.",
        "pdf_link": "https://arxiv.org/pdf/2306.11520v1.pdf"
    },
    {
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
        "authors": [
            "Yue Huang",
            "Qihui Zhang",
            "Philip S. Y",
            "Lichao Sun"
        ],
        "published": "2023-06-20T12:53:39Z",
        "summary": "Large Language Models (LLMs) such as ChatGPT, have gained significant\nattention due to their impressive natural language processing capabilities. It\nis crucial to prioritize human-centered principles when utilizing these models.\nSafeguarding the ethical and moral compliance of LLMs is of utmost importance.\nHowever, individual ethical issues have not been well studied on the latest\nLLMs. Therefore, this study aims to address these gaps by introducing a new\nbenchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in\nthree crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT\nexamines toxicity in language models by employing toxic prompt templates\nderived from social norms. It then quantifies the extent of bias in models by\nmeasuring quantifiable toxicity values across different groups. Lastly,\nTrustGPT assesses the value of conversation generation models from both active\nvalue-alignment and passive value-alignment tasks. Through the implementation\nof TrustGPT, this research aims to enhance our understanding of the performance\nof conversation generation models and promote the development of language\nmodels that are more ethical and socially responsible.",
        "pdf_link": "https://arxiv.org/pdf/2306.11507v1.pdf"
    },
    {
        "title": "Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling",
        "authors": [
            "Linyao Yang",
            "Hongyang Chen",
            "Zhao Li",
            "Xiao Ding",
            "Xindong Wu"
        ],
        "published": "2023-06-20T12:21:06Z",
        "summary": "Recently, ChatGPT, a representative large language model (LLM), has gained\nconsiderable attention due to its powerful emergent abilities. Some researchers\nsuggest that LLMs could potentially replace structured knowledge bases like\nknowledge graphs (KGs) and function as parameterized knowledge bases. However,\nwhile LLMs are proficient at learning probabilistic language patterns based on\nlarge corpus and engaging in conversations with humans, they, like previous\nsmaller pre-trained language models (PLMs), still have difficulty in recalling\nfacts while generating knowledge-grounded contents. To overcome these\nlimitations, researchers have proposed enhancing data-driven PLMs with\nknowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus\nimproving their performance to generate texts requiring factual knowledge and\nproviding more informed responses to user queries. This paper reviews the\nstudies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced\npre-trained language models (KGPLMs) as well as their applications. Inspired by\nexisting studies on KGPLM, this paper proposes to enhance LLMs with KGs by\ndeveloping knowledge graph-enhanced large language models (KGLLMs). KGLLM\nprovides a solution to enhance LLMs' factual reasoning ability, opening up new\navenues for LLM research.",
        "pdf_link": "https://arxiv.org/pdf/2306.11489v2.pdf"
    },
    {
        "title": "Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Motivations and Formal Specifications",
        "authors": [
            "Paola Merlo"
        ],
        "published": "2023-06-20T10:45:56Z",
        "summary": "We motivate and formally define a new task for fine-tuning rule-like\ngeneralization in large language models. It is conjectured that the\nshortcomings of current LLMs are due to a lack of ability to generalize. It has\nbeen argued that, instead, humans are better at generalization because they\nhave a tendency at extracting rules from complex data. We try to recreate this\ntendency to rule-based generalization. When exposed to tests of analytic\nintelligence, for example, the visual RAVEN IQ test, human problem-solvers\nidentify the relevant objects in the picture and their relevant attributes and\nreason based on rules applied to these objects and attributes. Based on the\ninduced rules, they are able to provide a solution to the test. We propose a\ntask that translates this IQ task into language. In this paper, we provide the\nformal specification for the task and the generative process of its datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.11444v1.pdf"
    },
    {
        "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts",
        "authors": [
            "Xuan-Phi Nguyen",
            "Sharifah Mahani Aljunied",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023-06-20T08:27:47Z",
        "summary": "Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2306.11372v1.pdf"
    },
    {
        "title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
        "authors": [
            "Zhiling Zheng",
            "Oufan Zhang",
            "Christian Borgs",
            "Jennifer T. Chayes",
            "Omar M. Yaghi"
        ],
        "published": "2023-06-20T05:20:29Z",
        "summary": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.",
        "pdf_link": "https://arxiv.org/pdf/2306.11296v2.pdf"
    },
    {
        "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
        "authors": [
            "Jiuding Sun",
            "Chantal Shaib",
            "Byron C. Wallace"
        ],
        "published": "2023-06-20T03:48:51Z",
        "summary": "Instruction fine-tuning has recently emerged as a promising approach for\nimproving the zero-shot capabilities of Large Language Models (LLMs) on new\ntasks. This technique has shown particular strength in improving the\nperformance of modestly sized LLMs, sometimes inducing performance competitive\nwith much larger model variants. In this paper we ask two questions: (1) How\nsensitive are instruction-tuned models to the particular phrasings of\ninstructions, and, (2) How can we make them more robust to such natural\nlanguage variation? To answer the former, we collect a set of 319 instructions\nmanually written by NLP practitioners for over 80 unique tasks included in\nwidely used benchmarks, and we evaluate the variance and average performance of\nthese instructions as compared to instruction phrasings observed during\ninstruction fine-tuning. We find that using novel (unobserved) but appropriate\ninstruction phrasings consistently degrades model performance, sometimes\nsubstantially so. Further, such natural instructions yield a wide variance in\ndownstream performance, despite their semantic equivalence. Put another way,\ninstruction-tuned models are not especially robust to instruction re-phrasings.\nWe propose a simple method to mitigate this issue by introducing ``soft\nprompt'' embedding parameters and optimizing these to maximize the similarity\nbetween representations of semantically equivalent instructions. We show that\nthis method consistently improves the robustness of instruction-tuned models.",
        "pdf_link": "https://arxiv.org/pdf/2306.11270v2.pdf"
    },
    {
        "title": "Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset",
        "authors": [
            "Saeid Naeini",
            "Raeid Saqur",
            "Mozhgan Saeidi",
            "John Giorgi",
            "Babak Taati"
        ],
        "published": "2023-06-19T21:14:57Z",
        "summary": "The quest for human imitative AI has been an enduring topic in AI research\nsince its inception. The technical evolution and emerging capabilities of the\nlatest cohort of large language models (LLMs) have reinvigorated the subject\nbeyond academia to the cultural zeitgeist. While recent NLP evaluation\nbenchmark tasks test some aspects of human-imitative behaviour (e.g.,\nBIG-bench's 'human-like behavior' tasks), few, if not none, examine creative\nproblem solving abilities. Creative problem solving in humans is a well-studied\ntopic in cognitive neuroscience with standardized tests that predominantly use\nthe ability to associate (heterogeneous) connections among clue words as a\nmetric for creativity. Exposure to misleading stimuli - distractors dubbed red\nherrings - impede human performance in such tasks via the fixation effect and\nEinstellung paradigm. In cognitive neuroscience studies, such fixations are\nexperimentally induced by pre-exposing participants to orthographically similar\nincorrect words to subsequent word-fragments or clues. The popular British quiz\nshow Only Connect's Connecting Wall segment essentially mimics Mednick's Remote\nAssociates Test (RAT) formulation with built-in, deliberate red herrings, which\nmakes it an ideal proxy dataset to explore and study fixation effect and\nEinstellung paradigm from cognitive neuroscience in LLMs. In this paper we\npresent the novel Only Connect Wall (OCW) dataset and report results from our\nevaluation of selected pre-trained language models and LLMs on creative problem\nsolving tasks like grouping clue words by heterogeneous connections, and\nidentifying correct open knowledge domain connections in respective groups. We\nsynthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to\nfurther analyze our red-herrings hypothesis in language models. The code and\nlink to the dataset are available at https://github.com/TaatiTeam/OCW.",
        "pdf_link": "https://arxiv.org/pdf/2306.11167v4.pdf"
    },
    {
        "title": "Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting",
        "authors": [
            "Xinli Yu",
            "Zheng Chen",
            "Yuan Ling",
            "Shujing Dong",
            "Zongyi Liu",
            "Yanbin Lu"
        ],
        "published": "2023-06-19T15:42:02Z",
        "summary": "This paper presents a novel study on harnessing Large Language Models' (LLMs)\noutstanding knowledge and reasoning abilities for explainable financial time\nseries forecasting. The application of machine learning models to financial\ntime series comes with several challenges, including the difficulty in\ncross-sequence reasoning and inference, the hurdle of incorporating multi-modal\nsignals from historical news, financial knowledge graphs, etc., and the issue\nof interpreting and explaining the model results. In this paper, we focus on\nNASDAQ-100 stocks, making use of publicly accessible historical stock price\ndata, company metadata, and historical economic/financial news. We conduct\nexperiments to illustrate the potential of LLMs in offering a unified solution\nto the aforementioned challenges. Our experiments include trying\nzero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with\na public LLM model Open LLaMA. We demonstrate our approach outperforms a few\nbaselines, including the widely applied classic ARMA-GARCH model and a\ngradient-boosting tree model. Through the performance comparison results and a\nfew examples, we find LLMs can make a well-thought decision by reasoning over\ninformation from both textual news and price time series and extracting\ninsights, leveraging cross-sequence information, and utilizing the inherent\nknowledge embedded within the LLM. Additionally, we show that a publicly\navailable LLM such as Open-LLaMA, after fine-tuning, can comprehend the\ninstruction to generate explainable forecasts and achieve reasonable\nperformance, albeit relatively inferior in comparison to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2306.11025v1.pdf"
    },
    {
        "title": "RepoFusion: Training Code Models to Understand Your Repository",
        "authors": [
            "Disha Shrivastava",
            "Denis Kocetkov",
            "Harm de Vries",
            "Dzmitry Bahdanau",
            "Torsten Scholak"
        ],
        "published": "2023-06-19T15:05:31Z",
        "summary": "Despite the huge success of Large Language Models (LLMs) in coding assistants\nlike GitHub Copilot, these models struggle to understand the context present in\nthe repository (e.g., imports, parent classes, files with similar names, etc.),\nthereby producing inaccurate code completions. This effect is more pronounced\nwhen using these assistants for repositories that the model has not seen during\ntraining, such as proprietary software or work-in-progress code projects.\nRecent work has shown the promise of using context from the repository during\ninference. In this work, we extend this idea and propose RepoFusion, a\nframework to train models to incorporate relevant repository context.\nExperiments on single-line code completion show that our models trained with\nrepository context significantly outperform much larger code models as\nCodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of\nthe $\\sim 70\\times$ larger StarCoderBase model that was trained with the\nFill-in-the-Middle objective. We find these results to be a novel and\ncompelling demonstration of the gains that training with repository context can\nbring. We carry out extensive ablation studies to investigate the impact of\ndesign choices such as context type, number of contexts, context length, and\ninitialization within our framework. Lastly, we release Stack-Repo, a dataset\nof 200 Java repositories with permissive licenses and near-deduplicated files\nthat are augmented with three types of repository contexts. Additionally, we\nare making available the code and trained checkpoints for our work. Our\nreleased resources can be found at \\url{https://huggingface.co/RepoFusion}.",
        "pdf_link": "https://arxiv.org/pdf/2306.10998v1.pdf"
    },
    {
        "title": "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models",
        "authors": [
            "Shaolei Zhang",
            "Qingkai Fang",
            "Zhuocheng Zhang",
            "Zhengrui Ma",
            "Yan Zhou",
            "Langlin Huang",
            "Mengyu Bu",
            "Shangtong Gui",
            "Yunji Chen",
            "Xilin Chen",
            "Yang Feng"
        ],
        "published": "2023-06-19T14:30:52Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable prowess in language\nunderstanding and generation. Advancing from foundation LLMs to\ninstructionfollowing LLMs, instruction tuning plays a vital role in aligning\nLLMs to human preferences. However, the existing LLMs are usually focused on\nEnglish, leading to inferior performance in non-English languages. In order to\nimprove the performance for non-English languages, it is necessary to collect\nlanguage-specific training data for foundation LLMs and construct\nlanguage-specific instructions for instruction tuning, both of which are heavy\nloads. To minimize human workload, we propose to transfer the capabilities of\nlanguage generation and instruction following from English to other languages\nthrough an interactive translation task. We have developed BayLing, an\ninstruction-following LLM by utilizing LLaMA as the foundation LLM and\nautomatically constructing interactive translation instructions for instructing\ntuning. Extensive assessments demonstrate that BayLing achieves comparable\nperformance to GPT-3.5-turbo, despite utilizing a considerably smaller\nparameter size of only 13 billion. Experimental results on translation tasks\nshow that BayLing achieves 95% of single-turn translation capability compared\nto GPT-4 with automatic evaluation and 96% of interactive translation\ncapability compared to GPT-3.5-turbo with human evaluation. To estimate the\nperformance on general tasks, we created a multi-turn instruction test set\ncalled BayLing-80. The experimental results on BayLing-80 indicate that BayLing\nachieves 89% of performance compared to GPT-3.5-turbo. BayLing also\ndemonstrates outstanding performance on knowledge assessment of Chinese GaoKao\nand English SAT, second only to GPT-3.5-turbo among a multitude of\ninstruction-following LLMs. Demo, homepage, code and models of BayLing are\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2306.10968v2.pdf"
    },
    {
        "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
        "authors": [
            "Jun-Min Lee",
            "Tae-Bin Ha"
        ],
        "published": "2023-06-19T10:22:12Z",
        "summary": "Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.",
        "pdf_link": "https://arxiv.org/pdf/2306.17181v4.pdf"
    },
    {
        "title": "Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost",
        "authors": [
            "Juexiao Zhou",
            "Xiuying Chen",
            "Xin Gao"
        ],
        "published": "2023-06-19T08:15:14Z",
        "summary": "Medical artificial general intelligence (AGI) is an emerging field that aims\nto develop systems specifically designed for medical applications that possess\nthe ability to understand, learn, and apply knowledge across a wide range of\ntasks and domains. Large language models (LLMs) represent a significant step\ntowards AGI. However, training cross-domain LLMs in the medical field poses\nsignificant challenges primarily attributed to the requirement of collecting\ndata from diverse domains. This task becomes particularly difficult due to\nprivacy restrictions and the scarcity of publicly available medical datasets.\nHere, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific\nmedical LLMs with the lowest cost, and suggest a possible path to achieve\nmedical AGI. With an increasing number of domain-specific professional\nmultimodal LLMs in the medical field being developed, MedAGI is designed to\nautomatically select appropriate medical models by analyzing users' questions\nwith our novel adaptive expert selection algorithm. It offers a unified\napproach to existing LLMs in the medical field, eliminating the need for\nretraining regardless of the introduction of new models. This characteristic\nrenders it a future-proof solution in the dynamically advancing medical domain.\nTo showcase the resilience of MedAGI, we conducted an evaluation across three\ndistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis\nof pathology pictures. The results demonstrated that MedAGI exhibited\nremarkable versatility and scalability, delivering exceptional performance\nacross diverse domains. Our code is publicly available to facilitate further\nresearch at https://github.com/JoshuaChou2018/MedAGI.",
        "pdf_link": "https://arxiv.org/pdf/2306.10765v1.pdf"
    },
    {
        "title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
        "authors": [
            "Teodoro Baldazzi",
            "Luigi Bellomarini",
            "Stefano Ceri",
            "Andrea Colombo",
            "Andrea Gentili",
            "Emanuel Sallinger"
        ],
        "published": "2023-06-19T06:48:45Z",
        "summary": "Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to\ndiverse goals, thanks to task-specific training data. Task specificity should\ngo hand in hand with domain orientation, that is, the specialization of an LLM\nto accurately address the tasks of a given realm of interest. However, models\nare usually fine-tuned over publicly available data or, at most, over ground\ndata from databases, ignoring business-level definitions and domain experience.\nOn the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and\naugment such domain knowledge via ontological reasoning. With the goal of\ncombining LLM flexibility with the domain orientation of EKGs, we propose a\nnovel neurosymbolic architecture that leverages the power of ontological\nreasoning to build task- and domain-specific corpora for LLM fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.10723v2.pdf"
    },
    {
        "title": "Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix)",
        "authors": [
            "Cansu Koyuturk",
            "Mona Yavari",
            "Emily Theophilou",
            "Sathya Bursic",
            "Gregor Donabauer",
            "Alessia Telari",
            "Alessia Testa",
            "Raffaele Boiano",
            "Alessandro Gabbiadini",
            "Davinia Hernandez-Leo",
            "Martin Ruskov",
            "Dimitri Ognibene"
        ],
        "published": "2023-06-18T22:23:18Z",
        "summary": "Educational chatbots come with a promise of interactive and personalized\nlearning experiences, yet their development has been limited by the restricted\nfree interaction capabilities of available platforms and the difficulty of\nencoding knowledge in a suitable format. Recent advances in language learning\nmodels with zero-shot learning capabilities, such as ChatGPT, suggest a new\npossibility for developing educational chatbots using a prompt-based approach.\nWe present a case study with a simple system that enables mixed-turn chatbot\ninteractions and discuss the insights and preliminary guidelines obtained from\ninitial tests. We examine ChatGPT's ability to pursue multiple interconnected\nlearning objectives, adapt the educational activity to users' characteristics,\nsuch as culture, age, and level of education, and its ability to use diverse\neducational strategies and conversational styles. Although the results are\nencouraging, challenges are posed by the limited history maintained for the\nconversation and the highly structured form of responses by ChatGPT, as well as\ntheir variability, which can lead to an unexpected switch of the chatbot's role\nfrom a teacher to a therapist. We provide some initial guidelines to address\nthese issues and to facilitate the development of effective educational\nchatbots.",
        "pdf_link": "https://arxiv.org/pdf/2306.10645v2.pdf"
    },
    {
        "title": "The Importance of Human-Labeled Data in the Era of LLMs",
        "authors": [
            "Yang Liu"
        ],
        "published": "2023-06-18T12:12:03Z",
        "summary": "The advent of large language models (LLMs) has brought about a revolution in\nthe development of tailored machine learning models and sparked debates on\nredefining data requirements. The automation facilitated by the training and\nimplementation of LLMs has led to discussions and aspirations that human-level\nlabeling interventions may no longer hold the same level of importance as in\nthe era of supervised learning. This paper presents compelling arguments\nsupporting the ongoing relevance of human-labeled data in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.14910v1.pdf"
    },
    {
        "title": "Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources",
        "authors": [
            "Paul Denny",
            "Hassan Khosravi",
            "Arto Hellas",
            "Juho Leinonen",
            "Sami Sarsa"
        ],
        "published": "2023-06-18T09:49:21Z",
        "summary": "As an increasing number of students move to online learning platforms that\ndeliver personalized learning experiences, there is a great need for the\nproduction of high-quality educational content. Large language models (LLMs)\nappear to offer a promising solution to the rapid creation of learning\nmaterials at scale, reducing the burden on instructors. In this study, we\ninvestigated the potential for LLMs to produce learning resources in an\nintroductory programming context, by comparing the quality of the resources\ngenerated by an LLM with those created by students as part of a learnersourcing\nactivity. Using a blind evaluation, students rated the correctness and\nhelpfulness of resources generated by AI and their peers, after both were\ninitially provided with identical exemplars. Our results show that the quality\nof AI-generated resources, as perceived by students, is equivalent to the\nquality of resources generated by their peers. This suggests that AI-generated\nresources may serve as viable supplementary material in certain contexts.\nResources generated by LLMs tend to closely mirror the given exemplars, whereas\nstudent-generated resources exhibit greater variety in terms of content length\nand specific syntax features used. The study highlights the need for further\nresearch exploring different types of learning resources and a broader range of\nsubject areas, and understanding the long-term impact of AI-generated resources\non learning outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2306.10509v2.pdf"
    },
    {
        "title": "News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",
        "authors": [
            "Kevin Matthe Caramancion"
        ],
        "published": "2023-06-18T04:30:29Z",
        "summary": "This study aimed to evaluate the proficiency of prominent Large Language\nModels (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and\nMicrosoft's Bing AI in discerning the truthfulness of news items using black\nbox testing. A total of 100 fact-checked news items, all sourced from\nindependent fact-checking agencies, were presented to each of these LLMs under\ncontrolled conditions. Their responses were classified into one of three\ncategories: True, False, and Partially True/False. The effectiveness of the\nLLMs was gauged based on the accuracy of their classifications against the\nverified facts provided by the independent agencies. The results showed a\nmoderate proficiency across all models, with an average score of 65.25 out of\n100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71,\nsuggesting an edge in newer LLMs' abilities to differentiate fact from\ndeception. However, when juxtaposed against the performance of human\nfact-checkers, the AI models, despite showing promise, lag in comprehending the\nsubtleties and contexts inherent in news information. The findings highlight\nthe potential of AI in the domain of fact-checking while underscoring the\ncontinued importance of human cognitive skills and the necessity for persistent\nadvancements in AI capabilities. Finally, the experimental data produced from\nthe simulation of this work is openly available on Kaggle.",
        "pdf_link": "https://arxiv.org/pdf/2306.17176v1.pdf"
    },
    {
        "title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents",
        "authors": [
            "Jeongeun Park",
            "Seungwon Lim",
            "Joonhyung Lee",
            "Sangbeom Park",
            "Minsuk Chang",
            "Youngjae Yu",
            "Sungjoon Choi"
        ],
        "published": "2023-06-17T15:24:54Z",
        "summary": "In this paper, we focus on inferring whether the given user command is clear,\nambiguous, or infeasible in the context of interactive robotic agents utilizing\nlarge language models (LLMs). To tackle this problem, we first present an\nuncertainty estimation method for LLMs to classify whether the command is\ncertain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command\nis classified as uncertain, we further distinguish it between ambiguous or\ninfeasible commands leveraging LLMs with situational aware context in a\nzero-shot manner. For ambiguous commands, we disambiguate the command by\ninteracting with users via question generation with LLMs. We believe that\nproper recognition of the given commands could lead to a decrease in\nmalfunction and undesired actions of the robot, enhancing the reliability of\ninteractive robot agents. We present a dataset for robotic situational\nawareness, consisting pair of high-level commands, scene descriptions, and\nlabels of command type (i.e., clear, ambiguous, or infeasible). We validate the\nproposed method on the collected dataset, pick-and-place tabletop simulation.\nFinally, we demonstrate the proposed approach in real-world human-robot\ninteraction experiments, i.e., handover scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.10376v5.pdf"
    },
    {
        "title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning",
        "authors": [
            "Yunlong Tang",
            "Jinrui Zhang",
            "Xiangchen Wang",
            "Teng Wang",
            "Feng Zheng"
        ],
        "published": "2023-06-17T13:55:54Z",
        "summary": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .",
        "pdf_link": "https://arxiv.org/pdf/2306.10354v1.pdf"
    },
    {
        "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
        "authors": [
            "Stephanie Schoch",
            "Ritwick Mishra",
            "Yangfeng Ji"
        ],
        "published": "2023-06-16T20:07:38Z",
        "summary": "Although Shapley values have been shown to be highly effective for\nidentifying harmful training instances, dataset size and model complexity\nconstraints limit the ability to apply Shapley-based data valuation to\nfine-tuning large pre-trained language models. To address this, we propose\nTS-DShapley, an algorithm that reduces computational cost of Shapley-based data\nvaluation through: 1) an efficient sampling-based method that aggregates\nShapley values computed from subsets for valuation of the entire training set,\nand 2) a value transfer method that leverages value information extracted from\na simple classifier trained using representations from the target language\nmodel. Our experiments applying TS-DShapley to select data for fine-tuning\nBERT-based language models on benchmark natural language understanding (NLU)\ndatasets show that TS-DShapley outperforms existing data selection methods.\nFurther, TS-DShapley can filter fine-tuning data to increase language model\nperformance compared to training with the full fine-tuning dataset.",
        "pdf_link": "https://arxiv.org/pdf/2306.10165v1.pdf"
    },
    {
        "title": "Evaluating Superhuman Models with Consistency Checks",
        "authors": [
            "Lukas Fluri",
            "Daniel Paleka",
            "Florian Tram\u00e8r"
        ],
        "published": "2023-06-16T17:26:38Z",
        "summary": "If machine learning models were to achieve superhuman abilities at various\nreasoning or decision-making tasks, how would we go about evaluating such\nmodels, given that humans would necessarily be poor proxies for ground truth?\nIn this paper, we propose a framework for evaluating superhuman models via\nconsistency checks. Our premise is that while the correctness of superhuman\ndecisions may be impossible to evaluate, we can still surface mistakes if the\nmodel's decisions fail to satisfy certain logical, human-interpretable rules.\nWe instantiate our framework on three tasks where correctness of decisions is\nhard to evaluate due to either superhuman model abilities, or to otherwise\nmissing ground truth: evaluating chess positions, forecasting future events,\nand making legal judgments. We show that regardless of a model's (possibly\nsuperhuman) performance on these tasks, we can discover logical inconsistencies\nin decision making. For example: a chess engine assigning opposing valuations\nto semantically identical boards; GPT-4 forecasting that sports records will\nevolve non-monotonically over time; or an AI judge assigning bail to a\ndefendant only after we add a felony to their criminal record.",
        "pdf_link": "https://arxiv.org/pdf/2306.09983v3.pdf"
    },
    {
        "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation",
        "authors": [
            "Guangyu Wang",
            "Guoxing Yang",
            "Zongxin Du",
            "Longjun Fan",
            "Xiaohu Li"
        ],
        "published": "2023-06-16T16:56:32Z",
        "summary": "Large language models have exhibited exceptional performance on various\nNatural Language Processing (NLP) tasks, leveraging techniques such as the\npre-training, and instruction fine-tuning. Despite these advances, their\neffectiveness in medical applications is limited, due to challenges such as\nfactual inaccuracies, reasoning abilities, and lack grounding in real-world\nexperience. In this study, we present ClinicalGPT, a language model explicitly\ndesigned and optimized for clinical scenarios. By incorporating extensive and\ndiverse real-world data, such as medical records, domain-specific knowledge,\nand multi-round dialogue consultations in the training process, ClinicalGPT is\nbetter prepared to handle multiple clinical task. Furthermore, we introduce a\ncomprehensive evaluation framework that includes medical knowledge\nquestion-answering, medical exams, patient consultations, and diagnostic\nanalysis of medical records. Our results demonstrate that ClinicalGPT\nsignificantly outperforms other models in these tasks, highlighting the\neffectiveness of our approach in adapting large language models to the critical\ndomain of healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2306.09968v1.pdf"
    },
    {
        "title": "Friend or Foe? Exploring the Implications of Large Language Models on the Science System",
        "authors": [
            "Benedikt Fecher",
            "Marcel Hebing",
            "Melissa Laufer",
            "J\u00f6rg Pohle",
            "Fabian Sofsky"
        ],
        "published": "2023-06-16T15:50:17Z",
        "summary": "The advent of ChatGPT by OpenAI has prompted extensive discourse on its\npotential implications for science and higher education. While the impact on\neducation has been a primary focus, there is limited empirical research on the\neffects of large language models (LLMs) and LLM-based chatbots on science and\nscientific practice. To investigate this further, we conducted a Delphi study\ninvolving 72 experts specialising in research and AI. The study focused on\napplications and limitations of LLMs, their effects on the science system,\nethical and legal considerations, and the required competencies for their\neffective use. Our findings highlight the transformative potential of LLMs in\nscience, particularly in administrative, creative, and analytical tasks.\nHowever, risks related to bias, misinformation, and quality assurance need to\nbe addressed through proactive regulation and science education. This research\ncontributes to informed discussions on the impact of generative AI in science\nand helps identify areas for future action.",
        "pdf_link": "https://arxiv.org/pdf/2306.09928v1.pdf"
    },
    {
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "authors": [
            "Theo X. Olausson",
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Jianfeng Gao",
            "Armando Solar-Lezama"
        ],
        "published": "2023-06-16T15:13:17Z",
        "summary": "Large language models have shown remarkable aptitude in code generation, but\nstill struggle to perform complex tasks. Self-repair -- in which the model\ndebugs and repairs its own code -- has recently become a popular way to boost\nperformance in these settings. However, despite its increasing popularity,\nexisting studies of self-repair have been limited in scope; in many settings,\nits efficacy thus remains poorly understood. In this paper, we analyze Code\nLlama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken\nfrom HumanEval and APPS. We find that when the cost of carrying out repair is\ntaken into account, performance gains are often modest, vary a lot between\nsubsets of the data, and are sometimes not present at all. We hypothesize that\nthis is because self-repair is bottlenecked by the model's ability to provide\nfeedback on its own code; using a stronger model to artificially boost the\nquality of the feedback, we observe substantially larger performance gains.\nSimilarly, a small-scale study in which we provide GPT-4 with feedback from\nhuman participants suggests that even for the strongest models, self-repair\nstill lags far behind what can be achieved with human-level debugging.",
        "pdf_link": "https://arxiv.org/pdf/2306.09896v5.pdf"
    },
    {
        "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
        "authors": [
            "Fangzhi Xu",
            "Qika Lin",
            "Jiawei Han",
            "Tianzhe Zhao",
            "Jun Liu",
            "Erik Cambria"
        ],
        "published": "2023-06-16T13:39:35Z",
        "summary": "Logical reasoning consistently plays a fundamental and significant role in\nthe domains of knowledge engineering and artificial intelligence. Recently,\nLarge Language Models (LLMs) have emerged as a noteworthy innovation in natural\nlanguage processing (NLP), exhibiting impressive achievements across various\nclassic NLP tasks. However, the question of whether LLMs can effectively\naddress the task of logical reasoning, which requires gradual cognitive\ninference similar to human intelligence, remains unanswered. To this end, we\naim to bridge this gap and provide comprehensive evaluations in this paper.\nFirstly, to offer systematic evaluations, we select fifteen typical logical\nreasoning datasets and organize them into deductive, inductive, abductive and\nmixed-form reasoning settings. Considering the comprehensiveness of\nevaluations, we include three representative LLMs (i.e., text-davinci-003,\nChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,\none-shot and three-shot settings. Secondly, different from previous evaluations\nrelying only on simple metrics (e.g., accuracy), we propose fine-level\nevaluations from objective and subjective manners, covering both answers and\nexplanations. Additionally, to uncover the logical flaws of LLMs, problematic\ncases will be attributed to five error types from two dimensions, i.e.,\nevidence selection process and reasoning process. Thirdly, to avoid the\ninfluences of knowledge bias and purely focus on benchmarking the logical\nreasoning capability of LLMs, we propose a new dataset with neutral content. It\ncontains 3,000 samples and covers deductive, inductive and abductive settings.\nBased on the in-depth evaluations, this paper finally forms a general\nevaluation scheme of logical reasoning capability from six dimensions. It\nreflects the pros and cons of LLMs and gives guiding directions for future\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2306.09841v3.pdf"
    },
    {
        "title": "Process Knowledge-infused Learning for Clinician-friendly Explanations",
        "authors": [
            "Kaushik Roy",
            "Yuxin Zi",
            "Manas Gaur",
            "Jinendra Malekar",
            "Qi Zhang",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-06-16T13:08:17Z",
        "summary": "Language models have the potential to assess mental health using social media\ndata. By analyzing online posts and conversations, these models can detect\npatterns indicating mental health conditions like depression, anxiety, or\nsuicidal thoughts. They examine keywords, language markers, and sentiment to\ngain insights into an individual's mental well-being. This information is\ncrucial for early detection, intervention, and support, improving mental health\ncare and prevention strategies. However, using language models for mental\nhealth assessments from social media has two limitations: (1) They do not\ncompare posts against clinicians' diagnostic processes, and (2) It's\nchallenging to explain language model outputs using concepts that the clinician\ncan understand, i.e., clinician-friendly explanations. In this study, we\nintroduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm\nthat layers clinical process knowledge structures on language model outputs,\nenabling clinician-friendly explanations of the underlying language model\npredictions. We rigorously test our methods on existing benchmark datasets,\naugmented with such clinical process knowledge, and release a new dataset for\nassessing suicidality. PK-iL performs competitively, achieving a 70% agreement\nwith users, while other XAI methods only achieve 47% agreement (average\ninter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL\neffectively explains model predictions to clinicians.",
        "pdf_link": "https://arxiv.org/pdf/2306.09824v1.pdf"
    },
    {
        "title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System",
        "authors": [
            "Zhiyuan Hu",
            "Yue Feng",
            "Anh Tuan Luu",
            "Bryan Hooi",
            "Aldo Lipani"
        ],
        "published": "2023-06-16T13:04:56Z",
        "summary": "Dialogue systems and large language models (LLMs) have gained considerable\nattention. However, the direct utilization of LLMs as task-oriented dialogue\n(TOD) models has been found to underperform compared to smaller task-specific\nmodels. Nonetheless, it is crucial to acknowledge the significant potential of\nLLMs and explore improved approaches for leveraging their impressive abilities.\nMotivated by the goal of leveraging LLMs, we propose an alternative approach\ncalled User-Guided Response Optimization (UGRO) to combine it with a smaller\nTOD model. This approach uses LLM as annotation-free user simulator to assess\ndialogue responses, combining them with smaller fine-tuned end-to-end TOD\nmodels. By utilizing the satisfaction feedback generated by LLMs, UGRO further\noptimizes the supervised fine-tuned TOD model. Specifically, the TOD model\ntakes the dialogue history as input and, with the assistance of the user\nsimulator's feedback, generates high-satisfaction responses that meet the\nuser's requirements. Through empirical experiments on two TOD benchmarks, we\nvalidate the effectiveness of our method. The results demonstrate that our\napproach outperforms previous state-of-the-art (SOTA) results.",
        "pdf_link": "https://arxiv.org/pdf/2306.09821v2.pdf"
    },
    {
        "title": "Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody",
        "authors": [
            "Sofoklis Kakouros",
            "Juraj \u0160imko",
            "Martti Vainio",
            "Antti Suni"
        ],
        "published": "2023-06-16T12:49:44Z",
        "summary": "This paper investigates the use of word surprisal, a measure of the\npredictability of a word in a given context, as a feature to aid speech\nsynthesis prosody. We explore how word surprisal extracted from large language\nmodels (LLMs) correlates with word prominence, a signal-based measure of the\nsalience of a word in a given discourse. We also examine how context length and\nLLM size affect the results, and how a speech synthesizer conditioned with\nsurprisal values compares with a baseline system. To evaluate these factors, we\nconducted experiments using a large corpus of English text and LLMs of varying\nsizes. Our results show that word surprisal and word prominence are moderately\ncorrelated, suggesting that they capture related but distinct aspects of\nlanguage use. We find that length of context and size of the LLM impact the\ncorrelations, but not in the direction anticipated, with longer contexts and\nlarger LLMs generally underpredicting prominent words in a nearly linear\nmanner. We demonstrate that, in line with these findings, a speech synthesizer\nconditioned with surprisal values provides a minimal improvement over the\nbaseline with the results suggesting a limited effect of using surprisal values\nfor eliciting appropriate prominence patterns.",
        "pdf_link": "https://arxiv.org/pdf/2306.09814v1.pdf"
    },
    {
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources",
        "authors": [
            "Kai Lv",
            "Yuqing Yang",
            "Tengxiao Liu",
            "Qinghui Gao",
            "Qipeng Guo",
            "Xipeng Qiu"
        ],
        "published": "2023-06-16T11:37:15Z",
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but demand massive GPU resources for training. Lowering the threshold for\nLLMs training would encourage greater participation from researchers,\nbenefiting both academia and society. While existing approaches have focused on\nparameter-efficient fine-tuning, which tunes or adds a small number of\nparameters, few have addressed the challenge of tuning the full parameters of\nLLMs with limited resources. In this work, we propose a new optimizer,\nLOw-Memory Optimization (LOMO), which fuses the gradient computation and the\nparameter update in one step to reduce memory usage. By integrating LOMO with\nexisting memory saving techniques, we reduce memory usage to 10.8% compared to\nthe standard approach (DeepSpeed solution). Consequently, our approach enables\nthe full parameter fine-tuning of a 65B model on a single machine with 8 RTX\n3090, each with 24GB memory.",
        "pdf_link": "https://arxiv.org/pdf/2306.09782v1.pdf"
    },
    {
        "title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models",
        "authors": [
            "Victor Steinborn",
            "Antonis Maronikolakis",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-06-16T10:36:18Z",
        "summary": "In efforts to keep up with the rapid progress and use of large language\nmodels, gender bias research is becoming more prevalent in NLP. Non-English\nbias research, however, is still in its infancy with most work focusing on\nEnglish. In our work, we study how grammatical gender bias relating to\npoliteness levels manifests in Japanese and Korean language models. Linguistic\nstudies in these languages have identified a connection between gender bias and\npoliteness levels, however it is not yet known if language models reproduce\nthese biases. We analyze relative prediction probabilities of the male and\nfemale grammatical genders using templates and find that informal polite speech\nis most indicative of the female grammatical gender, while rude and formal\nspeech is most indicative of the male grammatical gender. Further, we find\npoliteness levels to be an attack vector for allocational gender bias in\ncyberbullying detection models. Cyberbullies can evade detection through simple\ntechniques abusing politeness levels. We introduce an attack dataset to (i)\nidentify representational gender bias across politeness levels, (ii)\ndemonstrate how gender biases can be abused to bypass cyberbullying detection\nmodels and (iii) show that allocational biases can be mitigated via training on\nour proposed dataset. Through our findings we highlight the importance of bias\nresearch moving beyond its current English-centrism.",
        "pdf_link": "https://arxiv.org/pdf/2306.09752v1.pdf"
    },
    {
        "title": "Pushing the Limits of ChatGPT on NLP Tasks",
        "authors": [
            "Xiaofei Sun",
            "Linfeng Dong",
            "Xiaoya Li",
            "Zhen Wan",
            "Shuhe Wang",
            "Tianwei Zhang",
            "Jiwei Li",
            "Fei Cheng",
            "Lingjuan Lyu",
            "Fei Wu",
            "Guoyin Wang"
        ],
        "published": "2023-06-16T09:40:05Z",
        "summary": "Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n  In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n  We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.",
        "pdf_link": "https://arxiv.org/pdf/2306.09719v2.pdf"
    },
    {
        "title": "Clickbait Detection via Large Language Models",
        "authors": [
            "Han Wang",
            "Yi Zhu",
            "Ye Wang",
            "Yun Li",
            "Yunhao Yuan",
            "Jipeng Qiang"
        ],
        "published": "2023-06-16T02:49:20Z",
        "summary": "Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a series of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot and zero-shot\nscenarios on several English and Chinese benchmark datasets. Experimental\nresults show that LLMs cannot achieve the best results compared to the\nstate-of-the-art deep and fine-tuning PLMs methods. Different from human\nintuition, the experiments demonstrated that LLMs cannot make satisfied\nclickbait detection just by the headlines.",
        "pdf_link": "https://arxiv.org/pdf/2306.09597v3.pdf"
    },
    {
        "title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence",
        "authors": [
            "Sivaramakrishnan Swaminathan",
            "Antoine Dedieu",
            "Rajkumar Vasudeva Raju",
            "Murray Shanahan",
            "Miguel Lazaro-Gredilla",
            "Dileep George"
        ],
        "published": "2023-06-16T00:29:19Z",
        "summary": "In-context learning (ICL) is one of the most powerful and most unexpected\ncapabilities to emerge in recent transformer-based large language models\n(LLMs). Yet the mechanisms that underlie it are poorly understood. In this\npaper, we demonstrate that comparable ICL capabilities can be acquired by an\nalternative sequence prediction learning method using clone-structured causal\ngraphs (CSCGs). Moreover, a key property of CSCGs is that, unlike\ntransformer-based LLMs, they are {\\em interpretable}, which considerably\nsimplifies the task of explaining how ICL works. Specifically, we show that it\nuses a combination of (a) learning template (schema) circuits for pattern\ncompletion, (b) retrieving relevant templates in a context-sensitive manner,\nand (c) rebinding of novel tokens to appropriate slots in the templates. We go\non to marshall evidence for the hypothesis that similar mechanisms underlie ICL\nin LLMs. For example, we find that, with CSCGs as with LLMs, different\ncapabilities emerge at different levels of overparameterization, suggesting\nthat overparameterization helps in learning more complex template (schema)\ncircuits. By showing how ICL can be achieved with small models and datasets, we\nopen up a path to novel architectures, and take a vital step towards a more\ngeneral understanding of the mechanics behind this important capability.",
        "pdf_link": "https://arxiv.org/pdf/2307.01201v1.pdf"
    },
    {
        "title": "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Marshall An",
            "Chris Bogart",
            "Majd Sakr"
        ],
        "published": "2023-06-15T22:12:34Z",
        "summary": "This paper studies recent developments in large language models' (LLM)\nabilities to pass assessments in introductory and intermediate Python\nprogramming courses at the postsecondary level. The emergence of ChatGPT\nresulted in heated debates of its potential uses (e.g., exercise generation,\ncode explanation) as well as misuses in programming classes (e.g., cheating).\nRecent studies show that while the technology performs surprisingly well on\ndiverse sets of assessment instruments employed in typical programming classes\nthe performance is usually not sufficient to pass the courses. The release of\nGPT-4 largely emphasized notable improvements in the capabilities related to\nhandling assessments originally designed for human test-takers. This study is\nthe necessary analysis in the context of this ongoing transition towards mature\ngenerative AI systems. Specifically, we report the performance of GPT-4,\ncomparing it to the previous generations of GPT models, on three Python courses\nwith assessments ranging from simple multiple-choice questions (no code\ninvolved) to complex programming projects with code bases distributed into\nmultiple files (599 exercises overall). Additionally, we analyze the\nassessments that were not handled well by GPT-4 to understand the current\nlimitations of the model, as well as its capabilities to leverage feedback\nprovided by an auto-grader. We found that the GPT models evolved from\ncompletely failing the typical programming class' assessments (the original\nGPT-3) to confidently passing the courses with no human involvement (GPT-4).\nWhile we identified certain limitations in GPT-4's handling of MCQs and coding\nexercises, the rate of improvement across the recent generations of GPT models\nstrongly suggests their potential to handle almost any type of assessment\nwidely used in higher education programming courses. These findings could be\nleveraged by educators and institutions to adapt the design of programming\nassessments as well as to fuel the necessary discussions into how programming\nclasses should be updated to reflect the recent technological developments.\nThis study provides evidence that programming instructors need to prepare for a\nworld in which there is an easy-to-use widely accessible technology that can be\nutilized by learners to collect passing scores, with no effort whatsoever, on\nwhat today counts as viable programming knowledge and skills assessments.",
        "pdf_link": "https://arxiv.org/pdf/2306.10073v1.pdf"
    },
    {
        "title": "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)",
        "authors": [
            "Jaromir Savelka",
            "Kevin D. Ashley",
            "Morgan A. Gray",
            "Hannes Westermann",
            "Huihui Xu"
        ],
        "published": "2023-06-15T21:58:18Z",
        "summary": "Interpreting the meaning of legal open-textured terms is a key task of legal\nprofessionals. An important source for this interpretation is how the term was\napplied in previous court cases. In this paper, we evaluate the performance of\nGPT-4 in generating factually accurate, clear and relevant explanations of\nterms in legislation. We compare the performance of a baseline setup, where\nGPT-4 is directly asked to explain a legal term, to an augmented approach,\nwhere a legal information retrieval module is used to provide relevant context\nto the model, in the form of sentences from case law. We found that the direct\napplication of GPT-4 yields explanations that appear to be of very high quality\non their surface. However, detailed analysis uncovered limitations in terms of\nthe factual accuracy of the explanations. Further, we found that the\naugmentation leads to improved quality, and appears to eliminate the issue of\nhallucination, where models invent incorrect statements. These findings open\nthe door to the building of systems that can autonomously retrieve relevant\nsentences from case law and condense them into a useful explanation for legal\nscholars, educators or practicing lawyers alike.",
        "pdf_link": "https://arxiv.org/pdf/2306.09525v2.pdf"
    },
    {
        "title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models",
        "authors": [
            "Junting Pan",
            "Ziyi Lin",
            "Yuying Ge",
            "Xiatian Zhu",
            "Renrui Zhang",
            "Yi Wang",
            "Yu Qiao",
            "Hongsheng Li"
        ],
        "published": "2023-06-15T20:56:20Z",
        "summary": "Video Question Answering (VideoQA) has been significantly advanced from the\nscaling of recent Large Language Models (LLMs). The key idea is to convert the\nvisual information into the language feature space so that the capacity of LLMs\ncan be fully exploited. Existing VideoQA methods typically take two paradigms:\n(1) learning cross-modal alignment, and (2) using an off-the-shelf captioning\nmodel to describe the visual data. However, the first design needs costly\ntraining on many extra multi-modal data, whilst the second is further limited\nby limited domain generalization. To address these limitations, a simple yet\neffective Retrieving-to-Answer (R2A) framework is proposed.Given an input\nvideo, R2A first retrieves a set of semantically similar texts from a generic\ntext corpus using a pre-trained multi-modal model (e.g., CLIP). With both the\nquestion and the retrieved texts, a LLM (e.g., DeBERTa) can be directly used to\nyield a desired answer. Without the need for cross-modal fine-tuning, R2A\nallows for all the key components (e.g., LLM, retrieval model, and text corpus)\nto plug-and-play. Extensive experiments on several VideoQA benchmarks show that\ndespite with 1.3B parameters and no fine-tuning, our R2A can outperform the 61\ntimes larger Flamingo-80B model even additionally trained on nearly 2.1B\nmulti-modal data.",
        "pdf_link": "https://arxiv.org/pdf/2306.11732v1.pdf"
    },
    {
        "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
        "authors": [
            "Shubo Tian",
            "Qiao Jin",
            "Lana Yeganova",
            "Po-Ting Lai",
            "Qingqing Zhu",
            "Xiuying Chen",
            "Yifan Yang",
            "Qingyu Chen",
            "Won Kim",
            "Donald C. Comeau",
            "Rezarta Islamaj",
            "Aadit Kapoor",
            "Xin Gao",
            "Zhiyong Lu"
        ],
        "published": "2023-06-15T20:19:08Z",
        "summary": "ChatGPT has drawn considerable attention from both the general public and\ndomain experts with its remarkable text generation capabilities. This has\nsubsequently led to the emergence of diverse applications in the field of\nbiomedicine and health. In this work, we examine the diverse applications of\nlarge language models (LLMs), such as ChatGPT, in biomedicine and health.\nSpecifically we explore the areas of biomedical information retrieval, question\nanswering, medical text summarization, information extraction, and medical\neducation, and investigate whether LLMs possess the transformative power to\nrevolutionize these tasks or whether the distinct complexities of biomedical\ndomain presents unique challenges. Following an extensive literature survey, we\nfind that significant advances have been made in the field of text generation\ntasks, surpassing the previous state-of-the-art methods. For other\napplications, the advances have been modest. Overall, LLMs have not yet\nrevolutionized biomedicine, but recent rapid progress indicates that such\nmethods hold great potential to provide valuable means for accelerating\ndiscovery and improving health. We also find that the use of LLMs, like\nChatGPT, in the fields of biomedicine and health entails various risks and\nchallenges, including fabricated information in its generated responses, as\nwell as legal and privacy concerns associated with sensitive patient data. We\nbelieve this survey can provide a comprehensive and timely overview to\nbiomedical researchers and healthcare practitioners on the opportunities and\nchallenges associated with using ChatGPT and other LLMs for transforming\nbiomedicine and health.",
        "pdf_link": "https://arxiv.org/pdf/2306.10070v2.pdf"
    },
    {
        "title": "Inverse Scaling: When Bigger Isn't Better",
        "authors": [
            "Ian R. McKenzie",
            "Alexander Lyzhov",
            "Michael Pieler",
            "Alicia Parrish",
            "Aaron Mueller",
            "Ameya Prabhu",
            "Euan McLean",
            "Aaron Kirtland",
            "Alexis Ross",
            "Alisa Liu",
            "Andrew Gritsevskiy",
            "Daniel Wurgaft",
            "Derik Kauffman",
            "Gabriel Recchia",
            "Jiacheng Liu",
            "Joe Cavanagh",
            "Max Weiss",
            "Sicong Huang",
            "The Floating Droid",
            "Tom Tseng",
            "Tomasz Korbak",
            "Xudong Shen",
            "Yuhui Zhang",
            "Zhengping Zhou",
            "Najoung Kim",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-06-15T20:11:23Z",
        "summary": "Work on scaling laws has found that large language models (LMs) show\npredictable improvements to overall loss with increased scale (model size,\ntraining data, and compute). Here, we present evidence for the claim that LMs\nmay show inverse scaling, or worse task performance with increased scale, e.g.,\ndue to flaws in the training objective and data. We present empirical evidence\nof inverse scaling on 11 datasets collected by running a public contest, the\nInverse Scaling Prize, with a substantial prize pool. Through analysis of the\ndatasets, along with other examples found in the literature, we identify four\npotential causes of inverse scaling: (i) preference to repeat memorized\nsequences over following in-context instructions, (ii) imitation of undesirable\npatterns in the training data, (iii) tasks containing an easy distractor task\nwhich LMs could focus on, rather than the harder real task, and (iv) correct\nbut misleading few-shot demonstrations of the task. We release the winning\ndatasets at https://inversescaling.com/data to allow for further investigation\nof inverse scaling. Our tasks have helped drive the discovery of U-shaped and\ninverted-U scaling trends, where an initial trend reverses, suggesting that\nscaling trends are less reliable at predicting the behavior of larger-scale\nmodels than previously understood. Overall, our results suggest that there are\ntasks for which increased model scale alone may not lead to progress, and that\nmore careful thought needs to go into the data and objectives for training\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2306.09479v1.pdf"
    },
    {
        "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch",
        "authors": [
            "Stephen Casper",
            "Jason Lin",
            "Joe Kwon",
            "Gatlen Culp",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-06-15T18:49:50Z",
        "summary": "Deploying large language models (LMs) can pose hazards from harmful outputs\nsuch as toxic or false text. Prior work has introduced automated tools that\nelicit harmful outputs to identify these risks. While this is a valuable step\ntoward securing models, these approaches rely on a pre-existing way to\nefficiently classify undesirable outputs. Using a pre-existing classifier does\nnot allow for red-teaming to be tailored to the target model. Furthermore, when\nfailures can be easily classified in advance, red-teaming has limited marginal\nvalue because problems can be avoided by simply filtering training data and/or\nmodel outputs. Here, we consider red-teaming \"from scratch,\" in which the\nadversary does not begin with a way to classify failures. Our framework\nconsists of three steps: 1) Exploring the model's range of behaviors in the\ndesired context; 2) Establishing a definition and measurement for undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure to develop diverse adversarial\nprompts. We use this approach to red-team GPT-3 to discover classes of inputs\nthat elicit false statements. In doing so, we construct the CommonClaim dataset\nof 20,000 statements labeled by humans as common-knowledge-true, common\nknowledge-false, or neither. We are making code and data available.",
        "pdf_link": "https://arxiv.org/pdf/2306.09442v3.pdf"
    },
    {
        "title": "SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts",
        "authors": [
            "Rose E. Wang",
            "Pawan Wirawarn",
            "Noah Goodman",
            "Dorottya Demszky"
        ],
        "published": "2023-06-15T17:59:47Z",
        "summary": "Lectures are a learning experience for both students and teachers. Students\nlearn from teachers about the subject material, while teachers learn from\nstudents about how to refine their instruction. However, online student\nfeedback is unstructured and abundant, making it challenging for teachers to\nlearn and improve. We take a step towards tackling this challenge. First, we\ncontribute a dataset for studying this problem: SIGHT is a large dataset of 288\nmath lecture transcripts and 15,784 comments collected from the Massachusetts\nInstitute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we\ndevelop a rubric for categorizing feedback types using qualitative analysis.\nQualitative analysis methods are powerful in uncovering domain-specific\ninsights, however they are costly to apply to large data sources. To overcome\nthis challenge, we propose a set of best practices for using large language\nmodels (LLMs) to cheaply classify the comments at scale. We observe a striking\ncorrelation between the model's and humans' annotation: Categories with\nconsistent human annotations (>$0.9$ inter-rater reliability, IRR) also display\nhigher human-model agreement (>$0.7$), while categories with less consistent\nhuman annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower\nhuman-model agreement ($0.3$-$0.5$). These techniques uncover useful student\nfeedback from thousands of comments, costing around $\\$0.002$ per comment. We\nconclude by discussing exciting future directions on using online student\nfeedback and improving automated annotation techniques for qualitative\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2306.09343v1.pdf"
    },
    {
        "title": "Language-Guided Music Recommendation for Video via Prompt Analogies",
        "authors": [
            "Daniel McKee",
            "Justin Salamon",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "published": "2023-06-15T17:58:01Z",
        "summary": "We propose a method to recommend music for an input video while allowing a\nuser to guide music selection with free-form natural language. A key challenge\nof this problem setting is that existing music video datasets provide the\nneeded (video, music) training pairs, but lack text descriptions of the music.\nThis work addresses this challenge with the following three contributions.\nFirst, we propose a text-synthesis approach that relies on an analogy-based\nprompting procedure to generate natural language music descriptions from a\nlarge-scale language model (BLOOM-176B) given pre-trained music tagger outputs\nand a small number of human text descriptions. Second, we use these synthesized\nmusic descriptions to train a new trimodal model, which fuses text and video\ninput representations to query music samples. For training, we introduce a text\ndropout regularization mechanism which we show is critical to model\nperformance. Our model design allows for the retrieved music audio to agree\nwith the two input modalities by matching visual style depicted in the video\nand musical genre, mood, or instrumentation described in the natural language\nquery. Third, to evaluate our approach, we collect a testing dataset for our\nproblem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset\nwith natural language music descriptions which we make publicly available. We\nshow that our approach can match or exceed the performance of prior methods on\nvideo-to-music retrieval while significantly improving retrieval accuracy when\nusing text guidance.",
        "pdf_link": "https://arxiv.org/pdf/2306.09327v1.pdf"
    },
    {
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "authors": [
            "Myles Foley",
            "Ambrish Rawat",
            "Taesung Lee",
            "Yufang Hou",
            "Gabriele Picco",
            "Giulio Zizzo"
        ],
        "published": "2023-06-15T17:42:48Z",
        "summary": "The wide applicability and adaptability of generative large language models\n(LLMs) has enabled their rapid adoption. While the pre-trained models can\nperform many tasks, such models are often fine-tuned to improve their\nperformance on various downstream applications. However, this leads to issues\nover violation of model licenses, model theft, and copyright infringement.\nMoreover, recent advances show that generative technology is capable of\nproducing harmful content which exacerbates the problems of accountability\nwithin model supply chains. Thus, we need a method to investigate how a model\nwas trained or a piece of text was generated and what their pre-trained base\nmodel was. In this paper we take the first step to address this open problem by\ntracing back the origin of a given fine-tuned LLM to its corresponding\npre-trained base model. We consider different knowledge levels and attribution\nstrategies, and find that we can correctly trace back 8 out of the 10 fine\ntuned models with our best method.",
        "pdf_link": "https://arxiv.org/pdf/2306.09308v1.pdf"
    },
    {
        "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization",
        "authors": [
            "Swarnadeep Saha",
            "Peter Hase",
            "Mohit Bansal"
        ],
        "published": "2023-06-15T17:27:20Z",
        "summary": "A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.",
        "pdf_link": "https://arxiv.org/pdf/2306.09299v2.pdf"
    },
    {
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
        "authors": [
            "Jifan Yu",
            "Xiaozhi Wang",
            "Shangqing Tu",
            "Shulin Cao",
            "Daniel Zhang-Li",
            "Xin Lv",
            "Hao Peng",
            "Zijun Yao",
            "Xiaohan Zhang",
            "Hanming Li",
            "Chunyang Li",
            "Zheyuan Zhang",
            "Yushi Bai",
            "Yantao Liu",
            "Amy Xin",
            "Nianyi Lin",
            "Kaifeng Yun",
            "Linlu Gong",
            "Jianhui Chen",
            "Zhili Wu",
            "Yunjia Qi",
            "Weikai Li",
            "Yong Guan",
            "Kaisheng Zeng",
            "Ji Qi",
            "Hailong Jin",
            "Jinxin Liu",
            "Yu Gu",
            "Yuan Yao",
            "Ning Ding",
            "Lei Hou",
            "Zhiyuan Liu",
            "Bin Xu",
            "Jie Tang",
            "Juanzi Li"
        ],
        "published": "2023-06-15T17:20:46Z",
        "summary": "The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate $21$ open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems.",
        "pdf_link": "https://arxiv.org/pdf/2306.09296v2.pdf"
    },
    {
        "title": "SCALE: Scaling up the Complexity for Advanced Language Model Evaluation",
        "authors": [
            "Vishvaksenan Rasiah",
            "Ronja Stern",
            "Veton Matoshi",
            "Matthias St\u00fcrmer",
            "Ilias Chalkidis",
            "Daniel E. Ho",
            "Joel Niklaus"
        ],
        "published": "2023-06-15T16:19:15Z",
        "summary": "Recent strides in Large Language Models (LLMs) have saturated many NLP\nbenchmarks (even professional domain-specific ones), emphasizing the need for\nnovel, more challenging novel ones to properly assess LLM capabilities. In this\npaper, we introduce a novel NLP benchmark that poses challenges to current LLMs\nacross four key dimensions: processing long documents (up to 50K tokens),\nutilizing domain specific knowledge (embodied in legal texts), multilingual\nunderstanding (covering five languages), and multitasking (comprising legal\ndocument to document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks). Our benchmark comprises diverse legal NLP datasets from\nthe Swiss legal system, allowing for a comprehensive study of the underlying\nNon-English, inherently multilingual, federal legal system. Despite recent\nadvances, efficiently processing long documents for intense review/analysis\ntasks remains an open challenge for language models. Also, comprehensive,\ndomain-specific benchmarks requiring high expertise to develop are rare, as are\nmultilingual benchmarks. This scarcity underscores our contribution's value,\nconsidering most public models are trained predominantly on English corpora,\nwhile other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. Our benchmark allows for testing and advancing the\nstate-of-the-art LLMs. As part of our study, we evaluate several pre-trained\nmultilingual language models on our benchmark to establish strong baselines as\na point of reference. Despite the large size of our datasets (tens to hundreds\nof thousands of examples), existing publicly available models struggle with\nmost tasks, even after in-domain pretraining. We publish all resources\n(benchmark suite, pre-trained models, code) under a fully permissive open CC\nBY-SA license.",
        "pdf_link": "https://arxiv.org/pdf/2306.09237v2.pdf"
    },
    {
        "title": "ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",
        "authors": [
            "Hamideh Ghanadian",
            "Isar Nejadgholi",
            "Hussein Al Osman"
        ],
        "published": "2023-06-15T16:01:30Z",
        "summary": "This paper presents a novel framework for quantitatively evaluating the\ninteractive ChatGPT model in the context of suicidality assessment from social\nmedia posts, utilizing the University of Maryland Reddit suicidality dataset.\nWe conduct a technical evaluation of ChatGPT's performance on this task using\nZero-Shot and Few-Shot experiments and compare its results with those of two\nfine-tuned transformer-based models. Additionally, we investigate the impact of\ndifferent temperature parameters on ChatGPT's response generation and discuss\nthe optimal temperature based on the inconclusiveness rate of ChatGPT. Our\nresults indicate that while ChatGPT attains considerable accuracy in this task,\ntransformer-based models fine-tuned on human-annotated datasets exhibit\nsuperior performance. Moreover, our analysis sheds light on how adjusting the\nChatGPT's hyperparameters can improve its ability to assist mental health\nprofessionals in this critical task.",
        "pdf_link": "https://arxiv.org/pdf/2306.09390v1.pdf"
    },
    {
        "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
        "authors": [
            "Haonan Li",
            "Yixuan Zhang",
            "Fajri Koto",
            "Yifei Yang",
            "Hai Zhao",
            "Yeyun Gong",
            "Nan Duan",
            "Timothy Baldwin"
        ],
        "published": "2023-06-15T15:49:51Z",
        "summary": "As the capabilities of large language models (LLMs) continue to advance,\nevaluating their performance becomes increasingly crucial and challenging. This\npaper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese\nbenchmark that covers various subjects, including natural science, social\nsciences, engineering, and humanities. We conduct a thorough evaluation of 18\nadvanced multilingual- and Chinese-oriented LLMs, assessing their performance\nacross different subjects and settings. The results reveal that most existing\nLLMs struggle to achieve an average accuracy of 50%, even when provided with\nin-context examples and chain-of-thought prompts, whereas the random baseline\nstands at 25%. This highlights significant room for improvement in LLMs.\nAdditionally, we conduct extensive experiments to identify factors impacting\nthe models' performance and propose directions for enhancing LLMs. CMMLU fills\nthe gap in evaluating the knowledge and reasoning capabilities of large\nlanguage models within the Chinese context.",
        "pdf_link": "https://arxiv.org/pdf/2306.09212v2.pdf"
    },
    {
        "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",
        "authors": [
            "Chenyang Lyu",
            "Minghao Wu",
            "Longyue Wang",
            "Xinting Huang",
            "Bingshuai Liu",
            "Zefeng Du",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-06-15T12:45:25Z",
        "summary": "Although instruction-tuned large language models (LLMs) have exhibited\nremarkable capabilities across various NLP tasks, their effectiveness on other\ndata modalities beyond text has not been fully studied. In this work, we\npropose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,\naudio, and textual information. Macaw-LLM consists of three main components: a\nmodality module for encoding multi-modal data, a cognitive module for\nharnessing pretrained LLMs, and an alignment module for harmonizing diverse\nrepresentations. Our novel alignment module seamlessly bridges multi-modal\nfeatures to textual features, simplifying the adaptation process from the\nmodality modules to the cognitive module. In addition, we construct a\nlarge-scale multi-modal instruction dataset in terms of multi-turn dialogue,\nincluding 69K image instances and 50K video instances. We have made our data,\ncode and model publicly available, which we hope can pave the way for future\nresearch in multi-modal LLMs and expand the capabilities of LLMs to handle\ndiverse data modalities and address complex real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.09093v1.pdf"
    },
    {
        "title": "DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning",
        "authors": [
            "Hengli Li",
            "Song-Chun Zhu",
            "Zilong Zheng"
        ],
        "published": "2023-06-15T10:41:23Z",
        "summary": "Pragmatic reasoning plays a pivotal role in deciphering implicit meanings\nthat frequently arise in real-life conversations and is essential for the\ndevelopment of communicative social agents. In this paper, we introduce a novel\nchallenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic\nreasoning and situated conversational understanding. Compared with previous\nworks that treat different figurative expressions (e.g. metaphor, sarcasm) as\nindividual tasks, DiPlomat provides a cohesive framework towards general\npragmatic understanding. Our dataset is created through the utilization of\nAmazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn\ndialogues. In conjunction with the dataset, we propose two tasks, Pragmatic\nIdentification and Reasoning (PIR) and Conversational Question Answering (CQA).\nExperimental results with state-of-the-art (SOTA) neural architectures reveal\nseveral significant findings: 1) large language models ( LLMs) exhibit poor\nperformance in tackling this subjective domain; 2) comprehensive comprehension\nof context emerges as a critical factor for establishing benign human-machine\ninteractions; 3) current models defect in the application of pragmatic\nreasoning. As a result, we call on more attention to improve the ability of\ncontext understanding, reasoning, and implied meaning modeling.",
        "pdf_link": "https://arxiv.org/pdf/2306.09030v2.pdf"
    },
    {
        "title": "Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation",
        "authors": [
            "Zihui Gu",
            "Ju Fan",
            "Nan Tang",
            "Songyue Zhang",
            "Yuxin Zhang",
            "Zui Chen",
            "Lei Cao",
            "Guoliang Li",
            "Sam Madden",
            "Xiaoyong Du"
        ],
        "published": "2023-06-15T06:50:51Z",
        "summary": "Zero-shot NL2SQL is crucial in achieving natural language to SQL that is\nadaptive to new environments (e.g., new databases, new linguistic phenomena or\nSQL structures) with zero annotated NL2SQL samples from such environments.\nExisting approaches either fine-tune pre-trained language models (PLMs) based\non annotated data or use prompts to guide fixed large language models (LLMs)\nsuch as ChatGPT. PLMs can perform well in schema alignment but struggle to\nachieve complex reasoning, while LLMs is superior in complex reasoning tasks\nbut cannot achieve precise schema alignment. In this paper, we propose a\nZeroNL2SQL framework that combines the complementary advantages of PLMs and\nLLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an\nSQL sketch via schema alignment, then uses LLMs to fill the missing information\nvia complex reasoning. Moreover, in order to better align the generated SQL\nqueries with values in the given database instances, we design a predicate\ncalibration method to guide the LLM in completing the SQL sketches based on the\ndatabase instances and select the optimal SQL query via an execution-based\nstrategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best\nzero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL\noutperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds\nLLM-based methods by 10% to 20% on execution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2306.08891v1.pdf"
    },
    {
        "title": "Toward Grounded Commonsense Reasoning",
        "authors": [
            "Minae Kwon",
            "Hengyuan Hu",
            "Vivek Myers",
            "Siddharth Karamcheti",
            "Anca Dragan",
            "Dorsa Sadigh"
        ],
        "published": "2023-06-14T17:30:57Z",
        "summary": "Consider a robot tasked with tidying a desk with a meticulously constructed\nLego sports car. A human may recognize that it is not appropriate to\ndisassemble the sports car and put it away as part of the \"tidying.\" How can a\nrobot reach that conclusion? Although large language models (LLMs) have\nrecently been used to enable commonsense reasoning, grounding this reasoning in\nthe real world has been challenging. To reason in the real world, robots must\ngo beyond passively querying LLMs and actively gather information from the\nenvironment that is required to make the right decision. For instance, after\ndetecting that there is an occluded car, the robot may need to actively\nperceive the car to know whether it is an advanced model car made out of Legos\nor a toy car built by a toddler. We propose an approach that leverages an LLM\nand vision language model (VLM) to help a robot actively perceive its\nenvironment to perform grounded commonsense reasoning. To evaluate our\nframework at scale, we release the MessySurfaces dataset which contains images\nof 70 real-world surfaces that need to be cleaned. We additionally illustrate\nour approach with a robot on 2 carefully designed surfaces. We find an average\n12.9% improvement on the MessySurfaces benchmark and an average 15% improvement\non the robot experiments over baselines that do not use active perception. The\ndataset, code, and videos of our approach can be found at\nhttps://minaek.github.io/grounded_commonsense_reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2306.08651v2.pdf"
    },
    {
        "title": "Language to Rewards for Robotic Skill Synthesis",
        "authors": [
            "Wenhao Yu",
            "Nimrod Gileadi",
            "Chuyuan Fu",
            "Sean Kirmani",
            "Kuang-Huei Lee",
            "Montse Gonzalez Arenas",
            "Hao-Tien Lewis Chiang",
            "Tom Erez",
            "Leonard Hasenclever",
            "Jan Humplik",
            "Brian Ichter",
            "Ted Xiao",
            "Peng Xu",
            "Andy Zeng",
            "Tingnan Zhang",
            "Nicolas Heess",
            "Dorsa Sadigh",
            "Jie Tan",
            "Yuval Tassa",
            "Fei Xia"
        ],
        "published": "2023-06-14T17:27:10Z",
        "summary": "Large language models (LLMs) have demonstrated exciting progress in acquiring\ndiverse new capabilities through in-context learning, ranging from logical\nreasoning to code-writing. Robotics researchers have also explored using LLMs\nto advance the capabilities of robotic control. However, since low-level robot\nactions are hardware-dependent and underrepresented in LLM training corpora,\nexisting efforts in applying LLMs to robotics have largely treated LLMs as\nsemantic planners or relied on human-engineered control primitives to interface\nwith the robot. On the other hand, reward functions are shown to be flexible\nrepresentations that can be optimized for control policies to achieve diverse\ntasks, while their semantic richness makes them suitable to be specified by\nLLMs. In this work, we introduce a new paradigm that harnesses this realization\nby utilizing LLMs to define reward parameters that can be optimized and\naccomplish variety of robotic tasks. Using reward as the intermediate interface\ngenerated by LLMs, we can effectively bridge the gap between high-level\nlanguage instructions or corrections to low-level robot actions. Meanwhile,\ncombining this with a real-time optimizer, MuJoCo MPC, empowers an interactive\nbehavior creation experience where users can immediately observe the results\nand provide feedback to the system. To systematically evaluate the performance\nof our proposed method, we designed a total of 17 tasks for a simulated\nquadruped robot and a dexterous manipulator robot. We demonstrate that our\nproposed method reliably tackles 90% of the designed tasks, while a baseline\nusing primitive skills as the interface with Code-as-policies achieves 50% of\nthe tasks. We further validated our method on a real robot arm where complex\nmanipulation skills such as non-prehensile pushing emerge through our\ninteractive system.",
        "pdf_link": "https://arxiv.org/pdf/2306.08647v2.pdf"
    },
    {
        "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
        "authors": [
            "Lingxi Xie",
            "Longhui Wei",
            "Xiaopeng Zhang",
            "Kaifeng Bi",
            "Xiaotao Gu",
            "Jianlong Chang",
            "Qi Tian"
        ],
        "published": "2023-06-14T17:15:01Z",
        "summary": "The AI community has been pursuing algorithms known as artificial general\nintelligence (AGI) that apply to any kind of real-world problem. Recently, chat\nsystems powered by large language models (LLMs) emerge and rapidly become a\npromising direction to achieve AGI in natural language processing (NLP), but\nthe path towards AGI in computer vision (CV) remains unclear. One may owe the\ndilemma to the fact that visual signals are more complex than language signals,\nyet we are interested in finding concrete reasons, as well as absorbing\nexperiences from GPT and LLMs to solve the problem. In this paper, we start\nwith a conceptual definition of AGI and briefly review how NLP solves a wide\nrange of tasks via a chat system. The analysis inspires us that unification is\nthe next important goal of CV. But, despite various efforts in this direction,\nCV is still far from a system like GPT that naturally integrates all tasks. We\npoint out that the essential weakness of CV lies in lacking a paradigm to learn\nfrom environments, yet NLP has accomplished the task in the text world. We then\nimagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale,\ninteractable environments, pre-trains it to predict future frames with respect\nto its action, and then fine-tunes it with instruction to accomplish various\ntasks. We expect substantial research and engineering efforts to push the idea\nforward and scale it up, for which we share our perspectives on future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2306.08641v1.pdf"
    },
    {
        "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",
        "authors": [
            "Difei Gao",
            "Lei Ji",
            "Luowei Zhou",
            "Kevin Qinghong Lin",
            "Joya Chen",
            "Zihan Fan",
            "Mike Zheng Shou"
        ],
        "published": "2023-06-14T17:12:56Z",
        "summary": "Recent research on Large Language Models (LLMs) has led to remarkable\nadvancements in general NLP AI assistants. Some studies have further explored\nthe use of LLMs for planning and invoking models or APIs to address more\ngeneral multi-modal user queries. Despite this progress, complex visual-based\ntasks still remain challenging due to the diverse nature of visual tasks. This\ndiversity is reflected in two aspects: 1) Reasoning paths. For many real-life\napplications, it is hard to accurately decompose a query simply by examining\nthe query itself. Planning based on the specific visual content and the results\nof each step is usually required. 2) Flexible inputs and intermediate results.\nInput forms could be flexible for in-the-wild cases, and involves not only a\nsingle image or video but a mixture of videos and images, e.g., a user-view\nimage with some reference videos. Besides, a complex reasoning process will\nalso generate diverse multimodal intermediate results, e.g., video narrations,\nsegmented video clips, etc. To address such general cases, we propose a\nmulti-modal AI assistant, AssistGPT, with an interleaved code and language\nreasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate\nLLMs with various tools. Specifically, the Planner is capable of using natural\nlanguage to plan which tool in Executor should do next based on the current\nreasoning progress. Inspector is an efficient memory manager to assist the\nPlanner to feed proper visual information into a specific tool. Finally, since\nthe entire reasoning process is complex and flexible, a Learner is designed to\nenable the model to autonomously explore and discover the optimal solution. We\nconducted experiments on A-OKVQA and NExT-QA benchmarks, achieving\nstate-of-the-art results. Moreover, showcases demonstrate the ability of our\nsystem to handle questions far more complex than those found in the benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2306.08640v2.pdf"
    },
    {
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
            "Yuxian Gu",
            "Li Dong",
            "Furu Wei",
            "Minlie Huang"
        ],
        "published": "2023-06-14T14:44:03Z",
        "summary": "Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\nhttps://github.com/microsoft/LMOps/tree/main/minillm.",
        "pdf_link": "https://arxiv.org/pdf/2306.08543v4.pdf"
    },
    {
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "authors": [
            "Shirui Pan",
            "Linhao Luo",
            "Yufei Wang",
            "Chen Chen",
            "Jiapu Wang",
            "Xindong Wu"
        ],
        "published": "2023-06-14T07:15:26Z",
        "summary": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.",
        "pdf_link": "https://arxiv.org/pdf/2306.08302v3.pdf"
    },
    {
        "title": "Language models are not naysayers: An analysis of language models on negation benchmarks",
        "authors": [
            "Thinh Hung Truong",
            "Timothy Baldwin",
            "Karin Verspoor",
            "Trevor Cohn"
        ],
        "published": "2023-06-14T01:16:37Z",
        "summary": "Negation has been shown to be a major bottleneck for masked language models,\nsuch as BERT. However, whether this finding still holds for larger-sized\nauto-regressive language models (``LLMs'') has not been studied\ncomprehensively. With the ever-increasing volume of research and applications\nof LLMs, we take a step back to evaluate the ability of current-generation LLMs\nto handle negation, a fundamental linguistic phenomenon that is central to\nlanguage understanding. We evaluate different LLMs -- including the open-source\nGPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.\nThrough systematic experimentation with varying model sizes and prompts, we\nshow that LLMs have several limitations including insensitivity to the presence\nof negation, an inability to capture the lexical semantics of negation, and a\nfailure to reason under negation.",
        "pdf_link": "https://arxiv.org/pdf/2306.08189v1.pdf"
    },
    {
        "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation",
        "authors": [
            "Yuji Chai",
            "John Gkountouras",
            "Glenn G. Ko",
            "David Brooks",
            "Gu-Yeon Wei"
        ],
        "published": "2023-06-13T22:25:35Z",
        "summary": "We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations.",
        "pdf_link": "https://arxiv.org/pdf/2306.08162v1.pdf"
    },
    {
        "title": "Large-scale Language Model Rescoring on Long-form Data",
        "authors": [
            "Tongzhou Chen",
            "Cyril Allauzen",
            "Yinghui Huang",
            "Daniel Park",
            "David Rybach",
            "W. Ronny Huang",
            "Rodrigo Cabrera",
            "Kartik Audhkhasi",
            "Bhuvana Ramabhadran",
            "Pedro J. Moreno",
            "Michael Riley"
        ],
        "published": "2023-06-13T20:54:12Z",
        "summary": "In this work, we study the impact of Large-scale Language Models (LLM) on\nAutomated Speech Recognition (ASR) of YouTube videos, which we use as a source\nfor long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error\nEate (WER) on US English (en-us) and code-switched Indian English (en-in)\nlong-form ASR test sets and a reduction of up to 30\\% relative on Salient Term\nError Rate (STER) over a strong first-pass baseline that uses a maximum-entropy\nbased language model. Improved lattice processing that results in a lattice\nwith a proper (non-tree) digraph topology and carrying context from the 1-best\nhypothesis of the previous segment(s) results in significant wins in rescoring\nwith LLMs. We also find that the gains in performance from the combination of\nLLMs trained on vast quantities of available data (such as C4) and conventional\nneural LMs is additive and significantly outperforms a strong first-pass\nbaseline with a maximum entropy LM.\n  Copyright 2023 IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2306.08133v2.pdf"
    },
    {
        "title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Chen Sun",
            "Kai-Wei Chang",
            "Yizhou Sun",
            "David A Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
        ],
        "published": "2023-06-13T20:50:22Z",
        "summary": "In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.",
        "pdf_link": "https://arxiv.org/pdf/2306.08129v3.pdf"
    },
    {
        "title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level",
        "authors": [
            "Mujahid Ali Quidwai",
            "Chunhui Li",
            "Parijat Dube"
        ],
        "published": "2023-06-13T20:34:55Z",
        "summary": "The increasing reliance on large language models (LLMs) in academic writing\nhas led to a rise in plagiarism. Existing AI-generated text classifiers have\nlimited accuracy and often produce false positives. We propose a novel approach\nusing natural language processing (NLP) techniques, offering quantifiable\nmetrics at both sentence and document levels for easier interpretation by human\nevaluators. Our method employs a multi-faceted approach, generating multiple\nparaphrased versions of a given question and inputting them into the LLM to\ngenerate answers. By using a contrastive loss function based on cosine\nsimilarity, we match generated sentences with those from the student's\nresponse. Our approach achieves up to 94% accuracy in classifying human and AI\ntext, providing a robust and adaptable solution for plagiarism detection in\nacademic settings. This method improves with LLM advancements, reducing the\nneed for new model training or reconfiguration, and offers a more transparent\nway of evaluating and detecting AI-generated text.",
        "pdf_link": "https://arxiv.org/pdf/2306.08122v1.pdf"
    },
    {
        "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
        "authors": [
            "Alexander Tornede",
            "Difan Deng",
            "Theresa Eimer",
            "Joseph Giovanelli",
            "Aditya Mohan",
            "Tim Ruhkopf",
            "Sarah Segel",
            "Daphne Theodorakopoulos",
            "Tanja Tornede",
            "Henning Wachsmuth",
            "Marius Lindauer"
        ],
        "published": "2023-06-13T19:51:22Z",
        "summary": "The fields of both Natural Language Processing (NLP) and Automated Machine\nLearning (AutoML) have achieved remarkable results over the past years. In NLP,\nespecially Large Language Models (LLMs) have experienced a rapid series of\nbreakthroughs very recently. We envision that the two fields can radically push\nthe boundaries of each other through tight integration. To showcase this\nvision, we explore the potential of a symbiotic relationship between AutoML and\nLLMs, shedding light on how they can benefit each other. In particular, we\ninvestigate both the opportunities to enhance AutoML approaches with LLMs from\ndifferent perspectives and the challenges of leveraging AutoML to further\nimprove LLMs. To this end, we survey existing work, and we critically assess\nrisks. We strongly believe that the integration of the two fields has the\npotential to disrupt both fields, NLP and AutoML. By highlighting conceivable\nsynergies, but also risks, we aim to foster further exploration at the\nintersection of AutoML and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.08107v3.pdf"
    },
    {
        "title": "Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning",
        "authors": [
            "Michael Villarreal",
            "Bibek Poudel",
            "Weizi Li"
        ],
        "published": "2023-06-13T19:27:18Z",
        "summary": "The surge in Reinforcement Learning (RL) applications in Intelligent\nTransportation Systems (ITS) has contributed to its growth as well as\nhighlighted key challenges. However, defining objectives of RL agents in\ntraffic control and management tasks, as well as aligning policies with these\ngoals through an effective formulation of Markov Decision Process (MDP), can be\nchallenging and often require domain experts in both RL and ITS. Recent\nadvancements in Large Language Models (LLMs) such as GPT-4 highlight their\nbroad general knowledge, reasoning capabilities, and commonsense priors across\nvarious domains. In this work, we conduct a large-scale user study involving 70\nparticipants to investigate whether novices can leverage ChatGPT to solve\ncomplex mixed traffic control problems. Three environments are tested,\nincluding ring road, bottleneck, and intersection. We find ChatGPT has mixed\nresults. For intersection and bottleneck, ChatGPT increases number of\nsuccessful policies by 150% and 136% compared to solely beginner capabilities,\nwith some of them even outperforming experts. However, ChatGPT does not provide\nconsistent improvements across all scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.08094v2.pdf"
    },
    {
        "title": "FLamE: Few-shot Learning from Natural Language Explanations",
        "authors": [
            "Yangqiaoyu Zhou",
            "Yiming Zhang",
            "Chenhao Tan"
        ],
        "published": "2023-06-13T18:01:46Z",
        "summary": "Natural language explanations have the potential to provide rich information\nthat in principle guides model reasoning. Yet, recent work by Lampinen et al.\n(2022) has shown limited utility of natural language explanations in improving\nclassification. To effectively learn from explanations, we present FLamE, a\ntwo-stage few-shot learning framework that first generates explanations using\nGPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated\nexplanations. Our experiments on natural language inference demonstrate\neffectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3\nBabbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification\nperformance, human evaluation surprisingly reveals that the majority of\ngenerated explanations does not adequately justify classification decisions.\nAdditional analyses point to the important role of label-specific cues (e.g.,\n\"not know\" for the neutral label) in generated explanations.",
        "pdf_link": "https://arxiv.org/pdf/2306.08042v1.pdf"
    },
    {
        "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models",
        "authors": [
            "Omkar Thawkar",
            "Abdelrahman Shaker",
            "Sahal Shaji Mullappilly",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Jorma Laaksonen",
            "Fahad Shahbaz Khan"
        ],
        "published": "2023-06-13T17:59:59Z",
        "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
        "pdf_link": "https://arxiv.org/pdf/2306.07971v1.pdf"
    },
    {
        "title": "arXiVeri: Automatic table verification with GPT",
        "authors": [
            "Gyungin Shin",
            "Weidi Xie",
            "Samuel Albanie"
        ],
        "published": "2023-06-13T17:59:57Z",
        "summary": "Without accurate transcription of numerical data in scientific documents, a\nscientist cannot draw accurate conclusions. Unfortunately, the process of\ncopying numerical data from one paper to another is prone to human error. In\nthis paper, we propose to meet this challenge through the novel task of\nautomatic table verification (AutoTV), in which the objective is to verify the\naccuracy of numerical data in tables by cross-referencing cited sources. To\nsupport this task, we propose a new benchmark, arXiVeri, which comprises\ntabular data drawn from open-access academic papers on arXiv. We introduce\nmetrics to evaluate the performance of a table verifier in two key areas: (i)\ntable matching, which aims to identify the source table in a cited document\nthat corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row\nand column indices accurately. By leveraging the flexible capabilities of\nmodern large language models (LLMs), we propose simple baselines for table\nverification. Our findings highlight the complexity of this task, even for\nstate-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.07968v1.pdf"
    },
    {
        "title": "Questioning the Survey Responses of Large Language Models",
        "authors": [
            "Ricardo Dominguez-Olmedo",
            "Moritz Hardt",
            "Celestine Mendler-D\u00fcnner"
        ],
        "published": "2023-06-13T17:48:27Z",
        "summary": "As large language models increase in capability, researchers have started to\nconduct surveys of all kinds on these models in order to investigate the\npopulation represented by their responses. In this work, we critically examine\nlanguage models' survey responses on the basis of the well-established American\nCommunity Survey by the U.S. Census Bureau and investigate whether they elicit\na faithful representations of any human population. Using a de-facto standard\nmultiple-choice prompting technique and evaluating 39 different language models\nusing systematic experiments, we establish two dominant patterns: First,\nmodels' responses are governed by ordering and labeling biases, leading to\nvariations across models that do not persist after adjusting for systematic\nbiases. Second, models' responses do not contain the entropy variations and\nstatistical signals typically found in human populations. As a result, a binary\nclassifier can almost perfectly differentiate model-generated data from the\nresponses of the U.S. census. At the same time, models' relative alignment with\ndifferent demographic subgroups can be predicted from the subgroups' entropy,\nirrespective of the model's training data or training strategy. Taken together,\nour findings suggest caution in treating models' survey responses as equivalent\nto those of human populations.",
        "pdf_link": "https://arxiv.org/pdf/2306.07951v3.pdf"
    },
    {
        "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
        "authors": [
            "Xiao Liu",
            "Hanyu Lai",
            "Hao Yu",
            "Yifan Xu",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Peng Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-06-13T16:57:53Z",
        "summary": "We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\n\\url{https://github.com/THUDM/WebGLM}.",
        "pdf_link": "https://arxiv.org/pdf/2306.07906v1.pdf"
    },
    {
        "title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Robert West"
        ],
        "published": "2023-06-13T16:46:24Z",
        "summary": "Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk",
        "pdf_link": "https://arxiv.org/pdf/2306.07899v1.pdf"
    },
    {
        "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
        "authors": [
            "Longtao Zheng",
            "Rundong Wang",
            "Xinrun Wang",
            "Bo An"
        ],
        "published": "2023-06-13T15:49:41Z",
        "summary": "Building agents with large language models (LLMs) for computer control is a\nburgeoning research area, where the agent receives computer states and performs\nactions to complete complex tasks. Previous computer agents have demonstrated\nthe benefits of in-context learning (ICL); however, their performance is\nhindered by several issues. First, the limited context length of LLMs and\ncomplex computer states restrict the number of exemplars, as a single webpage\ncan consume the entire context. Second, the exemplars in current methods, such\nas high-level plans and multi-choice questions, cannot represent complete\ntrajectories, leading to suboptimal performance in long-horizon tasks. Third,\nexisting computer agents rely on task-specific exemplars and overlook the\nsimilarity among tasks, resulting in poor generalization to novel tasks. To\naddress these challenges, we introduce Synapse, a computer agent featuring\nthree key components: i) state abstraction, which filters out task-irrelevant\ninformation from raw states, allowing more exemplars within the limited\ncontext, ii) trajectory-as-exemplar prompting, which prompts the LLM with\ncomplete trajectories of the abstracted states and actions to improve\nmulti-step decision-making, and iii) exemplar memory, which stores the\nembeddings of exemplars and retrieves them via similarity search for\ngeneralization to novel tasks. We evaluate Synapse on MiniWoB++, a standard\ntask suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse\nachieves a 99.2% average success rate (a 10% relative improvement) across 64\ntasks using demonstrations from only 48 tasks. Notably, Synapse is the first\nICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a\n56% relative improvement in average step success rate over the previous\nstate-of-the-art prompting scheme in Mind2Web.",
        "pdf_link": "https://arxiv.org/pdf/2306.07863v3.pdf"
    },
    {
        "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
        "authors": [
            "Yin Fang",
            "Xiaozhuan Liang",
            "Ningyu Zhang",
            "Kangwei Liu",
            "Rui Huang",
            "Zhuo Chen",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2023-06-13T14:35:34Z",
        "summary": "Large Language Models (LLMs), with their remarkable task-handling\ncapabilities and innovative outputs, have catalyzed significant advancements\nacross a spectrum of fields. However, their proficiency within specialized\ndomains such as biomolecular studies remains limited. To address this\nchallenge, we introduce Mol-Instructions, a comprehensive instruction dataset\ndesigned for the biomolecular domain. Mol-Instructions encompasses three key\ncomponents: molecule-oriented instructions, protein-oriented instructions, and\nbiomolecular text instructions. Each component aims to improve the\nunderstanding and prediction capabilities of LLMs concerning biomolecular\nfeatures and behaviors. Through extensive instruction tuning experiments on\nLLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large\nmodels' performance in the intricate realm of biomolecular studies, thus\nfostering progress in the biomolecular research community. Mol-Instructions is\npublicly available for ongoing research and will undergo regular updates to\nenhance its applicability.",
        "pdf_link": "https://arxiv.org/pdf/2306.08018v5.pdf"
    },
    {
        "title": "NoCoLA: The Norwegian Corpus of Linguistic Acceptability",
        "authors": [
            "Matias Jentoft",
            "David Samuel"
        ],
        "published": "2023-06-13T14:11:19Z",
        "summary": "While there has been a surge of large language models for Norwegian in recent\nyears, we lack any tool to evaluate their understanding of grammaticality. We\npresent two new Norwegian datasets for this task. NoCoLA_class is a supervised\nbinary classification task where the goal is to discriminate between acceptable\nand non-acceptable sentences. On the other hand, NoCoLA_zero is a purely\ndiagnostic task for evaluating the grammatical judgement of a language model in\na completely zero-shot manner, i.e. without any further training. In this\npaper, we describe both datasets in detail, show how to use them for different\nflavors of language models, and conduct a comparative study of the existing\nNorwegian language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.07790v1.pdf"
    },
    {
        "title": "SqueezeLLM: Dense-and-Sparse Quantization",
        "authors": [
            "Sehoon Kim",
            "Coleman Hooper",
            "Amir Gholami",
            "Zhen Dong",
            "Xiuyu Li",
            "Sheng Shen",
            "Michael W. Mahoney",
            "Kurt Keutzer"
        ],
        "published": "2023-06-13T08:57:54Z",
        "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing model weights with reduced precision,\nprevious efforts have often resulted in notable performance degradation. To\naddress this, we introduce SqueezeLLM, a post-training quantization framework\nthat not only enables lossless compression to ultra-low precisions of up to\n3-bit, but also achieves higher quantization performance under the same memory\nconstraint. Our framework incorporates two novel ideas: (i) sensitivity-based\nnon-uniform quantization, which searches for the optimal bit precision\nassignment based on second-order information; and (ii) the Dense-and-Sparse\ndecomposition that stores outliers and sensitive weight values in an efficient\nsparse format. When applied to the LLaMA models, our 3-bit quantization\nsignificantly reduces the perplexity gap from the FP16 baseline by up to 2.1x\nas compared to the state-of-the-art methods with the same memory requirement.\nFurthermore, when deployed on an A6000 GPU, our quantized models achieve up to\n2.3x speedup compared to the baseline. Our code is open-sourced and available\nonline.",
        "pdf_link": "https://arxiv.org/pdf/2306.07629v3.pdf"
    },
    {
        "title": "Large Language Models Sometimes Generate Purely Negatively-Reinforced Text",
        "authors": [
            "Fabien Roger"
        ],
        "published": "2023-06-13T06:40:37Z",
        "summary": "When using adversarial training, it is common practice to train against the\nmost egregious failures. However, this might imply using examples with\nsensitive information (such as leaked passwords or security vulnerabilities) as\ntraining data. One might assume that language models trained with gradient\ndescent never generate text snippets which were only present in examples\nassociated with the lowest possible reward. In this paper, we show that this\nassumption is wrong: in some situations, large language models do learn from\nsuch negatively-reinforced examples. We present a specific training setup that\nenables Pythia-160M to guess passwords 13% more often than it would by guessing\nrandomly, despite only showing it these passwords on examples where the model\nis incentivized to not output these passwords. Our code is available at\nwww.github.com/FabienRoger/Learning-From-Negative-Examples",
        "pdf_link": "https://arxiv.org/pdf/2306.07567v2.pdf"
    },
    {
        "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
        "authors": [
            "Kush Bhatia",
            "Avanika Narayan",
            "Christopher De Sa",
            "Christopher R\u00e9"
        ],
        "published": "2023-06-13T04:37:00Z",
        "summary": "Large language models (LLMs) exhibit in-context learning abilities which\nenable the same model to perform several tasks without any task-specific\ntraining. In contrast, traditional adaptation approaches, such as fine-tuning,\nmodify the underlying models for each specific task. In-context learning,\nhowever, consistently underperforms task-specific tuning approaches even when\npresented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM's learned representations to patch this\nperformance gap, our analysis actually reveal that LLM representations contain\nsufficient information to make good predictions. As such, we focus on the LLM's\nreasoning abilities and demonstrate that this performance gap exists due to\ntheir inability to perform simple probabilistic reasoning tasks. This raises an\nintriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose TART which\ngenerically improves an LLM's reasoning abilities using a synthetically trained\nTransformer-based reasoning module. TART trains this reasoning module in a\ntask-agnostic manner using only synthetic logistic regression tasks and\ncomposes it with an arbitrary real-world pre-trained model without any\nadditional training. With a single inference module, TART improves performance\nacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -\n6B), tasks (14 NLP binary classification tasks), and even across different\nmodalities (audio and vision). Additionally, on the RAFT Benchmark, TART\nimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),\nand is within 4% of GPT-3 (175B). Our code and models are available at\nhttps://github.com/HazyResearch/TART .",
        "pdf_link": "https://arxiv.org/pdf/2306.07536v1.pdf"
    },
    {
        "title": "Assigning AI: Seven Approaches for Students, with Prompts",
        "authors": [
            "Ethan Mollick",
            "Lilach Mollick"
        ],
        "published": "2023-06-13T03:36:36Z",
        "summary": "This paper examines the transformative role of Large Language Models (LLMs)\nin education and their potential as learning tools, despite their inherent\nrisks and limitations. The authors propose seven approaches for utilizing AI in\nclassrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator,\nand AI-student, each with distinct pedagogical benefits and risks. The aim is\nto help students learn with and about AI, with practical strategies designed to\nmitigate risks such as complacency about the AI's output, errors, and biases.\nThese strategies promote active oversight, critical assessment of AI outputs,\nand complementarity of AI's capabilities with the students' unique insights. By\nchallenging students to remain the \"human in the loop,\" the authors aim to\nenhance learning outcomes while ensuring that AI serves as a supportive tool\nrather than a replacement. The proposed framework offers a guide for educators\nnavigating the integration of AI-assisted learning in classrooms",
        "pdf_link": "https://arxiv.org/pdf/2306.10052v1.pdf"
    },
    {
        "title": "Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment",
        "authors": [
            "Hao Yang",
            "Min Zhang",
            "Shimin Tao",
            "Minghan Wang",
            "Daimeng Wei",
            "Yanfei Jiang"
        ],
        "published": "2023-06-13T01:18:32Z",
        "summary": "Cross-lingual Machine Translation (MT) quality estimation plays a crucial\nrole in evaluating translation performance. GEMBA, the first MT quality\nassessment metric based on Large Language Models (LLMs), employs one-step\nprompting to achieve state-of-the-art (SOTA) in system-level MT quality\nestimation; however, it lacks segment-level analysis. In contrast,\nChain-of-Thought (CoT) prompting outperforms one-step prompting by offering\nimproved reasoning and explainability. In this paper, we introduce\nKnowledge-Prompted Estimator (KPE), a CoT prompting method that combines three\none-step prompting techniques, including perplexity, token-level similarity,\nand sentence-level similarity. This method attains enhanced performance for\nsegment-level estimation compared with previous deep learning models and\none-step prompting approaches. Furthermore, supplementary experiments on\nword-level visualized alignment demonstrate that our KPE method significantly\nimproves token alignment compared with earlier models and provides better\ninterpretability for MT quality estimation. Code will be released upon\npublication.",
        "pdf_link": "https://arxiv.org/pdf/2306.07486v1.pdf"
    },
    {
        "title": "Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling",
        "authors": [
            "Akshat Gupta"
        ],
        "published": "2023-06-12T19:20:18Z",
        "summary": "With their increasing size, large language models (LLMs) are becoming\nincreasingly good at language understanding tasks. But even with high\nperformance on specific downstream task, LLMs fail at simple linguistic tests\nfor negation or quantifier understanding. Previous work on quantifier\nunderstanding in LLMs show inverse scaling in understanding few-type\nquantifiers. In this paper, we question the claims of of previous work and show\nthat it is a result of inappropriate testing methodology. We also present\nalternate methods to measure quantifier comprehension in LLMs and show that\nLLMs are able to better understand the difference between the meaning of\nfew-type and most-type quantifiers as their size increases, although they are\nnot particularly good at it. We also observe inverse scaling for most-type\nquantifier understanding, which is contrary to human psycho-linguistic\nexperiments and previous work, where the model's understanding of most-type\nquantifier gets worse as the model size increases. We do this evaluation on\nmodels ranging from 125M-175B parameters, which suggests that LLMs do not do as\nwell as expected with quantifiers. We also discuss the possible reasons for\nthis and the relevance of quantifier understanding in evaluating language\nunderstanding in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.07384v3.pdf"
    },
    {
        "title": "Lost in Translation: Large Language Models in Non-English Content Analysis",
        "authors": [
            "Gabriel Nicholas",
            "Aliya Bhatia"
        ],
        "published": "2023-06-12T19:10:47Z",
        "summary": "In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\nGoogle's PaLM) have become the dominant approach for building AI systems to\nanalyze and generate language online. However, the automated systems that\nincreasingly mediate our interactions online -- such as chatbots, content\nmoderation systems, and search engines -- are primarily designed for and work\nfar more effectively in English than in the world's other 7,000 languages.\nRecently, researchers and technology companies have attempted to extend the\ncapabilities of large language models into languages other than English by\nbuilding what are called multilingual language models.\n  In this paper, we explain how these multilingual language models work and\nexplore their capabilities and limits. Part I provides a simple technical\nexplanation of how large language models work, why there is a gap in available\ndata between English and other languages, and how multilingual language models\nattempt to bridge that gap. Part II accounts for the challenges of doing\ncontent analysis with large language models in general and multilingual\nlanguage models in particular. Part III offers recommendations for companies,\nresearchers, and policymakers to keep in mind when considering researching,\ndeveloping and deploying large and multilingual language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.07377v1.pdf"
    },
    {
        "title": "Waffling around for Performance: Visual Classification with Random Words and Broad Concepts",
        "authors": [
            "Karsten Roth",
            "Jae Myung Kim",
            "A. Sophia Koepke",
            "Oriol Vinyals",
            "Cordelia Schmid",
            "Zeynep Akata"
        ],
        "published": "2023-06-12T17:59:48Z",
        "summary": "The visual classification performance of vision-language models such as CLIP\nhas been shown to benefit from additional semantic knowledge from large\nlanguage models (LLMs) such as GPT-3. In particular, averaging over\nLLM-generated class descriptors, e.g. \"waffle, which has a round shape\", can\nnotably improve generalization performance. In this work, we critically study\nthis behavior and propose WaffleCLIP, a framework for zero-shot visual\nclassification which simply replaces LLM-generated descriptors with random\ncharacter and word descriptors. Without querying external models, we achieve\ncomparable performance gains on a large number of visual classification tasks.\nThis allows WaffleCLIP to both serve as a low-cost alternative, as well as a\nsanity check for any future LLM-based vision-language model extensions. We\nconduct an extensive experimental study on the impact and shortcomings of\nadditional semantics introduced with LLM-generated descriptors, and showcase\nhow - if available - semantic context is better leveraged by querying LLMs for\nhigh-level concepts, which we show can be done to jointly resolve potential\nclass name ambiguities. Code is available here:\nhttps://github.com/ExplainableML/WaffleCLIP.",
        "pdf_link": "https://arxiv.org/pdf/2306.07282v2.pdf"
    },
    {
        "title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow",
        "authors": [
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Yueting Zhuang"
        ],
        "published": "2023-06-12T16:12:56Z",
        "summary": "Various industries such as finance, meteorology, and energy generate vast\namounts of heterogeneous data every day. There is a natural demand for humans\nto manage, process, and display data efficiently. However, it necessitates\nlabor-intensive efforts and a high level of expertise for these data-related\ntasks. Considering that large language models (LLMs) have showcased promising\ncapabilities in semantic understanding and reasoning, we advocate that the\ndeployment of LLMs could autonomously manage and process massive amounts of\ndata while displaying and interacting in a human-friendly manner. Based on this\nbelief, we propose Data-Copilot, an LLM-based system that connects numerous\ndata sources on one end and caters to diverse human demands on the other end.\nActing like an experienced expert, Data-Copilot autonomously transforms raw\ndata into visualization results that best match the user's intent.\nSpecifically, Data-Copilot autonomously designs versatile interfaces (tools)\nfor data management, processing, prediction, and visualization. In real-time\nresponse, it automatically deploys a concise workflow by invoking corresponding\ninterfaces step by step for the user's request. The interface design and\ndeployment processes are fully controlled by Data-Copilot itself, without human\nassistance. Besides, we create a Data-Copilot demo that links abundant data\nfrom different domains (stock, fund, company, economics, and live news) and\naccurately respond to diverse requests, serving as a reliable AI assistant.",
        "pdf_link": "https://arxiv.org/pdf/2306.07209v1.pdf"
    },
    {
        "title": "Mitigating Prior Errors in Causal Structure Learning: Towards LLM driven Prior Knowledge",
        "authors": [
            "Lyuzhou Chen",
            "Taiyu Ban",
            "Xiangyu Wang",
            "Derui Lyu",
            "Huanhuan Chen"
        ],
        "published": "2023-06-12T11:24:48Z",
        "summary": "Causal structure learning, a prominent technique for encoding cause and\neffect relationships among variables, through Bayesian Networks (BNs). Merely\nrecovering causal structures from real-world observed data lacks precision,\nwhile the development of Large Language Models (LLM) is opening a new frontier\nof causality. LLM presents strong capability in discovering causal\nrelationships between variables with the \"text\" inputs defining the\ninvestigated variables, leading to a potential new hierarchy and new ladder of\ncausality. We aim an critical issue in the emerging topic of LLM based causal\nstructure learning, to tackle erroneous prior causal statements from LLM, which\nis seldom considered in the current context of expert dominating prior\nresources. As a pioneer attempt, we propose a BN learning strategy resilient to\nprior errors without need of human intervention. Focusing on the edge-level\nprior, we classify the possible prior errors into three types:\norder-consistent, order-reversed, and irrelevant, and provide their theoretical\nimpact on the Structural Hamming Distance (SHD) under the presumption of\nsufficient data. Intriguingly, we discover and prove that only the\norder-reversed error contributes to an increase in a unique acyclic closed\nstructure, defined as a \"quasi-circle\". Leveraging this insight, a post-hoc\nstrategy is employed to identify the order-reversed prior error by its impact\non the increment of \"quasi-circles\". Through empirical evaluation on both real\nand synthetic datasets, we demonstrate our strategy's robustness against prior\nerrors. Specifically, we highlight its substantial ability to resist\norder-reversed errors while maintaining the majority of correct prior\nknowledge.",
        "pdf_link": "https://arxiv.org/pdf/2306.07032v1.pdf"
    },
    {
        "title": "Weakly supervised information extraction from inscrutable handwritten document images",
        "authors": [
            "Sujoy Paul",
            "Gagan Madan",
            "Akankshya Mishra",
            "Narayan Hegde",
            "Pradeep Kumar",
            "Gaurav Aggarwal"
        ],
        "published": "2023-06-12T02:22:30Z",
        "summary": "State-of-the-art information extraction methods are limited by OCR errors.\nThey work well for printed text in form-like documents, but unstructured,\nhandwritten documents still remain a challenge. Adapting existing models to\ndomain-specific training data is quite expensive, because of two factors, 1)\nlimited availability of the domain-specific documents (such as handwritten\nprescriptions, lab notes, etc.), and 2) annotations become even more\nchallenging as one needs domain-specific knowledge to decode inscrutable\nhandwritten document images. In this work, we focus on the complex problem of\nextracting medicine names from handwritten prescriptions using only weakly\nlabeled data. The data consists of images along with the list of medicine names\nin it, but not their location in the image. We solve the problem by first\nidentifying the regions of interest, i.e., medicine lines from just weak labels\nand then injecting a domain-specific medicine language model learned using only\nsynthetically generated data. Compared to off-the-shelf state-of-the-art\nmethods, our approach performs >2.5x better in medicine names extraction from\nprescriptions.",
        "pdf_link": "https://arxiv.org/pdf/2306.06823v1.pdf"
    },
    {
        "title": "Multimodal Audio-textual Architecture for Robust Spoken Language Understanding",
        "authors": [
            "Anderson R. Avila",
            "Mehdi Rezagholizadeh",
            "Chao Xing"
        ],
        "published": "2023-06-12T01:55:53Z",
        "summary": "Recent voice assistants are usually based on the cascade spoken language\nunderstanding (SLU) solution, which consists of an automatic speech recognition\n(ASR) engine and a natural language understanding (NLU) system. Because such\napproach relies on the ASR output, it often suffers from the so-called ASR\nerror propagation. In this work, we investigate impacts of this ASR error\npropagation on state-of-the-art NLU systems based on pre-trained language\nmodels (PLM), such as BERT and RoBERTa. Moreover, a multimodal language\nunderstanding (MLU) module is proposed to mitigate SLU performance degradation\ncaused by errors present in the ASR transcript. The MLU benefits from\nself-supervised features learned from both audio and text modalities,\nspecifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines\nan encoder network to embed the audio signal and a text encoder to process text\ntranscripts followed by a late fusion layer to fuse audio and text logits. We\nfound that the proposed MLU showed to be robust towards poor quality ASR\ntranscripts, while the performance of BERT and RoBERTa are severely\ncompromised. Our model is evaluated on five tasks from three SLU datasets and\nrobustness is tested using ASR transcripts from three ASR engines. Results show\nthat the proposed approach effectively mitigates the ASR error propagation\nproblem, surpassing the PLM models' performance across all datasets for the\nacademic ASR engine.",
        "pdf_link": "https://arxiv.org/pdf/2306.06819v2.pdf"
    },
    {
        "title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models",
        "authors": [
            "Jiaqi Xue",
            "Mengxin Zheng",
            "Ting Hua",
            "Yilin Shen",
            "Yepeng Liu",
            "Ladislau Boloni",
            "Qian Lou"
        ],
        "published": "2023-06-12T01:22:39Z",
        "summary": "Large Language Models (LLMs) are progressively being utilized as machine\nlearning services and interface tools for various applications. However, the\nsecurity implications of LLMs, particularly in relation to adversarial and\nTrojan attacks, remain insufficiently examined. In this paper, we propose\nTrojLLM, an automatic and black-box framework to effectively generate universal\nand stealthy triggers. When these triggers are incorporated into the input\ndata, the LLMs' outputs can be maliciously manipulated. Moreover, the framework\nalso supports embedding Trojans within discrete prompts, enhancing the overall\neffectiveness and precision of the triggers' attacks. Specifically, we propose\na trigger discovery algorithm for generating universal triggers for various\ninputs by querying victim LLM-based APIs using few-shot data samples.\nFurthermore, we introduce a novel progressive Trojan poisoning algorithm\ndesigned to generate poisoned prompts that retain efficacy and transferability\nacross a diverse range of models. Our experiments and results demonstrate\nTrojLLM's capacity to effectively insert Trojans into text prompts in\nreal-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining\nexceptional performance on clean test sets. Our work sheds light on the\npotential security risks in current models and offers a potential defensive\napproach. The source code of TrojLLM is available at\nhttps://github.com/UCF-ML-Research/TrojLLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.06815v3.pdf"
    },
    {
        "title": "A blind spot for large language models: Supradiegetic linguistic information",
        "authors": [
            "Julia Witte Zimmerman",
            "Denis Hudon",
            "Kathryn Cramer",
            "Jonathan St. Onge",
            "Mikaela Fudolig",
            "Milo Z. Trujillo",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "published": "2023-06-11T22:15:01Z",
        "summary": "Large Language Models (LLMs) like ChatGPT reflect profound changes in the\nfield of Artificial Intelligence, achieving a linguistic fluency that is\nimpressively, even shockingly, human-like. The extent of their current and\npotential capabilities is an active area of investigation by no means limited\nto scientific researchers. It is common for people to frame the training data\nfor LLMs as \"text\" or even \"language\". We examine the details of this framing\nusing ideas from several areas, including linguistics, embodied cognition,\ncognitive science, mathematics, and history. We propose that considering what\nit is like to be an LLM like ChatGPT, as Nagel might have put it, can help us\ngain insight into its capabilities in general, and in particular, that its\nexposure to linguistic training data can be productively reframed as exposure\nto the diegetic information encoded in language, and its deficits can be\nreframed as ignorance of extradiegetic information, including supradiegetic\nlinguistic information. Supradiegetic linguistic information consists of those\narbitrary aspects of the physical form of language that are not derivable from\nthe one-dimensional relations of context -- frequency, adjacency, proximity,\nco-occurrence -- that LLMs like ChatGPT have access to. Roughly speaking, the\ndiegetic portion of a word can be thought of as its function, its meaning, as\nthe information in a theoretical vector in a word embedding, while the\nsupradiegetic portion of the word can be thought of as its form, like the\nshapes of its letters or the sounds of its syllables. We use these concepts to\ninvestigate why LLMs like ChatGPT have trouble handling palindromes, the visual\ncharacteristics of symbols, translating Sumerian cuneiform, and continuing\ninteger sequences.",
        "pdf_link": "https://arxiv.org/pdf/2306.06794v2.pdf"
    },
    {
        "title": "Augmenting Greybox Fuzzing with Generative AI",
        "authors": [
            "Jie Hu",
            "Qian Zhang",
            "Heng Yin"
        ],
        "published": "2023-06-11T21:44:47Z",
        "summary": "Real-world programs expecting structured inputs often has a format-parsing\nstage gating the deeper program space. Neither a mutation-based approach nor a\ngenerative approach can provide a solution that is effective and scalable.\nLarge language models (LLM) pre-trained with an enormous amount of natural\nlanguage corpus have proved to be effective for understanding the implicit\nformat syntax and generating format-conforming inputs. In this paper, propose\nChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, we\npick a seed in the fuzzer's seed pool and prompt ChatGPT generative models to\nvariations, which are more likely to be format-conforming and thus of high\nquality. We conduct extensive experiments to explore the best practice for\nharvesting the power of generative LLM models. The experiment results show that\nour approach improves the edge coverage by 12.77\\% over the SOTA greybox fuzzer\n(AFL++) on 12 target programs from three well-tested benchmarks. As for\nvulnerability detection, \\sys is able to perform similar to or better than\nAFL++ for programs with explicit syntax rules but not for programs with\nnon-trivial syntax.",
        "pdf_link": "https://arxiv.org/pdf/2306.06782v1.pdf"
    },
    {
        "title": "Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis",
        "authors": [
            "James R. Kirk",
            "Robert E. Wray",
            "Peter Lindes",
            "John E. Laird"
        ],
        "published": "2023-06-11T20:50:14Z",
        "summary": "Large language models (LLMs) offer significant promise as a knowledge source\nfor task learning. Prompt engineering has been shown to be effective for\neliciting knowledge from an LLM, but alone it is insufficient for acquiring\nrelevant, situationally grounded knowledge for an embodied agent learning novel\ntasks. We describe a cognitive-agent approach, STARS, that extends and\ncomplements prompt engineering, mitigating its limitations and thus enabling an\nagent to acquire new task knowledge matched to its native language\ncapabilities, embodiment, environment, and user preferences. The STARS approach\nis to increase the response space of LLMs and deploy general strategies,\nembedded within the autonomous agent, to evaluate, repair, and select among\ncandidate responses produced by the LLM. We describe the approach and\nexperiments that show how an agent, by retrieving and evaluating a breadth of\nresponses from the LLM, can achieve 77-94% task completion in one-shot learning\nwithout user oversight. The approach achieves 100% task completion when human\noversight (such as an indication of preference) is provided. Further, the type\nof oversight largely shifts from explicit, natural language instruction to\nsimple confirmation/discomfirmation of high-quality responses that have been\nvetted by the agent before presentation to a user.",
        "pdf_link": "https://arxiv.org/pdf/2306.06770v4.pdf"
    },
    {
        "title": "The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases",
        "authors": [
            "Jiancheng Yang",
            "Hongwei Bran Li",
            "Donglai Wei"
        ],
        "published": "2023-06-11T20:39:13Z",
        "summary": "This study investigates the transformative potential of Large Language Models\n(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public\ndata, these models, which possess remarkable language understanding and\ngeneration capabilities, are augmenting the interpretive skills of\nradiologists, enhancing patient-physician communication, and streamlining\nclinical workflows. The paper introduces an analytic framework for presenting\nthe complex interactions between LLMs and the broader ecosystem of medical\nimaging stakeholders, including businesses, insurance entities, governments,\nresearch institutions, and hospitals (nicknamed BIGR-H). Through detailed\nanalyses, illustrative use cases, and discussions on the broader implications\nand future directions, this perspective seeks to raise discussion in strategic\nplanning and decision-making in the era of AI-enabled healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2306.06767v2.pdf"
    },
    {
        "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution",
        "authors": [
            "Prithwish Jana",
            "Piyush Jha",
            "Haoyang Ju",
            "Gautham Kishore",
            "Aryan Mahajan",
            "Vijay Ganesh"
        ],
        "published": "2023-06-11T19:47:52Z",
        "summary": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Current LLM-based code translation\nmethods lack a training approach to ensure that the translated code reliably\ncompiles or bears substantial functional equivalence to the input code. In our\nwork, we train an LLM via reinforcement learning, by modifying the fine-tuning\nprocess to incorporate compiler feedback and symbolic execution (symexec)-based\nequivalence testing feedback that checks for functional equivalence between the\ninput and output programs. The idea is to guide an LLM-in-training, via\ncompiler and symexec-based testing feedback, by letting it know how far it is\nfrom producing perfect translations. We report on extensive experiments\ncomparing CoTran with 14 other code translation tools that include\nhuman-written transpilers, LLM-based translation tools, and ChatGPT over a\nbenchmark of more than 57,000 Java-Python equivalent pairs, and we show that\nCoTran outperforms them on relevant metrics such as compilation accuracy\n(CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool\nachieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas\nthe nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp.\nAlso, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc\nand +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation\nresp.",
        "pdf_link": "https://arxiv.org/pdf/2306.06755v3.pdf"
    },
    {
        "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Yang Yang",
            "Hongyin Tang",
            "Keqing He",
            "Jiahao Liu",
            "Jingang Wang",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023-06-11T09:17:21Z",
        "summary": "Currently, the reduction in the parameter scale of large-scale pre-trained\nlanguage models (PLMs) through knowledge distillation has greatly facilitated\ntheir widespread deployment on various devices. However, the deployment of\nknowledge distillation systems faces great challenges in real-world\nindustrial-strength applications, which require the use of complex distillation\nmethods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the\nswitching of methods. To overcome these challenges, we propose GKD, a general\nknowledge distillation framework that supports distillation on larger-scale\nPLMs using various distillation methods. With GKD, developers can build larger\ndistillation models on memory-limited GPUs and easily switch and combine\ndifferent distillation methods within a single framework. Experimental results\nshow that GKD can support the distillation of at least 100B-scale PLMs and 25\nmainstream methods on 8 NVIDIA A100 (40GB) GPUs.",
        "pdf_link": "https://arxiv.org/pdf/2306.06629v1.pdf"
    },
    {
        "title": "Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023-06-11T08:53:27Z",
        "summary": "The large scale of pre-trained language models poses a challenge for their\ndeployment on various devices, with a growing emphasis on methods to compress\nthese models, particularly knowledge distillation. However, current knowledge\ndistillation methods rely on the model's intermediate layer features and the\ngolden labels (also called hard labels), which usually require aligned model\narchitecture and enough labeled data respectively. Moreover, the parameters of\nvocabulary are usually neglected in existing methods. To address these\nproblems, we propose a general language model distillation (GLMD) method that\nperforms two-stage word prediction distillation and vocabulary compression,\nwhich is simple and surprisingly shows extremely strong performance.\nSpecifically, GLMD supports more general application scenarios by eliminating\nthe constraints of dimension and structure between models and the need for\nlabeled datasets through the absence of intermediate layers and golden labels.\nMeanwhile, based on the long-tailed distribution of word frequencies in the\ndata, GLMD designs a strategy of vocabulary compression through decreasing\nvocabulary size instead of dimensionality. Experimental results show that our\nmethod outperforms 25 state-of-the-art methods on the SuperGLUE benchmark,\nachieving an average score that surpasses the best method by 3%.",
        "pdf_link": "https://arxiv.org/pdf/2306.06625v1.pdf"
    },
    {
        "title": "RestGPT: Connecting Large Language Models with Real-World RESTful APIs",
        "authors": [
            "Yifan Song",
            "Weimin Xiong",
            "Dawei Zhu",
            "Wenhao Wu",
            "Han Qian",
            "Mingbo Song",
            "Hailiang Huang",
            "Cheng Li",
            "Ke Wang",
            "Rong Yao",
            "Ye Tian",
            "Sujian Li"
        ],
        "published": "2023-06-11T08:53:12Z",
        "summary": "Tool-augmented large language models (LLMs) have achieved remarkable progress\nin tackling a broad range of tasks. However, existing methods are mainly\nrestricted to specifically designed tools and fail to fulfill complex\ninstructions, having great limitations when confronted with real-world\nscenarios. In this paper, we explore a more realistic scenario by connecting\nLLMs with RESTful APIs, which adhere to the widely adopted REST software\narchitectural style for web service development. To address the practical\nchallenges of tackling complex instructions, we propose RestGPT, which exploits\nthe power of LLMs and conducts a coarse-to-fine online planning mechanism to\nenhance the abilities of task decomposition and API selection. RestGPT also\ncontains an API executor tailored for calling RESTful APIs, which can\nmeticulously formulate parameters and parse API responses. To fully evaluate\nthe performance of RestGPT, we propose RestBench, a high-quality benchmark\nwhich consists of two real-world scenarios and human-annotated instructions\nwith gold solution paths. Experiments show that RestGPT is able to achieve\nimpressive results in complex tasks and has strong robustness, which paves a\nnew way towards AGI. RestGPT and RestBench is publicly available at\nhttps://restgpt.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2306.06624v2.pdf"
    },
    {
        "title": "Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective",
        "authors": [
            "Jiatong Li",
            "Yunqing Liu",
            "Wenqi Fan",
            "Xiao-Yong Wei",
            "Hui Liu",
            "Jiliang Tang",
            "Qing Li"
        ],
        "published": "2023-06-11T08:16:25Z",
        "summary": "Molecule discovery plays a crucial role in various scientific fields,\nadvancing the design of tailored materials and drugs. Traditional methods for\nmolecule discovery follow a trial-and-error process, which are both\ntime-consuming and costly, while computational approaches such as artificial\nintelligence (AI) have emerged as revolutionary tools to expedite various\ntasks, like molecule-caption translation. Despite the importance of\nmolecule-caption translation for molecule discovery, most of the existing\nmethods heavily rely on domain experts, require excessive computational cost,\nand suffer from poor performance. On the other hand, Large Language Models\n(LLMs), like ChatGPT, have shown remarkable performance in various cross-modal\ntasks due to their great powerful capabilities in natural language\nunderstanding, generalization, and reasoning, which provides unprecedented\nopportunities to advance molecule discovery. To address the above limitations,\nin this work, we propose a novel LLMs-based framework (\\textbf{MolReGPT}) for\nmolecule-caption translation, where a retrieval-based prompt paradigm is\nintroduced to empower molecule discovery with LLMs like ChatGPT without\nfine-tuning. More specifically, MolReGPT leverages the principle of molecular\nsimilarity to retrieve similar molecules and their text descriptions from a\nlocal database to ground the generation of LLMs through in-context few-shot\nmolecule learning. We evaluate the effectiveness of MolReGPT via\nmolecule-caption translation, which includes molecule understanding and\ntext-based molecule generation. Experimental results show that MolReGPT\noutperforms fine-tuned models like MolT5-base without any additional training.\nTo the best of our knowledge, MolReGPT is the first work to leverage LLMs in\nmolecule-caption translation for advancing molecule discovery.",
        "pdf_link": "https://arxiv.org/pdf/2306.06615v1.pdf"
    },
    {
        "title": "Inductive reasoning in humans and large language models",
        "authors": [
            "Simon J. Han",
            "Keith Ransom",
            "Andrew Perfors",
            "Charles Kemp"
        ],
        "published": "2023-06-11T00:23:25Z",
        "summary": "The impressive recent performance of large language models has led many to\nwonder to what extent they can serve as models of general intelligence or are\nsimilar to human cognition. We address this issue by applying GPT-3.5 and GPT-4\nto a classic problem in human inductive reasoning known as property induction.\nOver two experiments, we elicit human judgments on a range of property\ninduction tasks spanning multiple domains. Although GPT-3.5 struggles to\ncapture many aspects of human behaviour, GPT-4 is much more successful: for the\nmost part, its performance qualitatively matches that of humans, and the only\nnotable exception is its failure to capture the phenomenon of premise\nnon-monotonicity. Our work demonstrates that property induction allows for\ninteresting comparisons between human and machine intelligence and provides two\nlarge datasets that can serve as benchmarks for future work in this vein.",
        "pdf_link": "https://arxiv.org/pdf/2306.06548v3.pdf"
    },
    {
        "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
        "authors": [
            "Yongchao Chen",
            "Jacob Arkin",
            "Charles Dawson",
            "Yang Zhang",
            "Nicholas Roy",
            "Chuchu Fan"
        ],
        "published": "2023-06-10T21:58:29Z",
        "summary": "For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.",
        "pdf_link": "https://arxiv.org/pdf/2306.06531v3.pdf"
    },
    {
        "title": "Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification",
        "authors": [
            "Shouvon Sarker",
            "Lijun Qian",
            "Xishuang Dong"
        ],
        "published": "2023-06-10T20:55:21Z",
        "summary": "The identification of key factors such as medications, diseases, and\nrelationships within electronic health records and clinical notes has a wide\nrange of applications in the clinical field. In the N2C2 2022 competitions,\nvarious tasks were presented to promote the identification of key factors in\nelectronic health records (EHRs) using the Contextualized Medication Event\nDataset (CMED). Pretrained large language models (LLMs) demonstrated\nexceptional performance in these tasks. This study aims to explore the\nutilization of LLMs, specifically ChatGPT, for data augmentation to overcome\nthe limited availability of annotated data for identifying the key factors in\nEHRs. Additionally, different pre-trained BERT models, initially trained on\nextensive datasets like Wikipedia and MIMIC, were employed to develop models\nfor identifying these key variables in EHRs through fine-tuning on augmented\ndatasets. The experimental results of two EHR analysis tasks, namely medication\nidentification and medication event classification, indicate that data\naugmentation based on ChatGPT proves beneficial in improving performance for\nboth medication identification and medication event classification.",
        "pdf_link": "https://arxiv.org/pdf/2306.07297v1.pdf"
    },
    {
        "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
        "authors": [
            "Jianing Wang",
            "Qiushi Sun",
            "Nuo Chen",
            "Xiang Li",
            "Ming Gao"
        ],
        "published": "2023-06-10T12:42:36Z",
        "summary": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.06427v2.pdf"
    },
    {
        "title": "Human-in-the-Loop through Chain-of-Thought",
        "authors": [
            "Zefan Cai",
            "Baobao Chang",
            "Wenjuan Han"
        ],
        "published": "2023-06-10T04:31:57Z",
        "summary": "While the emergence of powerful language models along with Chain-of-thought\nprompting has made automation more and more omnipresent, it sometimes\ndemonstrates its weakness in long-term or multi-step logical reasoning. For\nexample, users don't always get desirable answers for complex mathematical\nproblems without human involvement. Against this background, we present the\nManual Correction System (MCS) -- a human-in-the-loop system enhanced by\nChain-of-Thought prompting, which explores how manual correction of sub-logics\nin rationales can improve LLM's reasoning performance. Moving one step forward,\nconsidering a system with human-in-the-loop involves more than having humans\nimprove performance but also controlling the cost. Therefore, we post a\nCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on\nclassical economics theory to analyze, quantify and balance the utility and the\ncorresponding cost. We conduct experiments of MCS and CAMLOP with twelve\ndatasets. A significant advantage w.r.t cost and utility proves its superiority\nover strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2306.07932v2.pdf"
    },
    {
        "title": "Measuring and Modifying Factual Knowledge in Large Language Models",
        "authors": [
            "Pouya Pezeshkpour"
        ],
        "published": "2023-06-09T21:25:48Z",
        "summary": "Large Language Models (LLMs) store an extensive amount of factual knowledge\nobtained from vast collections of text. To effectively utilize these models for\ndownstream tasks, it is crucial to have reliable methods for measuring their\nknowledge. However, existing approaches for knowledge measurement have certain\nlimitations, and despite recent efforts, they fail to provide accurate\nmeasurements and the necessary insights for modifying the knowledge within\nLLMs. In this work, we employ information theory-based measurements to provide\na framework estimating the factual knowledge contained within large language\nmodels. More specifically, we measure knowledge by analyzing the LLM's\nprediction probability distribution before and after instilling the target\nknowledge, employing metrics such as entropy and KL-divergence. Introducing our\nmetrics, we first assess their accuracy in comparison to previous ranking-based\nmethods, surpassing them by over $35\\%$ in a synthetic experiment. Then, we\nexplore two prominent methods of knowledge instillation, discovering that LLMs\nexhibit limitations in capturing new knowledge under specific circumstances for\none of these methods. Lastly, we demonstrate the applicability of our methods\nin extracting unlearned and mislearned facts in LLMs through their application\nto in-context learning. We make code and data for all methods and experiments\nin this paper publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.06264v1.pdf"
    },
    {
        "title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording",
        "authors": [
            "Aisha Khatun",
            "Daniel G. Brown"
        ],
        "published": "2023-06-09T19:07:31Z",
        "summary": "Large language models (LLMs) have become mainstream technology with their\nversatile use cases and impressive performance. Despite the countless\nout-of-the-box applications, LLMs are still not reliable. A lot of work is\nbeing done to improve the factual accuracy, consistency, and ethical standards\nof these models through fine-tuning, prompting, and Reinforcement Learning with\nHuman Feedback (RLHF), but no systematic analysis of the responses of these\nmodels to different categories of statements, or on their potential\nvulnerabilities to simple prompting changes is available. In this work, we\nanalyze what confuses GPT-3: how the model responds to certain sensitive topics\nand what effects the prompt wording has on the model response. We find that\nGPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes\nmistakes with common Misconceptions and Controversies. The model responses are\ninconsistent across prompts and settings, highlighting GPT-3's unreliability.\nDataset and code of our analysis is available in\nhttps://github.com/tanny411/GPT3-Reliability-Check.",
        "pdf_link": "https://arxiv.org/pdf/2306.06199v1.pdf"
    },
    {
        "title": "Trapping LLM Hallucinations Using Tagged Context Prompts",
        "authors": [
            "Philip Feldman",
            "James R. Foulds",
            "Shimei Pan"
        ],
        "published": "2023-06-09T17:48:54Z",
        "summary": "Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n  We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2306.06085v1.pdf"
    },
    {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-06-09T17:44:31Z",
        "summary": "We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.",
        "pdf_link": "https://arxiv.org/pdf/2306.06070v3.pdf"
    },
    {
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "authors": [
            "Hongyang Yang",
            "Xiao-Yang Liu",
            "Christina Dan Wang"
        ],
        "published": "2023-06-09T16:52:00Z",
        "summary": "Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
        "pdf_link": "https://arxiv.org/pdf/2306.06031v1.pdf"
    },
    {
        "title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput",
        "authors": [
            "Yunho Jin",
            "Chun-Feng Wu",
            "David Brooks",
            "Gu-Yeon Wei"
        ],
        "published": "2023-06-09T16:13:43Z",
        "summary": "Generating texts with a large language model (LLM) consumes massive amounts\nof memory. Apart from the already-large model parameters, the key/value (KV)\ncache that holds information about previous tokens in a sequence can grow to be\neven larger than the model itself. This problem is exacerbated in one of the\ncurrent LLM serving frameworks which reserves the maximum sequence length of\nmemory for the KV cache to guarantee generating a complete sequence as they do\nnot know the output sequence length. This restricts us to use a smaller batch\nsize leading to lower GPU utilization and above all, lower throughput. We argue\nthat designing a system with a priori knowledge of the output sequence can\nmitigate this problem. To this end, we propose S$^{3}$, which predicts the\noutput sequence length, schedules generation queries based on the prediction to\nincrease device resource utilization and throughput, and handle mispredictions.\nOur proposed method achieves 6.49$\\times$ throughput over those systems that\nassume the worst case for the output sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2306.06000v1.pdf"
    },
    {
        "title": "Language Models Can Learn Exceptions to Syntactic Rules",
        "authors": [
            "Cara Su-Yi Leong",
            "Tal Linzen"
        ],
        "published": "2023-06-09T15:35:11Z",
        "summary": "Artificial neural networks can generalize productively to novel contexts. Can\nthey also learn exceptions to those productive rules? We explore this question\nusing the case of restrictions on English passivization (e.g., the fact that\n\"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by\nthe vacation\" is not). We collect human acceptability judgments for passive\nsentences with a range of verbs, and show that the probability distribution\ndefined by GPT-2, a language model, matches the human judgments with high\ncorrelation. We also show that the relative acceptability of a verb in the\nactive vs. passive voice is positively correlated with the relative frequency\nof its occurrence in those voices. These results provide preliminary support\nfor the entrenchment hypothesis, according to which learners track and uses the\ndistributional properties of their input to learn negative exceptions to rules.\nAt the same time, this hypothesis fails to explain the magnitude of\nunpassivizability demonstrated by certain individual verbs, suggesting that\nother cues to exceptionality are available in the linguistic input.",
        "pdf_link": "https://arxiv.org/pdf/2306.05969v1.pdf"
    },
    {
        "title": "Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?",
        "authors": [
            "Wissam Antoun",
            "Virginie Mouilleron",
            "Beno\u00eet Sagot",
            "Djam\u00e9 Seddah"
        ],
        "published": "2023-06-09T13:03:53Z",
        "summary": "Recent advances in natural language processing (NLP) have led to the\ndevelopment of large language models (LLMs) such as ChatGPT. This paper\nproposes a methodology for developing and evaluating ChatGPT detectors for\nFrench text, with a focus on investigating their robustness on out-of-domain\ndata and against common attack schemes. The proposed method involves\ntranslating an English dataset into French and training a classifier on the\ntranslated data. Results show that the detectors can effectively detect\nChatGPT-generated text, with a degree of robustness against basic attack\ntechniques in in-domain settings. However, vulnerabilities are evident in\nout-of-domain contexts, highlighting the challenge of detecting adversarial\ntext. The study emphasizes caution when applying in-domain testing results to a\nwider variety of content. We provide our translated datasets and models as\nopen-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection",
        "pdf_link": "https://arxiv.org/pdf/2306.05871v1.pdf"
    },
    {
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
            "Zhijing Jin",
            "Jiarui Liu",
            "Zhiheng Lyu",
            "Spencer Poff",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Mona Diab",
            "Bernhard Sch\u00f6lkopf"
        ],
        "published": "2023-06-09T12:09:15Z",
        "summary": "Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.",
        "pdf_link": "https://arxiv.org/pdf/2306.05836v2.pdf"
    },
    {
        "title": "Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives",
        "authors": [
            "Rabee Qasem",
            "Banan Tantour",
            "Mohammed Maree"
        ],
        "published": "2023-06-09T11:57:57Z",
        "summary": "With the ever-increasing utilization of natural language processing (NLP), we\nstarted to witness over the past few years a significant transformation in our\ninteraction with legal texts. This technology has advanced the analysis and\nenhanced the understanding of complex legal terminology and contexts. The\ndevelopment of recent large language models (LLMs), particularly ChatGPT, has\nalso introduced a revolutionary contribution to the way that legal texts can be\nprocessed and comprehended. In this paper, we present our work on a\ncooperative-legal question-answering LLM-based chatbot, where we developed a\nset of legal questions about Palestinian cooperatives, associated with their\nregulations and compared the auto-generated answers by the chatbot to their\ncorrespondences that are designed by a legal expert. To evaluate the proposed\nchatbot, we have used 50 queries generated by the legal expert and compared the\nanswers produced by the chart to their relevance judgments. Finding\ndemonstrated that an overall accuracy rate of 82% has been achieved when\nanswering the queries, while exhibiting an F1 score equivalent to 79%.",
        "pdf_link": "https://arxiv.org/pdf/2306.05827v1.pdf"
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "authors": [
            "Jianghao Lin",
            "Xinyi Dai",
            "Yunjia Xi",
            "Weiwen Liu",
            "Bo Chen",
            "Hao Zhang",
            "Yong Liu",
            "Chuhan Wu",
            "Xiangyang Li",
            "Chenxu Zhu",
            "Huifeng Guo",
            "Yong Yu",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "published": "2023-06-09T11:31:50Z",
        "summary": "With the rapid development of online services, recommender systems (RS) have\nbecome increasingly indispensable for mitigating information overload. Despite\nremarkable progress, conventional recommendation models (CRM) still have some\nlimitations, e.g., lacking open-world knowledge, and difficulties in\ncomprehending users' underlying preferences and motivations. Meanwhile, large\nlanguage models (LLM) have shown impressive general intelligence and human-like\ncapabilities, which mainly stem from their extensive open-world knowledge,\nreasoning ability, as well as their comprehension of human culture and society.\nConsequently, the emergence of LLM is inspiring the design of recommender\nsystems and pointing out a promising research direction, i.e., whether we can\nincorporate LLM and benefit from their knowledge and capabilities to compensate\nfor the limitations of CRM. In this paper, we conduct a comprehensive survey on\nthis research direction from the perspective of the whole pipeline in\nreal-world recommender systems. Specifically, we summarize existing works from\ntwo orthogonal aspects: where and how to adapt LLM to RS. For the WHERE\nquestion, we discuss the roles that LLM could play in different stages of the\nrecommendation pipeline, i.e., feature engineering, feature encoder,\nscoring/ranking function, user interaction, and pipeline controller. For the\nHOW question, we investigate the training and inference strategies, resulting\nin two fine-grained taxonomy criteria, i.e., whether to tune LLM or not, and\nwhether to involve conventional recommendation models for inference. Then, we\nhighlight key challenges in adapting LLM to RS from three aspects, i.e.,\nefficiency, effectiveness, and ethics. Finally, we summarize the survey and\ndiscuss the future prospects. We actively maintain a GitHub repository for\npapers and other related resources:\nhttps://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.",
        "pdf_link": "https://arxiv.org/pdf/2306.05817v5.pdf"
    },
    {
        "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoning Ye",
            "Lin Zhang",
            "Jianchen Wang",
            "Yixin Zhu",
            "Sihang Jiang",
            "Zhuozhi Xiong",
            "Zihan Li",
            "Weijie Wu",
            "Qianyu He",
            "Rui Xu",
            "Wenhao Huang",
            "Jingping Liu",
            "Zili Wang",
            "Shusen Wang",
            "Weiguo Zheng",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-06-09T09:52:05Z",
        "summary": "New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 249,587 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in~\\url{https://github.com/MikeGu721/XiezhiBenchmark}.",
        "pdf_link": "https://arxiv.org/pdf/2306.05783v3.pdf"
    },
    {
        "title": "Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests",
        "authors": [
            "Arto Hellas",
            "Juho Leinonen",
            "Sami Sarsa",
            "Charles Koutcheme",
            "Lilja Kujanp\u00e4\u00e4",
            "Juha Sorva"
        ],
        "published": "2023-06-09T07:19:43Z",
        "summary": "Background and Context: Over the past year, large language models (LLMs) have\ntaken the world by storm. In computing education, like in other walks of life,\nmany opportunities and threats have emerged as a consequence.\n  Objectives: In this article, we explore such opportunities and threats in a\nspecific area: responding to student programmers' help requests. More\nspecifically, we assess how good LLMs are at identifying issues in problematic\ncode that students request help on.\n  Method: We collected a sample of help requests and code from an online\nprogramming course. We then prompted two different LLMs (OpenAI Codex and\nGPT-3.5) to identify and explain the issues in the students' code and assessed\nthe LLM-generated answers both quantitatively and qualitatively.\n  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently\nfind at least one actual issue in each student program (GPT-3.5 in 90% of the\ncases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%\nof the time). False positives are common (40% chance for GPT-3.5). The advice\nthat the LLMs provide on the issues is often sensible. The LLMs perform better\non issues involving program logic rather than on output formatting. Model\nsolutions are frequently provided even when the LLM is prompted not to. LLM\nresponses to prompts in a non-English language are only slightly worse than\nresponses to English prompts.\n  Implications: Our results continue to highlight the utility of LLMs in\nprogramming education. At the same time, the results highlight the\nunreliability of LLMs: LLMs make some of the same mistakes that students do,\nperhaps especially when formatting output as required by automated assessment\nsystems. Our study informs teachers interested in using LLMs as well as future\nefforts to customize LLMs for the needs of programming education.",
        "pdf_link": "https://arxiv.org/pdf/2306.05715v1.pdf"
    },
    {
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
        "authors": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-06-09T05:55:52Z",
        "summary": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
        "pdf_link": "https://arxiv.org/pdf/2306.05685v4.pdf"
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report Generation",
        "authors": [
            "Bang Yang",
            "Asif Raza",
            "Yuexian Zou",
            "Tong Zhang"
        ],
        "published": "2023-06-09T03:02:36Z",
        "summary": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.",
        "pdf_link": "https://arxiv.org/pdf/2306.05642v1.pdf"
    },
    {
        "title": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding",
        "authors": [
            "Mingqiu Wang",
            "Izhak Shafran",
            "Hagen Soltau",
            "Wei Han",
            "Yuan Cao",
            "Dian Yu",
            "Laurent El Shafey"
        ],
        "published": "2023-06-08T22:33:22Z",
        "summary": "Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.",
        "pdf_link": "https://arxiv.org/pdf/2306.07944v1.pdf"
    },
    {
        "title": "Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization",
        "authors": [
            "Oleksandr Yermilov",
            "Vipul Raheja",
            "Artem Chernodub"
        ],
        "published": "2023-06-08T21:06:19Z",
        "summary": "This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available",
        "pdf_link": "https://arxiv.org/pdf/2306.05561v1.pdf"
    },
    {
        "title": "Prompt Injection attack against LLM-integrated Applications",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Yuekang Li",
            "Kailong Wang",
            "Zihao Wang",
            "Xiaofeng Wang",
            "Tianwei Zhang",
            "Yepang Liu",
            "Haoyu Wang",
            "Yan Zheng",
            "Yang Liu"
        ],
        "published": "2023-06-08T18:43:11Z",
        "summary": "Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2306.05499v2.pdf"
    },
    {
        "title": "Multi-Modal Classifiers for Open-Vocabulary Object Detection",
        "authors": [
            "Prannay Kaul",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "published": "2023-06-08T18:31:56Z",
        "summary": "The goal of this paper is open-vocabulary object detection (OVOD)\n$\\unicode{x2013}$ building a model that can detect objects beyond the set of\ncategories seen at training, thus enabling the user to specify categories of\ninterest at inference without the need for model retraining. We adopt a\nstandard two-stage object detector architecture, and explore three ways for\nspecifying novel categories: via language descriptions, via image exemplars, or\nvia a combination of the two. We make three contributions: first, we prompt a\nlarge language model (LLM) to generate informative language descriptions for\nobject classes, and construct powerful text-based classifiers; second, we\nemploy a visual aggregator on image exemplars that can ingest any number of\nimages as input, forming vision-based classifiers; and third, we provide a\nsimple method to fuse information from language descriptions and image\nexemplars, yielding a multi-modal classifier. When evaluating on the\nchallenging LVIS open-vocabulary benchmark we demonstrate that: (i) our\ntext-based classifiers outperform all previous OVOD works; (ii) our\nvision-based classifiers perform as well as text-based classifiers in prior\nwork; (iii) using multi-modal classifiers perform better than either modality\nalone; and finally, (iv) our text-based and multi-modal classifiers yield\nbetter performance than a fully-supervised detector.",
        "pdf_link": "https://arxiv.org/pdf/2306.05493v1.pdf"
    },
    {
        "title": "Artificial General Intelligence for Medical Imaging",
        "authors": [
            "Xiang Li",
            "Lu Zhang",
            "Zihao Wu",
            "Zhengliang Liu",
            "Lin Zhao",
            "Yixuan Yuan",
            "Jun Liu",
            "Gang Li",
            "Dajiang Zhu",
            "Pingkun Yan",
            "Quanzheng Li",
            "Wei Liu",
            "Tianming Liu",
            "Dinggang Shen"
        ],
        "published": "2023-06-08T18:04:13Z",
        "summary": "In this review, we explore the potential applications of Artificial General\nIntelligence (AGI) models in healthcare, focusing on foundational Large\nLanguage Models (LLMs), Large Vision Models, and Large Multimodal Models. We\nemphasize the importance of integrating clinical expertise, domain knowledge,\nand multimodal capabilities into AGI models. In addition, we lay out key\nroadmaps that guide the development and deployment of healthcare AGI models.\nThroughout the review, we provide critical perspectives on the potential\nchallenges and pitfalls associated with deploying large-scale AGI models in the\nmedical field. This comprehensive review aims to offer insights into the future\nimplications of AGI in medical imaging, healthcare and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2306.05480v2.pdf"
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "authors": [
            "Quynh Phung",
            "Songwei Ge",
            "Jia-Bin Huang"
        ],
        "published": "2023-06-08T17:59:59Z",
        "summary": "Driven by the scalable diffusion models trained on large-scale datasets,\ntext-to-image synthesis methods have shown compelling results. However, these\nmodels still fail to precisely follow the text prompt involving multiple\nobjects, attributes, or spatial compositions. In this paper, we reveal the\npotential causes in the diffusion model's cross-attention and self-attention\nlayers. We propose two novel losses to refocus attention maps according to a\ngiven spatial layout during sampling. Creating the layouts manually requires\nadditional effort and can be tedious. Therefore, we explore using large\nlanguage models (LLM) to produce these layouts for our method. We conduct\nextensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate\nour proposed method. We show that our proposed attention refocusing effectively\nimproves the controllability of existing approaches.",
        "pdf_link": "https://arxiv.org/pdf/2306.05427v2.pdf"
    },
    {
        "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
        "authors": [
            "Bo Li",
            "Yuanhan Zhang",
            "Liangyu Chen",
            "Jinghao Wang",
            "Fanyi Pu",
            "Jingkang Yang",
            "Chunyuan Li",
            "Ziwei Liu"
        ],
        "published": "2023-06-08T17:59:56Z",
        "summary": "High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.",
        "pdf_link": "https://arxiv.org/pdf/2306.05425v1.pdf"
    },
    {
        "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
        "authors": [
            "Qiaoyu Tang",
            "Ziliang Deng",
            "Hongyu Lin",
            "Xianpei Han",
            "Qiao Liang",
            "Boxi Cao",
            "Le Sun"
        ],
        "published": "2023-06-08T15:46:32Z",
        "summary": "Enabling large language models to utilize real-world tools effectively is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have either primarily relied on extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nutilized supervised learning to train limited scopes of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without tool-specific training. To\naddress this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a diverse tool-use corpus and learn\ngeneralized tool-use abilities on compact language models with minimal human\nintervention. Specifically, ToolAlpaca first automatically creates a highly\ndiversified tool-use corpus by building a multi-agent simulation environment.\nThe corpus contains 3938 tool-use instances from more than 400 real-world tool\nAPIs spanning 50 distinct categories. Subsequently, the constructed corpus is\nemployed to fine-tune compact language models, resulting in two models, namely\nToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the\nability of these models to utilize previously unseen tools without specific\ntraining. Experimental results demonstrate that ToolAlpaca achieves effective\ngeneralized tool-use capabilities comparable to those of extremely large\nlanguage models like GPT-3.5, demonstrating that learning generalized tool-use\nability is feasible for compact language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.05301v2.pdf"
    },
    {
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Xiao Zhang",
            "Yanzhao Lai",
            "Min Peng",
            "Alejandro Lopez-Lira",
            "Jimin Huang"
        ],
        "published": "2023-06-08T14:20:29Z",
        "summary": "Although large language models (LLMs) has shown great performance on natural\nlanguage processing (NLP) in the financial domain, there are no publicly\navailable financial tailtored LLMs, instruction tuning datasets, and evaluation\nbenchmarks, which is critical for continually pushing forward the open-source\ndevelopment of financial artificial intelligence (AI). This paper introduces\nPIXIU, a comprehensive framework including the first financial LLM based on\nfine-tuning LLaMA with instruction data, the first instruction data with 136K\ndata samples to support the fine-tuning, and an evaluation benchmark with 5\ntasks and 9 datasets. We first construct the large-scale multi-task instruction\ndata considering a variety of financial tasks, financial document types, and\nfinancial data modalities. We then propose a financial LLM called FinMA by\nfine-tuning LLaMA with the constructed dataset to be able to follow\ninstructions for various financial tasks. To support the evaluation of\nfinancial LLMs, we propose a standardized benchmark that covers a set of\ncritical financial tasks, including five financial NLP tasks and one financial\nprediction task. With this benchmark, we conduct a detailed analysis of FinMA\nand several existing LLMs, uncovering their strengths and weaknesses in\nhandling critical financial tasks. The model, datasets, benchmark, and\nexperimental results are open-sourced to facilitate future research in\nfinancial AI.",
        "pdf_link": "https://arxiv.org/pdf/2306.05443v1.pdf"
    },
    {
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
        "authors": [
            "Wenxuan Zhang",
            "Sharifah Mahani Aljunied",
            "Chang Gao",
            "Yew Ken Chia",
            "Lidong Bing"
        ],
        "published": "2023-06-08T13:21:29Z",
        "summary": "Despite the existence of various benchmarks for evaluating natural language\nprocessing models, we argue that human exams are a more suitable means of\nevaluating general intelligence for large language models (LLMs), as they\ninherently demand a much wider range of abilities such as language\nunderstanding, domain knowledge, and problem-solving skills. To this end, we\nintroduce M3Exam, a novel benchmark sourced from real and official human exam\nquestions for evaluating LLMs in a multilingual, multimodal, and multilevel\ncontext. M3Exam exhibits three unique characteristics: (1) multilingualism,\nencompassing questions from multiple countries that require strong multilingual\nproficiency and cultural knowledge; (2) multimodality, accounting for the\nmultimodal nature of many exam questions to test the model's multimodal\nunderstanding capability; and (3) multilevel structure, featuring exams from\nthree critical educational periods to comprehensively assess a model's\nproficiency at different levels. In total, M3Exam contains 12,317 questions in\n9 diverse languages with three educational levels, where about 23\\% of the\nquestions require processing images for successful solving. We assess the\nperformance of top-performing LLMs on M3Exam and find that current models,\nincluding GPT-4, still struggle with multilingual text, particularly in\nlow-resource and non-Latin script languages. Multimodal LLMs also perform\npoorly with complex multimodal questions. We believe that M3Exam can be a\nvaluable resource for comprehensively evaluating LLMs by examining their\nmultilingual and multimodal abilities and tracking their development. Data and\nevaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.",
        "pdf_link": "https://arxiv.org/pdf/2306.05179v2.pdf"
    },
    {
        "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures",
        "authors": [
            "Yue Zhen",
            "Sheng Bi",
            "Lu Xing-tong",
            "Pan Wei-qin",
            "Shi Hai-peng",
            "Chen Zi-rui",
            "Fang Yi-shu"
        ],
        "published": "2023-06-08T13:10:00Z",
        "summary": "Traditional robot task planning methods face challenges when dealing with\nhighly unstructured environments and complex tasks. We propose a task planning\nmethod that combines human expertise with an LLM and have designed an LLM\nprompt template, Think_Net_Prompt, with stronger expressive power to represent\nstructured professional knowledge. We further propose a method to progressively\ndecompose tasks and generate a task tree to reduce the planning volume for each\ntask, and we have designed a strategy to decouple robot task planning. By\ndividing different planning entities and separating the task from the actual\nmachine binding process, the task planning process becomes more flexible.\nResearch results show that our method performs well in handling specified code\nformats, understanding the relationship between tasks and subtasks, and\nextracting parameters from text descriptions. However, there are also problems\nsuch as limited complexity of task logic handling, ambiguity in the quantity of\nparts and the precise location of assembly. Improving the precision of task\ndescription and cognitive structure can bring certain improvements.\nhttps://github.com/NOMIzy/Think_Net_Prompt",
        "pdf_link": "https://arxiv.org/pdf/2306.05171v1.pdf"
    },
    {
        "title": "Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming",
        "authors": [
            "Qianou Ma",
            "Tongshuang Wu",
            "Kenneth Koedinger"
        ],
        "published": "2023-06-08T12:22:56Z",
        "summary": "The emergence of large-language models (LLMs) that excel at code generation\nand commercial products such as GitHub's Copilot has sparked interest in\nhuman-AI pair programming (referred to as \"pAIr programming\") where an AI\nsystem collaborates with a human programmer. While traditional pair programming\nbetween humans has been extensively studied, it remains uncertain whether its\nfindings can be applied to human-AI pair programming. We compare human-human\nand human-AI pair programming, exploring their similarities and differences in\ninteraction, measures, benefits, and challenges. We find that the effectiveness\nof both approaches is mixed in the literature (though the measures used for\npAIr programming are not as comprehensive). We summarize moderating factors on\nthe success of human-human pair programming, which provides opportunities for\npAIr programming research. For example, mismatched expertise makes pair\nprogramming less productive, therefore well-designed AI programming assistants\nmay adapt to differences in expertise levels.",
        "pdf_link": "https://arxiv.org/pdf/2306.05153v2.pdf"
    },
    {
        "title": "Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet",
        "authors": [
            "Gonzalo Mart\u00ednez",
            "Lauren Watson",
            "Pedro Reviriego",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Marc Juarez",
            "Rik Sarkar"
        ],
        "published": "2023-06-08T11:14:51Z",
        "summary": "The rapid adoption of generative Artificial Intelligence (AI) tools that can\ngenerate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have\nput the societal impacts of these technologies at the center of public debate.\nThese tools are possible due to the massive amount of data (text and images)\nthat is publicly available through the Internet. At the same time, these\ngenerative AI tools become content creators that are already contributing to\nthe data that is available to train future models. Therefore, future versions\nof generative AI tools will be trained with a mix of human-created and\nAI-generated content, causing a potential feedback loop between generative AI\nand public data repositories. This interaction raises many questions: how will\nfuture versions of generative AI tools behave when trained on a mixture of real\nand AI generated data? Will they evolve and improve with the new data sets or\non the contrary will they degrade? Will evolution introduce biases or reduce\ndiversity in subsequent generations of generative AI tools? What are the\nsocietal implications of the possible degradation of these models? Can we\nmitigate the effects of this feedback loop? In this document, we explore the\neffect of this interaction and report some initial results using simple\ndiffusion models trained with various image datasets. Our results show that the\nquality and diversity of the generated images can degrade over time suggesting\nthat incorporating AI-created data can have undesired effects on future\nversions of generative models.",
        "pdf_link": "https://arxiv.org/pdf/2306.06130v1.pdf"
    },
    {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "authors": [
            "Yidong Wang",
            "Zhuohao Yu",
            "Zhengran Zeng",
            "Linyi Yang",
            "Cunxiang Wang",
            "Hao Chen",
            "Chaoya Jiang",
            "Rui Xie",
            "Jindong Wang",
            "Xing Xie",
            "Wei Ye",
            "Shikun Zhang",
            "Yue Zhang"
        ],
        "published": "2023-06-08T10:41:56Z",
        "summary": "Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.05087v1.pdf"
    },
    {
        "title": "Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models",
        "authors": [
            "Aleksa Bisercic",
            "Mladen Nikolic",
            "Mihaela van der Schaar",
            "Boris Delibasic",
            "Pietro Lio",
            "Andrija Petrovic"
        ],
        "published": "2023-06-08T09:12:28Z",
        "summary": "Tabular data is often hidden in text, particularly in medical diagnostic\nreports. Traditional machine learning (ML) models designed to work with tabular\ndata, cannot effectively process information in such form. On the other hand,\nlarge language models (LLMs) which excel at textual tasks, are probably not the\nbest tool for modeling tabular data. Therefore, we propose a novel, simple, and\neffective methodology for extracting structured tabular data from textual\nmedical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of\nLLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately\ninferring tabular features, even when their names are not explicitly mentioned\nin the text. This is achieved by combining domain-specific reasoning guidelines\nwith a proposed data validation and reasoning correction feedback loop. By\napplying interpretable ML models such as decision trees and logistic regression\nover the extracted and validated data, we obtain end-to-end interpretable\npredictions. We demonstrate that our approach significantly outperforms\nstate-of-the-art text classification models in medical diagnostics. Given its\npredictive performance, simplicity, and interpretability, TEMED-LLM underscores\nthe potential of leveraging LLMs to improve the performance and trustworthiness\nof ML models in medical applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.05052v1.pdf"
    },
    {
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": [
            "Zhiyi Wang",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Yan Xia",
            "Yan Deng",
            "Jonathan Tien"
        ],
        "published": "2023-06-08T07:10:39Z",
        "summary": "This work introduces approaches to assessing phrase breaks in ESL learners'\nspeech using pre-trained language models (PLMs) and large language models\n(LLMs). There are two tasks: overall assessment of phrase break for a speech\nclip and fine-grained assessment of every possible phrase break position. To\nleverage NLP models, speech input is first force-aligned with texts, and then\npre-processed into a token sequence, including words and phrase break\ninformation. To utilize PLMs, we propose a pre-training and fine-tuning\npipeline with the processed tokens. This process includes pre-training with a\nreplaced break token detection module and fine-tuning with text classification\nand sequence labeling. To employ LLMs, we design prompts for ChatGPT. The\nexperiments show that with the PLMs, the dependence on labeled training data\nhas been greatly reduced, and the performance has improved. Meanwhile, we\nverify that ChatGPT, a renowned LLM, has potential for further advancement in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2306.04980v1.pdf"
    },
    {
        "title": "FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs",
        "authors": [
            "Shanshan Han",
            "Baturalp Buyukates",
            "Zijian Hu",
            "Han Jin",
            "Weizhao Jin",
            "Lichao Sun",
            "Xiaoyang Wang",
            "Wenxuan Wu",
            "Chulin Xie",
            "Yuhang Yao",
            "Kai Zhang",
            "Qifan Zhang",
            "Yuhui Zhang",
            "Carlee Joe-Wong",
            "Salman Avestimehr",
            "Chaoyang He"
        ],
        "published": "2023-06-08T06:21:35Z",
        "summary": "This paper introduces FedSecurity, an end-to-end benchmark designed to\nsimulate adversarial attacks and corresponding defense mechanisms in Federated\nLearning (FL). FedSecurity comprises two pivotal components: FedAttacker, which\nfacilitates the simulation of a variety of attacks during FL training, and\nFedDefender, which implements defensive mechanisms to counteract these attacks.\nAs an open-source library, FedSecurity enhances its usability compared to\nfrom-scratch implementations that focus on specific attack/defense scenarios\nbased on the following features: i) It offers extensive customization options\nto accommodate a broad range of machine learning models (e.g., Logistic\nRegression, ResNet, and GAN) and FL optimizers (e.g., FedAVG, FedOPT, and\nFedNOVA); ii) it enables exploring the variability in the effectiveness of\nattacks and defenses across different datasets and models; and iii) it supports\nflexible configuration and customization through a configuration file and some\nprovided APIs. We further demonstrate FedSecurity's utility and adaptability\nthrough federated training of Large Language Models (LLMs), showcasing its\npotential to impact a wide range of complex applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.04959v4.pdf"
    },
    {
        "title": "covLLM: Large Language Models for COVID-19 Biomedical Literature",
        "authors": [
            "Yousuf A. Khan",
            "Clarisse Hokia",
            "Jennifer Xu",
            "Ben Ehlert"
        ],
        "published": "2023-06-08T04:08:32Z",
        "summary": "The COVID-19 pandemic led to 1.1 million deaths in the United States, despite\nthe explosion of coronavirus research. These new findings are slow to translate\nto clinical interventions, leading to poorer patient outcomes and unnecessary\ndeaths. One reason is that clinicians, overwhelmed by patients, struggle to\nkeep pace with the rate of new coronavirus literature. A potential solution is\ndeveloping a tool for evaluating coronavirus literature using large language\nmodels (LLMs) -- neural networks that are deployed for natural language\nprocessing. LLMs can be used to summarize and extract user-specified\ninformation. The greater availability and advancement of LLMs and pre-processed\ncoronavirus literature databases provide the opportunity to assist clinicians\nin evaluating coronavirus literature through a coronavirus literature specific\nLLM (covLLM), a tool that directly takes an inputted research article and a\nuser query to return an answer. Using the COVID-19 Open Research Dataset\n(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of\nhandwritten prompts and synthetic prompts generated using OpenAI, and (2) real\nabstracts, which contains abstract and title pairs. covLLM was trained with\nLLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca\nand synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real\nabstract datasets. These models were evaluated by two human evaluators and\nChatGPT. Results demonstrate that training covLLM on the synCovid and abstract\npairs datasets performs competitively with ChatGPT and outperforms covLLM\ntrained primarily using the Alpaca dataset.",
        "pdf_link": "https://arxiv.org/pdf/2306.04926v1.pdf"
    },
    {
        "title": "Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning",
        "authors": [
            "Jaehyung Kim",
            "Jinwoo Shin",
            "Dongyeop Kang"
        ],
        "published": "2023-06-08T04:04:47Z",
        "summary": "The development of largely human-annotated benchmarks has driven the success\nof deep neural networks in various NLP tasks. To enhance the effectiveness of\nexisting benchmarks, collecting new additional input-output pairs is often too\ncostly and challenging, particularly considering their marginal impact on\nimproving the current model accuracy. Instead, additional or complementary\nannotations on the existing input texts in the benchmarks can be preferable as\nan efficient way to pay the additional human cost. In this paper, we\ninvestigate task-specific preferences between pairs of input texts as a new\nalternative way for such auxiliary data annotation. From 'pair-wise'\ncomparisons with respect to the task, the auxiliary preference learning enables\nthe model to learn an additional informative training signal that cannot be\ncaptured with 'instance-wise' task labels. To this end, we propose a novel\nmulti-task learning framework, called prefer-to-classify (P2C), which can enjoy\nthe cooperative effect of learning both the given classification task and the\nauxiliary preferences. Here, we provide three different ways to collect\npreference signals in practice: (a) implicitly extracting from annotation\nrecords (for free, but often unavailable), (b) collecting explicitly from crowd\nworkers (high paid), or (c) pre-trained large language models such as GPT-3\n(low paid). Given existing classification NLP benchmarks, we demonstrate that\nthe proposed auxiliary preference learning via P2C on them is effective in\nimproving text classifiers. Our codes are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.04925v1.pdf"
    },
    {
        "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
        "authors": [
            "Ganesh Jawahar",
            "Haichuan Yang",
            "Yunyang Xiong",
            "Zechun Liu",
            "Dilin Wang",
            "Fei Sun",
            "Meng Li",
            "Aasish Pappu",
            "Barlas Oguz",
            "Muhammad Abdul-Mageed",
            "Laks V. S. Lakshmanan",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "published": "2023-06-08T00:35:36Z",
        "summary": "Weight-sharing supernet has become a vital component for performance\nestimation in the state-of-the-art (SOTA) neural architecture search (NAS)\nframeworks. Although supernet can directly generate different subnetworks\nwithout retraining, there is no guarantee for the quality of these subnetworks\nbecause of weight sharing. In NLP tasks such as machine translation and\npre-trained language modeling, we observe that given the same model\narchitecture, there is a large performance gap between supernet and training\nfrom scratch. Hence, supernet cannot be directly used and retraining is\nnecessary after finding the optimal architectures.\n  In this work, we propose mixture-of-supernets, a generalized supernet\nformulation where mixture-of-experts (MoE) is adopted to enhance the expressive\npower of the supernet model, with negligible training overhead. In this way,\ndifferent subnetworks do not share the model weights directly, but through an\narchitecture-based routing mechanism. As a result, model weights of different\nsubnetworks are customized towards their specific architectures and the weight\ngeneration is learned by gradient descent. Compared to existing weight-sharing\nsupernet for NLP, our method can minimize the retraining time, greatly\nimproving training efficiency. In addition, the proposed method achieves the\nSOTA performance in NAS for building fast machine translation models, yielding\nbetter latency-BLEU tradeoff compared to HAT, state-of-the-art NAS for MT. We\nalso achieve the SOTA performance in NAS for building memory-efficient\ntask-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various\nmodel sizes.",
        "pdf_link": "https://arxiv.org/pdf/2306.04845v1.pdf"
    },
    {
        "title": "Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",
        "authors": [
            "Shreya Chandrasekhar",
            "Chieh-Yang Huang",
            "Ting-Hao 'Kenneth' Huang"
        ],
        "published": "2023-06-07T22:56:53Z",
        "summary": "The rapid growth of scientific publications, particularly during the COVID-19\npandemic, emphasizes the need for tools to help researchers efficiently\ncomprehend the latest advancements. One essential part of understanding\nscientific literature is research aspect classification, which categorizes\nsentences in abstracts to Background, Purpose, Method, and Finding. In this\nstudy, we investigate the impact of different datasets on model performance for\nthe crowd-annotated CODA-19 research aspect classification task. Specifically,\nwe explore the potential benefits of using the large, automatically curated\nPubMed 200K RCT dataset and evaluate the effectiveness of large language models\n(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that\nusing the PubMed 200K RCT dataset does not improve performance for the CODA-19\ntask. We also observe that while GPT-4 performs well, it does not outperform\nthe SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance\nof a dedicated and task-aligned datasets dataset for the target task. Our code\nis available at https://github.com/Crowd-AI-Lab/CODA-19-exp.",
        "pdf_link": "https://arxiv.org/pdf/2306.04820v1.pdf"
    },
    {
        "title": "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation",
        "authors": [
            "Yinda Chen",
            "Che Liu",
            "Wei Huang",
            "Sibo Cheng",
            "Rossella Arcucci",
            "Zhiwei Xiong"
        ],
        "published": "2023-06-07T22:20:51Z",
        "summary": "Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in\nlearning visual representations from textual descriptions of images without\nannotations. Yet, effective VLP demands large-scale image-text pairs, a\nresource that suffers scarcity in the medical domain. Moreover, conventional\nVLP is limited to 2D images while medical images encompass diverse modalities,\noften in 3D, making the learning process more challenging. To address these\nchallenges, we present Generative Text-Guided 3D Vision-Language Pretraining\nfor Unified Medical Image Segmentation (GTGM), a framework that extends of VLP\nto 3D medical images without relying on paired textual descriptions.\nSpecifically, GTGM utilizes large language models (LLM) to generate\nmedical-style text from 3D medical images. This synthetic text is then used to\nsupervise 3D visual representation learning. Furthermore, a negative-free\ncontrastive learning objective strategy is introduced to cultivate consistent\nvisual representations between augmented 3D medical image patches, which\neffectively mitigates the biases associated with strict positive-negative\nsample pairings. We evaluate GTGM on three imaging modalities - Computed\nTomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM)\nover 13 datasets. GTGM's superior performance across various medical image\nsegmentation tasks underscores its effectiveness and versatility, by enabling\nVLP extension into 3D medical imagery while bypassing the need for paired text.",
        "pdf_link": "https://arxiv.org/pdf/2306.04811v1.pdf"
    },
    {
        "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",
        "authors": [
            "Hejie Cui",
            "Jiaying Lu",
            "Shiyu Wang",
            "Ran Xu",
            "Wenjing Ma",
            "Shaojun Yu",
            "Yue Yu",
            "Xuan Kan",
            "Chen Ling",
            "Tianfan Fu",
            "Liang Zhao",
            "Joyce Ho",
            "Fei Wang",
            "Carl Yang"
        ],
        "published": "2023-06-07T21:51:56Z",
        "summary": "Healthcare knowledge graphs (HKGs) are valuable tools for organizing\nbiomedical concepts and their relationships with interpretable structures. The\nrecent advent of large language models (LLMs) has paved the way for building\nmore comprehensive and accurate HKGs. This, in turn, can improve the\nreliability of generated content and enable better evaluation of LLMs. However,\nthe challenges of HKGs such as regarding data heterogeneity and limited\ncoverage are not fully understood, highlighting the need for detailed reviews.\nThis work provides the first comprehensive review of HKGs. It summarizes the\npipeline and key techniques for HKG construction, as well as the common\nutilization approaches, i.e., model-free and model-based. The existing HKG\nresources are also organized based on the data types they capture and\napplication domains they cover, along with relevant statistical information\n(Resource available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the\napplication level, we delve into the successful integration of HKGs across\nvarious health domains, ranging from fine-grained basic science research to\nhigh-level clinical decision support and public health. Lastly, the paper\nhighlights the opportunities for HKGs in the era of LLMs. This work aims to\nserve as a valuable resource for understanding the potential and opportunities\nof HKG in health research.",
        "pdf_link": "https://arxiv.org/pdf/2306.04802v3.pdf"
    },
    {
        "title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models",
        "authors": [
            "Yew Ken Chia",
            "Pengfei Hong",
            "Lidong Bing",
            "Soujanya Poria"
        ],
        "published": "2023-06-07T20:12:29Z",
        "summary": "Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.",
        "pdf_link": "https://arxiv.org/pdf/2306.04757v3.pdf"
    },
    {
        "title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models",
        "authors": [
            "Naoki Egami",
            "Musashi Hinck",
            "Brandon M. Stewart",
            "Hanying Wei"
        ],
        "published": "2023-06-07T19:49:41Z",
        "summary": "In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. One increasingly common way to annotate\ndocuments cheaply at scale is through large language models (LLMs). However,\nlike other scalable ways of producing annotations, such surrogate labels are\noften imperfect and biased. We present a new algorithm for using imperfect\nannotation surrogates for downstream statistical analyses while guaranteeing\nstatistical properties -- like asymptotic unbiasedness and proper uncertainty\nquantification -- which are fundamental to CSS research. We show that direct\nuse of surrogate labels in downstream statistical analyses leads to substantial\nbias and invalid confidence intervals, even with high surrogate accuracy of\n80-90%. To address this, we build on debiased machine learning to propose the\ndesign-based supervised learning (DSL) estimator. DSL employs a doubly-robust\nprocedure to combine surrogate labels with a smaller number of high-quality,\ngold-standard labels. Our approach guarantees valid inference for downstream\nstatistical analyses, even when surrogates are arbitrarily biased and without\nrequiring stringent assumptions, by controlling the probability of sampling\ndocuments for gold-standard labeling. Both our theoretical analysis and\nexperimental results show that DSL provides valid statistical inference while\nachieving root mean squared errors comparable to existing alternatives that\nfocus only on prediction without inferential guarantees.",
        "pdf_link": "https://arxiv.org/pdf/2306.04746v3.pdf"
    },
    {
        "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems",
        "authors": [
            "Yi Zhang",
            "Jan Deriu",
            "George Katsogiannis-Meimarakis",
            "Catherine Kosten",
            "Georgia Koutrika",
            "Kurt Stockinger"
        ],
        "published": "2023-06-07T19:37:55Z",
        "summary": "Natural Language to SQL systems (NL-to-SQL) have recently shown a significant\nincrease in accuracy for natural language to SQL query translation. This\nimprovement is due to the emergence of transformer-based language models, and\nthe popularity of the Spider benchmark - the de-facto standard for evaluating\nNL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\\%.\nHowever, Spider mainly contains simple databases with few tables, columns, and\nentries, which does not reflect a realistic setting. Moreover, complex\nreal-world databases with domain-specific content have little to no training\ndata available in the form of NL/SQL-pairs leading to poor performance of\nexisting NL-to-SQL systems.\n  In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL\nbenchmark for three real-world, highly domain-specific databases. For this new\nbenchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for\neach domain. To garner more data, we extended the small amount of\nhuman-generated data with synthetic data generated using GPT-3. We show that\nour benchmark is highly challenging, as the top performing systems on Spider\nachieve a very low performance on our benchmark. Thus, the challenge is\nmany-fold: creating NL-to-SQL systems for highly complex domains with a small\namount of hand-made training data augmented with synthetic data. To our\nknowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with\ncomplex real-world scientific databases, containing challenging training and\ntest data carefully validated by domain experts.",
        "pdf_link": "https://arxiv.org/pdf/2306.04743v2.pdf"
    },
    {
        "title": "Soft-prompt Tuning for Large Language Models to Evaluate Bias",
        "authors": [
            "Jacob-Junqi Tian",
            "David Emerson",
            "Sevil Zanjani Miyandoab",
            "Deval Pandya",
            "Laleh Seyyed-Kalantari",
            "Faiza Khan Khattak"
        ],
        "published": "2023-06-07T19:11:25Z",
        "summary": "Prompting large language models has gained immense popularity in recent years\ndue to the advantage of producing good results even without the need for\nlabelled data. However, this requires prompt tuning to get optimal prompts that\nlead to better model performances. In this paper, we explore the use of\nsoft-prompt tuning on sentiment classification task to quantify the biases of\nlarge language models (LLMs) such as Open Pre-trained Transformers (OPT) and\nGalactica language model. Since these models are trained on real-world data\nthat could be prone to bias toward certain groups of populations, it is\nimportant to identify these underlying issues. Using soft-prompts to evaluate\nbias gives us the extra advantage of avoiding the human-bias injection that can\nbe caused by manually designed prompts. We check the model biases on different\nsensitive attributes using the group fairness (bias) and find interesting bias\npatterns. Since LLMs have been used in the industry in various applications, it\nis crucial to identify the biases before deploying these models in practice. We\nopen-source our pipeline and encourage industry researchers to adapt our work\nto their use cases.",
        "pdf_link": "https://arxiv.org/pdf/2306.04735v2.pdf"
    },
    {
        "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
        "authors": [
            "Yikang Shen",
            "Zheyu Zhang",
            "Tianyou Cao",
            "Shawn Tan",
            "Zhenfang Chen",
            "Chuang Gan"
        ],
        "published": "2023-06-07T17:59:57Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable results. However,\nexisting models are expensive to train and deploy, and it is also difficult to\nexpand their knowledge beyond pre-training data without forgetting previous\nknowledge. This paper proposes a new neural network architecture, ModuleFormer,\nthat leverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model, which requires\ndomain-labeled data to learn domain-specific experts, ModuleFormer can induce\nmodularity from uncurated data with its new load balancing and concentration\nlosses. ModuleFormer is a modular architecture that includes two different\ntypes of modules: new stick-breaking attention heads and feedforward experts.\nDifferent modules are sparsely activated conditions on the input token during\ntraining and inference. In our experiment, we found that the modular\narchitecture enables three important abilities for large pre-trained language\nmodels: 1) Efficiency, since ModuleFormer only activates a subset of its\nmodules for each input token, thus it could achieve the same performance as\ndense LLMs with more than two times throughput; 2) Extendability, ModuleFormer\nis more immune to catastrophic forgetting than dense LLMs and can be easily\nextended with new modules to learn new knowledge that is not included in the\ntraining data; 3) Specialisation, finetuning ModuleFormer could specialize a\nsubset of modules to the finetuning task and the task-unrelated modules could\nbe easily pruned for a lightweight deployment.",
        "pdf_link": "https://arxiv.org/pdf/2306.04640v2.pdf"
    },
    {
        "title": "On the Reliability of Watermarks for Large Language Models",
        "authors": [
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Manli Shu",
            "Khalid Saifullah",
            "Kezhi Kong",
            "Kasun Fernando",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Tom Goldstein"
        ],
        "published": "2023-06-07T17:58:48Z",
        "summary": "As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection.\n  We study the robustness of watermarked text after it is re-written by humans,\nparaphrased by a non-watermarked LLM, or mixed into a longer hand-written\ndocument. We find that watermarks remain detectable even after human and\nmachine paraphrasing. While these attacks dilute the strength of the watermark,\nparaphrases are statistically likely to leak n-grams or even longer fragments\nof the original text, resulting in high-confidence detections when enough\ntokens are observed. For example, after strong human paraphrasing the watermark\nis detectable after observing 800 tokens on average, when setting a 1e-5 false\npositive rate. We also consider a range of new detection schemes that are\nsensitive to short spans of watermarked text embedded inside a large document,\nand we compare the robustness of watermarking to other kinds of detectors.",
        "pdf_link": "https://arxiv.org/pdf/2306.04634v3.pdf"
    },
    {
        "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",
        "authors": [
            "Lifan Yuan",
            "Yangyi Chen",
            "Ganqu Cui",
            "Hongcheng Gao",
            "Fangyuan Zou",
            "Xingyi Cheng",
            "Heng Ji",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-06-07T17:47:03Z",
        "summary": "This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.",
        "pdf_link": "https://arxiv.org/pdf/2306.04618v2.pdf"
    },
    {
        "title": "The Two Word Test: A Semantic Benchmark for Large Language Models",
        "authors": [
            "Nicholas Riccardi",
            "Rutvik H. Desai"
        ],
        "published": "2023-06-07T17:22:03Z",
        "summary": "Large Language Models (LLMs) have shown remarkable abilities recently,\nincluding passing advanced professional exams and demanding benchmark tests.\nThis performance has led many to suggest that they are close to achieving\nhumanlike or 'true' understanding of language, and even Artificial General\nIntelligence (AGI). Here, we provide a new open-source benchmark that can\nassess semantic abilities of LLMs using two-word phrases using a task that can\nbe performed relatively easily by humans without advanced training. Combining\nmultiple words into a single concept is a fundamental aspect of human language\nand intelligence. The test requires meaningfulness judgments of 1768 noun-noun\ncombinations that have been rated as meaningful (e.g., baby boy) or not\nmeaningful (e.g., goat sky). by 150 human raters. We provide versions of the\ntask that probe meaningfulness ratings on a 0-4 scale as well as binary\njudgments. We conducted a series of experiments using the TWT on GPT-4,\nGPT-3.5, and Bard, with both versions. Results demonstrated that, compared to\nhumans, all models perform poorly at rating meaningfulness of these phrases.\nGPT-3.5 and Bard are also unable to make binary discriminations between\nsensible and nonsense phrases as making sense. GPT-4 makes a substantial\nimprovement in binary discrimination of combinatorial phrases but is still\nsignificantly worse than human performance. The TWT can be used to understand\nthe limitations and weaknesses of current LLMs, and potentially improve them.\nThe test also reminds us that caution is warranted in attributing 'true\nunderstanding' or AGI to LLMs. TWT is available at:\nhttps://github.com/NickRiccardi/two-word-test",
        "pdf_link": "https://arxiv.org/pdf/2306.04610v1.pdf"
    },
    {
        "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
        "authors": [
            "Himanshu Thakur",
            "Atishay Jain",
            "Praneetha Vaddamanu",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "published": "2023-06-07T16:50:03Z",
        "summary": "Societal biases present in pre-trained large language models are a critical\nissue as these models have been shown to propagate biases in countless\ndownstream applications, rendering them unfair towards specific groups of\npeople. Since large-scale retraining of these models from scratch is both time\nand compute-expensive, a variety of approaches have been previously proposed\nthat de-bias a pre-trained model. While the majority of current\nstate-of-the-art debiasing methods focus on changes to the training regime, in\nthis paper, we propose data intervention strategies as a powerful yet simple\ntechnique to reduce gender bias in pre-trained models. Specifically, we\nempirically show that by fine-tuning a pre-trained model on only 10 de-biased\n(intervened) training examples, the tendency to favor any gender is\nsignificantly reduced. Since our proposed method only needs a few training\nexamples, our few-shot debiasing approach is highly feasible and practical.\nThrough extensive experimentation, we show that our debiasing technique\nperforms better than competitive state-of-the-art baselines with minimal loss\nin language modeling ability.",
        "pdf_link": "https://arxiv.org/pdf/2306.04597v1.pdf"
    },
    {
        "title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",
        "authors": [
            "Sophie Jentzsch",
            "Kristian Kersting"
        ],
        "published": "2023-06-07T16:10:21Z",
        "summary": "Humor is a central aspect of human communication that has not been solved for\nartificial agents so far. Large language models (LLMs) are increasingly able to\ncapture implicit and contextual information. Especially, OpenAI's ChatGPT\nrecently gained immense public attention. The GPT3-based model almost seems to\ncommunicate on a human level and can even tell jokes. Humor is an essential\ncomponent of human communication. But is ChatGPT really funny? We put ChatGPT's\nsense of humor to the test. In a series of exploratory experiments around\njokes, i.e., generation, explanation, and detection, we seek to understand\nChatGPT's capability to grasp and reproduce human humor. Since the model itself\nis not accessible, we applied prompt-based experiments. Our empirical evidence\nindicates that jokes are not hard-coded but mostly also not newly generated by\nthe model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system\naccurately explains valid jokes but also comes up with fictional explanations\nfor invalid jokes. Joke-typical characteristics can mislead ChatGPT in the\nclassification of jokes. ChatGPT has not solved computational humor yet but it\ncan be a big leap toward \"funny\" machines.",
        "pdf_link": "https://arxiv.org/pdf/2306.04563v1.pdf"
    },
    {
        "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
        "authors": [
            "Hannah McLean Babe",
            "Sydney Nguyen",
            "Yangtian Zi",
            "Arjun Guha",
            "Molly Q Feldman",
            "Carolyn Jane Anderson"
        ],
        "published": "2023-06-07T16:03:55Z",
        "summary": "Code LLMs are being rapidly deployed and there is evidence that they can make\nprofessional programmers more productive. Current benchmarks for code\ngeneration measure whether models generate correct programs given an expert\nprompt. In this paper, we present a new benchmark containing multiple prompts\nper problem, written by a specific population of non-expert prompters:\nbeginning programmers. StudentEval contains 1,749 prompts for 48 problems,\nwritten by 80 students who have only completed one semester of Python\nprogramming. Our students wrote these prompts while working interactively with\na Code LLM, and we observed very mixed success rates. We use StudentEval to\nevaluate 5 Code LLMs and find that StudentEval is a better discriminator of\nmodel performance than existing benchmarks. We analyze the prompts and find\nsignificant variation in students' prompting techniques. We also find that\nnondeterministic LLM sampling could mislead students into thinking that their\nprompts are more (or less) effective than they actually are, which has\nimplications for how to teach with Code LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.04556v1.pdf"
    },
    {
        "title": "Long-form analogies generated by chatGPT lack human-like psycholinguistic properties",
        "authors": [
            "S. M. Seals",
            "Valerie L. Shalin"
        ],
        "published": "2023-06-07T15:42:31Z",
        "summary": "Psycholinguistic analyses provide a means of evaluating large language model\n(LLM) output and making systematic comparisons to human-generated text. These\nmethods can be used to characterize the psycholinguistic properties of LLM\noutput and illustrate areas where LLMs fall short in comparison to\nhuman-generated text. In this work, we apply psycholinguistic methods to\nevaluate individual sentences from long-form analogies about biochemical\nconcepts. We compare analogies generated by human subjects enrolled in\nintroductory biochemistry courses to analogies generated by chatGPT. We perform\na supervised classification analysis using 78 features extracted from\nCoh-metrix that analyze text cohesion, language, and readability (Graesser et.\nal., 2004). Results illustrate high performance for classifying\nstudent-generated and chatGPT-generated analogies. To evaluate which features\ncontribute most to model performance, we use a hierarchical clustering\napproach. Results from this analysis illustrate several linguistic differences\nbetween the two sources.",
        "pdf_link": "https://arxiv.org/pdf/2306.04537v1.pdf"
    },
    {
        "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Jiaheng Zhou",
            "Zichen Wang",
            "Hao Chen",
            "Yidong Wang",
            "Linyi Yang",
            "Wei Ye",
            "Yue Zhang",
            "Neil Zhenqiang Gong",
            "Xing Xie"
        ],
        "published": "2023-06-07T15:37:00Z",
        "summary": "The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. The adversarial\nprompts, crafted to mimic plausible user errors like typos or synonyms, aim to\nevaluate how slight deviations can affect LLM outcomes while maintaining\nsemantic integrity. These prompts are then employed in diverse tasks, such as\nsentiment analysis, natural language inference, reading comprehension, machine\ntranslation, and math problem-solving. Our study generates 4788 adversarial\nprompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings\ndemonstrate that contemporary LLMs are not robust to adversarial prompts.\nFurthermore, we present comprehensive analysis to understand the mystery behind\nprompt robustness and its transferability. We then offer insightful robustness\nanalysis and pragmatic recommendations for prompt composition, beneficial to\nboth researchers and everyday users. Code is available at:\nhttps://github.com/microsoft/promptbench.",
        "pdf_link": "https://arxiv.org/pdf/2306.04528v4.pdf"
    },
    {
        "title": "Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs",
        "authors": [
            "Ines Reinig",
            "Katja Markert"
        ],
        "published": "2023-06-07T15:33:07Z",
        "summary": "Compared to English, German word order is freer and therefore poses\nadditional challenges for natural language inference (NLI). We create WOGLI\n(Word Order in German Language Inference), the first adversarial NLI dataset\nfor German word order that has the following properties: (i) each premise has\nan entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ\nonly in word order and necessary morphological changes to mark case and number.\nIn particular, each premise andits two hypotheses contain exactly the same\nlemmata. Our adversarial examples require the model to use morphological\nmarkers in order to recognise or reject entailment. We show that current German\nautoencoding models fine-tuned on translated NLI data can struggle on this\nchallenge set, reflecting the fact that translated NLI datasets will not mirror\nall necessary language phenomena in the target language. We also examine\nperformance after data augmentation as well as on related word order phenomena\nderived from WOGLI. Our datasets are publically available at\nhttps://github.com/ireinig/wogli.",
        "pdf_link": "https://arxiv.org/pdf/2306.04523v1.pdf"
    },
    {
        "title": "Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering",
        "authors": [
            "Zixian Huang",
            "Jiaying Zhou",
            "Gengyang Xiao",
            "Gong Cheng"
        ],
        "published": "2023-06-07T15:20:24Z",
        "summary": "Whereas the recent emergence of large language models (LLMs) like ChatGPT has\nexhibited impressive general performance, it still has a large gap with\nfully-supervised models on specific tasks such as multi-span question\nanswering. Previous researches found that in-context learning is an effective\napproach to exploiting LLM, by using a few task-related labeled data as\ndemonstration examples to construct a few-shot prompt for answering new\nquestions. A popular implementation is to concatenate a few questions and their\ncorrect answers through simple templates, informing LLM of the desired output.\nIn this paper, we propose a novel way of employing labeled data such that it\nalso informs LLM of some undesired output, by extending demonstration examples\nwith feedback about answers predicted by an off-the-shelf model, e.g., correct,\nincorrect, or incomplete. Experiments on three multi-span question answering\ndatasets as well as a keyphrase extraction dataset show that our new prompting\nstrategy consistently improves LLM's in-context learning performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.04508v1.pdf"
    },
    {
        "title": "STEPS: A Benchmark for Order Reasoning in Sequential Tasks",
        "authors": [
            "Weizhi Wang",
            "Hong Wang",
            "Xifeng Yan"
        ],
        "published": "2023-06-07T13:58:55Z",
        "summary": "Various human activities can be abstracted into a sequence of actions in\nnatural text, i.e. cooking, repairing, manufacturing, etc. Such action\nsequences heavily depend on the executing order, while disorder in action\nsequences leads to failure of further task execution by robots or AI agents.\nTherefore, to verify the order reasoning capability of current neural models in\nsequential tasks, we propose a challenging benchmark , named STEPS. STEPS\ninvolves two subtask settings, focusing on determining the rationality of given\nnext step in recipes and selecting the reasonable step from the multi-choice\nquestion, respectively. We describe the data construction and task\nformulations, and benchmark most of significant Large Language Models (LLMs).\nThe experimental results demonstrate 1) The commonsense reasoning of action\norders in sequential tasks are challenging to resolve via zero-shot prompting\nor few-shot in-context learning for LLMs; 2) Prompting method still\nsignificantly lags behind tuning-based method on STEPS.",
        "pdf_link": "https://arxiv.org/pdf/2306.04441v1.pdf"
    },
    {
        "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
        "authors": [
            "Zeyan Liu",
            "Zijun Yao",
            "Fengjun Li",
            "Bo Luo"
        ],
        "published": "2023-06-07T12:33:24Z",
        "summary": "With ChatGPT under the spotlight, utilizing large language models (LLMs) to\nassist academic writing has drawn a significant amount of debate in the\ncommunity. In this paper, we aim to present a comprehensive study of the\ndetectability of ChatGPT-generated content within the academic literature,\nparticularly focusing on the abstracts of scientific papers, to offer holistic\nsupport for the future development of LLM applications and policies in\nacademia. Specifically, we first present GPABench2, a benchmarking dataset of\nover 2.8 million comparative samples of human-written, GPT-written,\nGPT-completed, and GPT-polished abstracts of scientific writing in computer\nscience, physics, and humanities and social sciences. Second, we explore the\nmethodology for detecting ChatGPT content. We start by examining the\nunsatisfactory performance of existing ChatGPT detecting tools and the\nchallenges faced by human evaluators (including more than 240 researchers or\nstudents). We then test the hand-crafted linguistic features models as a\nbaseline and develop a deep neural framework named CheckGPT to better capture\nthe subtle and deep semantic and linguistic patterns in ChatGPT written\nliterature. Last, we conduct comprehensive experiments to validate the proposed\nCheckGPT framework in each benchmarking task over different disciplines. To\nevaluate the detectability of ChatGPT content, we conduct extensive experiments\non the transferability, prompt engineering, and robustness of CheckGPT.",
        "pdf_link": "https://arxiv.org/pdf/2306.05524v2.pdf"
    },
    {
        "title": "Multilingual Clinical NER: Translation or Cross-lingual Transfer?",
        "authors": [
            "Xavier Fontaine",
            "F\u00e9lix Gaschi",
            "Parisa Rastin",
            "Yannick Toussaint"
        ],
        "published": "2023-06-07T12:31:07Z",
        "summary": "Natural language tasks like Named Entity Recognition (NER) in the clinical\ndomain on non-English texts can be very time-consuming and expensive due to the\nlack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent\nthis issue thanks to the ability of multilingual large language models to be\nfine-tuned on a specific task in one language and to provide high accuracy for\nthe same task in another language. However, other methods leveraging\ntranslation models can be used to perform NER without annotated data in the\ntarget language, by either translating the training set or test set. This paper\ncompares cross-lingual transfer with these two alternative methods, to perform\nclinical NER in French and in German without any training data in those\nlanguages. To this end, we release MedNERF a medical NER test set extracted\nfrom French drug prescriptions and annotated with the same guidelines as an\nEnglish dataset. Through extensive experiments on this dataset and on a German\nmedical dataset (Frei and Kramer, 2021), we show that translation-based methods\ncan achieve similar performance to CLT but require more care in their design.\nAnd while they can take advantage of monolingual clinical language models,\nthose do not guarantee better results than large general-purpose multilingual\nmodels, whether with cross-lingual transfer or translation.",
        "pdf_link": "https://arxiv.org/pdf/2306.04384v1.pdf"
    },
    {
        "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks",
        "authors": [
            "Haiyang Xu",
            "Qinghao Ye",
            "Xuan Wu",
            "Ming Yan",
            "Yuan Miao",
            "Jiabo Ye",
            "Guohai Xu",
            "Anwen Hu",
            "Yaya Shi",
            "Guangwei Xu",
            "Chenliang Li",
            "Qi Qian",
            "Maofei Que",
            "Ji Zhang",
            "Xiao Zeng",
            "Fei Huang"
        ],
        "published": "2023-06-07T11:52:36Z",
        "summary": "To promote the development of Vision-Language Pre-training (VLP) and\nmultimodal Large Language Model (LLM) in the Chinese community, we firstly\nrelease the largest public Chinese high-quality video-language dataset named\nYouku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing\nwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUG\ncontains 10 million Chinese video-text pairs filtered from 400 million raw\nvideos across a wide range of 45 diverse categories for large-scale\npre-training. In addition, to facilitate a comprehensive evaluation of\nvideo-language models, we carefully build the largest human-annotated Chinese\nbenchmarks covering three popular video-language tasks of cross-modal\nretrieval, video captioning, and video category classification. Youku-mPLUG can\nenable researchers to conduct more in-depth multimodal research and develop\nbetter applications in the future. Furthermore, we release popular\nvideo-language pre-training models, ALPRO and mPLUG-2, and our proposed\nmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.\nExperiments show that models pre-trained on Youku-mPLUG gain up to 23.1%\nimprovement in video category classification. Besides, mPLUG-video achieves a\nnew state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in\nvideo category classification and 68.9 CIDEr score in video captioning,\nrespectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with\nonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate\nimpressive instruction and video understanding ability. The zero-shot\ninstruction understanding experiment indicates that pretraining with\nYouku-mPLUG can enhance the ability to comprehend overall and detailed visual\nsemantics, recognize scene text, and leverage open-domain knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2306.04362v1.pdf"
    },
    {
        "title": "GPT Self-Supervision for a Better Data Annotator",
        "authors": [
            "Xiaohuan Pei",
            "Yanxi Li",
            "Chang Xu"
        ],
        "published": "2023-06-07T11:33:14Z",
        "summary": "The task of annotating data into concise summaries poses a significant\nchallenge across various domains, frequently requiring the allocation of\nsignificant time and specialized knowledge by human experts. Despite existing\nefforts to use large language models for annotation tasks, significant problems\nsuch as limited applicability to unlabeled data, the absence of self-supervised\nmethods, and the lack of focus on complex structured data still persist. In\nthis work, we propose a GPT self-supervision annotation method, which embodies\na generating-recovering paradigm that leverages the one-shot learning\ncapabilities of the Generative Pretrained Transformer (GPT). The proposed\napproach comprises a one-shot tuning phase followed by a generation phase. In\nthe one-shot tuning phase, we sample a data from the support set as part of the\nprompt for GPT to generate a textual summary, which is then used to recover the\noriginal data. The alignment score between the recovered and original data\nserves as a self-supervision navigator to refine the process. In the generation\nstage, the optimally selected one-shot sample serves as a template in the\nprompt and is applied to generating summaries from challenging datasets. The\nannotation performance is evaluated by tuning several human feedback reward\nnetworks and by calculating alignment scores between original and recovered\ndata at both sentence and structure levels. Our self-supervised annotation\nmethod consistently achieves competitive scores, convincingly demonstrating its\nrobust strength in various data-to-summary annotation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.04349v2.pdf"
    },
    {
        "title": "Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",
        "authors": [
            "Bojana Bodroza",
            "Bojana M. Dinic",
            "Ljubisa Bojic"
        ],
        "published": "2023-06-07T10:14:17Z",
        "summary": "As AI-bots continue to gain popularity due to their human-like traits and the\nintimacy they offer to users, their societal impact inevitably expands. This\nleads to the rising necessity for comprehensive studies to fully understand\nAI-bots and reveal their potential opportunities, drawbacks, and overall\nsocietal impact. With that in mind, this research conducted an extensive\ninvestigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal\nreliability of its personality profile. Psychological questionnaires were\nadministered to the chatbot on two separate occasions, followed by a comparison\nof the responses to human normative data. The findings revealed varying levels\nof agreement in chatbot's responses over time, with some scales displaying\nexcellent agreement while others demonstrated poor agreement. Overall,\nDavinci-003 displayed a socially desirable and pro-social personality profile,\nparticularly in the domain of communion. However, the underlying basis of the\nchatbot's responses-whether driven by conscious self reflection or\npredetermined algorithms-remains uncertain.",
        "pdf_link": "https://arxiv.org/pdf/2306.04308v2.pdf"
    },
    {
        "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
        "authors": [
            "John Joon Young Chung",
            "Ece Kamar",
            "Saleema Amershi"
        ],
        "published": "2023-06-07T04:27:09Z",
        "summary": "Large language models (LLMs) can be used to generate text data for training\nand evaluating other models. However, creating high-quality datasets with LLMs\ncan be challenging. In this work, we explore human-AI partnerships to\nfacilitate high diversity and accuracy in LLM-based text data generation. We\nfirst examine two approaches to diversify text generation: 1) logit\nsuppression, which minimizes the generation of languages that have already been\nfrequently generated, and 2) temperature sampling, which flattens the token\nsampling probability. We found that diversification approaches can increase\ndata diversity but often at the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To address this issue, we examined\ntwo human interventions, 1) label replacement (LR), correcting misaligned\nlabels, and 2) out-of-scope filtering (OOSF), removing instances that are out\nof the user's domain of interest or to which no considered label applies. With\noracle studies, we found that LR increases the absolute accuracy of models\ntrained with diversified datasets by 14.4%. Moreover, we found that some models\ntrained with data generated with LR interventions outperformed LLM-based\nfew-shot classification. In contrast, OOSF was not effective in increasing\nmodel accuracy, implying the need for future work in human-in-the-loop text\ndata generation.",
        "pdf_link": "https://arxiv.org/pdf/2306.04140v1.pdf"
    },
    {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Amir Saffari"
        ],
        "published": "2023-06-07T04:15:21Z",
        "summary": "Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.",
        "pdf_link": "https://arxiv.org/pdf/2306.04136v1.pdf"
    },
    {
        "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
        "authors": [
            "Zhongbin Xie",
            "Thomas Lukasiewicz"
        ],
        "published": "2023-06-06T23:56:18Z",
        "summary": "The increasingly large size of modern pretrained language models not only\nmakes them inherit more human-like biases from the training corpora, but also\nmakes it computationally expensive to mitigate such biases. In this paper, we\ninvestigate recent parameter-efficient methods in combination with\ncounterfactual data augmentation (CDA) for bias mitigation. We conduct\nextensive experiments with prefix tuning, prompt tuning, and adapter tuning on\ndifferent language models and bias types to evaluate their debiasing\nperformance and abilities to preserve the internal knowledge of a pre-trained\nmodel. We find that the parameter-efficient methods (i) are effective in\nmitigating gender bias, where adapter tuning is consistently the most effective\none and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less\neffective when it comes to racial and religious bias, which may be attributed\nto the limitations of CDA, and (iii) can perform similarly to or sometimes\nbetter than full fine-tuning with improved time and memory efficiency, as well\nas maintain the internal knowledge in BERT and GPT-2, evaluated via fact\nretrieval and downstream fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.04067v1.pdf"
    },
    {
        "title": "Certified Deductive Reasoning with Language Models",
        "authors": [
            "Gabriel Poesia",
            "Kanishk Gandhi",
            "Eric Zelikman",
            "Noah D. Goodman"
        ],
        "published": "2023-06-06T21:49:00Z",
        "summary": "Language models often achieve higher accuracy when reasoning step-by-step in\ncomplex tasks. However, even when arriving at a correct final answer, their\nrationales are often logically unsound or inconsistent. This is a major issue\nwhen reliable reasoning traces are needed, such when fine-tuning on\nmodel-generated reasoning for self-improvement. To tackle these issues, we\nintroduce a class of tools for language models called \\emph{guides}, that use\nstate and incremental constraints to guide generation. A guide can be invoked\nby the model to constrain its own generation to a set of valid statements given\nby the tool. In turn, the model's choices can change the guide's state. We show\nhow a general system for logical reasoning can be used as a guide, which we\ncall \\textsc{LogicGuide}. Given a reasoning problem in natural language, a\nmodel can formalize its assumptions for \\textsc{LogicGuide} and guarantee that\nits step-by-step reasoning is sound. In experiments on PrOntoQA, ProofWriter\nand Syllogism Validity datasets, \\textsc{LogicGuide} significantly improves the\nperformance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35\\%),\nwhile drastically reducing \\emph{content effects} -- the interference between\nunwanted prior assumptions and reasoning, which humans and language models\nsuffer from. We then explore bootstrapping GPT-3.5 Turbo and LLaMA using their\nown reasoning traces. We find that LogicGuide is critical: by training only on\ncertified self-generated reasoning, models can self-improve, avoiding learning\nfrom their own hallucinations. Moreover, bootstrapped models enjoy significant\nboosts on ReClor, a challenging real-world reasoning dataset, even when not\nrelying on formalization at inference time.",
        "pdf_link": "https://arxiv.org/pdf/2306.04031v2.pdf"
    },
    {
        "title": "B\u00fcy\u00fck dil modellerinin T\u00fcrk\u00e7e verisetleri ile e\u011fitilmesi ve ince ayarlanmas\u0131",
        "authors": [
            "A. Taha Arslan"
        ],
        "published": "2023-06-06T19:31:08Z",
        "summary": "Large language models have advanced enormously, gained vast attraction and\nare having a phase of intensed research. Some of the developed models and\ntraining datasets have been made open-accessible. Hence these may be further\nfine-tuned with some techniques to obtain specialized models for specific\ntasks. When it comes to Turkish language, open-access models do not provide\nsatisfactory coverage. This is also observed over published datasets. In this\nwork, we propose some ideas to mitigate this issue: creating large Turkish\ndatasets, training LLMs with these and fine-tuning pre-trained models with\nTurkish inputs. We report our findings on Turkish-based trainings with the\nproblems encountered along the way. We conclude with outcomes of these\nexperiments and propose ideas for further works.\n  --\n  B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk\nilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir\nd\\\"onemdedirler. Geli\\c{s}tirilen modeller ve e\\u{g}itimde kullanilan\nverisetlerinden bazilari a\\c{c}ik eri\\c{s}imli olarak sunulmaktadir. B\\\"oylece\nince ayarlama teknikleri uygulayarak \\\"ozelle\\c{s}mi\\c{s} g\\\"orevler i\\c{c}in\n\\c{c}ali\\c{s}abilir modeller elde edilmektedir. T\\\"urk\\c{c}e s\\\"oz konusu\noldu\\u{g}unda bu modellerinin kapsayicili\\u{g}i yeterli d\\\"uzeyde de\\u{g}ildir.\nBu durum, yayimlanan verisetlerinde de g\\\"ozlemlenebilir. Bunu a\\c{s}manin\nyollari T\\\"urk\\c{c}e i\\c{c}erikli b\\\"uy\\\"uk verisetlerinin olu\\c{s}turulmasi,\nb\\\"uy\\\"uk dil modellerinin bunlarla e\\u{g}itilmesi ve \\\"onceden\ne\\u{g}itilmi\\c{s} modellerin T\\\"urk\\c{c}e girdilerle ince ayarlanmalari\nolabilir. Bu \\c{c}ali\\c{s}mada a\\c{c}ik eri\\c{s}imli dil modelleri ve\nverisetleri \\\"uzerinde durulmakta ve T\\\"urk\\c{c}e temelli bazi deneyler,\nkar\\c{s}ila\\c{s}ilan sorunlar ve sonu\\c{c}lar irdelenmektedir.",
        "pdf_link": "https://arxiv.org/pdf/2306.03978v1.pdf"
    },
    {
        "title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction",
        "authors": [
            "Julia White",
            "Arushi Raghuvanshi",
            "Yada Pruksachatkun"
        ],
        "published": "2023-06-06T18:42:08Z",
        "summary": "Task-oriented dialogues often require agents to enact complex, multi-step\nprocedures in order to meet user requests. While large language models have\nfound success automating these dialogues in constrained environments, their\nwidespread deployment is limited by the substantial quantities of task-specific\ndata required for training. The following paper presents a data-efficient\nsolution to constructing dialogue systems, leveraging explicit instructions\nderived from agent guidelines, such as company policies or customer service\nmanuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a\nlarge language model with a knowledge retrieval module that pulls documents\noutlining relevant procedures from a predefined set of policies, given a\nuser-agent interaction. To train this system, we introduce a semi-supervised\npre-training scheme that employs dialogue-document matching and action-oriented\nmasked language modeling with partial parameter freezing. We evaluate the\neffectiveness of our approach on prominent task-oriented dialogue datasets,\nAction-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue\ntasks: action state tracking and workflow discovery. Our results demonstrate\nthat procedural knowledge augmentation improves accuracy predicting in- and\nout-of-distribution actions while preserving high performance in settings with\nlow or sparse data.",
        "pdf_link": "https://arxiv.org/pdf/2306.03959v1.pdf"
    },
    {
        "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
        "authors": [
            "Tamanna Hossain",
            "Sunipa Dev",
            "Sameer Singh"
        ],
        "published": "2023-06-06T18:27:52Z",
        "summary": "Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n  Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging\n34.2% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nperformance for neo-pronouns, but only to 64.7% even with 20 shots. We release\nthe full dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/",
        "pdf_link": "https://arxiv.org/pdf/2306.03950v2.pdf"
    },
    {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "Junbo Zhao",
            "Hang Zhao"
        ],
        "published": "2023-06-06T17:58:24Z",
        "summary": "Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2306.03901v2.pdf"
    },
    {
        "title": "Deductive Verification of Chain-of-Thought Reasoning",
        "authors": [
            "Zhan Ling",
            "Yunhao Fang",
            "Xuanlin Li",
            "Zhiao Huang",
            "Mingu Lee",
            "Roland Memisevic",
            "Hao Su"
        ],
        "published": "2023-06-06T17:18:56Z",
        "summary": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated\nerrors, thereby limiting models' ability to solve complex reasoning tasks.\nInspired by how humans engage in careful and meticulous deductive logical\nreasoning processes to solve tasks, we seek to enable language models to\nperform explicit and rigorous deductive reasoning, and also ensure the\ntrustworthiness of their reasoning process through self-verification. However,\ndirectly verifying the validity of an entire deductive reasoning process is\nchallenging, even with advanced models like ChatGPT. In light of this, we\npropose to decompose a reasoning verification process into a series of\nstep-by-step subprocesses, each only receiving their necessary context and\npremises. To facilitate this procedure, we propose Natural Program, a natural\nlanguage-based deductive reasoning format. Our approach enables models to\ngenerate precise reasoning steps where subsequent steps are more rigorously\ngrounded on prior steps. It also empowers language models to carry out\nreasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly\nenhance the rigor and trustfulness of generated reasoning steps. Along this\nprocess, we also improve the answer correctness on complex reasoning tasks.\nCode will be released at https://github.com/lz1oceani/verify_cot.",
        "pdf_link": "https://arxiv.org/pdf/2306.03872v3.pdf"
    },
    {
        "title": "Can large language models democratize access to dual-use biotechnology?",
        "authors": [
            "Emily H. Soice",
            "Rafael Rocha",
            "Kimberlee Cordova",
            "Michael Specter",
            "Kevin M. Esvelt"
        ],
        "published": "2023-06-06T15:52:05Z",
        "summary": "Large language models (LLMs) such as those embedded in 'chatbots' are\naccelerating and democratizing research by providing comprehensible information\nand expertise from many different fields. However, these models may also confer\neasy access to dual-use technologies capable of inflicting great harm. To\nevaluate this risk, the 'Safeguarding the Future' course at MIT tasked\nnon-scientist students with investigating whether LLM chatbots could be\nprompted to assist non-experts in causing a pandemic. In one hour, the chatbots\nsuggested four potential pandemic pathogens, explained how they can be\ngenerated from synthetic DNA using reverse genetics, supplied the names of DNA\nsynthesis companies unlikely to screen orders, identified detailed protocols\nand how to troubleshoot them, and recommended that anyone lacking the skills to\nperform reverse genetics engage a core facility or contract research\norganization. Collectively, these results suggest that LLMs will make\npandemic-class agents widely accessible as soon as they are credibly\nidentified, even to people with little or no laboratory training. Promising\nnonproliferation measures include pre-release evaluations of LLMs by third\nparties, curating training datasets to remove harmful concepts, and verifiably\nscreening all DNA generated by synthesis providers or used by contract research\norganizations and robotic cloud laboratories to engineer organisms or viruses.",
        "pdf_link": "https://arxiv.org/pdf/2306.03809v1.pdf"
    },
    {
        "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
        "authors": [
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Tianlong Chen",
            "Zhangyang Wang"
        ],
        "published": "2023-06-06T15:49:09Z",
        "summary": "Large pre-trained transformers are show-stealer in modern-day deep learning,\nand it becomes crucial to comprehend the parsimonious patterns that exist\nwithin them as they grow in scale. With exploding parameter counts, Lottery\nTicket Hypothesis (LTH) and its variants, have lost their pragmatism in\nsparsifying them due to high computation and memory bottleneck of repetitive\ntrain-prune-retrain routine of iterative magnitude pruning (IMP) which worsens\nwith increasing model size. This paper comprehensively studies induced sparse\npatterns across multiple large pre-trained vision and language transformers. We\npropose the existence of -- essential sparsity defined with a sharp dropping\npoint beyond which the performance declines much faster w.r.t the rise of\nsparsity level, when we directly remove weights with the smallest magnitudes in\none-shot without re-training. We also find essential sparsity to hold valid for\nN:M sparsity patterns as well as on modern-scale large language models\n(Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt\nsparsification during the pre-training of BERT, i.e., BERT suddenly becomes\nheavily sparse in pre-training after certain iterations. Moreover, our\nobservations also indicate a counter-intuitive finding that BERT trained with a\nlarger amount of pre-training data tends to have a better ability to condense\nknowledge in comparatively relatively fewer parameters. Lastly, we investigate\nthe effect of the pre-training loss on essential sparsity and discover that\nself-supervised learning (SSL) objectives trigger stronger emergent\nsparsification properties than supervised learning (SL). Our codes are\navailable at \\url{https://github.com/VITA-Group/essential_sparsity}.",
        "pdf_link": "https://arxiv.org/pdf/2306.03805v2.pdf"
    },
    {
        "title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models",
        "authors": [
            "Fobo Shi",
            "Peijun Qing",
            "Dong Yang",
            "Nan Wang",
            "Youbo Lei",
            "Haonan Lu",
            "Xiaodong Lin",
            "Duantengchuan Li"
        ],
        "published": "2023-06-06T15:43:16Z",
        "summary": "Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt \"Let's think step by step\",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}",
        "pdf_link": "https://arxiv.org/pdf/2306.03799v2.pdf"
    },
    {
        "title": "Towards End-to-end Speech-to-text Summarization",
        "authors": [
            "Raul Monteiro",
            "Diogo Pernes"
        ],
        "published": "2023-06-06T15:22:16Z",
        "summary": "Speech-to-text (S2T) summarization is a time-saving technique for filtering\nand keeping up with the broadcast news uploaded online on a daily basis. The\nrise of large language models from deep learning with impressive text\ngeneration capabilities has placed the research focus on summarization systems\nthat produce paraphrased compact versions of the document content, also known\nas abstractive summaries. End-to-end (E2E) modelling of S2T abstractive\nsummarization is a promising approach that offers the possibility of generating\nrich latent representations that leverage non-verbal and acoustic information,\nas opposed to the use of only linguistic information from automatically\ngenerated transcripts in cascade systems. However, the few literature on E2E\nmodelling of this task fails on exploring different domains, namely broadcast\nnews, which is challenging domain where large and diversified volumes of data\nare presented to the user every day. We model S2T summarization both with a\ncascade and an E2E system for a corpus of broadcast news in French. Our novel\nE2E model leverages external data by resorting to transfer learning from a\npre-trained T2T summarizer. Experiments show that both our cascade and E2E\nabstractive summarizers are stronger than an extractive baseline. However, the\nperformance of the E2E model still lies behind the cascade one, which is object\nof an extensive analysis that includes future directions to close that gap.",
        "pdf_link": "https://arxiv.org/pdf/2306.05432v1.pdf"
    },
    {
        "title": "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach",
        "authors": [
            "Bin Hu",
            "Chenyang Zhao",
            "Pu Zhang",
            "Zihao Zhou",
            "Yuanhang Yang",
            "Zenglin Xu",
            "Bin Liu"
        ],
        "published": "2023-06-06T11:49:09Z",
        "summary": "Large language models (LLMs) encode a vast amount of world knowledge acquired\nfrom massive text datasets. Recent studies have demonstrated that LLMs can\nassist an embodied agent in solving complex sequential decision making tasks by\nproviding high-level instructions. However, interactions with LLMs can be\ntime-consuming. In many practical scenarios, they require a significant amount\nof storage space that can only be deployed on remote cloud server nodes.\nAdditionally, using commercial LLMs can be costly since they may charge based\non usage frequency. In this paper, we explore how to enable intelligent\ncost-effective interactions between the agent and an LLM. We find that this\nproblem can be naturally formulated by a Markov decision process (MDP), and\npropose When2Ask, a reinforcement learning based approach that learns when it\nis necessary to query LLMs for high-level instructions to accomplish a target\ntask. Experiments on MiniGrid and Habitat environments that entail planning\nsub-goals demonstrate that When2Ask learns to solve target tasks with only a\nfew necessary interactions with an LLM, and significantly reduces interaction\ncosts in testing environments compared with baseline methods. Experiment\nresults also suggest that by learning a mediator model to interact with the\nLLM, the agent's performance becomes more robust against partial observability\nof the environment. Our code is available at\nhttps://github.com/ZJLAB-AMMI/LLM4RL.",
        "pdf_link": "https://arxiv.org/pdf/2306.03604v6.pdf"
    },
    {
        "title": "Language acquisition: do children and language models follow similar learning stages?",
        "authors": [
            "Linnea Evanson",
            "Yair Lakretz",
            "Jean-R\u00e9mi King"
        ],
        "published": "2023-06-06T11:08:20Z",
        "summary": "During language acquisition, children follow a typical sequence of learning\nstages, whereby they first learn to categorize phonemes before they develop\ntheir lexicon and eventually master increasingly complex syntactic structures.\nHowever, the computational principles that lead to this learning trajectory\nremain largely unknown. To investigate this, we here compare the learning\ntrajectories of deep language models to those of children. Specifically, we\ntest whether, during its training, GPT-2 exhibits stages of language\nacquisition comparable to those observed in children aged between 18 months and\n6 years. For this, we train 48 GPT-2 models from scratch and evaluate their\nsyntactic and semantic abilities at each training step, using 96 probes curated\nfrom the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these\nevaluations with the behavior of 54 children during language production. Our\nanalyses reveal three main findings. First, similarly to children, the language\nmodels tend to learn linguistic skills in a systematic order. Second, this\nlearning scheme is parallel: the language tasks that are learned last improve\nfrom the very first training steps. Third, some - but not all - learning stages\nare shared between children and these language models. Overall, these results\nshed new light on the principles of language acquisition, and highlight\nimportant divergences in how humans and modern algorithms learn to process\nnatural language.",
        "pdf_link": "https://arxiv.org/pdf/2306.03586v1.pdf"
    },
    {
        "title": "An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge",
        "authors": [
            "Tan John Chong Min"
        ],
        "published": "2023-06-06T10:08:12Z",
        "summary": "We utilise the power of Large Language Models (LLMs), in particular GPT4, to\nbe prompt engineered into performing an arbitrary task. Here, we give the model\nsome human priors via text, along with some typical procedures for solving the\nARC tasks, and ask it to generate the i) broad description of the input-output\nrelation, ii) detailed steps of the input-output mapping, iii) use the detailed\nsteps to perform manipulation on the test input and derive the test output. The\ncurrent GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those\nwith small grids of 8x8 and below). With tweaks to the prompt to make it more\nspecific for the use case, it can solve more. We posit that when scaled to a\nmulti-agent system with usage of past memory and equipped with an image\ninterpretation tool via Visual Question Answering, we may actually be able to\nsolve the majority of the ARC challenge",
        "pdf_link": "https://arxiv.org/pdf/2306.03553v1.pdf"
    },
    {
        "title": "Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models",
        "authors": [
            "Jose Berengueres",
            "Marybeth Sandell"
        ],
        "published": "2023-06-06T08:47:42Z",
        "summary": "This paper explores how AI-owners can develop safeguards for AI-generated\ncontent by drawing from established codes of conduct and ethical standards in\nother content-creation industries. It delves into the current state of ethical\nawareness on Large Language Models (LLMs). By dissecting the mechanism of\ncontent generation by LLMs, four key areas (upstream/downstream and at user\nprompt/answer), where safeguards could be effectively applied, are identified.\nA comparative analysis of these four areas follows and includes an evaluation\nof the existing ethical safeguards in terms of cost, effectiveness, and\nalignment with established industry practices. The paper's key argument is that\nexisting IT-related ethical codes, while adequate for traditional IT\nengineering, are inadequate for the challenges posed by LLM-based content\ngeneration. Drawing from established practices within journalism, we propose\npotential standards for businesses involved in distributing and selling\nLLM-generated content. Finally, potential conflicts of interest between dataset\ncuration at upstream and ethical benchmarking downstream are highlighted to\nunderscore the need for a broader evaluation beyond mere output. This study\nprompts a nuanced conversation around ethical implications in this rapidly\nevolving field of content generation.",
        "pdf_link": "https://arxiv.org/pdf/2306.03503v2.pdf"
    },
    {
        "title": "Natural Language Commanding via Program Synthesis",
        "authors": [
            "Apurva Gandhi",
            "Thong Q. Nguyen",
            "Huitian Jiao",
            "Robert Steen",
            "Ameya Bhatawdekar"
        ],
        "published": "2023-06-06T07:28:49Z",
        "summary": "We present Semantic Interpreter, a natural language-friendly AI system for\nproductivity software such as Microsoft Office that leverages large language\nmodels (LLMs) to execute user intent across application features. While LLMs\nare excellent at understanding user intent expressed as natural language, they\nare not sufficient for fulfilling application-specific user intent that\nrequires more than text-to-text transformations. We therefore introduce the\nOffice Domain Specific Language (ODSL), a concise, high-level language\nspecialized for performing actions in and interacting with entities in Office\napplications. Semantic Interpreter leverages an Analysis-Retrieval prompt\nconstruction method with LLMs for program synthesis, translating natural\nlanguage user utterances to ODSL programs that can be transpiled to application\nAPIs and then executed. We focus our discussion primarily on a research\nexploration for Microsoft PowerPoint.",
        "pdf_link": "https://arxiv.org/pdf/2306.03460v1.pdf"
    },
    {
        "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
        "authors": [
            "Tuan Dinh",
            "Jinman Zhao",
            "Samson Tan",
            "Renato Negrinho",
            "Leonard Lausen",
            "Sheng Zha",
            "George Karypis"
        ],
        "published": "2023-06-06T06:35:27Z",
        "summary": "Large language models of code (Code-LLMs) have recently brought tremendous\nadvances to code completion, a fundamental feature of programming assistance\nand code intelligence. However, most existing works ignore the possible\npresence of bugs in the code context for generation, which are inevitable in\nsoftware development. Therefore, we introduce and study the buggy-code\ncompletion problem, inspired by the realistic scenario of real-time code\nsuggestion where the code context contains potential bugs -- anti-patterns that\ncan become bugs in the completed program. To systematically study the task, we\nintroduce two datasets: one with synthetic bugs derived from semantics-altering\noperator changes (buggy-HumanEval) and one with realistic bugs derived from\nuser submissions to coding problems (buggy-FixEval). We find that the presence\nof potential bugs significantly degrades the generation performance of the\nhigh-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO\non test cases of buggy-HumanEval drop more than 50% given a single potential\nbug in the context. Finally, we investigate several post-hoc methods for\nmitigating the adverse effect of potential bugs and find that there remains a\nsignificant gap in post-mitigation performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.03438v2.pdf"
    },
    {
        "title": "On the Role of Attention in Prompt-tuning",
        "authors": [
            "Samet Oymak",
            "Ankit Singh Rawat",
            "Mahdi Soltanolkotabi",
            "Christos Thrampoulidis"
        ],
        "published": "2023-06-06T06:23:38Z",
        "summary": "Prompt-tuning is an emerging strategy to adapt large language models (LLM) to\ndownstream tasks by learning a (soft-)prompt parameter from data. Despite its\nsuccess in LLMs, there is limited theoretical understanding of the power of\nprompt-tuning and the role of the attention mechanism in prompting. In this\nwork, we explore prompt-tuning for one-layer attention architectures and study\ncontextual mixture-models where each input token belongs to a context-relevant\nor -irrelevant set. We isolate the role of prompt-tuning through a\nself-contained prompt-attention model. Our contributions are as follows: (1) We\nshow that softmax-prompt-attention is provably more expressive than\nsoftmax-self-attention and linear-prompt-attention under our contextual data\nmodel. (2) We analyze the initial trajectory of gradient descent and show that\nit learns the prompt and prediction head with near-optimal sample complexity\nand demonstrate how prompt can provably attend to sparse context-relevant\ntokens. (3) Assuming a known prompt but an unknown prediction head, we\ncharacterize the exact finite sample performance of prompt-attention which\nreveals the fundamental performance limits and the precise benefit of the\ncontext information. We also provide experiments that verify our theoretical\ninsights on real datasets and demonstrate how prompt-tuning enables the model\nto attend to context-relevant information.",
        "pdf_link": "https://arxiv.org/pdf/2306.03435v1.pdf"
    },
    {
        "title": "Prompting Large Language Models to Reformulate Queries for Moment Localization",
        "authors": [
            "Wenfeng Yan",
            "Shaoxiang Chen",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "published": "2023-06-06T05:48:09Z",
        "summary": "The task of moment localization is to localize a temporal moment in an\nuntrimmed video for a given natural language query. Since untrimmed video\ncontains highly redundant contents, the quality of the query is crucial for\naccurately localizing moments, i.e., the query should provide precise\ninformation about the target moment so that the localization model can\nunderstand what to look for in the videos. However, the natural language\nqueries in current datasets may not be easy to understand for existing models.\nFor example, the Ego4D dataset uses question sentences as the query to describe\nrelatively complex moments. While being natural and straightforward for humans,\nunderstanding such question sentences are challenging for mainstream moment\nlocalization models like 2D-TAN. Inspired by the recent success of large\nlanguage models, especially their ability of understanding and generating\ncomplex natural language contents, in this extended abstract, we make early\nattempts at reformulating the moment queries into a set of instructions using\nlarge language models and making them more friendly to the localization models.",
        "pdf_link": "https://arxiv.org/pdf/2306.03422v1.pdf"
    },
    {
        "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
        "authors": [
            "Kenneth Li",
            "Oam Patel",
            "Fernanda Vi\u00e9gas",
            "Hanspeter Pfister",
            "Martin Wattenberg"
        ],
        "published": "2023-06-06T01:26:53Z",
        "summary": "We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the \"truthfulness\" of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.",
        "pdf_link": "https://arxiv.org/pdf/2306.03341v5.pdf"
    },
    {
        "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents",
        "authors": [
            "Yashar Talebirad",
            "Amirhossein Nadiri"
        ],
        "published": "2023-06-05T23:55:37Z",
        "summary": "In this paper, we present a novel framework for enhancing the capabilities of\nlarge language models (LLMs) by leveraging the power of multi-agent systems.\nOur framework introduces a collaborative environment where multiple intelligent\nagent components, each with distinctive attributes and roles, work together to\nhandle complex tasks more efficiently and effectively. We demonstrate the\npracticality and versatility of our framework through case studies in\nartificial general intelligence (AGI), specifically focusing on the Auto-GPT\nand BabyAGI models. We also examine the \"Gorilla\" model, which integrates\nexternal APIs into the LLM. Our framework addresses limitations and challenges\nsuch as looping issues, security risks, scalability, system evaluation, and\nethical considerations. By modeling various domains such as courtroom\nsimulations and software development scenarios, we showcase the potential\napplications and benefits of our proposed multi-agent system. Our framework\nprovides an avenue for advancing the capabilities and performance of LLMs\nthrough collaboration and knowledge exchange among intelligent agents.",
        "pdf_link": "https://arxiv.org/pdf/2306.03314v1.pdf"
    },
    {
        "title": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation",
        "authors": [
            "Sanjeev Kumar Karn",
            "Rikhiya Ghosh",
            "Kusuma P",
            "Oladimeji Farri"
        ],
        "published": "2023-06-05T21:33:04Z",
        "summary": "Instruction-tuned generative Large language models (LLMs) like ChatGPT and\nBloomz possess excellent generalization abilities, but they face limitations in\nunderstanding radiology reports, particularly in the task of generating the\nIMPRESSIONS section from the FINDINGS section. They tend to generate either\nverbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to\nmedical text data during training. We present a system which leverages\nlarge-scale medical text data for domain-adaptive pre-training of\ninstruction-tuned LLMs to enhance its medical knowledge and performance on\nspecific medical tasks. We show that this system performs better in a zero-shot\nsetting than a number of pretrain-and-finetune adaptation methods on the\nIMPRESSIONS generation task, and ranks 1st among participating systems in Task\n1B: Radiology Report Summarization at the BioNLP 2023 workshop.",
        "pdf_link": "https://arxiv.org/pdf/2306.03264v1.pdf"
    },
    {
        "title": "Early Weight Averaging meets High Learning Rates for LLM Pre-training",
        "authors": [
            "Sunny Sanyal",
            "Atula Neerkaje",
            "Jean Kaddour",
            "Abhishek Kumar",
            "Sujay Sanghavi"
        ],
        "published": "2023-06-05T20:51:44Z",
        "summary": "Training Large Language Models (LLMs) incurs significant cost; hence, any\nstrategy that accelerates model convergence is helpful. In this paper, we\ninvestigate the ability of a simple idea checkpoint averaging along the\ntrajectory of a training run to improve both convergence and generalization\nquite early on during training. Here we show that models trained with high\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\nthese gains are amplified when checkpoints are sampled with considerable\nspacing in training steps. Our training recipe outperforms conventional\ntraining and popular checkpoint averaging baselines such as exponential moving\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\non the PILE-deduped dataset containing 207B tokens.",
        "pdf_link": "https://arxiv.org/pdf/2306.03241v2.pdf"
    },
    {
        "title": "NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks",
        "authors": [
            "Jean-Michel Attendu",
            "Jean-Philippe Corbeil"
        ],
        "published": "2023-06-05T19:30:41Z",
        "summary": "Finetuning large language models inflates the costs of NLU applications and\nremains the bottleneck of development cycles. Recent works in computer vision\nuse data pruning to reduce training time. Pruned data selection with static\nmethods is based on a score calculated for each training example prior to\nfinetuning, which involves important computational overhead. Moreover, the\nscore may not necessarily be representative of sample importance throughout the\nentire training duration. We propose to address these issues with a refined\nversion of dynamic data pruning, a curriculum which periodically scores and\ndiscards unimportant examples during finetuning. Our method leverages an EL2N\nmetric that we extend to the joint intent and slot classification task, and an\ninitial finetuning phase on the full train set. Our results on the GLUE\nbenchmark and four joint NLU datasets show a better time-accuracy trade-off\ncompared to static methods. Our method preserves full accuracy while training\non 50% of the data points and reduces computational times by up to 41%. If we\ntolerate instead a minor drop of accuracy of 1%, we can prune 80% of the\ntraining examples for a reduction in finetuning time reaching 66%.",
        "pdf_link": "https://arxiv.org/pdf/2306.03208v1.pdf"
    },
    {
        "title": "ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs",
        "authors": [
            "Levente Juh\u00e1sz",
            "Peter Mooney",
            "Hartwig H. Hochmair",
            "Boyuan Guan"
        ],
        "published": "2023-06-05T19:26:21Z",
        "summary": "This paper explores the concept of leveraging generative AI as a mapping\nassistant for enhancing the efficiency of collaborative mapping. We present\nresults of an experiment that combines multiple sources of volunteered\ngeographic information (VGI) and large language models (LLMs). Three analysts\ndescribed the content of crowdsourced Mapillary street-level photographs taken\nalong roads in a small test area in Miami, Florida. GPT-3.5-turbo was\ninstructed to suggest the most appropriate tagging for each road in\nOpenStreetMap (OSM). The study also explores the utilization of BLIP-2, a\nstate-of-the-art multimodal pre-training method as an artificial analyst of\nstreet-level photographs in addition to human analysts. Results demonstrate two\nways to effectively increase the accuracy of mapping suggestions without\nmodifying the underlying AI models: by (1) providing a more detailed\ndescription of source photographs, and (2) combining prompt engineering with\nadditional context (e.g. location and objects detected along a road). The first\napproach increases the suggestion accuracy by up to 29%, and the second one by\nup to 20%.",
        "pdf_link": "https://arxiv.org/pdf/2306.03204v2.pdf"
    },
    {
        "title": "A Static Evaluation of Code Completion by Large Language Models",
        "authors": [
            "Hantian Ding",
            "Varun Kumar",
            "Yuchen Tian",
            "Zijian Wang",
            "Rob Kwiatkowski",
            "Xiaopeng Li",
            "Murali Krishna Ramanathan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Sudipta Sengupta",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023-06-05T19:23:34Z",
        "summary": "Large language models trained on code have shown great potential to increase\nproductivity of software developers. Several execution-based benchmarks have\nbeen proposed to evaluate functional correctness of model-generated code on\nsimple programming problems. Nevertheless, it is expensive to perform the same\nevaluation on complex real-world projects considering the execution cost. On\nthe contrary, static analysis tools such as linters, which can detect errors\nwithout running the program, haven't been well explored for evaluating code\ngeneration models. In this work, we propose a static evaluation framework to\nquantify static errors in Python code completions, by leveraging Abstract\nSyntax Trees. Compared with execution-based evaluation, our method is not only\nmore efficient, but also applicable to code in the wild. For experiments, we\ncollect code context from open source repos to generate one million function\nbodies using public models. Our static analysis reveals that Undefined Name and\nUnused Variable are the most common errors among others made by language\nmodels. Through extensive studies, we also show the impact of sampling\ntemperature, model size, and context on static errors in code completions.",
        "pdf_link": "https://arxiv.org/pdf/2306.03203v1.pdf"
    },
    {
        "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
        "authors": [
            "Lichang Chen",
            "Jiuhai Chen",
            "Tom Goldstein",
            "Heng Huang",
            "Tianyi Zhou"
        ],
        "published": "2023-06-05T17:55:22Z",
        "summary": "Large language models~(LLMs) are instruction followers, but it can be\nchallenging to find the best instruction for different situations, especially\nfor black-box LLMs on which backpropagation is forbidden. Instead of directly\noptimizing the discrete instruction, we optimize a low-dimensional soft prompt\napplied to an open-source LLM to generate the instruction for the black-box\nLLM. On each iteration of the proposed method, which we call InstructZero, a\nsoft prompt is converted into an instruction using the open-source LLM, which\nis then submitted to the black-box LLM for zero-shot evaluation, and the\nperformance is sent to Bayesian optimization to produce new soft prompts\nimproving the zero-shot performance. We evaluate InstructZero on different\ncombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our\nresults show that InstructZero outperforms SOTA auto-instruction methods across\na variety of downstream tasks. Our code and data are publicly available at\nhttps://github.com/Lichang-Chen/InstructZero.",
        "pdf_link": "https://arxiv.org/pdf/2306.03082v2.pdf"
    },
    {
        "title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
        "authors": [
            "Alexander K. Lew",
            "Tan Zhi-Xuan",
            "Gabriel Grand",
            "Vikash K. Mansinghka"
        ],
        "published": "2023-06-05T17:55:05Z",
        "summary": "Even after fine-tuning and reinforcement learning, large language models\n(LLMs) can be difficult, if not impossible, to control reliably with prompts\nalone. We propose a new inference-time approach to enforcing syntactic and\nsemantic constraints on the outputs of LLMs, called sequential Monte Carlo\n(SMC) steering. The key idea is to specify language generation tasks as\nposterior inference problems in a class of discrete probabilistic sequence\nmodels, and replace standard decoding with sequential Monte Carlo inference.\nFor a computational cost similar to that of beam search, SMC can steer LLMs to\nsolve diverse tasks, including infilling, generation under syntactic\nconstraints, and prompt intersection. To facilitate experimentation with SMC\nsteering, we present a probabilistic programming library, LLaMPPL\n(https://github.com/probcomp/hfppl), for concisely specifying new generation\ntasks as language model probabilistic programs, and automating steering of\nLLaMA-family Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2306.03081v2.pdf"
    },
    {
        "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
        "authors": [
            "Tim Dettmers",
            "Ruslan Svirschevski",
            "Vage Egiazarian",
            "Denis Kuznedelev",
            "Elias Frantar",
            "Saleh Ashkboos",
            "Alexander Borzunov",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "published": "2023-06-05T17:53:28Z",
        "summary": "Recent advances in large language model (LLM) pretraining have led to\nhigh-quality LLMs with impressive abilities. By compressing such LLMs via\nquantization to 3-4 bits per parameter, they can fit into memory-limited\ndevices such as laptops and mobile phones, enabling personalized use. However,\nquantization down to 3-4 bits per parameter usually leads to moderate-to-high\naccuracy losses, especially for smaller models in the 1-10B parameter range,\nwhich are well-suited for edge deployments. To address this accuracy issue, we\nintroduce the Sparse-Quantized Representation (SpQR), a new compressed format\nand quantization technique which enables for the first time near-lossless\ncompression of LLMs across model scales, while reaching similar compression\nlevels to previous methods. SpQR works by identifying and isolating outlier\nweights, which cause particularly-large quantization errors, and storing them\nin higher precision, while compressing all other weights to 3-4 bits, and\nachieves relative accuracy losses of less than 1% in perplexity for\nhighly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B\nparameter LLM on a single 24 GB consumer GPU without any performance\ndegradation at 15% speedup thus making powerful LLMs available to consumer\nwithout any downsides. SpQR comes with efficient algorithms for both encoding\nweights into its format, as well as decoding them efficiently at runtime.\nSpecifically, we provide an efficient GPU inference algorithm for SpQR which\nyields faster inference than 16-bit baselines at similar accuracy, while\nenabling memory compression gains of more than 4x.",
        "pdf_link": "https://arxiv.org/pdf/2306.03078v1.pdf"
    },
    {
        "title": "Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion",
        "authors": [
            "Ryo Sekizawa",
            "Hitomi Yanaka"
        ],
        "published": "2023-06-05T17:27:48Z",
        "summary": "Using Japanese honorifics is challenging because it requires not only\nknowledge of the grammatical rules but also contextual information, such as\nsocial relationships. It remains unclear whether pre-trained large language\nmodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyze\nthis, we introduce an honorific conversion task that considers social\nrelationships among people mentioned in a conversation. We construct a Japanese\nhonorifics dataset from problem templates of various sentence structures to\ninvestigate the syntactic generalization capacity of GPT-3, one of the leading\nLLMs, on this task under two settings: fine-tuning and prompt learning. Our\nresults showed that the fine-tuned GPT-3 performed better in a context-aware\nhonorific conversion task than the prompt-based one. The fine-tuned model\ndemonstrated overall syntactic generalizability towards compound honorific\nsentences, except when tested with the data involving direct speech.",
        "pdf_link": "https://arxiv.org/pdf/2306.03055v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset",
        "authors": [
            "Junling Liu",
            "Peilin Zhou",
            "Yining Hua",
            "Dading Chong",
            "Zhongyu Tian",
            "Andrew Liu",
            "Helin Wang",
            "Chenyu You",
            "Zhenhua Guo",
            "Lei Zhu",
            "Michael Lingzhi Li"
        ],
        "published": "2023-06-05T16:48:41Z",
        "summary": "Recent advancements in large language models (LLMs) have transformed the\nfield of question answering (QA). However, evaluating LLMs in the medical field\nis challenging due to the lack of standardized and comprehensive datasets. To\naddress this gap, we introduce CMExam, sourced from the Chinese National\nMedical Licensing Examination. CMExam consists of 60K+ multiple-choice\nquestions for standardized and objective evaluations, as well as solution\nexplanations for model reasoning evaluation in an open-ended manner. For\nin-depth analyses of LLMs, we invited medical professionals to label five\nadditional question-wise annotations, including disease groups, clinical\ndepartments, medical disciplines, areas of competency, and question difficulty\nlevels. Alongside the dataset, we further conducted thorough experiments with\nrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4\nhad the best accuracy of 61.6% and a weighted F1 score of 0.617. These results\nhighlight a great disparity when compared to human accuracy, which stood at\n71.6%. For explanation tasks, while LLMs could generate relevant reasoning and\ndemonstrate improved performance after finetuning, they fall short of a desired\nstandard, indicating ample room for improvement. To the best of our knowledge,\nCMExam is the first Chinese medical exam dataset to provide comprehensive\nmedical annotations. The experiments and findings of LLM evaluation also\nprovide valuable insights into the challenges and potential solutions in\ndeveloping Chinese medical QA systems and LLM evaluation pipelines. The dataset\nand relevant code are available at https://github.com/williamliujl/CMExam.",
        "pdf_link": "https://arxiv.org/pdf/2306.03030v3.pdf"
    },
    {
        "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
        "authors": [
            "Shuyang Jiang",
            "Yuhao Wang",
            "Yu Wang"
        ],
        "published": "2023-06-05T14:12:46Z",
        "summary": "Large language models (LLMs) have already revolutionized code generation,\nafter being pretrained on publicly available code data. However, while various\nmethods have been proposed to augment LLMs with retrieved knowledge and enhance\nthe quality of code generation, the performance of these retrieval-based\nmethods is limited by the strength of the retrievers used. In addition, while\nLLMs show great emergent ability, they still struggle to produce the correct\ncode in one turn. To address these challenges, we propose a novel two-step\npipeline, called \\autoknow, that leverages LLMs as both knowledge providers and\nself-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains\nthe knowledge from input prompts and generates intermediate code based on the\ngenerated knowledge. After that, \\autoknow~asks LLM to act as an expert\nprogrammer to perform debugging for the generated code. This is achieved by\nreceiving the error message from the interpreter, without requiring special\ntest cases for correctness verification. We evaluate \\autoknow~on three code\ngeneration datasets, including DS-1000 for data science code, HumanEval for\nsoftware engineering code, and TransCoder for C++-to-Python translation. Our\nempirical experiments show that \\autoknow~outperforms strong baselines by a\nsignificant margin on all datasets. We also conduct exhaustive analytical\nexperiments to validate the effectiveness of the two stages of \\autoknow, and\nfind that both are superior to other prompting-based methods. Further\nscalability analysis demonstrates that \\autoknow~can be adapted to other more\nadvanced models, such as GPT-4, and bring consistent efficacy improvement.",
        "pdf_link": "https://arxiv.org/pdf/2306.02907v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs",
        "authors": [
            "Alejandro Pe\u00f1a",
            "Aythami Morales",
            "Julian Fierrez",
            "Ignacio Serna",
            "Javier Ortega-Garcia",
            "I\u00f1igo Puente",
            "Jorge Cordova",
            "Gonzalo Cordova"
        ],
        "published": "2023-06-05T13:35:01Z",
        "summary": "The analysis of public affairs documents is crucial for citizens as it\npromotes transparency, accountability, and informed decision-making. It allows\ncitizens to understand government policies, participate in public discourse,\nand hold representatives accountable. This is crucial, and sometimes a matter\nof life or death, for companies whose operation depend on certain regulations.\nLarge Language Models (LLMs) have the potential to greatly enhance the analysis\nof public affairs documents by effectively processing and understanding the\ncomplex language used in such documents. In this work, we analyze the\nperformance of LLMs in classifying public affairs documents. As a natural\nmulti-label task, the classification of these documents presents important\nchallenges. In this work, we use a regex-powered tool to collect a database of\npublic affairs documents with more than 33K samples and 22.5M tokens. Our\nexperiments assess the performance of 4 different Spanish LLMs to classify up\nto 30 different topics in the data in different configurations. The results\nshows that LLMs can be of great use to process domain-specific documents, such\nas those in the domain of public affairs.",
        "pdf_link": "https://arxiv.org/pdf/2306.02864v2.pdf"
    },
    {
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "authors": [
            "Hang Zhang",
            "Xin Li",
            "Lidong Bing"
        ],
        "published": "2023-06-05T13:17:27Z",
        "summary": "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.",
        "pdf_link": "https://arxiv.org/pdf/2306.02858v4.pdf"
    },
    {
        "title": "Cheap-fake Detection with LLM using Prompt Engineering",
        "authors": [
            "Guangyang Wu",
            "Weijie Wu",
            "Xiaohong Liu",
            "Kele Xu",
            "Tianjiao Wan",
            "Wenyi Wang"
        ],
        "published": "2023-06-05T11:01:00Z",
        "summary": "The misuse of real photographs with conflicting image captions in news items\nis an example of the out-of-context (OOC) misuse of media. In order to detect\nOOC media, individuals must determine the accuracy of the statement and\nevaluate whether the triplet (~\\textit{i.e.}, the image and two captions)\nrelates to the same event. This paper presents a novel learnable approach for\ndetecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The\nproposed method is based on the COSMOS structure, which assesses the coherence\nbetween an image and captions, as well as between two captions. We enhance the\nbaseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a\nfeature extractor. Specifically, we propose an innovative approach to feature\nextraction utilizing prompt engineering to develop a robust and reliable\nfeature extractor with GPT3.5 model. The proposed method captures the\ncorrelation between two captions and effectively integrates this module into\nthe COSMOS baseline model, which allows for a deeper understanding of the\nrelationship between captions. By incorporating this module, we demonstrate the\npotential for significant improvements in cheap-fakes detection performance.\nThe proposed methodology holds promising implications for various applications\nsuch as natural language processing, image captioning, and text-to-image\nsynthesis. Docker for submission is available at\nhttps://hub.docker.com/repository/docker/mulns/ acmmmcheapfakes.",
        "pdf_link": "https://arxiv.org/pdf/2306.02776v1.pdf"
    },
    {
        "title": "Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia",
        "authors": [
            "Benjamin Kereopa-Yorke"
        ],
        "published": "2023-06-05T06:01:00Z",
        "summary": "The escalating digitalisation of our lives and enterprises has led to a\nparallel growth in the complexity and frequency of cyber-attacks. Small and\nmedium-sized enterprises (SMEs), particularly in Australia, are experiencing\nincreased vulnerability to cyber threats, posing a significant challenge to the\nnation's cyber security landscape. Embracing transformative technologies such\nas Artificial Intelligence (AI), Machine Learning (ML) and Large Language\nModels (LLMs) can potentially strengthen cyber security policies for Australian\nSMEs. However, their practical application, advantages, and limitations remain\nunderexplored, with prior research mainly focusing on large corporations. This\nstudy aims to address this gap by providing a comprehensive understanding of\nthe potential role of LLMs in enhancing cyber security policies for Australian\nSMEs. Employing a mixed-methods study design, this research includes a\nliterature review, qualitative analysis of SME case studies, and a quantitative\nassessment of LLM performance metrics in cyber security applications. The\nfindings highlight the promising potential of LLMs across various performance\ncriteria, including relevance, accuracy, and applicability, though gaps remain\nin areas such as completeness and clarity. The study underlines the importance\nof integrating human expertise with LLM technology and refining model\ndevelopment to address these limitations. By proposing a robust conceptual\nframework guiding the effective adoption of LLMs, this research aims to\ncontribute to a safer and more resilient cyber environment for Australian SMEs,\nenabling sustainable growth and competitiveness in the digital era.",
        "pdf_link": "https://arxiv.org/pdf/2306.02612v1.pdf"
    },
    {
        "title": "User Behavior Simulation with Large Language Model based Agents",
        "authors": [
            "Lei Wang",
            "Jingsen Zhang",
            "Hao Yang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Zeyu Zhang",
            "Xu Chen",
            "Yankai Lin",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Jun Xu",
            "Zhicheng Dou",
            "Jun Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-06-05T02:58:35Z",
        "summary": "Simulating high quality user behavior data has always been a fundamental\nproblem in human-centered applications, where the major difficulty originates\nfrom the intricate mechanism of human decision process. Recently, substantial\nevidences have suggested that by learning huge amounts of web knowledge, large\nlanguage models (LLMs) can achieve human-like intelligence. We believe these\nmodels can provide significant opportunities to more believable user behavior\nsimulation. To inspire such direction, we propose an LLM-based agent framework\nand design a sandbox environment to simulate real user behaviors. Based on\nextensive experiments, we find that the simulated behaviors of our method are\nvery close to the ones of real humans. Concerning potential applications, we\nsimulate and study two social phenomenons including (1) information cocoons and\n(2) user conformity behaviors. This research provides novel simulation\nparadigms for human-centered applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.02552v3.pdf"
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "authors": [
            "Alaleh Hamidi",
            "Kirk Roberts"
        ],
        "published": "2023-06-05T02:52:54Z",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "pdf_link": "https://arxiv.org/pdf/2306.02549v1.pdf"
    },
    {
        "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
        "authors": [
            "Beichen Zhang",
            "Kun Zhou",
            "Xilin Wei",
            "Wayne Xin Zhao",
            "Jing Sha",
            "Shijin Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-06-04T17:02:59Z",
        "summary": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in\nrecent work as effective practices for improving large language models~(LLMs)\nto perform step-by-step reasoning on complex math-related tasks. However, most\nexisting math reasoning datasets may be not able to fully evaluate and analyze\nthe ability of LLMs in manipulating tools and performing reasoning, as they may\nonly require very few invocations of tools or miss annotations for evaluating\nintermediate reasoning steps. To address the issue, we construct \\textbf{CARP},\na new Chinese dataset consisting of 4,886 computation-intensive algebra\nproblems with formulated annotations on intermediate steps. In CARP, we test\nfour LLMs with CoT prompting, and find that they are all prone to make mistakes\nat the early steps of the solution, leading to wrong answers. Based on this\nfinding, we propose a new approach that can deliberate the reasoning steps with\ntool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a\nstep-by-step solution based on retrieved exemplars, then iterate two\ndeliberation procedures that check and refine the intermediate steps of the\ngenerated solution, from the perspectives of tool manipulation and natural\nlanguage reasoning, until obtaining converged solutions or reaching the maximum\nturn. Experimental results on CARP and six other datasets show that the\nproposed DELI mostly outperforms competitive baselines, and can further boost\nthe performance of existing CoT methods. Our data and code are available in\n\\url{https://github.com/RUCAIBox/CARP}.",
        "pdf_link": "https://arxiv.org/pdf/2306.02408v1.pdf"
    },
    {
        "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
        "authors": [
            "Wangchunshu Zhou",
            "Ronan Le Bras",
            "Yejin Choi"
        ],
        "published": "2023-06-04T15:44:51Z",
        "summary": "Despite serving as the foundation models for a wide range of NLP benchmarks,\npre-trained language models have shown limited capabilities of acquiring\nimplicit commonsense knowledge from self-supervision alone, compared to\nlearning linguistic and factual knowledge that appear more explicitly in the\nsurface patterns in text. In this work, we introduce commonsense knowledge\ntransfer, a framework to transfer the commonsense knowledge stored in a neural\ncommonsense knowledge model to a general-purpose pre-trained language model. It\nfirst exploits general texts to form queries for extracting commonsense\nknowledge from the neural commonsense knowledge model and then refines the\nlanguage model with two self-supervised objectives: commonsense mask infilling\nand commonsense relation prediction, which align human language with the\nunderlying commonsense knowledge. Empirical results show that our approach\nconsistently improves the model's performance on downstream tasks that require\ncommonsense reasoning. Moreover, we find that the improvement is more\nsignificant in the few-shot setting. This suggests that our approach helps\nlanguage models better transfer to downstream tasks without extensive\nsupervision by injecting commonsense knowledge into their parameters.",
        "pdf_link": "https://arxiv.org/pdf/2306.02388v1.pdf"
    },
    {
        "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
        "authors": [
            "Celine Wald",
            "Lukas Pfahler"
        ],
        "published": "2023-06-04T08:09:26Z",
        "summary": "Progress in natural language generation research has been shaped by the\never-growing size of language models. While large language models pre-trained\non web data can generate human-sounding text, they also reproduce social biases\nand contribute to the propagation of harmful stereotypes. This work utilises\nthe flaw of bias in language models to explore the biases of six different\nonline communities. In order to get an insight into the communities'\nviewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias\nof the resulting models is evaluated by prompting the models with different\ndemographics and comparing the sentiment and toxicity values of these\ngenerations. Together, these methods reveal that bias differs in type and\nintensity for the various models. This work not only affirms how easily bias is\nabsorbed from training data but also presents a scalable method to identify and\ncompare the bias of different datasets or communities. Additionally, the\nexamples generated for this work demonstrate the limitations of using automated\nsentiment and toxicity classifiers in bias research.",
        "pdf_link": "https://arxiv.org/pdf/2306.02294v1.pdf"
    },
    {
        "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models",
        "authors": [
            "Changhun Lee",
            "Jungyu Jin",
            "Taesu Kim",
            "Hyungjun Kim",
            "Eunhyeok Park"
        ],
        "published": "2023-06-04T06:33:13Z",
        "summary": "Large language models (LLMs) with hundreds of billions of parameters require\npowerful server-grade GPUs for inference, limiting their practical deployment.\nTo address this challenge, we introduce the outlier-aware weight quantization\n(OWQ) method, which aims to minimize LLM's footprint through low-precision\nrepresentation. OWQ prioritizes a small subset of structured weights sensitive\nto quantization, storing them in high-precision, while applying highly tuned\nquantization to the remaining dense weights. This sensitivity-aware\nmixed-precision scheme reduces the quantization error notably, and extensive\nexperiments demonstrate that 3.1-bit models using OWQ perform comparably to\n4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a\nparameter-efficient fine-tuning for task-specific adaptation, called weak\ncolumn tuning (WCT), enabling accurate task-specific LLM adaptation with\nminimal memory overhead in the optimized format. OWQ represents a notable\nadvancement in the flexibility, efficiency, and practicality of LLM\noptimization literature. The source code is available at\nhttps://github.com/xvyaward/owq",
        "pdf_link": "https://arxiv.org/pdf/2306.02272v4.pdf"
    },
    {
        "title": "Probing Physical Reasoning with Counter-Commonsense Context",
        "authors": [
            "Kazushi Kondo",
            "Saku Sugawara",
            "Akiko Aizawa"
        ],
        "published": "2023-06-04T04:24:43Z",
        "summary": "In this study, we create a CConS (Counter-commonsense Contextual Size\ncomparison) dataset to investigate how physical commonsense affects the\ncontextualized size comparison task; the proposed dataset consists of both\ncontexts that fit physical commonsense and those that do not. This dataset\ntests the ability of language models to predict the size relationship between\nobjects under various contexts generated from our curated noun list and\ntemplates. We measure the ability of several masked language models and\ngenerative models. The results show that while large language models can use\nprepositions such as ``in'' and ``into'' in the provided context to infer size\nrelationships, they fail to use verbs and thus make incorrect judgments led by\ntheir prior physical commonsense.",
        "pdf_link": "https://arxiv.org/pdf/2306.02258v1.pdf"
    },
    {
        "title": "Large Language Model Augmented Narrative Driven Recommendations",
        "authors": [
            "Sheshera Mysore",
            "Andrew McCallum",
            "Hamed Zamani"
        ],
        "published": "2023-06-04T03:46:45Z",
        "summary": "Narrative-driven recommendation (NDR) presents an information access problem\nwhere users solicit recommendations with verbose descriptions of their\npreferences and context, for example, travelers soliciting recommendations for\npoints of interest while describing their likes/dislikes and travel\ncircumstances. These requests are increasingly important with the rise of\nnatural language-based conversational interfaces for search and recommendation\nsystems. However, NDR lacks abundant training data for models, and current\nplatforms commonly do not support these requests. Fortunately, classical\nuser-item interaction datasets contain rich textual data, e.g., reviews, which\noften describe user preferences and context - this may be used to bootstrap\ntraining for NDR models. In this work, we explore using large language models\n(LLMs) for data augmentation to train NDR models. We use LLMs for authoring\nsynthetic narrative queries from user-item interactions with few-shot prompting\nand train retrieval models for NDR on synthetic queries and user-item\ninteraction data. Our experiments demonstrate that this is an effective\nstrategy for training small-parameter retrieval models that outperform other\nretrieval and LLM baselines for narrative-driven recommendation.",
        "pdf_link": "https://arxiv.org/pdf/2306.02250v2.pdf"
    },
    {
        "title": "Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model",
        "authors": [
            "Lingfeng Shen",
            "Haiyun Jiang",
            "Lemao Liu",
            "Shuming Shi"
        ],
        "published": "2023-06-04T03:26:43Z",
        "summary": "Sentence embedding is one of the most fundamental tasks in Natural Language\nProcessing and plays an important role in various tasks. The recent\nbreakthrough in sentence embedding is achieved by pre-trained language models\n(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point\nestimate does not naturally express uncertainty in a taskagnostic way. This\npaper thereby proposes an efficient framework on probabilistic sentence\nembedding (Sen2Pro) from PLMs, and it represents a sentence as a probability\ndensity distribution in an embedding space to reflect both model uncertainty\nand data uncertainty (i.e., many-to-one nature) in the sentence representation.\nThe proposed framework performs in a plug-and-play way without retraining PLMs\nanymore, and it is easy to implement and generally applied on top of any PLM.\nThe superiority of Sen2Pro over Sen2Vec has been theoretically verified and\npractically illustrated on different NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.02247v1.pdf"
    },
    {
        "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions",
        "authors": [
            "Hui Yang",
            "Sifu Yue",
            "Yunzhong He"
        ],
        "published": "2023-06-04T01:07:20Z",
        "summary": "Auto-GPT is an autonomous agent that leverages recent advancements in\nadapting Large Language Models (LLMs) for decision-making tasks. While there\nhas been a growing interest in Auto-GPT stypled agents, questions remain\nregarding the effectiveness and flexibility of Auto-GPT in solving real-world\ndecision-making tasks. Its limited capability for real-world engagement and the\nabsence of benchmarks contribute to these uncertainties. In this paper, we\npresent a comprehensive benchmark study of Auto-GPT styled agents in\ndecision-making tasks that simulate real-world scenarios. Our aim is to gain\ndeeper insights into this problem and understand the adaptability of GPT-based\nagents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5,\nClaude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we\nintroduce the Additional Opinions algorithm, an easy and effective method that\nincorporates supervised/imitation-based learners into the Auto-GPT scheme. This\napproach enables lightweight supervised learning without requiring fine-tuning\nof the foundational LLMs. We demonstrate through careful baseline comparisons\nand ablation studies that the Additional Opinions algorithm significantly\nenhances performance in online decision-making benchmarks, including WebShop\nand ALFWorld.",
        "pdf_link": "https://arxiv.org/pdf/2306.02224v1.pdf"
    },
    {
        "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts",
        "authors": [
            "Haibin Wu",
            "Kai-Wei Chang",
            "Yuan-Kuei Wu",
            "Hung-yi Lee"
        ],
        "published": "2023-06-03T22:35:27Z",
        "summary": "Large language models (LLMs) have gained considerable attention for\nArtificial Intelligence Generated Content (AIGC), particularly with the\nemergence of ChatGPT. However, the direct adaptation of continuous speech to\nLLMs that process discrete tokens remains an unsolved challenge, hindering the\napplication of LLMs for speech generation. The advanced speech LMs are in the\ncorner, as that speech signals encapsulate a wealth of information, including\nspeaker and emotion, beyond textual data alone. Prompt tuning has demonstrated\nnotable gains in parameter efficiency and competitive performance on some\nspeech classification tasks. However, the extent to which prompts can\neffectively elicit generation tasks from speech LMs remains an open question.\nIn this paper, we present pioneering research that explores the application of\nprompt tuning to stimulate speech LMs for various generation tasks, within a\nunified framework called SpeechGen, with around 10M trainable parameters. The\nproposed unified framework holds great promise for efficiency and\neffectiveness, particularly with the imminent arrival of advanced speech LMs,\nwhich will significantly enhance the capabilities of the framework. The code\nand demos of SpeechGen will be available on the project website:\n\\url{https://ga642381.github.io/SpeechPrompt/speechgen}",
        "pdf_link": "https://arxiv.org/pdf/2306.02207v3.pdf"
    },
    {
        "title": "Towards Coding Social Science Datasets with Language Models",
        "authors": [
            "Christopher Michael Rytting",
            "Taylor Sorensen",
            "Lisa Argyle",
            "Ethan Busby",
            "Nancy Fulda",
            "Joshua Gubler",
            "David Wingate"
        ],
        "published": "2023-06-03T19:11:34Z",
        "summary": "Researchers often rely on humans to code (label, annotate, etc.) large sets\nof texts. This kind of human coding forms an important part of social science\nresearch, yet the coding process is both resource intensive and highly variable\nfrom application to application. In some cases, efforts to automate this\nprocess have achieved human-level accuracies, but to achieve this, these\nattempts frequently rely on thousands of hand-labeled training examples, which\nmakes them inapplicable to small-scale research studies and costly for large\nones. Recent advances in a specific kind of artificial intelligence tool -\nlanguage models (LMs) - provide a solution to this problem. Work in computer\nscience makes it clear that LMs are able to classify text, without the cost (in\nfinancial terms and human effort) of alternative methods. To demonstrate the\npossibilities of LMs in this area of political science, we use GPT-3, one of\nthe most advanced LMs, as a synthetic coder and compare it to human coders. We\nfind that GPT-3 can match the performance of typical human coders and offers\nbenefits over other machine learning methods of coding text. We find this\nacross a variety of domains using very different coding procedures. This\nprovides exciting evidence that language models can serve as a critical advance\nin the coding of open-ended texts in a variety of applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.02177v1.pdf"
    },
    {
        "title": "Unsupervised Human Activity Recognition through Two-stage Prompting with ChatGPT",
        "authors": [
            "Qingxin Xia",
            "Takuya Maekawa",
            "Takahiro Hara"
        ],
        "published": "2023-06-03T15:41:59Z",
        "summary": "Wearable sensor devices, which offer the advantage of recording daily objects\nused by a person while performing an activity, enable the feasibility of\nunsupervised Human Activity Recognition (HAR). Unfortunately, previous\nunsupervised approaches using the usage sequence of objects usually require a\nproper description of activities manually prepared by humans. Instead, we\nleverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT.\nBecause the sequence of objects robustly characterizes the activity identity,\nit is possible that ChatGPT already learned the association between activities\nand objects from existing contexts. However, previous prompt engineering for\nChatGPT exhibits limited generalization ability when dealing with a list of\nwords (i.e., sequence of objects) due to the similar weighting assigned to each\nword in the list. In this study, we propose a two-stage prompt engineering,\nwhich first guides ChatGPT to generate activity descriptions associated with\nobjects while emphasizing important objects for distinguishing similar\nactivities; then outputs activity classes and explanations for enhancing the\ncontexts that are helpful for HAR. To the best of our knowledge, this is the\nfirst study that utilizes ChatGPT to recognize activities using objects in an\nunsupervised manner. We conducted our approach on three datasets and\ndemonstrated the state-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.02140v1.pdf"
    },
    {
        "title": "Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions",
        "authors": [
            "Jana Strakov\u00e1",
            "Eva Fu\u010d\u00edkov\u00e1",
            "Jan Haji\u010d",
            "Zde\u0148ka Ure\u0161ov\u00e1"
        ],
        "published": "2023-06-03T14:57:47Z",
        "summary": "In this project, we have investigated the use of advanced machine learning\nmethods, specifically fine-tuned large language models, for pre-annotating data\nfor a lexical extension task, namely adding descriptive words (verbs) to an\nexisting (but incomplete, as of yet) ontology of event types. Several research\nquestions have been focused on, from the investigation of a possible heuristics\nto provide at least hints to annotators which verbs to include and which are\noutside the current version of the ontology, to the possible use of the\nautomatic scores to help the annotators to be more efficient in finding a\nthreshold for identifying verbs that cannot be assigned to any existing class\nand therefore they are to be used as seeds for a new class. We have also\ncarefully examined the correlation of the automatic scores with the human\nannotation. While the correlation turned out to be strong, its influence on the\nannotation proper is modest due to its near linearity, even though the mere\nfact of such pre-annotation leads to relatively short annotation times.",
        "pdf_link": "https://arxiv.org/pdf/2306.02130v2.pdf"
    },
    {
        "title": "MultiLegalPile: A 689GB Multilingual Legal Corpus",
        "authors": [
            "Joel Niklaus",
            "Veton Matoshi",
            "Matthias St\u00fcrmer",
            "Ilias Chalkidis",
            "Daniel E. Ho"
        ],
        "published": "2023-06-03T10:10:38Z",
        "summary": "Large, high-quality datasets are crucial for training Large Language Models\n(LLMs). However, so far, there are few datasets available for specialized\ncritical domains such as law and the available ones are often only for the\nEnglish language. We curate and release MultiLegalPile, a 689GB corpus in 24\nlanguages from 17 jurisdictions. The MultiLegalPile corpus, which includes\ndiverse legal data sources with varying licenses, allows for pretraining NLP\nmodels under fair use, with more permissive licenses for the Eurlex Resources\nand Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer\nmultilingually, and 24 monolingual models on each of the language-specific\nsubsets and evaluate them on LEXTREME. Additionally, we evaluate the English\nand multilingual models on LexGLUE. Our multilingual models set a new SotA on\nLEXTREME and our English models on LexGLUE. We release the dataset, the trained\nmodels, and all of the code under the most open possible licenses.",
        "pdf_link": "https://arxiv.org/pdf/2306.02069v2.pdf"
    },
    {
        "title": "LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas",
        "authors": [
            "Kensen Shi",
            "Hanjun Dai",
            "Wen-Ding Li",
            "Kevin Ellis",
            "Charles Sutton"
        ],
        "published": "2023-06-03T08:24:53Z",
        "summary": "Search is an important technique in program synthesis that allows for\nadaptive strategies such as focusing on particular search directions based on\nexecution results. Several prior works have demonstrated that neural models are\neffective at guiding program synthesis searches. However, a common drawback of\nthose approaches is the inability to handle iterative loops, higher-order\nfunctions, or lambda functions, thus limiting prior neural searches from\nsynthesizing longer and more general programs. We address this gap by designing\na search algorithm called LambdaBeam that can construct arbitrary lambda\nfunctions that compose operations within a given DSL. We create semantic vector\nrepresentations of the execution behavior of the lambda functions and train a\nneural policy network to choose which lambdas to construct during search, and\npass them as arguments to higher-order functions to perform looping\ncomputations. Our experiments show that LambdaBeam outperforms neural,\nsymbolic, and LLM-based techniques in an integer list manipulation domain.",
        "pdf_link": "https://arxiv.org/pdf/2306.02049v2.pdf"
    },
    {
        "title": "On Optimal Caching and Model Multiplexing for Large Model Inference",
        "authors": [
            "Banghua Zhu",
            "Ying Sheng",
            "Lianmin Zheng",
            "Clark Barrett",
            "Michael I. Jordan",
            "Jiantao Jiao"
        ],
        "published": "2023-06-03T05:01:51Z",
        "summary": "Large Language Models (LLMs) and other large foundation models have achieved\nnoteworthy success, but their size exacerbates existing resource consumption\nand latency challenges. In particular, the large-scale deployment of these\nmodels is hindered by the significant resource requirements during inference.\nIn this paper, we study two approaches for mitigating these challenges:\nemploying a cache to store previous queries and learning a model multiplexer to\nchoose from an ensemble of models for query processing.\n  Theoretically, we provide an optimal algorithm for jointly optimizing both\napproaches to reduce the inference cost in both offline and online tabular\nsettings. By combining a caching algorithm, namely Greedy Dual Size with\nFrequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we\nachieve optimal rates in both offline and online settings. Empirically,\nsimulations show that the combination of our caching and model multiplexing\nalgorithms greatly improves over the baselines, with up to $50\\times$\nimprovement over the baseline when the ratio between the maximum cost and\nminimum cost is $100$. Experiments on real datasets show a $4.3\\times$\nimprovement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a\n$1.8\\times$ improvement in latency when the ratio for average latency is\n$1.85$.",
        "pdf_link": "https://arxiv.org/pdf/2306.02003v2.pdf"
    },
    {
        "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
        "authors": [
            "Guangzhi Sun",
            "Xianrui Zheng",
            "Chao Zhang",
            "Philip C. Woodland"
        ],
        "published": "2023-06-02T22:56:01Z",
        "summary": "End-to-end automatic speech recognition (ASR) and large language models, such\nas Whisper and GPT-2, have recently been scaled to use vast amounts of training\ndata. Despite the large amount of training data, infrequent content words that\noccur in a particular task may still exhibit poor ASR performance, with\ncontextual biasing a possible remedy. This paper investigates the effectiveness\nof neural contextual biasing for Whisper combined with GPT-2. Specifically,\nthis paper proposes integrating an adapted tree-constrained pointer generator\n(TCPGen) component for Whisper and a dedicated training scheme to dynamically\nadjust the final output without modifying any Whisper model parameters.\nExperiments across three datasets show a considerable reduction in errors on\nbiasing words with a biasing list of 1000 words. Contextual biasing was more\neffective when applied to domain-specific data and can boost the performance of\nWhisper and GPT-2 without losing their generality.",
        "pdf_link": "https://arxiv.org/pdf/2306.01942v1.pdf"
    },
    {
        "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap",
        "authors": [
            "Q. Vera Liao",
            "Jennifer Wortman Vaughan"
        ],
        "published": "2023-06-02T22:51:26Z",
        "summary": "The rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2306.01941v2.pdf"
    },
    {
        "title": "Revisiting the Role of Language Priors in Vision-Language Models",
        "authors": [
            "Zhiqiu Lin",
            "Xinyue Chen",
            "Deepak Pathak",
            "Pengchuan Zhang",
            "Deva Ramanan"
        ],
        "published": "2023-06-02T19:19:43Z",
        "summary": "Vision-language models (VLMs) are impactful in part because they can be\napplied to a variety of visual understanding tasks in a zero-shot fashion,\nwithout any fine-tuning. We study $\\textit{generative VLMs}$ that are trained\nfor next-word generation given an image. We explore their zero-shot performance\non the illustrative task of image-text retrieval across 8 popular\nvision-language benchmarks. Our first observation is that they can be\nrepurposed for discriminative tasks (such as image-text retrieval) by simply\ncomputing the match score of generating a particular text string given an\nimage. We call this probabilistic score the $\\textit{Visual Generative\nPre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces\nnear-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on\nothers. We analyze this behavior through a probabilistic lens, pointing out\nthat some benchmarks inadvertently capture unnatural language distributions by\ncreating adversarial but unlikely text captions. In fact, we demonstrate that\neven a \"blind\" language model that ignores any image evidence can sometimes\noutperform all prior art, reminiscent of similar challenges faced by the\nvisual-question answering (VQA) community many years ago. We derive a\nprobabilistic post-processing scheme that controls for the amount of linguistic\nbias in generative VLMs at test time without having to retrain or fine-tune the\nmodel. We show that the VisualGPTScore, when appropriately debiased, is a\nstrong zero-shot baseline for vision-language understanding, oftentimes\nproducing state-of-the-art accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2306.01879v3.pdf"
    },
    {
        "title": "Knowledge of cultural moral norms in large language models",
        "authors": [
            "Aida Ramezani",
            "Yang Xu"
        ],
        "published": "2023-06-02T18:23:35Z",
        "summary": "Moral norms vary across cultures. A recent line of work suggests that English\nlarge language models contain human-like moral biases, but these studies\ntypically do not examine moral variation in a diverse cultural setting. We\ninvestigate the extent to which monolingual English language models contain\nknowledge about moral norms in different countries. We consider two levels of\nanalysis: 1) whether language models capture fine-grained moral variation\nacross countries over a variety of topics such as ``homosexuality'' and\n``divorce''; 2) whether language models capture cultural diversity and shared\ntendencies in which topics people around the globe tend to diverge or agree on\nin their moral judgment. We perform our analyses with two public datasets from\nthe World Values Survey (across 55 countries) and PEW global surveys (across 40\ncountries) on morality. We find that pre-trained English language models\npredict empirical moral norms across countries worse than the English moral\nnorms reported previously. However, fine-tuning language models on the survey\ndata improves inference across countries at the expense of a less accurate\nestimate of the English moral norms. We discuss the relevance and challenges of\nincorporating cultural knowledge into the automated inference of moral norms.",
        "pdf_link": "https://arxiv.org/pdf/2306.01857v1.pdf"
    },
    {
        "title": "Evaluating Language Models for Mathematics through Interactions",
        "authors": [
            "Katherine M. Collins",
            "Albert Q. Jiang",
            "Simon Frieder",
            "Lionel Wong",
            "Miri Zilka",
            "Umang Bhatt",
            "Thomas Lukasiewicz",
            "Yuhuai Wu",
            "Joshua B. Tenenbaum",
            "William Hart",
            "Timothy Gowers",
            "Wenda Li",
            "Adrian Weller",
            "Mateja Jamnik"
        ],
        "published": "2023-06-02T17:12:25Z",
        "summary": "There is much excitement about the opportunity to harness the power of large\nlanguage models (LLMs) when building problem-solving assistants. However, the\nstandard methodology of evaluating LLMs relies on static pairs of inputs and\noutputs, and is insufficient for making an informed decision about which LLMs\nand under which assistive settings can they be sensibly used. Static assessment\nfails to account for the essential interactive element in LLM deployment, and\ntherefore limits how we understand language model capabilities. We introduce\nCheckMate, an adaptable prototype platform for humans to interact with and\nevaluate LLMs. We conduct a study with CheckMate to evaluate three language\nmodels (InstructGPT, ChatGPT, and GPT-4) as assistants in proving\nundergraduate-level mathematics, with a mixed cohort of participants from\nundergraduate students to professors of mathematics. We release the resulting\ninteraction and rating dataset, MathConverse. By analysing MathConverse, we\nderive a taxonomy of human behaviours and uncover that despite a generally\npositive correlation, there are notable instances of divergence between\ncorrectness and perceived helpfulness in LLM generations, amongst other\nfindings. Further, we garner a more granular understanding of GPT-4\nmathematical problem-solving through a series of case studies, contributed by\nexpert mathematicians. We conclude with actionable takeaways for ML\npractitioners and mathematicians: models that communicate uncertainty respond\nwell to user corrections, and are more interpretable and concise may constitute\nbetter assistants. Interactive evaluation is a promising way to navigate the\ncapability of these models; humans should be aware of language models'\nalgebraic fallibility and discern where they are appropriate to use.",
        "pdf_link": "https://arxiv.org/pdf/2306.01694v2.pdf"
    },
    {
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
        "authors": [
            "Zeqiu Wu",
            "Yushi Hu",
            "Weijia Shi",
            "Nouha Dziri",
            "Alane Suhr",
            "Prithviraj Ammanabrolu",
            "Noah A. Smith",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-06-02T17:11:37Z",
        "summary": "Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2306.01693v2.pdf"
    },
    {
        "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates",
        "authors": [
            "Mohammad Mozaffari",
            "Sikan Li",
            "Zhao Zhang",
            "Maryam Mehri Dehnavi"
        ],
        "published": "2023-06-02T17:00:19Z",
        "summary": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using\nRank-1 updates, called MKOR, that improves the training time and convergence\nproperties of deep neural networks (DNNs). Second-order techniques, while\nenjoying higher convergence rates vs first-order counterparts, have cubic\ncomplexity with respect to either the model size and/or the training batch\nsize. Hence they exhibit poor scalability and performance in transformer\nmodels, e.g. large language models (LLMs), because the batch sizes in these\nmodels scale by the attention mechanism sequence length, leading to large model\nsize and batch sizes. MKOR's complexity is quadratic with respect to the model\nsize, alleviating the computation bottlenecks in second-order methods. Because\nof their high computation complexity, state-of-the-art implementations of\nsecond-order methods can only afford to update the second order information\ninfrequently, and thus do not fully exploit the promise of better convergence\nfrom these updates. By reducing the communication complexity of the\nsecond-order updates as well as achieving a linear communication complexity,\nMKOR increases the frequency of second order updates. We also propose a hybrid\nversion of MKOR (called MKOR-H) that mid-training falls backs to a first order\noptimizer if the second order updates no longer accelerate convergence. Our\nexperiments show that MKOR outperforms state -of-the-art first order methods,\ne.g. the LAMB optimizer, and best implementations of second-order methods, i.e.\nKAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64\nGPUs.",
        "pdf_link": "https://arxiv.org/pdf/2306.01685v2.pdf"
    },
    {
        "title": "Harnessing large-language models to generate private synthetic text",
        "authors": [
            "Alexey Kurakin",
            "Natalia Ponomareva",
            "Umar Syed",
            "Liam MacDermed",
            "Andreas Terzis"
        ],
        "published": "2023-06-02T16:59:36Z",
        "summary": "Differentially private training algorithms like DP-SGD protect sensitive\ntraining data by ensuring that trained models do not reveal private\ninformation. An alternative approach, which this paper studies, is to use a\nsensitive dataset to generate synthetic data that is differentially private\nwith respect to the original data, and then non-privately training a model on\nthe synthetic data. Doing so has several advantages: synthetic data can be\nreused for other tasks (including for hyper parameter tuning), retained\nindefinitely, and shared with third parties without sacrificing privacy.\nHowever, generating private synthetic data is much harder than training a\nprivate model. To improve performance on text data, recent work has utilized\npublic data by starting with a pre-trained generative language model and\nprivately fine-tuning it on sensitive data. This model can be used to sample a\nDP synthetic dataset. While this strategy seems straightforward, executing it\nhas proven problematic. Previous approaches either show significant performance\nloss, or have, as we show, critical design flaws. In this paper we demonstrate\nthat a proper training objective along with tuning fewer parameters results in\nexcellent DP synthetic data quality. Our approach is competitive with direct\nDP-training of downstream classifiers in terms of performance on downstream\ntasks. Further, we demonstrate that our DP synthetic data is not only useful\nfor downstream classifier training, but also to tune those same models.",
        "pdf_link": "https://arxiv.org/pdf/2306.01684v2.pdf"
    },
    {
        "title": "Log Parsing: How Far Can ChatGPT Go?",
        "authors": [
            "Van-Hoang Le",
            "Hongyu Zhang"
        ],
        "published": "2023-06-02T14:58:43Z",
        "summary": "Software logs play an essential role in ensuring the reliability and\nmaintainability of large-scale software systems, as they are often the sole\nsource of runtime information. Log parsing, which converts raw log messages\ninto structured data, is an important initial step towards downstream log\nanalytics. In recent studies, ChatGPT, the current cutting-edge large language\nmodel (LLM), has been widely applied to a wide range of software engineering\ntasks. However, its performance in automated log parsing remains unclear. In\nthis paper, we evaluate ChatGPT's ability to undertake log parsing by\naddressing two research questions. (1) Can ChatGPT effectively parse logs? (2)\nHow does ChatGPT perform with different prompting methods? Our results show\nthat ChatGPT can achieve promising results for log parsing with appropriate\nprompts, especially with few-shot prompting. Based on our findings, we outline\nseveral challenges and opportunities for ChatGPT-based log parsing.",
        "pdf_link": "https://arxiv.org/pdf/2306.01590v2.pdf"
    },
    {
        "title": "EmoUS: Simulating User Emotions in Task-Oriented Dialogues",
        "authors": [
            "Hsien-Chin Lin",
            "Shutong Feng",
            "Christian Geishauser",
            "Nurul Lubis",
            "Carel van Niekerk",
            "Michael Heck",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Milica Ga\u0161i\u0107"
        ],
        "published": "2023-06-02T14:48:19Z",
        "summary": "Existing user simulators (USs) for task-oriented dialogue systems only model\nuser behaviour on semantic and natural language levels without considering the\nuser persona and emotions. Optimising dialogue systems with generic user\npolicies, which cannot model diverse user behaviour driven by different\nemotional states, may result in a high drop-off rate when deployed in the real\nworld. Thus, we present EmoUS, a user simulator that learns to simulate user\nemotions alongside user behaviour. EmoUS generates user emotions, semantic\nactions, and natural language responses based on the user goal, the dialogue\nhistory, and the user persona. By analysing what kind of system behaviour\nelicits what kind of user emotions, we show that EmoUS can be used as a probe\nto evaluate a variety of dialogue systems and in particular their effect on the\nuser's emotional state. Developing such methods is important in the age of\nlarge language model chat-bots and rising ethical concerns.",
        "pdf_link": "https://arxiv.org/pdf/2306.01579v1.pdf"
    },
    {
        "title": "PassGPT: Password Modeling and (Guided) Generation with Large Language Models",
        "authors": [
            "Javier Rando",
            "Fernando Perez-Cruz",
            "Briland Hitaj"
        ],
        "published": "2023-06-02T13:49:53Z",
        "summary": "Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.",
        "pdf_link": "https://arxiv.org/pdf/2306.01545v2.pdf"
    },
    {
        "title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",
        "authors": [
            "Zhuo Wang",
            "Rongzhen Li",
            "Bowen Dong",
            "Jie Wang",
            "Xiuxing Li",
            "Ning Liu",
            "Chenhui Mao",
            "Wei Zhang",
            "Liling Dong",
            "Jing Gao",
            "Jianyong Wang"
        ],
        "published": "2023-06-02T12:47:45Z",
        "summary": "Recent investigations show that large language models (LLMs), specifically\nGPT-4, not only have remarkable capabilities in common Natural Language\nProcessing (NLP) tasks but also exhibit human-level performance on various\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\nused in practical applications and replace traditional artificial intelligence\n(AI) tools in specialized domains requires further experimental validation. In\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\naccuracy in a clinical setting. Experimental results on two real clinical\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\nadvancements in dementia diagnosis, they currently do not surpass the\nperformance of traditional AI tools. The interpretability and faithfulness of\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\nlimitations of GPT-4 in its current state and propose future research\ndirections to enhance GPT-4 in dementia diagnosis.",
        "pdf_link": "https://arxiv.org/pdf/2306.01499v1.pdf"
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations",
        "authors": [
            "Pan Li",
            "Yuyan Wang",
            "Ed H. Chi",
            "Minmin Chen"
        ],
        "published": "2023-06-02T12:00:03Z",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "pdf_link": "https://arxiv.org/pdf/2306.01475v1.pdf"
    },
    {
        "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
        "authors": [
            "Michael Heck",
            "Nurul Lubis",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Shutong Feng",
            "Christian Geishauser",
            "Hsien-Chin Lin",
            "Carel van Niekerk",
            "Milica Ga\u0161i\u0107"
        ],
        "published": "2023-06-02T09:15:01Z",
        "summary": "Recent research on dialogue state tracking (DST) focuses on methods that\nallow few- and zero-shot transfer to new domains or schemas. However,\nperformance gains heavily depend on aggressive data augmentation and\nfine-tuning of ever larger language model based architectures. In contrast,\ngeneral purpose language models, trained on large amounts of diverse data, hold\nthe promise of solving any kind of task without task-specific training. We\npresent preliminary experimental results on the ChatGPT research preview,\nshowing that ChatGPT achieves state-of-the-art performance in zero-shot DST.\nDespite our findings, we argue that properties inherent to general purpose\nmodels limit their ability to replace specialized systems. We further theorize\nthat the in-context learning capabilities of such models will likely become\npowerful tools to support the development of dedicated and dynamic dialogue\nstate trackers.",
        "pdf_link": "https://arxiv.org/pdf/2306.01386v1.pdf"
    },
    {
        "title": "An Empirical Study on Challenging Math Problem Solving with GPT-4",
        "authors": [
            "Yiran Wu",
            "Feiran Jia",
            "Shaokun Zhang",
            "Hangyu Li",
            "Erkang Zhu",
            "Yue Wang",
            "Yin Tat Lee",
            "Richard Peng",
            "Qingyun Wu",
            "Chi Wang"
        ],
        "published": "2023-06-02T08:02:15Z",
        "summary": "Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nWhile several prior works have investigated solving elementary mathematics\nusing LLMs, this work explores the frontier of using GPT-4 for solving more\ncomplex and challenging math problems. We evaluate various ways of using GPT-4.\nSome of them are adapted from existing work, and one is MathChat, a\nconversational problem-solving framework newly proposed in this work. We\nperform the evaluation on difficult high school competition problems from the\nMATH dataset, which shows the advantage of the proposed conversational\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2306.01337v2.pdf"
    },
    {
        "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
        "authors": [
            "Masoud Monajatipoor",
            "Liunian Harold Li",
            "Mozhdeh Rouhsedaghat",
            "Lin F. Yang",
            "Kai-Wei Chang"
        ],
        "published": "2023-06-02T07:21:03Z",
        "summary": "Large-scale language models have shown the ability to adapt to a new task via\nconditioning on a few demonstrations (i.e., in-context learning). However, in\nthe vision-language domain, most large-scale pre-trained vision-language (VL)\nmodels do not possess the ability to conduct in-context learning. How can we\nenable in-context learning for VL models? In this paper, we study an\ninteresting hypothesis: can we transfer the in-context learning ability from\nthe language domain to VL domain? Specifically, we first meta-trains a language\nmodel to perform in-context learning on NLP tasks (as in MetaICL); then we\ntransfer this model to perform VL tasks by attaching a visual encoder. Our\nexperiments suggest that indeed in-context learning ability can be transferred\ncross modalities: our model considerably improves the in-context learning\ncapability on VL tasks and can even compensate for the size of the model\nsignificantly. On VQA, OK-VQA, and GQA, our method could outperform the\nbaseline model while having 20 times fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2306.01311v1.pdf"
    },
    {
        "title": "Egocentric Planning for Scalable Embodied Task Achievement",
        "authors": [
            "Xiaotian Liu",
            "Hector Palacios",
            "Christian Muise"
        ],
        "published": "2023-06-02T06:41:24Z",
        "summary": "Embodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01295v1.pdf"
    },
    {
        "title": "ChatGPT is a Remarkable Tool -- For Experts",
        "authors": [
            "Amos Azaria",
            "Rina Azoulay",
            "Shulamit Reches"
        ],
        "published": "2023-06-02T06:28:21Z",
        "summary": "This paper investigates the capabilities of ChatGPT as an automated assistant\nin diverse domains, including scientific writing, mathematics, education,\nprogramming, and healthcare. We explore the potential of ChatGPT to enhance\nproductivity, streamline problem-solving processes, and improve writing style.\nFurthermore, we highlight the potential risks associated with excessive\nreliance on ChatGPT in these fields. These limitations encompass factors like\nincorrect and fictitious responses, inaccuracies in code, limited logical\nreasoning abilities, overconfidence, and critical ethical concerns of\ncopyrights and privacy violation. We outline areas and objectives where ChatGPT\nproves beneficial, applications where it should be used judiciously, and\nscenarios where its reliability may be limited. In light of observed\nlimitations, and given that the tool's fundamental errors may pose a special\nchallenge for non-experts, ChatGPT should be used with a strategic methodology.\nBy drawing from comprehensive experimental studies, we offer methods and flow\ncharts for effectively using ChatGPT. Our recommendations emphasize iterative\ninteraction with ChatGPT and independent verification of its outputs.\nConsidering the importance of utilizing ChatGPT judiciously and with expertise,\nwe recommend its usage for experts who are well-versed in the respective\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2306.03102v1.pdf"
    },
    {
        "title": "KL-Divergence Guided Temperature Sampling",
        "authors": [
            "Chung-Ching Chang",
            "David Reitter",
            "Renat Aksitov",
            "Yun-Hsuan Sung"
        ],
        "published": "2023-06-02T06:11:26Z",
        "summary": "Temperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01286v2.pdf"
    },
    {
        "title": "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?",
        "authors": [
            "Aniket Deroy",
            "Kripabandhu Ghosh",
            "Saptarshi Ghosh"
        ],
        "published": "2023-06-02T03:16:19Z",
        "summary": "Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.",
        "pdf_link": "https://arxiv.org/pdf/2306.01248v2.pdf"
    },
    {
        "title": "Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators",
        "authors": [
            "Zhizheng Zhang",
            "Xiaoyi Zhang",
            "Wenxuan Xie",
            "Yan Lu"
        ],
        "published": "2023-06-02T02:42:58Z",
        "summary": "The recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.01242v2.pdf"
    },
    {
        "title": "Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation",
        "authors": [
            "Bonan Kou",
            "Shengmai Chen",
            "Zhijie Wang",
            "Lei Ma",
            "Tianyi Zhang"
        ],
        "published": "2023-06-02T00:57:03Z",
        "summary": "Large Language Models (LLMs) have been demonstrated effective for code\ngeneration. Due to the complexity and opacity of LLMs, little is known about\nhow these models generate code. To deepen our understanding, we investigate\nwhether LLMs attend to the same parts of a natural language description as\nhuman programmers during code generation. An analysis of five LLMs on a popular\nbenchmark, HumanEval, revealed a consistent misalignment between LLMs' and\nprogrammers' attention. Furthermore, we found that there is no correlation\nbetween the code generation accuracy of LLMs and their alignment with human\nprogrammers. Through a quantitative experiment and a user study, we confirmed\nthat, among twelve different attention computation methods, attention computed\nby the perturbation-based method is most aligned with human attention and is\nconstantly favored by human programmers. Our findings highlight the need for\nhuman-aligned LLMs for better interpretability and programmer trust.",
        "pdf_link": "https://arxiv.org/pdf/2306.01220v1.pdf"
    },
    {
        "title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
        "authors": [
            "Adithya V Ganesan",
            "Yash Kumar Lal",
            "August H\u00e5kan Nilsson",
            "H. Andrew Schwartz"
        ],
        "published": "2023-06-01T22:43:37Z",
        "summary": "Very large language models (LLMs) perform extremely well on a spectrum of NLP\ntasks in a zero-shot setting. However, little is known about their performance\non human-level NLP problems which rely on understanding psychological concepts,\nsuch as assessing personality traits. In this work, we investigate the\nzero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'\nsocial media posts. Through a set of systematic experiments, we find that\nzero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA\nfor broad classification upon injecting knowledge about the trait in the\nprompts. However, when prompted to provide fine-grained classification, its\nperformance drops to close to a simple most frequent class (MFC) baseline. We\nfurther analyze where GPT-3 performs better, as well as worse, than a\npretrained lexical model, illustrating systematic errors that suggest ways to\nimprove LLMs on human-level NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01183v1.pdf"
    },
    {
        "title": "Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study",
        "authors": [
            "Guang Lu",
            "Sylvia B. Larcher",
            "Tu Tran"
        ],
        "published": "2023-06-01T21:58:33Z",
        "summary": "Text summarization is a downstream natural language processing (NLP) task\nthat challenges the understanding and generation capabilities of language\nmodels. Considerable progress has been made in automatically summarizing short\ntexts, such as news articles, often leading to satisfactory results. However,\nsummarizing long documents remains a major challenge. This is due to the\ncomplex contextual information in the text and the lack of open-source\nbenchmarking datasets and evaluation frameworks that can be used to develop and\ntest model performance. In this work, we use ChatGPT, the latest breakthrough\nin the field of large language models (LLMs), together with the extractive\nsummarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a\nhybrid extraction and summarization pipeline for long documents such as\nbusiness articles and books. We work with the world-renowned company\ngetAbstract AG and leverage their expertise and experience in professional book\nsummarization. A practical study has shown that machine-generated summaries can\nperform at least as well as human-written summaries when evaluated using\ncurrent automated evaluation metrics. However, a closer examination of the\ntexts generated by ChatGPT through human evaluations has shown that there are\nstill critical issues in terms of text coherence, faithfulness, and style.\nOverall, our results show that the use of ChatGPT is a very promising but not\nyet mature approach for summarizing long documents and can at best serve as an\ninspiration for human editors. We anticipate that our work will inform NLP\nresearchers about the extent to which ChatGPT's capabilities for summarizing\nlong documents overlap with practitioners' needs. Further work is needed to\ntest the proposed hybrid summarization pipeline, in particular involving GPT-4,\nand to propose a new evaluation framework tailored to the task of summarizing\nlong documents.",
        "pdf_link": "https://arxiv.org/pdf/2306.01169v1.pdf"
    },
    {
        "title": "Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data",
        "authors": [
            "Nathan Vaska",
            "Victoria Helus"
        ],
        "published": "2023-06-01T20:56:34Z",
        "summary": "The impressive advances and applications of large language and joint\nlanguage-and-visual understanding models has led to an increased need for\nmethods of probing their potential reasoning capabilities. However, the\ndifficulty of gather naturally-occurring data for complex multi-modal reasoning\ntasks bottlenecks the evaluation of AI methods on tasks which are not already\ncovered by an academic dataset. In this work, we leverage recent advances in\nhigh resolution text-to-image generation to develop a framework for generating\nevaluation data for multi-modal reasoning tasks. We apply this framework to\ngenerate context-dependent anomaly data, creating a synthetic dataset on a\nchallenging task which is not well covered by existing datasets. We benchmark\nthe performance of a state-of-the-art visual question answering (VQA) model\nagainst data generated with this method, and demonstrate that while the task is\ntractable, the model performs significantly worse on the context-dependent\nanomaly detection task than on standard VQA tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01144v1.pdf"
    },
    {
        "title": "LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization",
        "authors": [
            "Muhammad U. Nasir",
            "Sam Earle",
            "Julian Togelius",
            "Steven James",
            "Christopher Cleghorn"
        ],
        "published": "2023-06-01T19:33:21Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. Here, we propose using the coding abilities of LLMs to introduce\nmeaningful variations to code defining neural networks. Meanwhile,\nQuality-Diversity (QD) algorithms are known to discover diverse and robust\nsolutions. By merging the code-generating abilities of LLMs with the diversity\nand robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, \\texttt{LLMatic} uses a procedural approach,\nleveraging QD for prompts and network architecture to create diverse and\nhigh-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and\nNAS-bench-201 benchmarks, demonstrating that it can produce competitive\nnetworks while evaluating just $2,000$ candidates, even without prior knowledge\nof the benchmark domain or exposure to any previous top-performing models for\nthe benchmark. The open-sourced code is available in\n\\url{https://github.com/umair-nasir14/LLMatic}.",
        "pdf_link": "https://arxiv.org/pdf/2306.01102v7.pdf"
    },
    {
        "title": "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes",
        "authors": [
            "Revathy Venkataramanan",
            "Kaushik Roy",
            "Kanak Raj",
            "Renjith Prasad",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-06-01T18:49:47Z",
        "summary": "As people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2306.01805v1.pdf"
    },
    {
        "title": "TopEx: Topic-based Explanations for Model Comparison",
        "authors": [
            "Shreya Havaldar",
            "Adam Stein",
            "Eric Wong",
            "Lyle Ungar"
        ],
        "published": "2023-06-01T17:59:10Z",
        "summary": "Meaningfully comparing language models is challenging with current\nexplanation methods. Current explanations are overwhelming for humans due to\nlarge vocabularies or incomparable across models. We present TopEx, an\nexplanation method that enables a level playing field for comparing language\nmodels via model-agnostic topics. We demonstrate how TopEx can identify\nsimilarities and differences between DistilRoBERTa and GPT-2 on a variety of\nNLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.00976v2.pdf"
    },
    {
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
        "authors": [
            "Ji Lin",
            "Jiaming Tang",
            "Haotian Tang",
            "Shang Yang",
            "Xingyu Dang",
            "Chuang Gan",
            "Song Han"
        ],
        "published": "2023-06-01T17:59:10Z",
        "summary": "Large language models (LLMs) have shown excellent performance on various\ntasks, but the astronomical model size raises the hardware barrier for serving\n(memory size) and slows down token generation (memory bandwidth). In this\npaper, we propose Activation-aware Weight Quantization (AWQ), a\nhardware-friendly approach for LLM low-bit weight-only quantization. Our method\nis based on the observation that weights are not equally important: protecting\nonly 1% of salient weights can greatly reduce quantization error. We then\npropose to search for the optimal per-channel scaling that protects the salient\nweights by observing the activation, not weights. AWQ does not rely on any\nbackpropagation or reconstruction, so it can well preserve LLMs' generalization\nability on different domains and modalities, without overfitting to the\ncalibration set. AWQ outperforms existing work on various language modeling and\ndomain-specific benchmarks. Thanks to better generalization, it achieves\nexcellent quantization performance for instruction-tuned LMs and, for the first\ntime, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible\ninference framework tailored for LLMs on the edge, offering more than 3x\nspeedup over the Huggingface FP16 implementation on both desktop and mobile\nGPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile\nGPU (NVIDIA Jetson Orin 64GB).",
        "pdf_link": "https://arxiv.org/pdf/2306.00978v2.pdf"
    },
    {
        "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
        "authors": [
            "Bingbin Liu",
            "Jordan T. Ash",
            "Surbhi Goel",
            "Akshay Krishnamurthy",
            "Cyril Zhang"
        ],
        "published": "2023-06-01T17:44:35Z",
        "summary": "Why do large language models sometimes output factual inaccuracies and\nexhibit erroneous reasoning? The brittleness of these models, particularly when\nexecuting long chains of reasoning, currently seems to be an inevitable price\nto pay for their advanced capabilities of coherently synthesizing knowledge,\npragmatics, and abstract thought. Towards making sense of this fundamentally\nunsolved problem, this work identifies and analyzes the phenomenon of attention\nglitches, in which the Transformer architecture's inductive biases\nintermittently fail to capture robust reasoning. To isolate the issue, we\nintroduce flip-flop language modeling (FFLM), a parametric family of synthetic\nbenchmarks designed to probe the extrapolative behavior of neural language\nmodels. This simple generative task requires a model to copy binary symbols\nover long-range dependencies, ignoring the tokens in between. We find that\nTransformer FFLMs suffer from a long tail of sporadic reasoning errors, some of\nwhich we can eliminate using various regularization techniques. Our preliminary\nmechanistic analyses show why the remaining errors may be very difficult to\ndiagnose and resolve. We hypothesize that attention glitches account for (some\nof) the closed-domain hallucinations in natural LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.00946v2.pdf"
    },
    {
        "title": "Vocabulary-free Image Classification",
        "authors": [
            "Alessandro Conti",
            "Enrico Fini",
            "Massimiliano Mancini",
            "Paolo Rota",
            "Yiming Wang",
            "Elisa Ricci"
        ],
        "published": "2023-06-01T17:19:43Z",
        "summary": "Recent advances in large vision-language models have revolutionized the image\nclassification paradigm. Despite showing impressive zero-shot capabilities, a\npre-defined set of categories, a.k.a. the vocabulary, is assumed at test time\nfor composing the textual prompts. However, such assumption can be impractical\nwhen the semantic context is unknown and evolving. We thus formalize a novel\ntask, termed as Vocabulary-free Image Classification (VIC), where we aim to\nassign to an input image a class that resides in an unconstrained\nlanguage-induced semantic space, without the prerequisite of a known\nvocabulary. VIC is a challenging task as the semantic space is extremely large,\ncontaining millions of concepts, with hard-to-discriminate fine-grained\ncategories. In this work, we first empirically verify that representing this\nsemantic space by means of an external vision-language database is the most\neffective way to obtain semantically relevant content for classifying the\nimage. We then propose Category Search from External Databases (CaSED), a\nmethod that exploits a pre-trained vision-language model and an external\nvision-language database to address VIC in a training-free manner. CaSED first\nextracts a set of candidate categories from captions retrieved from the\ndatabase based on their semantic similarity to the image, and then assigns to\nthe image the best matching candidate category according to the same\nvision-language model. Experiments on benchmark datasets validate that CaSED\noutperforms other complex vision-language frameworks, while being efficient\nwith much fewer parameters, paving the way for future research in this\ndirection.",
        "pdf_link": "https://arxiv.org/pdf/2306.00917v3.pdf"
    },
    {
        "title": "The feasibility of artificial consciousness through the lens of neuroscience",
        "authors": [
            "Jaan Aru",
            "Matthew Larkum",
            "James M. Shine"
        ],
        "published": "2023-06-01T17:18:15Z",
        "summary": "Interactions with large language models have led to the suggestion that these\nmodels may soon be conscious. From the perspective of neuroscience, this\nposition is difficult to defend. For one, the inputs to large language models\nlack the embodied, embedded information content characteristic of our sensory\ncontact with the world around us. Secondly, the architecture of large language\nmodels is missing key features of the thalamocortical system that have been\nlinked to conscious awareness in mammals. Finally, the evolutionary and\ndevelopmental trajectories that led to the emergence of living conscious\norganisms arguably have no parallels in artificial systems as envisioned today.\nThe existence of living organisms depends on their actions, and their survival\nis intricately linked to multi-level cellular, inter-cellular, and organismal\nprocesses culminating in agency and consciousness.",
        "pdf_link": "https://arxiv.org/pdf/2306.00915v3.pdf"
    },
    {
        "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
        "authors": [
            "Chunyuan Li",
            "Cliff Wong",
            "Sheng Zhang",
            "Naoto Usuyama",
            "Haotian Liu",
            "Jianwei Yang",
            "Tristan Naumann",
            "Hoifung Poon",
            "Jianfeng Gao"
        ],
        "published": "2023-06-01T16:50:07Z",
        "summary": "Conversational generative AI has demonstrated remarkable promise for\nempowering biomedical practitioners, but current investigations focus on\nunimodal text. Multimodal conversational AI has seen rapid progress by\nleveraging billions of image-text pairs from the public web, but such\ngeneral-domain vision-language models still lack sophistication in\nunderstanding and conversing about biomedical images. In this paper, we propose\na cost-efficient approach for training a vision-language conversational\nassistant that can answer open-ended research questions of biomedical images.\nThe key idea is to leverage a large-scale, broad-coverage biomedical\nfigure-caption dataset extracted from PubMed Central, use GPT-4 to\nself-instruct open-ended instruction-following data from the captions, and then\nfine-tune a large general-domain vision-language model using a novel curriculum\nlearning method. Specifically, the model first learns to align biomedical\nvocabulary using the figure-caption pairs as is, then learns to master\nopen-ended conversational semantics using GPT-4 generated instruction-following\ndata, broadly mimicking how a layperson gradually acquires biomedical\nknowledge. This enables us to train a Large Language and Vision Assistant for\nBioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med\nexhibits excellent multimodal conversational capability and can follow\nopen-ended instruction to assist with inquiries about a biomedical image. On\nthree standard biomedical visual question answering datasets, LLaVA-Med\noutperforms previous supervised state-of-the-art on certain metrics. To\nfacilitate biomedical multimodal research, we will release our\ninstruction-following data and the LLaVA-Med model.",
        "pdf_link": "https://arxiv.org/pdf/2306.00890v1.pdf"
    },
    {
        "title": "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",
        "authors": [
            "Ruotong Wang",
            "Hongrui Chen",
            "Zihao Zhu",
            "Li Liu",
            "Yong Zhang",
            "Yanbo Fan",
            "Baoyuan Wu"
        ],
        "published": "2023-06-01T15:42:06Z",
        "summary": "Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non benign samples, dubbed backdoor attack. Some recent research has focused on\ndesigning invisible triggers for backdoor attacks to ensure visual\nstealthiness, while showing high effectiveness, even under backdoor defense.\nHowever, we find that these carefully designed invisible triggers are often\nsensitive to visual distortion during inference, such as Gaussian blurring or\nenvironmental variations in physical scenarios. This phenomenon could\nsignificantly undermine the practical effectiveness of attacks, but has been\nrarely paid attention to and thoroughly investigated. To address this\nlimitation, we define a novel trigger called the Visible, Semantic,\nSample-Specific, and Compatible trigger (VSSC trigger), to achieve effective,\nstealthy and robust to visual distortion simultaneously. To implement it, we\ndevelop an innovative approach by utilizing the powerful capabilities of large\nlanguage models for choosing the suitable trigger and text-guided image editing\ntechniques for generating the poisoned image with the trigger. Extensive\nexperimental results and analysis validate the effectiveness, stealthiness and\nrobustness of the VSSC trigger. It demonstrates superior robustness to\ndistortions compared with most digital backdoor attacks and allows more\nefficient and flexible trigger integration compared to physical backdoor\nattacks. We hope that the proposed VSSC trigger and implementation approach\ncould inspire future studies on designing more practical triggers in backdoor\nattacks.",
        "pdf_link": "https://arxiv.org/pdf/2306.00816v2.pdf"
    },
    {
        "title": "GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?",
        "authors": [
            "Ning Ding",
            "Yehui Tang",
            "Zhongqian Fu",
            "Chao Xu",
            "Kai Han",
            "Yunhe Wang"
        ],
        "published": "2023-06-01T14:02:45Z",
        "summary": "The recent upsurge in pre-trained large models (e.g. GPT-4) has swept across\nthe entire deep learning community. Such powerful large language models (LLMs)\ndemonstrate advanced generative ability and multimodal understanding\ncapability, which quickly achieve new state-of-the-art performances on a\nvariety of benchmarks. The pre-trained LLM usually plays the role as a\nuniversal AI model that can conduct various tasks, including context reasoning,\narticle analysis and image content comprehension. However, considering the\nprohibitively high memory and computational cost for implementing such a large\nmodel, the conventional models (such as CNN and ViT), are still essential for\nmany visual perception tasks. In this paper, we propose to enhance the\nrepresentation ability of ordinary vision models for perception tasks (e.g.\nimage classification) by taking advantage of large pre-trained models. We\npresent a new learning paradigm in which the knowledge extracted from large\npre-trained models are utilized to help models like CNN and ViT learn enhanced\nrepresentations and achieve better performance. Firstly, we curate a high\nquality description set by prompting a multimodal LLM to generate descriptive\ntext for all training images. Furthermore, we feed these detailed descriptions\ninto a pre-trained encoder to extract text embeddings with rich semantic\ninformation that encodes the content of images. During training, text\nembeddings will serve as extra supervising signals and be aligned with image\nrepresentations learned by vision models. The alignment process helps vision\nmodels learn better and achieve higher accuracy with the assistance of\npre-trained LLMs. We conduct extensive experiments to verify that the proposed\nalgorithm consistently improves the performance for various vision models with\nheterogeneous architectures.",
        "pdf_link": "https://arxiv.org/pdf/2306.00693v2.pdf"
    },
    {
        "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs",
        "authors": [
            "Han Cui",
            "Shangzhan Li",
            "Yu Zhang",
            "Qi Shi"
        ],
        "published": "2023-06-01T13:20:22Z",
        "summary": "The generation of explanation graphs is a significant task that aims to\nproduce explanation graphs in response to user input, revealing the internal\nreasoning process. This task is challenging due to the significant discrepancy\nbetween unstructured user queries and structured explanation graphs. Current\nresearch commonly fine-tunes a text-based pre-trained language model on a small\ndownstream dataset that is annotated with labeled graphs. However, due to the\nlimited scale of available datasets, this approach may prove to be insufficient\nin bridging the gap between natural language text and structured graphs. In\nthis paper, to alleviate the above limitations, we propose a novel pre-trained\nframework EG3P(for Explanation Graph Generation via Generative Pre-training\nover synthetic graphs) for the explanation graph generation task. Specifically,\nwe first propose a text-to-graph generative task to pre-train the model with\nthe goal of bridging the text-graph gap. Additionally, we propose an automatic\ncorpus synthesis strategy for synthesizing a large scale of high-quality\ncorpus, reducing the reliance on costly manual annotation methods. Experimental\nresults on ExplaGraphs show the effectiveness of EG3P that our model surpasses\nall baseline systems with remarkable margins. Besides, further analysis\ndemonstrates that EG3P is able to generate better explanation graphs on actual\nreasoning tasks such as CommonsenseQA and OpenbookQA.",
        "pdf_link": "https://arxiv.org/pdf/2306.00652v1.pdf"
    },
    {
        "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
        "authors": [
            "Ryan Liu",
            "Nihar B. Shah"
        ],
        "published": "2023-06-01T12:45:53Z",
        "summary": "Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.",
        "pdf_link": "https://arxiv.org/pdf/2306.00622v1.pdf"
    },
    {
        "title": "Analysis of ChatGPT on Source Code",
        "authors": [
            "Ahmed R. Sadik",
            "Antonello Ceravola",
            "Frank Joublin",
            "Jibesh Patra"
        ],
        "published": "2023-06-01T12:12:59Z",
        "summary": "This paper explores the use of Large Language Models (LLMs) and in particular\nChatGPT in programming, source code analysis, and code generation. LLMs and\nChatGPT are built using machine learning and artificial intelligence\ntechniques, and they offer several benefits to developers and programmers.\nWhile these models can save time and provide highly accurate results, they are\nnot yet advanced enough to replace human programmers entirely. The paper\ninvestigates the potential applications of LLMs and ChatGPT in various areas,\nsuch as code creation, code documentation, bug detection, refactoring, and\nmore. The paper also suggests that the usage of LLMs and ChatGPT is expected to\nincrease in the future as they offer unparalleled benefits to the programming\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2306.00597v2.pdf"
    },
    {
        "title": "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study",
        "authors": [
            "Yuxin Tang"
        ],
        "published": "2023-06-01T11:11:39Z",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.00550v1.pdf"
    },
    {
        "title": "CapText: Large Language Model-based Caption Generation From Image Context and Description",
        "authors": [
            "Shinjini Ghosh",
            "Sagnik Anupam"
        ],
        "published": "2023-06-01T02:40:44Z",
        "summary": "While deep-learning models have been shown to perform well on image-to-text\ndatasets, it is difficult to use them in practice for captioning images. This\nis because captions traditionally tend to be context-dependent and offer\ncomplementary information about an image, while models tend to produce\ndescriptions that describe the visual features of the image. Prior research in\ncaption generation has explored the use of models that generate captions when\nprovided with the images alongside their respective descriptions or contexts.\nWe propose and evaluate a new approach, which leverages existing large language\nmodels to generate captions from textual descriptions and context alone,\nwithout ever processing the image directly. We demonstrate that after\nfine-tuning, our approach outperforms current state-of-the-art image-text\nalignment models like OSCAR-VinVL on this task on the CIDEr metric.",
        "pdf_link": "https://arxiv.org/pdf/2306.00301v2.pdf"
    },
    {
        "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap",
        "authors": [
            "Q. Vera Liao",
            "Ziang Xiao"
        ],
        "published": "2023-06-01T00:01:43Z",
        "summary": "The recent development of generative and large language models (LLMs) poses\nnew challenges for model evaluation that the research community and industry\nare grappling with. While the versatile capabilities of these models ignite\nexcitement, they also inevitably make a leap toward homogenization: powering a\nwide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in downstream use cases can be\nsatisfied by the given model (socio-technical gap). By drawing on lessons from\nthe social sciences, human-computer interaction (HCI), and the\ninterdisciplinary field of explainable AI (XAI), we urge the community to\ndevelop evaluation methods based on real-world socio-requirements and embrace\ndiverse evaluation methods with an acknowledgment of trade-offs between realism\nto socio-requirements and pragmatic costs to conduct the evaluation. By mapping\nHCI and current NLG evaluation methods, we identify opportunities for\nevaluation methods for LLMs to narrow the socio-technical gap and pose open\nquestions.",
        "pdf_link": "https://arxiv.org/pdf/2306.03100v3.pdf"
    },
    {
        "title": "An Invariant Learning Characterization of Controlled Text Generation",
        "authors": [
            "Carolina Zheng",
            "Claudia Shi",
            "Keyon Vafa",
            "Amir Feder",
            "David M. Blei"
        ],
        "published": "2023-05-31T21:35:08Z",
        "summary": "Controlled generation refers to the problem of creating text that contains\nstylistic or semantic attributes of interest. Many approaches reduce this\nproblem to training a predictor of the desired attribute. For example,\nresearchers hoping to deploy a large language model to produce non-toxic\ncontent may use a toxicity classifier to filter generated text. In practice,\nthe generated text to classify, which is determined by user prompts, may come\nfrom a wide range of distributions. In this paper, we show that the performance\nof controlled generation may be poor if the distributions of text in response\nto user prompts differ from the distribution the predictor was trained on. To\naddress this problem, we cast controlled generation under distribution shift as\nan invariant learning problem: the most effective predictor should be invariant\nacross multiple text environments. We then discuss a natural solution that\narises from this characterization and propose heuristics for selecting natural\nenvironments. We study this characterization and the proposed method\nempirically using both synthetic and real data. Experiments demonstrate both\nthe challenge of distribution shift in controlled generation and the potential\nof invariance methods in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2306.00198v1.pdf"
    },
    {
        "title": "Automated Annotation with Generative AI Requires Validation",
        "authors": [
            "Nicholas Pangakis",
            "Samuel Wolken",
            "Neil Fasching"
        ],
        "published": "2023-05-31T20:50:45Z",
        "summary": "Generative large language models (LLMs) can be a powerful tool for augmenting\ntext annotation procedures, but their performance varies across annotation\ntasks due to prompt quality, text data idiosyncrasies, and conceptual\ndifficulty. Because these challenges will persist even as LLM technology\nimproves, we argue that any automated annotation process using an LLM must\nvalidate the LLM's performance against labels generated by humans. To this end,\nwe outline a workflow to harness the annotation potential of LLMs in a\nprincipled, efficient way. Using GPT-4, we validate this approach by\nreplicating 27 annotation tasks across 11 datasets from recent social science\narticles in high-impact journals. We find that LLM performance for text\nannotation is promising but highly contingent on both the dataset and the type\nof annotation task, which reinforces the necessity to validate on a\ntask-by-task basis. We make available easy-to-use software designed to\nimplement our workflow and streamline the deployment of LLMs for automated\nannotation.",
        "pdf_link": "https://arxiv.org/pdf/2306.00176v1.pdf"
    },
    {
        "title": "Measuring the Robustness of NLP Models to Domain Shifts",
        "authors": [
            "Nitay Calderon",
            "Naveh Porat",
            "Eyal Ben-David",
            "Alexander Chapanin",
            "Zorik Gekhman",
            "Nadav Oved",
            "Vitaly Shalumov",
            "Roi Reichart"
        ],
        "published": "2023-05-31T20:25:08Z",
        "summary": "Existing research on Domain Robustness (DR) suffers from disparate setups,\nlack of task variety, and scarce research on recent capabilities such as\nfew-shot learning. Furthermore, we claim that the common practice of measuring\nDR might further obscure the picture. Current research focuses on challenge\nsets and relies solely on the Source Drop (SD): Using the source in-domain\nperformance as a reference point for degradation. However, the Target Drop\n(TD), which measures degradation from the target in-domain performance, should\nbe used as a complementary point of view. In this study, we developed a\nbenchmark comprised of seven NLP tasks, including classification, QA, and\ngeneration. Our benchmark focuses on natural topical domain shifts and enables\nmeasuring both the SD and the TD. Our comprehensive study, involving over\n14,000 domain shifts across 18 fine-tuned and few-shot models, shows that both\nmodel types suffer from drops upon domain shifts. While fine-tuned models excel\nin-domain, few-shot LLMs often surpass them cross-domain, showing better\nrobustness. In addition, we found that a large SD can be explained by shifting\nto a harder domain rather than by a genuine DR challenge. Thus, the TD is a\nmore reliable metric for assessing DR.",
        "pdf_link": "https://arxiv.org/pdf/2306.00168v4.pdf"
    },
    {
        "title": "Better patching using LLM prompting, via Self-Consistency",
        "authors": [
            "Toufique Ahmed",
            "Premkumar Devanbu"
        ],
        "published": "2023-05-31T18:28:46Z",
        "summary": "Large Language models (LLMs) can be induced to solve non-trivial problems\nwith \"few-shot\" prompts including illustrative problem-solution examples. Now\nif the few-shots also include \"chain of thought\" (CoT) explanations, which are\nof the form problem-explanation-solution, LLMs will generate a \"explained\"\nsolution, and perform even better. Recently an exciting, substantially better\ntechnique, self-consistency [1] (S-C) has emerged, based on the intuition that\nthere are many plausible explanations for the right solution; when the LLM is\nsampled repeatedly to generate a pool of explanation-solution pairs, for a\ngiven problem, the most frequently occurring solutions in the pool (ignoring\nthe explanations) tend to be even more likely to be correct! Unfortunately, the\nuse of this highly-performant S-C (or even CoT) approach in software\nengineering settings is hampered by the lack of explanations; most software\ndatasets lack explanations. In this paper, we describe an application of the\nS-C approach to program repair, using the commit log on the fix as the\nexplanation, only in the illustrative few-shots. We achieve state-of-the art\nresults, beating previous approaches to prompting-based program repair, on the\nMODIT dataset; we also find evidence suggesting that the correct commit\nmessages are helping the LLM learn to produce better patches.",
        "pdf_link": "https://arxiv.org/pdf/2306.00108v2.pdf"
    },
    {
        "title": "Improving CLIP Training with Language Rewrites",
        "authors": [
            "Lijie Fan",
            "Dilip Krishnan",
            "Phillip Isola",
            "Dina Katabi",
            "Yonglong Tian"
        ],
        "published": "2023-05-31T17:59:04Z",
        "summary": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.",
        "pdf_link": "https://arxiv.org/pdf/2305.20088v2.pdf"
    },
    {
        "title": "Scaling Evidence-based Instructional Design Expertise through Large Language Models",
        "authors": [
            "Gautam Yadav"
        ],
        "published": "2023-05-31T17:54:07Z",
        "summary": "This paper presents a comprehensive exploration of leveraging Large Language\nModels (LLMs), specifically GPT-4, in the field of instructional design. With a\nfocus on scaling evidence-based instructional design expertise, our research\naims to bridge the gap between theoretical educational studies and practical\nimplementation. We discuss the benefits and limitations of AI-driven content\ngeneration, emphasizing the necessity of human oversight in ensuring the\nquality of educational materials. This work is elucidated through two detailed\ncase studies where we applied GPT-4 in creating complex higher-order\nassessments and active learning components for different courses. From our\nexperiences, we provide best practices for effectively using LLMs in\ninstructional design tasks, such as utilizing templates, fine-tuning, handling\nunexpected output, implementing LLM chains, citing references, evaluating\noutput, creating rubrics, grading, and generating distractors. We also share\nour vision of a future recommendation system, where a customized GPT-4 extracts\ninstructional design principles from educational studies and creates\npersonalized, evidence-supported strategies for users' unique educational\ncontexts. Our research contributes to understanding and optimally harnessing\nthe potential of AI-driven language models in enhancing educational outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2306.01006v2.pdf"
    },
    {
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "authors": [
            "Jessy Lin",
            "Nicholas Tomlin",
            "Jacob Andreas",
            "Jason Eisner"
        ],
        "published": "2023-05-31T17:50:02Z",
        "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI\nassistants must collaborate with one or more humans via natural language to\nhelp them make complex decisions. We formalize three domains in which users\nface everyday decisions: (1) choosing an assignment of reviewers to conference\npapers, (2) planning a multi-step itinerary in a city, and (3) negotiating\ntravel plans for a group of friends. In each of these settings, AI assistants\nand users have disparate abilities that they must combine to arrive at the best\ndecision: assistants can access and process large amounts of information, while\nusers have preferences and constraints external to the system. For each task,\nwe build a dialogue environment where agents receive a reward based on the\nquality of the final decision they reach. Using these environments, we collect\nhuman-human dialogues with humans playing the role of assistant. To compare how\ncurrent AI assistants communicate in these settings, we present baselines using\nlarge language models in self-play. Finally, we highlight a number of\nchallenges models face in decision-oriented dialogues, ranging from efficient\ncommunication to reasoning and optimization, and release our environments as a\ntestbed for future modeling work.",
        "pdf_link": "https://arxiv.org/pdf/2305.20076v2.pdf"
    },
    {
        "title": "Let's Verify Step by Step",
        "authors": [
            "Hunter Lightman",
            "Vineet Kosaraju",
            "Yura Burda",
            "Harri Edwards",
            "Bowen Baker",
            "Teddy Lee",
            "Jan Leike",
            "John Schulman",
            "Ilya Sutskever",
            "Karl Cobbe"
        ],
        "published": "2023-05-31T17:24:00Z",
        "summary": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.",
        "pdf_link": "https://arxiv.org/pdf/2305.20050v1.pdf"
    },
    {
        "title": "Supplementary Features of BiLSTM for Enhanced Sequence Labeling",
        "authors": [
            "Conglei Xu",
            "Kun Shen",
            "Hongguang Sun"
        ],
        "published": "2023-05-31T15:05:25Z",
        "summary": "Sequence labeling tasks require the computation of sentence representations\nfor each word within a given sentence. A prevalent method incorporates a\nBi-directional Long Short-Term Memory (BiLSTM) layer to enhance the sequence\nstructure information. However, empirical evidence Li (2020) suggests that the\ncapacity of BiLSTM to produce sentence representations for sequence labeling\ntasks is inherently limited. This limitation primarily results from the\nintegration of fragments from past and future sentence representations to\nformulate a complete sentence representation. In this study, we observed that\nthe entire sentence representation, found in both the first and last cells of\nBiLSTM, can supplement each the individual sentence representation of each\ncell. Accordingly, we devised a global context mechanism to integrate entire\nfuture and past sentence representations into each cell's sentence\nrepresentation within the BiLSTM framework. By incorporating the BERT model\nwithin BiLSTM as a demonstration, and conducting exhaustive experiments on nine\ndatasets for sequence labeling tasks, including named entity recognition (NER),\npart of speech (POS) tagging, and End-to-End Aspect-Based sentiment analysis\n(E2E-ABSA). We noted significant improvements in F1 scores and accuracy across\nall examined datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.19928v4.pdf"
    },
    {
        "title": "Revisiting the Reliability of Psychological Scales on Large Language Models",
        "authors": [
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Man Ho Lam",
            "Eric John Li",
            "Wenxiang Jiao",
            "Michael R. Lyu"
        ],
        "published": "2023-05-31T15:03:28Z",
        "summary": "Recent research has extended beyond assessing the performance of Large\nLanguage Models (LLMs) to examining their characteristics from a psychological\nstandpoint, acknowledging the necessity of understanding their behavioral\ncharacteristics. The administration of personality tests to LLMs has emerged as\na noteworthy area in this context. However, the suitability of employing\npsychological scales, initially devised for humans, on LLMs is a matter of\nongoing debate. Our study aims to determine the reliability of applying\npersonality assessments to LLMs, explicitly investigating whether LLMs\ndemonstrate consistent personality traits. Analyzing responses under 2,500\nsettings reveals that gpt-3.5-turbo shows consistency in responses to the Big\nFive Inventory, indicating a high degree of reliability. Furthermore, our\nresearch explores the potential of gpt-3.5-turbo to emulate diverse\npersonalities and represent various groups, which is a capability increasingly\nsought after in social sciences for substituting human participants with LLMs\nto reduce costs. Our findings reveal that LLMs have the potential to represent\ndifferent personalities with specific prompt instructions. By shedding light on\nthe personalization of LLMs, our study endeavors to pave the way for future\nexplorations in this field. We have made our experimental results and the\ncorresponding code openly accessible via\nhttps://github.com/CUHK-ARISE/LLMPersonality.",
        "pdf_link": "https://arxiv.org/pdf/2305.19926v3.pdf"
    },
    {
        "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
        "authors": [
            "Alex Foote",
            "Neel Nanda",
            "Esben Kran",
            "Ioannis Konstas",
            "Shay Cohen",
            "Fazl Barez"
        ],
        "published": "2023-05-31T14:44:33Z",
        "summary": "Advances in Large Language Models (LLMs) have led to remarkable capabilities,\nyet their inner mechanisms remain largely unknown. To understand these models,\nwe need to unravel the functions of individual neurons and their contribution\nto the network. This paper introduces a novel automated approach designed to\nscale interpretability techniques across a vast array of neurons within LLMs,\nto make them more interpretable and ultimately safe. Conventional methods\nrequire examination of examples with strong neuron activation and manual\nidentification of patterns to decipher the concepts a neuron responds to. We\npropose Neuron to Graph (N2G), an innovative tool that automatically extracts a\nneuron's behaviour from the dataset it was trained on and translates it into an\ninterpretable graph. N2G uses truncation and saliency methods to emphasise only\nthe most pertinent tokens to a neuron while enriching dataset examples with\ndiverse samples to better encompass the full spectrum of neuron behaviour.\nThese graphs can be visualised to aid researchers' manual interpretation, and\ncan generate token activations on text for automatic validation by comparison\nwith the neuron's ground truth activations, which we use to show that the model\nis better at predicting neuron activation than two baseline methods. We also\ndemonstrate how the generated graph representations can be flexibly used to\nfacilitate further automation of interpretability research, by searching for\nneurons with particular properties, or programmatically comparing neurons to\neach other to identify similar neurons. Our method easily scales to build graph\nrepresentations for all neurons in a 6-layer Transformer model using a single\nTesla T4 GPU, allowing for wide usability. We release the code and instructions\nfor use at https://github.com/alexjfoote/Neuron2Graph.",
        "pdf_link": "https://arxiv.org/pdf/2305.19911v1.pdf"
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "authors": [
            "Likang Wu",
            "Zhi Zheng",
            "Zhaopeng Qiu",
            "Hao Wang",
            "Hongchao Gu",
            "Tingjia Shen",
            "Chuan Qin",
            "Chen Zhu",
            "Hengshu Zhu",
            "Qi Liu",
            "Hui Xiong",
            "Enhong Chen"
        ],
        "published": "2023-05-31T13:51:26Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration. We have also created a GitHub repository to\nindex relevant papers on LLMs for recommendation,\nhttps://github.com/WLiK/LLM4Rec.",
        "pdf_link": "https://arxiv.org/pdf/2305.19860v4.pdf"
    },
    {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-05-31T10:08:37Z",
        "summary": "The prevalence and strong capability of large language models (LLMs) present\nsignificant safety and ethical risks if exploited by malicious users. To\nprevent the potentially deceptive usage of LLMs, recent works have proposed\nalgorithms to detect LLM-generated text and protect LLMs. In this paper, we\ninvestigate the robustness and reliability of these LLM detectors under\nadversarial attacks. We study two types of attack strategies: 1) replacing\ncertain words in an LLM's output with their synonyms given the context; 2)\nautomatically searching for an instructional prompt to alter the writing style\nof the generation. In both strategies, we leverage an auxiliary LLM to generate\nthe word replacements or the instructional prompt. Different from previous\nworks, we consider a challenging setting where the auxiliary LLM can also be\nprotected by a detector. Experiments reveal that our attacks effectively\ncompromise the performance of all detectors in the study with plausible\ngenerations, underscoring the urgent need to improve the robustness of\nLLM-generated text detection systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.19713v2.pdf"
    },
    {
        "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM",
        "authors": [
            "Nghi D. Q. Bui",
            "Hung Le",
            "Yue Wang",
            "Junnan Li",
            "Akhilesh Deepak Gotmare",
            "Steven C. H. Hoi"
        ],
        "published": "2023-05-31T05:24:48Z",
        "summary": "Code intelligence plays a key role in transforming modern software\nengineering. Recently, deep learning-based models, especially Transformer-based\nlarge language models (LLMs), have demonstrated remarkable potential in\ntackling these tasks by leveraging massive open-source code data and\nprogramming language features. However, the development and deployment of such\nmodels often require expertise in both machine learning and software\nengineering, creating a barrier for the model adoption. In this paper, we\npresent CodeTF, an open-source Transformer-based library for state-of-the-art\nCode LLMs and code intelligence. Following the principles of modular design and\nextensible framework, we design CodeTF with a unified interface to enable rapid\naccess and development across different types of models, datasets and tasks.\nOur library supports a collection of pretrained Code LLM models and popular\ncode benchmarks, including a standardized interface to train and serve code\nLLMs efficiently, and data features such as language-specific parsers and\nutility functions for extracting code attributes. In this paper, we describe\nthe design principles, the architecture, key modules and components, and\ncompare with other related library tools. Finally, we hope CodeTF is able to\nbridge the gap between machine learning/generative AI and software engineering,\nproviding a comprehensive open-source solution for developers, researchers, and\npractitioners.",
        "pdf_link": "https://arxiv.org/pdf/2306.00029v1.pdf"
    },
    {
        "title": "Large Language Models Are Not Strong Abstract Reasoners",
        "authors": [
            "Ga\u00ebl Gendron",
            "Qiming Bao",
            "Michael Witbrock",
            "Gillian Dobbie"
        ],
        "published": "2023-05-31T04:50:29Z",
        "summary": "Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.19555v3.pdf"
    },
    {
        "title": "Catalysis distillation neural network for the few shot open catalyst challenge",
        "authors": [
            "Bowen Deng"
        ],
        "published": "2023-05-31T04:23:56Z",
        "summary": "The integration of artificial intelligence and science has resulted in\nsubstantial progress in computational chemistry methods for the design and\ndiscovery of novel catalysts. Nonetheless, the challenges of electrocatalytic\nreactions and developing a large-scale language model in catalysis persist, and\nthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)\nfew-shot methods surpassing BERT (Bidirectional Encoder Representation from\nTransformers) underscores the importance of addressing limited data, expensive\ncomputations, time constraints and structure-activity relationship in research.\nHence, the development of few-shot techniques for catalysis is critical and\nessential, regardless of present and future requirements. This paper introduces\nthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the\napplication of machine learning technology for predicting catalytic reactions\non catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen\nperoxide electrocatalysis. To address the challenge of limited data in\ncatalysis, we propose a machine learning approach based on MLP-Like and a\nframework called Catalysis Distillation Graph Neural Network (CDGNN). Our\nresults demonstrate that CDGNN effectively learns embeddings from catalytic\nstructures, enabling the capture of structure-adsorption relationships. This\naccomplishment has resulted in the utmost advanced and efficient determination\nof the reaction pathway for hydrogen peroxide, surpassing the current graph\nneural network approach by 16.1%.. Consequently, CDGNN presents a promising\napproach for few-shot learning in catalysis.",
        "pdf_link": "https://arxiv.org/pdf/2305.19545v1.pdf"
    },
    {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
        "authors": [
            "Xiaoxin He",
            "Xavier Bresson",
            "Thomas Laurent",
            "Adam Perold",
            "Yann LeCun",
            "Bryan Hooi"
        ],
        "published": "2023-05-31T03:18:03Z",
        "summary": "Representation learning on text-attributed graphs (TAGs) has become a\ncritical research problem in recent years. A typical example of a TAG is a\npaper citation graph, where the text of each paper serves as node attributes.\nInitial graph neural network (GNN) pipelines handled these text attributes by\ntransforming them into shallow or hand-crafted features, such as skip-gram or\nbag-of-words features. Recent efforts have focused on enhancing these pipelines\nwith language models (LMs), which typically demand intricate designs and\nsubstantial computational resources. With the advent of powerful large language\nmodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and\nto utilize general knowledge, there is a growing need for techniques which\ncombine the textual modelling abilities of LLMs with the structural learning\ncapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to\ncapture textual information as features, which can be used to boost GNN\nperformance on downstream tasks. A key innovation is our use of explanations as\nfeatures: we prompt an LLM to perform zero-shot classification, request textual\nexplanations for its decision-making process, and design an LLM-to-LM\ninterpreter to translate these explanations into informative features for\ndownstream GNNs. Our experiments demonstrate that our method achieves\nstate-of-the-art results on well-established TAG datasets, including Cora,\nPubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23.\nFurthermore, our method significantly speeds up training, achieving a 2.88\ntimes improvement over the closest baseline on ogbn-arxiv. Lastly, we believe\nthe versatility of the proposed method extends beyond TAGs and holds the\npotential to enhance other tasks involving graph-text data. Our codes and\ndatasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.19523v5.pdf"
    },
    {
        "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning",
        "authors": [
            "Faeze Brahman",
            "Chandra Bhagavatula",
            "Valentina Pyatkin",
            "Jena D. Hwang",
            "Xiang Lorraine Li",
            "Hirona J. Arai",
            "Soumya Sanyal",
            "Keisuke Sakaguchi",
            "Xiang Ren",
            "Yejin Choi"
        ],
        "published": "2023-05-31T00:55:40Z",
        "summary": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex contextualized situations that are often counterfactual, e.g.\n\"scheduling a doctor's appointment without a phone\". While current approaches\nshow encouraging results using large language models (LLMs), they are hindered\nby drawbacks such as costly API calls and reproducibility issues. In this\npaper, we advocate planning using smaller language models. We present PlaSma, a\nnovel two-pronged approach to endow small language models with procedural\nknowledge and (counterfactual) planning capabilities. More concretely, we\ndevelop symbolic procedural knowledge distillation to enhance the implicit\nknowledge in small language models and an inference-time algorithm to\nfacilitate more structured and accurate reasoning. In addition, we introduce a\nnovel task, Counterfactual Planning, that requires a revision of a plan to cope\nwith a counterfactual situation. In both the original and counterfactual\nsetting, we show that orders-of-magnitude smaller models (770M-11B parameters)\ncan compete and often surpass their larger teacher models' capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.19472v2.pdf"
    },
    {
        "title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
        "authors": [
            "Zelalem Gero",
            "Chandan Singh",
            "Hao Cheng",
            "Tristan Naumann",
            "Michel Galley",
            "Jianfeng Gao",
            "Hoifung Poon"
        ],
        "published": "2023-05-30T22:05:11Z",
        "summary": "Extracting patient information from unstructured text is a critical task in\nhealth decision-support and clinical research. Large language models (LLMs)\nhave shown the potential to accelerate clinical curation via few-shot\nin-context learning, in contrast to supervised learning which requires much\nmore costly human annotations. However, despite drastic advances in modern LLMs\nsuch as GPT-4, they still struggle with issues regarding accuracy and\ninterpretability, especially in mission-critical domains such as health. Here,\nwe explore a general mitigation framework using self-verification, which\nleverages the LLM to provide provenance for its own extraction and check its\nown outputs. This is made possible by the asymmetry between verification and\ngeneration, where the latter is often much easier than the former. Experimental\nresults show that our method consistently improves accuracy for various LLMs in\nstandard clinical information extraction tasks. Additionally, self-verification\nyields interpretations in the form of a short text span corresponding to each\noutput, which makes it very efficient for human experts to audit the results,\npaving the way towards trustworthy extraction of clinical information in\nresource-constrained scenarios. To facilitate future research in this\ndirection, we release our code and prompts.",
        "pdf_link": "https://arxiv.org/pdf/2306.00024v1.pdf"
    },
    {
        "title": "Stable Anisotropic Regularization",
        "authors": [
            "William Rudman",
            "Carsten Eickhoff"
        ],
        "published": "2023-05-30T18:57:45Z",
        "summary": "Given the success of Large Language Models (LLMs), there has been\nconsiderable interest in studying the properties of model activations. The\nliterature overwhelmingly agrees that LLM representations are dominated by a\nfew \"outlier dimensions\" with exceedingly high variance and magnitude. Several\nstudies in Natural Language Processing (NLP) have sought to mitigate the impact\nof such outlier dimensions and force LLMs to be isotropic (i.e., have uniform\nvariance across all dimensions in embedding space). Isotropy is thought to be a\ndesirable property for LLMs that improves model performance and more closely\naligns textual representations with human intuition. However, many of the\nclaims regarding isotropy in NLP have been based on the average cosine\nsimilarity of embeddings, which has recently been shown to be a flawed measure\nof isotropy. In this paper, we propose I-STAR: IsoScore*-based STable\nAnisotropic Regularization, a novel regularization method that can be used to\nincrease or decrease levels of isotropy in embedding space during training.\nI-STAR uses IsoScore*, the first accurate measure of isotropy that is both\ndifferentiable and stable on mini-batch computations. In contrast to several\nprevious works, we find that decreasing isotropy in contextualized embeddings\nimproves performance on the majority of tasks and models considered in this\npaper.",
        "pdf_link": "https://arxiv.org/pdf/2305.19358v3.pdf"
    },
    {
        "title": "GPT4GEO: How a Language Model Sees the World's Geography",
        "authors": [
            "Jonathan Roberts",
            "Timo L\u00fcddecke",
            "Sowmen Das",
            "Kai Han",
            "Samuel Albanie"
        ],
        "published": "2023-05-30T18:28:04Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities across a\nbroad range of tasks involving question answering and the generation of\ncoherent text and code. Comprehensively understanding the strengths and\nweaknesses of LLMs is beneficial for safety, downstream applications and\nimproving performance. In this work, we investigate the degree to which GPT-4\nhas acquired factual geographic knowledge and is capable of using this\nknowledge for interpretative reasoning, which is especially important for\napplications that involve geographic data, such as geospatial analysis, supply\nchain management, and disaster response. To this end, we design and conduct a\nseries of diverse experiments, starting from factual tasks such as location,\ndistance and elevation estimation to more complex questions such as generating\ncountry outlines and travel networks, route finding under constraints and\nsupply chain analysis. We provide a broad characterisation of what GPT-4\n(without plugins or Internet access) knows about the world, highlighting both\npotentially surprising capabilities but also limitations.",
        "pdf_link": "https://arxiv.org/pdf/2306.00020v1.pdf"
    },
    {
        "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models",
        "authors": [
            "Hongxin Li",
            "Jingran Su",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "published": "2023-05-30T17:59:30Z",
        "summary": "Computer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.19308v2.pdf"
    },
    {
        "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
        "authors": [
            "Bailin Wang",
            "Zi Wang",
            "Xuezhi Wang",
            "Yuan Cao",
            "Rif A. Saurous",
            "Yoon Kim"
        ],
        "published": "2023-05-30T17:26:01Z",
        "summary": "Large language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for\ngenerating strings from highly structured languages (e.g., semantic parsing to\ncomplex domain-specific languages), it is challenging for the LLM to generalize\nfrom just a few exemplars. We propose \\emph{grammar prompting}, a simple\napproach to enable LLMs to use external knowledge and domain-specific\nconstraints, expressed through a grammar in Backus--Naur Form (BNF), during\nin-context learning. Grammar prompting augments each demonstration example with\na specialized grammar that is minimally sufficient for generating the\nparticular output example, where the specialized grammar is a subset of the\nfull DSL grammar. For inference, the LLM first predicts a BNF grammar given a\ntest input, and then generates the output according to the rules of the\ngrammar. Experiments demonstrate that grammar prompting can enable LLMs to\nperform competitively on a diverse set of DSL generation tasks, including\nsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and\nSMILES-based molecule generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.19234v3.pdf"
    },
    {
        "title": "Controlled Text Generation with Hidden Representation Transformations",
        "authors": [
            "Vaibhav Kumar",
            "Hana Koorehdavoudi",
            "Masud Moshtaghi",
            "Amita Misra",
            "Ankit Chadha",
            "Emilio Ferrara"
        ],
        "published": "2023-05-30T17:21:17Z",
        "summary": "We propose CHRT (Control Hidden Representation Transformation) - a controlled\nlanguage generation framework that steers large language models to generate\ntext pertaining to certain attributes (such as toxicity). CHRT gains attribute\ncontrol by modifying the hidden representation of the base model through\nlearned transformations. We employ a contrastive-learning framework to learn\nthese transformations that can be combined to gain multi-attribute control. The\neffectiveness of CHRT is experimentally shown by comparing it with seven\nbaselines over three attributes. CHRT outperforms all the baselines in the task\nof detoxification, positive sentiment steering, and text simplification while\nminimizing the loss in linguistic qualities. Further, our approach has the\nlowest inference latency of only 0.01 seconds more than the base model, making\nit the most suitable for high-performance production environments. We\nopen-source our code and release two novel datasets to further propel\ncontrolled language generation research.",
        "pdf_link": "https://arxiv.org/pdf/2305.19230v2.pdf"
    },
    {
        "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
        "authors": [
            "Xiao Liu",
            "Da Yin",
            "Chen Zhang",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2023-05-30T17:02:58Z",
        "summary": "Causal reasoning, the ability to identify cause-and-effect relationship, is\ncrucial in human thinking. Although large language models (LLMs) succeed in\nmany NLP tasks, it is still challenging for them to conduct complex causal\nreasoning like abductive reasoning and counterfactual reasoning. Given the fact\nthat programming code may express causal relations more often and explicitly\nwith conditional statements like ``if``, we want to explore whether Code-LLMs\nacquire better causal reasoning abilities. Our experiments show that compared\nto text-only LLMs, Code-LLMs with code prompts are significantly better in\ncausal reasoning. We further intervene on the prompts from different aspects,\nand discover that the programming structure is crucial in code prompt design,\nwhile Code-LLMs are robust towards format perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2305.19213v1.pdf"
    },
    {
        "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
        "authors": [
            "Zhen Lin",
            "Shubhendu Trivedi",
            "Jimeng Sun"
        ],
        "published": "2023-05-30T16:31:26Z",
        "summary": "Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for black-box LLMs. We first differentiate\nuncertainty vs confidence: the former refers to the \"dispersion\" of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty metrics, applying them to selective NLG where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple metric for the\nsemantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.",
        "pdf_link": "https://arxiv.org/pdf/2305.19187v2.pdf"
    },
    {
        "title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images",
        "authors": [
            "Viraj Prabhu",
            "Sriram Yenamandra",
            "Prithvijit Chattopadhyay",
            "Judy Hoffman"
        ],
        "published": "2023-05-30T16:09:16Z",
        "summary": "We propose an automated algorithm to stress-test a trained visual model by\ngenerating language-guided counterfactual test images (LANCE). Our method\nleverages recent progress in large language modeling and text-based image\nediting to augment an IID test set with a suite of diverse, realistic, and\nchallenging test images without altering model weights. We benchmark the\nperformance of a diverse set of pre-trained models on our generated data and\nobserve significant and consistent performance drops. We further analyze model\nsensitivity across different types of edits, and demonstrate its applicability\nat surfacing previously unknown class-level model biases in ImageNet. Code is\navailable at https://github.com/virajprabhu/lance.",
        "pdf_link": "https://arxiv.org/pdf/2305.19164v2.pdf"
    },
    {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "published": "2023-05-30T15:25:45Z",
        "summary": "Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Codes:\nhttps://github.com/Skytliang/Multi-Agents-Debate",
        "pdf_link": "https://arxiv.org/pdf/2305.19118v1.pdf"
    },
    {
        "title": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-05-30T15:15:40Z",
        "summary": "Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
        "pdf_link": "https://arxiv.org/pdf/2306.00017v4.pdf"
    },
    {
        "title": "Does Conceptual Representation Require Embodiment? Insights From Large Language Models",
        "authors": [
            "Qihui Xu",
            "Yingying Peng",
            "Samuel A. Nastase",
            "Martin Chodorow",
            "Minghua Wu",
            "Ping Li"
        ],
        "published": "2023-05-30T15:06:28Z",
        "summary": "To what extent can language alone give rise to complex concepts, or is\nembodied experience essential? Recent advancements in large language models\n(LLMs) offer fresh perspectives on this question. Although LLMs are trained on\nrestricted modalities, they exhibit human-like performance in diverse\npsychological tasks. Our study compared representations of 4,442 lexical\nconcepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple\ndimensions, including five key domains: emotion, salience, mental\nvisualization, sensory, and motor experience. We identify two main findings: 1)\nBoth models strongly align with human representations in non-sensorimotor\ndomains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;\n2) GPT-4's gains are associated with its additional visual learning, which also\nappears to benefit related dimensions like haptics and imageability. These\nresults highlight the limitations of language in isolation, and that the\nintegration of diverse modalities of inputs leads to a more human-like\nconceptual representation.",
        "pdf_link": "https://arxiv.org/pdf/2305.19103v3.pdf"
    },
    {
        "title": "GPT Models in Construction Industry: Opportunities, Limitations, and a Use Case Validation",
        "authors": [
            "Abdullahi Saka",
            "Ridwan Taiwo",
            "Nurudeen Saka",
            "Babatunde Salami",
            "Saheed Ajayi",
            "Kabiru Akande",
            "Hadi Kazemi"
        ],
        "published": "2023-05-30T12:50:51Z",
        "summary": "Large Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry.",
        "pdf_link": "https://arxiv.org/pdf/2305.18997v1.pdf"
    },
    {
        "title": "Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",
        "authors": [
            "Vagelis Plevris",
            "George Papazafeiropoulos",
            "Alejandro Jim\u00e9nez Rios"
        ],
        "published": "2023-05-30T11:18:05Z",
        "summary": "A comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world.",
        "pdf_link": "https://arxiv.org/pdf/2305.18618v1.pdf"
    },
    {
        "title": "Multitask learning for recognizing stress and depression in social media",
        "authors": [
            "Loukas Ilias",
            "Dimitris Askounis"
        ],
        "published": "2023-05-30T10:04:01Z",
        "summary": "Stress and depression are prevalent nowadays across people of all ages due to\nthe quick paces of life. People use social media to express their feelings.\nThus, social media constitute a valuable form of information for the early\ndetection of stress and depression. Although many research works have been\nintroduced targeting the early recognition of stress and depression, there are\nstill limitations. There have been proposed multi-task learning settings, which\nuse depression and emotion (or figurative language) as the primary and\nauxiliary tasks respectively. However, although stress is inextricably linked\nwith depression, researchers face these two tasks as two separate tasks. To\naddress these limitations, we present the first study, which exploits two\ndifferent datasets collected under different conditions, and introduce two\nmultitask learning frameworks, which use depression and stress as the main and\nauxiliary tasks respectively. Specifically, we use a depression dataset and a\nstressful dataset including stressful posts from ten subreddits of five\ndomains. In terms of the first approach, each post passes through a shared BERT\nlayer, which is updated by both tasks. Next, two separate BERT encoder layers\nare exploited, which are updated by each task separately. Regarding the second\napproach, it consists of shared and task-specific layers weighted by attention\nfusion networks. We conduct a series of experiments and compare our approaches\nwith existing research initiatives, single-task learning, and transfer\nlearning. Experiments show multiple advantages of our approaches over\nstate-of-the-art ones.",
        "pdf_link": "https://arxiv.org/pdf/2305.18907v2.pdf"
    },
    {
        "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation",
        "authors": [
            "Chuhao Jin",
            "Wenhui Tan",
            "Jiange Yang",
            "Bei Liu",
            "Ruihua Song",
            "Limin Wang",
            "Jianlong Fu"
        ],
        "published": "2023-05-30T09:54:20Z",
        "summary": "We propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
        "pdf_link": "https://arxiv.org/pdf/2305.18898v1.pdf"
    },
    {
        "title": "Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge",
        "authors": [
            "Xingyu Fu",
            "Sheng Zhang",
            "Gukyeong Kwon",
            "Pramuditha Perera",
            "Henghui Zhu",
            "Yuhao Zhang",
            "Alexander Hanbo Li",
            "William Yang Wang",
            "Zhiguo Wang",
            "Vittorio Castelli",
            "Patrick Ng",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023-05-30T08:34:13Z",
        "summary": "The open-ended Visual Question Answering (VQA) task requires AI models to\njointly reason over visual and natural language inputs using world knowledge.\nRecently, pre-trained Language Models (PLM) such as GPT-3 have been applied to\nthe task and shown to be powerful world knowledge sources. However, these\nmethods suffer from low knowledge coverage caused by PLM bias -- the tendency\nto generate certain tokens over other tokens regardless of prompt changes, and\nhigh dependency on the PLM quality -- only models using GPT-3 can achieve the\nbest result.\n  To address the aforementioned challenges, we propose RASO: a new VQA pipeline\nthat deploys a generate-then-select strategy guided by world knowledge for the\nfirst time. Rather than following the de facto standard to train a multi-modal\nmodel that directly generates the VQA answer, RASO first adopts PLM to generate\nall the possible answers, and then trains a lightweight answer selection model\nfor the correct answer. As proved in our analysis, RASO expands the knowledge\ncoverage from in-domain training data by a large margin. We provide extensive\nexperimentation and show the effectiveness of our pipeline by advancing the\nstate-of-the-art by 4.1% on OK-VQA, without additional computation cost. Code\nand models are released at http://cogcomp.org/page/publication_view/1010",
        "pdf_link": "https://arxiv.org/pdf/2305.18842v1.pdf"
    },
    {
        "title": "Universality and Limitations of Prompt Tuning",
        "authors": [
            "Yihan Wang",
            "Jatin Chauhan",
            "Wei Wang",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-05-30T06:47:07Z",
        "summary": "Despite the demonstrated empirical efficacy of prompt tuning to adapt a\npretrained language model for a new task, the theoretical underpinnings of the\ndifference between \"tuning parameters before the input\" against \"the tuning of\nmodel weights\" are limited. We thus take one of the first steps to understand\nthe role of soft-prompt tuning for transformer-based architectures. By\nconsidering a general purpose architecture, we analyze prompt tuning from the\nlens of both: universal approximation and limitations with finite-depth\nfixed-weight pretrained transformers for continuous-valued functions. Our\nuniversality result guarantees the existence of a strong transformer with a\nprompt to approximate any sequence-to-sequence function in the set of Lipschitz\nfunctions. The limitations of prompt tuning for limited-depth transformers are\nfirst proved by constructing a set of datasets, that cannot be memorized by a\nprompt of any length for a given single encoder layer. We also provide a lower\nbound on the required number of tunable prompt parameters and compare the\nresult with the number of parameters required for a low-rank update (based on\nLoRA) for a single-layer setting. We finally extend our analysis to multi-layer\nsettings by providing sufficient conditions under which the transformer can at\nbest learn datasets from invertible functions only. Our theoretical claims are\nalso corroborated by empirical results.",
        "pdf_link": "https://arxiv.org/pdf/2305.18787v2.pdf"
    },
    {
        "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
        "authors": [
            "Rui Yang",
            "Lin Song",
            "Yanwei Li",
            "Sijie Zhao",
            "Yixiao Ge",
            "Xiu Li",
            "Ying Shan"
        ],
        "published": "2023-05-30T05:27:21Z",
        "summary": "This paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs\nand publicly inaccessible data. To address these challenges, we propose the\nGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and\nOPT, to use tools. It generates an instruction-following dataset by prompting\nan advanced teacher with various multi-modal contexts. By using the Low-Rank\nAdaptation (LoRA) optimization, our approach facilitates the open-source LLMs\nto solve a range of visual problems, including visual comprehension and image\ngeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs to\nuse tools, which is performed in both zero-shot and fine-tuning ways. Extensive\nexperiments demonstrate the effectiveness of our method on various language\nmodels, which not only significantly improves the accuracy of invoking seen\ntools, but also enables the zero-shot capacity for unseen tools. The code and\ndemo are available at https://github.com/StevenGrove/GPT4Tools.",
        "pdf_link": "https://arxiv.org/pdf/2305.18752v1.pdf"
    },
    {
        "title": "AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity Matching using Adapter-tuning",
        "authors": [
            "John Bosco Mugeni",
            "Steven Lynden",
            "Toshiyuki Amagasa",
            "Akiyoshi Matono"
        ],
        "published": "2023-05-30T04:03:23Z",
        "summary": "Entity Matching (EM) involves identifying different data representations\nreferring to the same entity from multiple data sources and is typically\nformulated as a binary classification problem. It is a challenging problem in\ndata integration due to the heterogeneity of data representations.\nState-of-the-art solutions have adopted NLP techniques based on pre-trained\nlanguage models (PrLMs) via the fine-tuning paradigm, however, sequential\nfine-tuning of overparameterized PrLMs can lead to catastrophic forgetting,\nespecially in low-resource scenarios. In this study, we propose a\nparameter-efficient paradigm for fine-tuning PrLMs based on adapters, small\nneural networks encapsulated between layers of a PrLM, by optimizing only the\nadapter and classifier weights while the PrLMs parameters are frozen.\nAdapter-based methods have been successfully applied to multilingual speech\nproblems achieving promising results, however, the effectiveness of these\nmethods when applied to EM is not yet well understood, particularly for\ngeneralized EM with heterogeneous data. Furthermore, we explore using (i)\npre-trained adapters and (ii) invertible adapters to capture token-level\nlanguage representations and demonstrate their benefits for transfer learning\non the generalized EM benchmark. Our results show that our solution achieves\ncomparable or superior performance to full-scale PrLM fine-tuning and\nprompt-tuning baselines while utilizing a significantly smaller computational\nfootprint $\\approx 13\\%$ of the PrLM parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.18725v1.pdf"
    },
    {
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Jiaying Lu",
            "Chengyuan Deng",
            "Can Zheng",
            "Junxiang Wang",
            "Tanmoy Chowdhury",
            "Yun Li",
            "Hejie Cui",
            "Xuchao Zhang",
            "Tianjiao Zhao",
            "Amit Panalkar",
            "Dhagash Mehta",
            "Stefano Pasquali",
            "Wei Cheng",
            "Haoyu Wang",
            "Yanchi Liu",
            "Zhengzhang Chen",
            "Haifeng Chen",
            "Chris White",
            "Quanquan Gu",
            "Jian Pei",
            "Carl Yang",
            "Liang Zhao"
        ],
        "published": "2023-05-30T03:00:30Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.",
        "pdf_link": "https://arxiv.org/pdf/2305.18703v7.pdf"
    },
    {
        "title": "Faith and Fate: Limits of Transformers on Compositionality",
        "authors": [
            "Nouha Dziri",
            "Ximing Lu",
            "Melanie Sclar",
            "Xiang Lorraine Li",
            "Liwei Jiang",
            "Bill Yuchen Lin",
            "Peter West",
            "Chandra Bhagavatula",
            "Ronan Le Bras",
            "Jena D. Hwang",
            "Soumya Sanyal",
            "Sean Welleck",
            "Xiang Ren",
            "Allyson Ettinger",
            "Zaid Harchaoui",
            "Yejin Choi"
        ],
        "published": "2023-05-29T23:24:14Z",
        "summary": "Transformer large language models (LLMs) have sparked admiration for their\nexceptional performance on tasks that demand intricate multi-step reasoning.\nYet, these models simultaneously show failures on surprisingly trivial\nproblems. This begs the question: Are these errors incidental, or do they\nsignal more substantial limitations? In an attempt to demystify transformer\nLLMs, we investigate the limits of these models across three representative\ncompositional tasks -- multi-digit multiplication, logic grid puzzles, and a\nclassic dynamic programming problem. These tasks require breaking problems down\ninto sub-steps and synthesizing these steps into a precise answer. We formulate\ncompositional tasks as computation graphs to systematically quantify the level\nof complexity, and break down reasoning steps into intermediate sub-procedures.\nOur empirical findings suggest that transformer LLMs solve compositional tasks\nby reducing multi-step compositional reasoning into linearized subgraph\nmatching, without necessarily developing systematic problem-solving skills. To\nround off our empirical study, we provide theoretical arguments on abstract\nmulti-step reasoning problems that highlight how autoregressive generations'\nperformance can rapidly decay with\\,increased\\,task\\,complexity.",
        "pdf_link": "https://arxiv.org/pdf/2305.18654v3.pdf"
    },
    {
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "authors": [
            "Yi Wu",
            "Nan Jiang",
            "Hung Viet Pham",
            "Thibaud Lutellier",
            "Jordan Davis",
            "Lin Tan",
            "Petr Babkin",
            "Sameena Shah"
        ],
        "published": "2023-05-29T20:50:27Z",
        "summary": "Security vulnerability repair is a difficult task that is in dire need of\nautomation. Two groups of techniques have shown promise: (1) large code\nlanguage models (LLMs) that have been pre-trained on source code for tasks such\nas code completion, and (2) automated program repair (APR) techniques that use\ndeep learning (DL) models to automatically fix software bugs.\n  This paper is the first to study and compare Java vulnerability repair\ncapabilities of LLMs and DL-based APR models. The contributions include that we\n(1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder),\nfour fine-tuned LLMs, and four DL-based APR techniques on two real-world Java\nvulnerability benchmarks (Vul4J and VJBench), (2) design code transformations\nto address the training and test data overlapping threat to Codex, (3) create a\nnew Java vulnerability repair benchmark VJBench, and its transformed version\nVJBench-trans and (4) evaluate LLMs and APR techniques on the transformed\nvulnerabilities in VJBench-trans.\n  Our findings include that (1) existing LLMs and APR models fix very few Java\nvulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities.\n(2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing\ncapabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix\nmany Common Weakness Enumeration (CWE) types, such as CWE-325 Missing\ncryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes\n8.3 transformed vulnerabilities, outperforming all the other LLMs and APR\nmodels on transformed vulnerabilities. The results call for innovations to\nenhance automated Java vulnerability repair such as creating larger\nvulnerability repair training data, tuning LLMs with such data, and applying\ncode simplification transformation to facilitate vulnerability repair.",
        "pdf_link": "https://arxiv.org/pdf/2305.18607v2.pdf"
    },
    {
        "title": "Controllable Text-to-Image Generation with GPT-4",
        "authors": [
            "Tianjun Zhang",
            "Yi Zhang",
            "Vibhav Vineet",
            "Neel Joshi",
            "Xin Wang"
        ],
        "published": "2023-05-29T19:56:47Z",
        "summary": "Current text-to-image generation models often struggle to follow textual\ninstructions, especially the ones requiring spatial reasoning. On the other\nhand, Large Language Models (LLMs), such as GPT-4, have shown remarkable\nprecision in generating code snippets for sketching out text inputs\ngraphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide\nthe diffusion-based text-to-image pipelines with programmatic sketches\ngenerated by GPT-4, enhancing their abilities for instruction following.\nControl-GPT works by querying GPT-4 to write TikZ code, and the generated\nsketches are used as references alongside the text instructions for diffusion\nmodels (e.g., ControlNet) to generate photo-realistic images. One major\nchallenge to training our pipeline is the lack of a dataset containing aligned\ntext, images, and sketches. We address the issue by converting instance masks\nin existing datasets into polygons to mimic the sketches used at test time. As\na result, Control-GPT greatly boosts the controllability of image generation.\nIt establishes a new state-of-art on the spatial arrangement and object\npositioning generation and enhances users' control of object positions, sizes,\netc., nearly doubling the accuracy of prior models. Our work, as a first\nattempt, shows the potential for employing LLMs to enhance the performance in\ncomputer vision tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.18583v1.pdf"
    },
    {
        "title": "Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy",
        "authors": [
            "Pengfei Yu",
            "Heng Ji"
        ],
        "published": "2023-05-29T19:48:37Z",
        "summary": "Large Language Models~(LLMs) struggle with providing current information due\nto the outdated pre-training data. Existing methods for updating LLMs, such as\nknowledge editing and continual fine-tuning, have significant drawbacks in\ngeneralizability of new information and the requirements on structured updating\ncorpus. We identify the core challenge behind these drawbacks: the LM-logical\ndiscrepancy featuring the difference between language modeling probabilities\nand logical probabilities. To evaluate and address the core challenge, we\npropose a new task formulation of the information updating task that only\nrequires the provision of an unstructured updating corpus and evaluates the\nperformance of information updating on the generalizability to question-answer\npairs pertaining to the updating information. We further propose a novel and\neffective pipeline approach for the task, highlighting a self-prompting-based\nquestion-answer generation process and a associative distillation methods to\nbridge the LM-logical discrepancy. We develop two datasets for evaluation, one\nsourced from news articles published in March and April 2023, and the other\nfrom the Natural Questions benchmark. Experimental results demonstrate the\nsuperiority of our approach, significantly increasing the factual consistency\nscore (on a scale from 0 to 1) by up to 0.16. Furthermore, our method\neffectively mitigates forgetting utilizing a compact replay buffer with only\n2.3% of the training tokens.",
        "pdf_link": "https://arxiv.org/pdf/2305.18582v2.pdf"
    },
    {
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
        "authors": [
            "Rafael Rafailov",
            "Archit Sharma",
            "Eric Mitchell",
            "Stefano Ermon",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-05-29T17:57:46Z",
        "summary": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.",
        "pdf_link": "https://arxiv.org/pdf/2305.18290v2.pdf"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections",
        "authors": [
            "M. Jehanzeb Mirza",
            "Leonid Karlinsky",
            "Wei Lin",
            "Mateusz Kozinski",
            "Horst Possegger",
            "Rogerio Feris",
            "Horst Bischof"
        ],
        "published": "2023-05-29T17:56:35Z",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.18287v2.pdf"
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "authors": [
            "Yuhang Zang",
            "Wei Li",
            "Jun Han",
            "Kaiyang Zhou",
            "Chen Change Loy"
        ],
        "published": "2023-05-29T17:50:33Z",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "pdf_link": "https://arxiv.org/pdf/2305.18279v1.pdf"
    },
    {
        "title": "Beyond Confidence: Reliable Models Should Also Consider Atypicality",
        "authors": [
            "Mert Yuksekgonul",
            "Linjun Zhang",
            "James Zou",
            "Carlos Guestrin"
        ],
        "published": "2023-05-29T17:37:09Z",
        "summary": "While most machine learning models can provide confidence in their\npredictions, confidence is insufficient to understand a prediction's\nreliability. For instance, the model may have a low confidence prediction if\nthe input is not well-represented in the training dataset or if the input is\ninherently ambiguous. In this work, we investigate the relationship between how\natypical(rare) a sample or a class is and the reliability of a model's\npredictions. We first demonstrate that atypicality is strongly related to\nmiscalibration and accuracy. In particular, we empirically show that\npredictions for atypical inputs or atypical classes are more overconfident and\nhave lower accuracy. Using these insights, we show incorporating atypicality\nimproves uncertainty quantification and model performance for discriminative\nneural networks and large language models. In a case study, we show that using\natypicality improves the performance of a skin lesion classifier across\ndifferent skin tone groups without having access to the group attributes.\nOverall, we propose that models should use not only confidence but also\natypicality to improve uncertainty quantification and performance. Our results\ndemonstrate that simple post-hoc atypicality estimators can provide significant\nvalue.",
        "pdf_link": "https://arxiv.org/pdf/2305.18262v2.pdf"
    },
    {
        "title": "Do Language Models Know When They're Hallucinating References?",
        "authors": [
            "Ayush Agrawal",
            "Mirac Suzgun",
            "Lester Mackey",
            "Adam Tauman Kalai"
        ],
        "published": "2023-05-29T17:12:03Z",
        "summary": "State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.",
        "pdf_link": "https://arxiv.org/pdf/2305.18248v3.pdf"
    },
    {
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "authors": [
            "Myra Cheng",
            "Esin Durmus",
            "Dan Jurafsky"
        ],
        "published": "2023-05-29T16:29:22Z",
        "summary": "To recognize and mitigate harms from large language models (LLMs), we need to\nunderstand the prevalence and nuances of stereotypes in LLM outputs. Toward\nthis end, we present Marked Personas, a prompt-based method to measure\nstereotypes in LLMs for intersectional demographic groups without any lexicon\nor data labeling. Grounded in the sociolinguistic concept of markedness (which\ncharacterizes explicitly linguistically marked categories versus unmarked\ndefaults), our proposed method is twofold: 1) prompting an LLM to generate\npersonas, i.e., natural language descriptions, of the target demographic group\nalongside personas of unmarked, default groups; 2) identifying the words that\nsignificantly distinguish personas of the target group from corresponding\nunmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4\ncontain higher rates of racial stereotypes than human-written portrayals using\nthe same prompts. The words distinguishing personas of marked (non-white,\nnon-male) groups reflect patterns of othering and exoticizing these\ndemographics. An intersectional lens further reveals tropes that dominate\nportrayals of marginalized groups, such as tropicalism and the\nhypersexualization of minoritized women. These representational harms have\nconcerning implications for downstream applications like story generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.18189v1.pdf"
    },
    {
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
        "authors": [
            "Zhanming Jie",
            "Wei Lu"
        ],
        "published": "2023-05-29T16:01:40Z",
        "summary": "Chain-of-thought (CoT) prompting with large language models has proven\neffective in numerous natural language processing tasks, but designing prompts\nthat generalize well to diverse problem types can be challenging, especially in\nthe context of math word problem (MWP) solving. Additionally, it is common to\nhave a large amount of training data that have a better diversity coverage but\nCoT annotations are not available, which limits the use of supervised learning\ntechniques. To address these issues, we investigate two approaches to leverage\nthe training data in a few-shot prompting scenario: dynamic program prompting\nand program distillation. Our approach is largely inspired by Gao et al.,\n(2022), where they proposed to replace the CoT with the programs as the\nintermediate reasoning step. Such a prompting strategy allows us to accurately\nverify the answer correctness through program execution in MWP solving. Our\ndynamic program prompting involves annotating the training data by sampling\ncorrect programs from a large language model, while program distillation\ninvolves adapting a smaller model to the program-annotated training data. Our\nexperiments on three standard MWP datasets demonstrate the effectiveness of\nthese approaches, yielding significant improvements over previous baselines for\nprompting and fine-tuning. Our results suggest that leveraging a large amount\nof training data can improve the generalization ability of prompts and boost\nthe performance of fine-tuned small models in MWP solving.",
        "pdf_link": "https://arxiv.org/pdf/2305.18170v2.pdf"
    },
    {
        "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
        "authors": [
            "Amirhossein Abaskohi",
            "Sascha Rothe",
            "Yadollah Yaghoobzadeh"
        ],
        "published": "2023-05-29T15:59:51Z",
        "summary": "In recent years, there has been significant progress in developing\npre-trained language models for NLP. However, these models often struggle when\nfine-tuned on small datasets. To address this issue, researchers have proposed\nvarious adaptation approaches. Prompt-based tuning is arguably the most common\nway, especially for larger models. Previous research shows that adding\ncontrastive learning to prompt-based fine-tuning is effective as it helps the\nmodel generate embeddings that are more distinguishable between classes, and it\ncan also be more sample-efficient as the model learns from positive and\nnegative examples simultaneously. One of the most important components of\ncontrastive learning is data augmentation, but unlike computer vision,\neffective data augmentation for NLP is still challenging. This paper proposes\nLM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language\nModels, which leverages prompt-based few-shot paraphrasing using generative\nlanguage models, especially large language models such as GPT-3 and OPT-175B,\nfor data augmentation. Our experiments on multiple text classification\nbenchmarks show that this augmentation method outperforms other methods, such\nas easy data augmentation, back translation, and multiple templates.",
        "pdf_link": "https://arxiv.org/pdf/2305.18169v3.pdf"
    },
    {
        "title": "Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods",
        "authors": [
            "Mengsay Loem",
            "Masahiro Kaneko",
            "Sho Takase",
            "Naoaki Okazaki"
        ],
        "published": "2023-05-29T15:31:29Z",
        "summary": "Large-scale pre-trained language models such as GPT-3 have shown remarkable\nperformance across various natural language processing tasks. However, applying\nprompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks\nand their controllability remains underexplored. Controllability in GEC is\ncrucial for real-world applications, particularly in educational settings,\nwhere the ability to tailor feedback according to learner levels and specific\nerror types can significantly enhance the learning process. This paper\ninvestigates the performance and controllability of prompt-based methods with\nGPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact\nof task instructions and examples on GPT-3's output, focusing on controlling\naspects such as minimal edits, fluency edits, and learner levels. Our findings\ndemonstrate that GPT-3 could effectively perform GEC tasks, outperforming\nexisting supervised and unsupervised approaches. We also showed that GPT-3\ncould achieve controllability when appropriate task instructions and examples\nare given.",
        "pdf_link": "https://arxiv.org/pdf/2305.18156v1.pdf"
    },
    {
        "title": "Do Large Language Models Know What They Don't Know?",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023-05-29T15:30:13Z",
        "summary": "Large language models (LLMs) have a wealth of knowledge that allows them to\nexcel in various Natural Language Processing (NLP) tasks. Current research\nfocuses on enhancing their performance within their existing knowledge. Despite\ntheir vast knowledge, LLMs are still limited by the amount of information they\ncan accommodate and comprehend. Therefore, the ability to understand their own\nlimitations on the unknows, referred to as self-knowledge, is of paramount\nimportance. This study aims to evaluate LLMs' self-knowledge by assessing their\nability to identify unanswerable or unknowable questions. We introduce an\nautomated methodology to detect uncertainty in the responses of these models,\nproviding a novel measure of their self-knowledge. We further introduce a\nunique dataset, SelfAware, consisting of unanswerable questions from five\ndiverse categories and their answerable counterparts. Our extensive analysis,\ninvolving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an\nintrinsic capacity for self-knowledge within these models. Moreover, we\ndemonstrate that in-context learning and instruction tuning can further enhance\nthis self-knowledge. Despite this promising insight, our findings also\nhighlight a considerable gap between the capabilities of these models and human\nproficiency in recognizing the limits of their knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2305.18153v2.pdf"
    },
    {
        "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
        "authors": [
            "Yuchuan Tian",
            "Hanting Chen",
            "Xutao Wang",
            "Zheyuan Bai",
            "Qinghua Zhang",
            "Ruifeng Li",
            "Chao Xu",
            "Yunhe Wang"
        ],
        "published": "2023-05-29T15:25:00Z",
        "summary": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are\nastonishing at generating human-like texts, but they may impact the\nauthenticity of texts. Previous works proposed methods to detect these\nAI-generated texts, including simple ML classifiers, pretrained-model-based\nzero-shot methods, and finetuned language classification models. However,\nmainstream detectors always fail on short texts, like SMSes, Tweets, and\nreviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training\nframework is proposed to address the difficulty of short-text detection without\nsacrificing long-texts. Firstly, we acknowledge the human-resemblance property\nof short machine texts, and rephrase AI text detection as a partial\nPositive-Unlabeled (PU) problem by regarding these short machine texts as\npartially ``unlabeled\". Then in this PU context, we propose the\nlength-sensitive Multiscale PU Loss, where a recurrent model in abstraction is\nused to estimate positive priors of scale-variant corpora. Additionally, we\nintroduce a Text Multiscaling module to enrich training corpora. Experiments\nshow that our MPU method augments detection performance on long AI-generated\ntexts, and significantly improves short-text detection of language model\ndetectors. Language Models trained with MPU could outcompete existing detectors\non various short-text and long-text detection benchmarks. The codes are\navailable at\nhttps://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt\nand https://github.com/YuchuanTian/AIGC_text_detector.",
        "pdf_link": "https://arxiv.org/pdf/2305.18149v4.pdf"
    },
    {
        "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
        "authors": [
            "Yi Hu",
            "Haotong Yang",
            "Zhouchen Lin",
            "Muhan Zhang"
        ],
        "published": "2023-05-29T15:14:09Z",
        "summary": "Large language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
        "pdf_link": "https://arxiv.org/pdf/2305.18507v2.pdf"
    },
    {
        "title": "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback",
        "authors": [
            "Shengchao Liu",
            "Jiongxiao Wang",
            "Yijin Yang",
            "Chengpeng Wang",
            "Ling Liu",
            "Hongyu Guo",
            "Chaowei Xiao"
        ],
        "published": "2023-05-29T14:43:24Z",
        "summary": "Recent advancements in conversational large language models (LLMs), such as\nChatGPT, have demonstrated remarkable promise in various domains, including\ndrug discovery. However, existing works mainly focus on investigating the\ncapabilities of conversational LLMs on chemical reaction and retrosynthesis.\nWhile drug editing, a critical task in the drug discovery pipeline, remains\nlargely unexplored. To bridge this gap, we propose ChatDrug, a framework to\nfacilitate the systematic investigation of drug editing using LLMs. ChatDrug\njointly leverages a prompt module, a retrieval and domain feedback (ReDF)\nmodule, and a conversation module to streamline effective drug editing. We\nempirically show that ChatDrug reaches the best performance on 33 out of 39\ndrug editing tasks, encompassing small molecules, peptides, and proteins. We\nfurther demonstrate, through 10 case studies, that ChatDrug can successfully\nidentify the key substructures (e.g., the molecule functional groups, peptide\nmotifs, and protein structures) for manipulation, generating diverse and valid\nsuggestions for drug editing. Promisingly, we also show that ChatDrug can offer\ninsightful explanations from a domain-specific perspective, enhancing\ninterpretability and enabling informed decision-making. This research sheds\nlight on the potential of ChatGPT and conversational LLMs for drug editing. It\npaves the way for a more efficient and collaborative drug discovery pipeline,\ncontributing to the advancement of pharmaceutical research and development.",
        "pdf_link": "https://arxiv.org/pdf/2305.18090v1.pdf"
    },
    {
        "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
        "authors": [
            "Sihan Chen",
            "Handong Li",
            "Qunbo Wang",
            "Zijia Zhao",
            "Mingzhen Sun",
            "Xinxin Zhu",
            "Jing Liu"
        ],
        "published": "2023-05-29T14:34:50Z",
        "summary": "Vision and text have been fully explored in contemporary video-text\nfoundational models, while other modalities such as audio and subtitles in\nvideos have not received sufficient attention. In this paper, we resort to\nestablish connections between multi-modality video tracks, including Vision,\nAudio, and Subtitle, and Text by exploring an automatically generated\nlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,\nwe first collect 27 million open-domain video clips and separately train a\nvision and an audio captioner to generate vision and audio captions. Then, we\nemploy an off-the-shelf Large Language Model (LLM) to integrate the generated\ncaptions, together with subtitles and instructional prompts into omni-modality\ncaptions. Based on the proposed VAST-27M dataset, we train an omni-modality\nvideo-text foundational model named VAST, which can perceive and process\nvision, audio, and subtitle modalities from video, and better support various\ntasks including vision-text, audio-text, and multi-modal video-text tasks\n(retrieval, captioning and QA). Extensive experiments have been conducted to\ndemonstrate the effectiveness of our proposed VAST-27M corpus and VAST\nfoundation model. VAST achieves 22 new state-of-the-art results on various\ncross-modality benchmarks. Code, model and dataset will be released at\nhttps://github.com/TXH-mercury/VAST.",
        "pdf_link": "https://arxiv.org/pdf/2305.18500v2.pdf"
    },
    {
        "title": "ANPL: Towards Natural Programming with Interactive Decomposition",
        "authors": [
            "Di Huang",
            "Ziyuan Nan",
            "Xing Hu",
            "Pengwei Jin",
            "Shaohui Peng",
            "Yuanbo Wen",
            "Rui Zhang",
            "Zidong Du",
            "Qi Guo",
            "Yewen Pu",
            "Yunji Chen"
        ],
        "published": "2023-05-29T14:19:40Z",
        "summary": "Though LLMs are capable of generating plausible programs, it's challenging to\ninteract with the LLMs further to revise the program, especially if the user's\nspecific requirements are different from the initial proposal. In this paper,\nwe introduce ANPL, an interactive programming system that ensures users can\nalways refine the generated code towards their specific programmatic intents\nvia structured decompositions. Borrowing the paradigm of sketching from program\nsynthesis, an ANPL program consists of a set of input-outputs that it must\nsatisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g.\nPython), and ``holes'' -- sub-modules to be implemented by the LLM specified\nwith natural language. The user revises an ANPL program by either modifying the\nsketch, changing the language used to describe the holes, or providing\nadditional input-outputs to a particular hole, turning it into a sub-ANPL\nprogram that can be solved recursively. This workflow allows the users to\noffload programming burdens to the LLM as much as possible while retaining the\nability to pinpoint and resolve bugs locally, without exposing the rest of the\nprogram to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus\n(ARC), a set of unique tasks that are challenging for state-of-the-art AI\nsystems, showing it outperforms baseline programming systems that (a) without\nthe ability to decompose tasks interactively and (b) without the guarantee that\nthe modules can be correctly composed together. Additional evaluations on APPS,\nHumanEval, and real-world programming tasks have validated that the ANPL\nframework is applicable to multiple programming domains. We release the ANPL\nsolutions to the ARC tasks as a dataset, providing insights into how humans\ndecompose novel tasks programmatically. See our code at\nhttps://iprc-dip.github.io/ANPL/.",
        "pdf_link": "https://arxiv.org/pdf/2305.18498v2.pdf"
    },
    {
        "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
        "authors": [
            "Wen Yang",
            "Chong Li",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "published": "2023-05-29T14:07:52Z",
        "summary": "Large language models (LLMs) demonstrate promising translation performance\namong various natural languages. However, many LLMs especially the open-sourced\nones, such as BLOOM and LLaMA, are English-dominant and support only dozens of\nnatural languages, making the potential of LLMs on language translation less\nexplored. In this work, we present BigTranslate which adapts LLaMA that covers\nonly 20 languages and enhances it with multilingual translation capability on\nmore than 100 languages. BigTranslate is built upon LLaMA-13B and it is\noptimized in three steps. First, we continue training LLaMA with massive\nChinese monolingual data. Second, we continue training the model with a\nlarge-scale parallel dataset that covers 102 natural languages. Third, we\ninstruct-tune the foundation model with multilingual translation instructions,\nleading to our BigTranslate model. The preliminary experiments on multilingual\ntranslation show that BigTranslate performs comparably with ChatGPT and Google\nTranslate in many languages and even outperforms ChatGPT in 8 language pairs.\nWe release the BigTranslate model and hope it can advance the research\nprogress.",
        "pdf_link": "https://arxiv.org/pdf/2305.18098v3.pdf"
    },
    {
        "title": "Game of Tones: Faculty detection of GPT-4 generated content in university assessments",
        "authors": [
            "Mike Perkins",
            "Jasper Roe",
            "Darius Postma",
            "James McGaughran",
            "Don Hickerson"
        ],
        "published": "2023-05-29T13:31:58Z",
        "summary": "This study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity.",
        "pdf_link": "https://arxiv.org/pdf/2305.18081v1.pdf"
    },
    {
        "title": "Image Captioning with Multi-Context Synthetic Data",
        "authors": [
            "Feipeng Ma",
            "Yizhou Zhou",
            "Fengyun Rao",
            "Yueyi Zhang",
            "Xiaoyan Sun"
        ],
        "published": "2023-05-29T13:18:59Z",
        "summary": "Image captioning requires numerous annotated image-text pairs, resulting in\nsubstantial annotation costs. Recently, large models (e.g. diffusion models and\nlarge language models) have excelled in producing high-quality images and text.\nThis potential can be harnessed to create synthetic image-text pairs for\ntraining captioning models. Synthetic data can improve cost and time efficiency\nin data collection, allow for customization to specific domains, bootstrap\ngeneralization capability for zero-shot performance, and circumvent privacy\nconcerns associated with real-world data. However, existing methods struggle to\nattain satisfactory performance solely through synthetic data. We identify the\nissue as generated images from simple descriptions mostly capture a solitary\nperspective with limited context, failing to align with the intricate scenes\nprevalent in real-world imagery. To tackle this, we present an innovative\npipeline that introduces multi-context data generation. Beginning with an\ninitial text corpus, our approach employs a large language model to extract\nmultiple sentences portraying the same scene from diverse viewpoints. These\nsentences are then condensed into a single sentence with multiple contexts.\nSubsequently, we generate intricate images using the condensed captions through\ndiffusion models. Our model is exclusively trained on synthetic image-text\npairs crafted through this process. The effectiveness of our pipeline is\nvalidated through experimental results in both the in-domain and cross-domain\nsettings, where it achieves state-of-the-art performance on well-known datasets\nsuch as MSCOCO, Flickr30k, and NoCaps.",
        "pdf_link": "https://arxiv.org/pdf/2305.18072v2.pdf"
    },
    {
        "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "M Saiful Bari",
            "Mizanur Rahman",
            "Md Amran Hossen Bhuiyan",
            "Shafiq Joty",
            "Jimmy Xiangji Huang"
        ],
        "published": "2023-05-29T12:37:21Z",
        "summary": "The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.18486v4.pdf"
    },
    {
        "title": "Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",
        "authors": [
            "Attia Qammar",
            "Hongmei Wang",
            "Jianguo Ding",
            "Abdenacer Naouri",
            "Mahmoud Daneshmand",
            "Huansheng Ning"
        ],
        "published": "2023-05-29T12:26:44Z",
        "summary": "Chatbots shifted from rule-based to artificial intelligence techniques and\ngained traction in medicine, shopping, customer services, food delivery,\neducation, and research. OpenAI developed ChatGPT blizzard on the Internet as\nit crossed one million users within five days of its launch. However, with the\nenhanced popularity, chatbots experienced cybersecurity threats and\nvulnerabilities. This paper discussed the relevant literature, reports, and\nexplanatory incident attacks generated against chatbots. Our initial point is\nto explore the timeline of chatbots from ELIZA (an early natural language\nprocessing computer program) to GPT-4 and provide the working mechanism of\nChatGPT. Subsequently, we explored the cybersecurity attacks and\nvulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically\nin the context of creating the malware code, phishing emails, undetectable\nzero-day attacks, and generation of macros and LOLBINs. Furthermore, the\nhistory of cyberattacks and vulnerabilities exploited by cybercriminals are\ndiscussed, particularly considering the risk and vulnerabilities in ChatGPT.\nAddressing these threats and vulnerabilities requires specific strategies and\nmeasures to reduce the harmful consequences. Therefore, the future directions\nto address the challenges were presented.",
        "pdf_link": "https://arxiv.org/pdf/2306.09255v1.pdf"
    },
    {
        "title": "Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation",
        "authors": [
            "Jiawei Huang",
            "Yi Ren",
            "Rongjie Huang",
            "Dongchao Yang",
            "Zhenhui Ye",
            "Chen Zhang",
            "Jinglin Liu",
            "Xiang Yin",
            "Zejun Ma",
            "Zhou Zhao"
        ],
        "published": "2023-05-29T10:41:28Z",
        "summary": "Large diffusion models have been successful in text-to-audio (T2A) synthesis\ntasks, but they often suffer from common issues such as semantic misalignment\nand poor temporal consistency due to limited natural language understanding and\ndata scarcity. Additionally, 2D spatial structures widely used in T2A works\nlead to unsatisfactory audio quality when generating variable-length audio\nsamples since they do not adequately prioritize temporal information. To\naddress these challenges, we propose Make-an-Audio 2, a latent diffusion-based\nT2A method that builds on the success of Make-an-Audio. Our approach includes\nseveral techniques to improve semantic alignment and temporal consistency:\nFirstly, we use pre-trained large language models (LLMs) to parse the text into\nstructured <event & order> pairs for better temporal information capture. We\nalso introduce another structured-text encoder to aid in learning semantic\nalignment during the diffusion denoising process. To improve the performance of\nvariable length generation and enhance the temporal information extraction, we\ndesign a feed-forward Transformer-based diffusion denoiser. Finally, we use\nLLMs to augment and transform a large amount of audio-label data into\naudio-text datasets to alleviate the problem of scarcity of temporal data.\nExtensive experiments show that our method outperforms baseline models in both\nobjective and subjective metrics, and achieves significant gains in temporal\ninformation understanding, semantic consistency, and sound quality.",
        "pdf_link": "https://arxiv.org/pdf/2305.18474v1.pdf"
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "authors": [
            "Peiyi Wang",
            "Lei Li",
            "Liang Chen",
            "Zefan Cai",
            "Dawei Zhu",
            "Binghuai Lin",
            "Yunbo Cao",
            "Qi Liu",
            "Tianyu Liu",
            "Zhifang Sui"
        ],
        "published": "2023-05-29T07:41:03Z",
        "summary": "In this paper, we uncover a systematic bias in the evaluation paradigm of\nadopting large language models~(LLMs), e.g., GPT-4, as a referee to score and\ncompare the quality of responses generated by candidate models. We find that\nthe quality ranking of candidate responses can be easily hacked by simply\naltering their order of appearance in the context. This manipulation allows us\nto skew the evaluation result, making one model appear considerably superior to\nthe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries\nwith ChatGPT as an evaluator. To address this issue, we propose a calibration\nframework with three simple yet effective strategies: 1) Multiple Evidence\nCalibration, which requires the evaluator model to generate multiple evaluation\nevidence before assigning ratings; 2) Balanced Position Calibration, which\naggregates results across various orders to determine the final score; 3)\nHuman-in-the-Loop Calibration, which introduces a balanced position diversity\nentropy to measure the difficulty of each example and seeks human assistance\nwhen needed. We also manually annotate the \"win/tie/lose\" outcomes of responses\nfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and\nextensive experiments demonstrate that our approach successfully mitigates\nevaluation bias, resulting in closer alignment with human judgments. We release\nour code and human annotation at \\url{https://github.com/i-Eval/FairEval} to\nfacilitate future research.",
        "pdf_link": "https://arxiv.org/pdf/2305.17926v2.pdf"
    },
    {
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
        "authors": [
            "Zechun Liu",
            "Barlas Oguz",
            "Changsheng Zhao",
            "Ernie Chang",
            "Pierre Stock",
            "Yashar Mehdad",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "published": "2023-05-29T05:22:11Z",
        "summary": "Several post-training quantization methods have been applied to large\nlanguage models (LLMs), and have been shown to perform well down to 8-bits. We\nfind that these methods break down at lower bit precision, and investigate\nquantization aware training for LLMs (LLM-QAT) to push quantization levels even\nfurther. We propose a data-free distillation method that leverages generations\nproduced by the pre-trained model, which better preserves the original output\ndistribution and allows quantizing any generative model independent of its\ntraining data, similar to post-training quantization methods. In addition to\nquantizing weights and activations, we also quantize the KV cache, which is\ncritical for increasing throughput and support long sequence dependencies at\ncurrent model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B,\nat quantization levels down to 4-bits. We observe large improvements over\ntraining-free methods, especially in the low-bit settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.17888v1.pdf"
    },
    {
        "title": "Baselines for Identifying Watermarked Large Language Models",
        "authors": [
            "Leonard Tang",
            "Gavin Uberti",
            "Tom Shlomi"
        ],
        "published": "2023-05-29T04:26:16Z",
        "summary": "We consider the emerging problem of identifying the presence and use of\nwatermarking schemes in widely used, publicly hosted, closed source large\nlanguage models (LLMs). We introduce a suite of baseline algorithms for\nidentifying watermarks in LLMs that rely on analyzing distributions of output\ntokens and logits generated by watermarked and unmarked LLMs. Notably,\nwatermarked LLMs tend to produce distributions that diverge qualitatively and\nidentifiably from standard models. Furthermore, we investigate the\nidentifiability of watermarks at varying strengths and consider the tradeoffs\nof each of our identification mechanisms with respect to watermarking scenario.\nAlong the way, we formalize the specific problem of identifying watermarks in\nLLMs, as well as LLM watermarks and watermark detection in general, providing a\nframework and foundations for studying them.",
        "pdf_link": "https://arxiv.org/pdf/2305.18456v1.pdf"
    },
    {
        "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
        "authors": [
            "Qiang Zhang",
            "Jason Naradowsky",
            "Yusuke Miyao"
        ],
        "published": "2023-05-29T04:19:35Z",
        "summary": "Existing dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
        "pdf_link": "https://arxiv.org/pdf/2305.17878v1.pdf"
    },
    {
        "title": "Taming AI Bots: Controllability of Neural States in Large Language Models",
        "authors": [
            "Stefano Soatto",
            "Paulo Tabuada",
            "Pratik Chaudhari",
            "Tian Yu Liu"
        ],
        "published": "2023-05-29T03:58:33Z",
        "summary": "We tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
        "pdf_link": "https://arxiv.org/pdf/2305.18449v1.pdf"
    },
    {
        "title": "Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",
        "authors": [
            "Magdalena Wysocka",
            "Oskar Wysocki",
            "Maxime Delmas",
            "Vincent Mutel",
            "Andre Freitas"
        ],
        "published": "2023-05-28T22:46:21Z",
        "summary": "Inferring over and extracting information from Large Language Models (LLMs)\ntrained on a large corpus of scientific literature can potentially drive a new\nera in biomedical research, reducing the barriers for accessing existing\nmedical evidence. This work examines the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nsystematic analysis is applied to ten state-of-the-art models, from models\nspecialised on biomedical scientific corpora to general models such as ChatGPT,\nGPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition\ngeneration and chemical compound-fungus relation determination. The work\nprovides a systematic assessment on the ability of LLMs to encode and express\nthese relations, verifying for fluency, prompt-alignment, semantic coherence,\nfactual knowledge and specificity of generated responses. Results show that\nwhile recent models have improved in fluency, factual accuracy is still low and\nmodels are biased towards over-represented entities. The ability of LLMs to\nserve as biomedical knowledge bases is questioned, and the need for additional\nsystematic evaluation frameworks is highlighted. The best performing GPT-4\nproduced a factual definition for 70% of chemical compounds and 43.6% factual\nrelations to fungi, whereas the best open source model BioGPT-large 30% of the\ncompounds and 30% of the relations for the best-performing prompt. The results\nshow that while LLMs are currently not fit for purpose to be used as biomedical\nfactual knowledge bases, there is a promising emerging property in the\ndirection of factuality as the models become domain specialised, scale-up in\nsize and level of human feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.17819v2.pdf"
    },
    {
        "title": "Transfer Learning for Power Outage Detection Task with Limited Training Data",
        "authors": [
            "Olukunle Owolabi"
        ],
        "published": "2023-05-28T22:36:35Z",
        "summary": "Early detection of power outages is crucial for maintaining a reliable power\ndistribution system. This research investigates the use of transfer learning\nand language models in detecting outages with limited labeled data. By\nleveraging pretraining and transfer learning, models can generalize to unseen\nclasses.\n  Using a curated balanced dataset of social media tweets related to power\noutages, we conducted experiments using zero-shot and few-shot learning. Our\nhypothesis is that Language Models pretrained with limited data could achieve\nhigh performance in outage detection tasks over baseline models. Results show\nthat while classical models outperform zero-shot Language Models, few-shot\nfine-tuning significantly improves their performance. For example, with 10%\nfine-tuning, BERT achieves 81.3% accuracy (+15.3%), and GPT achieves 74.5%\naccuracy (+8.5%). This has practical implications for analyzing and localizing\noutages in scenarios with limited data availability.\n  Our evaluation provides insights into the potential of few-shot fine-tuning\nwith Language Models for power outage detection, highlighting their strengths\nand limitations. This research contributes to the knowledge base of leveraging\nadvanced natural language processing techniques for managing critical\ninfrastructure.",
        "pdf_link": "https://arxiv.org/pdf/2305.17817v1.pdf"
    },
    {
        "title": "Targeted Data Generation: Finding and Fixing Model Weaknesses",
        "authors": [
            "Zexue He",
            "Marco Tulio Ribeiro",
            "Fereshte Khani"
        ],
        "published": "2023-05-28T19:36:50Z",
        "summary": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail\nsystematically on specific subgroups of data, resulting in unfair outcomes and\neroding user trust. Additional data collection may not help in addressing these\nweaknesses, as such challenging subgroups may be unknown to users, and\nunderrepresented in the existing and new data. We propose Targeted Data\nGeneration (TDG), a framework that automatically identifies challenging\nsubgroups, and generates new data for those subgroups using large language\nmodels (LLMs) with a human in the loop. TDG estimates the expected benefit and\npotential harm of data augmentation for each subgroup, and selects the ones\nmost likely to improve within group performance without hurting overall\nperformance. In our experiments, TDG significantly improves the accuracy on\nchallenging subgroups for state-of-the-art sentiment analysis and natural\nlanguage inference models, while also improving overall test accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.17804v1.pdf"
    },
    {
        "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
        "authors": [
            "W. Ronny Huang",
            "Hao Zhang",
            "Shankar Kumar",
            "Shuo-yiin Chang",
            "Tara N. Sainath"
        ],
        "published": "2023-05-28T19:31:45Z",
        "summary": "We propose a method of segmenting long-form speech by separating semantically\ncomplete sentences within the utterance. This prevents the ASR decoder from\nneedlessly processing faraway context while also preventing it from missing\nrelevant context within the current sentence. Semantically complete sentence\nboundaries are typically demarcated by punctuation in written text; but\nunfortunately, spoken real-world utterances rarely contain punctuation. We\naddress this limitation by distilling punctuation knowledge from a\nbidirectional teacher language model (LM) trained on written, punctuated text.\nWe compare our segmenter, which is distilled from the LM teacher, against a\nsegmenter distilled from a acoustic-pause-based teacher used in other works, on\na streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%\nrelative WER gain along with a 60 ms median end-of-segment latency reduction on\na YouTube captioning task.",
        "pdf_link": "https://arxiv.org/pdf/2305.18419v1.pdf"
    },
    {
        "title": "Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data",
        "authors": [
            "Mugariya Farooq",
            "Shahad Hardan",
            "Aigerim Zhumbhayeva",
            "Yujia Zheng",
            "Preslav Nakov",
            "Kun Zhang"
        ],
        "published": "2023-05-28T17:07:46Z",
        "summary": "The need for more usable and explainable machine learning models in\nhealthcare increases the importance of developing and utilizing causal\ndiscovery algorithms, which aim to discover causal relations by analyzing\nobservational data. Explainable approaches aid clinicians and biologists in\npredicting the prognosis of diseases and suggesting proper treatments. However,\nvery little research has been conducted at the crossroads between causal\ndiscovery, genomics, and breast cancer, and we aim to bridge this gap.\nMoreover, evaluation of causal discovery methods on real data is in general\nnotoriously difficult because ground-truth causal relations are usually\nunknown, and accordingly, in this paper, we also propose to address the\nevaluation problem with large language models. In particular, we exploit\nsuitable causal discovery algorithms to investigate how various perturbations\nin the genome can affect the survival of patients diagnosed with breast cancer.\nWe used three main causal discovery algorithms: PC, Greedy Equivalence Search\n(GES), and a Generalized Precision Matrix-based one. We experiment with a\nsubset of The Cancer Genome Atlas, which contains information about mutations,\ncopy number variations, protein levels, and gene expressions for 705 breast\ncancer patients. Our findings reveal important factors related to the vital\nstatus of patients using causal discovery algorithms. However, the reliability\nof these results remains a concern in the medical domain. Accordingly, as\nanother contribution of the work, the results are validated through language\nmodels trained on biomedical literature, such as BlueBERT and other large\nlanguage models trained on medical corpora. Our results profess proper\nutilization of causal discovery algorithms and language models for revealing\nreliable causal relations for clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.18410v1.pdf"
    },
    {
        "title": "Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective",
        "authors": [
            "Khanh Nguyen"
        ],
        "published": "2023-05-28T16:04:48Z",
        "summary": "How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.17760v6.pdf"
    },
    {
        "title": "Mitigating Label Biases for In-context Learning",
        "authors": [
            "Yu Fei",
            "Yifan Hou",
            "Zeming Chen",
            "Antoine Bosselut"
        ],
        "published": "2023-05-28T15:37:39Z",
        "summary": "Various design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
        "pdf_link": "https://arxiv.org/pdf/2305.19148v3.pdf"
    },
    {
        "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
        "authors": [
            "Bhawesh Kumar",
            "Charlie Lu",
            "Gauri Gupta",
            "Anil Palepu",
            "David Bellamy",
            "Ramesh Raskar",
            "Andrew Beam"
        ],
        "published": "2023-05-28T15:26:10Z",
        "summary": "As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.",
        "pdf_link": "https://arxiv.org/pdf/2305.18404v3.pdf"
    },
    {
        "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
        "authors": [
            "Mingyang Zhang",
            "Hao Chen",
            "Chunhua Shen",
            "Zhen Yang",
            "Linlin Ou",
            "Xinyi Yu",
            "Bohan Zhuang"
        ],
        "published": "2023-05-28T15:15:48Z",
        "summary": "Large pre-trained models (LPMs), such as LLaMA and GLM, have shown\nexceptional performance across various tasks through fine-tuning. Although\nlow-rank adaption (LoRA) has emerged to cheaply fine-tune these LPMs on\ndownstream tasks, their deployment is still hindered by the vast model scale\nand computational costs. Neural network pruning offers a way to compress LPMs.\nHowever, the current pruning methods designed for LPMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LPMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate, compact model for efficient inference in a highly memory-effective\nmanner. Specifically, we first design a LoRA-guided pruning criterion, which\nuses the weights and gradients of LoRA, rather than the gradients of\npre-trained weights for importance estimation. We then propose a structured\niterative pruning procedure, to remove redundant channels and heads. Extensive\nexperimental results demonstrate the superior performance of our LoRAPrune over\nexisting approaches on the LLaMA series models. For instance, at a 50\\%\ncompression rate, LoRAPrune outperforms LLM-Pruner by a perplexity reduction of\n8.0 on WikiText2 and 16.05 on PTB datasets, while concurrently reducing memory\nusage by 52.6\\%. The code will be released after review",
        "pdf_link": "https://arxiv.org/pdf/2305.18403v3.pdf"
    },
    {
        "title": "Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs",
        "authors": [
            "Akshay Nambi",
            "Vaibhav Balloli",
            "Mercy Ranjit",
            "Tanuja Ganu",
            "Kabir Ahuja",
            "Sunayana Sitaram",
            "Kalika Bali"
        ],
        "published": "2023-05-28T14:48:38Z",
        "summary": "Large language models (LLMs) are at the forefront of transforming numerous\ndomains globally. However, their inclusivity and effectiveness remain limited\nfor non-Latin scripts and low-resource languages. This paper tackles the\nimperative challenge of enhancing the multilingual performance of LLMs,\nspecifically focusing on Generative models. Through systematic investigation\nand evaluation of diverse languages using popular question-answering (QA)\ndatasets, we present novel techniques that unlock the true potential of LLMs in\na polyglot landscape. Our approach encompasses three key strategies that yield\nremarkable improvements in multilingual proficiency. First, by meticulously\noptimizing prompts tailored for polyglot LLMs, we unlock their latent\ncapabilities, resulting in substantial performance boosts across languages.\nSecond, we introduce a new hybrid approach that synergizes GPT generation with\nmultilingual embeddings and achieves significant multilingual performance\nimprovement on critical tasks like QA and retrieval. Finally, to further propel\nthe performance of polyglot LLMs, we introduce a novel learning algorithm that\ndynamically selects the optimal prompt strategy, LLM model, and embeddings per\nquery. This dynamic adaptation maximizes the efficacy of LLMs across languages,\noutperforming best static and random strategies. Our results show substantial\nadvancements in multilingual understanding and generation across a diverse\nrange of languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.17740v1.pdf"
    },
    {
        "title": "FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions",
        "authors": [
            "Noam Rotstein",
            "David Bensaid",
            "Shaked Brody",
            "Roy Ganz",
            "Ron Kimmel"
        ],
        "published": "2023-05-28T13:16:03Z",
        "summary": "The advent of vision-language pre-training techniques enhanced substantial\nprogress in the development of models for image captioning. However, these\nmodels frequently produce generic captions and may omit semantically important\nimage details. This limitation can be traced back to the image-text datasets;\nwhile their captions typically offer a general description of image content,\nthey frequently omit salient details. Considering the magnitude of these\ndatasets, manual reannotation is impractical, emphasizing the need for an\nautomated approach. To address this challenge, we leverage existing captions\nand explore augmenting them with visual details using \"frozen\" vision experts\nincluding an object detector, an attribute recognizer, and an Optical Character\nRecognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such\nvision experts with the original captions using a large language model (LLM),\nyielding comprehensive image descriptions. We automatically curate a training\nset of 12M image-enriched caption pairs. These pairs undergo extensive\nevaluation through both quantitative and qualitative analyses. Subsequently,\nthis data is utilized to train a captioning generation BLIP-based model. This\nmodel outperforms current state-of-the-art approaches, producing more precise\nand detailed descriptions, demonstrating the effectiveness of the proposed\ndata-centric approach. We release this large-scale dataset of enriched\nimage-caption pairs for the community.",
        "pdf_link": "https://arxiv.org/pdf/2305.17718v2.pdf"
    },
    {
        "title": "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers",
        "authors": [
            "Xuanqi Liu",
            "Zhuotao Liu"
        ],
        "published": "2023-05-28T13:08:13Z",
        "summary": "The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.18396v3.pdf"
    },
    {
        "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
        "authors": [
            "Minki Kang",
            "Seanie Lee",
            "Jinheon Baek",
            "Kenji Kawaguchi",
            "Sung Ju Hwang"
        ],
        "published": "2023-05-28T13:00:00Z",
        "summary": "Large Language Models (LLMs) have shown promising performance in\nknowledge-intensive reasoning tasks that require a compound understanding of\nknowledge. However, deployment of the LLMs in real-world applications can be\nchallenging due to their high computational requirements and concerns on data\nprivacy. Previous studies have focused on building task-specific small Language\nModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,\nthese approaches are ill-suited for knowledge-intensive reasoning tasks due to\nthe limited capacity of small LMs in memorizing the knowledge required.\nMotivated by our theoretical analysis on memorization, we propose\nKnowledge-Augmented Reasoning Distillation (KARD), a novel method that\nfine-tunes small LMs to generate rationales obtained from LLMs with augmented\nknowledge retrieved from an external knowledge base. Moreover, we further\npropose a neural reranker to obtain documents relevant to rationale generation.\nWe empirically show that KARD significantly improves the performance of small\nT5 and GPT models on the challenging knowledge-intensive reasoning datasets,\nnamely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the\n250M T5 models achieve superior performance against the fine-tuned 3B models,\nhaving 12 times larger parameters, on both MedQA-USMLE and StrategyQA\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2305.18395v2.pdf"
    },
    {
        "title": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Gunhee Kim",
            "Jung-Woo Ha"
        ],
        "published": "2023-05-28T12:07:16Z",
        "summary": "Large language models (LLMs) learn not only natural text generation abilities\nbut also social biases against different demographic groups from real-world\ndata. This poses a critical risk when deploying LLM-based applications.\nExisting research and resources are not readily applicable in South Korea due\nto the differences in language and culture, both of which significantly affect\nthe biases and targeted demographic groups. This limitation requires localized\nsocial bias datasets to ensure the safe and effective deployment of LLMs. To\nthis end, we present KO SB I, a new social bias dataset of 34k pairs of\ncontexts and sentences in Korean covering 72 demographic groups in 15\ncategories. We find that through filtering-based moderation, social biases in\ngenerated content can be reduced by 16.47%p on average for HyperCLOVA (30B and\n82B), and GPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2305.17701v2.pdf"
    },
    {
        "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Meeyoung Cha",
            "Yejin Choi",
            "Byoung Pil Kim",
            "Gunhee Kim",
            "Eun-Ju Lee",
            "Yong Lim",
            "Alice Oh",
            "Sangchul Park",
            "Jung-Woo Ha"
        ],
        "published": "2023-05-28T11:51:20Z",
        "summary": "The potential social harms that large language models pose, such as\ngenerating offensive content and reinforcing biases, are steeply rising.\nExisting works focus on coping with this concern while interacting with\nill-intentioned users, such as those who explicitly make hate speech or elicit\nharmful responses. However, discussions on sensitive issues can become toxic\neven if the users are well-intentioned. For safer models in such scenarios, we\npresent the Sensitive Questions and Acceptable Response (SQuARe) dataset, a\nlarge-scale Korean dataset of 49k sensitive questions with 42k acceptable and\n46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA\nin a human-in-the-loop manner based on real news headlines. Experiments show\nthat acceptable response generation significantly improves for HyperCLOVA and\nGPT-3, demonstrating the efficacy of this dataset.",
        "pdf_link": "https://arxiv.org/pdf/2305.17696v1.pdf"
    },
    {
        "title": "Evaluating GPT-3 Generated Explanations for Hateful Content Moderation",
        "authors": [
            "Han Wang",
            "Ming Shan Hee",
            "Md Rabiul Awal",
            "Kenny Tsu Wei Choo",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-05-28T10:05:13Z",
        "summary": "Recent research has focused on using large language models (LLMs) to generate\nexplanations for hate speech through fine-tuning or prompting. Despite the\ngrowing interest in this area, these generated explanations' effectiveness and\npotential limitations remain poorly understood. A key concern is that these\nexplanations, generated by LLMs, may lead to erroneous judgments about the\nnature of flagged content by both users and content moderators. For instance,\nan LLM-generated explanation might inaccurately convince a content moderator\nthat a benign piece of content is hateful. In light of this, we propose an\nanalytical framework for examining hate speech explanations and conducted an\nextensive survey on evaluating such explanations. Specifically, we prompted\nGPT-3 to generate explanations for both hateful and non-hateful content, and a\nsurvey was conducted with 2,400 unique respondents to evaluate the generated\nexplanations. Our findings reveal that (1) human evaluators rated the\nGPT-generated explanations as high quality in terms of linguistic fluency,\ninformativeness, persuasiveness, and logical soundness, (2) the persuasive\nnature of these explanations, however, varied depending on the prompting\nstrategy employed, and (3) this persuasiveness may result in incorrect\njudgments about the hatefulness of the content. Our study underscores the need\nfor caution in applying LLM-generated explanations for content moderation. Code\nand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.",
        "pdf_link": "https://arxiv.org/pdf/2305.17680v4.pdf"
    },
    {
        "title": "Reward Collapse in Aligning Large Language Models",
        "authors": [
            "Ziang Song",
            "Tianle Cai",
            "Jason D. Lee",
            "Weijie J. Su"
        ],
        "published": "2023-05-28T02:12:00Z",
        "summary": "The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.",
        "pdf_link": "https://arxiv.org/pdf/2305.17608v1.pdf"
    },
    {
        "title": "Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds",
        "authors": [
            "Yan Ding",
            "Xiaohan Zhang",
            "Saeid Amiri",
            "Nieqing Cao",
            "Hao Yang",
            "Andy Kaminski",
            "Chad Esselink",
            "Shiqi Zhang"
        ],
        "published": "2023-05-27T22:30:15Z",
        "summary": "Task planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2305.17590v2.pdf"
    },
    {
        "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
        "authors": [
            "Jason Hoelscher-Obermaier",
            "Julia Persson",
            "Esben Kran",
            "Ioannis Konstas",
            "Fazl Barez"
        ],
        "published": "2023-05-27T19:08:04Z",
        "summary": "Recent model editing techniques promise to mitigate the problem of memorizing\nfalse or outdated associations during LLM training. However, we show that these\ntechniques can introduce large unwanted side effects which are not detected by\nexisting specificity benchmarks. We extend the existing CounterFact benchmark\nto include a dynamic component and dub our benchmark CounterFact+.\nAdditionally, we extend the metrics used for measuring specificity by a\nprincipled KL divergence-based metric. We use this improved benchmark to\nevaluate recent model editing techniques and find that they suffer from low\nspecificity. Our findings highlight the need for improved specificity\nbenchmarks that identify and prevent unwanted side effects.",
        "pdf_link": "https://arxiv.org/pdf/2305.17553v2.pdf"
    },
    {
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Yarin Gal",
            "Nicolas Papernot",
            "Ross Anderson"
        ],
        "published": "2023-05-27T15:10:41Z",
        "summary": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
        "pdf_link": "https://arxiv.org/pdf/2305.17493v2.pdf"
    },
    {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "authors": [
            "Jasivan Alex Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2023-05-27T15:00:45Z",
        "summary": "While pre-trained language models achieve impressive performance on various\nNLP benchmarks, they still struggle with tasks that require numerical\nreasoning. Recent advances in improving numerical reasoning are mostly achieved\nusing very large language models that contain billions of parameters and are\nnot accessible to everyone. In addition, numerical reasoning is measured using\na single score on existing datasets. As a result, we do not have a clear\nunderstanding of the strengths and shortcomings of existing models on different\nnumerical reasoning aspects and therefore, potential ways to improve them apart\nfrom scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we\nintroduce a multi-view evaluation set for numerical reasoning in English,\ncalled FERMAT. Instead of reporting a single score on a whole dataset, FERMAT\nevaluates models on various key numerical reasoning aspects such as number\nunderstanding, mathematical operations, and training dependency. Apart from\nproviding a comprehensive evaluation of models on different numerical reasoning\naspects, FERMAT enables a systematic and automated generation of an arbitrarily\nlarge training or evaluation set for each aspect.The datasets and codes are\npublicly available to generate further multi-view data for ulterior tasks and\nlanguages.",
        "pdf_link": "https://arxiv.org/pdf/2305.17491v1.pdf"
    },
    {
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
        "authors": [
            "Taicheng Guo",
            "Kehan Guo",
            "Bozhao Nan",
            "Zhenwen Liang",
            "Zhichun Guo",
            "Nitesh V. Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "published": "2023-05-27T14:17:33Z",
        "summary": "Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.",
        "pdf_link": "https://arxiv.org/pdf/2305.18365v3.pdf"
    },
    {
        "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "authors": [
            "Deokjae Lee",
            "JunYeong Lee",
            "Jung-Woo Ha",
            "Jin-Hwa Kim",
            "Sang-Woo Lee",
            "Hwaran Lee",
            "Hyun Oh Song"
        ],
        "published": "2023-05-27T11:00:15Z",
        "summary": "The deployment of large-scale generative models is often restricted by their\npotential risk of causing harm to users in unpredictable ways. We focus on the\nproblem of black-box red teaming, where a red team generates test cases and\ninteracts with the victim model to discover a diverse set of failures with\nlimited query access. Existing red teaming methods construct test cases based\non human supervision or language model (LM) and query all test cases in a\nbrute-force manner without incorporating any information from past evaluations,\nresulting in a prohibitively large number of queries. To this end, we propose\nBayesian red teaming (BRT), novel query-efficient black-box red teaming methods\nbased on Bayesian optimization, which iteratively identify diverse positive\ntest cases leading to model failures by utilizing the pre-defined user input\npool and the past evaluations. Experimental results on various user input pools\ndemonstrate that our method consistently finds a significantly larger number of\ndiverse positive test cases under the limited query budget than the baseline\nmethods. The source code is available at\nhttps://github.com/snu-mllab/Bayesian-Red-Teaming.",
        "pdf_link": "https://arxiv.org/pdf/2305.17444v1.pdf"
    },
    {
        "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
        "authors": [
            "Xuanjie Fang",
            "Sijie Cheng",
            "Yang Liu",
            "Wei Wang"
        ],
        "published": "2023-05-27T10:33:53Z",
        "summary": "Pre-trained language models (PLMs) have been widely used to underpin various\ndownstream tasks. However, the adversarial attack task has found that PLMs are\nvulnerable to small perturbations. Mainstream methods adopt a detached\ntwo-stage framework to attack without considering the subsequent influence of\nsubstitution at each step. In this paper, we formally model the adversarial\nattack task on PLMs as a sequential decision-making problem, where the whole\nattack process is sequential with two decision-making problems, i.e., word\nfinder and word substitution. Considering the attack process can only receive\nthe final state without any direct intermediate signals, we propose to use\nreinforcement learning to find an appropriate sequential attack path to\ngenerate adversaries, named SDM-Attack. Extensive experimental results show\nthat SDM-Attack achieves the highest attack success rate with a comparable\nmodification rate and semantic similarity to attack fine-tuned BERT.\nFurthermore, our analyses demonstrate the generalization and transferability of\nSDM-Attack. The code is available at https://github.com/fduxuan/SDM-Attack.",
        "pdf_link": "https://arxiv.org/pdf/2305.17440v1.pdf"
    },
    {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Prithviraj Ammanabrolu",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-05-27T07:04:15Z",
        "summary": "We introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.17390v2.pdf"
    },
    {
        "title": "Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques",
        "authors": [
            "Daking Rai",
            "Bailin Wang",
            "Yilun Zhou",
            "Ziyu Yao"
        ],
        "published": "2023-05-27T06:09:03Z",
        "summary": "Compositional and domain generalization present significant challenges in\nsemantic parsing, even for state-of-the-art semantic parsers based on\npre-trained language models (LMs). In this study, we empirically investigate\nimproving an LM's generalization in semantic parsing with two simple\ntechniques: at the token level, we introduce a token preprocessing method to\npreserve the semantic boundaries of tokens produced by LM tokenizers; at the\nsequence level, we propose to use special tokens to mark the boundaries of\ncomponents aligned between input and output. Our experimental results on two\ntext-to-SQL semantic parsing datasets show that our token preprocessing,\nalthough simple, can substantially improve the LM performance on both types of\ngeneralization, and our component boundary marking method is particularly\nhelpful for compositional generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.17378v1.pdf"
    },
    {
        "title": "Augmenting Large Language Model Translators via Translation Memories",
        "authors": [
            "Yongyu Mu",
            "Abudurexiti Reheman",
            "Zhiquan Cao",
            "Yuchun Fan",
            "Bei Li",
            "Yinqiao Li",
            "Tong Xiao",
            "Chunliang Zhang",
            "Jingbo Zhu"
        ],
        "published": "2023-05-27T04:47:09Z",
        "summary": "Using translation memories (TMs) as prompts is a promising approach to\nin-context learning of machine translation models. In this work, we take a step\ntowards prompting large language models (LLMs) with TMs and making them better\ntranslators. We find that the ability of LLMs to ``understand'' prompts is\nindeed helpful for making better use of TMs. Experiments show that the results\nof a pre-trained LLM translator can be greatly improved by using high-quality\nTM-based prompts. These results are even comparable to those of the\nstate-of-the-art NMT systems which have access to large-scale in-domain\nbilingual data and are well tuned on the downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.17367v1.pdf"
    },
    {
        "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
        "authors": [
            "Xianjun Yang",
            "Wei Cheng",
            "Yue Wu",
            "Linda Petzold",
            "William Yang Wang",
            "Haifeng Chen"
        ],
        "published": "2023-05-27T03:58:29Z",
        "summary": "Large language models (LLMs) have notably enhanced the fluency and diversity\nof machine-generated text. However, this progress also presents a significant\nchallenge in detecting the origin of a given text, and current research on\ndetection methods lags behind the rapid evolution of LLMs. Conventional\ntraining-based methods have limitations in flexibility, particularly when\nadapting to new domains, and they often lack explanatory power. To address this\ngap, we propose a novel training-free detection strategy called Divergent\nN-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and\nthen use only the preceding portion as input to the LLMs to regenerate the new\nremaining parts. By analyzing the differences between the original and new\nremaining parts through N-gram analysis in black-box or probability divergence\nin white-box, we unveil significant discrepancies between the distribution of\nmachine-generated text and the distribution of human-written text. We conducted\nextensive experiments on the most advanced LLMs from OpenAI, including\ntext-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such\nas GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach\nexhibits state-of-the-art performance in distinguishing between human and\nGPT-generated text on four English and one German dataset, outperforming\nOpenAI's own classifier, which is trained on millions of text. Additionally,\nour methods provide reasonable explanations and evidence to support our claim,\nwhich is a unique feature of explainable detection. Our method is also robust\nunder the revised text attack and can additionally solve model sourcing. Codes\nare available at https://github.com/Xianjun-Yang/DNA-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.17359v2.pdf"
    },
    {
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
        "authors": [
            "Yao Fu",
            "Litu Ou",
            "Mingyu Chen",
            "Yuhao Wan",
            "Hao Peng",
            "Tushar Khot"
        ],
        "published": "2023-05-26T23:46:42Z",
        "summary": "As large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF.",
        "pdf_link": "https://arxiv.org/pdf/2305.17306v1.pdf"
    },
    {
        "title": "Improved Instruction Ordering in Recipe-Grounded Conversation",
        "authors": [
            "Duong Minh Le",
            "Ruohao Guo",
            "Wei Xu",
            "Alan Ritter"
        ],
        "published": "2023-05-26T21:57:11Z",
        "summary": "In this paper, we study the task of instructional dialogue and focus on the\ncooking domain. Analyzing the generated output of the GPT-J model, we reveal\nthat the primary challenge for a recipe-grounded dialog system is how to\nprovide the instructions in the correct order. We hypothesize that this is due\nto the model's lack of understanding of user intent and inability to track the\ninstruction state (i.e., which step was last instructed). Therefore, we propose\nto explore two auxiliary subtasks, namely User Intent Detection and Instruction\nState Tracking, to support Response Generation with improved instruction\ngrounding. Experimenting with our newly collected dataset, ChattyChef, shows\nthat incorporating user intent and instruction state information helps the\nresponse generation model mitigate the incorrect order issue. Furthermore, to\ninvestigate whether ChatGPT has completely solved this task, we analyze its\noutputs and find that it also makes mistakes (10.7% of the responses), about\nhalf of which are out-of-order instructions. We will release ChattyChef to\nfacilitate further research in this area at:\nhttps://github.com/octaviaguo/ChattyChef.",
        "pdf_link": "https://arxiv.org/pdf/2305.17280v1.pdf"
    },
    {
        "title": "SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)",
        "authors": [
            "Ruoxi Sun",
            "Sercan \u00d6. Arik",
            "Alex Muzio",
            "Lesly Miculicich",
            "Satya Gundabathula",
            "Pengcheng Yin",
            "Hanjun Dai",
            "Hootan Nakhost",
            "Rajarishi Sinha",
            "Zifeng Wang",
            "Tomas Pfister"
        ],
        "published": "2023-05-26T21:39:05Z",
        "summary": "Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.",
        "pdf_link": "https://arxiv.org/pdf/2306.00739v4.pdf"
    },
    {
        "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
        "authors": [
            "Ruixiang Tang",
            "Dehan Kong",
            "Longtao Huang",
            "Hui Xue"
        ],
        "published": "2023-05-26T20:56:30Z",
        "summary": "Large language models (LLMs) have recently shown great potential for\nin-context learning, where LLMs learn a new task simply by conditioning on a\nfew input-label pairs (prompts). Despite their potential, our understanding of\nthe factors influencing end-task performance and the robustness of in-context\nlearning remains limited. This paper aims to bridge this knowledge gap by\ninvestigating the reliance of LLMs on shortcuts or spurious correlations within\nprompts. Through comprehensive experiments on classification and extraction\ntasks, we reveal that LLMs are \"lazy learners\" that tend to exploit shortcuts\nin prompts for downstream tasks. Additionally, we uncover a surprising finding\nthat larger models are more likely to utilize shortcuts in prompts during\ninference. Our findings provide a new perspective on evaluating robustness in\nin-context learning and pose new challenges for detecting and mitigating the\nuse of shortcuts in prompts.",
        "pdf_link": "https://arxiv.org/pdf/2305.17256v2.pdf"
    },
    {
        "title": "Large language models improve Alzheimer's disease diagnosis using multi-modality data",
        "authors": [
            "Yingjie Feng",
            "Jun Wang",
            "Xianfeng Gu",
            "Xiaoyin Xu",
            "Min Zhang"
        ],
        "published": "2023-05-26T18:42:19Z",
        "summary": "In diagnosing challenging conditions such as Alzheimer's disease (AD),\nimaging is an important reference. Non-imaging patient data such as patient\ninformation, genetic data, medication information, cognitive and memory tests\nalso play a very important role in diagnosis. Effect. However, limited by the\nability of artificial intelligence models to mine such information, most of the\nexisting models only use multi-modal image data, and cannot make full use of\nnon-image data. We use a currently very popular pre-trained large language\nmodel (LLM) to enhance the model's ability to utilize non-image data, and\nachieved SOTA results on the ADNI dataset.",
        "pdf_link": "https://arxiv.org/pdf/2305.19280v1.pdf"
    },
    {
        "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
        "authors": [
            "Julia Mendelsohn",
            "Ronan Le Bras",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023-05-26T18:00:57Z",
        "summary": "Dogwhistles are coded expressions that simultaneously convey one meaning to a\nbroad audience and a second one, often hateful or provocative, to a narrow\nin-group; they are deployed to evade both political repercussions and\nalgorithmic content moderation. For example, in the sentence 'we need to end\nthe cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to\nmany, but secretly means 'Jewish' to a select few. We present the first\nlarge-scale computational investigation of dogwhistles. We develop a typology\nof dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles\nwith rich contextual information and examples, and analyze their usage in\nhistorical U.S. politicians' speeches. We then assess whether a large language\nmodel (GPT-3) can identify dogwhistles and their meanings, and find that\nGPT-3's performance varies widely across types of dogwhistles and targeted\ngroups. Finally, we show that harmful content containing dogwhistles avoids\ntoxicity detection, highlighting online risks of such coded language. This work\nsheds light on the theoretical and applied importance of dogwhistles in both\nNLP and computational social science, and provides resources for future\nresearch in modeling dogwhistles and mitigating their online harms.",
        "pdf_link": "https://arxiv.org/pdf/2305.17174v1.pdf"
    },
    {
        "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
        "authors": [
            "Zichang Liu",
            "Aditya Desai",
            "Fangshuo Liao",
            "Weitao Wang",
            "Victor Xie",
            "Zhaozhuo Xu",
            "Anastasios Kyrillidis",
            "Anshumali Shrivastava"
        ],
        "published": "2023-05-26T17:39:58Z",
        "summary": "Large language models(LLMs) have sparked a new wave of exciting AI\napplications. Hosting these models at scale requires significant memory\nresources. One crucial memory bottleneck for the deployment stems from the\ncontext window. It is commonly recognized that model weights are memory hungry;\nhowever, the size of key-value embedding stored during the generation process\n(KV cache) can easily surpass the model size. The enormous size of the KV cache\nputs constraints on the inference batch size, which is crucial for high\nthroughput inference workload. Inspired by an interesting observation of the\nattention scores, we hypothesize the persistence of importance: only pivotal\ntokens, which had a substantial influence at one step, will significantly\ninfluence future generations. Based on our empirical verification and\ntheoretical analysis around this hypothesis, we propose Scissorhands, a system\nthat maintains the memory usage of the KV cache at a fixed budget without\nfinetuning the model. In essence, Scissorhands manages the KV cache by storing\nthe pivotal tokens with a higher probability. We validate that Scissorhands\nreduces the inference memory usage of the KV cache by up to 5X without\ncompromising model quality. We further demonstrate that Scissorhands can be\ncombined with 4-bit quantization, traditionally used to compress model weights,\nto achieve up to 20X compression.",
        "pdf_link": "https://arxiv.org/pdf/2305.17118v2.pdf"
    },
    {
        "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
        "authors": [
            "David Soong",
            "Sriram Sridhar",
            "Han Si",
            "Jan-Samuel Wagner",
            "Ana Caroline Costa S\u00e1",
            "Christina Y Yu",
            "Kubra Karagoz",
            "Meijian Guan",
            "Hisham Hamadeh",
            "Brandon W Higgs"
        ],
        "published": "2023-05-26T17:33:05Z",
        "summary": "Large language models (LLMs) have made significant advancements in natural\nlanguage processing (NLP). Broad corpora capture diverse patterns but can\nintroduce irrelevance, while focused corpora enhance reliability by reducing\nmisleading information. Training LLMs on focused corpora poses computational\nchallenges. An alternative approach is to use a retrieval-augmentation (RetA)\nmethod tested in a specific domain.\n  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a\ncustom RetA model were compared using 19 questions on diffuse large B-cell\nlymphoma (DLBCL) disease. Eight independent reviewers assessed responses based\non accuracy, relevance, and readability (rated 1-3).\n  The RetA model performed best in accuracy (12/19 3-point scores, total=47)\nand relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4\nreceived the highest readability scores (17/19, 55), followed by GPT-3 (15/19,\n53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34),\nrelevance (32), and readability (38).\n  Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared\nto the RetA model and Prometheus. Hallucinations were mostly associated with\nnon-existent references or fabricated efficacy data.\n  These findings suggest that RetA models, supplemented with domain-specific\ncorpora, may outperform general-purpose LLMs in accuracy and relevance within\nspecific domains. However, this evaluation was limited to specific questions\nand metrics and may not capture challenges in semantic search and other NLP\ntasks. Further research will explore different LLM architectures, RetA\nmethodologies, and evaluation methods to assess strengths and limitations more\ncomprehensively.",
        "pdf_link": "https://arxiv.org/pdf/2305.17116v2.pdf"
    },
    {
        "title": "Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models",
        "authors": [
            "Daman Arora",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-26T16:36:55Z",
        "summary": "There have been wide spread claims in the literature about the emergent\nreasoning capabilities of Pretrained Large Language Models. However, recent\nstudies, have found that their ability to plan remains questionable. Through\nour experiments using GPT-2, we empirically demonstrate that the performance of\na finetuned baseline remains poor because it violates pre-conditions of actions\nin the plans that it generates. To improve the planning capabilities of a\nfinetuned LLM, we train a verifier, which can classify actions as being valid\nor invalid in a particular state. By randomly sampling actions from the same\ndataset, we generate examples of invalid actions which are then used to train a\nverifier which can check for action applicability. In the presence of diverse\nsampling from a generator and a verifier which can prune invalid trajectories,\nwe show significant gains in the success rate on the Blocksworld domain.\nAdditionally, we show that finetuning the GPT-2 generator itself to create the\nverifier generalizes better than finetuning the base GPT-2. Lastly, we\ninvestigate the role of the sampling temperature which can be used to control\nthe exploration-exploitation tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2305.17077v1.pdf"
    },
    {
        "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
        "authors": [
            "Yudong Xu",
            "Wenhao Li",
            "Pashootan Vaezipoor",
            "Scott Sanner",
            "Elias B. Khalil"
        ],
        "published": "2023-05-26T16:32:17Z",
        "summary": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We\nexplore this broad question through a systematic analysis of GPT on the\nAbstraction and Reasoning Corpus (ARC), a representative benchmark of abstract\nreasoning ability from limited examples in which solutions require some \"core\nknowledge\" of concepts such as objects, goal states, counting, and basic\ngeometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when\nusing textual encodings for their two-dimensional input-output grids. Our\nfailure analysis reveals that GPT-4's capacity to identify objects and reason\nabout them is significantly influenced by the sequential nature of the text\nthat represents an object within a text encoding of a task. To test this\nhypothesis, we design a new benchmark, the 1D-ARC, which consists of\none-dimensional (array-like) tasks that are more conducive to GPT-based\nreasoning, and where it indeed performs better than on the (2D) ARC. To\nalleviate this issue, we propose an object-based representation that is\nobtained through an external tool, resulting in nearly doubling the performance\non solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the\nstate-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language\ndomains such as the 1D-ARC or a simple ARC subset, our study reveals that the\nuse of object-based representations can significantly improve its reasoning\nability. Visualizations, GPT logs, and data are available at\nhttps://khalil-research.github.io/LLM4ARC.",
        "pdf_link": "https://arxiv.org/pdf/2305.18354v2.pdf"
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "authors": [
            "Mingchen Zhuge",
            "Haozhe Liu",
            "Francesco Faccio",
            "Dylan R. Ashley",
            "R\u00f3bert Csord\u00e1s",
            "Anand Gopalakrishnan",
            "Abdullah Hamdi",
            "Hasan Abed Al Kader Hammoud",
            "Vincent Herrmann",
            "Kazuki Irie",
            "Louis Kirsch",
            "Bing Li",
            "Guohao Li",
            "Shuming Liu",
            "Jinjie Mai",
            "Piotr Pi\u0119kos",
            "Aditya Ramesh",
            "Imanol Schlag",
            "Weimin Shi",
            "Aleksandar Stani\u0107",
            "Wenyi Wang",
            "Yuhui Wang",
            "Mengmeng Xu",
            "Deng-Ping Fan",
            "Bernard Ghanem",
            "J\u00fcrgen Schmidhuber"
        ],
        "published": "2023-05-26T16:21:25Z",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "pdf_link": "https://arxiv.org/pdf/2305.17066v1.pdf"
    },
    {
        "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks",
        "authors": [
            "Jacob Abernethy",
            "Alekh Agarwal",
            "Teodor V. Marinov",
            "Manfred K. Warmuth"
        ],
        "published": "2023-05-26T15:49:43Z",
        "summary": "We study the phenomenon of \\textit{in-context learning} (ICL) exhibited by\nlarge language models, where they can adapt to a new learning task, given a\nhandful of labeled examples, without any explicit parameter optimization. Our\ngoal is to explain how a pre-trained transformer model is able to perform ICL\nunder reasonable assumptions on the pre-training process and the downstream\ntasks. We posit a mechanism whereby a transformer can achieve the following:\n(a) receive an i.i.d. sequence of examples which have been converted into a\nprompt using potentially-ambiguous delimiters, (b) correctly segment the prompt\ninto examples and labels, (c) infer from the data a \\textit{sparse linear\nregressor} hypothesis, and finally (d) apply this hypothesis on the given test\nexample and return a predicted label. We establish that this entire procedure\nis implementable using the transformer mechanism, and we give sample complexity\nguarantees for this learning framework. Our empirical findings validate the\nchallenge of segmentation, and we show a correspondence between our posited\nmechanisms and observed attention maps for step (c).",
        "pdf_link": "https://arxiv.org/pdf/2305.17040v1.pdf"
    },
    {
        "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
        "authors": [
            "Gengze Zhou",
            "Yicong Hong",
            "Qi Wu"
        ],
        "published": "2023-05-26T14:41:06Z",
        "summary": "Trained with an unprecedented scale of data, large language models (LLMs)\nlike ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities\nfrom model scaling. Such a trend underscored the potential of training LLMs\nwith unlimited language data, advancing the development of a universal embodied\nagent. In this work, we introduce the NavGPT, a purely LLM-based\ninstruction-following navigation agent, to reveal the reasoning capability of\nGPT models in complex embodied scenes by performing zero-shot sequential action\nprediction for vision-and-language navigation (VLN). At each step, NavGPT takes\nthe textual descriptions of visual observations, navigation history, and future\nexplorable directions as inputs to reason the agent's current status, and makes\nthe decision to approach the target. Through comprehensive experiments, we\ndemonstrate NavGPT can explicitly perform high-level planning for navigation,\nincluding decomposing instruction into sub-goal, integrating commonsense\nknowledge relevant to navigation task resolution, identifying landmarks from\nobserved scenes, tracking navigation progress, and adapting to exceptions with\nplan adjustment. Furthermore, we show that LLMs is capable of generating\nhigh-quality navigational instructions from observations and actions along a\npath, as well as drawing accurate top-down metric trajectory given the agent's\nnavigation history. Despite the performance of using NavGPT to zero-shot R2R\ntasks still falling short of trained models, we suggest adapting multi-modality\ninputs for LLMs to use as visual navigation agents and applying the explicit\nreasoning of LLMs to benefit learning-based models.",
        "pdf_link": "https://arxiv.org/pdf/2305.16986v3.pdf"
    },
    {
        "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Chongxuan Li",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "published": "2023-05-26T13:49:44Z",
        "summary": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented\nperformance in response generation, especially with visual inputs, enabling\nmore creative and adaptable interaction than large language models such as\nChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable modality (e.g., vision). To this end, we propose evaluating the\nrobustness of open-source large VLMs in the most realistic and high-risk\nsetting, where adversaries have only black-box system access and seek to\ndeceive the model into returning the targeted responses. In particular, we\nfirst craft targeted adversarial examples against pretrained models such as\nCLIP and BLIP, and then transfer these adversarial examples to other VLMs such\nas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we\nobserve that black-box queries on these VLMs can further improve the\neffectiveness of targeted evasion, resulting in a surprisingly high success\nrate for generating targeted responses. Our findings provide a quantitative\nunderstanding regarding the adversarial vulnerability of large VLMs and call\nfor a more thorough examination of their potential security flaws before\ndeployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.16934v2.pdf"
    },
    {
        "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
        "authors": [
            "Suet-Ying Lam",
            "Qingcheng Zeng",
            "Kexun Zhang",
            "Chenyu You",
            "Rob Voigt"
        ],
        "published": "2023-05-26T13:30:48Z",
        "summary": "While a large body of literature suggests that large language models (LLMs)\nacquire rich linguistic representations, little is known about whether they\nadapt to linguistic biases in a human-like way. The present study probes this\nquestion by asking whether LLMs display human-like referential biases using\nstimuli and procedures from real psycholinguistic experiments. Recent\npsycholinguistic studies suggest that humans adapt their referential biases\nwith recent exposure to referential patterns; closely replicating three\nrelevant psycholinguistic experiments from Johnson & Arnold (2022) in an\nin-context learning (ICL) framework, we found that InstructGPT adapts its\npronominal interpretations in response to the frequency of referential patterns\nin the local discourse, though in a limited fashion: adaptation was only\nobserved relative to syntactic but not semantic biases. By contrast, FLAN-UL2\nfails to generate meaningful patterns. Our results provide further evidence\nthat contemporary LLMs discourse representations are sensitive to syntactic\npatterns in the local context but less so to semantic patterns. Our data and\ncode are available at \\url{https://github.com/zkx06111/llm_priming}.",
        "pdf_link": "https://arxiv.org/pdf/2305.16917v1.pdf"
    },
    {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "published": "2023-05-26T12:17:59Z",
        "summary": "Large Language Models (LLMs) are transforming society and permeating into\ndiverse applications. As a result, LLMs will frequently interact with us and\nother agents. It is, therefore, of great societal value to understand how LLMs\nbehave in interactive social settings. Here, we propose to use behavioral game\ntheory to study LLM's cooperation and coordination behavior. To do so, we let\ndifferent LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with\neach other and with other, human-like strategies. Our results show that LLMs\ngenerally perform well in such tasks and also uncover persistent behavioral\nsignatures. In a large set of two players-two strategies games, we find that\nLLMs are particularly good at games where valuing their own self-interest pays\noff, like the iterated Prisoner's Dilemma family. However, they behave\nsub-optimally in games that require coordination. We, therefore, further focus\non two games from these distinct families. In the canonical iterated Prisoner's\nDilemma, we find that GPT-4 acts particularly unforgivingly, always defecting\nafter another agent has defected only once. In the Battle of the Sexes, we find\nthat GPT-4 cannot match the behavior of the simple convention to alternate\nbetween options. We verify that these behavioral signatures are stable across\nrobustness checks. Finally, we show how GPT-4's behavior can be modified by\nproviding further information about the other player as well as by asking it to\npredict the other player's actions before making a choice. These results enrich\nour understanding of LLM's social behavior and pave the way for a behavioral\ngame theory for machines.",
        "pdf_link": "https://arxiv.org/pdf/2305.16867v1.pdf"
    },
    {
        "title": "HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis",
        "authors": [
            "Christoforos Vasilatos",
            "Manaar Alam",
            "Talal Rahwan",
            "Yasir Zaki",
            "Michail Maniatakos"
        ],
        "published": "2023-05-26T11:07:25Z",
        "summary": "As the use of Large Language Models (LLMs) in text generation tasks\nproliferates, concerns arise over their potential to compromise academic\nintegrity. The education sector currently tussles with distinguishing\nstudent-authored homework assignments from AI-generated ones. This paper\naddresses the challenge by introducing HowkGPT, designed to identify homework\nassignments generated by AI. HowkGPT is built upon a dataset of academic\nassignments and accompanying metadata [17] and employs a pretrained LLM to\ncompute perplexity scores for student-authored and ChatGPT-generated responses.\nThese scores then assist in establishing a threshold for discerning the origin\nof a submitted assignment. Given the specificity and contextual nature of\nacademic work, HowkGPT further refines its analysis by defining\ncategory-specific thresholds derived from the metadata, enhancing the precision\nof the detection. This study emphasizes the critical need for effective\nstrategies to uphold academic integrity amidst the growing influence of LLMs\nand provides an approach to ensuring fair and accurate grading in educational\ninstitutions.",
        "pdf_link": "https://arxiv.org/pdf/2305.18226v2.pdf"
    },
    {
        "title": "Do GPTs Produce Less Literal Translations?",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes",
            "Matt Post",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-05-26T10:38:31Z",
        "summary": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.",
        "pdf_link": "https://arxiv.org/pdf/2305.16806v4.pdf"
    },
    {
        "title": "Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media",
        "authors": [
            "Loukas Ilias",
            "Spiros Mouzakitis",
            "Dimitris Askounis"
        ],
        "published": "2023-05-26T10:19:04Z",
        "summary": "In today's fast-paced world, the rates of stress and depression present a\nsurge. Social media provide assistance for the early detection of mental health\nconditions. Existing methods mainly introduce feature extraction approaches and\ntrain shallow machine learning classifiers. Other researches use deep neural\nnetworks or transformers. Despite the fact that transformer-based models\nachieve noticeable improvements, they cannot often capture rich factual\nknowledge. Although there have been proposed a number of studies aiming to\nenhance the pretrained transformer-based models with extra information or\nadditional modalities, no prior work has exploited these modifications for\ndetecting stress and depression through social media. In addition, although the\nreliability of a machine learning model's confidence in its predictions is\ncritical for high-risk applications, there is no prior work taken into\nconsideration the model calibration. To resolve the above issues, we present\nthe first study in the task of depression and stress detection in social media,\nwhich injects extra linguistic information in transformer-based models, namely\nBERT and MentalBERT. Specifically, the proposed approach employs a Multimodal\nAdaptation Gate for creating the combined embeddings, which are given as input\nto a BERT (or MentalBERT) model. For taking into account the model calibration,\nwe apply label smoothing. We test our proposed approaches in three publicly\navailable datasets and demonstrate that the integration of linguistic features\ninto transformer-based models presents a surge in the performance. Also, the\nusage of label smoothing contributes to both the improvement of the model's\nperformance and the calibration of the model. We finally perform a linguistic\nanalysis of the posts and show differences in language between stressful and\nnon-stressful texts, as well as depressive and non-depressive posts.",
        "pdf_link": "https://arxiv.org/pdf/2305.16797v2.pdf"
    },
    {
        "title": "Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning",
        "authors": [
            "Niful Islam",
            "Debopom Sutradhar",
            "Humaira Noor",
            "Jarin Tasnim Raya",
            "Monowara Tabassum Maisha",
            "Dewan Md Farid"
        ],
        "published": "2023-05-26T09:27:43Z",
        "summary": "ChatGPT is a conversational artificial intelligence that is a member of the\ngenerative pre-trained transformer of the large language model family. This\ntext generative model was fine-tuned by both supervised learning and\nreinforcement learning so that it can produce text documents that seem to be\nwritten by natural intelligence. Although there are numerous advantages of this\ngenerative model, it comes with some reasonable concerns as well. This paper\npresents a machine learning-based solution that can identify the ChatGPT\ndelivered text from the human written text along with the comparative analysis\nof a total of 11 machine learning and deep learning algorithms in the\nclassification process. We have tested the proposed model on a Kaggle dataset\nconsisting of 10,000 texts out of which 5,204 texts were written by humans and\ncollected from news and social media. On the corpus generated by GPT-3.5, the\nproposed algorithm presents an accuracy of 77%.",
        "pdf_link": "https://arxiv.org/pdf/2306.01761v1.pdf"
    },
    {
        "title": "Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification",
        "authors": [
            "Nicol\u00f2 Tamagnone",
            "Selim Fekih",
            "Ximena Contla",
            "Nayid Orozco",
            "Navid Rekabsaz"
        ],
        "published": "2023-05-26T09:15:05Z",
        "summary": "Accurate and rapid situation analysis during humanitarian crises is critical\nto delivering humanitarian aid efficiently and is fundamental to humanitarian\nimperatives and the Leave No One Behind (LNOB) principle. This data analysis\ncan highly benefit from language processing systems, e.g., by classifying the\ntext data according to a humanitarian ontology. However, approaching this by\nsimply fine-tuning a generic large language model (LLM) involves considerable\npractical and ethical issues, particularly the lack of effectiveness on\ndata-sparse and complex subdomains, and the encoding of societal biases and\nunwanted associations. In this work, we aim to provide an effective and\nethically-aware system for humanitarian data analysis. We approach this by (1)\nintroducing a novel architecture adjusted to the humanitarian analysis\nframework, (2) creating and releasing a novel humanitarian-specific LLM called\nHumBert, and (3) proposing a systematic way to measure and mitigate biases. Our\nexperiments' results show the better performance of our approach on zero-shot\nand full-training settings in comparison with strong baseline models, while\nalso revealing the existence of biases in the resulting LLMs. Utilizing a\ntargeted counterfactual data augmentation approach, we significantly reduce\nthese biases without compromising performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.16756v2.pdf"
    },
    {
        "title": "Can large language models generate salient negative statements?",
        "authors": [
            "Hiba Arnaout",
            "Simon Razniewski"
        ],
        "published": "2023-05-26T09:13:59Z",
        "summary": "We examine the ability of large language models (LLMs) to generate salient\n(interesting) negative statements about real-world entities; an emerging\nresearch topic of the last few years. We probe the LLMs using zero- and k-shot\nunconstrained probes, and compare with traditional methods for negation\ngeneration, i.e., pattern-based textual extractions and knowledge-graph-based\ninferences, as well as crowdsourced gold statements. We measure the correctness\nand salience of the generated lists about subjects from different domains. Our\nevaluation shows that guided probes do in fact improve the quality of generated\nnegatives, compared to the zero-shot variant. Nevertheless, using both prompts,\nLLMs still struggle with the notion of factuality of negatives, frequently\ngenerating many ambiguous statements, or statements with negative keywords but\na positive meaning.",
        "pdf_link": "https://arxiv.org/pdf/2305.16755v2.pdf"
    },
    {
        "title": "A Closer Look at In-Context Learning under Distribution Shifts",
        "authors": [
            "Kartik Ahuja",
            "David Lopez-Paz"
        ],
        "published": "2023-05-26T07:47:21Z",
        "summary": "In-context learning, a capability that enables a model to learn from input\nexamples on the fly without necessitating weight updates, is a defining\ncharacteristic of large language models. In this work, we follow the setting\nproposed in (Garg et al., 2022) to better understand the generality and\nlimitations of in-context learning from the lens of the simple yet fundamental\ntask of linear regression. The key question we aim to address is: Are\ntransformers more adept than some natural and simpler architectures at\nperforming in-context learning under varying distribution shifts? To compare\ntransformers, we propose to use a simple architecture based on set-based\nMulti-Layer Perceptrons (MLPs). We find that both transformers and set-based\nMLPs exhibit in-context learning under in-distribution evaluations, but\ntransformers more closely emulate the performance of ordinary least squares\n(OLS). Transformers also display better resilience to mild distribution shifts,\nwhere set-based MLPs falter. However, under severe distribution shifts, both\nmodels' in-context learning abilities diminish.",
        "pdf_link": "https://arxiv.org/pdf/2305.16704v1.pdf"
    },
    {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "published": "2023-05-26T05:52:27Z",
        "summary": "Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.16653v1.pdf"
    },
    {
        "title": "TADA: Task-Agnostic Dialect Adapters for English",
        "authors": [
            "Will Held",
            "Caleb Ziems",
            "Diyi Yang"
        ],
        "published": "2023-05-26T05:45:03Z",
        "summary": "Large Language Models, the dominant starting point for Natural Language\nProcessing (NLP) applications, fail at a higher rate for speakers of English\ndialects other than Standard American English (SAE). Prior work addresses this\nusing task-specific data or synthetic data augmentation, both of which require\nintervention for each dialect and task pair. This poses a scalability issue\nthat prevents the broad adoption of robust dialectal English NLP. We introduce\na simple yet effective method for task-agnostic dialect adaptation by aligning\nnon-SAE dialects using adapters and composing them with task-specific adapters\nfrom SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on\n4 dialectal variants of the GLUE benchmark without task-specific supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.16651v1.pdf"
    },
    {
        "title": "Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing",
        "authors": [
            "Jaehun Jung",
            "Peter West",
            "Liwei Jiang",
            "Faeze Brahman",
            "Ximing Lu",
            "Jillian Fisher",
            "Taylor Sorensen",
            "Yejin Choi"
        ],
        "published": "2023-05-26T05:19:24Z",
        "summary": "We present Impossible Distillation, a novel framework for paraphrasing and\nsentence summarization, that distills a high-quality dataset and model from a\nlow-quality teacher that itself cannot perform these tasks. Unlike prior works\nthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specific\narchitecture, we hypothesize and verify the paraphrastic proximity intrinsic to\npre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in\nthe LM distribution. By identifying and distilling generations from these\nsubspaces, Impossible Distillation produces a high-quality dataset and model\neven from GPT2-scale LMs. We evaluate our method on multiple benchmarks\nspanning unconstrained / syntax-controlled paraphrase generation and sentence\nsummarization. Our model with 770M parameters consistently outperforms strong\nbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPT\nitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher\ndiversity and fidelity than up to 13 times larger datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.16635v3.pdf"
    },
    {
        "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks",
        "authors": [
            "Agam Shah",
            "Sudheer Chava"
        ],
        "published": "2023-05-26T05:13:01Z",
        "summary": "Recently large language models (LLMs) like ChatGPT have shown impressive\nperformance on many natural language processing tasks with zero-shot. In this\npaper, we investigate the effectiveness of zero-shot LLMs in the financial\ndomain. We compare the performance of ChatGPT along with some open-source\ngenerative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We\naddress three inter-related research questions on data annotation, performance\ngaps, and the feasibility of employing generative models in the finance domain.\nOur findings demonstrate that ChatGPT performs well even without labeled data\nbut fine-tuned models generally outperform it. Our research also highlights how\nannotating with generative models can be time-intensive. Our codebase is\npublicly available on GitHub under CC BY-NC 4.0 license.",
        "pdf_link": "https://arxiv.org/pdf/2305.16633v1.pdf"
    },
    {
        "title": "Evaluation of Question Generation Needs More References",
        "authors": [
            "Shinhyeok Oh",
            "Hyojun Go",
            "Hyeongdon Moon",
            "Yunsung Lee",
            "Myeongho Jeong",
            "Hyun Seung Lee",
            "Seungtaek Choi"
        ],
        "published": "2023-05-26T04:40:56Z",
        "summary": "Question generation (QG) is the task of generating a valid and fluent\nquestion based on a given context and the target answer. According to various\npurposes, even given the same context, instructors can ask questions about\ndifferent concepts, and even the same concept can be written in different ways.\nHowever, the evaluation for QG usually depends on single reference-based\nsimilarity metrics, such as n-gram-based metric or learned metric, which is not\nsufficient to fully evaluate the potential of QG methods. To this end, we\npropose to paraphrase the reference question for a more robust QG evaluation.\nUsing large language models such as GPT-3, we created semantically and\nsyntactically diverse questions, then adopt the simple aggregation of the\npopular evaluation metrics as the final scores. Through our experiments, we\nfound that using multiple (pseudo) references is more effective for QG\nevaluation while showing a higher correlation with human evaluations than\nevaluation with a single reference.",
        "pdf_link": "https://arxiv.org/pdf/2305.16626v1.pdf"
    },
    {
        "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
        "authors": [
            "Zhijie Deng",
            "Hongcheng Gao",
            "Yibo Miao",
            "Hao Zhang"
        ],
        "published": "2023-05-26T04:23:10Z",
        "summary": "The detection of machine-generated text, especially from large language\nmodels (LLMs), is crucial in preventing serious social problems resulting from\ntheir misuse. Some methods train dedicated detectors on specific datasets but\nfall short in generalizing to unseen test data, while other zero-shot ones\noften yield suboptimal performance. Although the recent DetectGPT has shown\npromising detection performance, it suffers from significant inefficiency\nissues, as detecting a single candidate requires scoring hundreds of its\nperturbations with the source LLM. This paper aims to bridge this gap.\nTechnically, we propose to incorporate a Bayesian surrogate model, which allows\nus to select typical samples based on Bayesian uncertainty and interpolate\nscores from typical samples to other ones, to improve query efficiency. Our\nempirical results demonstrate that our method significantly outperforms\nexisting approaches under a low query budget. Notably, our method achieves\nsimilar performance with up to 2 times fewer queries than DetectGPT and 3.7%\nhigher AUROC at a query number of 5.",
        "pdf_link": "https://arxiv.org/pdf/2305.16617v1.pdf"
    },
    {
        "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
        "authors": [
            "Guan Wang",
            "Weihua Li",
            "Edmund M-K. Lai",
            "Quan Bai"
        ],
        "published": "2023-05-26T03:44:35Z",
        "summary": "The rapid growth of information on the Internet has led to an overwhelming\namount of opinions and comments on various activities, products, and services.\nThis makes it difficult and time-consuming for users to process all the\navailable information when making decisions. Text summarization, a Natural\nLanguage Processing (NLP) task, has been widely explored to help users quickly\nretrieve relevant information by generating short and salient content from long\nor multiple documents. Recent advances in pre-trained language models, such as\nChatGPT, have demonstrated the potential of Large Language Models (LLMs) in\ntext generation. However, LLMs require massive amounts of data and resources\nand are challenging to implement as offline applications. Furthermore, existing\ntext summarization approaches often lack the ``adaptive\" nature required to\ncapture diverse aspects in opinion summarization, which is particularly\ndetrimental to users with specific requirements or preferences. In this paper,\nwe propose an Aspect-adaptive Knowledge-based Opinion Summarization model for\nproduct reviews, which effectively captures the adaptive nature required for\nopinion summarization. The model generates aspect-oriented summaries given a\nset of reviews for a particular product, efficiently providing users with\nuseful information on specific aspects they are interested in, ensuring the\ngenerated summaries are more personalized and informative. Extensive\nexperiments have been conducted using real-world datasets to evaluate the\nproposed model. The results demonstrate that our model outperforms\nstate-of-the-art approaches and is adaptive and efficient in generating\nsummaries that focus on particular aspects, enabling users to make\nwell-informed decisions and catering to their diverse interests and\npreferences.",
        "pdf_link": "https://arxiv.org/pdf/2306.05537v1.pdf"
    },
    {
        "title": "Heterogeneous Value Alignment Evaluation for Large Language Models",
        "authors": [
            "Zhaowei Zhang",
            "Ceyao Zhang",
            "Nian Liu",
            "Siyuan Qi",
            "Ziqi Rong",
            "Song-Chun Zhu",
            "Shuguang Cui",
            "Yaodong Yang"
        ],
        "published": "2023-05-26T02:34:20Z",
        "summary": "The emergent capabilities of Large Language Models (LLMs) have made it\ncrucial to align their values with those of humans. However, current\nmethodologies typically attempt to assign value as an attribute to LLMs, yet\nlack attention to the ability to pursue value and the importance of\ntransferring heterogeneous values in specific practical applications. In this\npaper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,\ndesigned to assess the success of aligning LLMs with heterogeneous values.\nSpecifically, our approach first brings the Social Value Orientation (SVO)\nframework from social psychology, which corresponds to how much weight a person\nattaches to the welfare of others in relation to their own. We then assign the\nLLMs with different social values and measure whether their behaviors align\nwith the inducing values. We conduct evaluations with new auto-metric\n\\textit{value rationality} to represent the ability of LLMs to align with\nspecific values. Evaluating the value rationality of five mainstream LLMs, we\ndiscern a propensity in LLMs towards neutral values over pronounced personal\nvalues. By examining the behavior of these LLMs, we contribute to a deeper\ninsight into the value alignment of LLMs within a heterogeneous value system.",
        "pdf_link": "https://arxiv.org/pdf/2305.17147v3.pdf"
    },
    {
        "title": "Neural Task Synthesis for Visual Programming",
        "authors": [
            "Victor-Alexandru P\u0103durean",
            "Georgios Tzannetos",
            "Adish Singla"
        ],
        "published": "2023-05-26T01:08:18Z",
        "summary": "Generative neural models hold great promise in enhancing programming\neducation by synthesizing new content. We seek to design neural models that can\nautomatically generate programming tasks for a given specification in the\ncontext of visual programming domains. Despite the recent successes of large\ngenerative models like GPT-4, our initial results show that these models are\nineffective in synthesizing visual programming tasks and struggle with logical\nand spatial reasoning. We propose a novel neuro-symbolic technique,\nNeurTaskSyn, that can synthesize programming tasks for a specification given in\nthe form of desired programming concepts exercised by its solution code and\nconstraints on the visual task. NeurTaskSyn has two components: the first\ncomponent is trained via imitation learning procedure to generate possible\nsolution codes, and the second component is trained via reinforcement learning\nprocedure to guide an underlying symbolic execution engine that generates\nvisual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn\nthrough an extensive empirical evaluation and a qualitative study on reference\ntasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and\nthe Intro to Programming with Karel course by CodeHS-dot-com.",
        "pdf_link": "https://arxiv.org/pdf/2305.18342v3.pdf"
    },
    {
        "title": "CONA: A novel CONtext-Aware instruction paradigm for communication using large language model",
        "authors": [
            "Nan Zhou",
            "Xinghui Tao",
            "Xi Chen"
        ],
        "published": "2023-05-26T00:53:18Z",
        "summary": "We introduce CONA, a novel context-aware instruction paradigm for effective\nknowledge dissemination using generative pre-trained transformer (GPT) models.\nCONA is a flexible framework designed to leverage the capabilities of Large\nLanguage Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,\nWisdom) hierarchy to automatically instruct and optimise presentation content,\nanticipate potential audience inquiries, and provide context-aware answers that\nadaptive to the knowledge level of the audience group. The unique aspect of the\nCONA paradigm lies in its combination of an independent advisory mechanism and\na recursive feedback loop rooted on the DIKW hierarchy. This synergy\nsignificantly enhances context-aware contents, ensuring they are accessible and\neasily comprehended by the audience. This paradigm is an early pioneer to\nexplore new methods for knowledge dissemination and communication in the LLM\nera, offering effective support for everyday knowledge sharing scenarios. We\nconduct experiments on a range of audience roles, along with materials from\nvarious disciplines using GPT4. Both quantitative and qualitative results\ndemonstrated that the proposed CONA paradigm achieved remarkable performance\ncompared to the outputs guided by conventional prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2305.18620v1.pdf"
    },
    {
        "title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering",
        "authors": [
            "Sabrina Chiesurin",
            "Dimitris Dimakopoulos",
            "Marco Antonio Sobrevilla Cabezudo",
            "Arash Eshghi",
            "Ioannis Papaioannou",
            "Verena Rieser",
            "Ioannis Konstas"
        ],
        "published": "2023-05-25T22:54:13Z",
        "summary": "Large language models are known to produce output which sounds fluent and\nconvincing, but is also often wrong, e.g. \"unfaithful\" with respect to a\nrationale as retrieved from a knowledge base. In this paper, we show that\ntask-based systems which exhibit certain advanced linguistic dialog behaviors,\nsuch as lexical alignment (repeating what the user said), are in fact preferred\nand trusted more, whereas other phenomena, such as pronouns and ellipsis are\ndis-preferred. We use open-domain question answering systems as our test-bed\nfor task based dialog generation and compare several open- and closed-book\nmodels. Our results highlight the danger of systems that appear to be\ntrustworthy by parroting user input while providing an unfaithful response.",
        "pdf_link": "https://arxiv.org/pdf/2305.16519v1.pdf"
    },
    {
        "title": "On the Tool Manipulation Capability of Open-source Large Language Models",
        "authors": [
            "Qiantong Xu",
            "Fenglu Hong",
            "Bo Li",
            "Changran Hu",
            "Zhengyu Chen",
            "Jian Zhang"
        ],
        "published": "2023-05-25T22:10:20Z",
        "summary": "Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.16504v1.pdf"
    },
    {
        "title": "Coarse-Tuning Models of Code with Reinforcement Learning Feedback",
        "authors": [
            "Abhinav Jain",
            "Chima Adiole",
            "Swarat Chaudhuri",
            "Thomas Reps",
            "Chris Jermaine"
        ],
        "published": "2023-05-25T22:09:08Z",
        "summary": "Large Language Models (LLMs) pre-trained on code have recently emerged as the\ndominant approach to program synthesis. However, these models are trained using\nnext-token prediction, which ignores the syntax and semantics of code. We\npropose RLCF, that further trains a pre-trained LLM via reinforcement learning,\nusing feedback from a grounding function that scores the quality of the code.\nThe grounding function uses (i) compiler-derived feedback on whether the code\nit generates passes a set of correctness checks; and (ii) feedback from a\ndifferent LLM that compares the generated code to a reference code. RLCF is\nmodel- and language-agnostic. We empirically evaluate it on the MBJP and MathQA\ntasks for Java. Our experiments show that RLCF raises the odds that an\nLLM-generated program compiles, is executable, and produces the right output on\ntests, often allowing LLMs to match the performance of 2x-8x larger LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.18341v2.pdf"
    },
    {
        "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training",
        "authors": [
            "Federico Cassano",
            "Ming-Ho Yee",
            "Noah Shinn",
            "Arjun Guha",
            "Steven Holtzen"
        ],
        "published": "2023-05-25T21:16:09Z",
        "summary": "TypeScript and Python are two programming languages that support optional\ntype annotations, which are useful but tedious to introduce and maintain. This\nhas motivated automated type prediction: given an untyped program, produce a\nwell-typed output program. Large language models (LLMs) are promising for type\nprediction, but there are challenges: fill-in-the-middle performs poorly,\nprograms may not fit into the context window, generated types may not type\ncheck, and it is difficult to measure how well-typed the output program is. We\naddress these challenges by building OpenTau, a search-based approach for type\nprediction that leverages large language models. We propose a new metric for\ntype prediction quality, give a tree-based program decomposition that searches\na space of generated types, and present fill-in-the-type fine-tuning for LLMs.\nWe evaluate our work with a new dataset for TypeScript type prediction, and\nshow that 47.4% of files type check (14.5% absolute improvement) with an\noverall rate of 3.3 type errors per file. All code, data, and models are\navailable at: https://github.com/GammaTauAI/opentau.",
        "pdf_link": "https://arxiv.org/pdf/2305.17145v1.pdf"
    },
    {
        "title": "Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models",
        "authors": [
            "Isabelle Lorge",
            "Janet Pierrehumbert"
        ],
        "published": "2023-05-25T18:56:26Z",
        "summary": "Vector space models of word meaning all share the assumption that words\noccurring in similar contexts have similar meanings. In such models, words that\nare similar in their topical associations but differ in their logical force\ntend to emerge as semantically close, creating well-known challenges for NLP\napplications that involve logical reasoning. Modern pretrained language models,\nsuch as BERT, RoBERTa and GPT-3 hold the promise of performing better on\nlogical tasks than classic static word embeddings. However, reports are mixed\nabout their success. In the current paper, we advance this discussion through a\nsystematic study of scalar adverbs, an under-explored class of words with\nstrong logical force. Using three different tasks, involving both naturalistic\nsocial media data and constructed examples, we investigate the extent to which\nBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these\ncommon words. We ask: 1) Do the models distinguish amongst the three semantic\ncategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit\nrepresentations of full scales from maximally negative to maximally positive?\n3) How do word frequency and contextual factors impact model performance? We\nfind that despite capturing some aspects of logical meaning, the models fall\nfar short of human performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.16426v2.pdf"
    },
    {
        "title": "Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech",
        "authors": [
            "Loukas Ilias",
            "Dimitris Askounis"
        ],
        "published": "2023-05-25T18:18:09Z",
        "summary": "Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is\nthe main cause of dementia. Although many studies have been proposed targeting\nat diagnosing dementia through spontaneous speech, there are still limitations.\nExisting state-of-the-art approaches, which propose multimodal methods, train\nseparately language and acoustic models, employ majority-vote approaches, and\nconcatenate the representations of the different modalities either at the input\nlevel, i.e., early fusion, or during training. Also, some of them employ\nself-attention layers, which calculate the dependencies between representations\nwithout considering the contextual information. In addition, no prior work has\ntaken into consideration the model calibration. To address these limitations,\nwe propose some new methods for detecting AD patients, which capture the intra-\nand cross-modal interactions. First, we convert the audio files into log-Mel\nspectrograms, their delta, and delta-delta and create in this way an image per\naudio file consisting of three channels. Next, we pass each transcript and\nimage through BERT and DeiT models respectively. After that, context-based\nself-attention layers, self-attention layers with a gate model, and optimal\ntransport domain adaptation methods are employed for capturing the intra- and\ninter-modal interactions. Finally, we exploit two methods for fusing the self\nand cross-attention features. For taking into account the model calibration, we\napply label smoothing. We use both performance and calibration metrics.\nExperiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the\nefficacy of our introduced approaches over existing research initiatives with\nour best performing model reaching Accuracy and F1-score up to 91.25% and\n91.06% respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.16406v2.pdf"
    },
    {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "published": "2023-05-25T17:59:49Z",
        "summary": "The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
        "pdf_link": "https://arxiv.org/pdf/2305.17144v2.pdf"
    },
    {
        "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
        "authors": [
            "Amirkeivan Mohtashami",
            "Martin Jaggi"
        ],
        "published": "2023-05-25T17:53:42Z",
        "summary": "While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.",
        "pdf_link": "https://arxiv.org/pdf/2305.16300v2.pdf"
    },
    {
        "title": "Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots",
        "authors": [
            "Sukhpal Singh Gill",
            "Minxian Xu",
            "Panos Patros",
            "Huaming Wu",
            "Rupinder Kaur",
            "Kamalpreet Kaur",
            "Stephanie Fuller",
            "Manmeet Singh",
            "Priyansh Arora",
            "Ajith Kumar Parlikad",
            "Vlado Stankovski",
            "Ajith Abraham",
            "Soumya K. Ghosh",
            "Hanan Lutfiyya",
            "Salil S. Kanhere",
            "Rami Bahsoon",
            "Omer Rana",
            "Schahram Dustdar",
            "Rizos Sakellariou",
            "Steve Uhlig",
            "Rajkumar Buyya"
        ],
        "published": "2023-05-25T17:35:57Z",
        "summary": "ChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial.",
        "pdf_link": "https://arxiv.org/pdf/2306.03823v1.pdf"
    },
    {
        "title": "UFO: Unified Fact Obtaining for Commonsense Question Answering",
        "authors": [
            "Zhifeng Li",
            "Yifan Fan",
            "Bowei Zou",
            "Yu Hong"
        ],
        "published": "2023-05-25T13:25:49Z",
        "summary": "Leveraging external knowledge to enhance the reasoning ability is crucial for\ncommonsense question answering. However, the existing knowledge bases heavily\nrely on manual annotation which unavoidably causes deficiency in coverage of\nworld-wide commonsense knowledge. Accordingly, the knowledge bases fail to be\nflexible enough to support the reasoning over diverse questions. Recently,\nlarge-scale language models (LLMs) have dramatically improved the intelligence\nin capturing and leveraging knowledge, which opens up a new way to address the\nissue of eliciting knowledge from language models. We propose a Unified Facts\nObtaining (UFO) approach. UFO turns LLMs into knowledge sources and produces\nrelevant facts (knowledge statements) for the given question. We first develop\na unified prompt consisting of demonstrations that cover different aspects of\ncommonsense and different question styles. On this basis, we instruct the LLMs\nto generate question-related supporting facts for various commonsense questions\nvia prompting. After facts generation, we apply a dense retrieval-based fact\nselection strategy to choose the best-matched fact. This kind of facts will be\nfed into the answer inference model along with the question. Notably, due to\nthe design of unified prompts, UFO can support reasoning in various commonsense\naspects (including general commonsense, scientific commonsense, and social\ncommonsense). Extensive experiments on CommonsenseQA 2.0, OpenBookQA, QASC, and\nSocial IQA benchmarks show that UFO significantly improves the performance of\nthe inference model and outperforms manually constructed knowledge sources.",
        "pdf_link": "https://arxiv.org/pdf/2305.16048v1.pdf"
    },
    {
        "title": "ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs",
        "authors": [
            "Zihao Zhao",
            "Sheng Wang",
            "Jinchen Gu",
            "Yitao Zhu",
            "Lanzhuju Mei",
            "Zixu Zhuang",
            "Zhiming Cui",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-05-25T12:03:31Z",
        "summary": "The integration of Computer-Assisted Diagnosis (CAD) with Large Language\nModels (LLMs) holds great potential in clinical applications, specifically in\nthe roles of virtual family doctors and clinic assistants. However, current\nworks in this field are plagued by limitations, specifically a restricted scope\nof applicable image domains and the provision of unreliable medical advice.\nThis restricts their overall processing capabilities. Furthermore, the mismatch\nin writing style between LLMs and radiologists undermines their practical\nusefulness. To tackle these challenges, we introduce ChatCAD+, which is\ndesigned to be universal and reliable. It is capable of handling medical images\nfrom diverse domains and leveraging up-to-date information from reputable\nmedical websites to provide reliable medical advice. Additionally, it\nincorporates a template retrieval system that improves report generation\nperformance via exemplar reports. This approach ensures greater consistency\nwith the expertise of human professionals. The source code is available at\nhttps://github.com/zhaozh10/ChatCAD.",
        "pdf_link": "https://arxiv.org/pdf/2305.15964v4.pdf"
    },
    {
        "title": "Linguistic Properties of Truthful Response",
        "authors": [
            "Bruce W. Lee",
            "Benedict Florance Arockiaraj",
            "Helen Jin"
        ],
        "published": "2023-05-25T09:17:39Z",
        "summary": "We investigate the phenomenon of an LLM's untruthful response using a large\nset of 220 handcrafted linguistic features. We focus on GPT-3 models and find\nthat the linguistic profiles of responses are similar across model sizes. That\nis, how varying-sized LLMs respond to given prompts stays similar on the\nlinguistic properties level. We expand upon this finding by training support\nvector machines that rely only upon the stylistic components of model responses\nto classify the truthfulness of statements. Though the dataset size limits our\ncurrent findings, we show the possibility that truthfulness detection is\npossible without evaluating the content itself. But at the same time, the\nlimited scope of our experiments must be taken into account in interpreting the\nresults.",
        "pdf_link": "https://arxiv.org/pdf/2305.15875v2.pdf"
    },
    {
        "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
        "authors": [
            "Niels M\u00fcndler",
            "Jingxuan He",
            "Slobodan Jenko",
            "Martin Vechev"
        ],
        "published": "2023-05-25T08:43:46Z",
        "summary": "Large language models (large LMs) are susceptible to producing text that\ncontains hallucinated content. An important instance of this problem is\nself-contradiction, where the LM generates two contradictory sentences within\nthe same context. In this work, we present a comprehensive investigation into\nself-contradiction for various instruction-tuned LMs, covering evaluation,\ndetection, and mitigation. Our primary evaluation task is open-domain text\ngeneration, but we also demonstrate the applicability of our approach to\nshorter question answering. Our analysis reveals the prevalence of\nself-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We\nthen propose a novel prompting-based framework designed to effectively detect\nand mitigate self-contradictions. Our detector achieves high accuracy, e.g.,\naround 80% F1 score when prompting ChatGPT. The mitigation algorithm\niteratively refines the generated text to remove contradictory information\nwhile preserving text fluency and informativeness. Importantly, our entire\nframework is applicable to black-box LMs and does not require retrieval of\nexternal knowledge. Rather, our method complements retrieval-based methods, as\na large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be\nverified using online text. Our approach is practically effective and has been\nreleased as a push-button tool to benefit the public at\nhttps://chatprotect.ai/.",
        "pdf_link": "https://arxiv.org/pdf/2305.15852v3.pdf"
    },
    {
        "title": "ChatGPT for PLC/DCS Control Logic Generation",
        "authors": [
            "Heiko Koziolek",
            "Sten Gruener",
            "Virendra Ashiwal"
        ],
        "published": "2023-05-25T07:46:53Z",
        "summary": "Large language models (LLMs) providing generative AI have become popular to\nsupport software engineers in creating, summarizing, optimizing, and\ndocumenting source code. It is still unknown how LLMs can support control\nengineers using typical control programming languages in programming tasks.\nResearchers have explored GitHub CoPilot or DeepMind AlphaCode for source code\ngeneration but did not yet tackle control logic programming. The contribution\nof this paper is an exploratory study, for which we created 100 LLM prompts in\n10 representative categories to analyze control logic generation for of PLCs\nand DCS from natural language. We tested the prompts by generating answers with\nChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3\nStructured Text code in many cases and demonstrated useful reasoning skills\nthat could boost control engineer productivity. Our prompt collection is the\nbasis for a more formal LLM benchmark to test and compare such models for\ncontrol logic generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.15809v1.pdf"
    },
    {
        "title": "Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback",
        "authors": [
            "Yiqi Lin",
            "Hao Wu",
            "Ruichen Wang",
            "Haonan Lu",
            "Xiaodong Lin",
            "Hui Xiong",
            "Lin Wang"
        ],
        "published": "2023-05-25T07:43:39Z",
        "summary": "Generating and editing a 3D scene guided by natural language poses a\nchallenge, primarily due to the complexity of specifying the positional\nrelations and volumetric changes within the 3D space. Recent advancements in\nLarge Language Models (LLMs) have demonstrated impressive reasoning,\nconversational, and zero-shot generation abilities across various domains.\nSurprisingly, these models also show great potential in realizing and\ninterpreting the 3D space. In light of this, we propose a novel language-guided\ninteractive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D\nlayout interpreter into the off-the-shelf layout-to-3D generative models,\nallowing users to flexibly and interactively generate visual content.\nSpecifically, we design a versatile layout structure base on the bounding boxes\nand semantics to prompt the LLMs to model the spatial generation and reasoning\nfrom language. Our system also incorporates LLaVA, a large language and vision\nassistant, to provide generative feedback from the visual aspect for improving\nthe visual quality of generated content. We validate the effectiveness of LI3D,\nprimarily in 3D generation and editing through multi-round interactions, which\ncan be flexibly extended to 2D generation and editing. Various experiments\ndemonstrate the potential benefits of incorporating LLMs in generative AI for\napplications, e.g., metaverse. Moreover, we benchmark the layout reasoning\nperformance of LLMs with neural visual artist tasks, revealing their emergent\nability in the spatial layout domain.",
        "pdf_link": "https://arxiv.org/pdf/2305.15808v1.pdf"
    },
    {
        "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
        "authors": [
            "Sotiris Anagnostidis",
            "Dario Pavllo",
            "Luca Biggio",
            "Lorenzo Noci",
            "Aurelien Lucchi",
            "Thomas Hofmann"
        ],
        "published": "2023-05-25T07:39:41Z",
        "summary": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard\nto scale to long sequences. Despite several works trying to reduce their\ncomputational cost, most of LLMs still adopt attention layers between all pairs\nof tokens in the sequence, thus incurring a quadratic cost. In this study, we\npresent a novel approach that dynamically prunes contextual information while\npreserving the model's expressiveness, resulting in reduced memory and\ncomputational requirements during inference. Our method employs a learnable\nmechanism that determines which uninformative tokens can be dropped from the\ncontext at any point across the generation process. By doing so, our approach\nnot only addresses performance concerns but also enhances interpretability,\nproviding valuable insight into the model's decision-making process. Our\ntechnique can be applied to existing pre-trained models through a\nstraightforward fine-tuning process, and the pruning strength can be specified\nby a sparsity parameter. Notably, our empirical findings demonstrate that we\ncan effectively prune up to 80\\% of the context without significant performance\ndegradation on downstream tasks, offering a valuable tool for mitigating\ninference costs. Our reference implementation achieves up to $2\\times$ increase\nin inference throughput and even greater memory savings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15805v2.pdf"
    },
    {
        "title": "On the Planning Abilities of Large Language Models : A Critical Investigation",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-25T06:32:23Z",
        "summary": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2305.15771v2.pdf"
    },
    {
        "title": "The False Promise of Imitating Proprietary LLMs",
        "authors": [
            "Arnav Gudibande",
            "Eric Wallace",
            "Charlie Snell",
            "Xinyang Geng",
            "Hao Liu",
            "Pieter Abbeel",
            "Sergey Levine",
            "Dawn Song"
        ],
        "published": "2023-05-25T05:00:12Z",
        "summary": "An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.15717v1.pdf"
    },
    {
        "title": "Asking Before Action: Gather Information in Embodied Decision Making with Language Models",
        "authors": [
            "Xiaoyu Chen",
            "Shenao Zhang",
            "Pushi Zhang",
            "Li Zhao",
            "Jianyu Chen"
        ],
        "published": "2023-05-25T04:05:08Z",
        "summary": "With strong capabilities of reasoning and a generic understanding of the\nworld, Large Language Models (LLMs) have shown great potential in building\nversatile embodied decision making agents capable of performing diverse tasks.\nHowever, when deployed to unfamiliar environments, we show that LLM agents face\nchallenges in efficiently gathering necessary information, leading to\nsuboptimal performance. On the other hand, in unfamiliar scenarios, human\nindividuals often seek additional information from their peers before taking\naction, leveraging external knowledge to avoid unnecessary trial and error.\nBuilding upon this intuition, we propose \\textit{Asking Before Action} (ABA), a\nmethod that empowers the agent to proactively query external sources for\npertinent information using natural language during their interactions in the\nenvironment. In this way, the agent is able to enhance its efficiency and\nperformance by mitigating wasteful steps and circumventing the difficulties\nassociated with exploration in unfamiliar environments. We empirically evaluate\nour method on an embodied decision making benchmark, ALFWorld, and demonstrate\nthat despite modest modifications in prompts, our method exceeds baseline LLM\nagents by more than $40$%. Further experiments on two variants of ALFWorld\nillustrate that by imitation learning, ABA effectively retains and reuses\nqueried and known information in subsequent tasks, mitigating the need for\nrepetitive inquiries. Both qualitative and quantitative results exhibit\nremarkable performance on tasks that previous methods struggle to solve.",
        "pdf_link": "https://arxiv.org/pdf/2305.15695v1.pdf"
    },
    {
        "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting",
        "authors": [
            "Lei Shu",
            "Liangchen Luo",
            "Jayakumar Hoskere",
            "Yun Zhu",
            "Yinxiao Liu",
            "Simon Tong",
            "Jindong Chen",
            "Lei Meng"
        ],
        "published": "2023-05-25T03:26:26Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncreative tasks such as storytelling and E-mail generation. However, as LLMs are\nprimarily trained on final text results rather than intermediate revisions, it\nmight be challenging for them to perform text rewriting tasks. Most studies in\nthe rewriting tasks focus on a particular transformation type within the\nboundaries of single sentences. In this work, we develop new strategies for\ninstruction tuning and reinforcement learning to better align LLMs for\ncross-sentence rewriting tasks using diverse wording and structures expressed\nthrough natural languages including 1) generating rewriting instruction data\nfrom Wiki edits and public corpus through instruction generation and\nchain-of-thought prompting; 2) collecting comparison data for reward model\ntraining through a new ranking function. To facilitate this research, we\nintroduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting\ntypes expressed through natural language instructions. Our results show\nsignificant improvements over a variety of baselines. The public repository is\navailable on GitHub under Google Research\n(https://github.com/google-research/google-research/tree/master/rewritelm).",
        "pdf_link": "https://arxiv.org/pdf/2305.15685v2.pdf"
    },
    {
        "title": "Undetectable Watermarks for Language Models",
        "authors": [
            "Miranda Christ",
            "Sam Gunn",
            "Or Zamir"
        ],
        "published": "2023-05-25T02:57:16Z",
        "summary": "Recent advances in the capabilities of large language models such as GPT-4\nhave spurred increasing concern about our ability to detect AI-generated text.\nPrior works have suggested methods of embedding watermarks in model outputs, by\nnoticeably altering the output distribution. We ask: Is it possible to\nintroduce a watermark without incurring any detectable change to the output\ndistribution?\n  To this end we introduce a cryptographically-inspired notion of undetectable\nwatermarks for language models. That is, watermarks can be detected only with\nthe knowledge of a secret key; without the secret key, it is computationally\nintractable to distinguish watermarked outputs from those of the original\nmodel. In particular, it is impossible for a user to observe any degradation in\nthe quality of the text. Crucially, watermarks should remain undetectable even\nwhen the user is allowed to adaptively query the model with arbitrarily chosen\nprompts. We construct undetectable watermarks based on the existence of one-way\nfunctions, a standard assumption in cryptography.",
        "pdf_link": "https://arxiv.org/pdf/2306.09194v1.pdf"
    },
    {
        "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
        "authors": [
            "Haonan Duan",
            "Adam Dziedzic",
            "Nicolas Papernot",
            "Franziska Boenisch"
        ],
        "published": "2023-05-24T22:06:08Z",
        "summary": "Large language models (LLMs) are excellent in-context learners. However, the\nsensitivity of data contained in prompts raises privacy concerns. Our work\nfirst shows that these concerns are valid: we instantiate a simple but highly\neffective membership inference attack against the data used to prompt LLMs. To\naddress this vulnerability, one could forego prompting and resort to\nfine-tuning LLMs with known algorithms for private gradient descent. However,\nthis comes at the expense of the practicality and efficiency offered by\nprompting. Therefore, we propose to privately learn to prompt. We first show\nthat soft prompts can be obtained privately through gradient descent on\ndownstream data. However, this is not the case for discrete prompts. Thus, we\norchestrate a noisy vote among an ensemble of LLMs presented with different\nprompts, i.e., a flock of stochastic parrots. The vote privately transfers the\nflock's knowledge into a single public prompt. We show that LLMs prompted with\nour private algorithms closely match the non-private baselines. For example,\nusing GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the\nsst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.\n95.2% for the non-private baseline. Through our experiments, we also show that\nour prompt-based approach is easily deployed with existing commercial APIs.",
        "pdf_link": "https://arxiv.org/pdf/2305.15594v1.pdf"
    },
    {
        "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
        "authors": [
            "Yuan Yang",
            "Siheng Xiong",
            "Ali Payani",
            "Ehsan Shareghi",
            "Faramarz Fekri"
        ],
        "published": "2023-05-24T19:59:51Z",
        "summary": "Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel\ngener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}$.",
        "pdf_link": "https://arxiv.org/pdf/2305.15541v1.pdf"
    },
    {
        "title": "Large Language Models are Few-Shot Health Learners",
        "authors": [
            "Xin Liu",
            "Daniel McDuff",
            "Geza Kovacs",
            "Isaac Galatzer-Levy",
            "Jacob Sunshine",
            "Jiening Zhan",
            "Ming-Zher Poh",
            "Shun Liao",
            "Paolo Di Achille",
            "Shwetak Patel"
        ],
        "published": "2023-05-24T19:25:16Z",
        "summary": "Large language models (LLMs) can capture rich representations of concepts\nthat are useful for real-world tasks. However, language alone is limited. While\nexisting LLMs excel at text-based inferences, health applications require that\nmodels be grounded in numerical data (e.g., vital signs, laboratory values in\nclinical domains; steps, movement in the wellness domain) that is not easily or\nreadily expressed as text in existing training corpus. We demonstrate that with\nonly few-shot tuning, a large language model is capable of grounding various\nphysiological and behavioral time-series data and making meaningful inferences\non numerous health tasks for both clinical and wellness contexts. Using data\nfrom wearable and medical sensor recordings, we evaluate these capabilities on\nthe tasks of cardiac signal analysis, physical activity recognition, metabolic\ncalculation (e.g., calories burned), and estimation of stress reports and\nmental health screeners.",
        "pdf_link": "https://arxiv.org/pdf/2305.15525v1.pdf"
    },
    {
        "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",
        "authors": [
            "Antonio Valerio Miceli-Barone",
            "Fazl Barez",
            "Ioannis Konstas",
            "Shay B. Cohen"
        ],
        "published": "2023-05-24T18:54:39Z",
        "summary": "Large Language Models (LLMs) have successfully been applied to code\ngeneration tasks, raising the question of how well these models understand\nprogramming. Typical programming languages have invariances and equivariances\nin their semantics that human programmers intuitively understand and exploit,\nsuch as the (near) invariance to the renaming of identifiers. We show that LLMs\nnot only fail to properly generate correct Python code when default function\nnames are swapped, but some of them even become more confident in their\nincorrect predictions as the model size increases, an instance of the recently\ndiscovered phenomenon of Inverse Scaling, which runs contrary to the commonly\nobserved trend of increasing prediction quality with increasing model size. Our\nfindings indicate that, despite their astonishing typical-case performance,\nLLMs still lack a deep, abstract understanding of the content they manipulate,\nmaking them unsuitable for tasks that statistically deviate from their training\ndata, and that mere scaling is not enough to achieve such capability.",
        "pdf_link": "https://arxiv.org/pdf/2305.15507v1.pdf"
    },
    {
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective",
        "authors": [
            "Guhao Feng",
            "Bohang Zhang",
            "Yuntian Gu",
            "Haotian Ye",
            "Di He",
            "Liwei Wang"
        ],
        "published": "2023-05-24T17:59:21Z",
        "summary": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can\ndramatically improve the performance of Large Language Models (LLMs),\nparticularly when dealing with complex tasks involving mathematics or\nreasoning. Despite the enormous empirical success, the underlying mechanisms\nbehind CoT and how it unlocks the potential of LLMs remain elusive. In this\npaper, we take a first step towards theoretically answering these questions.\nSpecifically, we examine the expressivity of LLMs with CoT in solving\nfundamental mathematical and decision-making problems. By using circuit\ncomplexity theory, we first give impossibility results showing that\nbounded-depth Transformers are unable to directly produce correct answers for\nbasic arithmetic/equation tasks unless the model size grows super-polynomially\nwith respect to the input length. In contrast, we then prove by construction\nthat autoregressive Transformers of constant size suffice to solve both tasks\nby generating CoT derivations using a commonly used math language format.\nMoreover, we show LLMs with CoT can handle a general class of decision-making\nproblems known as Dynamic Programming, thus justifying its power in tackling\ncomplex real-world tasks. Finally, an extensive set of experiments show that,\nwhile Transformers always fail to directly predict the answers, they can\nconsistently learn to generate correct solutions step-by-step given sufficient\nCoT demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2305.15408v5.pdf"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
        "authors": [
            "Weixi Feng",
            "Wanrong Zhu",
            "Tsu-jui Fu",
            "Varun Jampani",
            "Arjun Akula",
            "Xuehai He",
            "Sugato Basu",
            "Xin Eric Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-24T17:56:16Z",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "pdf_link": "https://arxiv.org/pdf/2305.15393v2.pdf"
    },
    {
        "title": "Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing",
        "authors": [
            "Shufan Wang",
            "Sebastien Jean",
            "Sailik Sengupta",
            "James Gung",
            "Nikolaos Pappas",
            "Yi Zhang"
        ],
        "published": "2023-05-24T16:50:36Z",
        "summary": "In executable task-oriented semantic parsing, the system aims to translate\nusers' utterances in natural language to machine-interpretable programs (API\ncalls) that can be executed according to pre-defined API specifications. With\nthe popularity of Large Language Models (LLMs), in-context learning offers a\nstrong baseline for such scenarios, especially in data-limited regimes.\nHowever, LLMs are known to hallucinate and therefore pose a formidable\nchallenge in constraining generated content. Thus, it remains uncertain if LLMs\ncan effectively perform task-oriented utterance-to-API generation where\nrespecting API's structural and task-specific constraints is crucial.\n  In this work, we seek to measure, analyze and mitigate such constraints\nviolations. First, we identify the categories of various constraints in\nobtaining API-semantics from task-oriented utterances, and define fine-grained\nmetrics that complement traditional ones. Second, we leverage these metrics to\nconduct a detailed error analysis of constraints violations seen in\nstate-of-the-art LLMs, which motivates us to investigate two mitigation\nstrategies: Semantic-Retrieval of Demonstrations (SRD) and API-aware\nConstrained Decoding (API-CD). Our experiments show that these strategies are\neffective at reducing constraints violations and improving the quality of the\ngenerated API calls, but require careful consideration given their\nimplementation complexity and latency.",
        "pdf_link": "https://arxiv.org/pdf/2305.15338v1.pdf"
    },
    {
        "title": "Gorilla: Large Language Model Connected with Massive APIs",
        "authors": [
            "Shishir G. Patil",
            "Tianjun Zhang",
            "Xin Wang",
            "Joseph E. Gonzalez"
        ],
        "published": "2023-05-24T16:48:11Z",
        "summary": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu",
        "pdf_link": "https://arxiv.org/pdf/2305.15334v1.pdf"
    },
    {
        "title": "Visual Programming for Text-to-Image Generation and Evaluation",
        "authors": [
            "Jaemin Cho",
            "Abhay Zala",
            "Mohit Bansal"
        ],
        "published": "2023-05-24T16:42:17Z",
        "summary": "As large language models have demonstrated impressive performance in many\ndomains, recent works have adopted language models (LMs) as controllers of\nvisual modules for vision-and-language tasks. While existing work focuses on\nequipping LMs with visual understanding, we propose two novel\ninterpretable/explainable visual programming frameworks for text-to-image (T2I)\ngeneration and evaluation. First, we introduce VPGen, an interpretable\nstep-by-step T2I generation framework that decomposes T2I generation into three\nsteps: object/count generation, layout generation, and image generation. We\nemploy an LM to handle the first two steps (object/count generation and layout\ngeneration), by finetuning it on text-layout pairs. Our step-by-step T2I\ngeneration framework provides stronger spatial control than end-to-end models,\nthe dominant approach for this task. Furthermore, we leverage the world\nknowledge of pretrained LMs, overcoming the limitation of previous\nlayout-guided T2I works that can only handle predefined object classes. We\ndemonstrate that our VPGen has improved control in counts/spatial\nrelations/scales of objects than state-of-the-art T2I generation models.\nSecond, we introduce VPEval, an interpretable and explainable evaluation\nframework for T2I generation based on visual programming. Unlike previous T2I\nevaluations with a single scoring model that is accurate in some skills but\nunreliable in others, VPEval produces evaluation programs that invoke a set of\nvisual modules that are experts in different skills, and also provides\nvisual+textual explanations of the evaluation results. Our analysis shows that\nVPEval provides a more human-correlated evaluation for skill-specific and\nopen-ended prompts than widely used single model-based evaluation. We hope that\nour work encourages future progress on interpretable/explainable generation and\nevaluation for T2I models.",
        "pdf_link": "https://arxiv.org/pdf/2305.15328v2.pdf"
    },
    {
        "title": "Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",
        "authors": [
            "Evangelos Pournaras"
        ],
        "published": "2023-05-24T16:23:46Z",
        "summary": "Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
        "pdf_link": "https://arxiv.org/pdf/2305.15299v4.pdf"
    },
    {
        "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
        "authors": [
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Minlie Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-05-24T16:17:36Z",
        "summary": "Large language models are powerful text processors and reasoners, but are\nstill subject to limitations including outdated knowledge and hallucinations,\nwhich necessitates connecting them to the world. Retrieval-augmented large\nlanguage models have raised extensive attention for grounding model generation\non external knowledge. However, retrievers struggle to capture relevance,\nespecially for queries with complex information needs. Recent work has proposed\nto improve relevance modeling by having large language models actively involved\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\nshow that strong performance can be achieved by a method we call Iter-RetGen,\nwhich synergizes retrieval and generation in an iterative manner. A model\noutput shows what might be needed to finish a task, and thus provides an\ninformative context for retrieving more relevant knowledge which in turn helps\ngenerate a better output in the next iteration. Compared with recent work which\ninterleaves retrieval with generation when producing an output, Iter-RetGen\nprocesses all retrieved knowledge as a whole and largely preserves the\nflexibility in generation without structural constraints. We evaluate\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\nreasoning, and show that it can flexibly leverage parametric knowledge and\nnon-parametric knowledge, and is superior to or competitive with\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\nretrieval and generation. We can further improve performance via\ngeneration-augmented retrieval adaptation.",
        "pdf_link": "https://arxiv.org/pdf/2305.15294v2.pdf"
    },
    {
        "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification",
        "authors": [
            "Rohan Bhambhoria",
            "Lei Chen",
            "Xiaodan Zhu"
        ],
        "published": "2023-05-24T16:04:26Z",
        "summary": "In recent years, large language models (LLMs) have achieved strong\nperformance on benchmark tasks, especially in zero or few-shot settings.\nHowever, these benchmarks often do not adequately address the challenges posed\nin the real-world, such as that of hierarchical classification. In order to\naddress this challenge, we propose refactoring conventional tasks on\nhierarchical datasets into a more indicative long-tail prediction task. We\nobserve LLMs are more prone to failure in these cases. To address these\nlimitations, we propose the use of entailment-contradiction prediction in\nconjunction with LLMs, which allows for strong performance in a strict\nzero-shot setting. Importantly, our method does not require any parameter\nupdates, a resource-intensive process and achieves strong performance across\nmultiple datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.15282v2.pdf"
    },
    {
        "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Xuebo Liu",
            "Min Zhang",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023-05-24T15:59:44Z",
        "summary": "Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.",
        "pdf_link": "https://arxiv.org/pdf/2305.15273v1.pdf"
    },
    {
        "title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples",
        "authors": [
            "Abulhair Saparov",
            "Richard Yuanzhe Pang",
            "Vishakh Padmakumar",
            "Nitish Joshi",
            "Seyed Mehran Kazemi",
            "Najoung Kim",
            "He He"
        ],
        "published": "2023-05-24T15:55:51Z",
        "summary": "Given the intractably large size of the space of proofs, any model that is\ncapable of general deductive reasoning must generalize to proofs of greater\ncomplexity. Recent studies have shown that large language models (LLMs) possess\nsome abstract deductive reasoning ability given chain-of-thought prompts.\nHowever, they have primarily been tested on proofs using modus ponens or of a\nspecific size, and from the same distribution as the in-context examples. To\nmeasure the general deductive reasoning ability of LLMs, we test on a broad set\nof deduction rules and measure their ability to generalize to more complex\nproofs from simpler demonstrations from multiple angles: depth-, width-, and\ncompositional generalization. To facilitate systematic exploration, we\nconstruct a new synthetic and programmable reasoning dataset that enables\ncontrol over deduction rules and proof complexity. Our experiments on four LLMs\nof various sizes and training objectives show that they are able to generalize\nto compositional proofs. However, they have difficulty generalizing to longer\nproofs, and they require explicit demonstrations to produce hypothetical\nsubproofs, specifically in proof by cases and proof by contradiction.",
        "pdf_link": "https://arxiv.org/pdf/2305.15269v3.pdf"
    },
    {
        "title": "EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models",
        "authors": [
            "Zhengwei Tao",
            "Zhi Jin",
            "Xiaoying Bai",
            "Haiyan Zhao",
            "Yanlin Feng",
            "Jia Li",
            "Wenpeng Hu"
        ],
        "published": "2023-05-24T15:55:40Z",
        "summary": "Events serve as fundamental units of occurrence within various contexts. The\nprocessing of event semantics in textual information forms the basis of\nnumerous natural language processing (NLP) applications. Recent studies have\nbegun leveraging large language models (LLMs) to address event semantic\nprocessing. However, the extent that LLMs can effectively tackle these\nchallenges remains uncertain. Furthermore, the lack of a comprehensive\nevaluation framework for event semantic processing poses a significant\nchallenge in evaluating these capabilities. In this paper, we propose an\noverarching framework for event semantic processing, encompassing\nunderstanding, reasoning, and prediction, along with their fine-grained\naspects. To comprehensively evaluate the event semantic processing abilities of\nmodels, we introduce a novel benchmark called EVEVAL. We collect 8 datasets\nthat cover all aspects of event semantic processing. Extensive experiments are\nconducted on EVEVAL, leading to several noteworthy findings based on the\nobtained results.",
        "pdf_link": "https://arxiv.org/pdf/2305.15268v1.pdf"
    },
    {
        "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
        "authors": [
            "Zirui Liu",
            "Guanchu Wang",
            "Shaochen Zhong",
            "Zhaozhuo Xu",
            "Daochen Zha",
            "Ruixiang Tang",
            "Zhimeng Jiang",
            "Kaixiong Zhou",
            "Vipin Chaudhary",
            "Shuai Xu",
            "Xia Hu"
        ],
        "published": "2023-05-24T15:52:08Z",
        "summary": "With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.",
        "pdf_link": "https://arxiv.org/pdf/2305.15265v2.pdf"
    },
    {
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration",
        "authors": [
            "Kejuan Yang",
            "Xiao Liu",
            "Kaiwen Men",
            "Aohan Zeng",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-05-24T15:48:29Z",
        "summary": "We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models'\nlong context understanding ability should be paid.",
        "pdf_link": "https://arxiv.org/pdf/2305.15262v1.pdf"
    },
    {
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
            "Eliya Nachmani",
            "Alon Levkovitch",
            "Roy Hirsch",
            "Julian Salazar",
            "Chulayuth Asawaroengchai",
            "Soroosh Mariooryad",
            "Ehud Rivlin",
            "RJ Skerry-Ryan",
            "Michelle Tadmor Ramanovich"
        ],
        "published": "2023-05-24T15:39:43Z",
        "summary": "We present a novel approach to adapting pre-trained large language models\n(LLMs) to perform question answering (QA) and speech continuation. By endowing\nthe LLM with a pre-trained speech encoder, our model becomes able to take\nspeech inputs and generate speech outputs. The entire system is trained\nend-to-end and operates directly on spectrograms, simplifying our architecture.\nKey to our approach is a training objective that jointly supervises speech\nrecognition, text continuation, and speech synthesis using only paired\nspeech-text pairs, enabling a `cross-modal' chain-of-thought within a single\ndecoding pass. Our method surpasses existing spoken language models in speaker\npreservation and semantic coherence. Furthermore, the proposed model improves\nupon direct initialization in retaining the knowledge of the original LLM as\ndemonstrated through spoken QA datasets. Audio samples can be found at\nhttps://michelleramanovich.github.io/spectron/spectron",
        "pdf_link": "https://arxiv.org/pdf/2305.15255v3.pdf"
    },
    {
        "title": "Machine Unlearning: its nature, scope, and importance for a \"delete culture\"",
        "authors": [
            "Luciano Floridi"
        ],
        "published": "2023-05-24T15:27:04Z",
        "summary": "The article explores the cultural shift from recording to deleting\ninformation in the digital age and its implications on privacy, intellectual\nproperty (IP), and Large Language Models like ChatGPT. It begins by defining a\ndelete culture where information, in principle legal, is made unavailable or\ninaccessible because unacceptable or undesirable, especially but not only due\nto its potential to infringe on privacy or IP. Then it focuses on two\nstrategies in this context: deleting, to make information unavailable; and\nblocking, to make it inaccessible. The article argues that both strategies have\nsignificant implications, particularly for machine learning (ML) models where\ninformation is not easily made unavailable. However, the emerging research area\nof Machine Unlearning (MU) is highlighted as a potential solution. MU, still in\nits infancy, seeks to remove specific data points from ML models, effectively\nmaking them 'forget' completely specific information. If successful, MU could\nprovide a feasible means to manage the overabundance of information and ensure\na better protection of privacy and IP. However, potential ethical risks, such\nas misuse, overuse, and underuse of MU, should be systematically studied to\ndevise appropriate policies.",
        "pdf_link": "https://arxiv.org/pdf/2305.15242v1.pdf"
    },
    {
        "title": "SAIL: Search-Augmented Instruction Learning",
        "authors": [
            "Hongyin Luo",
            "Yung-Sung Chuang",
            "Yuan Gong",
            "Tianhua Zhang",
            "Yoon Kim",
            "Xixin Wu",
            "Danny Fox",
            "Helen Meng",
            "James Glass"
        ],
        "published": "2023-05-24T15:07:30Z",
        "summary": "Large language models (LLMs) have been significantly improved by instruction\nfine-tuning, but still lack transparency and the ability to utilize up-to-date\nknowledge and information. In this work, we propose search-augmented\ninstruction learning (SAIL), which grounds the language generation and\ninstruction following abilities on complex search results generated by in-house\nand external search engines. With an instruction tuning corpus, we collect\nsearch results for each training case from different search APIs and domains,\nand construct a new search-grounded training set containing\n\\textit{(instruction, grounding information, response)} triplets. We then\nfine-tune the LLaMA-7B model on the constructed training set. Since the\ncollected results contain unrelated and disputing languages, the model needs to\nlearn to ground on trustworthy search results, filter out distracting passages,\nand generate the target response. The search result-denoising process entails\nexplicit trustworthy information selection and multi-hop reasoning, since the\nretrieved passages might be informative but not contain the\ninstruction-following answer. Experiments show that the fine-tuned SAIL-7B\nmodel has a strong instruction-following ability, and it performs significantly\nbetter on transparency-sensitive tasks, including open-ended question answering\nand fact checking.",
        "pdf_link": "https://arxiv.org/pdf/2305.15225v2.pdf"
    },
    {
        "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
        "authors": [
            "Tetsu Kasanishi",
            "Masaru Isonuma",
            "Junichiro Mori",
            "Ichiro Sakata"
        ],
        "published": "2023-05-24T14:26:30Z",
        "summary": "Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.",
        "pdf_link": "https://arxiv.org/pdf/2305.15186v1.pdf"
    },
    {
        "title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models",
        "authors": [
            "Mingyu Derek Ma",
            "Xiaoxuan Wang",
            "Po-Nien Kung",
            "P. Jeffrey Brantingham",
            "Nanyun Peng",
            "Wei Wang"
        ],
        "published": "2023-05-24T12:15:19Z",
        "summary": "Information extraction tasks such as event extraction require an in-depth\nunderstanding of the output structure and sub-task dependencies. They heavily\nrely on task-specific training data in the form of (passage, target structure)\npairs to obtain reasonable performance. However, obtaining such data through\nhuman annotation is costly, leading to a pressing need for low-resource\ninformation extraction approaches that require minimal human labeling for\nreal-world applications. Fine-tuning supervised models with synthesized\ntraining data would be a generalizable method, but the existing data generation\nmethods either still rely on large-scale ground-truth data or cannot be applied\nto complicated IE tasks due to their poor performance. To address these\nchallenges, we propose STAR, a data generation method that leverages Large\nLanguage Models (LLMs) to synthesize data instances given limited seed\ndemonstrations, thereby boosting low-resource information extraction\nperformance. Our approach involves generating target structures (Y) followed by\ngenerating passages (X), all accomplished with the aid of LLMs. We design\nfine-grained step-by-step instructions to obtain the initial data instances. We\nfurther reduce errors and improve data quality through self-reflection error\nidentification and self-refinement with iterative revision. Our experiments\nshow that the data generated by STAR significantly improve the performance of\nlow-resource event extraction and relation extraction tasks, even surpassing\nthe effectiveness of human-curated data. Human assessment of the data quality\nshows STAR-generated data exhibits higher passage quality and better align with\nthe task definitions compared with the human-curated data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15090v3.pdf"
    },
    {
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "authors": [
            "Geewook Kim",
            "Hodong Lee",
            "Daehee Kim",
            "Haeji Jung",
            "Sanghee Park",
            "Yoonsik Kim",
            "Sangdoo Yun",
            "Taeho Kil",
            "Bado Lee",
            "Seunghyun Park"
        ],
        "published": "2023-05-24T11:59:13Z",
        "summary": "Recent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream .",
        "pdf_link": "https://arxiv.org/pdf/2305.15080v2.pdf"
    },
    {
        "title": "Contrastive Learning of Sentence Embeddings from Scratch",
        "authors": [
            "Junlei Zhang",
            "Zhenzhong Lan",
            "Junxian He"
        ],
        "published": "2023-05-24T11:56:21Z",
        "summary": "Contrastive learning has been the dominant approach to train state-of-the-art\nsentence embeddings. Previous studies have typically learned sentence\nembeddings either through the use of human-annotated natural language inference\n(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.\nHowever, even in the case of unlabeled data, their acquisition presents\nchallenges in certain domains due to various reasons. To address these issues,\nwe present SynCSE, a contrastive learning framework that trains sentence\nembeddings with synthesized data. Specifically, we explore utilizing large\nlanguage models to synthesize the required data samples for contrastive\nlearning, including (1) producing positive and negative annotations given\nunlabeled sentences (SynCSE-partial), and (2) generating sentences along with\ntheir corresponding annotations from scratch (SynCSE-scratch). Experimental\nresults on sentence similarity and reranking tasks indicate that both\nSynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,\nand SynCSE-partial even achieves comparable performance to the supervised\nmodels in most settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15077v2.pdf"
    },
    {
        "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor",
        "authors": [
            "Hongbo Zhang",
            "Junying Chen",
            "Feng Jiang",
            "Fei Yu",
            "Zhihong Chen",
            "Jianquan Li",
            "Guiming Chen",
            "Xiangbo Wu",
            "Zhiyi Zhang",
            "Qingying Xiao",
            "Xiang Wan",
            "Benyou Wang",
            "Haizhou Li"
        ],
        "published": "2023-05-24T11:56:01Z",
        "summary": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
        "pdf_link": "https://arxiv.org/pdf/2305.15075v1.pdf"
    },
    {
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "authors": [
            "Daman Arora",
            "Himanshu Gaurav Singh",
            "Mausam"
        ],
        "published": "2023-05-24T11:55:59Z",
        "summary": "The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.15074v3.pdf"
    },
    {
        "title": "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind",
        "authors": [
            "Xiaomeng Ma",
            "Lingyu Gao",
            "Qihui Xu"
        ],
        "published": "2023-05-24T11:54:07Z",
        "summary": "Theory of Mind (ToM), the capacity to comprehend the mental states of\ndistinct individuals, is essential for numerous practical applications. With\nthe development of large language models (LLMs), there is a heated debate about\nwhether they are able to perform ToM tasks. Previous studies have used\ndifferent tasks and prompts to test the ToM on LLMs and the results are\ninconsistent: some studies asserted these models are capable of exhibiting ToM,\nwhile others suggest the opposite. In this study, We present ToMChallenges, a\ndataset for comprehensively evaluating the Theory of Mind based on the\nSally-Anne and Smarties tests with a diverse set of tasks. In addition, we also\npropose an auto-grader to streamline the answer evaluation process. We tested\nthree models: davinci, turbo, and gpt-4. Our evaluation results and error\nanalyses show that LLMs have inconsistent behaviors across prompts and tasks.\nPerforming the ToM tasks robustly remains a challenge for the LLMs. In\naddition, our paper wants to raise awareness in evaluating the ToM in LLMs and\nwe want to invite more discussion on how to design the prompts and tasks for\nToM tasks that can better assess the LLMs' ability.",
        "pdf_link": "https://arxiv.org/pdf/2305.15068v2.pdf"
    },
    {
        "title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References",
        "authors": [
            "Tianyi Tang",
            "Hongyuan Lu",
            "Yuchen Eleanor Jiang",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Wayne Xin Zhao",
            "Tom Kocmi",
            "Furu Wei"
        ],
        "published": "2023-05-24T11:53:29Z",
        "summary": "Most research about natural language generation (NLG) relies on evaluation\nbenchmarks with limited references for a sample, which may result in poor\ncorrelations with human judgements. The underlying reason is that one semantic\nmeaning can actually be expressed in different forms, and the evaluation with a\nsingle or few references may not accurately reflect the quality of the model's\nhypotheses. To address this issue, this paper presents a simple and effective\nmethod, named Div-Ref, to enhance existing evaluation benchmarks by enriching\nthe number of references. We leverage large language models (LLMs) to diversify\nthe expression of a single reference into multiple high-quality ones to cover\nthe semantic space of the reference sentence as much as possible. We conduct\ncomprehensive experiments to empirically demonstrate that diversifying the\nexpression of reference can significantly enhance the correlation between\nautomatic evaluation and human evaluation. This idea is compatible with recent\nLLM-based evaluation which can similarly derive advantages from incorporating\nmultiple references. We strongly encourage future generation benchmarks to\ninclude more references, even if they are generated by LLMs, which is once for\nall. We release all the code and data at https://github.com/RUCAIBox/Div-Ref to\nfacilitate research.",
        "pdf_link": "https://arxiv.org/pdf/2305.15067v2.pdf"
    },
    {
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "authors": [
            "Jiayan Guo",
            "Lun Du",
            "Hengyu Liu",
            "Mengyu Zhou",
            "Xinyi He",
            "Shi Han"
        ],
        "published": "2023-05-24T11:53:19Z",
        "summary": "Large language models~(LLM) like ChatGPT have become indispensable to\nartificial general intelligence~(AGI), demonstrating excellent performance in\nvarious natural language processing tasks. In the real world, graph data is\nubiquitous and an essential part of AGI and prevails in domains like social\nnetwork analysis, bioinformatics and recommender systems. The training corpus\nof large language models often includes some algorithmic components, which\nallows them to achieve certain effects on some graph data-related problems.\nHowever, there is still little research on their performance on a broader range\nof graph-structured data. In this study, we conduct an extensive investigation\nto assess the proficiency of LLMs in comprehending graph data, employing a\ndiverse range of structural and semantic-related tasks. Our analysis\nencompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph\nunderstanding. Through our study, we not only uncover the current limitations\nof language models in comprehending graph structures and performing associated\nreasoning tasks but also emphasize the necessity for further advancements and\nnovel approaches to enhance their graph processing capabilities. Our findings\ncontribute valuable insights towards bridging the gap between language models\nand graph understanding, paving the way for more effective graph mining and\nknowledge extraction.",
        "pdf_link": "https://arxiv.org/pdf/2305.15066v2.pdf"
    },
    {
        "title": "Lawyer LLaMA Technical Report",
        "authors": [
            "Quzhe Huang",
            "Mingxu Tao",
            "Chen Zhang",
            "Zhenwei An",
            "Cong Jiang",
            "Zhibin Chen",
            "Zirui Wu",
            "Yansong Feng"
        ],
        "published": "2023-05-24T11:52:07Z",
        "summary": "Large Language Models (LLMs), like LLaMA, have exhibited remarkable\nperformance across various tasks. Nevertheless, when deployed to specific\ndomains such as law or medicine, the models still confront the challenge of a\ndeficiency in domain-specific knowledge and an inadequate capability to\nleverage that knowledge to resolve domain-related problems. In this paper, we\npropose a new framework to adapt LLMs to specific domains and build Lawyer\nLLaMA, a legal domain LLM, based on this framework. Specifically, we inject\ndomain knowledge during the continual training stage and teach the model to\nlearn professional skills using properly designed supervised fine-tuning tasks.\nMoreover, to alleviate the hallucination problem during the model's generation,\nwe add a retrieval module and extract relevant legal articles before the model\nanswers any queries. When learning domain-specific skills, we find that\nexperts' experience is much more useful than experiences distilled from\nChatGPT, where hundreds of expert-written data outperform tens of thousands of\nChatGPT-generated ones. We will release our model and data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15062v2.pdf"
    },
    {
        "title": "Who Wrote this Code? Watermarking for Code Generation",
        "authors": [
            "Taehyun Lee",
            "Seokhee Hong",
            "Jaewoo Ahn",
            "Ilgee Hong",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Jamin Shin",
            "Gunhee Kim"
        ],
        "published": "2023-05-24T11:49:52Z",
        "summary": "With the remarkable generation performance of large language models, ethical\nand legal concerns about using them have been raised, such as plagiarism and\ncopyright issues. For such concerns, several approaches to watermark and detect\nLLM-generated text have been proposed very recently. However, we discover that\nthe previous methods fail to function appropriately with code generation tasks\nbecause of the syntactic and semantic characteristics of code. Based on\n\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,\nSelective WatErmarking via Entropy Thresholding (SWEET), that promotes \"green\"\ntokens only at the position with high entropy of the token distribution during\ngeneration, thereby preserving the correctness of the generated code. The\nwatermarked code is detected by the statistical test and Z-score based on the\nentropy information. Our experiments on HumanEval and MBPP show that SWEET\nsignificantly improves the Pareto Frontier between the code correctness and\nwatermark detection performance. We also show that notable post-hoc detection\nmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show that\nsetting a reasonable entropy threshold is not much of a challenge. Code is\navailable at https://github.com/hongcheki/sweet-watermark.",
        "pdf_link": "https://arxiv.org/pdf/2305.15060v3.pdf"
    },
    {
        "title": "A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction",
        "authors": [
            "Erica Cai",
            "Brendan O'Connor"
        ],
        "published": "2023-05-24T11:41:33Z",
        "summary": "We consider dyadic zero-shot event extraction (EE) to identify actions\nbetween pairs of actors. The \\emph{zero-shot} setting allows social scientists\nor other non-computational researchers to extract any customized,\nuser-specified set of events without training, resulting in a \\emph{dyadic}\nevent database, allowing insight into sociopolitical relational dynamics among\nactors and the higher level organizations or countries they represent.\nUnfortunately, we find that current zero-shot EE methods perform poorly for the\ntask, with issues including word sense ambiguity, modality mismatch, and\nefficiency. Straightforward application of large language model prompting\ntypically performs even worse. We address these challenges with a new\nfine-grained, multi-stage generative question-answer method, using a Monte\nCarlo approach to exploit and overcome the randomness of generative outputs. It\nperforms 90\\% fewer queries than a previous approach, with strong performance\non the widely-used Automatic Content Extraction dataset. Finally, we extend our\nmethod to extract affiliations of actor arguments and demonstrate our method\nand findings on a dyadic international relations case study.",
        "pdf_link": "https://arxiv.org/pdf/2305.15051v1.pdf"
    },
    {
        "title": "Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Akhil Arora",
            "Martin Josifoski",
            "Ashton Anderson",
            "Robert West"
        ],
        "published": "2023-05-24T11:27:59Z",
        "summary": "Large Language Models (LLMs) have democratized synthetic data generation,\nwhich in turn has the potential to simplify and broaden a wide gamut of NLP\ntasks. Here, we tackle a pervasive problem in synthetic data generation: its\ngenerative distribution often differs from the distribution of real-world data\nresearchers care about (in other words, it is unfaithful). In a case study on\nsarcasm detection, we study three strategies to increase the faithfulness of\nsynthetic data: grounding, filtering, and taxonomy-based generation. We\nevaluate these strategies using the performance of classifiers trained with\ngenerated synthetic data on real-world data. While all three strategies improve\nthe performance of classifiers, we find that grounding works best for the task\nat hand. As synthetic data generation plays an ever-increasing role in NLP\nresearch, we expect this work to be a stepping stone in improving its utility.\nWe conclude this paper with some recommendations on how to generate\nhigh(er)-fidelity synthetic data for specific tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.15041v1.pdf"
    },
    {
        "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Yun-Nung Chen",
            "Hsin-Hsi Chen"
        ],
        "published": "2023-05-24T11:22:34Z",
        "summary": "Large language models (LLMs) have exhibited striking in-context learning\n(ICL) ability to adapt to target tasks with a few input-output demonstrations.\nFor better ICL, different methods are proposed to select representative\ndemonstrations from existing training corpora. However, such settings are not\naligned with real-world practices, as end-users usually query LMs without\naccess to demonstration pools. In this work, we introduce Self-ICL -- a simple\nframework which bootstraps LMs' intrinsic capabilities to perform zero-shot\nICL. Given a test input, Self-ICL first prompts the model to generate\npseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via\nzero-shot prompting. Finally, we perform ICL for the test input with the\npseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard\ntasks shows Self-ICL outperforms zero-shot baselines on both average accuracy\nand head-to-head comparison. Moreover, with zero-shot chain-of-thought,\nSelf-ICL achieves results comparable to using real demonstrations.\nAdditionally, we conduct a range of analyses to validate Self-ICL's\neffectiveness and provide insights for its behaviors under different settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15035v2.pdf"
    },
    {
        "title": "ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories",
        "authors": [
            "Heming Xia",
            "Qingxiu Dong",
            "Lei Li",
            "Jingjing Xu",
            "Tianyu Liu",
            "Ziwei Qin",
            "Zhifang Sui"
        ],
        "published": "2023-05-24T11:14:31Z",
        "summary": "Recently, Large Language Models (LLMs) have been serving as general-purpose\ninterfaces, posing a significant demand for comprehensive visual knowledge.\nHowever, it remains unclear how well current LLMs and their visually augmented\ncounterparts (VaLMs) can master visual commonsense knowledge. To investigate\nthis, we propose ImageNetVC, a human-annotated dataset specifically designed\nfor zero- and few-shot visual commonsense evaluation across 1,000 ImageNet\ncategories. Utilizing ImageNetVC, we benchmark the fundamental visual\ncommonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze\nthe factors affecting the visual commonsense knowledge of large-scale models,\nproviding insights into the development of language models enriched with visual\ncommonsense knowledge. Our code and dataset are available at\nhttps://github.com/hemingkx/ImageNetVC.",
        "pdf_link": "https://arxiv.org/pdf/2305.15028v2.pdf"
    },
    {
        "title": "ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification",
        "authors": [
            "Biao Zhao",
            "Weiqiang Jin",
            "Javier Del Ser",
            "Guang Yang"
        ],
        "published": "2023-05-24T11:06:23Z",
        "summary": "In the era of sustainable smart agriculture, a massive amount of agricultural\nnews text is being posted on the Internet, in which massive agricultural\nknowledge has been accumulated. In this context, it is urgent to explore\neffective text classification techniques for users to access the required\nagricultural knowledge with high efficiency. Mainstream deep learning\napproaches employing fine-tuning strategies on pre-trained language models\n(PLMs), have demonstrated remarkable performance gains over the past few years.\nNonetheless, these methods still face many drawbacks that are complex to solve,\nincluding: 1. Limited agricultural training data due to the expensive-cost and\nlabour-intensive annotation; 2. Poor domain transferability, especially of\ncross-linguistic ability; 3. Complex and expensive large models\ndeployment.Inspired by the extraordinary success brought by the recent ChatGPT\n(e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore\nthe capability and utilization of ChatGPT applying to the agricultural\ninformatization field. ....(shown in article).... Code has been released on\nGithub\nhttps://github.com/albert-jin/agricultural_textual_classification_ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.15024v1.pdf"
    }
]