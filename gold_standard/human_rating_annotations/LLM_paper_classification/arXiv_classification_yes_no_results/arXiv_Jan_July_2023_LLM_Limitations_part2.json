[
    {
        "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
        "authors": [
            "Gen Luo",
            "Yiyi Zhou",
            "Tianhe Ren",
            "Shengxin Chen",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published": "2023-05-24T11:06:15Z",
        "summary": "Recently, growing interest has been aroused in extending the multimodal\ncapability of large language models (LLMs), e.g., vision-language (VL)\nlearning, which is regarded as the next milestone of artificial general\nintelligence. However, existing solutions are prohibitively expensive, which\nnot only need to optimize excessive parameters, but also require another\nlarge-scale pre-training before VL instruction tuning. In this paper, we\npropose a novel and affordable solution for the effective VL adaption of LLMs,\ncalled Mixture-of-Modality Adaptation (MMA). Instead of using large neural\nnetworks to connect the image encoder and LLM, MMA adopts lightweight modules,\ni.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables\nthe joint optimization of the image and language models. Meanwhile, MMA is also\nequipped with a routing algorithm to help LLMs achieve an automatic shift\nbetween single- and multi-modal instructions without compromising their ability\nof natural language understanding. To validate MMA, we apply it to a recent LLM\ncalled LLaMA and term this formed large vision-language instructed model as\nLaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two\nsetups, namely multimodal science question answering and multimodal dialogue.\nThe experimental results not only demonstrate the competitive performance and\nthe superior training efficiency of LaVIN than existing multimodal LLMs, but\nalso confirm its great potential as a general-purpose chatbot. More\nimportantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4\ntraining hours with 3.8M trainable parameters, greatly confirming the\neffectiveness of MMA. Our project is released at\nhttps://luogen1996.github.io/lavin.",
        "pdf_link": "https://arxiv.org/pdf/2305.15023v3.pdf"
    },
    {
        "title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems",
        "authors": [
            "Marek Kadl\u010d\u00edk",
            "Michal \u0160tef\u00e1nik",
            "Ond\u0159ej Sotol\u00e1\u0159",
            "Vlastimil Martinek"
        ],
        "published": "2023-05-24T10:58:20Z",
        "summary": "Despite outstanding performance in many tasks, language models are\nnotoriously inclined to make factual errors in tasks requiring arithmetic\ncomputation. We address this deficiency by creating Calc-X, a collection of\ndatasets that demonstrates the appropriate use of a calculator in reasoning\nchains. Calc-X is suitable for teaching language models to offload computations\nto a symbolic system. We survey and unify several existing chain-of-thought\ndatasets into a proposed format, resulting in a standard collection of over\n300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X\ncollection to train open-source calculator-using models we call Calcformers and\nshow that these models approximately double the accuracy of generating correct\nresults compared to vanilla language model baselines. We make all Calc-X\ndatasets, source code and Calcformers models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.15017v2.pdf"
    },
    {
        "title": "Estimating Class Separability of Datasets Using Persistent Homology with Application to LLM Fine-Tuning",
        "authors": [
            "Najah Ghalyan",
            "Kostis Gourgoulias",
            "Yash Satsangi",
            "Sean Moran",
            "Maxime Labonne",
            "Joseph Sabelja"
        ],
        "published": "2023-05-24T10:58:09Z",
        "summary": "This paper proposes a method to estimate the class separability of an\nunlabeled text dataset by inspecting the topological characteristics of\nsentence-transformer embeddings of the text. Experiments conducted involve both\nbinary and multi-class cases, with balanced and imbalanced scenarios. The\nresults demonstrate a clear correlation and a better consistency between the\nproposed method and other separability and classification metrics, such as\nThornton's method and the AUC score of a logistic regression classifier, as\nwell as unsupervised methods. Finally, we empirically show that the proposed\nmethod can be part of a stopping criterion for fine-tuning language-model\nclassifiers. By monitoring the class separability of the embedding space after\neach training iteration, we can detect when the training process stops\nimproving the separability of the embeddings without using additional labels.",
        "pdf_link": "https://arxiv.org/pdf/2305.15016v3.pdf"
    },
    {
        "title": "Unlocking Temporal Question Answering for Large Language Models Using Code Execution",
        "authors": [
            "Xingxuan Li",
            "Liying Cheng",
            "Qingyu Tan",
            "Hwee Tou Ng",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023-05-24T10:57:53Z",
        "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing (NLP), and are utilized extensively in various\napplications. Recent works, such as chain-of-thought (CoT), have shown that\nintermediate reasoning steps can improve the performance of LLMs for complex\nreasoning tasks, such as math problems and symbolic question-answering tasks.\nHowever, we notice the challenge that LLMs face when it comes to temporal\nreasoning. Our preliminary experiments show that generating intermediate\nreasoning steps does not always boost the performance of complex temporal\nquestion-answering tasks. Therefore, we propose a novel framework that combines\nthe extraction capability of LLMs and the logical reasoning capability of a\nPython solver to tackle this issue. Extensive experiments and analysis\ndemonstrate the effectiveness of our framework in handling intricate time-bound\nreasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.15014v1.pdf"
    },
    {
        "title": "Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization",
        "authors": [
            "Aman Priyanshu",
            "Supriti Vijay",
            "Ayush Kumar",
            "Rakshit Naidu",
            "Fatemehsadat Mireshghallah"
        ],
        "published": "2023-05-24T10:48:05Z",
        "summary": "LLM-powered chatbots are becoming widely adopted in applications such as\nhealthcare, personal assistants, industry hiring decisions, etc. In many of\nthese cases, chatbots are fed sensitive, personal information in their prompts,\nas samples for in-context learning, retrieved records from a database, or as\npart of the conversation. The information provided in the prompt could directly\nappear in the output, which might have privacy ramifications if there is\nsensitive information there. As such, in this paper, we aim to understand the\ninput copying and regurgitation capabilities of these models during inference\nand how they can be directly instructed to limit this copying by complying with\nregulations such as HIPAA and GDPR, based on their internal knowledge of them.\nMore specifically, we find that when ChatGPT is prompted to summarize cover\nletters of a 100 candidates, it would retain personally identifiable\ninformation (PII) verbatim in 57.4% of cases, and we find this retention to be\nnon-uniform between different subgroups of people, based on attributes such as\ngender identity. We then probe ChatGPT's perception of privacy-related policies\nand privatization mechanisms by directly instructing it to provide compliant\noutputs and observe a significant omission of PII from output.",
        "pdf_link": "https://arxiv.org/pdf/2305.15008v1.pdf"
    },
    {
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
        "authors": [
            "Wenxuan Zhang",
            "Yue Deng",
            "Bing Liu",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-05-24T10:45:25Z",
        "summary": "Sentiment analysis (SA) has been a long-standing research area in natural\nlanguage processing. It can offer rich insights into human sentiments and\nopinions and has thus seen considerable interest from both academia and\nindustry. With the advent of large language models (LLMs) such as ChatGPT,\nthere is a great potential for their employment on SA problems. However, the\nextent to which existing LLMs can be leveraged for different sentiment analysis\ntasks remains unclear. This paper aims to provide a comprehensive investigation\ninto the capabilities of LLMs in performing various sentiment analysis tasks,\nfrom conventional sentiment classification to aspect-based sentiment analysis\nand multifaceted analysis of subjective texts. We evaluate performance across\n13 tasks on 26 datasets and compare the results against small language models\n(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs\ndemonstrate satisfactory performance in simpler tasks, they lag behind in more\ncomplex tasks requiring deeper understanding or structured sentiment\ninformation. However, LLMs significantly outperform SLMs in few-shot learning\nsettings, suggesting their potential when annotation resources are limited. We\nalso highlight the limitations of current evaluation practices in assessing\nLLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a\nmore comprehensive and realistic evaluation. Data and code during our\ninvestigations are available at\n\\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.",
        "pdf_link": "https://arxiv.org/pdf/2305.15005v1.pdf"
    },
    {
        "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
        "authors": [
            "Kangxi Wu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-24T10:45:16Z",
        "summary": "Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.",
        "pdf_link": "https://arxiv.org/pdf/2305.15004v3.pdf"
    },
    {
        "title": "A RelEntLess Benchmark for Modelling Graded Relations between Named Entities",
        "authors": [
            "Asahi Ushio",
            "Jose Camacho Collados",
            "Steven Schockaert"
        ],
        "published": "2023-05-24T10:41:24Z",
        "summary": "Relations such as \"is influenced by\", \"is known for\" or \"is a competitor of\"\nare inherently graded: we can rank entity pairs based on how well they satisfy\nthese relations, but it is hard to draw a line between those pairs that satisfy\nthem and those that do not. Such graded relations play a central role in many\napplications, yet they are typically not covered by existing Knowledge Graphs.\nIn this paper, we consider the possibility of using Large Language Models\n(LLMs) to fill this gap. To this end, we introduce a new benchmark, in which\nentity pairs have to be ranked according to how much they satisfy a given\ngraded relation. The task is formulated as a few-shot ranking problem, where\nmodels only have access to a description of the relation and five prototypical\ninstances. We use the proposed benchmark to evaluate state-of-the-art relation\nembedding strategies as well as several recent LLMs, covering both publicly\navailable LLMs and closed models such as GPT-4. Overall, we find a strong\ncorrelation between model size and performance, with smaller Language Models\nstruggling to outperform a naive baseline. The results of the largest Flan-T5\nand OPT models are remarkably strong, although a clear gap with human\nperformance remains.",
        "pdf_link": "https://arxiv.org/pdf/2305.15002v2.pdf"
    },
    {
        "title": "Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs",
        "authors": [
            "Chongjian Yue",
            "Xinrun Xu",
            "Xiaojun Ma",
            "Lun Du",
            "Hengyu Liu",
            "Zhiming Ding",
            "Yanbing Jiang",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-05-24T10:35:58Z",
        "summary": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2305.16344v2.pdf"
    },
    {
        "title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",
        "authors": [
            "Dongjie Yang",
            "Ruifeng Yuan",
            "Yuantao Fan",
            "Yifei Yang",
            "Zili Wang",
            "Shusen Wang",
            "Hai Zhao"
        ],
        "published": "2023-05-24T10:30:42Z",
        "summary": "Large Language Models (LLMs) have attained the impressive capability to\nresolve a wide range of NLP tasks by fine-tuning high-quality instruction data.\nHowever, collecting human-written data of high quality, especially multi-turn\ndialogues, is expensive and unattainable for most people. Though previous\nstudies have used powerful LLMs to generate the dialogues automatically, they\nall suffer from generating untruthful dialogues because of the model\nhallucination. Therefore, we propose a method called RefGPT to generate\nenormous truthful and customized dialogues without worrying about factual\nerrors caused by the model hallucination. RefGPT solves the model hallucination\nin dialogue generation by restricting the LLMs to leverage the given reference\ninstead of reciting their own knowledge to generate dialogues. Additionally,\nRefGPT adds detailed controls on every utterance to enable high customization\ncapability, which previous studies have ignored. On the basis of RefGPT, we\nalso propose two high-quality dialogue datasets generated by GPT-4, namely\nRefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn\ndialogues based on factual knowledge and RefGPT-Code has 76k multi-turn\ndialogues covering a wide range of coding scenarios. Our code and datasets are\nreleased in https://github.com/mutonix/RefGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.14994v3.pdf"
    },
    {
        "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
        "authors": [
            "Sweta Agrawal",
            "Marine Carpuat"
        ],
        "published": "2023-05-24T10:29:45Z",
        "summary": "Text simplification (TS) systems rewrite text to make it more readable while\npreserving its content. However, what makes a text easy to read depends on the\nintended readers. Recent work has shown that pre-trained language models can\nsimplify text using a wealth of techniques to control output simplicity,\nranging from specifying only the desired reading grade level, to directly\nspecifying low-level edit operations. Yet it remains unclear how to set these\ncontrol parameters in practice. Existing approaches set them at the corpus\nlevel, disregarding the complexity of individual inputs and considering only\none level of output complexity. In this work, we conduct an empirical study to\nunderstand how different control mechanisms impact the adequacy and simplicity\nof text simplification systems. Based on these insights, we introduce a simple\nmethod that predicts the edit operations required for simplifying a text for a\nspecific grade level on an instance-per-instance basis. This approach improves\nthe quality of the simplified outputs over corpus-level search-based\nheuristics.",
        "pdf_link": "https://arxiv.org/pdf/2305.14993v2.pdf"
    },
    {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Jiahua Hong",
            "Zhen Wang",
            "Daisy Zhe Wang",
            "Zhiting Hu"
        ],
        "published": "2023-05-24T10:28:28Z",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting.",
        "pdf_link": "https://arxiv.org/pdf/2305.14992v2.pdf"
    },
    {
        "title": "Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios",
        "authors": [
            "Yilun Zhao",
            "Haowei Zhang",
            "Shengyun Si",
            "Linyong Nan",
            "Xiangru Tang",
            "Arman Cohan"
        ],
        "published": "2023-05-24T10:22:30Z",
        "summary": "Tabular data is prevalent across various industries, necessitating\nsignificant time and effort for users to understand and manipulate for their\ninformation-seeking purposes. The advancements in large language models (LLMs)\nhave shown enormous potential to improve user efficiency. However, the adoption\nof LLMs in real-world applications for table information seeking remains\nunderexplored. In this paper, we investigate the table-to-text capabilities of\ndifferent LLMs using four datasets within two real-world information seeking\nscenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets\nfor data insight generation, along with the FeTaQA and our newly-constructed\nF2WTQ datasets for query-based generation. We structure our investigation\naround three research questions, evaluating the performance of LLMs in\ntable-to-text generation, automated evaluation, and feedback generation,\nrespectively. Experimental results indicate that the current high-performing\nLLM, specifically GPT-4, can effectively serve as a table-to-text generator,\nevaluator, and feedback generator, facilitating users' information seeking\npurposes in real-world scenarios. However, a significant performance gap still\nexists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4\nmodels. Our data and code are publicly available at\nhttps://github.com/yale-nlp/LLM-T2T.",
        "pdf_link": "https://arxiv.org/pdf/2305.14987v2.pdf"
    },
    {
        "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
        "authors": [
            "Haoxuan You",
            "Rui Sun",
            "Zhecan Wang",
            "Long Chen",
            "Gengyu Wang",
            "Hammad A. Ayyubi",
            "Kai-Wei Chang",
            "Shih-Fu Chang"
        ],
        "published": "2023-05-24T10:19:57Z",
        "summary": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT",
        "pdf_link": "https://arxiv.org/pdf/2305.14985v1.pdf"
    },
    {
        "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Abdul Waheed",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-05-24T10:12:39Z",
        "summary": "ChatGPT's emergence heralds a transformative phase in NLP, particularly\ndemonstrated through its excellent performance on many English benchmarks.\nHowever, the model's efficacy across diverse linguistic contexts remains\nlargely uncharted territory. This work aims to bridge this knowledge gap, with\na primary focus on assessing ChatGPT's capabilities on Arabic languages and\ndialectal varieties. Our comprehensive study conducts a large-scale automated\nand human evaluation of ChatGPT, encompassing 44 distinct language\nunderstanding and generation tasks on over 60 different datasets. To our\nknowledge, this marks the first extensive performance analysis of ChatGPT's\ndeployment in Arabic NLP. Our findings indicate that, despite its remarkable\nperformance in English, ChatGPT is consistently surpassed by smaller models\nthat have undergone finetuning on Arabic. We further undertake a meticulous\ncomparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), unveiling the relative shortcomings of both models in handling\nArabic dialects compared to MSA. Although we further explore and confirm the\nutility of employing GPT-4 as a potential alternative for human evaluation, our\nwork adds to a growing body of research underscoring the limitations of\nChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.14976v2.pdf"
    },
    {
        "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Allan Zhou",
            "Archit Sharma",
            "Rafael Rafailov",
            "Huaxiu Yao",
            "Chelsea Finn",
            "Christopher D. Manning"
        ],
        "published": "2023-05-24T10:12:33Z",
        "summary": "A trustworthy real-world prediction system should produce well-calibrated\nconfidence scores; that is, its confidence in an answer should be indicative of\nthe likelihood that the answer is correct, enabling deferral to an expert in\ncases of low-confidence predictions. Recent studies have shown that\nunsupervised pre-training produces large language models (LMs) whose\nconditional probabilities are remarkably well-calibrated. However, the most\nwidely-used LMs are fine-tuned with reinforcement learning from human feedback\n(RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional\nprobabilities that are very poorly calibrated. In light of this perceived\nweakness, we conduct a broad evaluation of methods for extracting confidence\nscores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find\nthat verbalized confidences emitted as output tokens are typically\nbetter-calibrated than the model's conditional probabilities on the TriviaQA,\nSciQ, and TruthfulQA benchmarks, often reducing the expected calibration error\nby a relative 50%.",
        "pdf_link": "https://arxiv.org/pdf/2305.14975v2.pdf"
    },
    {
        "title": "OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning",
        "authors": [
            "Jiazheng Li",
            "Runcong Zhao",
            "Yongxin Yang",
            "Yulan He",
            "Lin Gui"
        ],
        "published": "2023-05-24T10:08:04Z",
        "summary": "The remarkable performance of pre-trained large language models has\nrevolutionised various natural language processing applications. Due to huge\nparametersizes and extensive running costs, companies or organisations tend to\ntransfer the models to the target task by zero-shot prompting techniques.\nHowever, the prohibitive costs of tokens and time have hindered their adoption\nin applications. We propose OverPrompt, leveraging the in-context learning\ncapability of LLMs to handle multiple task inputs, thereby reducing token and\ntime costs. This approach could potentially improve task performance during API\nqueries due to better conditional distribution mapping. Evaluated across\ndiverse classification datasets, our experiments show that OverPrompt can\nachieve cost-efficient zero-shot classification without causing significant\ndetriment to task performance, and in some cases, even improving it. An\nablation study conducted on various LLMs, along with an investigation into the\nrobustness of our prompting strategy to different input ordering, offers\nvaluable insights into the broader applicability of our method across diverse\ntasks. These findings also suggest a more seamless integration of our method\nwith LLMs through an API.",
        "pdf_link": "https://arxiv.org/pdf/2305.14973v2.pdf"
    },
    {
        "title": "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
        "authors": [
            "Abhinav Rao",
            "Sachin Vashistha",
            "Atharva Naik",
            "Somak Aditya",
            "Monojit Choudhury"
        ],
        "published": "2023-05-24T09:57:37Z",
        "summary": "Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14965v4.pdf"
    },
    {
        "title": "Editing Common Sense in Transformers",
        "authors": [
            "Anshita Gupta",
            "Debanjan Mondal",
            "Akshay Krishna Sheshadri",
            "Wenlong Zhao",
            "Xiang Lorraine Li",
            "Sarah Wiegreffe",
            "Niket Tandon"
        ],
        "published": "2023-05-24T09:50:54Z",
        "summary": "Editing model parameters directly in Transformers makes updating open-source\ntransformer-based models possible without re-training (Meng et al., 2023).\nHowever, these editing methods have only been evaluated on statements about\nencyclopedic knowledge with a single correct answer. Commonsense knowledge with\nmultiple correct answers, e.g., an apple can be green or red but not\ntransparent, has not been studied but is as essential for enhancing\ntransformers' reliability and usefulness. In this paper, we investigate whether\ncommonsense judgments are causally associated with localized, editable\nparameters in Transformers, and we provide an affirmative answer. We find that\ndirectly applying the MEMIT editing algorithm results in sub-par performance\nand improve it for the commonsense domain by varying edit tokens and improving\nthe layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models\nedited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and\n10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel\nevaluation dataset, PROBE SET, that contains unaffected and affected\nneighborhoods, affected paraphrases, and affected reasoning challenges.\n$MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines show\nsignificant trade-offs between unaffected and affected metrics. These results\nsuggest a compelling future direction for incorporating feedback about common\nsense into Transformers through direct model editing.",
        "pdf_link": "https://arxiv.org/pdf/2305.14956v3.pdf"
    },
    {
        "title": "Adversarial Demonstration Attacks on Large Language Models",
        "authors": [
            "Jiongxiao Wang",
            "Zichen Liu",
            "Keun Hee Park",
            "Zhuojun Jiang",
            "Zhaoheng Zheng",
            "Zhuofeng Wu",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2023-05-24T09:40:56Z",
        "summary": "With the emergence of more powerful large language models (LLMs), such as\nChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence\nin leveraging these models for specific tasks by utilizing data-label pairs as\nprecondition prompts. While incorporating demonstrations can greatly enhance\nthe performance of LLMs across various tasks, it may introduce a new security\nconcern: attackers can manipulate only the demonstrations without changing the\ninput to perform an attack. In this paper, we investigate the security concern\nof ICL from an adversarial perspective, focusing on the impact of\ndemonstrations. We propose a novel attack method named advICL, which aims to\nmanipulate only the demonstration without changing the input to mislead the\nmodels. Our results demonstrate that as the number of demonstrations increases,\nthe robustness of in-context learning would decrease. Additionally, we also\nidentify the intrinsic property of the demonstrations is that they can be used\n(prepended) with different inputs. As a result, it introduces a more practical\nthreat model in which an attacker can attack the test input example even\nwithout knowing and manipulating it. To achieve it, we propose the transferable\nversion of advICL, named Transferable-advICL. Our experiment shows that the\nadversarial demonstration generated by Transferable-advICL can successfully\nattack the unseen test input examples. We hope that our study reveals the\ncritical security risks associated with ICL and underscores the need for\nextensive research on the robustness of ICL, particularly given its increasing\nsignificance in the advancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14950v2.pdf"
    },
    {
        "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench",
        "authors": [
            "Qinyuan Ye",
            "Harvey Yiyun Fu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023-05-24T09:35:34Z",
        "summary": "We investigate the predictability of large language model (LLM) capabilities:\ngiven records of past experiments using different model families, numbers of\nparameters, tasks, and numbers of in-context examples, can we accurately\npredict LLM performance on new experiment configurations? Answering this\nquestion has practical implications for LLM users (e.g., deciding which models\nto try), developers (e.g., prioritizing evaluation on representative tasks),\nand the research community (e.g., identifying hard-to-predict capabilities that\nwarrant further investigation).\n  We study the performance prediction problem on experiment records from\nBIG-bench. On a random train-test split, an MLP-based predictor achieves an\n$R^2$ score greater than 95%, indicating the presence of learnable patterns\nwithin the experiment records. We then formulate the problem of searching for\n\"small-bench,\" an informative subset of BIG-bench tasks from which the\nperformance on the full set can be maximally recovered. We find a subset as\ninformative as BIG-bench Hard for evaluating new model families, while being\n$3\\times$ smaller. Additionally, we find competitive subsets by clustering task\nrepresentations learned by our MLP-based predictor and selecting tasks close to\ncluster centroids, highlighting the importance of task diversity in\nconstructing \"small-bench.\"",
        "pdf_link": "https://arxiv.org/pdf/2305.14947v2.pdf"
    },
    {
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "authors": [
            "Minje Choi",
            "Jiaxin Pei",
            "Sagar Kumar",
            "Chang Shu",
            "David Jurgens"
        ],
        "published": "2023-05-24T09:21:06Z",
        "summary": "Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor & sarcasm, offensiveness, sentiment & emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.",
        "pdf_link": "https://arxiv.org/pdf/2305.14938v2.pdf"
    },
    {
        "title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023-05-24T09:16:51Z",
        "summary": "In the context of multi-step reasoning, e.g., with chain-of-thought, language\nmodels (LMs) can easily assign a high likelihood to incorrect steps. As a\nresult, decoding strategies that optimize for solution likelihood often yield\nincorrect solutions. To address this issue, we propose Guiding chain-of-thought\nReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding\napproach that steers the decoding process towards producing correct reasoning\nsteps. GRACE employs a discriminator trained with a contrastive loss over\ncorrect and incorrect steps, which is used during decoding to score next-step\ncandidates based on their correctness. Importantly, GRACE only requires\nsampling from the LM, without the need for LM training or fine-tuning. Using\nmodels from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and\ntwo symbolic reasoning tasks, where it exhibits substantial performance gains\ncompared to greedy decoding, verifiers, and self-consistency in most settings.\nWhen further combined with self-consistency, GRACE outperforms all the\nbaselines by sizeable margins. Human and LLM evaluations over GSM8K show that\nGRACE not only improves the final answer accuracy but also the correctness of\nthe intermediate reasoning. Our implementation can be accessed at\n\\url{https://github.com/mukhal/grace}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14934v2.pdf"
    },
    {
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
        "authors": [
            "Leonard Salewski",
            "Stephan Alaniz",
            "Isabel Rio-Torto",
            "Eric Schulz",
            "Zeynep Akata"
        ],
        "published": "2023-05-24T09:13:15Z",
        "summary": "In everyday conversations, humans can take on different roles and adapt their\nvocabulary to their chosen roles. We explore whether LLMs can take on, that is\nimpersonate, different roles when they generate text in-context. We ask LLMs to\nassume different personas before solving vision and language tasks. We do this\nby prefixing the prompt with a persona that is associated either with a social\nidentity or domain expertise. In a multi-armed bandit task, we find that LLMs\npretending to be children of different ages recover human-like developmental\nstages of exploration. In a language-based reasoning task, we find that LLMs\nimpersonating domain experts perform better than LLMs impersonating non-domain\nexperts. Finally, we test whether LLMs' impersonations are complementary to\nvisual information when describing different categories. We find that\nimpersonation can improve performance: an LLM prompted to be a bird expert\ndescribes birds better than one prompted to be a car expert. However,\nimpersonation can also uncover LLMs' biases: an LLM prompted to be a man\ndescribes cars better than one prompted to be a woman. These findings\ndemonstrate that LLMs are capable of taking on diverse roles and that this\nin-context impersonation can be used to uncover their hidden strengths and\nbiases.",
        "pdf_link": "https://arxiv.org/pdf/2305.14930v2.pdf"
    },
    {
        "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
        "authors": [
            "Kellin Pelrine",
            "Anne Imouza",
            "Camille Thibault",
            "Meilina Reksoprodjo",
            "Caleb Gupta",
            "Joel Christoph",
            "Jean-Fran\u00e7ois Godbout",
            "Reihaneh Rabbany"
        ],
        "published": "2023-05-24T09:10:20Z",
        "summary": "Misinformation poses a critical societal challenge, and current approaches\nhave yet to produce an effective solution. We propose focusing on\ngeneralization, uncertainty, and how to leverage recent large language models,\nin order to create more practical tools to evaluate information veracity in\ncontexts where perfect classification is impossible. We first demonstrate that\nGPT-4 can outperform prior methods in multiple settings and languages. Next, we\nexplore generalization, revealing that GPT-4 and RoBERTa-large exhibit\ndifferences in failure modes. Third, we propose techniques to handle\nuncertainty that can detect impossible examples and strongly improve outcomes.\nWe also discuss results on other language models, temperature, prompting,\nversioning, explainability, and web retrieval, each one providing practical\ninsights and directions for future research. Finally, we publish the LIAR-New\ndataset with novel paired English and French misinformation data and\nPossibility labels that indicate if there is sufficient context for veracity\nevaluation. Overall, this research lays the groundwork for future tools that\ncan drive real-world progress to combat misinformation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14928v3.pdf"
    },
    {
        "title": "Universal Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Hanjun Dai",
            "Julian Martin Eisenschlos",
            "Sercan O. Arik",
            "Tomas Pfister"
        ],
        "published": "2023-05-24T09:09:48Z",
        "summary": "A hallmark of modern large language models (LLMs) is their impressive general\nzero-shot and few-shot abilities, often elicited through in-context learning\n(ICL) via prompting. However, while highly coveted and being the most general,\nzero-shot performances in LLMs are still typically weaker due to the lack of\nguidance and the difficulty of applying existing automatic prompt design\nmethods in general tasks when ground-truth labels are unavailable. In this\nstudy, we address this by presenting Universal Self-Adaptive Prompting (USP),\nan automatic prompt design approach specifically tailored for zero-shot\nlearning (while compatible with few-shot). Requiring only a small amount of\nunlabeled data and an inference-only LLM, USP is highly versatile: to achieve\nuniversal prompting, USP categorizes a possible NLP task into one of the three\npossible task types and then uses a corresponding selector to select the most\nsuitable queries and zero-shot model-generated responses as\npseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a\nfully automated way. We evaluate USP with PaLM and PaLM 2 models and\ndemonstrate performances that are considerably stronger than standard zero-shot\nbaselines and often comparable to or even superior to few-shot baselines across\nmore than 40 natural language understanding, natural language generation, and\nreasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14926v2.pdf"
    },
    {
        "title": "Frugal Prompting for Dialog Models",
        "authors": [
            "Bishal Santra",
            "Sakya Basak",
            "Abhinandan De",
            "Manish Gupta",
            "Pawan Goyal"
        ],
        "published": "2023-05-24T09:06:49Z",
        "summary": "The use of large language models (LLMs) in natural language processing (NLP)\ntasks is rapidly increasing, leading to changes in how researchers approach\nproblems in the field. To fully utilize these models' abilities, a better\nunderstanding of their behavior for different input protocols is required. With\nLLMs, users can directly interact with the models through a text-based\ninterface to define and solve various tasks. Hence, understanding the\nconversational abilities of these LLMs, which may not have been specifically\ntrained for dialog modeling, is also important. This study examines different\napproaches for building dialog systems using LLMs by considering various\naspects of the prompt. As part of prompt tuning, we experiment with various\nways of providing instructions, exemplars, current query and additional\ncontext. The research also analyzes the representations of dialog history that\nhave the optimal usable-information density. Based on the findings, the paper\nsuggests more compact ways of providing dialog history information while\nensuring good performance and reducing model's inference-API costs. The\nresearch contributes to a better understanding of how LLMs can be effectively\nused for building interactive systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.14919v2.pdf"
    },
    {
        "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
        "authors": [
            "Lin Guan",
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-24T08:59:15Z",
        "summary": "There is a growing interest in applying pre-trained large language models\n(LLMs) to planning problems. However, methods that use LLMs directly as\nplanners are currently impractical due to several factors, including limited\ncorrectness of plans, strong reliance on feedback from interactions with\nsimulators or even the actual environment, and the inefficiency in utilizing\nhuman feedback. In this work, we introduce a novel alternative paradigm that\nconstructs an explicit world (domain) model in planning domain definition\nlanguage (PDDL) and then uses it to plan with sound domain-independent\nplanners. To address the fact that LLMs may not generate a fully functional\nPDDL model initially, we employ LLMs as an interface between PDDL and sources\nof corrective feedback, such as PDDL validators and humans. For users who lack\na background in PDDL, we show that LLMs can translate PDDL into natural\nlanguage and effectively encode corrective feedback back to the underlying\ndomain model. Our framework not only enjoys the correctness guarantee offered\nby the external planners but also reduces human involvement by allowing users\nto correct domain models at the beginning, rather than inspecting and\ncorrecting (through interactive prompting) every generated plan as in previous\nwork. On two IPC domains and a Household domain that is more complicated than\ncommonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be\nleveraged to produce high-quality PDDL models for over 40 actions, and the\ncorrected PDDL models are then used to successfully solve 48 challenging\nplanning tasks. Resources, including the source code, are released at:\nhttps://guansuns.github.io/pages/llm-dm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14909v2.pdf"
    },
    {
        "title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions",
        "authors": [
            "Anthony Chen",
            "Panupong Pasupat",
            "Sameer Singh",
            "Hongrae Lee",
            "Kelvin Guu"
        ],
        "published": "2023-05-24T08:59:00Z",
        "summary": "The remarkable capabilities of large language models have been accompanied by\na persistent drawback: the generation of false and unsubstantiated claims\ncommonly known as \"hallucinations\". To combat this issue, recent research has\nintroduced approaches that involve editing and attributing the outputs of\nlanguage models, particularly through prompt-based editing. However, the\ninference cost and speed of using large language models for editing currently\nbottleneck prompt-based methods. These bottlenecks motivate the training of\ncompact editors, which is challenging due to the scarcity of training data for\nthis purpose. To overcome these challenges, we exploit the power of large\nlanguage models to introduce corruptions (i.e., noise) into text and\nsubsequently fine-tune compact editors to denoise the corruptions by\nincorporating relevant evidence. Our methodology is entirely unsupervised and\nprovides us with faux hallucinations for training in any domain. Our Petite\nUnsupervised Research and Revision model, PURR, not only improves attribution\nover existing editing methods based on fine-tuning and prompting, but also\nachieves faster execution times by orders of magnitude.",
        "pdf_link": "https://arxiv.org/pdf/2305.14908v1.pdf"
    },
    {
        "title": "Coverage-based Example Selection for In-Context Learning",
        "authors": [
            "Shivanshu Gupta",
            "Matt Gardner",
            "Sameer Singh"
        ],
        "published": "2023-05-24T08:58:28Z",
        "summary": "In-context learning (ICL), the ability of large language models to perform\nnovel tasks by conditioning on a prompt with a few task examples, requires\nthese examples to be informative about the test instance. The standard approach\nof independently ranking and selecting the most similar examples selects\nredundant examples while omitting important information. In this work, we show\nthat BERTScore-Recall (BSR) selects better examples that demonstrate more of\nthe salient aspects, e.g. reasoning patterns, of the test input. We further\nextend BSR and many standard metrics to easily optimizable set-level metrics,\ngiving still better coverage of those salient aspects. On 15 datasets spanning\n6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric\nfor in-context example selection across the board, and (2) for compositional\ntasks, set selection using Set-BSR outperforms independent ranking by up to 17\npoints on average and, despite being training-free, surpasses methods that\nleverage task or LLM-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2305.14907v3.pdf"
    },
    {
        "title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",
        "authors": [
            "Yuxia Wang",
            "Jonibek Mansurov",
            "Petar Ivanov",
            "Jinyan Su",
            "Artem Shelmanov",
            "Akim Tsvigun",
            "Chenxi Whitehouse",
            "Osama Mohammed Afzal",
            "Tarek Mahmoud",
            "Toru Sasaki",
            "Thomas Arnold",
            "Alham Fikri Aji",
            "Nizar Habash",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2023-05-24T08:55:11Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capability to\ngenerate fluent responses to a wide variety of user queries. However, this has\nalso raised concerns about the potential misuse of such texts in journalism,\neducation, and academia. In this study, we strive to create automated systems\nthat can detect machine-generated texts and pinpoint potential misuse. We first\nintroduce a large-scale benchmark \\textbf{M4}, which is a multi-generator,\nmulti-domain, and multi-lingual corpus for machine-generated text detection.\nThrough an extensive empirical study of this dataset, we show that it is\nchallenging for detectors to generalize well on instances from unseen domains\nor LLMs. In such cases, detectors tend to misclassify machine-generated text as\nhuman-written. These results show that the problem is far from solved and that\nthere is a lot of room for improvement. We believe that our dataset will enable\nfuture research towards more robust approaches to this pressing societal\nproblem. The dataset is available at https://github.com/mbzuai-nlp/M4.",
        "pdf_link": "https://arxiv.org/pdf/2305.14902v2.pdf"
    },
    {
        "title": "PIVOINE: Instruction Tuning for Open-world Information Extraction",
        "authors": [
            "Keming Lu",
            "Xiaoman Pan",
            "Kaiqiang Song",
            "Hongming Zhang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2023-05-24T08:52:08Z",
        "summary": "We consider the problem of Open-world Information Extraction (Open-world IE),\nwhich extracts comprehensive entity profiles from unstructured texts. Different\nfrom the conventional closed-world setting of Information Extraction (IE),\nOpen-world IE considers a more general situation where entities and relations\ncould be beyond a predefined ontology. More importantly, we seek to develop a\nlarge language model (LLM) that is able to perform Open-world IE to extract\ndesirable entity profiles characterized by (possibly fine-grained) natural\nlanguage instructions. We achieve this by finetuning LLMs using instruction\ntuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction\ntuning dataset for Open-world IE enriched with a comprehensive corpus,\nextensive annotations, and diverse instructions. We finetune the pretrained\nBLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE\nwith strong instruction-following capabilities. Our experiments demonstrate\nthat PIVOINE significantly outperforms traditional closed-world methods and\nother LLM baselines, displaying impressive generalization capabilities on both\nunseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as\na promising solution to tackle the open-world challenge in IE effectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.14898v1.pdf"
    },
    {
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "authors": [
            "Ziang Xiao",
            "Susu Zhang",
            "Vivian Lai",
            "Q. Vera Liao"
        ],
        "published": "2023-05-24T08:38:23Z",
        "summary": "We address a fundamental challenge in Natural Language Generation (NLG) model\nevaluation -- the design and evaluation of evaluation metrics. Recognizing the\nlimitations of existing automatic metrics and noises from how current human\nevaluation was conducted, we propose MetricEval, a framework informed by\nmeasurement theory, the foundation of educational test design, for\nconceptualizing and evaluating the reliability and validity of NLG evaluation\nmetrics. The framework formalizes the source of measurement error and offers\nstatistical tools for evaluating evaluation metrics based on empirical data.\nWith our framework, one can quantify the uncertainty of the metrics to better\ninterpret the result. To exemplify the use of our framework in practice, we\nanalyzed a set of evaluation metrics for summarization and identified issues\nrelated to conflated validity structure in human-eval and reliability in\nLLM-based metrics. Through MetricEval, we aim to promote the design,\nevaluation, and interpretation of valid and reliable metrics to advance robust\nand effective NLG models.",
        "pdf_link": "https://arxiv.org/pdf/2305.14889v2.pdf"
    },
    {
        "title": "Leveraging GPT-4 for Automatic Translation Post-Editing",
        "authors": [
            "Vikas Raunak",
            "Amr Sharaf",
            "Yiren Wang",
            "Hany Hassan Awadallah",
            "Arul Menezes"
        ],
        "published": "2023-05-24T08:30:05Z",
        "summary": "While Neural Machine Translation (NMT) represents the leading approach to\nMachine Translation (MT), the outputs of NMT models still require translation\npost-editing to rectify errors and enhance quality under critical settings. In\nthis work, we formalize the task of direct translation post-editing with Large\nLanguage Models (LLMs) and explore the use of GPT-4 to automatically post-edit\nNMT outputs across several language pairs. Our results demonstrate that GPT-4\nis adept at translation post-editing, producing meaningful and trustworthy\nedits to translations that help improve its general quality as well as remove\ndifferent classes of major errors in translations. In particular, human\nevaluations on assessing edit trustworthiness show that GPT-4 exhibits a large\nimprovement over the prior state-of-the-art LLM. Notably, we improve upon\nstate-of-the-art performance on WMT-22 English-Chinese, English-German,\nChinese-English and German-English language pairs using GPT-4 based\npost-editing, as evaluated by state-of-the-art MT quality metrics. However, we\nalso show that GPT-4 could produce hallucinated edits, thereby urging caution\nin its use as an expert translation post-editor.",
        "pdf_link": "https://arxiv.org/pdf/2305.14878v2.pdf"
    },
    {
        "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
        "authors": [
            "Yuwei Zhang",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2023-05-24T08:24:25Z",
        "summary": "We introduce ClusterLLM, a novel text clustering framework that leverages\nfeedback from an instruction-tuned large language model, such as ChatGPT.\nCompared with traditional unsupervised methods that builds upon \"small\"\nembedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the\nemergent capability of LLM even if its embeddings are inaccessible; and (2) it\nunderstands the user's preference on clustering through textual instruction\nand/or a few annotated data. First, we prompt ChatGPT for insights on\nclustering perspective by constructing hard triplet questions <does A better\ncorrespond to B than C>, where A, B and C are similar data points that belong\nto different clusters according to small embedder. We empirically show that\nthis strategy is both effective for fine-tuning small embedder and\ncost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on\nclustering granularity by carefully designed pairwise questions <do A and B\nbelong to the same category>, and tune the granularity from cluster hierarchies\nthat is the most consistent with the ChatGPT answers. Extensive experiments on\n14 datasets show that ClusterLLM consistently improves clustering quality, at\nan average cost of ~$0.6 per dataset. The code will be available at\nhttps://github.com/zhang-yu-wei/ClusterLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.14871v2.pdf"
    },
    {
        "title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Wenxuan Ding",
            "Baixuan Xu",
            "Xin Liu",
            "Yangqiu Song",
            "Antoine Bosselut"
        ],
        "published": "2023-05-24T08:21:31Z",
        "summary": "The task of zero-shot commonsense question answering evaluates models on\ntheir capacity to reason about general scenarios beyond those presented in\nspecific datasets. Existing approaches for tackling this task leverage external\nknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on\nsynthetic QA pairs constructed from CSKBs. In these approaches, negative\nexamples (distractors) are formulated by randomly sampling from CSKBs using\nfairly primitive keyword constraints. However, two bottlenecks limit these\napproaches: the inherent incompleteness of CSKBs limits the semantic coverage\nof synthetic QA pairs, and the lack of human annotations makes the sampled\nnegative examples potentially uninformative and contradictory. To tackle these\nlimitations above, we propose Conceptualization-Augmented Reasoner (CAR), a\nzero-shot commonsense question-answering framework that fully leverages the\npower of conceptualization. Specifically, CAR abstracts a commonsense knowledge\ntriple to many higher-level instances, which increases the coverage of CSKB and\nexpands the ground-truth answer space, reducing the likelihood of selecting\nfalse-negative distractors. Extensive experiments demonstrate that CAR more\nrobustly generalizes to answering questions about zero-shot commonsense\nscenarios than existing methods, including large language models, such as\nGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at\nhttps://github.com/HKUST-KnowComp/CAR.",
        "pdf_link": "https://arxiv.org/pdf/2305.14869v2.pdf"
    },
    {
        "title": "How To Train Your (Compressed) Large Language Model",
        "authors": [
            "Ananya Harsh Jha",
            "Tom Sherborne",
            "Evan Pete Walsh",
            "Dirk Groeneveld",
            "Emma Strubell",
            "Iz Beltagy"
        ],
        "published": "2023-05-24T08:18:35Z",
        "summary": "With the increase in the size of large language models (LLMs), we need\ncompression methods that can reduce the model size while preserving the\ngenerality and zero-shot promptability of the model. This goal is more\nambitious than the typical compression setup, which reduces the model's size at\nthe expense of specializing it to a specific end-task. To study this, we\ndevelop a task-agnostic compression pipeline with a large-scale evaluation\ncomprising language modeling perplexity and 12 zero-shot end-tasks. Our results\nshow that a simple layer-wise pruning followed by continued language model\npretraining matches or outperforms three existing state-of-the-art baselines\nwhile being 1.5x more computationally efficient. However, unlike typical\ntask-specialized compression, our best-compressed model significantly\nunderperforms a similar-sized model trained from scratch. We posit the\nhalf-sized pretrained model as an upper bound for task-agnostic compression and\ncall for future work to bridge this gap under a reasonable token budget. Our\nfindings highlight the inadequacy of existing compression methods for LLMs and\nestablish a requirement for new methods that preserve a model's generality and\nzero-shot promptability under compression. We release our code and evaluation\nsetup to facilitate reproducibility and help iterate on method design.",
        "pdf_link": "https://arxiv.org/pdf/2305.14864v2.pdf"
    },
    {
        "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
        "authors": [
            "Akari Asai",
            "Sneha Kudugunta",
            "Xinyan Velocity Yu",
            "Terra Blevins",
            "Hila Gonen",
            "Machel Reid",
            "Yulia Tsvetkov",
            "Sebastian Ruder",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-24T08:06:33Z",
        "summary": "Despite remarkable advancements in few-shot generalization in natural\nlanguage processing, most models are developed and evaluated primarily in\nEnglish. To facilitate research on few-shot cross-lingual transfer, we\nintroduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across\n54 languages in a sequence-to-sequence format and provides a fixed set of\nfew-shot examples and instructions. BUFFET is designed to establish a rigorous\nand equitable evaluation framework for few-shot cross-lingual transfer across a\nbroad range of tasks and languages. Using BUFFET, we perform thorough\nevaluations of state-of-the-art multilingual large language models with\ndifferent transfer methods, namely in-context learning and fine-tuning. Our\nfindings reveal significant room for improvement in few-shot in-context\ncross-lingual transfer. In particular, ChatGPT with in-context learning often\nperforms worse than much smaller mT5-base models fine-tuned on English task\ndata and few-shot in-language examples. Our analysis suggests various avenues\nfor future research in few-shot cross-lingual transfer, such as improved\npretraining, understanding, and future evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14857v1.pdf"
    },
    {
        "title": "SummIt: Iterative Text Summarization via ChatGPT",
        "authors": [
            "Haopeng Zhang",
            "Xiao Liu",
            "Jiawei Zhang"
        ],
        "published": "2023-05-24T07:40:06Z",
        "summary": "Text summarization systems have made significant progress in recent years,\nbut typically generate summaries in one single step. However, the one-shot\nsummarization setting is sometimes inadequate, as the generated summary may\ncontain hallucinations or overlook essential details related to the reader's\ninterests. This paper addresses this limitation by proposing SummIt, an\niterative text summarization framework based on large language models like\nChatGPT. Our framework enables the model to refine the generated summary\niteratively through self-evaluation and feedback, resembling humans' iterative\nprocess when drafting and revising summaries. Furthermore, we explore the\npotential benefits of integrating knowledge and topic extractors into the\nframework to enhance summary faithfulness and controllability. We automatically\nevaluate the performance of our framework on three benchmark summarization\ndatasets. We also conduct a human evaluation to validate the effectiveness of\nthe iterative refinements and identify a potential issue of over-correction.",
        "pdf_link": "https://arxiv.org/pdf/2305.14835v2.pdf"
    },
    {
        "title": "PromptNER: Prompting For Named Entity Recognition",
        "authors": [
            "Dhananjay Ashok",
            "Zachary C. Lipton"
        ],
        "published": "2023-05-24T07:38:24Z",
        "summary": "In a surprising turn, Large Language Models (LLMs) together with a growing\narsenal of prompt-based heuristics now offer powerful off-the-shelf approaches\nproviding few-shot solutions to myriad classic NLP problems. However, despite\npromising early results, these LLM-based few-shot methods remain far from the\nstate of the art in Named Entity Recognition (NER), where prevailing methods\ninclude learning representations via end-to-end structural understanding and\nfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,\na new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to\nany new NER task PromptNER requires a set of entity definitions in addition to\nthe standard few-shot examples. Given a sentence, PromptNER prompts an LLM to\nproduce a list of potential entities along with corresponding explanations\njustifying their compatibility with the provided entity type definitions.\nRemarkably, PromptNER achieves state-of-the-art performance on few-shot NER,\nachieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%\n(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on\nthe FewNERD dataset. PromptNER also moves the state of the art on Cross Domain\nNER, outperforming prior methods (including those not limited to the few-shot\nsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1\ngain of 3%, despite using less than 2% of the available data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15444v2.pdf"
    },
    {
        "title": "Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation",
        "authors": [
            "Prashant Krishnan",
            "Zilong Wang",
            "Yangkun Wang",
            "Jingbo Shang"
        ],
        "published": "2023-05-24T07:34:33Z",
        "summary": "Recent advances of incorporating layout information, typically bounding box\ncoordinates, into pre-trained language models have achieved significant\nperformance in entity recognition from document images. Using coordinates can\neasily model the absolute position of each token, but they might be sensitive\nto manipulations in document images (e.g., shifting, rotation or scaling),\nespecially when the training data is limited in few-shot settings. In this\npaper, we propose to further introduce the topological adjacency relationship\namong the tokens, emphasizing their relative position information.\nSpecifically, we consider the tokens in the documents as nodes and formulate\nthe edges based on the topological heuristics from the k-nearest bounding\nboxes. Such adjacency graphs are invariant to affine transformations including\nshifting, rotations and scaling. We incorporate these graphs into the\npre-trained language model by adding graph neural network layers on top of the\nlanguage model embeddings, leading to a novel model LAGER. Extensive\nexperiments on two benchmark datasets show that LAGER significantly outperforms\nstrong baselines under different few-shot settings and also demonstrate better\nrobustness to manipulations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14828v2.pdf"
    },
    {
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
        "authors": [
            "Xiaojuan Tang",
            "Zilong Zheng",
            "Jiaqi Li",
            "Fanxu Meng",
            "Song-Chun Zhu",
            "Yitao Liang",
            "Muhan Zhang"
        ],
        "published": "2023-05-24T07:33:34Z",
        "summary": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14825v2.pdf"
    },
    {
        "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
        "authors": [
            "Michael J. Q. Zhang",
            "Eunsol Choi"
        ],
        "published": "2023-05-24T07:30:08Z",
        "summary": "While large language models are able to retain vast amounts of world\nknowledge seen during pretraining, such knowledge is prone to going out of date\nand is nontrivial to update. Furthermore, these models are often used under\ntemporal misalignment, tasked with answering questions about the present,\ndespite having only been trained on data collected in the past. To mitigate the\neffects of temporal misalignment, we propose fact duration prediction: the task\nof predicting how long a given fact will remain true. In our experiments, we\ndemonstrate that identifying which facts are prone to rapid change can help\nmodels avoid reciting outdated information and determine which predictions\nrequire seeking out up-to-date knowledge sources. We also show how modeling\nfact duration improves calibration for knowledge-intensive tasks, such as\nopen-retrieval question answering, under temporal misalignment, by discarding\nvolatile facts. Our data and code are released publicly at\nhttps://github.com/mikejqzhang/mitigating_misalignment.",
        "pdf_link": "https://arxiv.org/pdf/2305.14824v3.pdf"
    },
    {
        "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
        "authors": [
            "Harvey Yiyun Fu",
            "Qinyuan Ye",
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023-05-24T06:55:09Z",
        "summary": "Large Language Models (LLMs) have the impressive ability to perform\nin-context learning (ICL) from only a few examples, but the success of ICL\nvaries widely from task to task. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but directly evaluating ICL accuracy\ncan be expensive in situations where test data is expensive to annotate -- the\nexact situations where ICL is most appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we predict the accuracy of an LLM\nwhen doing in-context learning on a new task given only unlabeled test data for\nthat task. To perform ICL accuracy estimation, we propose a method that trains\na meta-model using LLM confidence scores as features. We compare our method to\nseveral strong accuracy estimation baselines on a new benchmark that covers 4\nLLMs and 3 task collections. The meta-model improves over all baselines across\n8 out of 12 settings and achieves the same estimation performance as directly\nevaluating on 40 collected labeled test examples per task. At the same time, no\nexisting approach provides an accurate and reliable ICL accuracy estimation in\nevery setting, highlighting the need for better ways to measure the uncertainty\nof LLM predictions.",
        "pdf_link": "https://arxiv.org/pdf/2305.14802v2.pdf"
    },
    {
        "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
        "authors": [
            "Zexuan Zhong",
            "Zhengxuan Wu",
            "Christopher D. Manning",
            "Christopher Potts",
            "Danqi Chen"
        ],
        "published": "2023-05-24T06:48:41Z",
        "summary": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2305.14795v2.pdf"
    },
    {
        "title": "Prompting Large Language Models for Counterfactual Generation: An Empirical Study",
        "authors": [
            "Yongqi Li",
            "Mayi Xu",
            "Xin Miao",
            "Shen Zhou",
            "Tieyun Qian"
        ],
        "published": "2023-05-24T06:44:32Z",
        "summary": "Large language models (LLMs) have made remarkable progress in a wide range of\nnatural language understanding and generation tasks. However, their ability to\ngenerate counterfactuals has not been examined systematically. To bridge this\ngap, we present a comprehensive evaluation framework on various types of NLU\ntasks, which covers all key factors in determining LLMs' capability of\ngenerating counterfactuals. Based on this framework, we 1) investigate the\nstrengths and weaknesses of LLMs as the counterfactual generator, and 2)\ndisclose the factors that affect LLMs when generating counterfactuals,\nincluding both the intrinsic properties of LLMs and prompt designing. The\nresults show that, though LLMs are promising in most cases, they face\nchallenges in complex tasks like RE since they are bounded by task-specific\nperformance, entity constraints, and inherent selection bias. We also find that\nalignment techniques, e.g., instruction-tuning and reinforcement learning from\nhuman feedback, may potentially enhance the counterfactual generation ability\nof LLMs. On the contrary, simply increasing the parameter size does not yield\nthe desired improvements. Besides, from the perspective of prompt designing,\ntask guidelines unsurprisingly play an important role. However, the\nchain-of-thought approach does not always help due to inconsistency issues.",
        "pdf_link": "https://arxiv.org/pdf/2305.14791v2.pdf"
    },
    {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "published": "2023-05-24T06:42:44Z",
        "summary": "Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These language models are capable\nof compressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments, and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show\nthat AutoCompressors can utilize long contexts to improve perplexity. We\nevaluate AutoCompressors on in-context learning by compressing task\ndemonstrations and find that summary vectors are good substitutes for\nplain-text demonstrations, increasing accuracy while reducing inference costs.\nFinally, we explore the benefits of pre-computing summary vectors for large\ncorpora by applying summary vectors to retrievalaugmented language modeling and\na passage re-ranking task. Overall, AutoCompressors emerge as a simple and\ninexpensive solution to extend the context window of LMs while speeding up\ninference over long contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14788v2.pdf"
    },
    {
        "title": "ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds",
        "authors": [
            "Victoria Basmov",
            "Yoav Goldberg",
            "Reut Tsarfaty"
        ],
        "published": "2023-05-24T06:41:09Z",
        "summary": "This paper sheds light on the limitations of ChatGPT's understanding\ncapabilities, focusing on simple inference tasks that are typically easy for\nhumans but appear to be challenging for the model. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We present expert-designed\nevaluation sets for these inference types and conduct experiments in a\nzero-shot setup. Our results show that the model struggles with these types of\ninferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT\ndemonstrates knowledge of the underlying linguistic concepts when prompted\ndirectly, it often fails to incorporate this knowledge to make correct\ninferences. Even more strikingly, further experiments show that embedding the\npremise under presupposition triggers or non-factive verbs causes the model to\npredict entailment more frequently {regardless} of the correct semantic label.\nOverall these results suggest that, despite GPT's celebrated language\nunderstanding capacity, ChatGPT has blindspots with respect to certain types of\nentailment, and that certain entailment-cancelling features act as ``blinds''\novershadowing the semantics of the embedded premise. Our analyses emphasize the\nneed for further research into the linguistic comprehension and reasoning\ncapabilities of LLMs, in order to improve their reliability, and establish\ntheir trustworthiness for real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.14785v1.pdf"
    },
    {
        "title": "Anthropomorphization of AI: Opportunities and Risks",
        "authors": [
            "Ameet Deshpande",
            "Tanmay Rajpurohit",
            "Karthik Narasimhan",
            "Ashwin Kalyan"
        ],
        "published": "2023-05-24T06:39:45Z",
        "summary": "Anthropomorphization is the tendency to attribute human-like traits to\nnon-human entities. It is prevalent in many social contexts -- children\nanthropomorphize toys, adults do so with brands, and it is a literary device.\nIt is also a versatile tool in science, with behavioral psychology and\nevolutionary biology meticulously documenting its consequences. With widespread\nadoption of AI systems, and the push from stakeholders to make it human-like\nthrough alignment techniques, human voice, and pictorial avatars, the tendency\nfor users to anthropomorphize it increases significantly. We take a dyadic\napproach to understanding this phenomenon with large language models (LLMs) by\nstudying (1) the objective legal implications, as analyzed through the lens of\nthe recent blueprint of AI bill of rights and the (2) subtle psychological\naspects customization and anthropomorphization. We find that anthropomorphized\nLLMs customized for different user bases violate multiple provisions in the\nlegislative blueprint. In addition, we point out that anthropomorphization of\nLLMs affects the influence they can have on their users, thus having the\npotential to fundamentally change the nature of human-AI interaction, with\npotential for manipulation and negative influence. With LLMs being\nhyper-personalized for vulnerable groups like children and patients among\nothers, our work is a timely and important contribution. We propose a\nconservative strategy for the cautious use of anthropomorphization to improve\ntrustworthiness of AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.14784v1.pdf"
    },
    {
        "title": "Using Natural Language Explanations to Rescale Human Judgments",
        "authors": [
            "Manya Wadhwa",
            "Jifan Chen",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "published": "2023-05-24T06:19:14Z",
        "summary": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover crowdworker judgments. However, annotators' judgments for subjective tasks\ncan differ in many ways: they may have different qualitative judgments about an\nexample, and they may map those to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.",
        "pdf_link": "https://arxiv.org/pdf/2305.14770v2.pdf"
    },
    {
        "title": "Allies: Prompting Large Language Model with Beam Search",
        "authors": [
            "Hao Sun",
            "Xiao Liu",
            "Yeyun Gong",
            "Yan Zhang",
            "Daxin Jiang",
            "Linjun Yang",
            "Nan Duan"
        ],
        "published": "2023-05-24T06:16:44Z",
        "summary": "With the advance of large language models (LLMs), the research field of LLM\napplications becomes more and more popular and the idea of constructing\npipelines to accomplish complex tasks by stacking LLM API calls come true.\nHowever, this kind of methods face two limitations: narrow information coverage\nand low fault tolerance. In this work, we propose a novel method called ALLIES.\nGiven an input query, ALLIES leverages LLMs to iteratively generate new queries\nrelated to the original query, enabling an iterative reasoning process. By\niteratively refining and expanding the scope of the original query, ALLIES\ncaptures and utilizes hidden knowledge that may not be directly obtainable\nthrough retrieval. We take zero-shot open-domain question answering (ODQA) as\nan application scene and evaluate ALLIES on the widely-used benchmarks, such as\nNQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES\nsignificantly outperforms other zero-shot baselines, indicating its\neffectiveness in tackling those challenges. Our code is available in\nhttps://github.com/microsoft/SimXNS/tree/main/ALLIES.",
        "pdf_link": "https://arxiv.org/pdf/2305.14766v3.pdf"
    },
    {
        "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
        "authors": [
            "Natalie Shapira",
            "Mosh Levy",
            "Seyed Hossein Alavi",
            "Xuhui Zhou",
            "Yejin Choi",
            "Yoav Goldberg",
            "Maarten Sap",
            "Vered Shwartz"
        ],
        "published": "2023-05-24T06:14:31Z",
        "summary": "The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.",
        "pdf_link": "https://arxiv.org/pdf/2305.14763v1.pdf"
    },
    {
        "title": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
        "authors": [
            "Yiannis Charalambous",
            "Norbert Tihanyi",
            "Ridhi Jain",
            "Youcheng Sun",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro"
        ],
        "published": "2023-05-24T05:54:10Z",
        "summary": "In this paper we present a novel solution that combines the capabilities of\nLarge Language Models (LLMs) with Formal Verification strategies to verify and\nautomatically repair software vulnerabilities. Initially, we employ Bounded\nModel Checking (BMC) to locate the software vulnerability and derive a\ncounterexample. The counterexample provides evidence that the system behaves\nincorrectly or contains a vulnerability. The counterexample that has been\ndetected, along with the source code, are provided to the LLM engine. Our\napproach involves establishing a specialized prompt language for conducting\ncode debugging and generation to understand the vulnerability's root cause and\nrepair the code. Finally, we use BMC to verify the corrected version of the\ncode generated by the LLM. As a proof of concept, we create ESBMC-AI based on\nthe Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained\nTransformer model, specifically gpt-3.5-turbo, to detect and fix errors in C\nprograms. Our experimentation involved generating a dataset comprising 1000 C\ncode samples, each consisting of 20 to 50 lines of code. Notably, our proposed\nmethod achieved an impressive success rate of up to 80% in repairing vulnerable\ncode encompassing buffer overflow and pointer dereference failures. We assert\nthat this automated approach can effectively incorporate into the software\ndevelopment lifecycle's continuous integration and deployment (CI/CD) process.",
        "pdf_link": "https://arxiv.org/pdf/2305.14752v1.pdf"
    },
    {
        "title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation",
        "authors": [
            "Nishant Balepur",
            "Jie Huang",
            "Samraj Moorjani",
            "Hari Sundaram",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-05-24T05:53:11Z",
        "summary": "When answering complex questions, large language models (LLMs) may produce\nanswers that do not satisfy all criteria of the question. While existing\nself-evaluation techniques aim to detect if such answers are correct, these\ntechniques are unable to determine which criteria of the question are satisfied\nby the generated answers. To address this issue, we propose answer-based claim\ndecomposition (ABCD), a prompting strategy that decomposes questions into a\nseries of true/false claims that can be used to verify which criteria of the\ninput question an answer satisfies. Using the decomposed ABCD claims, we\nperform fine-grained self-evaluation. Through preliminary experiments on three\ndatasets, including a newly-collected challenge dataset ObscureQA, we find that\nGPT-3.5 has some ability to determine to what extent its answer satisfies the\ncriteria of the input question, and can give insights into the errors and\nknowledge gaps of the model.",
        "pdf_link": "https://arxiv.org/pdf/2305.14750v1.pdf"
    },
    {
        "title": "ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation",
        "authors": [
            "Dongxu Yue",
            "Qin Guo",
            "Munan Ning",
            "Jiaxi Cui",
            "Yuesheng Zhu",
            "Li Yuan"
        ],
        "published": "2023-05-24T05:28:37Z",
        "summary": "Editing real facial images is a crucial task in computer vision with\nsignificant demand in various real-world applications. While GAN-based methods\nhave showed potential in manipulating images especially when combined with\nCLIP, these methods are limited in their ability to reconstruct real images due\nto challenging GAN inversion capability. Despite the successful image\nreconstruction achieved by diffusion-based methods, there are still challenges\nin effectively manipulating fine-gained facial attributes with textual\ninstructions.To address these issues and facilitate convenient manipulation of\nreal facial images, we propose a novel approach that conduct text-driven image\nediting in the semantic latent space of diffusion model. By aligning the\ntemporal feature of the diffusion model with the semantic condition at\ngenerative process, we introduce a stable manipulation strategy, which perform\nprecise zero-shot manipulation effectively. Furthermore, we develop an\ninteractive system named ChatFace, which combines the zero-shot reasoning\nability of large language models to perform efficient manipulations in\ndiffusion semantic latent space. This system enables users to perform complex\nmulti-attribute manipulations through dialogue, opening up new possibilities\nfor interactive image editing. Extensive experiments confirmed that our\napproach outperforms previous methods and enables precise editing of real\nfacial images, making it a promising candidate for real-world applications.\nProject page: https://dongxuyue.github.io/chatface/",
        "pdf_link": "https://arxiv.org/pdf/2305.14742v2.pdf"
    },
    {
        "title": "In-Context Demonstration Selection with Cross Entropy Difference",
        "authors": [
            "Dan Iter",
            "Reid Pryzant",
            "Ruochen Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Yichong Xu",
            "Chenguang Zhu"
        ],
        "published": "2023-05-24T05:04:00Z",
        "summary": "Large language models (LLMs) can use in-context demonstrations to improve\nperformance on zero-shot tasks. However, selecting the best in-context examples\nis challenging because model performance can vary widely depending on the\nselected examples. We present a cross-entropy difference (CED) method for\nselecting in-context demonstrations. Our method is based on the observation\nthat the effectiveness of in-context demonstrations negatively correlates with\nthe perplexity of the test example by a language model that was finetuned on\nthat demonstration. We utilize parameter efficient finetuning to train small\nmodels on training data that are used for computing the cross-entropy\ndifference between a test example and every candidate in-context demonstration.\nThis metric is used to rank and select in-context demonstrations independently\nfor each test input. We evaluate our method on a mix-domain dataset that\ncombines 8 benchmarks, representing 4 text generation tasks, showing that CED\nfor in-context demonstration selection can improve performance for a variety of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14726v2.pdf"
    },
    {
        "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Olivia Winn",
            "Artemis Panagopoulou",
            "Yue Yang",
            "Marianna Apidianaki",
            "Smaranda Muresan"
        ],
        "published": "2023-05-24T05:01:10Z",
        "summary": "Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask . To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.",
        "pdf_link": "https://arxiv.org/pdf/2305.14724v2.pdf"
    },
    {
        "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
        "authors": [
            "Jiashu Xu",
            "Mingyu Derek Ma",
            "Fei Wang",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-05-24T04:27:21Z",
        "summary": "We investigate security concerns of the emergent instruction tuning paradigm,\nthat models are trained on crowdsourced datasets with task instructions to\nachieve superior performance. Our studies demonstrate that an attacker can\ninject backdoors by issuing very few malicious instructions (~1000 tokens) and\ncontrol model behavior through data poisoning, without even the need to modify\ndata instances or labels themselves. Through such instruction attacks, the\nattacker can achieve over 90% attack success rate across four commonly used NLP\ndatasets. As an empirical study on instruction attacks, we systematically\nevaluated unique perspectives of instruction attacks, such as poison transfer\nwhere poisoned models can transfer to 15 diverse generative datasets in a\nzero-shot manner; instruction transfer where attackers can directly apply\npoisoned instruction on many other datasets; and poison resistance to continual\nfinetuning. Lastly, we show that RLHF and clean demonstrations might mitigate\nsuch backdoors to some degree. These findings highlight the need for more\nrobust defenses against poisoning attacks in instruction-tuning models and\nunderscore the importance of ensuring data quality in instruction\ncrowdsourcing.",
        "pdf_link": "https://arxiv.org/pdf/2305.14710v2.pdf"
    },
    {
        "title": "SciFix: Outperforming GPT3 on Scientific Factual Error Correction",
        "authors": [
            "Dhananjay Ashok",
            "Atharva Kulkarni",
            "Hai Pham",
            "Barnab\u00e1s P\u00f3czos"
        ],
        "published": "2023-05-24T04:24:16Z",
        "summary": "Due to the prohibitively high cost of creating error correction datasets,\nmost Factual Claim Correction methods rely on a powerful verification model to\nguide the correction process. This leads to a significant drop in performance\nin domains like scientific claims, where good verification models do not always\nexist. In this work, we introduce SciFix, a scientific claim correction system\nthat does not require a verifier but can outperform existing methods by a\nconsiderable margin -- achieving correction accuracy of 84% on the SciFact\ndataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to next\nbest accuracies of 7%, 5%, and 15% on the same datasets respectively. Our\nmethod leverages the power of prompting with LLMs during training to create a\nrichly annotated dataset that can be used for fully supervised training and\nregularization. We additionally use a claim-aware decoding procedure to improve\nthe quality of corrected claims. Our method outperforms the very LLM that was\nused to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5\nachieving 58%, 61%, and 64% on the respective datasets, a consistently lower\ncorrection accuracy, despite using nearly 800 times as many parameters as our\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2305.14707v2.pdf"
    },
    {
        "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models",
        "authors": [
            "Sheng Shen",
            "Le Hou",
            "Yanqi Zhou",
            "Nan Du",
            "Shayne Longpre",
            "Jason Wei",
            "Hyung Won Chung",
            "Barret Zoph",
            "William Fedus",
            "Xinyun Chen",
            "Tu Vu",
            "Yuexin Wu",
            "Wuyang Chen",
            "Albert Webson",
            "Yunxuan Li",
            "Vincent Zhao",
            "Hongkun Yu",
            "Kurt Keutzer",
            "Trevor Darrell",
            "Denny Zhou"
        ],
        "published": "2023-05-24T04:22:26Z",
        "summary": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be\nutilized to add learnable parameters to Large Language Models (LLMs) without\nincreasing inference cost. Instruction tuning is a technique for training LLMs\nto follow instructions. We advocate combining these two approaches, as we find\nthat MoE models benefit more from instruction tuning than dense models. In\nparticular, we conduct empirical studies across three experimental setups: (i)\nDirect finetuning on individual downstream tasks devoid of instruction tuning;\n(ii) Instructiontuning followed by in-context few-shot or zero-shot\ngeneralization on downstream tasks; and (iii) Instruction tuning supplemented\nby further finetuning on individual downstream tasks. In the first scenario,\nMoE models overall underperform dense models of identical computational\ncapacity. This narrative, however, dramatically changes with the introduction\nof instruction tuning (second and third scenario), used independently or in\nconjunction with task-specific finetuning. Our most powerful model,\nFLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark\ntasks, while using only a third of the FLOPs. The advancements embodied\nbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,\nhigh-performance language models in the framework of task-agnostic learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.14705v2.pdf"
    },
    {
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Fei Liu"
        ],
        "published": "2023-05-24T04:13:15Z",
        "summary": "Human preference judgments are pivotal in guiding large language models\n(LLMs) to produce outputs that align with human values. Human evaluations are\nalso used in summarization tasks to compare outputs from various systems,\ncomplementing existing automatic metrics. Despite their significance, however,\nthere has been limited research probing these pairwise or $k$-wise comparisons.\nThe collective impact and relative importance of factors such as output length,\ninformativeness, fluency, and factual consistency are still not well\nunderstood. It is also unclear if there are other hidden factors influencing\nhuman judgments. In this paper, we conduct an in-depth examination of a\ncollection of pairwise human judgments released by OpenAI. Utilizing the\nBradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in\nthese human judgments. We find that the most favored factors vary across tasks\nand genres, whereas the least favored factors tend to be consistent, e.g.,\noutputs are too brief, contain excessive off-focus content or hallucinated\nfacts. Our findings have implications on the construction of balanced datasets\nin human preference evaluations, which is a crucial step in shaping the\nbehaviors of future LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14702v3.pdf"
    },
    {
        "title": "A Causal View of Entity Bias in (Large) Language Models",
        "authors": [
            "Fei Wang",
            "Wenjie Mo",
            "Yiwei Wang",
            "Wenxuan Zhou",
            "Muhao Chen"
        ],
        "published": "2023-05-24T03:59:18Z",
        "summary": "Entity bias widely affects pretrained (large) language models, causing them\nto rely on (biased) parametric knowledge to make unfaithful predictions.\nAlthough causality-inspired methods have shown great potential to mitigate\nentity bias, it is hard to precisely estimate the parameters of underlying\ncausal models in practice. The rise of black-box LLMs also makes the situation\neven worse, because of their inaccessible parameters and uncalibrated logits.\nTo address these problems, we propose a specific structured causal model (SCM)\nwhose parameters are comparatively easier to estimate. Building upon this SCM,\nwe propose causal intervention techniques to mitigate entity bias for both\nwhite-box and black-box settings. The proposed causal intervention perturbs the\noriginal entity with neighboring entities. This intervention reduces specific\nbiasing information pertaining to the original entity while still preserving\nsufficient semantic information from similar entities. Under the white-box\nsetting, our training-time intervention improves OOD performance of PLMs on\nrelation extraction (RE) and machine reading comprehension (MRC) by 5.7 points\nand by 9.1 points, respectively. Under the black-box setting, our in-context\nintervention effectively reduces the entity-based knowledge conflicts of\nGPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on\nMRC and up to 17.6 points of reduction in memorization ratio on RE. Our code is\navailable at https://github.com/luka-group/Causal-View-of-Entity-Bias.",
        "pdf_link": "https://arxiv.org/pdf/2305.14695v2.pdf"
    },
    {
        "title": "Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs",
        "authors": [
            "Xiaoyang Song",
            "Akshat Gupta",
            "Kiyan Mohebbizadeh",
            "Shujie Hu",
            "Anant Singh"
        ],
        "published": "2023-05-24T03:53:43Z",
        "summary": "Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.",
        "pdf_link": "https://arxiv.org/pdf/2305.14693v1.pdf"
    },
    {
        "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
        "authors": [
            "Benfeng Xu",
            "An Yang",
            "Junyang Lin",
            "Quan Wang",
            "Chang Zhou",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023-05-24T03:51:31Z",
        "summary": "The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\n\\url{https://github.com/OFA-Sys/ExpertLLaMA}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14688v1.pdf"
    },
    {
        "title": "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response",
        "authors": [
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-05-24T02:52:48Z",
        "summary": "LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.",
        "pdf_link": "https://arxiv.org/pdf/2305.14658v2.pdf"
    },
    {
        "title": "Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering",
        "authors": [
            "Davit Soselia",
            "Khalid Saifullah",
            "Tianyi Zhou"
        ],
        "published": "2023-05-24T02:17:32Z",
        "summary": "Automated reverse engineering of HTML/CSS code from UI screenshots is an\nimportant yet challenging problem with broad applications in website\ndevelopment and design. In this paper, we propose a novel vision-code\ntransformer (ViCT) composed of a vision encoder processing the screenshots and\na language decoder to generate the code. They are initialized by pre-trained\nmodels such as ViT/DiT and GPT-2/LLaMA but aligning the two modalities requires\nend-to-end finetuning, which aims to minimize the visual discrepancy between\nthe code-rendered webpage and the original screenshot. However, the rendering\nis non-differentiable and causes costly overhead. We address this problem by\nactor-critic fine-tuning where a visual critic without rendering (ViCR) is\ndeveloped to predict visual discrepancy given the original and generated code.\nTo train and evaluate our models, we created two synthetic datasets of varying\ncomplexity, with over 75,000 unique (code, screenshot) pairs. We evaluate the\nUI-to-Code performance using a combination of automated metrics such as MSE,\nBLEU, IoU, and a novel htmlBLEU score. ViCT outperforms a strong baseline model\nDiT-GPT2, improving IoU from 0.64 to 0.79 and lowering MSE from 12.25 to 9.02.\nWith much lower computational cost, it can achieve comparable performance as\nwhen using a larger decoder such as LLaMA.",
        "pdf_link": "https://arxiv.org/pdf/2305.14637v2.pdf"
    },
    {
        "title": "Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
        "authors": [
            "Xiang Zhang",
            "Senyu Li",
            "Bradley Hauer",
            "Ning Shi",
            "Grzegorz Kondrak"
        ],
        "published": "2023-05-24T02:05:03Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.16339v2.pdf"
    },
    {
        "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
        "authors": [
            "Chenglei Si",
            "Weijia Shi",
            "Chen Zhao",
            "Luke Zettlemoyer",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-05-24T02:00:51Z",
        "summary": "While recent large language models (LLMs) improve on various question\nanswering (QA) datasets, it remains difficult for a single model to generalize\nacross question types that require distinct reasoning abilities. We provide\nempirical evidence that state-of-the-art LLMs suffer from poor generalizability\non reasoning types beyond those seen in the prompt. To remedy this, we propose\na Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse\nspecialized language models. We specialize the backbone language model with\nprompts optimized for different reasoning categories, including factual,\nmultihop, mathematical, and commonsense reasoning. Our key insight is to\nleverage agreement among the specialized experts to select the best answer for\neach question, or to abstain from answering. This gives MoRE higher accuracy\nthan any single specialized model on a collection of 12 QA datasets from four\nreasoning types. Beyond generalizability, the interpretable design of MoRE\nimproves selective question answering results compared to baselines without\nincorporating inter-expert agreement. This framework is also more interpretable\nand useful to human consumers of QA outputs. Our human study confirms that\npresenting expert predictions and the answer selection process helps annotators\nmore accurately calibrate when to trust the system's output. We release all\ncode and data to facilitate future work.",
        "pdf_link": "https://arxiv.org/pdf/2305.14628v2.pdf"
    },
    {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published": "2023-05-24T01:53:49Z",
        "summary": "Large language models (LLMs) have emerged as a widely-used tool for\ninformation seeking, but their generated outputs are prone to hallucination. In\nthis work, our aim is to allow LLMs to generate text with citations, improving\ntheir factual correctness and verifiability. Existing work mainly relies on\ncommercial search engines and human evaluation, making it challenging to\nreproduce and compare different modeling approaches. We propose ALCE, the first\nbenchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set\nof questions and retrieval corpora and requires building end-to-end systems to\nretrieve supporting evidence and generate answers with citations. We develop\nautomatic metrics along three dimensions -- fluency, correctness, and citation\nquality -- and demonstrate their strong correlation with human judgements. Our\nexperiments with state-of-the-art LLMs and novel prompting strategies show that\ncurrent systems have considerable room for improvement -- For example, on the\nELI5 dataset, even the best models lack complete citation support 50% of the\ntime. Our analyses further highlight promising future directions, including\ndeveloping better retrievers, advancing long-context LLMs, and improving the\nability to synthesize information from multiple sources.",
        "pdf_link": "https://arxiv.org/pdf/2305.14627v2.pdf"
    },
    {
        "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
        "authors": [
            "Miaoran Li",
            "Baolin Peng",
            "Michel Galley",
            "Jianfeng Gao",
            "Zhu Zhang"
        ],
        "published": "2023-05-24T01:46:07Z",
        "summary": "Fact-checking is an essential task in NLP that is commonly utilized for\nvalidating the factual accuracy of claims. Prior work has mainly focused on\nfine-tuning pre-trained languages models on specific datasets, which can be\ncomputationally intensive and time-consuming. With the rapid development of\nlarge language models (LLMs), such as ChatGPT and GPT-3, researchers are now\nexploring their in-context learning capabilities for a wide range of tasks. In\nthis paper, we aim to assess the capacity of LLMs for fact-checking by\nintroducing Self-Checker, a framework comprising a set of plug-and-play modules\nthat facilitate fact-checking by purely prompting LLMs in an almost zero-shot\nsetting. This framework provides a fast and efficient way to construct\nfact-checking systems in low-resource environments. Empirical results\ndemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.\nHowever, there is still significant room for improvement compared to SOTA\nfine-tuned models, which suggests that LLM adoption could be a promising\napproach for future fact-checking research.",
        "pdf_link": "https://arxiv.org/pdf/2305.14623v2.pdf"
    },
    {
        "title": "Think Before You Act: Decision Transformers with Internal Working Memory",
        "authors": [
            "Jikun Kang",
            "Romain Laroche",
            "Xindi Yuan",
            "Adam Trischler",
            "Xue Liu",
            "Jie Fu"
        ],
        "published": "2023-05-24T01:20:22Z",
        "summary": "Large language model (LLM)-based decision-making agents have shown the\nability to generalize across multiple tasks. However, their performance relies\non massive data and compute. We argue that this inefficiency stems from the\nforgetting phenomenon, in which a model memorizes its behaviors in parameters\nthroughout training. As a result, training on a new task may deteriorate the\nmodel's performance on previous tasks. In contrast to LLMs' implicit memory\nmechanism, the human brain utilizes distributed memory storage, which helps\nmanage and organize multiple skills efficiently, mitigating the forgetting\nphenomenon. Thus inspired, we propose an internal working memory module to\nstore, blend, and retrieve information for different downstream tasks.\nEvaluation results show that the proposed method improves training efficiency\nand generalization in both Atari games and meta-world object manipulation\ntasks. Moreover, we demonstrate that memory fine-tuning further enhances the\nadaptability of the proposed architecture.",
        "pdf_link": "https://arxiv.org/pdf/2305.16338v1.pdf"
    },
    {
        "title": "This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models",
        "authors": [
            "Bryan Li",
            "Samar Haider",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-24T01:16:17Z",
        "summary": "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A\npretrained large language model (LLM) may answer differently if asked in the\nlanguages of each claimant country: Chinese, Tagalog, or Vietnamese. This\ncontrasts with a multilingual human, who would likely answer consistently. In\nthis paper, we show that LLMs recall certain geographical knowledge\ninconsistently when queried in different languages -- a phenomenon we term\ngeopolitical bias. As a targeted case study, we consider territorial disputes,\nan inherently controversial and multilingual task. We introduce BorderLines, a\ndataset of territorial disputes which covers 251 territories, each associated\nwith a set of multiple-choice questions in the languages of each claimant\ncountry (49 languages in total). We also propose a suite of evaluation metrics\nto precisely quantify bias and consistency in responses across different\nlanguages. We then evaluate various multilingual LLMs on our dataset and\nmetrics to probe their internal knowledge and use the proposed metrics to\ndiscover numerous inconsistencies in how these models respond in different\nlanguages. Finally, we explore several prompt modification strategies, aiming\nto either amplify or mitigate geopolitical bias, which highlights how brittle\nLLMs are and how they tailor their responses depending on cues from the\ninteraction context. Our code and data are available at\nhttps://github.com/manestay/borderlines",
        "pdf_link": "https://arxiv.org/pdf/2305.14610v4.pdf"
    },
    {
        "title": "ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers",
        "authors": [
            "Kexun Zhang",
            "Danqing Wang",
            "Jingtao Xia",
            "William Yang Wang",
            "Lei Li"
        ],
        "published": "2023-05-24T00:10:15Z",
        "summary": "Large language models (LLMs) excel at implementing code from functionality\ndescriptions but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ngeneration and verify their correctness. ALGO first generates a reference\noracle by prompting an LLM to exhaustively enumerate all the combinations of\nrelevant variables. This oracle is then utilized to guide an arbitrary search\nstrategy in exploring the algorithm space and to verify the synthesized\nalgorithms. Our study shows that the LLM-generated oracles are correct for 88%\nof the cases. With the oracles as verifiers, ALGO can be integrated with any\nexisting code generation model in a model-agnostic manner to enhance its\nperformance. Experiments show that when equipped with ALGO, we achieve an 8x\nbetter one-submission pass rate over the Codex model and a 2.6x better\none-submission pass rate over CodeT, the current state-of-the-art model on\nCodeContests. We can also get 1.3x better pass rate over the ChatGPT Code\nInterpreter on unseen problems. The problem set we used for testing, the\nprompts we used, the verifier and solution programs, and the test cases\ngenerated by ALGO are available at https://github.com/zkx06111/ALGO.",
        "pdf_link": "https://arxiv.org/pdf/2305.14591v3.pdf"
    },
    {
        "title": "Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person",
        "authors": [
            "Lucas Rafael Stefanel Gris",
            "Ricardo Marcacini",
            "Arnaldo Candido Junior",
            "Edresson Casanova",
            "Anderson Soares",
            "Sandra Maria Alu\u00edsio"
        ],
        "published": "2023-05-23T23:37:29Z",
        "summary": "Automatic speech recognition (ASR) systems play a key role in applications\ninvolving human-machine interactions. Despite their importance, ASR models for\nthe Portuguese language proposed in the last decade have limitations in\nrelation to the correct identification of punctuation marks in automatic\ntranscriptions, which hinder the use of transcriptions by other systems,\nmodels, and even by humans. However, recently Whisper ASR was proposed by\nOpenAI, a general-purpose speech recognition model that has generated great\nexpectations in dealing with such limitations. This chapter presents the first\nstudy on the performance of Whisper for punctuation prediction in the\nPortuguese language. We present an experimental evaluation considering both\ntheoretical aspects involving pausing points (comma) and complete ideas\n(exclamation, question, and fullstop), as well as practical aspects involving\ntranscript-based topic modeling - an application dependent on punctuation marks\nfor promising performance. We analyzed experimental results from videos of\nMuseum of the Person, a virtual museum that aims to tell and preserve people's\nlife histories, thus discussing the pros and cons of Whisper in a real-world\nscenario. Although our experiments indicate that Whisper achieves\nstate-of-the-art results, we conclude that some punctuation marks require\nimprovements, such as exclamation, semicolon and colon.",
        "pdf_link": "https://arxiv.org/pdf/2305.14580v2.pdf"
    },
    {
        "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Shuohang Wang",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2023-05-23T23:06:04Z",
        "summary": "Strategies such as chain-of-thought prompting improve the performance of\nlarge language models (LLMs) on complex reasoning tasks by decomposing input\nexamples into intermediate steps. However, it remains unclear how to apply such\nmethods to reason over long input documents, in which both the decomposition\nand the output of each intermediate step are non-trivial to obtain. In this\nwork, we propose PEARL, a prompting framework to improve reasoning over long\ndocuments, which consists of three stages: action mining, plan formulation, and\nplan execution. More specifically, given a question about a long document,\nPEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then executes them over the document to obtain\nthe answer. Each stage of PEARL is implemented via zero-shot or few-shot\nprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate\nPEARL on a challenging subset of the QuALITY dataset, which contains questions\nthat require complex reasoning over long narrative texts. PEARL outperforms\nzero-shot and chain-of-thought prompting on this dataset, and ablation\nexperiments show that each stage of PEARL is critical to its performance.\nOverall, PEARL is a first step towards leveraging LLMs to reason over long\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2305.14564v1.pdf"
    },
    {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published": "2023-05-23T22:24:44Z",
        "summary": "Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization. We present a series of behavioral studies on several LLM\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\nexperiments. We establish two biases originating from pretraining which predict\nmuch of their behavior, and show that these are major sources of hallucination\nin generative LLMs. First, memorization at the level of sentences: we show\nthat, regardless of the premise, models falsely label NLI test samples as\nentailing when the hypothesis is attested in training data, and that entities\nare used as ``indices'' to access the memorized data. Second, statistical\npatterns of usage learned at the level of corpora: we further show a similar\neffect when the premise predicate is less frequent than that of the hypothesis\nin the training data, a bias following from previous studies. We demonstrate\nthat LLMs perform significantly worse on NLI test samples which do not conform\nto these biases than those which do, and we offer these as valuable controls\nfor future LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14552v2.pdf"
    },
    {
        "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
        "authors": [
            "Philippe Laban",
            "Wojciech Kry\u015bci\u0144ski",
            "Divyansh Agarwal",
            "Alexander R. Fabbri",
            "Caiming Xiong",
            "Shafiq Joty",
            "Chien-Sheng Wu"
        ],
        "published": "2023-05-23T21:50:06Z",
        "summary": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.",
        "pdf_link": "https://arxiv.org/pdf/2305.14540v1.pdf"
    },
    {
        "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
        "authors": [
            "Jakub Macina",
            "Nico Daheim",
            "Sankalan Pal Chowdhury",
            "Tanmay Sinha",
            "Manu Kapur",
            "Iryna Gurevych",
            "Mrinmaya Sachan"
        ],
        "published": "2023-05-23T21:44:56Z",
        "summary": "While automatic dialogue tutors hold great potential in making education\npersonalized and more accessible, research on such systems has been hampered by\na lack of sufficiently large and high-quality datasets. Collecting such\ndatasets remains challenging, as recording tutoring sessions raises privacy\nconcerns and crowdsourcing leads to insufficient data quality. To address this,\nwe propose a framework to generate such dialogues by pairing human teachers\nwith a Large Language Model (LLM) prompted to represent common student errors.\nWe describe how we use this framework to collect MathDial, a dataset of 3k\none-to-one teacher-student tutoring dialogues grounded in multi-step math\nreasoning problems. While models like GPT-3 are good problem solvers, they fail\nat tutoring because they generate factually incorrect feedback or are prone to\nrevealing solutions to students too early. To overcome this, we let teachers\nprovide learning opportunities to students by guiding them using various\nscaffolding questions according to a taxonomy of teacher moves. We demonstrate\nMathDial and its extensive annotations can be used to finetune models to be\nmore effective tutors (and not just solvers). We confirm this by automatic and\nhuman evaluation, notably in an interactive setting that measures the trade-off\nbetween student solving success and telling solutions. The dataset is released\npublicly.",
        "pdf_link": "https://arxiv.org/pdf/2305.14536v2.pdf"
    },
    {
        "title": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "Richard G. Baraniuk"
        ],
        "published": "2023-05-23T20:26:03Z",
        "summary": "We explore whether Large Language Models (LLMs) are capable of logical\nreasoning with distorted facts, which we call Deduction under Perturbed\nEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typically\nrely on their parameters, which encode mostly accurate information, to reason\nand make inferences. However, in DUPE, LLMs must reason over manipulated or\nfalsified evidence present in their prompts, which can result in false\nconclusions that are valid only under the manipulated evidence. Our goal with\nDUPE is to determine whether LLMs can arrive at these false conclusions and\nidentify whether the dominant factor influencing the deduction process is the\nencoded data in the parameters or the manipulated evidence in the prompts. To\nevaluate the DUPE capabilities of LLMs, we create a DUPEd version of the\nStrategyQA dataset, where facts are manipulated to reverse the answer to the\nquestion. Our findings show that even the most advanced GPT models struggle to\nreason on manipulated facts - showcasing poor DUPE skills - with accuracy\ndropping by 45% compared to the original dataset. We also investigate prompt\nsettings inspired from student simulation models, which mitigate the accuracy\ndrop to some extent. Our findings have practical implications for understanding\nthe performance of LLMs in real-world applications such as student simulation\nmodels that involve reasoning over inaccurate information.",
        "pdf_link": "https://arxiv.org/pdf/2305.14507v1.pdf"
    },
    {
        "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
        "authors": [
            "Zhiheng Xi",
            "Senjie Jin",
            "Yuhao Zhou",
            "Rui Zheng",
            "Songyang Gao",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-05-23T19:58:30Z",
        "summary": "Prompting methods such as Chain-of-Thought (CoT) have shed new light on\nenhancing the reasoning capabilities of large language models, and researchers\nhave extensively explored the generation process of rationales and answers.\nHowever, they have overlooked the potential challenges posed by the poor\nquality of reasoning problems, which may influence the reasoning performance\nsignificantly. In this work, we propose Self-Polish (SP), a novel method that\nfacilitates the model's problem-solving process by prompting them to\nprogressively refine the given problems to be more comprehensible and solvable.\nSpecifically, the method teaches models to eliminate irrelevant information,\nrearrange the logic structure and organize local conditions into new ones\nparallelly. SP is orthogonal to all other prompting methods, making it\nconvenient to integrate with state-of-the-art techniques for further\nimprovement. We conduct thorough experiments on five benchmarks to illustrate\nthe effectiveness of the proposed method. For example, with Text-davinci-003,\nour method boosts the performance of standard few-shot prompting by $8.0\\%$ on\nGSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by\n$6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method\nalso showcases impressive performance on robustness evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14497v1.pdf"
    },
    {
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
        "authors": [
            "Jing-Cheng Pang",
            "Pengyuan Wang",
            "Kaiyuan Li",
            "Xiong-Hui Chen",
            "Jiacheng Xu",
            "Zongzhang Zhang",
            "Yang Yu"
        ],
        "published": "2023-05-23T19:25:52Z",
        "summary": "Large Language Models (LLMs) have exhibited remarkable performance across\nvarious natural language processing (NLP) tasks. However, fine-tuning these\nmodels often necessitates substantial supervision, which can be expensive and\ntime-consuming to obtain. This paper introduces a novel unsupervised method\ncalled LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n(SIRLC) that improves LLMs without reliance on external labels. Our approach is\ngrounded in the observation that it is simpler for language models to assess\ntext quality than to generate text. Building on this insight, SIRLC assigns\nLLMs dual roles as both student and teacher. As a student, the LLM generates\nanswers to unlabeled questions, while as a teacher, it evaluates the generated\ntext and assigns scores accordingly. The model parameters are updated using\nreinforcement learning to maximize the evaluation score. We demonstrate that\nSIRLC can be applied to various NLP tasks, such as reasoning problems, text\ngeneration, and machine translation. Our experiments show that SIRLC\neffectively improves LLM performance without external supervision, resulting in\na 5.6% increase in answering accuracy for reasoning tasks and a rise in\nBERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\napplied to models of different sizes, showcasing its broad applicability.",
        "pdf_link": "https://arxiv.org/pdf/2305.14483v1.pdf"
    },
    {
        "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
        "authors": [
            "David Heineman",
            "Yao Dou",
            "Mounica Maddela",
            "Wei Xu"
        ],
        "published": "2023-05-23T18:30:49Z",
        "summary": "Large language models (e.g., GPT-4) are uniquely capable of producing highly\nrated text simplification, yet current human evaluation methods fail to provide\na clear understanding of systems' specific strengths and weaknesses. To address\nthis limitation, we introduce SALSA, an edit-based human annotation framework\nthat enables holistic and fine-grained text simplification evaluation. We\ndevelop twenty one linguistically grounded edit types, covering the full\nspectrum of success and failure across dimensions of conceptual, syntactic and\nlexical simplicity. Using SALSA, we collect 19K edit annotations on 840\nsimplifications, revealing discrepancies in the distribution of simplification\nstrategies performed by fine-tuned models, prompted LLMs and humans, and find\nGPT-3.5 performs more quality edits than humans, but still exhibits frequent\nerrors. Using our fine-grained annotations, we develop LENS-SALSA, a\nreference-free automatic simplification metric, trained to predict sentence-\nand word-level quality simultaneously. Additionally, we introduce word-level\nquality estimation for simplification and report promising baseline results.\nOur data, new metric, and annotation toolkit are available at\nhttps://salsa-eval.com.",
        "pdf_link": "https://arxiv.org/pdf/2305.14458v2.pdf"
    },
    {
        "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
        "authors": [
            "Tarek Naous",
            "Michael J. Ryan",
            "Alan Ritter",
            "Wei Xu"
        ],
        "published": "2023-05-23T18:27:51Z",
        "summary": "As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel",
        "pdf_link": "https://arxiv.org/pdf/2305.14456v4.pdf"
    },
    {
        "title": "On Robustness of Finetuned Transformer-based NLP Models",
        "authors": [
            "Pavan Kalyan Reddy Neerudu",
            "Subba Reddy Oota",
            "Mounika Marreddy",
            "Venkateswara Rao Kagita",
            "Manish Gupta"
        ],
        "published": "2023-05-23T18:25:18Z",
        "summary": "Transformer-based pretrained models like BERT, GPT-2 and T5 have been\nfinetuned for a large number of natural language processing (NLP) tasks, and\nhave been shown to be very effective. However, while finetuning, what changes\nacross layers in these models with respect to pretrained checkpoints is\nunder-studied. Further, how robust are these models to perturbations in input\ntext? Does the robustness vary depending on the NLP task for which the models\nhave been finetuned? While there exists some work on studying the robustness of\nBERT finetuned for a few NLP tasks, there is no rigorous study that compares\nthis robustness across encoder only, decoder only and encoder-decoder models.\nIn this paper, we characterize changes between pretrained and finetuned\nlanguage model representations across layers using two metrics: CKA and STIR.\nFurther, we study the robustness of three language models (BERT, GPT-2 and T5)\nwith eight different text perturbations on classification tasks from the\nGeneral Language Understanding Evaluation (GLUE) benchmark, and generation\ntasks like summarization, free-form generation and question generation. GPT-2\nrepresentations are more robust than BERT and T5 across multiple types of input\nperturbation. Although models exhibit good robustness broadly, dropping nouns,\nverbs or changing characters are the most impactful. Overall, this study\nprovides valuable insights into perturbation-specific weaknesses of popular\nTransformer-based models, which should be kept in mind when passing inputs. We\nmake the code and models publicly available\n[https://github.com/PavanNeerudu/Robustness-of-Transformers-models].",
        "pdf_link": "https://arxiv.org/pdf/2305.14453v2.pdf"
    },
    {
        "title": "Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding",
        "authors": [
            "Zheng Chen",
            "Ziyan Jiang",
            "Fan Yang",
            "Eunah Cho",
            "Xing Fan",
            "Xiaojiang Huang",
            "Yanbin Lu",
            "Aram Galstyan"
        ],
        "published": "2023-05-23T18:15:29Z",
        "summary": "Conversational AI systems such as Alexa need to understand defective queries\nto ensure robust conversational understanding and reduce user friction. These\ndefective queries often arise from user ambiguities, mistakes, or errors in\nautomatic speech recognition (ASR) and natural language understanding (NLU).\n  Personalized query rewriting is an approach that focuses on reducing defects\nin queries by taking into account the user's individual behavior and\npreferences. It typically relies on an index of past successful user\ninteractions with the conversational AI. However, unseen interactions within\nthe user's history present additional challenges for personalized query\nrewriting. This paper presents our \"Collaborative Query Rewriting\" approach,\nwhich specifically addresses the task of rewriting new user interactions that\nhave not been previously observed in the user's history. This approach builds a\n\"User Feedback Interaction Graph\" (FIG) of historical user-entity interactions\nand leverages multi-hop graph traversal to enrich each user's index to cover\nfuture unseen defective queries. The enriched user index is called a\nCollaborative User Index and contains hundreds of additional entries. To\ncounteract precision degradation from the enlarged index, we add additional\ntransformer layers to the L1 retrieval model and incorporate graph-based and\nguardrail features into the L2 ranking model.\n  Since the user index can be pre-computed, we further investigate the\nutilization of a Large Language Model (LLM) to enhance the FIG for user-entity\nlink prediction in the Video/Music domains. Specifically, this paper\ninvestigates the Dolly-V2 7B model. We found that the user index augmented by\nthe fine-tuned Dolly-V2 generation significantly enhanced the coverage of\nfuture unseen user interactions, thereby boosting QR performance on unseen\nqueries compared with the graph traversal only approach.",
        "pdf_link": "https://arxiv.org/pdf/2305.14449v3.pdf"
    },
    {
        "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
        "authors": [
            "Wentao Bao",
            "Lichang Chen",
            "Heng Huang",
            "Yu Kong"
        ],
        "published": "2023-05-23T18:00:22Z",
        "summary": "Compositional zero-shot learning (CZSL) task aims to recognize unseen\ncompositional visual concepts, e.g., sliced tomatoes, where the model is\nlearned only from the seen compositions, e.g., sliced potatoes and red\ntomatoes. Thanks to the prompt tuning on large pre-trained visual language\nmodels such as CLIP, recent literature shows impressively better CZSL\nperformance than traditional vision-based methods. However, the key aspects\nthat impact the generalization to unseen compositions, including the diversity\nand informativeness of class context, and the entanglement between visual\nprimitives, i.e., state and object, are not properly addressed in existing\nCLIP-based CZSL literature. In this paper, we propose a model by prompting the\nlanguage-informed distribution, aka., PLID, for the CZSL task. Specifically,\nthe PLID leverages pre-trained large language models (LLM) to 1) formulate the\nlanguage-informed class distributions which are diverse and informative, and 2)\nenhance the compositionality of the class embedding. Moreover, a\nvisual-language primitive decomposition (VLPD) module and a stochastic logit\nmixup (SLM) strategy are proposed to dynamically fuse the decisions from the\ncompositional and the primitive logit space. Orthogonal to the existing\nliterature of soft, hard, or distributional prompts, our method advocates\nprompting the LLM-supported class distribution that leads to a better zero-shot\ngeneralization. Experimental results on MIT-States, UT-Zappos, and C-GQA\ndatasets show the superior performance of the PLID to the prior arts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14428v2.pdf"
    },
    {
        "title": "Schema-Driven Information Extraction from Heterogeneous Tables",
        "authors": [
            "Fan Bai",
            "Junmo Kang",
            "Gabriel Stanovsky",
            "Dayne Freitag",
            "Alan Ritter"
        ],
        "published": "2023-05-23T17:58:10Z",
        "summary": "In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.",
        "pdf_link": "https://arxiv.org/pdf/2305.14336v3.pdf"
    },
    {
        "title": "DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation",
        "authors": [
            "Susung Hong",
            "Junyoung Seo",
            "Heeseong Shin",
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "published": "2023-05-23T17:57:09Z",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention to transferring knowledge from pre-trained text-to-image (T2I) models\nto text-to-video (T2V) generation. Despite their effectiveness, these\nframeworks face challenges in maintaining consistent narratives and handling\nshifts in scene composition or object placement from a single abstract user\nprompt. Exploring the ability of large language models (LLMs) to generate\ntime-dependent, frame-by-frame prompts, this paper introduces a new framework,\ndubbed DirecT2V. DirecT2V leverages instruction-tuned LLMs as directors,\nenabling the inclusion of time-varying content and facilitating consistent\nvideo generation. To maintain temporal consistency and prevent mapping the\nvalue to a different object, we equip a diffusion model with a novel value\nmapping method and dual-softmax filtering, which do not require any additional\ntraining. The experimental results validate the effectiveness of our framework\nin producing visually coherent and storyful videos from abstract user prompts,\nsuccessfully addressing the challenges of zero-shot video generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14330v3.pdf"
    },
    {
        "title": "Benchmarking LLM-based Machine Translation on Cultural Awareness",
        "authors": [
            "Binwei Yao",
            "Ming Jiang",
            "Diyi Yang",
            "Junjie Hu"
        ],
        "published": "2023-05-23T17:56:33Z",
        "summary": "Translating cultural-specific content is crucial for effective cross-cultural\ncommunication. However, many MT systems still struggle to translate sentences\ncontaining cultural-specific entities accurately and understandably. Recent\nadvancements in in-context learning utilize lightweight prompts to guide large\nlanguage models (LLMs) in machine translation tasks. Nevertheless, the\neffectiveness of this approach in enhancing machine translation with cultural\nawareness remains uncertain. To address this gap, we introduce a new data\ncuration pipeline to construct a culturally relevant parallel corpus, enriched\nwith annotations of cultural-specific items. Furthermore, we devise a novel\nevaluation metric to assess the understandability of translations in a\nreference-free manner by GPT-4. We evaluate a variety of neural machine\ntranslation (NMT) and LLM-based MT systems using our dataset. Additionally, we\npropose several prompting strategies for LLMs to incorporate external and\ninternal cultural knowledge into the translation process. Our results\ndemonstrate that eliciting explanations can significantly enhance the\nunderstandability of cultural-specific entities, especially those without\nwell-known translations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14328v2.pdf"
    },
    {
        "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
        "authors": [
            "Da Yin",
            "Xiao Liu",
            "Fan Yin",
            "Ming Zhong",
            "Hritik Bansal",
            "Jiawei Han",
            "Kai-Wei Chang"
        ],
        "published": "2023-05-23T17:56:26Z",
        "summary": "Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) to comprehend instructions and generate appropriate responses.\nExisting methods either manually annotate or employ LLM (e.g., GPT-series) to\ngenerate data for instruction tuning. However, they often overlook associating\ninstructions with existing annotated datasets. In this paper, we propose\nDynosaur, a dynamic growth paradigm for the automatic curation of\ninstruction-tuning data. Based on the metadata of existing datasets, we use\nLLMs to automatically construct instruction-tuning data by identifying relevant\ndata fields and generating appropriate instructions.\n  By leveraging the existing annotated datasets, Dynosaur offers several\nadvantages: 1) it reduces the API cost for generating instructions (e.g., it\ncosts less than $12 USD by calling GPT-3.5-turbo for generating 800K\ninstruction tuning samples; 2) it provides high-quality data for instruction\ntuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform\nwith comparable data sizes); and 3) it supports the continuous improvement of\nmodels by generating instruction-tuning data when a new annotated dataset\nbecomes available. We further investigate a continual learning scheme for\nlearning with the ever-growing instruction-tuning dataset, and demonstrate that\nreplaying tasks with diverse instruction embeddings not only helps mitigate\nforgetting issues but generalizes to unseen tasks better.\n  Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
        "pdf_link": "https://arxiv.org/pdf/2305.14327v2.pdf"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
        "authors": [
            "Yilun Du",
            "Shuang Li",
            "Antonio Torralba",
            "Joshua B. Tenenbaum",
            "Igor Mordatch"
        ],
        "published": "2023-05-23T17:55:11Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "pdf_link": "https://arxiv.org/pdf/2305.14325v1.pdf"
    },
    {
        "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
        "authors": [
            "Zhipeng Chen",
            "Kun Zhou",
            "Beichen Zhang",
            "Zheng Gong",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-23T17:54:33Z",
        "summary": "Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought\nreasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model\nthe chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the knowledge about tools,\ntasks, and reasoning format, and propose an iterative tool-augmented reasoning\nstep to perform step-by-step tool-augmented reasoning. The experiment results\non two complex reasoning datasets (MATH and HotpotQA) have shown the\neffectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative\nimprovement over the state-of-the-art baseline. Our code and data are available\nat: \\url{https://github.com/RUCAIBOX/ChatCoT}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14323v3.pdf"
    },
    {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-05-23T17:53:38Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.",
        "pdf_link": "https://arxiv.org/pdf/2305.14322v1.pdf"
    },
    {
        "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
        "authors": [
            "Cheng Qian",
            "Chi Han",
            "Yi R. Fung",
            "Yujia Qin",
            "Zhiyuan Liu",
            "Heng Ji"
        ],
        "published": "2023-05-23T17:51:52Z",
        "summary": "Large Language Models (LLMs) have made significant progress in utilizing\ntools, but their ability is limited by API availability and the instability of\nimplicit reasoning, particularly when both planning and execution are involved.\nTo overcome these limitations, we propose CREATOR, a novel framework that\nenables LLMs to create their own tools using documentation and code\nrealization. CREATOR disentangles abstract tool creation and concrete decision\nexecution, resulting in improved performance. We evaluate CREATOR on MATH and\nTabMWP benchmarks, respectively consisting of challenging math competition\nproblems and diverse tabular contents. Remarkably, CREATOR outperforms existing\nchain-of-thought, program-of-thought, and tool-using baselines. Additionally,\nwe introduce the Creation Challenge dataset, featuring 2K diverse questions, to\nemphasize the necessity and benefits of LLMs' tool creation ability. Further\nresearch demonstrates that leveraging LLMs as tool creators facilitates\nknowledge transfer, and LLMs exhibit varying levels of tool creation abilities,\nenabling them to adapt to diverse situations. The tool creation ability\nrevolutionizes the LLM's problem-solving paradigm, driving us closer to the\nnext frontier of artificial intelligence. All the codes and data are released.",
        "pdf_link": "https://arxiv.org/pdf/2305.14318v2.pdf"
    },
    {
        "title": "Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science",
        "authors": [
            "Yida Mu",
            "Ben P. Wu",
            "William Thorne",
            "Ambrose Robinson",
            "Nikolaos Aletras",
            "Carolina Scarton",
            "Kalina Bontcheva",
            "Xingyi Song"
        ],
        "published": "2023-05-23T17:48:21Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.",
        "pdf_link": "https://arxiv.org/pdf/2305.14310v3.pdf"
    },
    {
        "title": "QTSumm: Query-Focused Summarization over Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Zhenting Qi",
            "Linyong Nan",
            "Boyu Mi",
            "Yixin Liu",
            "Weijin Zou",
            "Simeng Han",
            "Ruizhe Chen",
            "Xiangru Tang",
            "Yumo Xu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-05-23T17:43:51Z",
        "summary": "People primarily consult tables to conduct data analysis or answer specific\nquestions. Text generation systems that can provide accurate table summaries\ntailored to users' information needs can facilitate more efficient access to\nrelevant data insights. Motivated by this, we define a new query-focused table\nsummarization task, where text generation models have to perform human-like\nreasoning and analysis over the given table to generate a tailored summary. We\nintroduce a new benchmark named QTSumm for this task, which contains 7,111\nhuman-annotated query-summary pairs over 2,934 tables covering diverse topics.\nWe investigate a set of strong baselines on QTSumm, including text generation,\ntable-to-text generation, and large language models. Experimental results and\nmanual analysis reveal that the new task presents significant challenges in\ntable-to-text generation for future research. Moreover, we propose a new\napproach named ReFactor, to retrieve and reason over query-relevant information\nfrom tabular data to generate several natural language facts. Experimental\nresults demonstrate that ReFactor can bring improvements to baselines by\nconcatenating the generated facts to the model input. Our data and code are\npublicly available at https://github.com/yale-nlp/QTSumm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14303v2.pdf"
    },
    {
        "title": "Evaluation of African American Language Bias in Natural Language Generation",
        "authors": [
            "Nicholas Deas",
            "Jessi Grieser",
            "Shana Kleiner",
            "Desmond Patton",
            "Elsbeth Turcan",
            "Kathleen McKeown"
        ],
        "published": "2023-05-23T17:34:37Z",
        "summary": "We evaluate how well LLMs understand African American Language (AAL) in\ncomparison to their performance on White Mainstream English (WME), the\nencouraged \"standard\" form of English taught in American classrooms. We measure\nLLM performance using automatic metrics and human judgments for two tasks: a\ncounterpart generation task, where a model generates AAL (or WME) given WME (or\nAAL), and a masked span prediction (MSP) task, where models predict a phrase\nthat was removed from their input. Our contributions include: (1) evaluation of\nsix pre-trained, large language models on the two language generation tasks;\n(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop\nlyrics, focus groups, and linguistic interviews) with human-annotated\ncounterparts in WME; and (3) documentation of model performance gaps that\nsuggest bias and identification of trends in lack of understanding of AAL\nfeatures.",
        "pdf_link": "https://arxiv.org/pdf/2305.14291v2.pdf"
    },
    {
        "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance",
        "authors": [
            "Chenxi Whitehouse",
            "Monojit Choudhury",
            "Alham Fikri Aji"
        ],
        "published": "2023-05-23T17:33:27Z",
        "summary": "This paper explores the potential of leveraging Large Language Models (LLMs)\nfor data augmentation in multilingual commonsense reasoning datasets where the\navailable training data is extremely limited. To achieve this, we utilise\nseveral LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment\nthree datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate\nthe effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR,\nusing the synthesised data. We compare the performance of training with data\ngenerated in English and target languages, as well as translated\nEnglish-generated data, revealing the overall advantages of incorporating data\ngenerated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best\ncase. Furthermore, we conduct a human evaluation by asking native speakers to\nassess the naturalness and logical coherence of the generated examples across\ndifferent languages. The results of the evaluation indicate that LLMs such as\nChatGPT and GPT-4 excel at producing natural and coherent text in most\nlanguages, however, they struggle to generate meaningful text in certain\nlanguages like Tamil. We also observe that ChatGPT falls short in generating\nplausible alternatives compared to the original dataset, whereas examples from\nGPT-4 exhibit competitive logical consistency.",
        "pdf_link": "https://arxiv.org/pdf/2305.14288v2.pdf"
    },
    {
        "title": "INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback",
        "authors": [
            "Wenda Xu",
            "Danqing Wang",
            "Liangming Pan",
            "Zhenqiao Song",
            "Markus Freitag",
            "William Yang Wang",
            "Lei Li"
        ],
        "published": "2023-05-23T17:27:22Z",
        "summary": "Automatically evaluating the quality of language generation is critical.\nAlthough recent learned metrics show high correlation with human judgement,\nthese metrics can not explain their verdict or associate the scores with\ndefects in generated text. To address this limitation, we present\nInstructScore, an explainable evaluation metric for text generation. By\nharnessing both explicit human instruction and the implicit knowledge of GPT-4,\nwe fine-tune a text evaluation metric based on LLaMA, producing both a score\nfor generated text and a human readable diagnostic report. We evaluate\nInstructScore on a variety of generation tasks, including translation,\ncaptioning, data-to-text and commonsense generation. Experiments show that our\n7B model surpasses all other unsupervised metrics, including those based on\n175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct\nsupervision from human-rated data, achieves performance levels on par with\nstate-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
        "pdf_link": "https://arxiv.org/pdf/2305.14282v3.pdf"
    },
    {
        "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
        "authors": [
            "Angelica Chen",
            "Jason Phang",
            "Alicia Parrish",
            "Vishakh Padmakumar",
            "Chen Zhao",
            "Samuel R. Bowman",
            "Kyunghyun Cho"
        ],
        "published": "2023-05-23T17:25:59Z",
        "summary": "Large language models (LLMs) have achieved widespread success on a variety of\nin-context few-shot tasks, but this success is typically evaluated via\ncorrectness rather than consistency. We argue that self-consistency is an\nimportant criteria for valid multi-step reasoning in tasks where the solution\nis composed of the answers to multiple sub-steps. We propose two types of\nself-consistency that are particularly important for multi-step reasoning --\nhypothetical consistency (a model's ability to predict what its output would be\nin a hypothetical other context) and compositional consistency (consistency of\na model's final outputs when intermediate sub-steps are replaced with the\nmodel's outputs for those steps). We demonstrate that multiple variants of the\nGPT-3/-4 models exhibit poor consistency rates across both types of consistency\non a variety of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14279v4.pdf"
    },
    {
        "title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
        "authors": [
            "Qingyun Wang",
            "Doug Downey",
            "Heng Ji",
            "Tom Hope"
        ],
        "published": "2023-05-23T17:12:08Z",
        "summary": "We explore and enhance the ability of neural language models to generate\nnovel scientific directions grounded in literature. Work on literature-based\nhypothesis generation has traditionally focused on binary link prediction --\nseverely limiting the expressivity of hypotheses. This line of work also does\nnot focus on optimizing novelty. We take a dramatic departure with a novel\nsetting in which models use as input background contexts (e.g., problems,\nexperimental settings, goals), and output natural language ideas grounded in\nliterature. We present SciMON, a modeling framework that uses retrieval of\n\"inspirations\" from past scientific papers, and explicitly optimizes for\nnovelty by iteratively comparing to prior papers and updating idea suggestions\nuntil sufficient novelty is achieved. Comprehensive evaluations reveal that\nGPT-4 tends to generate ideas with overall low technical depth and novelty,\nwhile our methods partially mitigate this issue. Our work represents a first\nstep toward evaluating and developing language models that generate new ideas\nderived from the scientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2305.14259v5.pdf"
    },
    {
        "title": "Hierarchical Prompting Assists Large Language Model on Web Navigation",
        "authors": [
            "Abishek Sridhar",
            "Robert Lo",
            "Frank F. Xu",
            "Hao Zhu",
            "Shuyan Zhou"
        ],
        "published": "2023-05-23T17:10:39Z",
        "summary": "Large language models (LLMs) struggle on processing complicated observations\nin interactive decision making tasks. To alleviate this issue, we propose a\nsimple hierarchical prompting approach. Diverging from previous prompting\napproaches that always put the full observation (e.g. a web page) to the\nprompt, we propose to first construct an action-aware observation which is more\ncondensed and relevant with a dedicated SUMMARIZER prompt. The ACTOR prompt\nthen predicts the next action based on the summarized observation. While our\nmethod has broad applicability, we particularly demonstrate its efficacy in the\ncomplex domain of web navigation where a full observation often contains\nredundant and irrelevant information. Our approach outperforms the previous\nstate-of-the-art prompting mechanics by 6.2% on task success rate,\ndemonstrating its potential on interactive decision making tasks with long\nobservation traces.",
        "pdf_link": "https://arxiv.org/pdf/2305.14257v3.pdf"
    },
    {
        "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
        "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Wei Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-23T17:06:00Z",
        "summary": "Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI -- and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via `pip install factscore`.",
        "pdf_link": "https://arxiv.org/pdf/2305.14251v2.pdf"
    },
    {
        "title": "Language Models with Rationality",
        "authors": [
            "Nora Kassner",
            "Oyvind Tafjord",
            "Ashish Sabharwal",
            "Kyle Richardson",
            "Hinrich Schuetze",
            "Peter Clark"
        ],
        "published": "2023-05-23T17:04:25Z",
        "summary": "While large language models (LLMs) are proficient at question-answering (QA),\nit is not always clear how (or even if) an answer follows from their latent\n\"beliefs\". This lack of interpretability is a growing impediment to widespread\nuse of LLMs. To address this, our goals are to make model beliefs and their\ninferential relationships explicit, and to resolve inconsistencies that may\nexist, so that answers are supported by interpretable chains of reasoning drawn\nfrom a consistent network of beliefs. Our approach, which we call REFLEX, is to\nadd a rational, self-reflecting layer on top of the LLM. First, given a\nquestion, we construct a belief graph using a backward-chaining process to\nmaterialize relevant model beliefs (including beliefs about answer candidates)\nand their inferential relationships. Second, we identify and minimize\ncontradictions in that graph using a formal constraint reasoner. We find that\nREFLEX significantly improves consistency (by 8%-11% absolute) without harming\noverall answer accuracy, resulting in answers supported by faithful chains of\nreasoning drawn from a more consistent belief system. This suggests a new style\nof system architecture in which an LLM extended with a rational layer can\nprovide an interpretable window into system beliefs, add a systematic reasoning\ncapability, and repair latent inconsistencies present in the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.14250v2.pdf"
    },
    {
        "title": "On Learning to Summarize with Large Language Models as References",
        "authors": [
            "Yixin Liu",
            "Kejian Shi",
            "Katherine S He",
            "Longtian Ye",
            "Alexander R. Fabbri",
            "Pengfei Liu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-05-23T16:56:04Z",
        "summary": "Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we investigate a new learning\nsetting of text summarization models that considers the LLMs as the reference\nor the gold-standard oracle on these datasets. To examine the standard\npractices that are aligned with this new learning setting, we investigate two\nLLM-based summary quality evaluation methods for model training and adopt a\ncontrastive learning training method to leverage the LLM-guided learning\nsignals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate\nthat smaller summarization models can achieve similar performance as LLMs under\nLLM-based evaluation. However, we found that the smaller models can not yet\nreach LLM-level performance under human evaluation despite promising\nimprovements brought by our proposed training methods. Meanwhile, we perform a\nmeta-analysis on this new learning setting that reveals a discrepancy between\nhuman and LLM-based evaluation, highlighting the benefits and risks of this\nLLM-as-reference setting we investigated.",
        "pdf_link": "https://arxiv.org/pdf/2305.14239v2.pdf"
    },
    {
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "authors": [
            "Ruochen Zhang",
            "Samuel Cahyawijaya",
            "Jan Christian Blaise Cruz",
            "Genta Indra Winata",
            "Alham Fikri Aji"
        ],
        "published": "2023-05-23T16:50:48Z",
        "summary": "Multilingual Large Language Models (LLMs) have recently shown great\ncapabilities in a wide range of tasks, exhibiting state-of-the-art performance\nthrough zero-shot or few-shot prompting methods. While there have been\nextensive studies on their abilities in monolingual tasks, the investigation of\ntheir potential in the context of code-switching (CSW), the practice of\nalternating languages within an utterance, remains relatively uncharted. In\nthis paper, we provide a comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their performance across four tasks: sentiment\nanalysis, machine translation, summarization and word-level language\nidentification. Our results indicate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero or few-shot prompting, they\nstill underperform in comparison to fine-tuned models of much smaller scales.\nWe argue that current \"multilingualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling for future research to bridge\nthis discrepancy.",
        "pdf_link": "https://arxiv.org/pdf/2305.14235v2.pdf"
    },
    {
        "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
        "authors": [
            "Kung-Hsiang Huang",
            "Hou Pong Chan",
            "Kathleen McKeown",
            "Heng Ji"
        ],
        "published": "2023-05-23T16:40:07Z",
        "summary": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.",
        "pdf_link": "https://arxiv.org/pdf/2305.14225v1.pdf"
    },
    {
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "authors": [
            "Xinyu Zhu",
            "Cheng Yang",
            "Bei Chen",
            "Siheng Li",
            "Jian-Guang Lou",
            "Yujiu Yang"
        ],
        "published": "2023-05-23T16:35:16Z",
        "summary": "Question answering plays a pivotal role in human daily life because it\ninvolves our acquisition of knowledge about the world. However, due to the\ndynamic and ever-changing nature of real-world facts, the answer can be\ncompletely different when the time constraint in the question changes.\nRecently, Large Language Models (LLMs) have shown remarkable intelligence in\nquestion answering, while our experiments reveal that the aforementioned\nproblems still pose a significant challenge to existing LLMs. This can be\nattributed to the LLMs' inability to perform rigorous reasoning based on\nsurface-level text semantics. To overcome this limitation, rather than\nrequiring LLMs to directly answer the question, we propose a novel approach\nwhere we reframe the $\\textbf{Q}$uestion $\\textbf{A}$nswering task\n$\\textbf{a}$s $\\textbf{P}$rogramming ($\\textbf{QAaP}$). Concretely, by\nleveraging modern LLMs' superior capability in understanding both natural\nlanguage and programming language, we endeavor to harness LLMs to represent\ndiversely expressed text as well-structured code and select the best matching\nanswer from multiple candidates through programming. We evaluate our QAaP\nframework on several time-sensitive question answering datasets and achieve\ndecent improvement, up to $14.5$% over strong baselines. Our codes and data are\navailable at https://github.com/TianHongZXY/qaap",
        "pdf_link": "https://arxiv.org/pdf/2305.14221v3.pdf"
    },
    {
        "title": "Exploring Chain-of-Thought Style Prompting for Text-to-SQL",
        "authors": [
            "Chang-You Tai",
            "Ziru Chen",
            "Tianshu Zhang",
            "Xiang Deng",
            "Huan Sun"
        ],
        "published": "2023-05-23T16:32:36Z",
        "summary": "In-context learning with large language models (LLMs) has recently caught\nincreasing attention due to its superior few-shot performance on various tasks.\nHowever, its performance on text-to-SQL parsing still has much room for\nimprovement. In this paper, we hypothesize that a crucial aspect of LLMs to\nimprove for text-to-SQL parsing is their multi-step reasoning ability. Thus, we\nsystematically study how to enhance LLMs' reasoning ability through chain of\nthought (CoT) style prompting, including the original chain-of-thought\nprompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023).\nOur experiments demonstrate that iterative prompting as in Zhou et al. (2023)\nmay be unnecessary for text-to-SQL parsing, and using detailed reasoning steps\ntends to have more error propagation issues. Based on these findings, we\npropose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2\nand 6.5 point absolute gains on the Spider development set and the Spider\nRealistic set, respectively, compared to the standard prompting method without\nreasoning steps; 2.4 and 1.5 point absolute gains, compared to the\nleast-to-most prompting method.",
        "pdf_link": "https://arxiv.org/pdf/2305.14215v2.pdf"
    },
    {
        "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
        "authors": [
            "Benjamin Minixhofer",
            "Jonas Pfeiffer",
            "Ivan Vuli\u0107"
        ],
        "published": "2023-05-23T16:32:27Z",
        "summary": "While many languages possess processes of joining two or more words to create\ncompound words, previous studies have been typically limited only to languages\nwith excessively productive compound formation (e.g., German, Dutch) and there\nis no public dataset containing compound and non-compound words across a large\nnumber of languages. In this work, we systematically study decompounding, the\ntask of splitting compound words into their constituents, at a wide scale. We\nfirst address the data gap by introducing a dataset of 255k compound and\nnon-compound words across 56 diverse languages obtained from Wiktionary. We\nthen use this dataset to evaluate an array of Large Language Models (LLMs) on\nthe decompounding task. We find that LLMs perform poorly, especially on words\nwhich are tokenized unfavorably by subword tokenization. We thus introduce a\nnovel methodology to train dedicated models for decompounding. The proposed\ntwo-stage procedure relies on a fully self-supervised objective in the first\nstage, while the second, supervised learning stage optionally fine-tunes the\nmodel on the annotated Wiktionary data. Our self-supervised models outperform\nthe prior best unsupervised decompounding models by 13.9% accuracy on average.\nOur fine-tuned models outperform all prior (language-specific) decompounding\ntools. Furthermore, we use our models to leverage decompounding during the\ncreation of a subword tokenizer, which we refer to as CompoundPiece.\nCompoundPiece tokenizes compound words more favorably on average, leading to\nimproved performance on decompounding over an otherwise equivalent model using\nSentencePiece tokenization.",
        "pdf_link": "https://arxiv.org/pdf/2305.14214v2.pdf"
    },
    {
        "title": "Domain Private Transformers for Multi-Domain Dialog Systems",
        "authors": [
            "Anmol Kabra",
            "Ethan R. Elenberg"
        ],
        "published": "2023-05-23T16:27:12Z",
        "summary": "Large, general purpose language models have demonstrated impressive\nperformance across many different conversational domains. While multi-domain\nlanguage models achieve low overall perplexity, their outputs are not\nguaranteed to stay within the domain of a given input prompt. This paper\nproposes domain privacy as a novel way to quantify how likely a conditional\nlanguage model will leak across domains. We also develop policy functions based\non token-level domain classification, and propose an efficient fine-tuning\nmethod to improve the trained model's domain privacy. Experiments on membership\ninference attacks show that our proposed method has comparable resiliency to\nmethods adapted from recent literature on differentially private language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.14208v2.pdf"
    },
    {
        "title": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",
        "authors": [
            "Silei Xu",
            "Shicheng Liu",
            "Theo Culhane",
            "Elizaveta Pertseva",
            "Meng-Hsi Wu",
            "Sina J. Semnani",
            "Monica S. Lam"
        ],
        "published": "2023-05-23T16:20:43Z",
        "summary": "While large language models (LLMs) can answer many questions correctly, they\ncan also hallucinate and give wrong answers. Wikidata, with its over 12 billion\nfacts, can be used to ground LLMs to improve their factuality. This paper\npresents WikiWebQuestions, a high-quality question answering benchmark for\nWikidata. Ported over from WebQuestions for Freebase, it consists of real-world\ndata with SPARQL annotation. This paper presents a few-shot\nsequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the\nunique domain and property names instead of their IDs. We train the parser to\nuse either the results from an entity linker or mentions in the query. We\nfine-tune LLaMA by adding the few-shot training data to that used to fine-tune\nAlpaca. Our experimental results demonstrate the effectiveness of this\nmethodology, establishing a strong baseline of 76% and 65% answer accuracy in\nthe dev and test sets of WikiWebQuestions, respectively. By pairing our\nsemantic parser with GPT-3, we combine verifiable results with qualified GPT-3\nguesses to provide useful answers to 96% of the questions in dev. We also show\nthat our method outperforms the state-of-the-art for the QALD-7 Wikidata\ndataset by 3.6% in F1 score.",
        "pdf_link": "https://arxiv.org/pdf/2305.14202v2.pdf"
    },
    {
        "title": "HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations",
        "authors": [
            "Anthony Sicilia",
            "Jennifer C. Gates",
            "Malihe Alikhani"
        ],
        "published": "2023-05-23T16:15:24Z",
        "summary": "While demographic factors like age and gender change the way people talk, and\nin particular, the way people talk to machines, there is little investigation\ninto how large pre-trained language models (LMs) can adapt to these changes. To\nremedy this gap, we consider how demographic factors in LM language skills can\nbe measured to determine compatibility with a target demographic. We suggest\nclinical techniques from Speech Language Pathology, which has norms for\nacquisition of language skills in humans. We conduct evaluation with a domain\nexpert (i.e., a clinically licensed speech language pathologist), and also\npropose automated techniques to complement clinical evaluation at scale.\nEmpirically, we focus on age, finding LM capability varies widely depending on\ntask: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasks\nrequiring inference, and simultaneously, outperforms a typical 21 year old at\nmemorization. GPT-3.5 also has trouble with social language use, exhibiting\nless than 50% of the tested pragmatic skills. Findings affirm the importance of\nconsidering demographic alignment and conversational goals when using LMs as\npublic-facing tools. Code, data, and a package will be available.",
        "pdf_link": "https://arxiv.org/pdf/2305.14195v3.pdf"
    },
    {
        "title": "In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models",
        "authors": [
            "Afra Amini",
            "Massimiliano Ciaramita"
        ],
        "published": "2023-05-23T15:43:04Z",
        "summary": "Large language models are able to learn new tasks in context, where they are\nprovided with instructions and a few annotated examples. However, the\neffectiveness of in-context learning is dependent on the provided context, and\nthe performance on a downstream task can vary considerably, depending on the\ninstruction. Importantly, such dependency on the context can surface in\nunpredictable ways, e.g., a seemingly more informative instruction might lead\nto a worse performance. In this paper, we propose an alternative approach,\nwhich we term In-Context Probing (ICP). Similar to in-context learning, we\ncontextualize the representation of the input with an instruction, but instead\nof decoding the output prediction, we probe the contextualized representation\nto predict the label. Through a series of experiments on a diverse set of\nclassification tasks, we show that in-context probing is significantly more\nrobust to changes in instructions. We further show that ICP performs\ncompetitive or superior to finetuning and can be particularly helpful to build\nclassifiers on top of smaller models, with less than a hundred training\nexamples.",
        "pdf_link": "https://arxiv.org/pdf/2305.14171v3.pdf"
    },
    {
        "title": "DetGPT: Detect What You Need via Reasoning",
        "authors": [
            "Renjie Pi",
            "Jiahui Gao",
            "Shizhe Diao",
            "Rui Pan",
            "Hanze Dong",
            "Jipeng Zhang",
            "Lewei Yao",
            "Jianhua Han",
            "Hang Xu",
            "Lingpeng Kong",
            "Tong Zhang"
        ],
        "published": "2023-05-23T15:37:28Z",
        "summary": "In recent years, the field of computer vision has seen significant\nadvancements thanks to the development of large language models (LLMs). These\nmodels have enabled more effective and sophisticated interactions between\nhumans and machines, paving the way for novel techniques that blur the lines\nbetween human and machine intelligence. In this paper, we introduce a new\nparadigm for object detection that we call reasoning-based object detection.\nUnlike conventional object detection methods that rely on specific object\nnames, our approach enables users to interact with the system using natural\nlanguage instructions, allowing for a higher level of interactivity. Our\nproposed method, called DetGPT, leverages state-of-the-art multi-modal models\nand open-vocabulary object detectors to perform reasoning within the context of\nthe user's instructions and the visual scene. This enables DetGPT to\nautomatically locate the object of interest based on the user's expressed\ndesires, even if the object is not explicitly mentioned. For instance, if a\nuser expresses a desire for a cold beverage, DetGPT can analyze the image,\nidentify a fridge, and use its knowledge of typical fridge contents to locate\nthe beverage. This flexibility makes our system applicable across a wide range\nof fields, from robotics and automation to autonomous driving. Overall, our\nproposed paradigm and DetGPT demonstrate the potential for more sophisticated\nand intuitive interactions between humans and machines. We hope that our\nproposed paradigm and approach will provide inspiration to the community and\nopen the door to more interative and versatile object detection systems. Our\nproject page is launched at detgpt.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2305.14167v2.pdf"
    },
    {
        "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
        "authors": [
            "Jeonghoon Kim",
            "Jung Hyun Lee",
            "Sungdong Kim",
            "Joonsuk Park",
            "Kang Min Yoo",
            "Se Jung Kwon",
            "Dongsoo Lee"
        ],
        "published": "2023-05-23T15:20:01Z",
        "summary": "Large language models (LLMs) face the challenges in fine-tuning and\ndeployment due to their high memory demands and computational costs. While\nparameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage\nof the optimizer state during fine-tuning, the inherent size of pre-trained LLM\nweights continues to be a pressing concern. Even though quantization techniques\nare widely proposed to ease memory demands and accelerate LLM inference, most\nof these techniques are geared towards the deployment phase. To bridge this\ngap, this paper presents Parameter-Efficient and Quantization-aware Adaptation\n(PEQA) - a simple yet effective method that combines the advantages of PEFT\nwith quantized LLMs. By updating solely the quantization scales, PEQA can be\ndirectly applied to quantized LLMs, ensuring seamless task transitions.\nParallel to existing PEFT methods, PEQA significantly reduces the memory\noverhead associated with the optimizer state. Furthermore, it leverages the\nadvantages of quantization to substantially reduce model sizes. Even after\nfine-tuning, the quantization structure of a PEQA-tuned LLM remains intact,\nallowing for accelerated inference on the deployment stage. We employ\nPEQA-tuning for task-specific adaptation on LLMs with up to 65 billion\nparameters. To assess the logical reasoning and language comprehension of\nPEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction\ndataset. Our results show that even when LLMs are quantized to below 4-bit\nprecision, their capabilities in language modeling, few-shot in-context\nlearning, and comprehension can be resiliently restored to (or even improved\nover) their full-precision original performances with PEQA.",
        "pdf_link": "https://arxiv.org/pdf/2305.14152v2.pdf"
    },
    {
        "title": "GrACE: Generation using Associated Code Edits",
        "authors": [
            "Priyanshu Gupta",
            "Avishree Khare",
            "Yasharth Bajpai",
            "Saikat Chakraborty",
            "Sumit Gulwani",
            "Aditya Kanade",
            "Arjun Radhakrishna",
            "Gustavo Soares",
            "Ashish Tiwari"
        ],
        "published": "2023-05-23T14:55:44Z",
        "summary": "Developers expend a significant amount of time in editing code for a variety\nof reasons such as bug fixing or adding new features. Designing effective\nmethods to predict code edits has been an active yet challenging area of\nresearch due to the diversity of code edits and the difficulty of capturing the\ndeveloper intent. In this work, we address these challenges by endowing\npre-trained large language models (LLMs) of code with the knowledge of prior,\nrelevant edits. The generative capability of the LLMs helps address the\ndiversity in code changes and conditioning code generation on prior edits helps\ncapture the latent developer intent. We evaluate two well-known LLMs, Codex and\nCodeT5, in zero-shot and fine-tuning settings respectively. In our experiments\nwith two datasets, the knowledge of prior edits boosts the performance of the\nLLMs significantly and enables them to generate 29% and 54% more correctly\nedited code in top-1 suggestions relative to the current state-of-the-art\nsymbolic and neural approaches, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.14129v3.pdf"
    },
    {
        "title": "Dr.ICL: Demonstration-Retrieved In-context Learning",
        "authors": [
            "Man Luo",
            "Xin Xu",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Mehran Kazemi",
            "Chitta Baral",
            "Vaiva Imbrasaite",
            "Vincent Y Zhao"
        ],
        "published": "2023-05-23T14:55:25Z",
        "summary": "In-context learning (ICL), teaching a large language model (LLM) to perform a\ntask with few-shot demonstrations rather than adjusting the model parameters,\nhas emerged as a strong paradigm for using LLMs. While early studies primarily\nused a fixed or random set of demonstrations for all test queries, recent\nresearch suggests that retrieving semantically similar demonstrations to the\ninput from a pool of available demonstrations results in better performance.\nThis work expands the applicability of retrieval-based ICL approaches by\ndemonstrating that even simple word-overlap similarity measures such as BM25\noutperform randomly selected demonstrations. Furthermore, we extend the success\nof retrieval-based ICL to instruction-finetuned LLMs as well as\nChain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that\nalthough a model has already seen the training data at training time,\nretrieving demonstrations from the training data at test time yields better\nresults compared to using no demonstrations or random demonstrations. Last but\nnot least, we train a task-specific demonstration retriever that outperforms\noff-the-shelf retrievers.",
        "pdf_link": "https://arxiv.org/pdf/2305.14128v1.pdf"
    },
    {
        "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hanjun Dai",
            "Sercan O. Arik",
            "Tomas Pfister"
        ],
        "published": "2023-05-23T14:27:16Z",
        "summary": "Modern large language models (LLMs) have demonstrated impressive capabilities\nat sophisticated tasks, often through step-by-step reasoning similar to humans.\nThis is made possible by their strong few and zero-shot abilities -- they can\neffectively learn from a handful of handcrafted, completed responses\n(\"in-context examples\"), or are prompted to reason spontaneously through\nspecially designed triggers. Nonetheless, some limitations have been observed.\nFirst, performance in the few-shot setting is sensitive to the choice of\nexamples, whose design requires significant human effort. Moreover, given the\ndiverse downstream tasks of LLMs, it may be difficult or laborious to handcraft\nper-task labels. Second, while the zero-shot setting does not require\nhandcrafting, its performance is limited due to the lack of guidance to the\nLLMs. To address these limitations, we propose Consistency-based Self-adaptive\nPrompting (COSP), a novel prompt design method for LLMs. Requiring neither\nhandcrafted responses nor ground-truth labels, COSP selects and builds the set\nof examples from the LLM zero-shot outputs via carefully designed criteria that\ncombine consistency, diversity and repetition. In the zero-shot setting for\nthree different LLMs, we show that using only LLM predictions, COSP improves\nperformance up to 15% compared to zero-shot baselines and matches or exceeds\nfew-shot baselines for a range of reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14106v1.pdf"
    },
    {
        "title": "Revisiting Acceptability Judgements",
        "authors": [
            "Hai Hu",
            "Ziyin Zhang",
            "Weifang Huang",
            "Jackie Yan-Ki Lai",
            "Aini Li",
            "Yina Patterson",
            "Jiahui Huang",
            "Peng Zhang",
            "Chien-Jer Charles Lin",
            "Rui Wang"
        ],
        "published": "2023-05-23T14:16:22Z",
        "summary": "In this work, we revisit linguistic acceptability in the context of large\nlanguage models. We introduce CoLAC - Corpus of Linguistic Acceptability in\nChinese, the first large-scale acceptability dataset for a non-Indo-European\nlanguage. It is verified by native speakers and is the first acceptability\ndataset that comes with two sets of labels: a linguist label and a crowd label.\nOur experiments show that even the largest InstructGPT model performs only at\nchance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much\nbelow supervised models (59.03 MCC) and human (65.11 MCC). Through\ncross-lingual transfer experiments and fine-grained linguistic analysis, we\nprovide detailed analysis of the model predictions and demonstrate for the\nfirst time that knowledge of linguistic acceptability can be transferred across\ntypologically distinct languages, as well as be traced back to pre-training.\nOur dataset is publicly available at\n\\url{https://github.com/huhailinguist/CoLAC}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14091v3.pdf"
    },
    {
        "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
        "authors": [
            "Shiqi Chen",
            "Siyang Gao",
            "Junxian He"
        ],
        "published": "2023-05-23T13:48:32Z",
        "summary": "Detecting factual errors in summaries has been an important and challenging\nsubject in summarization research. Inspired by the emergent ability of large\nlanguage models (LLMs), we explore evaluating factual consistency of summaries\nby directly prompting LLMs. We present a comprehensive empirical study to\nassess the ability of LLMs as factual consistency evaluators, which consists of\n(1) analyzing different LLMs such as the GPT model series and Flan-T5; (2)\ninvestigating a variety of prompting methods including vanilla prompting,\nchain-of-thought prompting, and a sentence-by-sentence prompting method to\ntackle long summaries; and (3) evaluating on diverse summaries generated by\nmultiple summarization systems, ranging from pre-transformer methods to SOTA\npretrained models. Our experiments demonstrate that prompting LLMs is able to\noutperform the previous best factuality systems in all settings, by up to 12.2\nabsolute points in terms of the binary classification accuracy on inconsistency\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2305.14069v2.pdf"
    },
    {
        "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
        "authors": [
            "Seungone Kim",
            "Se June Joo",
            "Doyoung Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Jamin Shin",
            "Minjoon Seo"
        ],
        "published": "2023-05-23T13:14:59Z",
        "summary": "Language models (LMs) with less than 100B parameters are known to perform\npoorly on chain-of-thought (CoT) reasoning in contrast to large LMs when\nsolving unseen tasks. In this work, we aim to equip smaller LMs with the\nstep-by-step reasoning capability by instruction tuning with CoT rationales. In\norder to achieve this goal, we first introduce a new instruction-tuning dataset\ncalled the CoT Collection, which augments the existing Flan Collection\n(including only 9 CoT tasks) with additional 1.84 million rationales across\n1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT\nCollection enables smaller LMs to have better CoT capabilities on unseen tasks.\nOn the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of\n+4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task\naccuracy. Furthermore, we show that instruction tuning with CoT Collection\nallows LMs to possess stronger few-shot learning capabilities on 4\ndomain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and\n+2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until\nthe max length by a +13.98% margin. Our code, the CoT Collection data, and\nmodel checkpoints are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.14045v2.pdf"
    },
    {
        "title": "ChipGPT: How far are we from natural language hardware design",
        "authors": [
            "Kaiyan Chang",
            "Ying Wang",
            "Haimeng Ren",
            "Mengdi Wang",
            "Shengwen Liang",
            "Yinhe Han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "published": "2023-05-23T12:54:02Z",
        "summary": "As large language models (LLMs) like ChatGPT exhibited unprecedented machine\nintelligence, it also shows great performance in assisting hardware engineers\nto realize higher-efficiency logic design via natural language interaction. To\nestimate the potential of the hardware design process assisted by LLMs, this\nwork attempts to demonstrate an automated design environment that explores LLMs\nto generate hardware logic designs from natural language specifications. To\nrealize a more accessible and efficient chip development flow, we present a\nscalable four-stage zero-code logic design framework based on LLMs without\nretraining or finetuning. At first, the demo, ChipGPT, begins by generating\nprompts for the LLM, which then produces initial Verilog programs. Second, an\noutput manager corrects and optimizes these programs before collecting them\ninto the final design space. Eventually, ChipGPT will search through this space\nto select the optimal design under the target metrics. The evaluation sheds\nsome light on whether LLMs can generate correct and complete hardware logic\ndesigns described by natural language for some specifications. It is shown that\nChipGPT improves programmability, and controllability, and shows broader design\noptimization space compared to prior work and native LLMs alone.",
        "pdf_link": "https://arxiv.org/pdf/2305.14019v3.pdf"
    },
    {
        "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions",
        "authors": [
            "Wenhao Yu",
            "Meng Jiang",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023-05-23T12:43:19Z",
        "summary": "Although counterfactual reasoning is a fundamental aspect of intelligence,\nthe lack of large-scale counterfactual open-domain question-answering (QA)\nbenchmarks makes it difficult to evaluate and improve models on this ability.\nTo address this void, we introduce the first such dataset, named IfQA, where\neach question is based on a counterfactual presupposition via an \"if\" clause.\nFor example, if Los Angeles was on the east coast of the U.S., what would be\nthe time difference between Los Angeles and Paris? Such questions require\nmodels to go beyond retrieving direct factual knowledge from the Web: they must\nidentify the right information to retrieve and reason about an imagined\nsituation that may even go against the facts built into their parameters. The\nIfQA dataset contains over 3,800 questions that were annotated annotated by\ncrowdworkers on relevant Wikipedia passages. Empirical analysis reveals that\nthe IfQA dataset is highly challenging for existing open-domain QA methods,\nincluding supervised retrieve-then-read pipeline methods (EM score 36.2), as\nwell as recent few-shot approaches such as chain-of-thought prompting with\nGPT-3 (EM score 27.4). The unique challenges posed by the IfQA benchmark will\npush open-domain QA research on both retrieval and counterfactual reasoning\nfronts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14010v1.pdf"
    },
    {
        "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
        "authors": [
            "Wenhao Yu",
            "Zhihan Zhang",
            "Zhenwen Liang",
            "Meng Jiang",
            "Ashish Sabharwal"
        ],
        "published": "2023-05-23T12:29:44Z",
        "summary": "Large language models (LLMs) exhibit remarkable performance across various\nNLP tasks. However, they often generate incorrect or hallucinated information,\nwhich hinders their practical applicability in real-world scenarios. Human\nfeedback has been shown to effectively enhance the factuality and quality of\ngenerated content, addressing some of these limitations. However, this approach\nis resource-intensive, involving manual input and supervision, which can be\ntime-consuming and expensive. Moreover, it cannot be provided during inference,\nfurther limiting its practical utility in dynamic and interactive applications.\nIn this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs\nby providing automatic retrieval feedback in a plug-and-play framework without\nthe need for expensive fine-tuning. ReFeed first generates initial outputs,\nthen utilizes a retrieval model to acquire relevant information from large\ndocument collections, and finally incorporates the retrieved information into\nthe in-context demonstration for output refinement, thereby addressing the\nlimitations of LLMs in a more efficient and cost-effective manner. Experiments\non four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed\ncould improve over +6.0% under zero-shot setting and +2.5% under few-shot\nsetting, compared to baselines without using retrieval feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.14002v1.pdf"
    },
    {
        "title": "Make a Choice! Knowledge Base Question Answering with In-Context Learning",
        "authors": [
            "Chuanyuan Tan",
            "Yuehe Chen",
            "Wenbiao Shao",
            "Wenliang Chen"
        ],
        "published": "2023-05-23T11:56:03Z",
        "summary": "Question answering over knowledge bases (KBQA) aims to answer factoid\nquestions with a given knowledge base (KB). Due to the large scale of KB,\nannotated data is impossible to cover all fact schemas in KB, which poses a\nchallenge to the generalization ability of methods that require a sufficient\namount of annotated data. Recently, LLMs have shown strong few-shot performance\nin many NLP tasks. We expect LLM can help existing methods improve their\ngeneralization ability, especially in low-resource situations. In this paper,\nwe present McL-KBQA, a framework that incorporates the few-shot ability of LLM\ninto the KBQA method via ICL-based multiple choice and then improves the\neffectiveness of the QA tasks. Experimental results on two KBQA datasets\ndemonstrate the competitive performance of McL-KBQA with strong improvements in\ngeneralization. We expect to explore a new way to QA tasks from KBQA in\nconjunction with LLM, how to generate answers normatively and correctly with\nstrong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.13972v1.pdf"
    },
    {
        "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
        "authors": [
            "Saibo Geng",
            "Martin Josifoski",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023-05-23T11:54:37Z",
        "summary": "Despite their impressive performance, large language models (LMs) still\nstruggle with reliably generating complex output structures when not finetuned\nto follow the required output format exactly. To address this issue,\ngrammar-constrained decoding (GCD) can be used to control the generation of\nLMs, guaranteeing that the output follows a given structure. Most existing GCD\nmethods are, however, limited to specific tasks, such as parsing or code\ngeneration. In this work, we demonstrate that formal grammars can describe the\noutput space for a much wider range of tasks and argue that GCD can serve as a\nunified framework for structured NLP tasks in general. For increased\nflexibility, we introduce input-dependent grammars, which allow the grammar to\ndepend on the input and thus enable the generation of different output\nstructures for different inputs. We then empirically demonstrate the power and\nflexibility of GCD-enhanced LMs on (1) information extraction, (2) entity\ndisambiguation, and (3) constituency parsing. Our results indicate that\ngrammar-constrained LMs substantially outperform unconstrained LMs or even beat\ntask-specific finetuned models. Grammar constraints thus hold great promise for\nharnessing off-the-shelf LMs for a wide range of structured NLP tasks,\nespecially where training data is scarce or finetuning is expensive. Code and\ndata: https://github.com/epfl-dlab/GCD.",
        "pdf_link": "https://arxiv.org/pdf/2305.13971v6.pdf"
    },
    {
        "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Yixin Cao",
            "Jizhi Zhang",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-23T11:30:43Z",
        "summary": "Large Language Model (LLM) has demonstrated significant ability in various\nNatural Language Processing tasks. However, their effectiveness is highly\ndependent on the phrasing of the task prompt, leading to research on automatic\nprompt optimization using labeled task data. We reveal that these prompt\noptimization techniques are vulnerable to distribution shifts such as\nsubpopulation shifts, which are common for LLMs in real-world scenarios such as\ncustomer reviews analysis. In this light, we propose a new problem of robust\nprompt optimization for LLMs against distribution shifts, which requires the\nprompt optimized over the labeled source group can simultaneously generalize to\nan unlabeled target group. To solve this problem, we propose Generalized Prompt\nOptimization framework, which incorporates the unlabeled data from the target\ngroup into prompt optimization. Extensive experimental results demonstrate the\neffectiveness of the proposed framework with significant performance\nimprovement on the target group and comparable performance on the source group.",
        "pdf_link": "https://arxiv.org/pdf/2305.13954v3.pdf"
    },
    {
        "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
        "authors": [
            "Jinyan Su",
            "Terry Yue Zhuo",
            "Di Wang",
            "Preslav Nakov"
        ],
        "published": "2023-05-23T11:18:30Z",
        "summary": "With the rapid progress of large language models (LLMs) and the huge amount\nof text they generated, it becomes more and more impractical to manually\ndistinguish whether a text is machine-generated. Given the growing use of LLMs\nin social media and education, it prompts us to develop methods to detect\nmachine-generated text, preventing malicious usage such as plagiarism,\nmisinformation, and propaganda. Previous work has studied several zero-shot\nmethods, which require no training data. These methods achieve good\nperformance, but there is still a lot of room for improvement. In this paper,\nwe introduce two novel zero-shot methods for detecting machine-generated text\nby leveraging the log rank information. One is called DetectLLM-LRR, which is\nfast and efficient, and the other is called DetectLLM-NPR, which is more\naccurate, but slower due to the need for perturbations. Our experiments on\nthree datasets and seven language models show that our proposed methods improve\nover the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,\nDetectLLM-NPR needs fewer perturbations than previous work to achieve the same\nlevel of performance, which makes it more practical for real-world use. We also\ninvestigate the efficiency--performance trade-off based on users preference on\nthese two measures and we provide intuition for using them in practice\neffectively. We release the data and the code of both methods in\nhttps://github.com/mbzuai-nlp/DetectLLM",
        "pdf_link": "https://arxiv.org/pdf/2306.05540v1.pdf"
    },
    {
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": [
            "Jiacheng Ye",
            "Chengzu Li",
            "Lingpeng Kong",
            "Tao Yu"
        ],
        "published": "2023-05-23T10:44:00Z",
        "summary": "While large language models (LLMs) bring not only performance but also\ncomplexity, recent work has started to turn LLMs into data generators rather\nthan task inferencers, where another affordable task model is trained for\nefficient deployment and inference. However, such an approach has primarily\nbeen applied to natural language tasks and has not yet been explored for\nsymbolic language tasks with complex structured outputs (e.g., semantic parsing\nand code generation). In this paper, we propose SymGen which utilizes LLMs for\ngenerating various annotation-expensive symbolic language data. SymGen consists\nof an informative prompt to steer generation and an agreement-based verifier to\nimprove data correctness. We conduct extensive experiments on six symbolic\nlanguage tasks across various settings. Compared with the LLMs, we demonstrate\nthe 1\\%-sized task model can achieve comparable or better performance, largely\ncutting inference and deployment costs. We also show that generated data with\nonly a few human demonstrations can be as effective as over 10 times the amount\nof human-annotated data when training the task model, saving a considerable\namount of annotation effort. SymGen sheds new light on data generation for\ncomplex tasks, and we release the code at\n\\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.",
        "pdf_link": "https://arxiv.org/pdf/2305.13917v1.pdf"
    },
    {
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "authors": [
            "Xuekai Zhu",
            "Biqing Qi",
            "Kaiyan Zhang",
            "Xinwei Long",
            "Zhouhan Lin",
            "Bowen Zhou"
        ],
        "published": "2023-05-23T10:11:56Z",
        "summary": "While large language models (LLMs) excel in various natural language\nprocessing tasks, their huge size and the inaccessibility of parameters present\nchallenges for practical deployment. Previous studies try to distill\ntask-specific ability from LLMs to smaller models, using data synthesis and\nchain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains\nfaulty reasoning, which deteriorates the quality of distillation, especially in\nreasoning capabilities. In this work, we propose Program-aided Distillation\n(PaD), which introduces reasoning programs to suppress the errors in distilled\ndata, and thus achieves better distillation quality for reasoning tasks. In\nPaD, we utilize the reasoning program to substitute the CoT, allowing automated\nerror checking of synthetic data. Further, through error injecting and further\ntraining, the small distilling model could iteratively self-refine the\nreasoning. Moreover, we conduct a step-wise beam search by step-by-step\nverifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic\nreasoning, symbolic reasoning, and general ability. Experimental results\ndemonstrate that smaller models using PaD can not only outperform certain\nLLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines\nwith a significantly smaller scale of parameters and data. The source code is\npublicly available at https://github.com/Xuekai-Zhu/pad.",
        "pdf_link": "https://arxiv.org/pdf/2305.13888v2.pdf"
    },
    {
        "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models",
        "authors": [
            "Leonardo Ranaldi",
            "Elena Sofia Ruzzetti",
            "Davide Venditti",
            "Dario Onorati",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2023-05-23T09:35:37Z",
        "summary": "Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training\nare emerging as the next big revolution in natural language processing and\nunderstanding. These CtB-LLMs are democratizing access to trainable Very\nLarge-Language Models (VLLMs) and, thus, may represent the building blocks of\nmany NLP systems solving downstream tasks. Hence, a little or a large bias in\nCtB-LLMs may cause huge harm. In this paper, we performed a large investigation\nof the bias of three families of CtB-LLMs, and we showed that debiasing\ntechniques are effective and usable. Indeed, according to current tests, the\nLLaMA and the OPT families have an important bias in gender, race, religion,\nand profession. In contrast to the analysis for other LLMs, we discovered that\nbias depends not on the number of parameters but on the perplexity. Finally,\nthe debiasing of OPT using LoRA reduces bias up to 4.12 points in the\nnormalized stereotype score.",
        "pdf_link": "https://arxiv.org/pdf/2305.13862v2.pdf"
    },
    {
        "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Zhengzi Xu",
            "Yuekang Li",
            "Yaowen Zheng",
            "Ying Zhang",
            "Lida Zhao",
            "Tianwei Zhang",
            "Kailong Wang",
            "Yang Liu"
        ],
        "published": "2023-05-23T09:33:38Z",
        "summary": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.",
        "pdf_link": "https://arxiv.org/pdf/2305.13860v2.pdf"
    },
    {
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": [
            "Danqing Wang",
            "Lei Li"
        ],
        "published": "2023-05-23T08:51:08Z",
        "summary": "Large language models (LLMs) have demonstrated their potential to refine\ntheir generation based on their own feedback. However, the feedback from LLM\nitself is often inaccurate, thereby limiting its benefits. In this paper, we\npropose Study Assistant for Large LAnguage Model (SALAM), a novel framework\nwith an auxiliary agent to assist the main LLM in learning from mistakes\nthrough interactive cooperation. In the gathering phase, the student assistant\nagent probes the main LLM, analyzes its errors, and collects the interaction in\na mistake memory. During the examination phase, the study assistant provides\nguidelines by retrieving relevant cases to help the main LLM anticipate and\navoid similar errors. We first investigate the effectiveness of a general study\nassistant and then customize it to provide LLM-specific guidance through\nimitation learning from successful guidance experiences. Our experiments on\nthree LLMs using two challenging frameworks demonstrate that SALAM can\nsignificantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on\nBBQ.",
        "pdf_link": "https://arxiv.org/pdf/2305.13829v3.pdf"
    },
    {
        "title": "\"Is the Pope Catholic?\" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",
        "authors": [
            "Zae Myung Kim",
            "David E. Taylor",
            "Dongyeop Kang"
        ],
        "published": "2023-05-23T08:49:50Z",
        "summary": "Conversational implicatures are pragmatic inferences that require listeners\nto deduce the intended meaning conveyed by a speaker from their explicit\nutterances. Although such inferential reasoning is fundamental to human\ncommunication, recent research indicates that large language models struggle to\ncomprehend these implicatures as effectively as the average human. This paper\ndemonstrates that by incorporating Grice's Four Maxims into the model through\nchain-of-thought prompting, we can significantly enhance its performance,\nsurpassing even the average human performance on this task.",
        "pdf_link": "https://arxiv.org/pdf/2305.13826v1.pdf"
    },
    {
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": [
            "Noah Lee",
            "Na Min An",
            "James Thorne"
        ],
        "published": "2023-05-23T07:55:34Z",
        "summary": "Large language models (LLMs) have shown impressive achievements in solving a\nbroad range of tasks. Augmented by instruction fine-tuning, LLMs have also been\nshown to generalize in zero-shot settings as well. However, whether LLMs\nclosely align with the human disagreement distribution has not been\nwell-studied, especially within the scope of natural language inference (NLI).\nIn this paper, we evaluate the performance and alignment of LLM distribution\nwith humans using two different techniques to estimate the multinomial\ndistribution: Monte Carlo Estimation (MCE) and Log Probability Estimation\n(LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks\nand simultaneously fail to capture human disagreement distribution. The\ninference and human alignment performances plunge even further on data samples\nwith high human disagreement levels, raising concerns about their natural\nlanguage understanding (NLU) ability and their representativeness to a larger\nhuman population. The source code for the experiments is available at\nhttps://github.com/xfactlab/emnlp2023-LLM-Disagreement",
        "pdf_link": "https://arxiv.org/pdf/2305.13788v2.pdf"
    },
    {
        "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation",
        "authors": [
            "Danqing Luo",
            "Chen Zhang",
            "Jiahui Xu",
            "Bin Wang",
            "Yiming Chen",
            "Yan Zhang",
            "Haizhou Li"
        ],
        "published": "2023-05-23T07:54:34Z",
        "summary": "Training or finetuning large-scale language models (LLMs) such as GPT-3\nrequires substantial computation resources, motivating recent efforts to\nexplore parameter-efficient adaptation to downstream tasks. One practical area\nof research is to treat these models as black boxes and interact with them\nthrough their inference APIs. In this paper, we investigate how to optimize\nfew-shot text classification without accessing the gradients of the LLMs. To\nachieve this, we treat the black-box model as a feature extractor and train a\nclassifier with the augmented text data. Data augmentation is performed using\nprompt-based finetuning on an auxiliary language model with a much smaller\nparameter size than the black-box model. Through extensive experiments on eight\ntext classification datasets, we show that our approach, dubbed BT-Classifier,\nsignificantly outperforms state-of-the-art black-box few-shot learners and\nperforms on par with methods that rely on full-model tuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.13785v2.pdf"
    },
    {
        "title": "Aligning Large Language Models through Synthetic Feedback",
        "authors": [
            "Sungdong Kim",
            "Sanghwan Bae",
            "Jamin Shin",
            "Soyoung Kang",
            "Donghyun Kwak",
            "Kang Min Yoo",
            "Minjoon Seo"
        ],
        "published": "2023-05-23T06:41:16Z",
        "summary": "Aligning large language models (LLMs) to human values has become increasingly\nimportant as it enables sophisticated steering of LLMs. However, it requires\nsignificant human demonstrations and feedback or distillation from proprietary\nLLMs such as ChatGPT. In this work, we propose a novel alignment learning\nframework with synthetic feedback not dependent on extensive human annotations\nand proprietary LLMs. First, we perform reward modeling (RM) with synthetic\nfeedback by contrasting responses from vanilla LLMs with various sizes and\nprompts. Then, we use the RM to simulate high-quality demonstrations to train a\nsupervised policy and further optimize the model with reinforcement learning.\nOur resulting model, Aligned Language Model with Synthetic Training dataset\n(ALMoST), outperforms recent open-sourced models, which are trained on the\noutputs of InstructGPT or human-annotated demonstrations, in alignment\nbenchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,\n55.0% and 58.5% of the time, respectively. Further analyses demonstrate the\nefficacy and importance of synthetic feedback in our framework. The code is\navailable at https://github.com/naver-ai/almost",
        "pdf_link": "https://arxiv.org/pdf/2305.13735v2.pdf"
    },
    {
        "title": "Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting",
        "authors": [
            "Rui Wang",
            "Hongru Wang",
            "Fei Mi",
            "Yi Chen",
            "Boyang Xue",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2023-05-23T06:38:20Z",
        "summary": "Numerous works are proposed to align large language models (LLMs) with human\nintents to better fulfill instructions, ensuring they are trustful and helpful.\nNevertheless, some human instructions are often malicious or misleading and\nfollowing them will lead to untruthful and unsafe responses. Previous work\nrarely focused on understanding how LLMs manage instructions based on\ncounterfactual premises, referred to here as \\textit{inductive instructions},\nwhich may stem from users' false beliefs or malicious intents. In this paper,\nwe aim to reveal the behaviors of LLMs towards \\textit{inductive instructions}\nand enhance their truthfulness and helpfulness accordingly. Specifically, we\nfirst introduce a benchmark of \\underline{\\textbf{Indu}}ctive\n{In\\underline{\\textbf{st}}ruct}ions (\\textsc{\\textbf{INDust}}), where the false\nknowledge is incorporated into instructions in multiple different styles. After\nextensive human and automatic evaluations, we uncovered a universal\nvulnerability among LLMs in processing inductive instructions. Additionally, we\nidentified that different inductive styles affect the models' ability to\nidentify the same underlying errors, and the complexity of the underlying\nassumptions also influences the model's performance. Motivated by these\nresults, we propose \\textsc{Dual-critique} prompting to improve LLM robustness\nagainst inductive instructions. Our experiments demonstrate that\n\\textsc{Dual-critique} prompting significantly bolsters the robustness of a\ndiverse array of LLMs, even when confronted with varying degrees of inductive\ninstruction complexity and differing inductive styles.",
        "pdf_link": "https://arxiv.org/pdf/2305.13733v2.pdf"
    },
    {
        "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
        "authors": [
            "Sukmin Cho",
            "Soyeong Jeong",
            "Jeongyeon Seo",
            "Jong C. Park"
        ],
        "published": "2023-05-23T06:35:33Z",
        "summary": "Re-rankers, which order retrieved documents with respect to the relevance\nscore on the given query, have gained attention for the information retrieval\n(IR) task. Rather than fine-tuning the pre-trained language model (PLM), the\nlarge-scale language model (LLM) is utilized as a zero-shot re-ranker with\nexcellent results. While LLM is highly dependent on the prompts, the impact and\nthe optimization of the prompts for the zero-shot re-ranker are not explored\nyet. Along with highlighting the impact of optimization on the zero-shot\nre-ranker, we propose a novel discrete prompt optimization method, Constrained\nPrompt generation (Co-Prompt), with the metric estimating the optimum for\nre-ranking. Co-Prompt guides the generated texts from PLM toward optimal\nprompts based on the metric without parameter update. The experimental results\ndemonstrate that Co-Prompt leads to outstanding re-ranking performance against\nthe baselines. Also, Co-Prompt generates more interpretable prompts for humans\nagainst other prompt optimization methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.13729v1.pdf"
    },
    {
        "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
        "authors": [
            "Fangkai Jiao",
            "Zhiyang Teng",
            "Bosheng Ding",
            "Zhengyuan Liu",
            "Nancy F. Chen",
            "Shafiq Joty"
        ],
        "published": "2023-05-23T06:13:10Z",
        "summary": "Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.13718v5.pdf"
    },
    {
        "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
        "authors": [
            "Alfonso Amayuelas",
            "Liangming Pan",
            "Wenhu Chen",
            "William Wang"
        ],
        "published": "2023-05-23T05:59:21Z",
        "summary": "This paper investigates the capabilities of Large Language Models (LLMs) in\nthe context of understanding their own knowledge and measuring their\nuncertainty. We argue this is an important feature for mitigating\nhallucinations. Specifically, we focus on addressing \\textit{known-unknown}\nquestions, characterized by high uncertainty due to the absence of definitive\nanswers. To facilitate our study, we collect a dataset with new Known-Unknown\nQuestions (KUQ) and propose a novel categorization scheme to elucidate the\nsources of uncertainty. Subsequently, we assess the LLMs' ability to\ndifferentiate between known and unknown questions and classify them\naccordingly. Moreover, we evaluate the quality of their answers in an\nOpen-Ended QA setting. To quantify the uncertainty expressed in the answers, we\ncreate a semantic evaluation method that measures the model's accuracy in\nexpressing uncertainty between known vs unknown questions.",
        "pdf_link": "https://arxiv.org/pdf/2305.13712v1.pdf"
    },
    {
        "title": "Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering",
        "authors": [
            "Mingda Chen",
            "Xilun Chen",
            "Wen-tau Yih"
        ],
        "published": "2023-05-23T04:57:31Z",
        "summary": "Few-shot learning for open domain multi-hop question answering typically\nrelies on the incontext learning capability of large language models (LLMs).\nWhile powerful, these LLMs usually contain tens or hundreds of billions of\nparameters, making them rather inefficient at inference time. To improve\nperformance of smaller language models, we propose a data synthesis framework\nfor multi-hop question answering that requires less than 10 human annotated\nquestion answer pairs. Our framework depends only on rich, naturally-occurring\nrelationships among documents and is built upon the data generation functions\nparameterized by LLMs and prompts. We synthesize millions of multi-hop\nquestions and claims to finetune language models, evaluated on popular\nbenchmarks for multi-hop question answering and fact verification. Empirically,\nour approach improves model performance significantly, allowing the finetuned\nmodels to be competitive with GPT-3.5 based approaches while being almost\none-third the size in parameter count.",
        "pdf_link": "https://arxiv.org/pdf/2305.13691v2.pdf"
    },
    {
        "title": "Error Detection for Text-to-SQL Semantic Parsing",
        "authors": [
            "Shijie Chen",
            "Ziru Chen",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-05-23T04:44:22Z",
        "summary": "Despite remarkable progress in text-to-SQL semantic parsing in recent years,\nthe performance of existing parsers is still far from perfect. Specifically,\nmodern text-to-SQL parsers based on deep learning are often over-confident,\nthus casting doubt on their trustworthiness when deployed for real use. In this\npaper, we propose a parser-independent error detection model for text-to-SQL\nsemantic parsing. Using a language model of code as its bedrock, we enhance our\nerror detection model with graph neural networks that learn structural features\nof both natural language questions and SQL queries. We train our model on\nrealistic parsing errors collected from a cross-domain setting, which leads to\nstronger generalization ability. Experiments with three strong text-to-SQL\nparsers featuring different decoding mechanisms show that our approach\noutperforms parser-dependent uncertainty metrics. Our model could also\neffectively improve the performance and usability of text-to-SQL semantic\nparsers regardless of their architectures. (Our implementation is available at\nhttps://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)",
        "pdf_link": "https://arxiv.org/pdf/2305.13683v2.pdf"
    },
    {
        "title": "Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models",
        "authors": [
            "Tim Schott",
            "Daniel Furman",
            "Shreshta Bhat"
        ],
        "published": "2023-05-23T04:31:39Z",
        "summary": "In this work, we assess the ability of foundation models to recall\nencyclopedic knowledge across a wide range of linguistic contexts. To support\nthis, we: 1) produce a 20-language dataset that contains 303k factual\nassociations paired with counterfactuals, 2) evaluate 5 models in a\nmultilingual test, and 3) benchmark a diverse set of 24 models in an\nEnglish-only test. Meta's LLaMA achieves the highest scores in both\nmultilingual and English-only evaluations. Yet, an analysis of LLaMA's errors\nreveals significant limitations in its ability to recall facts in languages\nother than English, plus difficulties related to the location and gender of\nfact subjects. Overall, our findings suggest that today's foundation models are\nfar from polyglots.",
        "pdf_link": "https://arxiv.org/pdf/2305.13675v2.pdf"
    },
    {
        "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
        "authors": [
            "Shuo Zhang",
            "Liangming Pan",
            "Junzhou Zhao",
            "William Yang Wang"
        ],
        "published": "2023-05-23T04:22:50Z",
        "summary": "Large language models often necessitate grounding on external knowledge to\ngenerate faithful and reliable answers. Yet even with the correct groundings in\nthe reference, they can ignore them and rely on wrong groundings or their\ninherent biases to hallucinate when users, being largely unaware of the\nspecifics of the stored information, pose questions that might not directly\ncorrelate with the retrieved groundings. In this work, we formulate this\nknowledge alignment problem and introduce MixAlign, a framework that interacts\nwith both the human user and the knowledge base to obtain and integrate\nclarifications on how the user question relates to the stored information.\nMixAlign employs a language model to achieve automatic knowledge alignment and,\nif necessary, further enhances this alignment through human user\nclarifications. Experimental results highlight the crucial role of knowledge\nalignment in boosting model performance and mitigating hallucination, with\nimprovements noted up to 22.2% and 27.1% respectively. We also demonstrate the\neffectiveness of MixAlign in improving knowledge alignment by producing\nhigh-quality, user-centered clarifications.",
        "pdf_link": "https://arxiv.org/pdf/2305.13669v2.pdf"
    },
    {
        "title": "On the Risk of Misinformation Pollution with Large Language Models",
        "authors": [
            "Yikang Pan",
            "Liangming Pan",
            "Wenhu Chen",
            "Preslav Nakov",
            "Min-Yen Kan",
            "William Yang Wang"
        ],
        "published": "2023-05-23T04:10:26Z",
        "summary": "In this paper, we comprehensively investigate the potential misuse of modern\nLarge Language Models (LLMs) for generating credible-sounding misinformation\nand its subsequent impact on information-intensive applications, particularly\nOpen-Domain Question Answering (ODQA) systems. We establish a threat model and\nsimulate potential misuse scenarios, both unintentional and intentional, to\nassess the extent to which LLMs can be utilized to produce misinformation. Our\nstudy reveals that LLMs can act as effective misinformation generators, leading\nto a significant degradation in the performance of ODQA systems. To mitigate\nthe harm caused by LLM-generated misinformation, we explore three defense\nstrategies: prompting, misinformation detection, and majority voting. While\ninitial results show promising trends for these defensive strategies, much more\nwork needs to be done to address the challenge of misinformation pollution. Our\nwork highlights the need for further research and interdisciplinary\ncollaboration to address LLM-generated misinformation and to promote\nresponsible use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.13661v2.pdf"
    },
    {
        "title": "Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning",
        "authors": [
            "Xiao Yu",
            "Maximillian Chen",
            "Zhou Yu"
        ],
        "published": "2023-05-23T04:07:03Z",
        "summary": "Planning for goal-oriented dialogue often requires simulating future dialogue\ninteractions and estimating task progress. Many approaches thus consider\ntraining neural networks to perform look-ahead search algorithms such as A*\nsearch and Monte Carlo Tree Search (MCTS). However, this training often\nrequires abundant annotated data, which creates challenges when faced with\nnoisy annotations or low-resource settings. We introduce GDP-Zero, an approach\nusing Open-Loop MCTS to perform goal-oriented dialogue policy planning without\nany model training. GDP-Zero prompts a large language model to act as a policy\nprior, value function, user simulator, and system model during the tree search.\nWe evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that\nits responses are preferred over ChatGPT up to 59.32% of the time, and are\nrated more persuasive than ChatGPT during interactive evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.13660v2.pdf"
    },
    {
        "title": "ChatGPT as your Personal Data Scientist",
        "authors": [
            "Md Mahadi Hassan",
            "Alex Knipper",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2023-05-23T04:00:16Z",
        "summary": "The rise of big data has amplified the need for efficient, user-friendly\nautomated machine learning (AutoML) tools. However, the intricacy of\nunderstanding domain-specific data and defining prediction tasks necessitates\nhuman intervention making the process time-consuming while preventing full\nautomation. Instead, envision an intelligent agent capable of assisting users\nin conducting AutoML tasks through intuitive, natural conversations without\nrequiring in-depth knowledge of the underlying machine learning (ML) processes.\nThis agent's key challenge is to accurately comprehend the user's prediction\ngoals and, consequently, formulate precise ML tasks, adjust data sets and model\nparameters accordingly, and articulate results effectively. In this paper, we\ntake a pioneering step towards this ambitious goal by introducing a\nChatGPT-based conversational data-science framework to act as a \"personal data\nscientist\". Precisely, we utilize Large Language Models (ChatGPT) to build a\nnatural interface between the users and the ML models (Scikit-Learn), which in\nturn, allows us to approach this ambitious problem with a realistic solution.\n  Our model pivots around four dialogue states: Data Visualization, Task\nFormulation, Prediction Engineering, and Result Summary and Recommendation.\nEach state marks a unique conversation phase, impacting the overall user-system\ninteraction. Multiple LLM instances, serving as \"micro-agents\", ensure a\ncohesive conversation flow, granting us granular control over the\nconversation's progression. In summary, we developed an end-to-end system that\nnot only proves the viability of the novel concept of conversational data\nscience but also underscores the potency of LLMs in solving complex tasks.\nInterestingly, its development spotlighted several critical weaknesses in the\ncurrent LLMs (ChatGPT) and highlighted substantial opportunities for\nimprovement.",
        "pdf_link": "https://arxiv.org/pdf/2305.13657v1.pdf"
    },
    {
        "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
        "authors": [
            "Long Lian",
            "Boyi Li",
            "Adam Yala",
            "Trevor Darrell"
        ],
        "published": "2023-05-23T03:59:06Z",
        "summary": "Recent advancements in text-to-image diffusion models have yielded impressive\nresults in generating realistic and diverse images. However, these models still\nstruggle with complex prompts, such as those that involve numeracy and spatial\nreasoning. This work proposes to enhance prompt understanding capabilities in\ndiffusion models. Our method leverages a pretrained large language model (LLM)\nfor grounded generation in a novel two-stage process. In the first stage, the\nLLM generates a scene layout that comprises captioned bounding boxes from a\ngiven prompt describing the desired image. In the second stage, a novel\ncontroller guides an off-the-shelf diffusion model for layout-grounded image\ngeneration. Both stages utilize existing pretrained models without additional\nmodel parameter optimization. Our method significantly outperforms the base\ndiffusion model and several strong baselines in accurately generating images\naccording to prompts that require various capabilities, doubling the generation\naccuracy across four tasks on average. Furthermore, our method enables\ninstruction-based multi-round scene specification and can handle prompts in\nlanguages not supported by the underlying diffusion model. We anticipate that\nour method will unleash users' creativity by accurately following more complex\nprompts. Our code, demo, and benchmark are available at:\nhttps://llm-grounded-diffusion.github.io",
        "pdf_link": "https://arxiv.org/pdf/2305.13655v3.pdf"
    },
    {
        "title": "InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning",
        "authors": [
            "Samuel Cahyawijaya",
            "Holy Lovenia",
            "Tiezheng Yu",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023-05-23T02:51:34Z",
        "summary": "Large language models (LLMs) that are tuned with instructions have\ndemonstrated remarkable capabilities in various tasks and languages. However,\ntheir ability to generalize to underrepresented languages is limited due to the\nscarcity of available data. Additionally, directly adapting new languages to\ninstruction-tuned LLMs can result in catastrophic forgetting, which leads to\nthe loss of multitasking ability. To address this issue, we propose\nInstructAlign which uses continual crosslingual instruction tuning to enable\nLLMs to align new unseen languages with previously learned high-resource\nlanguages. Our results demonstrate the effectiveness of InstructAlign in\nenabling the model to understand low-resource languages with limited parallel\ndata while preventing catastrophic forgetting. Our work contributes to the\nadvancement of language adaptation methods, particularly for adapting\ninstruction-tuned LLMs to underrepresented languages. Our code is released on\nhttps://github.com/HLTCHKUST/InstructAlign",
        "pdf_link": "https://arxiv.org/pdf/2305.13627v2.pdf"
    },
    {
        "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
        "authors": [
            "Yang Deng",
            "Lizi Liao",
            "Liang Chen",
            "Hongru Wang",
            "Wenqiang Lei",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-23T02:49:35Z",
        "summary": "Conversational systems based on Large Language Models (LLMs), such as\nChatGPT, show exceptional proficiency in context understanding and response\ngeneration. However, despite their impressive capabilities, they still possess\nlimitations, such as providing randomly-guessed answers to ambiguous queries or\nfailing to refuse users' requests, both of which are considered aspects of a\nconversational agent's proactivity. This raises the question of whether\nLLM-based conversational systems are equipped to handle proactive dialogue\nproblems. In this work, we conduct a comprehensive analysis of LLM-based\nconversational systems, specifically focusing on three aspects of proactive\ndialogue systems: clarification, target-guided, and non-collaborative\ndialogues. To trigger the proactivity of LLMs, we propose the Proactive\nChain-of-Thought prompting scheme, which augments LLMs with the goal planning\ncapability over descriptive reasoning chains. Empirical findings are discussed\nto promote future studies on LLM-based proactive dialogue systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.13626v2.pdf"
    },
    {
        "title": "Understanding Programs by Exploiting (Fuzzing) Test Cases",
        "authors": [
            "Jianyu Zhao",
            "Yuyang Rong",
            "Yiwen Guo",
            "Yifeng He",
            "Hao Chen"
        ],
        "published": "2023-05-23T01:51:46Z",
        "summary": "Semantic understanding of programs has attracted great attention in the\ncommunity. Inspired by recent successes of large language models (LLMs) in\nnatural language understanding, tremendous progress has been made by treating\nprogramming language as another sort of natural language and training LLMs on\ncorpora of program code. However, programs are essentially different from texts\nafter all, in a sense that they are normally heavily structured and\nsyntax-strict. In particular, programs and their basic units (i.e., functions\nand subroutines) are designed to demonstrate a variety of behaviors and/or\nprovide possible outputs, given different inputs. The relationship between\ninputs and possible outputs/behaviors represents the functions/subroutines and\nprofiles the program as a whole. Therefore, we propose to incorporate such a\nrelationship into learning, for achieving a deeper semantic understanding of\nprograms. To obtain inputs that are representative enough to trigger the\nexecution of most part of the code, we resort to fuzz testing and propose fuzz\ntuning to boost the performance of program understanding and code\nrepresentation learning, given a pre-trained LLM. The effectiveness of the\nproposed method is verified on two program understanding tasks including code\nclone detection and code classification, and it outperforms current\nstate-of-the-arts by large margins. Code is available at\nhttps://github.com/rabbitjy/FuzzTuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.13592v2.pdf"
    },
    {
        "title": "Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",
        "authors": [
            "Aaron Chan",
            "Anant Kharkar",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Alec Helyar",
            "Eslam Kamal",
            "Mohamed Elkamhawy",
            "Neel Sundaresan"
        ],
        "published": "2023-05-23T01:21:55Z",
        "summary": "Software vulnerabilities bear enterprises significant costs. Despite\nextensive efforts in research and development of software vulnerability\ndetection methods, uncaught vulnerabilities continue to put software owners and\nusers at risk. Many current vulnerability detection methods require that code\nsnippets can compile and build before attempting detection. This,\nunfortunately, introduces a long latency between the time a vulnerability is\ninjected to the time it is removed, which can substantially increases the cost\nof fixing a vulnerability. We recognize that the current advances in machine\nlearning can be used to detect vulnerable code patterns on syntactically\nincomplete code snippets as the developer is writing the code at EditTime. In\nthis paper we present a practical system that leverages deep learning on a\nlarge-scale data set of vulnerable code patterns to learn complex\nmanifestations of more than 250 vulnerability types and detect vulnerable code\npatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuning\napproaches on state of the art pre-trained Large Language Models (LLMs). We\nshow that in comparison with state of the art vulnerability detection models\nour approach improves the state of the art by 10%. We also evaluate our\napproach to detect vulnerability in auto-generated code by code LLMs.\nEvaluation on a benchmark of high-risk code scenarios shows a reduction of up\nto 90% vulnerability reduction.",
        "pdf_link": "https://arxiv.org/pdf/2306.01754v1.pdf"
    },
    {
        "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
        "authors": [
            "Binfeng Xu",
            "Zhiyuan Peng",
            "Bowen Lei",
            "Subhabrata Mukherjee",
            "Yuchen Liu",
            "Dongkuan Xu"
        ],
        "published": "2023-05-23T00:16:48Z",
        "summary": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large\nLanguage Models (LLMs) with tools that allow for knowledge retrieval and action\nexecution. Existing ALM systems trigger LLM thought processes while pulling\nobservations from these tools in an interleaved fashion. Specifically, an LLM\nreasons to call an external tool, gets halted to fetch the tool's response, and\nthen decides the next action based on all preceding response tokens. Such a\nparadigm, though straightforward and easy to implement, often leads to huge\ncomputation complexity from redundant prompts and repeated execution. This\nstudy addresses such challenges for the first time, proposing a modular\nparadigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning\nprocess from external observations, thus significantly reducing token\nconsumption. Comprehensive evaluations across six public NLP benchmarks and a\ncurated dataset reveal consistent performance enhancements with our proposed\nmethodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy\nimprovement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO\ndemonstrates robustness under tool-failure scenarios. Beyond prompt efficiency,\ndecoupling parametric modules from non-parametric tool calls enables\ninstruction fine-tuning to offload LLMs into smaller language models, thus\nsubstantially reducing model parameters. Our illustrative work offloads\nreasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant\npotential for truly efficient and scalable ALM systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.18323v1.pdf"
    },
    {
        "title": "How Language Model Hallucinations Can Snowball",
        "authors": [
            "Muru Zhang",
            "Ofir Press",
            "William Merrill",
            "Alisa Liu",
            "Noah A. Smith"
        ],
        "published": "2023-05-22T23:14:28Z",
        "summary": "A major risk of using language models in practical applications is their\ntendency to hallucinate incorrect statements. Hallucinations are often\nattributed to knowledge gaps in LMs, but we hypothesize that in some cases,\nwhen justifying previously generated hallucinations, LMs output false claims\nthat they can separately recognize as incorrect. We construct three\nquestion-answering datasets where ChatGPT and GPT-4 often state an incorrect\nanswer and offer an explanation with at least one incorrect claim. Crucially,\nwe find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes,\nrespectively. We refer to this phenomenon as hallucination snowballing: an LM\nover-commits to early mistakes, leading to more mistakes that it otherwise\nwould not make.",
        "pdf_link": "https://arxiv.org/pdf/2305.13534v1.pdf"
    },
    {
        "title": "A Study of Generative Large Language Model for Medical Research and Healthcare",
        "authors": [
            "Cheng Peng",
            "Xi Yang",
            "Aokun Chen",
            "Kaleb E Smith",
            "Nima PourNejatian",
            "Anthony B Costa",
            "Cheryl Martin",
            "Mona G Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Gloria Lipori",
            "Duane A Mitchell",
            "Naykky S Ospina",
            "Mustafa M Ahmed",
            "William R Hogan",
            "Elizabeth A Shenkman",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-05-22T22:37:24Z",
        "summary": "There is enormous enthusiasm and concerns in using large language models\n(LLMs) in healthcare, yet current assumptions are all based on general-purpose\nLLMs such as ChatGPT. This study develops a clinical generative LLM,\nGatorTronGPT, using 277 billion words of mixed clinical and English text with a\nGPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical\nnatural language processing for medical research. Synthetic NLP models trained\nusing GatorTronGPT generated text outperform NLP models trained using\nreal-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)\nscale shows that there is no significant difference in linguistic readability\n(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical\nrelevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that\nphysicians cannot differentiate them (p < 0.001). This study provides insights\non the opportunities and challenges of LLMs for medical research and\nhealthcare.",
        "pdf_link": "https://arxiv.org/pdf/2305.13523v1.pdf"
    },
    {
        "title": "Small Language Models Improve Giants by Rewriting Their Outputs",
        "authors": [
            "Giorgos Vernikos",
            "Arthur Bra\u017einskas",
            "Jakub Adamek",
            "Jonathan Mallinson",
            "Aliaksei Severyn",
            "Eric Malmi"
        ],
        "published": "2023-05-22T22:07:50Z",
        "summary": "Despite the impressive performance of large language models (LLMs), they\noften lag behind specialized models in various tasks. LLMs only use a fraction\nof the existing training data for in-context learning, while task-specific\nmodels harness the full dataset for fine-tuning. In this work, we tackle the\nproblem of leveraging training data to improve the performance of LLMs without\nfine-tuning. Our approach directly targets LLM predictions without requiring\naccess to their weights. We create a pool of candidates from the LLM through\nfew-shot prompting and we employ a compact model, the LM-corrector (LMCor),\nspecifically trained to merge these candidates to produce an enhanced output.\nOur experiments on four natural language generation tasks demonstrate that even\na small LMCor model (250M) substantially improves the few-shot performance of\nLLMs (62B), matching and even outperforming standard fine-tuning. Furthermore,\nwe illustrate the robustness of LMCor against different prompts, thereby\nminimizing the need for extensive prompt engineering. Finally, we show that\nLMCor can be seamlessly integrated with different LLMs at inference, serving as\na plug-and-play module to improve their performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.13514v2.pdf"
    },
    {
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": [
            "Mutian He",
            "Philip N. Garner"
        ],
        "published": "2023-05-22T21:59:26Z",
        "summary": "Recently, large pretrained language models have demonstrated strong language\nunderstanding capabilities. This is particularly reflected in their zero-shot\nand in-context learning abilities on downstream tasks through prompting. To\nassess their impact on spoken language understanding (SLU), we evaluate several\nsuch models like ChatGPT and OPT of different sizes on multiple benchmarks. We\nverify the emergent ability unique to the largest models as they can reach\nintent classification accuracy close to that of supervised models with zero or\nfew shots on various languages given oracle transcripts. By contrast, the\nresults for smaller models fitting a single GPU fall far behind. We note that\nthe error cases often arise from the annotation scheme of the dataset;\nresponses from ChatGPT are still reasonable. We show, however, that the model\nis worse at slot filling, and its performance is sensitive to ASR errors,\nsuggesting serious challenges for the application of those textual models on\nSLU.",
        "pdf_link": "https://arxiv.org/pdf/2305.13512v2.pdf"
    },
    {
        "title": "Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "authors": [
            "Kranti Chalamalasetti",
            "Jana G\u00f6tze",
            "Sherzod Hakimov",
            "Brielen Madureira",
            "Philipp Sadler",
            "David Schlangen"
        ],
        "published": "2023-05-22T19:56:10Z",
        "summary": "Recent work has proposed a methodology for the systematic evaluation of\n\"Situated Language Understanding Agents\"-agents that operate in rich linguistic\nand non-linguistic contexts-through testing them in carefully constructed\ninteractive settings. Other recent work has argued that Large Language Models\n(LLMs), if suitably set up, can be understood as (simulators of) such agents. A\nconnection suggests itself, which this paper explores: Can LLMs be evaluated\nmeaningfully by exposing them to constrained game-like settings that are built\nto challenge specific capabilities? As a proof of concept, this paper\ninvestigates five interaction settings, showing that current chat-optimised\nLLMs are, to an extent, capable to follow game-play instructions. Both this\ncapability and the quality of the game play, measured by how well the\nobjectives of the different games are met, follows the development cycle, with\nnewer models performing better. The metrics even for the comparatively simple\nexample games are far from being saturated, suggesting that the proposed\ninstrument will remain to have diagnostic value. Our general framework for\nimplementing and evaluating games with LLMs is available at\nhttps://github.com/clembench .",
        "pdf_link": "https://arxiv.org/pdf/2305.13455v3.pdf"
    },
    {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2023-05-22T18:54:35Z",
        "summary": "Automatic summarization generates concise summaries that contain key ideas of\nsource documents. As the most mainstream datasets for the news sub-domain,\nCNN/DailyMail and BBC XSum have been widely used for performance benchmarking.\nHowever, the reference summaries of those datasets turn out to be noisy, mainly\nin terms of factual hallucination and information redundancy. To address this\nchallenge, we first annotate new expert-writing Element-aware test sets\nfollowing the \"Lasswell Communication Model\" proposed by Lasswell (1948),\nallowing reference summaries to focus on more fine-grained news elements\nobjectively and comprehensively. Utilizing the new test sets, we observe the\nsurprising zero-shot summary ability of LLMs, which addresses the issue of the\ninconsistent results between human preference and automatic evaluation metrics\nof LLMs' zero-shot summaries in prior work. Further, we propose a Summary\nChain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step\nby step, which helps them integrate more fine-grained details of source\ndocuments into the final summaries that correlate with the human writing\nmindset. Experimental results show our method outperforms state-of-the-art\nfine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two\ndatasets, respectively. Dataset and code are publicly available at\nhttps://github.com/Alsace08/SumCoT.",
        "pdf_link": "https://arxiv.org/pdf/2305.13412v1.pdf"
    },
    {
        "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
        "authors": [
            "Yanchen Liu",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2023-05-22T18:43:31Z",
        "summary": "Existing large language models (LLMs) that mainly focus on Standard American\nEnglish (SAE) often lead to significantly worse performance when being applied\nto other English dialects. While existing mitigations tackle discrepancies for\nindividual target dialects, they assume access to high-accuracy dialect\nidentification systems. The boundaries between dialects are inherently\nflexible, making it difficult to categorize language into discrete predefined\ncategories. In this paper, we propose DADA (Dialect Adaptation via Dynamic\nAggregation), a modular approach to imbue SAE-trained models with\nmulti-dialectal robustness by composing adapters which handle specific\nlinguistic features. The compositional architecture of DADA allows for both\ntargeted adaptation to specific dialect variants and simultaneous adaptation to\nvarious dialects. We show that DADA is effective for both single task and\ninstruction finetuned language models, offering an extensible and interpretable\nframework for adapting existing LLMs to different English dialects.",
        "pdf_link": "https://arxiv.org/pdf/2305.13406v3.pdf"
    },
    {
        "title": "Can LLMs facilitate interpretation of pre-trained language models?",
        "authors": [
            "Basel Mousi",
            "Nadir Durrani",
            "Fahim Dalvi"
        ],
        "published": "2023-05-22T18:03:13Z",
        "summary": "Work done to uncover the knowledge encoded within pre-trained language models\nrely on annotated corpora or human-in-the-loop methods. However, these\napproaches are limited in terms of scalability and the scope of interpretation.\nWe propose using a large language model, ChatGPT, as an annotator to enable\nfine-grained interpretation analysis of pre-trained language models. We\ndiscover latent concepts within pre-trained language models by applying\nagglomerative hierarchical clustering over contextualized representations and\nthen annotate these concepts using ChatGPT. Our findings demonstrate that\nChatGPT produces accurate and semantically richer annotations compared to\nhuman-annotated concepts. Additionally, we showcase how GPT-based annotations\nempower interpretation analysis methodologies of which we demonstrate two:\nprobing frameworks and neuron interpretation. To facilitate further exploration\nand experimentation in the field, we make available a substantial ConceptNet\ndataset (TCN) comprising 39,000 annotated concepts.",
        "pdf_link": "https://arxiv.org/pdf/2305.13386v2.pdf"
    },
    {
        "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Peng Cui",
            "Tiannan Wang",
            "Zhenxin Xiao",
            "Yifan Hou",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "published": "2023-05-22T17:58:10Z",
        "summary": "The fixed-size context of Transformer makes GPT models incapable of\ngenerating arbitrarily long text. In this paper, we introduce RecurrentGPT, a\nlanguage-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT is\nbuilt upon a large language model (LLM) such as ChatGPT and uses natural\nlanguage to simulate the Long Short-Term Memory mechanism in an LSTM. At each\ntimestep, RecurrentGPT generates a paragraph of text and updates its\nlanguage-based long-short term memory stored on the hard drive and the prompt,\nrespectively. This recurrence mechanism enables RecurrentGPT to generate texts\nof arbitrary length without forgetting. Since human users can easily observe\nand edit the natural language memories, RecurrentGPT is interpretable and\nenables interactive generation of long text. RecurrentGPT is an initial step\ntowards next-generation computer-assisted writing systems beyond local editing\nsuggestions. In addition to producing AI-generated content (AIGC), we also\ndemonstrate the possibility of using RecurrentGPT as an interactive fiction\nthat directly interacts with consumers. We call this usage of generative models\nby ``AI As Contents'' (AIAC), which we believe is the next form of conventional\nAIGC. We further demonstrate the possibility of using RecurrentGPT to create\npersonalized interactive fiction that directly interacts with readers instead\nof interacting with writers. More broadly, RecurrentGPT demonstrates the\nutility of borrowing ideas from popular model designs in cognitive science and\ndeep learning for prompting LLMs. Our code is available at\nhttps://github.com/aiwaves-cn/RecurrentGPT and an online demo is available at\nhttps://www.aiwaves.org/recurrentgpt.",
        "pdf_link": "https://arxiv.org/pdf/2305.13304v1.pdf"
    },
    {
        "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
        "authors": [
            "Abdullatif K\u00f6ksal",
            "Omer Faruk Yalcin",
            "Ahmet Akbiyik",
            "M. Tahir Kilavuz",
            "Anna Korhonen",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-05-22T17:58:01Z",
        "summary": "Pretrained language models (PLMs) are key components in NLP, but they contain\nstrong social biases. Quantifying these biases is challenging because current\nmethods focusing on fill-the-mask objectives are sensitive to slight changes in\ninput. To address this, we propose a bias probing technique called LABDet, for\nevaluating social bias in PLMs with a robust and language-agnostic method. For\nnationality as a case study, we show that LABDet `surfaces' nationality bias by\ntraining a classifier on top of a frozen PLM on non-nationality sentiment\ndetection. We find consistent patterns of nationality bias across monolingual\nPLMs in six languages that align with historical and political context. We also\nshow for English BERT that bias surfaced by LABDet correlates well with bias in\nthe pretraining data; thus, our work is one of the few studies that directly\nlinks pretraining data to PLM behavior. Finally, we verify LABDet's reliability\nand applicability to different templates and languages through an extensive set\nof robustness checks. We publicly share our code and dataset in\nhttps://github.com/akoksal/LABDet.",
        "pdf_link": "https://arxiv.org/pdf/2305.13302v2.pdf"
    },
    {
        "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Renze Lou",
            "Yu Su"
        ],
        "published": "2023-05-22T17:57:41Z",
        "summary": "By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
        "pdf_link": "https://arxiv.org/pdf/2305.13300v4.pdf"
    },
    {
        "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
        "authors": [
            "Chenglei Si",
            "Dan Friedman",
            "Nitish Joshi",
            "Shi Feng",
            "Danqi Chen",
            "He He"
        ],
        "published": "2023-05-22T17:56:31Z",
        "summary": "In-context learning (ICL) is an important paradigm for adapting large\nlanguage models (LLMs) to new tasks, but the generalization behavior of ICL\nremains poorly understood. We investigate the inductive biases of ICL from the\nperspective of feature bias: which feature ICL is more likely to use given a\nset of underspecified demonstrations in which two features are equally\npredictive of the labels. First, we characterize the feature biases of GPT-3\nmodels by constructing underspecified demonstrations from a range of NLP\ndatasets and feature combinations. We find that LLMs exhibit clear feature\nbiases - for example, demonstrating a strong bias to predict labels according\nto sentiment rather than shallow lexical features, like punctuation. Second, we\nevaluate the effect of different interventions that are designed to impose an\ninductive bias in favor of a particular feature, such as adding a natural\nlanguage instruction or using semantically relevant label words. We find that,\nwhile many interventions can influence the learner to prefer a particular\nfeature, it can be difficult to overcome strong prior biases. Overall, our\nresults provide a broader picture of the types of features that ICL may be more\nlikely to exploit and how to impose inductive biases that are better aligned\nwith the intended task.",
        "pdf_link": "https://arxiv.org/pdf/2305.13299v1.pdf"
    },
    {
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
        "authors": [
            "Yann Dubois",
            "Xuechen Li",
            "Rohan Taori",
            "Tianyi Zhang",
            "Ishaan Gulrajani",
            "Jimmy Ba",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-05-22T17:55:50Z",
        "summary": "Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their strong instruction-following abilities. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following requires\ntackling three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 50x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14387v4.pdf"
    },
    {
        "title": "Fairness of ChatGPT",
        "authors": [
            "Yunqi Li",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-22T17:51:56Z",
        "summary": "Understanding and addressing unfairness in LLMs are crucial for responsible\nAI deployment. However, there is a limited availability of quantitative\nanalyses and in-depth studies regarding fairness evaluations in LLMs,\nespecially when applying LLMs to high-stakes fields. This work aims to fill\nthis gap by providing a systematic evaluation of the effectiveness and fairness\nof LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's\nperformance in high-takes fields including education, criminology, finance and\nhealthcare. To make thorough evaluation, we consider both group fairness and\nindividual fairness and we also observe the disparities in ChatGPT's outputs\nunder a set of biased or unbiased prompts. This work contributes to a deeper\nunderstanding of LLMs' fairness performance, facilitates bias mitigation and\nfosters the development of responsible artificial intelligence systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.18569v1.pdf"
    },
    {
        "title": "Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection",
        "authors": [
            "Mithun Das",
            "Saurabh Kumar Pandey",
            "Animesh Mukherjee"
        ],
        "published": "2023-05-22T17:36:58Z",
        "summary": "Hate speech is a severe issue that affects many online platforms. So far,\nseveral studies have been performed to develop robust hate speech detection\nsystems. Large language models like ChatGPT have recently shown a great promise\nin performing several tasks, including hate speech detection. However, it is\ncrucial to comprehend the limitations of these models to build robust hate\nspeech detection systems. To bridge this gap, our study aims to evaluate the\nstrengths and weaknesses of the ChatGPT model in detecting hate speech at a\ngranular level across 11 languages. Our evaluation employs a series of\nfunctionality tests that reveals various intricate failures of the model which\nthe aggregate metrics like macro F1 or accuracy are not able to unfold. In\naddition, we investigate the influence of complex emotions, such as the use of\nemojis in hate speech, on the performance of the ChatGPT model. Our analysis\nhighlights the shortcomings of the generative models in detecting certain types\nof hate speech and highlighting the need for further research and improvements\nin the workings of these models.",
        "pdf_link": "https://arxiv.org/pdf/2305.13276v2.pdf"
    },
    {
        "title": "CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles",
        "authors": [
            "Shashank Sonkar",
            "Naiming Liu",
            "Debshila Basu Mallick",
            "Richard G. Baraniuk"
        ],
        "published": "2023-05-22T17:35:05Z",
        "summary": "We present a design framework called Conversational Learning with Analytical\nStep-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring\nSystems (ITS) powered by high-performance Large Language Models (LLMs). The\nCLASS framework empowers ITS with two key capabilities. First, through a\ncarefully curated scaffolding dataset, CLASS equips ITS with essential\nproblem-solving strategies, enabling it to provide tutor-like, step-by-step\nguidance to students. Second, by using a dynamic conversational dataset, CLASS\nassists ITS in facilitating natural language interactions, fostering engaging\nstudent-tutor conversations. The CLASS framework also provides valuable\ninsights into ITS' internal decision-making process which allows seamless\nintegration of user feedback, thus enabling continuous refinement and\nimprovement. We also present a proof-of-concept ITS, referred to as SPOCK,\nwhich is trained using the CLASS framework with a focus on introductory\ncollege-level biology content. A carefully constructed protocol was developed\nfor SPOCK's preliminary evaluation, examining aspects such as the factual\naccuracy and relevance of its responses. Experts in the field of biology\noffered favorable remarks, particularly highlighting SPOCK's capability to\nbreak down questions into manageable subproblems and provide encouraging\nresponses to students. Code and models are available at\nhttps://github.com/luffycodes/Tutorbot-Spock.",
        "pdf_link": "https://arxiv.org/pdf/2305.13272v2.pdf"
    },
    {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Shafiq Joty",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "published": "2023-05-22T17:34:23Z",
        "summary": "We present chain-of-knowledge (CoK), a novel framework that augments large\nlanguage models (LLMs) by dynamically incorporating grounding information from\nheterogeneous sources. It results in more factual rationales and reduced\nhallucination in generation. Specifically, CoK consists of three stages:\nreasoning preparation, dynamic knowledge adapting, and answer consolidation.\nGiven a knowledge-intensive question, CoK first prepares several preliminary\nrationales and answers while identifying the relevant knowledge domains. If\nthere is no majority consensus among the answers from samples, CoK corrects the\nrationales step by step by adapting knowledge from the identified domains.\nThese corrected rationales can plausibly serve as a better foundation for the\nfinal answer consolidation. Unlike prior studies that primarily use\nunstructured data, CoK also leverages structured knowledge sources such as\nWikidata and tables that provide more reliable factual information. To access\nboth unstructured and structured knowledge sources in the dynamic knowledge\nadapting stage, we propose an adaptive query generator that allows the\ngeneration of queries for various types of query languages, including SPARQL,\nSQL, and natural sentences. Moreover, to minimize error propagation between\nrationales, CoK corrects the rationales progressively using preceding corrected\nrationales to generate and correct subsequent rationales. Extensive experiments\nshow that CoK consistently improves the performance of LLMs on\nknowledge-intensive tasks across different domains.",
        "pdf_link": "https://arxiv.org/pdf/2305.13269v4.pdf"
    },
    {
        "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
        "authors": [
            "Yueting Yang",
            "Xintong Zhang",
            "Wenjuan Han"
        ],
        "published": "2023-05-22T17:33:44Z",
        "summary": "Pre-trained visual language models (VLM) have shown excellent performance in\nimage caption tasks. However, it sometimes shows insufficient reasoning\nability. In contrast, large language models (LLMs) emerge with powerful\nreasoning capabilities. Therefore, we propose a method called TReE, which\ntransfers the reasoning ability of a large language model to a visual language\nmodel in zero-shot scenarios. TReE contains three stages: observation,\nthinking, and re-thinking. Observation stage indicates that VLM obtains the\noverall information of the relative image. Thinking stage combines the image\ninformation and task description as the prompt of the LLM, inference with the\nrationals. Re-Thinking stage learns from rationale and then inference the final\nresult through VLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.13267v1.pdf"
    },
    {
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "authors": [
            "Jennifer Hu",
            "Roger Levy"
        ],
        "published": "2023-05-22T17:33:17Z",
        "summary": "Prompting is now a dominant method for evaluating the linguistic knowledge of\nlarge language models (LLMs). While other methods directly read out models'\nprobability distributions over strings, prompting requires models to access\nthis internal information by processing linguistic input, thereby implicitly\ntesting a new type of emergent ability: metalinguistic judgment. In this study,\nwe compare metalinguistic prompting and direct probability measurements as ways\nof measuring models' linguistic knowledge. Broadly, we find that LLMs'\nmetalinguistic judgments are inferior to quantities directly derived from\nrepresentations. Furthermore, consistency gets worse as the prompt query\ndiverges from direct measurements of next-word probabilities. Our findings\nsuggest that negative results relying on metalinguistic prompts cannot be taken\nas conclusive evidence that an LLM lacks a particular linguistic\ngeneralization. Our results also highlight the value that is lost with the move\nto closed APIs where access to probability distributions is limited.",
        "pdf_link": "https://arxiv.org/pdf/2305.13264v2.pdf"
    },
    {
        "title": "\"According to ...\": Prompting Language Models Improves Quoting from Pre-Training Data",
        "authors": [
            "Orion Weller",
            "Marc Marone",
            "Nathaniel Weir",
            "Dawn Lawrie",
            "Daniel Khashabi",
            "Benjamin Van Durme"
        ],
        "published": "2023-05-22T17:25:24Z",
        "summary": "Large Language Models (LLMs) may hallucinate and generate fake information,\ndespite pre-training on factual data. Inspired by the journalistic device of\n\"according to sources\", we propose according-to prompting: directing LLMs to\nground responses against previously observed text. To quantify this grounding,\nwe propose a novel evaluation metric (QUIP-Score) that measures the extent to\nwhich model-produced answers are directly found in underlying text corpora. We\nillustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S.\nlegal tax code) that these prompts improve grounding under our metrics, with\nthe additional benefit of often improving end-task performance. Furthermore,\nprompts that ask the model to decrease grounding (or to ground to other\ncorpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase\nor decrease grounded generations on request.",
        "pdf_link": "https://arxiv.org/pdf/2305.13252v2.pdf"
    },
    {
        "title": "Chip-Chat: Challenges and Opportunities in Conversational Hardware Design",
        "authors": [
            "Jason Blocklove",
            "Siddharth Garg",
            "Ramesh Karri",
            "Hammond Pearce"
        ],
        "published": "2023-05-22T17:13:33Z",
        "summary": "Modern hardware design starts with specifications provided in natural\nlanguage. These are then translated by hardware engineers into appropriate\nHardware Description Languages (HDLs) such as Verilog before synthesizing\ncircuit elements. Automating this translation could reduce sources of human\nerror from the engineering process. But, it is only recently that artificial\nintelligence (AI) has demonstrated capabilities for machine-based end-to-end\ndesign translations. Commercially-available instruction-tuned Large Language\nModels (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to\nproduce code in a variety of programming languages; but studies examining them\nfor hardware are still lacking. In this work, we thus explore the challenges\nfaced and opportunities presented when leveraging these recent advances in LLMs\nfor hardware design. Given that these `conversational' LLMs perform best when\nused interactively, we perform a case study where a hardware engineer\nco-architects a novel 8-bit accumulator-based microprocessor architecture with\nthe LLM according to real-world hardware constraints. We then sent the\nprocessor to tapeout in a Skywater 130nm shuttle, meaning that this `Chip-Chat'\nresulted in what we believe to be the world's first wholly-AI-written HDL for\ntapeout.",
        "pdf_link": "https://arxiv.org/pdf/2305.13243v2.pdf"
    },
    {
        "title": "Deepfake Text Detection in the Wild",
        "authors": [
            "Yafu Li",
            "Qintong Li",
            "Leyang Cui",
            "Wei Bi",
            "Longyue Wang",
            "Linyi Yang",
            "Shuming Shi",
            "Yue Zhang"
        ],
        "published": "2023-05-22T17:13:29Z",
        "summary": "Recent advances in large language models have enabled them to reach a level\nof text generation comparable to that of humans. These models show powerful\ncapabilities across a wide range of content, including news article writing,\nstory generation, and scientific writing. Such capability further narrows the\ngap between human-authored and machine-generated texts, highlighting the\nimportance of deepfake text detection to avoid potential risks such as fake\nnews propagation and plagiarism. However, previous work has been limited in\nthat they testify methods on testbed of specific domains or certain language\nmodels. In practical scenarios, the detector faces texts from various domains\nor LLMs without knowing their sources. To this end, we build a wild testbed by\ngathering texts from various human writings and deepfake texts generated by\ndifferent LLMs. Human annotators are only slightly better than random guessing\nat identifying machine-generated texts. Empirical results on automatic\ndetection methods further showcase the challenges of deepfake text detection in\na wild testbed. In addition, out-of-distribution poses a greater challenge for\na detector to be employed in realistic application scenarios. We release our\nresources at https://github.com/yafuly/DeepfakeTextDetect.",
        "pdf_link": "https://arxiv.org/pdf/2305.13242v1.pdf"
    },
    {
        "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
        "authors": [
            "Fuzhao Xue",
            "Yao Fu",
            "Wangchunshu Zhou",
            "Zangwei Zheng",
            "Yang You"
        ],
        "published": "2023-05-22T17:02:15Z",
        "summary": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_link": "https://arxiv.org/pdf/2305.13230v2.pdf"
    },
    {
        "title": "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Deng Cai",
            "Xinting Huang",
            "Tao Fang",
            "Wei Bi"
        ],
        "published": "2023-05-22T16:56:44Z",
        "summary": "Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered\nsignificant attention due to their exceptional capabilities in handling a\ndiverse range of tasks. Recent studies demonstrate that open-sourced smaller\nfoundational models, such as 7B-size LLaMA, can also display remarkable\nproficiency in tackling diverse tasks when fine-tuned using instruction-driven\ndata. In this work, we investigate a practical problem setting where the\nprimary focus is on one or a few particular tasks rather than general-purpose\ninstruction following, and explore whether LLMs can be beneficial and further\nimproved for such targeted scenarios. We choose the writing-assistant scenario\nas the testbed, which includes seven writing tasks. We collect training data\nfor these tasks, reframe them in an instruction-following format, and\nsubsequently refine the LLM, specifically LLaMA, via instruction tuning.\nExperimental results show that fine-tuning LLaMA on writing instruction data\nsignificantly improves its ability on writing tasks. We also conduct more\nexperiments and analyses to offer insights for future work on effectively\nfine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion\nregarding the necessity of employing LLMs for only one targeted task, taking\ninto account the efforts required for tuning and the resources consumed during\ndeployment.",
        "pdf_link": "https://arxiv.org/pdf/2305.13225v2.pdf"
    },
    {
        "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
        "authors": [
            "Xinyuan Lu",
            "Liangming Pan",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan"
        ],
        "published": "2023-05-22T16:13:50Z",
        "summary": "Current scientific fact-checking benchmarks exhibit several shortcomings,\nsuch as biases arising from crowd-sourced claims and an over-reliance on\ntext-based evidence. We present SCITAB, a challenging evaluation dataset\nconsisting of 1.2K expert-verified scientific claims that 1) originate from\nauthentic scientific publications and 2) require compositional reasoning for\nverification. The claims are paired with evidence-containing scientific tables\nannotated with labels. Through extensive evaluations, we demonstrate that\nSCITAB poses a significant challenge to state-of-the-art models, including\ntable-based pretraining models and large language models. All models except\nGPT-4 achieved performance barely above random guessing. Popular prompting\ntechniques, such as Chain-of-Thought, do not achieve much performance gains on\nSCITAB. Our analysis uncovers several unique challenges posed by SCITAB,\nincluding table grounding, claim ambiguity, and compositional reasoning. Our\ncodes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
        "pdf_link": "https://arxiv.org/pdf/2305.13186v3.pdf"
    },
    {
        "title": "Teaching Probabilistic Logical Reasoning to Transformers",
        "authors": [
            "Aliakbar Nafar",
            "Kristen Brent Venable",
            "Parisa Kordjamshidi"
        ],
        "published": "2023-05-22T16:08:20Z",
        "summary": "In this paper, we evaluate the capability of transformer-based language\nmodels in making inferences over uncertain text that includes uncertain rules\nof reasoning. We cover both Pre-trained Language Models (PLMs) and generative\nLarge Language Models (LLMs). Our evaluation results show that both generations\nof language models struggle with reasoning over uncertain text. We propose a\nnovel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT),\nthat utilizes probabilistic logical rules as constraints in the fine-tuning\nphase without relying on these rules in the inference stage. To assess the\neffectiveness of PCT, we utilize the related corpora and, additionally, create\na new and more challenging benchmark that, unlike the previous ones, uses\ninstance-specific rules. Our study demonstrates that PCT improves the\ntransformer-based language model's intrinsic reasoning and makes their\nprobabilistic logical reasoning process more explicit and explainable.\nFurthermore, PCT equips these models to effectively handle novel situations,\nincluding higher reasoning depth, new domains, and complex probabilistic\nstructures.",
        "pdf_link": "https://arxiv.org/pdf/2305.13179v2.pdf"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Bozhong Tian",
            "Siyuan Cheng",
            "Zhoubo Li",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T16:00:00Z",
        "summary": "Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.",
        "pdf_link": "https://arxiv.org/pdf/2305.13172v3.pdf"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities",
        "authors": [
            "Yuqi Zhu",
            "Xiaohan Wang",
            "Jing Chen",
            "Shuofei Qiao",
            "Yixin Ou",
            "Yunzhi Yao",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T15:56:44Z",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.",
        "pdf_link": "https://arxiv.org/pdf/2305.13168v2.pdf"
    },
    {
        "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
        "authors": [
            "Boshi Wang",
            "Xiang Yue",
            "Huan Sun"
        ],
        "published": "2023-05-22T15:47:31Z",
        "summary": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive\nperformance in complex reasoning tasks. However, it is difficult to know\nwhether the models are reasoning based on deep understandings of truth and\nlogic, or leveraging their memorized patterns in a relatively superficial way.\nIn this work, we explore testing LLMs' reasoning by engaging with them in a\ndebate-like conversation, where given a question, the LLM and the user need to\ndiscuss to make the correct decision starting from opposing arguments. Upon\nmitigating the Clever Hans effect, our task requires the LLM to not only\nachieve the correct answer on its own, but also be able to hold and defend its\nbelief instead of blindly believing or getting misled by the user's (invalid)\narguments and critiques, thus testing in greater depth whether the LLM grasps\nthe essence of the reasoning required to solve the problem. Across a range of\ncomplex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench\ntasks, we find that despite their impressive performance as reported in\nexisting work on generating correct step-by-step solutions in the beginning,\nLLMs like ChatGPT cannot maintain their beliefs in truth for a significant\nportion of examples when challenged by oftentimes absurdly invalid arguments.\nOur work points to danger zones of model alignment, and also suggests more\ncareful treatments and interpretations of the recent findings that LLMs can\nimprove their responses based on feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.13160v2.pdf"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
        "authors": [
            "Xiaolei Wang",
            "Xinyu Tang",
            "Wayne Xin Zhao",
            "Jingyuan Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-22T15:12:43Z",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "pdf_link": "https://arxiv.org/pdf/2305.13112v2.pdf"
    },
    {
        "title": "Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",
        "authors": [
            "Katherine Abramski",
            "Salvatore Citraro",
            "Luigi Lombardi",
            "Giulio Rossetti",
            "Massimo Stella"
        ],
        "published": "2023-05-22T15:06:51Z",
        "summary": "Large language models are becoming increasingly integrated into our lives.\nHence, it is important to understand the biases present in their outputs in\norder to avoid perpetuating harmful stereotypes, which originate in our own\nflawed ways of thinking. This challenge requires developing new benchmarks and\nmethods for quantifying affective and semantic bias, keeping in mind that LLMs\nact as psycho-social mirrors that reflect the views and tendencies that are\nprevalent in society. One such tendency that has harmful negative effects is\nthe global phenomenon of anxiety toward math and STEM subjects. Here, we\ninvestigate perceptions of math and STEM fields provided by cutting-edge\nlanguage models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach\nfrom network science and cognitive psychology. Specifically, we use behavioral\nforma mentis networks (BFMNs) to understand how these LLMs frame math and STEM\ndisciplines in relation to other concepts. We use data obtained by probing the\nthree LLMs in a language generation task that has previously been applied to\nhumans. Our findings indicate that LLMs have an overall negative perception of\nmath and STEM fields, with math being perceived most negatively. We observe\nsignificant differences across the three LLMs. We observe that newer versions\n(i.e. GPT-4) produce richer, more complex perceptions as well as less negative\nperceptions compared to older versions and N=159 high-school students. These\nfindings suggest that advances in the architecture of LLMs may lead to\nincreasingly less biased models that could even perhaps someday aid in reducing\nharmful stereotypes in society rather than perpetuating them.",
        "pdf_link": "https://arxiv.org/pdf/2305.18320v1.pdf"
    },
    {
        "title": "Observations on LLMs for Telecom Domain: Capabilities and Limitations",
        "authors": [
            "Sumit Soman",
            "Ranjani H G"
        ],
        "published": "2023-05-22T15:04:16Z",
        "summary": "The landscape for building conversational interfaces (chatbots) has witnessed\na paradigm shift with recent developments in generative Artificial Intelligence\n(AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and\nGPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In\nthis paper, we analyze capabilities and limitations of incorporating such\nmodels in conversational interfaces for the telecommunication domain,\nspecifically for enterprise wireless products and services. Using Cradlepoint's\npublicly available data for our experiments, we present a comparative analysis\nof the responses from such models for multiple use-cases including domain\nadaptation for terminology and product taxonomy, context continuity, robustness\nto input perturbations and errors. We believe this evaluation would provide\nuseful insights to data scientists engaged in building customized\nconversational interfaces for domain-specific requirements.",
        "pdf_link": "https://arxiv.org/pdf/2305.13102v1.pdf"
    },
    {
        "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
        "authors": [
            "Chenhui Shen",
            "Liying Cheng",
            "Xuan-Phi Nguyen",
            "Yang You",
            "Lidong Bing"
        ],
        "published": "2023-05-22T14:58:13Z",
        "summary": "With the recent undeniable advancement in reasoning abilities in large\nlanguage models (LLMs) like ChatGPT and GPT-4, there is a growing trend for\nusing LLMs on various tasks. One area where LLMs can be employed is as an\nalternative evaluation metric for complex generative tasks, which generally\ndemands expensive human judges to complement the traditional automatic metrics\nfor various evaluation dimensions such as fluency and consistency. In this\nwork, we conduct extensive analysis to investigate the stability and\nreliability of LLMs as automatic evaluators for abstractive summarization. We\nfound that while ChatGPT and GPT-4 outperform the commonly used automatic\nmetrics, they are not ready as human replacements due to significant\nlimitations. That is, LLM evaluators rate each candidate system inconsistently\nand are dimension-dependent. They also struggle to compare candidates with\nclose performance and become more unreliable with higher-quality summaries by\nobtaining a lower correlation with humans. In other words, with better\nabstractive summarization systems being introduced at a fast pace, LLMs may\nresult in misleading and unreliable evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.13091v2.pdf"
    },
    {
        "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
        "authors": [
            "Yichong Xu",
            "Ruochen Xu",
            "Dan Iter",
            "Yang Liu",
            "Shuohang Wang",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023-05-22T14:52:32Z",
        "summary": "While large models such as GPT-3 demonstrate exceptional performance in\nzeroshot and fewshot summarization tasks, their extensive serving and\nfine-tuning costs hinder their utilization in various applications. Conversely,\nprevious studies have found that although automatic metrics tend to favor\nsmaller fine-tuned models, the quality of the summaries they generate is\ninferior to that of larger models like GPT-3 when assessed by human evaluators.\nTo address this issue, we propose InheritSumm, a versatile and compact\nsummarization model derived from GPT-3.5 through distillation. InheritSumm not\nonly exhibits comparable zeroshot and fewshot summarization capabilities to\nGPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental\nresults demonstrate that InheritSumm achieves similar or superior performance\nto GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the\npreviously established best small models in both prefix-tuning and full-data\nfine-tuning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.13083v1.pdf"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Chengfei Lv",
            "Qianghuai Jia",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T14:37:05Z",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.",
        "pdf_link": "https://arxiv.org/pdf/2305.13068v3.pdf"
    },
    {
        "title": "Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study",
        "authors": [
            "Yuan Sui",
            "Mengyu Zhou",
            "Mingjie Zhou",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-05-22T14:23:46Z",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, there is still much to\nlearn about how well LLMs understand structured data, such as tables. Although\ntables can be used as input to LLMs with serialization, there is a lack of\ncomprehensive studies that examine whether LLMs can truly comprehend such data.\nIn this paper, we try to understand this by designing a benchmark to evaluate\nthe structural understanding capabilities (SUC) of LLMs. The benchmark we\ncreate includes seven tasks, each with its own unique challenges, e.g., cell\nlookup, row retrieval, and size detection. We perform a series of evaluations\non GPT-3.5 and GPT-4. We find that performance varied depending on several\ninput choices, including table input format, content order, role prompting, and\npartition marks. Drawing from the insights gained through the benchmark\nevaluations, we propose \\textit{self-augmentation} for effective structural\nprompting, such as critical value / range identification using internal\nknowledge of LLMs. When combined with carefully chosen input choices, these\nstructural prompting methods lead to promising improvements in LLM performance\non a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe that our open source benchmark and\nproposed prompting methods can serve as a simple yet generic selection for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2305.13062v4.pdf"
    },
    {
        "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
        "authors": [
            "Mark Mets",
            "Andres Karjus",
            "Indrek Ibrus",
            "Maximilian Schich"
        ],
        "published": "2023-05-22T13:56:35Z",
        "summary": "Automated stance detection and related machine learning methods can provide\nuseful insights for media monitoring and academic research. Many of these\napproaches require annotated training datasets, which limits their\napplicability for languages where these may not be readily available. This\npaper explores the applicability of large language models for automated stance\ndetection in a challenging scenario, involving a morphologically complex,\nlower-resource language, and a socio-culturally complex topic, immigration. If\nthe approach works in this case, it can be expected to perform as well or\nbetter in less demanding scenarios. We annotate a large set of pro and\nanti-immigration examples, and compare the performance of multiple language\nmodels as supervised learners. We also probe the usability of ChatGPT as an\ninstructable zero-shot classifier for the same task. Supervised achieves\nacceptable performance, and ChatGPT yields similar accuracy. This is promising\nas a potentially simpler and cheaper alternative for text classification tasks,\nincluding in lower-resource languages. We further use the best-performing model\nto investigate diachronic trends over seven years in two corpora of Estonian\nmainstream and right-wing populist news sources, demonstrating the\napplicability of the approach for news analytics and media monitoring settings,\nand discuss correspondences between stance changes and real-world events.",
        "pdf_link": "https://arxiv.org/pdf/2305.13047v1.pdf"
    },
    {
        "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models",
        "authors": [
            "Jiaxi Yang",
            "Binyuan Hui",
            "Min Yang",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-05-22T13:18:17Z",
        "summary": "Large language models (LLMs) have exhibited an emergent in-context learning\n(ICL) ability. However, the ICL models that can solve ordinary cases are hardly\nextended to solve more complex tasks by processing the demonstration examples\nonce. This single-turn ICL is incoordinate with the decision making process of\nhumans by learning from analogy. In this paper, we propose an effective and\nefficient two-stage framework to boost ICL in LLMs by exploiting a dual form\nbetween Transformer attention and gradient descent-based optimization.\nConcretely, we divide the ICL process into \"Deep-Thinking\" and inference\nstages. The \"Deep-Thinking\" stage performs iterative forward optimization of\ndemonstrations, which is expected to boost the reasoning abilities of LLMs at\ntest time by \"thinking\" demonstrations multiple times. It produces accumulated\nmeta-gradients by manipulating the Key-Value matrices in the self-attention\nmodules of the Transformer. Then, the inference stage only takes the test query\nas input without concatenating demonstrations and applies the learned\nmeta-gradients through attention for output prediction. In this way,\ndemonstrations are not required during the inference stage since they are\nalready learned and stored in the definitive meta-gradients. LLMs can be\neffectively and efficiently adapted to downstream tasks. Extensive experiments\non ten classification and multiple-choice datasets show that our method\nachieves substantially better performance than standard ICL in terms of both\naccuracy and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.13016v2.pdf"
    },
    {
        "title": "Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model",
        "authors": [
            "Stefano De Paoli"
        ],
        "published": "2023-05-22T13:16:07Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful generative Artificial\nIntelligence solutions which can be applied to several fields and areas of\nwork. This paper presents results and reflection of an experiment done to use\nthe model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic\nAnalysis. Previous research on this subject has largely worked on conducting\ndeductive analysis. Thematic Analysis is a qualitative method for analysis\ncommonly used in social sciences and it is based on interpretations made by the\nhuman analyst(s) and the identification of explicit and latent meanings in\nqualitative data. Attempting an analysis based on human interpretation with an\nLLM clearly is a provocation but also a way to learn something about how these\nsystems can or cannot be used in qualitative research. The paper presents the\nmotivations for attempting this emulation, it reflects on how the six steps to\na Thematic Analysis proposed by Braun and Clarke can at least partially be\nreproduced with the LLM and it also reflects on what are the outputs produced\nby the model. The paper used two existing datasets of open access\nsemi-structured interviews, previously analysed with Thematic Analysis by other\nresearchers. It used the previously produced analysis (and the related themes)\nto compare with the results produced by the LLM. The results show that the\nmodel can infer at least partially some of the main Themes. The objective of\nthe paper is not to replace human analysts in qualitative analysis but to learn\nif some elements of LLM data manipulation can to an extent be of support for\nqualitative research.",
        "pdf_link": "https://arxiv.org/pdf/2305.13014v4.pdf"
    },
    {
        "title": "Text-based Person Search without Parallel Image-Text Data",
        "authors": [
            "Yang Bai",
            "Jingyao Wang",
            "Min Cao",
            "Chen Chen",
            "Ziqiang Cao",
            "Liqiang Nie",
            "Min Zhang"
        ],
        "published": "2023-05-22T12:13:08Z",
        "summary": "Text-based person search (TBPS) aims to retrieve the images of the target\nperson from a large image gallery based on a given natural language\ndescription. Existing methods are dominated by training models with parallel\nimage-text pairs, which are very costly to collect. In this paper, we make the\nfirst attempt to explore TBPS without parallel image-text data ($\\mu$-TBPS), in\nwhich only non-parallel images and texts, or even image-only data, can be\nadopted. Towards this end, we propose a two-stage framework,\ngeneration-then-retrieval (GTR), to first generate the corresponding pseudo\ntext for each image and then perform the retrieval in a supervised manner. In\nthe generation stage, we propose a fine-grained image captioning strategy to\nobtain an enriched description of the person image, which firstly utilizes a\nset of instruction prompts to activate the off-the-shelf pretrained\nvision-language model to capture and generate fine-grained person attributes,\nand then converts the extracted attributes into a textual description via the\nfinetuned large language model or the hand-crafted template. In the retrieval\nstage, considering the noise interference of the generated texts for training\nmodel, we develop a confidence score-based training scheme by enabling more\nreliable texts to contribute more during the training. Experimental results on\nmultiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that\nthe proposed GTR can achieve a promising performance without relying on\nparallel image-text data.",
        "pdf_link": "https://arxiv.org/pdf/2305.12964v2.pdf"
    },
    {
        "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
        "authors": [
            "Jan Cegin",
            "Jakub Simko",
            "Peter Brusilovsky"
        ],
        "published": "2023-05-22T11:46:32Z",
        "summary": "The emergence of generative large language models (LLMs) raises the question:\nwhat will be its impact on crowdsourcing? Traditionally, crowdsourcing has been\nused for acquiring solutions to a wide variety of human-intelligence tasks,\nincluding ones involving text generation, modification or evaluation. For some\nof these tasks, models like ChatGPT can potentially substitute human workers.\nIn this study, we investigate whether this is the case for the task of\nparaphrase generation for intent classification. We apply data collection\nmethodology of an existing crowdsourcing study (similar scale, prompts and seed\ndata) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases\nare more diverse and lead to at least as robust models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12947v2.pdf"
    },
    {
        "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
        "authors": [
            "Dongfang Li",
            "Jindi Yu",
            "Baotian Hu",
            "Zhenran Xu",
            "Min Zhang"
        ],
        "published": "2023-05-22T11:45:42Z",
        "summary": "As ChatGPT and GPT-4 spearhead the development of Large Language Models\n(LLMs), more researchers are investigating their performance across various\ntasks. But more research needs to be done on the interpretability capabilities\nof LLMs, that is, the ability to generate reasons after an answer has been\ngiven. Existing explanation datasets are mostly English-language general\nknowledge questions, which leads to insufficient thematic and linguistic\ndiversity. To address the language bias and lack of medical resources in\ngenerating rationales QA datasets, we present ExplainCPE (over 7k instances), a\nchallenging medical benchmark in Simplified Chinese. We analyzed the errors of\nChatGPT and GPT-4, pointing out the limitations of current LLMs in\nunderstanding text and computational reasoning. During the experiment, we also\nfound that different LLMs have different preferences for in-context learning.\nExplainCPE presents a significant challenge, but its potential for further\ninvestigation is promising, and it can be used to evaluate the ability of a\nmodel to generate explanations. AI safety and trustworthiness need more\nattention, and this work makes the first step to explore the medical\ninterpretability of LLMs.The dataset is available at\nhttps://github.com/HITsz-TMG/ExplainCPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.12945v2.pdf"
    },
    {
        "title": "Album Storytelling with Iterative Story-aware Captioning and Large Language Models",
        "authors": [
            "Munan Ning",
            "Yujia Xie",
            "Dongdong Chen",
            "Zeyin Song",
            "Lu Yuan",
            "Yonghong Tian",
            "Qixiang Ye",
            "Li Yuan"
        ],
        "published": "2023-05-22T11:45:10Z",
        "summary": "This work studies how to transform an album to vivid and coherent stories, a\ntask we refer to as \"album storytelling\". While this task can help preserve\nmemories and facilitate experience sharing, it remains an underexplored area in\ncurrent literature. With recent advances in Large Language Models (LLMs), it is\nnow possible to generate lengthy, coherent text, opening up the opportunity to\ndevelop an AI assistant for album storytelling. One natural approach is to use\ncaption models to describe each photo in the album, and then use LLMs to\nsummarize and rewrite the generated captions into an engaging story. However,\nwe find this often results in stories containing hallucinated information that\ncontradicts the images, as each generated caption (\"story-agnostic\") is not\nalways about the description related to the whole story or miss some necessary\ninformation. To address these limitations, we propose a new iterative album\nstorytelling pipeline. Specifically, we start with an initial story and build a\nstory-aware caption model to refine the captions using the whole story as\nguidance. The polished captions are then fed into the LLMs to generate a new\nrefined story. This process is repeated iteratively until the story contains\nminimal factual errors while maintaining coherence. To evaluate our proposed\npipeline, we introduce a new dataset of image collections from vlogs and a set\nof systematic evaluation metrics. Our results demonstrate that our method\neffectively generates more accurate and engaging stories for albums, with\nenhanced coherence and vividness.",
        "pdf_link": "https://arxiv.org/pdf/2305.12943v2.pdf"
    },
    {
        "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Chunkit Chan",
            "Mingyang Chen",
            "Wei Wang"
        ],
        "published": "2023-05-22T09:49:16Z",
        "summary": "The practice of transferring knowledge from a sophisticated, proprietary\nlarge language model (LLM) to a compact, open-source LLM has garnered\nconsiderable attention. Previous works have focused on a unidirectional\nknowledge distillation way by aligning the responses of the student model with\nthose of the teacher model to a set of instructions. Nevertheless, they\noverlooked the possibility of incorporating any reciprocal\n\"feedback\"--identifying challenging instructions where the student model's\nperformance falls short--to boost the student model's proficiency iteratively.\nTo this end, we propose a novel adversarial distillation framework for a more\nefficient knowledge transfer. Leveraging the versatile role adaptability of\nLLMs, we prompt the teacher model to identify \"hard\" instructions and generate\nnew \"hard\" instructions for the student model, creating a three-stage\nadversarial loop of imitation, discrimination, and generation. By applying this\nadversarial framework, we successfully transfer knowledge from ChatGPT to a\nstudent model (named Lion), using a mere 70k training data. Our results show\nthat Lion-13B not only achieves comparable open-ended generation capabilities\nto ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned\nmodels like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks\nsuch as BIG-Bench Hard (BBH) and 16.7% on AGIEval. Code and model can be found\nat https://github.com/YJiangcm/Lion.",
        "pdf_link": "https://arxiv.org/pdf/2305.12870v2.pdf"
    },
    {
        "title": "Automatic Code Summarization via ChatGPT: How Far Are We?",
        "authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yudu You",
            "Yun Miao",
            "Yi Liu",
            "Yuekang Li",
            "Gelei Deng",
            "Shenghan Huang",
            "Yuchen Chen",
            "Quanjun Zhang",
            "Hanwei Qian",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "published": "2023-05-22T09:43:40Z",
        "summary": "To support software developers in understanding and maintaining programs,\nvarious automatic code summarization techniques have been proposed to generate\na concise natural language comment for a given code snippet. Recently, the\nemergence of large language models (LLMs) has led to a great boost in the\nperformance of natural language processing tasks. Among them, ChatGPT is the\nmost popular one which has attracted wide attention from the software\nengineering community. However, it still remains unclear how ChatGPT performs\nin (automatic) code summarization. Therefore, in this paper, we focus on\nevaluating ChatGPT on a widely-used Python dataset called CSN-Python and\ncomparing it with several state-of-the-art (SOTA) code summarization models.\nSpecifically, we first explore an appropriate prompt to guide ChatGPT to\ngenerate in-distribution comments. Then, we use such a prompt to ask ChatGPT to\ngenerate comments for all code snippets in the CSN-Python test set. We adopt\nthree widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the\nquality of the comments generated by ChatGPT and SOTA models (including NCS,\nCodeBERT, and CodeT5). The experimental results show that in terms of BLEU and\nROUGE-L, ChatGPT's code summarization performance is significantly worse than\nall three SOTA models. We also present some cases and discuss the advantages\nand disadvantages of ChatGPT in code summarization. Based on the findings, we\noutline several open challenges and opportunities in ChatGPT-based code\nsummarization.",
        "pdf_link": "https://arxiv.org/pdf/2305.12865v1.pdf"
    },
    {
        "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
        "authors": [
            "Vaishali Pal",
            "Andrew Yates",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "published": "2023-05-22T08:25:15Z",
        "summary": "Recent advances in tabular question answering (QA) with large language models\nare constrained in their coverage and only answer questions over a single\ntable. However, real-world queries are complex in nature, often over multiple\ntables in a relational database or web page. Single table questions do not\ninvolve common table operations such as set operations, Cartesian products\n(joins), or nested queries. Furthermore, multi-table operations often result in\na tabular output, which necessitates table generation capabilities of tabular\nQA models. To fill this gap, we propose a new task of answering questions over\nmultiple tables. Our model, MultiTabQA, not only answers questions over\nmultiple tables, but also generalizes to generate tabular answers. To enable\neffective training, we build a pre-training dataset comprising of 132,645 SQL\nqueries and tabular answers. Further, we evaluate the generated tables by\nintroducing table-specific metrics of varying strictness assessing various\nlevels of granularity of the table structure. MultiTabQA outperforms\nstate-of-the-art single table QA models adapted to a multi-table QA setting by\nfinetuning on three datasets: Spider, Atis and GeoQuery.",
        "pdf_link": "https://arxiv.org/pdf/2305.12820v2.pdf"
    },
    {
        "title": "LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space",
        "authors": [
            "Chi Han",
            "Jialiang Xu",
            "Manling Li",
            "Yi Fung",
            "Chenkai Sun",
            "Nan Jiang",
            "Tarek Abdelzaher",
            "Heng Ji"
        ],
        "published": "2023-05-22T07:52:04Z",
        "summary": "In recent years, large language models (LMs) have achieved remarkable\nprogress across various natural language processing tasks. As pre-training and\nfine-tuning are costly and might negatively impact model performance, it is\ndesired to efficiently adapt an existing model to different conditions such as\nstyles, sentiments or narratives, when facing different audiences or scenarios.\nHowever, efficient adaptation of a language model to diverse conditions remains\nan open challenge. This work is inspired by the observation that text\nconditions are often associated with selection of certain words in a context.\nTherefore we introduce LM-Switch, a theoretically grounded, lightweight and\nsimple method for generative language model conditioning. We begin by\ninvestigating the effect of conditions in Hidden Markov Models (HMMs), and\nestablish a theoretical connection with language model. Our finding suggests\nthat condition shifts in HMMs are associated with linear transformations in\nword embeddings. LM-Switch is then designed to deploy a learnable linear factor\nin the word embedding space for language model conditioning. We show that\nLM-Switch can model diverse tasks, and achieves comparable or better\nperformance compared with state-of-the-art baselines in LM detoxification and\ngeneration control, despite requiring no more than 1% of parameters compared\nwith baselines and little extra time overhead compared with base LMs. It is\nalso able to learn from as few as a few sentences or one document. Moreover, a\nlearned LM-Switch can be transferred to other LMs of different sizes, achieving\na detoxification performance similar to the best baseline. We will make our\ncode available to the research community following publication.",
        "pdf_link": "https://arxiv.org/pdf/2305.12798v1.pdf"
    },
    {
        "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
        "authors": [
            "Pengcheng Jiang",
            "Cao Xiao",
            "Adam Cross",
            "Jimeng Sun"
        ],
        "published": "2023-05-22T07:35:43Z",
        "summary": "Clinical predictive models often rely on patients' electronic health records\n(EHR), but integrating medical knowledge to enhance predictions and\ndecision-making is challenging. This is because personalized predictions\nrequire personalized knowledge graphs (KGs), which are difficult to generate\nfrom patient EHR data. To address this, we propose \\textsc{GraphCare}, an\nopen-world framework that uses external KGs to improve EHR-based predictions.\nOur method extracts knowledge from large language models (LLMs) and external\nbiomedical KGs to build patient-specific KGs, which are then used to train our\nproposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare\npredictions. On two public datasets, MIMIC-III and MIMIC-IV, \\textsc{GraphCare}\nsurpasses baselines in four vital healthcare prediction tasks: mortality,\nreadmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it\nboosts AUROC by 17.6\\% and 6.6\\% for mortality and readmission, and F1-score by\n7.9\\% and 10.8\\% for LOS and drug recommendation, respectively. Notably,\n\\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited\ndata availability. Our findings highlight the potential of using external KGs\nin healthcare prediction tasks and demonstrate the promise of\n\\textsc{GraphCare} in generating personalized KGs for promoting personalized\nmedicine.",
        "pdf_link": "https://arxiv.org/pdf/2305.12788v3.pdf"
    },
    {
        "title": "Explaining Emergent In-Context Learning as Kernel Regression",
        "authors": [
            "Chi Han",
            "Ziqi Wang",
            "Han Zhao",
            "Heng Ji"
        ],
        "published": "2023-05-22T06:45:02Z",
        "summary": "Large language models (LLMs) have initiated a paradigm shift in transfer\nlearning. In contrast to the classic pretraining-then-finetuning procedure, in\norder to use LLMs for downstream prediction tasks, one only needs to provide a\nfew demonstrations, known as in-context examples, without adding more or\nupdating existing model parameters. This in-context learning (ICL) capability\nof LLMs is intriguing, and it is not yet fully understood how pretrained LLMs\nacquire such capabilities. In this paper, we investigate the reason why a\ntransformer-based language model can accomplish in-context learning after\npre-training on a general language corpus by proposing one hypothesis that LLMs\ncan simulate kernel regression with internal representations when faced with\nin-context examples. More concretely, we first prove that Bayesian inference on\nin-context prompts can be asymptotically understood as kernel regression $\\hat\ny = \\sum_i y_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context\ndemonstrations grows. Then, we empirically investigate the in-context behaviors\nof language models. We find that during ICL, the attention and hidden features\nin LLMs match the behaviors of a kernel regression. Finally, our theory\nprovides insights into multiple phenomena observed in the ICL field: why\nretrieving demonstrative samples similar to test samples can help, why ICL\nperformance is sensitive to the output formats, and why ICL accuracy benefits\nfrom selecting in-distribution and representative samples.",
        "pdf_link": "https://arxiv.org/pdf/2305.12766v2.pdf"
    },
    {
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "authors": [
            "Liangming Pan",
            "Xiaobao Wu",
            "Xinyuan Lu",
            "Anh Tuan Luu",
            "William Yang Wang",
            "Min-Yen Kan",
            "Preslav Nakov"
        ],
        "published": "2023-05-22T06:11:15Z",
        "summary": "Fact-checking real-world claims often requires collecting multiple pieces of\nevidence and applying complex multi-step reasoning. In this paper, we present\nProgram-Guided Fact-Checking (ProgramFC), a novel fact-checking model that\ndecomposes complex claims into simpler sub-tasks that can be solved using a\nshared library of specialized functions. We first leverage the in-context\nlearning ability of large language models to generate reasoning programs to\nguide the verification process. Afterward, we execute the program by delegating\neach sub-task to the corresponding sub-task handler. This process makes our\nmodel both explanatory and data-efficient, providing clear explanations of its\nreasoning process and requiring minimal training data. We evaluate ProgramFC on\ntwo challenging fact-checking datasets and show that it outperforms seven\nfact-checking baselines across different settings of evidence availability,\nwith explicit output programs that benefit human debugging. Our codes and data\nare publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "pdf_link": "https://arxiv.org/pdf/2305.12744v1.pdf"
    },
    {
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "authors": [
            "Ce Zheng",
            "Lei Li",
            "Qingxiu Dong",
            "Yuxuan Fan",
            "Zhiyong Wu",
            "Jingjing Xu",
            "Baobao Chang"
        ],
        "published": "2023-05-22T06:07:58Z",
        "summary": "Previous studies have shown that large language models (LLMs) like GPTs store\nmassive factual knowledge in their parameters. However, the stored knowledge\ncould be false or out-dated. Traditional knowledge editing methods refine LLMs\nvia fine-tuning on texts containing specific knowledge. However, with the\nincreasing scales of LLMs, these gradient-based approaches bring large\ncomputation costs. The trend of model-as-a-service also makes it impossible to\nmodify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new\nparadigm based on demonstration contexts without parameter updating, we explore\nwhether ICL can edit factual knowledge. To answer this question, we give a\ncomprehensive empirical study of ICL strategies. Experiments show that\nin-context knowledge editing (IKE), without any gradient and parameter\nupdating, achieves a competitive success rate compared to gradient-based\nmethods on GPT-J (6B) but with much fewer side effects, including less\nover-editing on similar but unrelated facts and less knowledge forgetting on\npreviously stored knowledge. We also apply the method to larger LMs with tens\nor hundreds of parameters like OPT-175B, which shows the scalability of our\nmethod. The code is available at https://github.com/Zce1112zslx/IKE.",
        "pdf_link": "https://arxiv.org/pdf/2305.12740v1.pdf"
    },
    {
        "title": "llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "authors": [
            "Masanori Hirano",
            "Masahiro Suzuki",
            "Hiroki Sakaji"
        ],
        "published": "2023-05-22T04:59:33Z",
        "summary": "This study constructed a Japanese chat dataset for tuning large language\nmodels (LLMs), which consist of about 8.4 million records. Recently, LLMs have\nbeen developed and gaining popularity. However, high-performing LLMs are\nusually mainly for English. There are two ways to support languages other than\nEnglish by those LLMs: constructing LLMs from scratch or tuning existing\nmodels. However, in both ways, datasets are necessary parts. In this study, we\nfocused on supporting Japanese in those LLMs and making a dataset for training\nor tuning LLMs in Japanese. The dataset we constructed consisted of various\ntasks, such as translation and knowledge tasks. In our experiment, we tuned an\nexisting LLM using our dataset and evaluated the performance qualitatively. The\nresults suggest that our dataset is possibly beneficial for LLMs. However, we\nalso revealed some difficulties in constructing LLMs in languages other than\nEnglish.",
        "pdf_link": "https://arxiv.org/pdf/2305.12720v1.pdf"
    },
    {
        "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
        "authors": [
            "Hanyin Shao",
            "Jie Huang",
            "Shen Zheng",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-05-22T04:30:35Z",
        "summary": "The advancement of large language models (LLMs) brings notable improvements\nacross various applications, while simultaneously raising concerns about\npotential private data exposure. One notable capability of LLMs is their\nability to form associations between different pieces of information, but this\nraises concerns when it comes to personally identifiable information (PII).\nThis paper delves into the association capabilities of language models, aiming\nto uncover the factors that influence their proficiency in associating\ninformation. Our study reveals that as models scale up, their capacity to\nassociate entities/information intensifies, particularly when target pairs\ndemonstrate shorter co-occurrence distances or higher co-occurrence\nfrequencies. However, there is a distinct performance gap when associating\ncommonsense knowledge versus PII, with the latter showing lower accuracy.\nDespite the proportion of accurately predicted PII being relatively small, LLMs\nstill demonstrate the capability to predict specific instances of email\naddresses and phone numbers when provided with appropriate prompts. These\nfindings underscore the potential risk to PII confidentiality posed by the\nevolving capabilities of LLMs, especially as they continue to expand in scale\nand power.",
        "pdf_link": "https://arxiv.org/pdf/2305.12707v2.pdf"
    },
    {
        "title": "G3Detector: General GPT-Generated Text Detector",
        "authors": [
            "Haolan Zhan",
            "Xuanli He",
            "Qiongkai Xu",
            "Yuxiang Wu",
            "Pontus Stenetorp"
        ],
        "published": "2023-05-22T03:35:00Z",
        "summary": "The burgeoning progress in the field of Large Language Models (LLMs) heralds\nsignificant benefits due to their unparalleled capacities. However, it is\ncritical to acknowledge the potential misuse of these models, which could give\nrise to a spectrum of social and ethical dilemmas. Despite numerous preceding\nefforts centered around distinguishing synthetic text, most existing detection\nsystems fail to identify data synthesized by the latest LLMs, such as ChatGPT\nand GPT-4. In response to this challenge, we introduce an unpretentious yet\npotent detection approach proficient in identifying synthetic text across a\nwide array of fields. Moreover, our detector demonstrates outstanding\nperformance uniformly across various model architectures and decoding\nstrategies. It also possesses the capability to identify text generated\nutilizing a potent detection-evasion technique. Our comprehensive research\nunderlines our commitment to boosting the robustness and efficiency of\nmachine-generated text detection mechanisms, particularly in the context of\nswiftly progressing and increasingly adaptive AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2305.12680v2.pdf"
    },
    {
        "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
        "authors": [
            "Siyu Yuan",
            "Jiangjie Chen",
            "Xuyang Ge",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "published": "2023-05-22T03:04:06Z",
        "summary": "The vital role of analogical reasoning in human cognition allows us to grasp\nnovel concepts by linking them with familiar ones through shared relational\nstructures. Despite the attention previous research has given to word\nanalogies, this work suggests that Large Language Models (LLMs) often overlook\nthe structures that underpin these analogies, raising questions about the\nefficacy of word analogies as a measure of analogical reasoning skills akin to\nhuman cognition. In response to this, our paper introduces a task of analogical\nstructure abduction, grounded in cognitive psychology, designed to abduce\nstructures that form an analogy between two systems. In support of this task,\nwe establish a benchmark called SCAR, containing 400 scientific analogies from\n13 distinct fields, tailored for evaluating analogical reasoning with structure\nabduction. The empirical evidence underlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need\nfor future exploration to enhance their abilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.12660v2.pdf"
    },
    {
        "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
        "authors": [
            "Qiming Bao",
            "Alex Yuxuan Peng",
            "Zhenyun Deng",
            "Wanjun Zhong",
            "Gael Gendron",
            "Timothy Pistotti",
            "Neset Tan",
            "Nathan Young",
            "Yang Chen",
            "Yonghua Zhu",
            "Paul Denny",
            "Michael Witbrock",
            "Jiamou Liu"
        ],
        "published": "2023-05-21T23:16:26Z",
        "summary": "Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges to gathering reliable\ndata from the web for building comprehensive training datasets, subsequently\naffecting the performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logic structure of the sentence,\nupon which operations are performed to generate logically modified AMR graphs.\nThe modified AMR graphs are subsequently converted back into text to create\naugmented data. Notably, our methodology is architecture-agnostic and enhances\nboth generative large language models, such as GPT-3.5 and GPT-4, through\nprompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard\n(https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347). The\nsource code and data are publicly available https://bit.ly/3OWKe8r.",
        "pdf_link": "https://arxiv.org/pdf/2305.12599v4.pdf"
    },
    {
        "title": "On the Limitations of Simulating Active Learning",
        "authors": [
            "Katerina Margatina",
            "Nikolaos Aletras"
        ],
        "published": "2023-05-21T22:52:13Z",
        "summary": "Active learning (AL) is a human-and-model-in-the-loop paradigm that\niteratively selects informative unlabeled data for human annotation, aiming to\nimprove over random sampling. However, performing AL experiments with human\nannotations on-the-fly is a laborious and expensive process, thus unrealistic\nfor academic research. An easy fix to this impediment is to simulate AL, by\ntreating an already labeled and publicly available dataset as the pool of\nunlabeled data. In this position paper, we first survey recent literature and\nhighlight the challenges across all different steps within the AL loop. We\nfurther unveil neglected caveats in the experimental setup that can\nsignificantly affect the quality of AL research. We continue with an\nexploration of how the simulation setting can govern empirical findings,\narguing that it might be one of the answers behind the ever posed question\n``why do active learning algorithms sometimes fail to outperform random\nsampling?''. We argue that evaluating AL algorithms on available labeled\ndatasets might provide a lower bound as to their effectiveness in real data. We\nbelieve it is essential to collectively shape the best practices for AL\nresearch, particularly as engineering advancements in LLMs push the research\nfocus towards data-driven approaches (e.g., data efficiency, alignment,\nfairness). In light of this, we have developed guidelines for future work. Our\naim is to draw attention to these limitations within the community, in the hope\nof finding ways to address them.",
        "pdf_link": "https://arxiv.org/pdf/2305.13342v1.pdf"
    },
    {
        "title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models",
        "authors": [
            "Oana Ignat",
            "Zhijing Jin",
            "Artem Abzaliev",
            "Laura Biester",
            "Santiago Castro",
            "Naihao Deng",
            "Xinyi Gao",
            "Aylin Gunal",
            "Jacky He",
            "Ashkan Kazemi",
            "Muhammad Khalifa",
            "Namho Koh",
            "Andrew Lee",
            "Siyang Liu",
            "Do June Min",
            "Shinka Mori",
            "Joan Nwatu",
            "Veronica Perez-Rosas",
            "Siqi Shen",
            "Zekun Wang",
            "Winston Wu",
            "Rada Mihalcea"
        ],
        "published": "2023-05-21T19:06:30Z",
        "summary": "Recent progress in large language models (LLMs) has enabled the deployment of\nmany generative NLP applications. At the same time, it has also led to a\nmisleading public discourse that ``it's all been solved.'' Not surprisingly,\nthis has, in turn, made many NLP researchers -- especially those at the\nbeginning of their careers -- worry about what NLP research area they should\nfocus on. Has it all been solved, or what remaining questions can we work on\nregardless of LLMs? To address this question, this paper compiles NLP research\ndirections rich for exploration. We identify fourteen different research areas\nencompassing 45 research directions that require new research and are not\ndirectly solvable by LLMs. While we identify many research areas, many others\nexist; we do not cover areas currently addressed by LLMs, but where LLMs lag\nbehind in performance or those focused on LLM development. We welcome\nsuggestions for other research directions to include:\nhttps://bit.ly/nlp-era-llm",
        "pdf_link": "https://arxiv.org/pdf/2305.12544v2.pdf"
    },
    {
        "title": "TheoremQA: A Theorem-driven Question Answering dataset",
        "authors": [
            "Wenhu Chen",
            "Ming Yin",
            "Max Ku",
            "Pan Lu",
            "Yixin Wan",
            "Xueguang Ma",
            "Jianyu Xu",
            "Xinyi Wang",
            "Tony Xia"
        ],
        "published": "2023-05-21T17:51:35Z",
        "summary": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in\nsolving fundamental math problems like GSM8K by achieving over 90% accuracy.\nHowever, their capabilities to solve more challenging math problems which\nrequire domain-specific knowledge (i.e. theorem) have yet to be investigated.\nIn this paper, we introduce TheoremQA, the first theorem-driven\nquestion-answering dataset designed to evaluate AI models' capabilities to\napply theorems to solve challenging science problems. TheoremQA is curated by\ndomain experts containing 800 high-quality questions covering 350 theorems\n(e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem,\nElasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a\nwide spectrum of 16 large language and code models with different prompting\nstrategies like Chain-of-Thoughts and Program-of-Thoughts. We found that\nGPT-4's capabilities to solve these problems are unparalleled, achieving an\naccuracy of 51% with Program-of-Thoughts Prompting. All the existing\nopen-sourced models are below 15%, barely surpassing the random-guess baseline.\nGiven the diversity and broad coverage of TheoremQA, we believe it can be used\nas a better benchmark to evaluate LLMs' capabilities to solve challenging\nscience problems. The data and code are released in\nhttps://github.com/wenhuchen/TheoremQA.",
        "pdf_link": "https://arxiv.org/pdf/2305.12524v3.pdf"
    },
    {
        "title": "LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance",
        "authors": [
            "Xiao Yu",
            "Yuang Qi",
            "Kejiang Chen",
            "Guoqiang Chen",
            "Xi Yang",
            "Pengyuan Zhu",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2023-05-21T17:26:16Z",
        "summary": "Large language models (LLMs) can generate texts that carry the risk of\nvarious misuses, including plagiarism, planting fake reviews on e-commerce\nplatforms, or creating inflammatory false tweets. Detecting whether a text is\nmachine-generated has thus become increasingly important. While existing\ndetection methods exhibit superior performance, they often lack\ngeneralizability due to their heavy dependence on training data. To alleviate\nthis problem, we propose a model-related generated text detection method, the\nLLM Paternity Test (LLM-Pat). Specifically, given any candidate text\n(\\textit{child}), LLM-Pat employs an intermediary LLM (\\textit{parent}) to\nreconstruct a \\textit{sibling} text corresponding to the given text and then\nmeasures the similarity between candidate texts and their sibling texts. High\nsimilarity indicates that the candidate text is machine-generated, akin to\ngenetic traits. We have constructed datasets encompassing four scenarios:\nstudent responses in educational settings, news creation, academic paper\nwriting, and social media bots to assess the performance of LLM-Pat. The\nexperiments show that LLM-Pat outperforms the existing detection methods and is\nmore robust against paraphrasing attacks and re-translating attacks. Besides,\nLLM-Pat can also be used to trace which large language model the text was\ngenerated by. The constructed dataset and code will be released to benefit the\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2305.12519v2.pdf"
    },
    {
        "title": "Retrieving Texts based on Abstract Descriptions",
        "authors": [
            "Shauli Ravfogel",
            "Valentina Pyatkin",
            "Amir DN Cohen",
            "Avshalom Manevich",
            "Yoav Goldberg"
        ],
        "published": "2023-05-21T17:14:31Z",
        "summary": "While instruction-tuned Large Language Models (LLMs) excel at extracting\ninformation from text, they are not suitable for locating texts conforming to a\ngiven description in a large document collection (semantic retrieval).\nSimilarity search over embedding vectors does allow to perform retrieval by\nquery, but the similarity reflected in the embedding is ill-defined and\nnon-consistent, and is sub-optimal for many use cases. What, then, is a good\nquery representation for effective retrieval?\n  We identify the well defined and consistent task of retrieving sentences\nbased on abstract descriptions of their content. We demonstrate the inadequacy\nof current text embeddings and propose an alternative model that significantly\nimproves when used in standard nearest neighbor search. The model is trained\nusing positive and negative pairs sourced through prompting a LLM. While it is\neasy to source the training material from an LLM, the retrieval task cannot be\nperformed by the LLM directly. This demonstrates that data from LLMs can be\nused not only for distilling more efficient specialized models than the\noriginal LLM, but also for creating new capabilities not immediately possible\nusing the original model.",
        "pdf_link": "https://arxiv.org/pdf/2305.12517v2.pdf"
    },
    {
        "title": "GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",
        "authors": [
            "Jessica L\u00f3pez Espejel",
            "El Hassane Ettifouri",
            "Mahaman Sanoussi Yahaya Alassan",
            "El Mehdi Chouham",
            "Walid Dahhane"
        ],
        "published": "2023-05-21T14:45:17Z",
        "summary": "Large Language Models (LLMs) have exhibited remarkable performance on various\nNatural Language Processing (NLP) tasks. However, there is a current hot debate\nregarding their reasoning capacity. In this paper, we examine the performance\nof GPT-3.5, GPT-4, and BARD models, by performing a thorough technical\nevaluation on different reasoning tasks across eleven distinct datasets. Our\npaper provides empirical evidence showcasing the superior performance of\nChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting\nthroughout almost all evaluated tasks. While the superiority of GPT-4 compared\nto GPT-3.5 might be explained by its larger size and NLP efficiency, this was\nnot evident for BARD. We also demonstrate that the three models show limited\nproficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To\nbolster our findings, we present a detailed and comprehensive analysis of the\nresults from these three models. Furthermore, we propose a set of engineered\nprompts that enhances the zero-shot setting performance of all three models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12477v2.pdf"
    },
    {
        "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
        "authors": [
            "Xiaotian Zhang",
            "Chunyang Li",
            "Yi Zong",
            "Zhengyu Ying",
            "Liang He",
            "Xipeng Qiu"
        ],
        "published": "2023-05-21T14:39:28Z",
        "summary": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious natural language processing tasks; however, how to comprehensively and\naccurately assess their performance becomes an urgent issue to be addressed.\nThis paper introduces GAOKAO-Bench, an intuitive benchmark that employs\nquestions from the Chinese GAOKAO examination as test samples, including both\nsubjective and objective questions. To align with human examination methods, we\ndesign a method based on zero-shot settings to evaluate the performance of\nLLMs. With human evaluation, we obtain the converted total score of LLMs,\nincluding GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have\nachieved competitive scores in Chinese GAOKAO examination, while they exhibit\nsignificant performance disparities across various subjects. We also use LLMs\nto grade the subjective questions, and find that model scores achieve a\nmoderate level of consistency with human scores. In conclusion, this research\ncontributes a robust evaluation benchmark for future large language models and\noffers valuable insights into the advantages and limitations of such models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12474v3.pdf"
    },
    {
        "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
        "authors": [
            "Renliang Sun",
            "Wei Xu",
            "Xiaojun Wan"
        ],
        "published": "2023-05-21T14:03:49Z",
        "summary": "Randomly masking text spans in ordinary texts in the pre-training stage\nhardly allows models to acquire the ability to generate simple texts. It can\nhurt the performance of pre-trained models on text simplification tasks. In\nthis paper, we propose a new continued pre-training strategy to teach the\npre-trained model to generate simple texts. We continue pre-training BART, a\nrepresentative model, to obtain SimpleBART. It consistently and significantly\nimproves the results on lexical simplification, sentence simplification, and\ndocument-level simplification tasks over BART. At the end, we compare\nSimpleBART with several representative large language models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2305.12463v1.pdf"
    },
    {
        "title": "Evaluating Open-QA Evaluation",
        "authors": [
            "Cunxiang Wang",
            "Sirui Cheng",
            "Qipeng Guo",
            "Yuanhao Yue",
            "Bowen Ding",
            "Zhikun Xu",
            "Yidong Wang",
            "Xiangkun Hu",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "published": "2023-05-21T10:40:55Z",
        "summary": "This study focuses on the evaluation of the Open Question Answering (Open-QA)\ntask, which can directly estimate the factuality of large language models\n(LLMs). Current automatic evaluation methods have shown limitations, indicating\nthat human evaluation still remains the most reliable approach. We introduce a\nnew task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset\nEVOUNA, designed to assess the accuracy of AI-generated answers in relation to\nstandard answers within Open-QA. Our evaluation of these methods utilizes\nhuman-annotated results to measure their performance. Specifically, the work\ninvestigates methods that show high correlation with human evaluations, deeming\nthem more reliable. We also discuss the pitfalls of current methods and methods\nto improve LLM-based evaluators. We believe this new QA-Eval task and\ncorresponding dataset EVOUNA will facilitate the development of more effective\nautomatic evaluation tools and prove valuable for future research in this area.\nAll resources are available at \\url{https://github.com/wangcunxiang/QA-Eval}\nand it is under the Apache-2.0 License.",
        "pdf_link": "https://arxiv.org/pdf/2305.12421v4.pdf"
    },
    {
        "title": "Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition",
        "authors": [
            "Haojun Xu",
            "Yan Gao",
            "Zheng Hui",
            "Jie Li",
            "Xinbo Gao"
        ],
        "published": "2023-05-21T08:29:16Z",
        "summary": "How humans understand and recognize the actions of others is a complex\nneuroscientific problem that involves a combination of cognitive mechanisms and\nneural networks. Research has shown that humans have brain areas that recognize\nactions that process top-down attentional information, such as the\ntemporoparietal association area. Also, humans have brain regions dedicated to\nunderstanding the minds of others and analyzing their intentions, such as the\nmedial prefrontal cortex of the temporal lobe. Skeleton-based action\nrecognition creates mappings for the complex connections between the human\nskeleton movement patterns and behaviors. Although existing studies encoded\nmeaningful node relationships and synthesized action representations for\nclassification with good results, few of them considered incorporating a priori\nknowledge to aid potential representation learning for better performance.\nLA-GCN proposes a graph convolution network using large-scale language models\n(LLM) knowledge assistance. First, the LLM knowledge is mapped into a priori\nglobal relationship (GPR) topology and a priori category relationship (CPR)\ntopology between nodes. The GPR guides the generation of new \"bone\"\nrepresentations, aiming to emphasize essential node information from the data\nlevel. The CPR mapping simulates category prior knowledge in human brain\nregions, encoded by the PC-AC module and used to add additional\nsupervision-forcing the model to learn class-distinguishable features. In\naddition, to improve information transfer efficiency in topology modeling, we\npropose multi-hop attention graph convolution. It aggregates each node's\nk-order neighbor simultaneously to speed up model convergence. LA-GCN reaches\nstate-of-the-art on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.12398v1.pdf"
    },
    {
        "title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
        "authors": [
            "Jiuzhou Han",
            "Nigel Collier",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "published": "2023-05-21T08:11:24Z",
        "summary": "Large language models (LLMs) have shown great abilities of solving various\nnatural language tasks in different domains. Due to the training objective of\nLLMs and their pre-training data, LLMs are not very well equipped for tasks\ninvolving structured data generation. We propose a framework, Prompting with\nIterative Verification (PiVe), to improve graph-based generative capability of\nLLMs. We show how a small language model could be trained to act as a verifier\nmodule for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively\nimprove its performance via fine-grained corrective instructions. We also show\nhow the verifier module could apply iterative corrections offline for a more\ncost-effective solution to the text-to-graph generation task. Experiments on\nthree graph-based datasets show consistent improvement gained via PiVe.\nAdditionally, we create GenWiki-HIQ and highlight that the verifier module can\nbe used as a data augmentation tool to help improve the quality of\nautomatically generated parallel text-graph datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.12392v2.pdf"
    },
    {
        "title": "Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models",
        "authors": [
            "Yijia Zhang",
            "Lingran Zhao",
            "Shijie Cao",
            "Wenqiang Wang",
            "Ting Cao",
            "Fan Yang",
            "Mao Yang",
            "Shanghang Zhang",
            "Ningyi Xu"
        ],
        "published": "2023-05-21T05:28:37Z",
        "summary": "Efficient deployment of large language models (LLMs) necessitates low-bit\nquantization to minimize model size and inference cost. While low-bit integer\nformats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit\nfloating-point formats (e.g., FP8/FP4) offer a compelling alternative and are\ngaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However,\nthe superiority of low-bit INT versus FP formats for quantization on LLMs\nremains unclear. In this study, we conduct a comparative analysis of INT and FP\nquantization with the same bit-width, revealing that the optimal quantization\nformat varies across different layers due to the complexity and diversity of\ntensor distribution. Consequently, we advocate the Mixture of Formats\nQuantization (MoFQ), which selects the optimal format on a layer-wise basis.\nThis simple yet effective approach achieves state-of-the-art results in both\nweight-only (W-only) and weight-activation (WA) post-training quantization\nscenarios when tested on LLaMA across various tasks. In 4-bit W-only\nquantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and\nwith an order of magnitude faster quantization speed. While in 8-bit WA\nquantization, MoFQ significantly outperforms INT/FP-only methods, achieving\nperformance close to the full precision model. Notably, MoFQ incurs no hardware\noverhead compared to INT/FP-only quantization, as the bit-width remains\nunchanged.",
        "pdf_link": "https://arxiv.org/pdf/2305.12356v1.pdf"
    },
    {
        "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
        "authors": [
            "Chen Zhang",
            "Yang Yang",
            "Jingang Wang",
            "Dawei Song"
        ],
        "published": "2023-05-21T03:35:45Z",
        "summary": "Finetuning pretrained language models (LMs) have enabled appealing\nperformance on a diverse array of tasks. The intriguing task-agnostic property\nhas driven a shifted focus from task-specific to task-agnostic distillation of\nLMs. While task-agnostic, compute-efficient, performance-preserved LMs can be\nyielded by task-agnostic distillation, previous studies mainly sit in\ndistillation of either encoder-only LMs (e.g., BERT) or decoder-only ones\n(e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g.,\nT5) can posit very distinguished behaviors. Frustratingly, we discover that\nexisting task-agnostic distillation methods can fail to handle the distillation\nof encoder-decoder LMs. To the demand, we explore a few paths and uncover a\npath named as MiniEnD that successfully tackles the distillation of\nencoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language\nunderstanding and abstractive summarization. The results showcase that MiniEnD\nis generally effective and is competitive compared to other alternatives. We\nfurther scale MiniEnD up to distillation of 3B encoder-decoder language models\nwith interpolated distillation. The results imply the opportunities and\nchallenges in distilling large language models (e.g., LLaMA).",
        "pdf_link": "https://arxiv.org/pdf/2305.12330v1.pdf"
    },
    {
        "title": "Gene Set Summarization using Large Language Models",
        "authors": [
            "Marcin P. Joachimiak",
            "J. Harry Caufield",
            "Nomi L. Harris",
            "Hyeongsik Kim",
            "Christopher J. Mungall"
        ],
        "published": "2023-05-21T02:06:33Z",
        "summary": "Molecular biologists frequently interpret gene lists derived from\nhigh-throughput experiments and computational analysis. This is typically done\nas a statistical enrichment analysis that measures the over- or\nunder-representation of biological function terms associated with genes or\ntheir properties, based on curated assertions from a knowledge base (KB) such\nas the Gene Ontology (GO). Interpreting gene lists can also be framed as a\ntextual summarization task, enabling the use of Large Language Models (LLMs),\npotentially utilizing scientific texts directly and avoiding reliance on a KB.\n  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language\nDescriptions of Controlled Terms for Ontology Reporting), a method that uses\nGPT models to perform gene set function summarization as a complement to\nstandard enrichment analysis. This method can use different sources of gene\nfunctional information: (1) structured text derived from curated ontological KB\nannotations, (2) ontology-free narrative gene summaries, or (3) direct model\nretrieval.\n  We demonstrate that these methods are able to generate plausible and\nbiologically valid summary GO term lists for gene sets. However, GPT-based\napproaches are unable to deliver reliable scores or p-values and often return\nterms that are not statistically significant. Crucially, these methods were\nrarely able to recapitulate the most precise and informative term from standard\nenrichment, likely due to an inability to generalize and reason using an\nontology. Results are highly nondeterministic, with minor variations in prompt\nresulting in radically different term lists. Our results show that at this\npoint, LLM-based methods are unsuitable as a replacement for standard term\nenrichment analysis and that manual curation of ontological assertions remains\nnecessary.",
        "pdf_link": "https://arxiv.org/pdf/2305.13338v2.pdf"
    },
    {
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
        "authors": [
            "Liangming Pan",
            "Alon Albalak",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-20T22:25:38Z",
        "summary": "Large Language Models (LLMs) have shown human-like reasoning abilities but\nstill struggle with complex logical problems. This paper introduces a novel\nframework, Logic-LM, which integrates LLMs with symbolic solvers to improve\nlogical problem-solving. Our method first utilizes LLMs to translate a natural\nlanguage problem into a symbolic formulation. Afterward, a deterministic\nsymbolic solver performs inference on the formulated problem. We also introduce\na self-refinement module, which utilizes the symbolic solver's error messages\nto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on\nfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,\nLogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant\nperformance boost of 39.2% over using LLM alone with standard prompting and\n18.4% over LLM with chain-of-thought prompting. Our findings suggest that\nLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue for\nfaithful logical reasoning. Code and data are publicly available at\nhttps://github.com/teacherpeterpan/Logic-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.12295v2.pdf"
    },
    {
        "title": "Tweetorial Hooks: Generative AI Tools to Motivate Science on Social Media",
        "authors": [
            "Tao Long",
            "Dorothy Zhang",
            "Grace Li",
            "Batool Taraif",
            "Samia Menon",
            "Kynnedy Simone Smith",
            "Sitong Wang",
            "Katy Ilonka Gero",
            "Lydia B. Chilton"
        ],
        "published": "2023-05-20T18:47:40Z",
        "summary": "Communicating science and technology is essential for the public to\nunderstand and engage in a rapidly changing world. Tweetorials are an emerging\nphenomenon where experts explain STEM topics on social media in creative and\nengaging ways. However, STEM experts struggle to write an engaging \"hook\" in\nthe first tweet that captures the reader's attention. We propose methods to use\nlarge language models (LLMs) to help users scaffold their process of writing a\nrelatable hook for complex scientific topics. We demonstrate that LLMs can help\nwriters find everyday experiences that are relatable and interesting to the\npublic, avoid jargon, and spark curiosity. Our evaluation shows that the system\nreduces cognitive load and helps people write better hooks. Lastly, we discuss\nthe importance of interactivity with LLMs to preserve the correctness,\neffectiveness, and authenticity of the writing.",
        "pdf_link": "https://arxiv.org/pdf/2305.12265v2.pdf"
    },
    {
        "title": "Collaborative Development of NLP models",
        "authors": [
            "Fereshte Khani",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-05-20T15:55:39Z",
        "summary": "Despite substantial advancements, Natural Language Processing (NLP) models\noften require post-training adjustments to enforce business rules, rectify\nundesired behavior, and align with user values. These adjustments involve\noperationalizing \"concepts\"--dictating desired model responses to certain\ninputs. However, it's difficult for a single entity to enumerate and define all\npossible concepts, indicating a need for a multi-user, collaborative model\nalignment framework. Moreover, the exhaustive delineation of a concept is\nchallenging, and an improper approach can create shortcuts or interfere with\noriginal data or other concepts.\n  To address these challenges, we introduce CoDev, a framework that enables\nmulti-user interaction with the model, thereby mitigating individual\nlimitations. CoDev aids users in operationalizing their concepts using Large\nLanguage Models, and relying on the principle that NLP models exhibit simpler\nbehaviors in local regions. Our main insight is learning a \\emph{local} model\nfor each concept, and a \\emph{global} model to integrate the original data with\nall concepts. We then steer a large language model to generate instances within\nconcept boundaries where local and global disagree. Our experiments show CoDev\nis effective at helping multiple users operationalize concepts and avoid\ninterference for a variety of scenarios, tasks, and models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12219v2.pdf"
    },
    {
        "title": "VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models",
        "authors": [
            "Xuan-Quy Dao",
            "Ngoc-Bich Le",
            "The-Duy Vo",
            "Xuan-Dung Phan",
            "Bac-Bien Ngo",
            "Van-Tien Nguyen",
            "Thi-My-Thanh Nguyen",
            "Hong-Phuoc Nguyen"
        ],
        "published": "2023-05-20T14:13:08Z",
        "summary": "The VNHSGE (VietNamese High School Graduation Examination) dataset, developed\nexclusively for evaluating large language models (LLMs), is introduced in this\narticle. The dataset, which covers nine subjects, was generated from the\nVietnamese National High School Graduation Examination and comparable tests.\n300 literary essays have been included, and there are over 19,000\nmultiple-choice questions on a range of topics. The dataset assesses LLMs in\nmultitasking situations such as question answering, text generation, reading\ncomprehension, visual question answering, and more by including both textual\ndata and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on\nthe VNHSGE dataset and contrasted their performance with that of Vietnamese\nstudents to see how well they performed. The results show that ChatGPT and\nBingChat both perform at a human level in a number of areas, including\nliterature, English, history, geography, and civics education. They still have\nspace to grow, though, especially in the areas of mathematics, physics,\nchemistry, and biology. The VNHSGE dataset seeks to provide an adequate\nbenchmark for assessing the abilities of LLMs with its wide-ranging coverage\nand variety of activities. We intend to promote future developments in the\ncreation of LLMs by making this dataset available to the scientific community,\nespecially in resolving LLMs' limits in disciplines involving mathematics and\nthe natural sciences.",
        "pdf_link": "https://arxiv.org/pdf/2305.12199v1.pdf"
    },
    {
        "title": "The Case Against Explainability",
        "authors": [
            "Hofit Wasserman Rozen",
            "Niva Elkin-Koren",
            "Ran Gilad-Bachrach"
        ],
        "published": "2023-05-20T10:56:19Z",
        "summary": "As artificial intelligence (AI) becomes more prevalent there is a growing\ndemand from regulators to accompany decisions made by such systems with\nexplanations. However, a persistent gap exists between the need to execute a\nmeaningful right to explanation vs. the ability of Machine Learning systems to\ndeliver on such a legal requirement. The regulatory appeal towards \"a right to\nexplanation\" of AI systems can be attributed to the significant role of\nexplanations, part of the notion called reason-giving, in law. Therefore, in\nthis work we examine reason-giving's purposes in law to analyze whether reasons\nprovided by end-user Explainability can adequately fulfill them.\n  We find that reason-giving's legal purposes include: (a) making a better and\nmore just decision, (b) facilitating due-process, (c) authenticating human\nagency, and (d) enhancing the decision makers' authority. Using this\nmethodology, we demonstrate end-user Explainabilty's inadequacy to fulfil\nreason-giving's role in law, given reason-giving's functions rely on its impact\nover a human decision maker. Thus, end-user Explainability fails, or is\nunsuitable, to fulfil the first, second and third legal function. In contrast\nwe find that end-user Explainability excels in the fourth function, a quality\nwhich raises serious risks considering recent end-user Explainability research\ntrends, Large Language Models' capabilities, and the ability to manipulate\nend-users by both humans and machines. Hence, we suggest that in some cases the\nright to explanation of AI systems could bring more harm than good to end\nusers. Accordingly, this study carries some important policy ramifications, as\nit calls upon regulators and Machine Learning practitioners to reconsider the\nwidespread pursuit of end-user Explainability and a right to explanation of AI\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2305.12167v1.pdf"
    },
    {
        "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning",
        "authors": [
            "Hanmeng Liu",
            "Zhiyang Teng",
            "Leyang Cui",
            "Chaoli Zhang",
            "Qiji Zhou",
            "Yue Zhang"
        ],
        "published": "2023-05-20T09:23:09Z",
        "summary": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive\nchain-of-thought reasoning ability. Recent work on self-instruction tuning,\nsuch as Alpaca, has focused on enhancing the general proficiency of models.\nThese instructions enable the model to achieve performance comparable to\nGPT-3.5 on general tasks like open-domain text generation and paraphrasing.\nHowever, they fall short of helping the model handle complex reasoning tasks.\nTo bridge the gap, this paper presents LogiCoT, a new instruction-tuning\ndataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the\nprocess of harvesting instructions for prompting GPT-4 to generate\nchain-of-thought rationales. LogiCoT serves as an instruction set for teaching\nmodels of logical reasoning and elicits general reasoning skills.",
        "pdf_link": "https://arxiv.org/pdf/2305.12147v2.pdf"
    },
    {
        "title": "LMs: Understanding Code Syntax and Semantics for Code Analysis",
        "authors": [
            "Wei Ma",
            "Shangqing Liu",
            "Zhihao Lin",
            "Wenhan Wang",
            "Qiang Hu",
            "Ye Liu",
            "Cen Zhang",
            "Liming Nie",
            "Li Li",
            "Yang Liu"
        ],
        "published": "2023-05-20T08:43:49Z",
        "summary": "Large language models~(LLMs) demonstrate significant potential to\nrevolutionize software engineering (SE) by exhibiting outstanding performance\nin SE tasks such as code and document generation. However, the high reliability\nand risk control requirements in software engineering raise concerns about the\nlack of interpretability of LLMs. To address this concern, we conducted a study\nto evaluate the capabilities of LLMs and their limitations for code analysis in\nSE. We break down the abilities needed for artificial intelligence~(AI) models\nto address SE tasks related to code analysis into three categories: 1) syntax\nunderstanding, 2) static behavior understanding, and 3) dynamic behavior\nunderstanding. Our investigation focused on the ability of LLMs to comprehend\ncode syntax and semantic structures, which include abstract syntax trees (AST),\ncontrol flow graphs (CFG), and call graphs (CG). We employed four\nstate-of-the-art foundational models, GPT4, GPT3.5, StarCoder and\nCodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language\ntasks involving C, Java, Python, and Solidity.\n  Our findings revealed that while LLMs have a talent for understanding code\nsyntax, they struggle with comprehending code semantics, particularly dynamic\nsemantics. We conclude that LLMs possess capabilities similar to an Abstract\nSyntax Tree (AST) parser, demonstrating initial competencies in static code\nanalysis. Furthermore, our study highlights that LLMs are susceptible to\nhallucinations when interpreting code semantic structures and fabricating\nnonexistent facts. These results indicate the need to explore methods to verify\nthe correctness of LLM output to ensure its dependability in SE. More\nimportantly, our study provides an initial answer to why the codes generated by\nLLM are usually syntax-correct but vulnerable.",
        "pdf_link": "https://arxiv.org/pdf/2305.12138v4.pdf"
    },
    {
        "title": "Can Public Large Language Models Help Private Cross-device Federated Learning?",
        "authors": [
            "Boxin Wang",
            "Yibo Jacky Zhang",
            "Yuan Cao",
            "Bo Li",
            "H. Brendan McMahan",
            "Sewoong Oh",
            "Zheng Xu",
            "Manzil Zaheer"
        ],
        "published": "2023-05-20T07:55:58Z",
        "summary": "We study (differentially) private federated learning (FL) of language models.\nThe language models in cross-device FL are relatively small, which can be\ntrained with meaningful formal user-level differential privacy (DP) guarantees\nwhen massive parallelism in training is enabled by the participation of a\nmoderate size of users. Recently, public data has been used to improve\nprivacy-utility trade-offs for both large and small language models. In this\nwork, we provide a systematic study of using large-scale public data and LLMs\nto help differentially private training of on-device FL models, and further\nimprove the privacy-utility tradeoff by techniques of distillation. Moreover,\nwe propose a novel distribution matching algorithm with theoretical grounding\nto sample public data close to private data distribution, which significantly\nimproves the sample efficiency of (pre-)training on public data. The proposed\nmethod is efficient and effective for training private model by taking\nadvantage of public data, especially for customized on-device architectures\nthat do not have ready-to-use pre-trained models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12132v1.pdf"
    },
    {
        "title": "Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?",
        "authors": [
            "Neeraj Varshney",
            "Mihir Parmar",
            "Nisarg Patel",
            "Divij Handa",
            "Sayantan Sarkar",
            "Man Luo",
            "Chitta Baral"
        ],
        "published": "2023-05-20T05:20:37Z",
        "summary": "Pre-training on large corpora of text enables the language models to acquire\na vast amount of factual and commonsense knowledge which allows them to achieve\nremarkable performance on a variety of language understanding tasks. They\ntypically acquire this knowledge by learning from the pre-training text and\ncapturing certain patterns from it. However, real-world settings often present\nscenarios that do not abide by these patterns i.e. scenarios that break the\ncommon assumptions. Can state-of-the-art NLP models correctly reason over the\ncontexts of such scenarios?\n  Addressing the above question, in this paper, we investigate the ability of\nmodels to correctly reason over contexts that break the common assumptions. To\nthis end, we first systematically create evaluation data in which each data\ninstance consists of (a) a common assumption, (b) a context that follows the\nassumption, (c) a context that breaks the assumption, and (d) questions based\non the contexts. Then, through evaluations on multiple models including GPT-3\nand Flan T5, we show that while doing fairly well on contexts that follow the\ncommon assumptions, the models struggle to correctly reason over contexts that\nbreak those assumptions. Specifically, the performance gap is as high as 20%\nabsolute points. Furthermore, we thoroughly analyze these results revealing\nseveral interesting findings. We believe our work and findings will encourage\nand facilitate further research in developing more robust models that can also\nreliably reason over contexts that break the common assumptions. Data is\navailable at \\url{https://github.com/nrjvarshney/break_the_common_assumptions}.",
        "pdf_link": "https://arxiv.org/pdf/2305.12096v1.pdf"
    },
    {
        "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
        "authors": [
            "Wenyue Hua",
            "Yingqiang Ge",
            "Shuyuan Xu",
            "Jianchao Ji",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-20T04:32:59Z",
        "summary": "Recent advancements in foundation models such as large language models (LLM)\nhave propelled them to the forefront of recommender systems (RS). Moreover,\nfairness in RS is critical since many users apply it for decision-making and\ndemand fulfillment. However, at present, there is a lack of understanding\nregarding the level of fairness exhibited by recommendation foundation models\nand the appropriate methods for equitably treating different groups of users in\nfoundation models. In this paper, we focus on user-side unfairness problem and\nshow through a thorough examination that there is unfairness involved in LLMs\nthat lead to unfair recommendation results. To eliminate bias from LLM for\nfairness-aware recommendation, we introduce a novel Unbiased P5 (UP5)\nfoundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP\nincludes two sub-modules: a personalized prefix prompt that enhances fairness\nwith respect to individual sensitive attributes, and a Prompt Mixture that\nintegrates multiple counterfactually-fair prompts for a set of sensitive\nattributes. Experiments are conducted on two real-world datasets, MovieLens-1M\nand Insurance, and results are compared with both matching-based and\nsequential-based fairness-aware recommendation models. The results show that\nUP5 achieves better recommendation performance and meanwhile exhibits a high\nlevel of fairness.",
        "pdf_link": "https://arxiv.org/pdf/2305.12090v1.pdf"
    },
    {
        "title": "MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",
        "authors": [
            "Zifeng Wang",
            "Chufan Gao",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "published": "2023-05-20T03:37:09Z",
        "summary": "Tabular data prediction has been employed in medical applications such as\npatient health risk prediction. However, existing methods usually revolve\naround the algorithm design while overlooking the significance of data\nengineering. Medical tabular datasets frequently exhibit significant\nheterogeneity across different sources, with limited sample sizes per source.\nAs such, previous predictors are often trained on manually curated small\ndatasets that struggle to generalize across different tabular datasets during\ninference. This paper proposes to scale medical tabular data predictors\n(MediTab) to various tabular inputs with varying features. The method uses a\ndata engine that leverages large language models (LLMs) to consolidate tabular\nsamples to overcome the barrier across tables with distinct schema. It also\naligns out-domain data with the target task using a \"learn, annotate, and\nrefinement\" pipeline. The expanded training data then enables the pre-trained\nMediTab to infer for arbitrary tabular input in the domain without fine-tuning,\nresulting in significant improvements over supervised baselines: it reaches an\naverage ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3\ntrial outcome prediction datasets, respectively. In addition, MediTab exhibits\nimpressive zero-shot performances: it outperforms supervised XGBoost models by\n8.9% and 17.2% on average in two prediction tasks, respectively. The code is\navailable at https://github.com/RyanWangZf/MediTab.",
        "pdf_link": "https://arxiv.org/pdf/2305.12081v2.pdf"
    },
    {
        "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
        "authors": [
            "Weifeng Jiang",
            "Qianren Mao",
            "Chenghua Lin",
            "Jianxin Li",
            "Ting Deng",
            "Weiyi Yang",
            "Zheng Wang"
        ],
        "published": "2023-05-20T03:23:16Z",
        "summary": "Many text mining models are constructed by fine-tuning a large deep\npre-trained language model (PLM) in downstream tasks. However, a significant\nchallenge nowadays is maintaining performance when we use a lightweight model\nwith limited labelled samples. We present DisCo, a semi-supervised learning\n(SSL) framework for fine-tuning a cohort of small student models generated from\na large PLM using knowledge distillation. Our key insight is to share\ncomplementary knowledge among distilled student cohorts to promote their SSL\neffectiveness. DisCo employs a novel co-training technique to optimize a cohort\nof multiple small student models by promoting knowledge sharing among students\nunder diversified views: model views produced by different distillation\nstrategies and data views produced by various input augmentations. We evaluate\nDisCo on both semi-supervised text classification and extractive summarization\ntasks. Experimental results show that DisCo can produce student models that are\n7.6 times smaller and 4.8 times faster in inference than the baseline PLMs\nwhile maintaining comparable performance. We also show that DisCo-generated\nstudent models outperform the similar-sized models elaborately tuned in\ndistinct tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.12074v3.pdf"
    },
    {
        "title": "AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation",
        "authors": [
            "Vijayaraghavan Murali",
            "Chandra Maddila",
            "Imad Ahmad",
            "Michael Bolin",
            "Daniel Cheng",
            "Negar Ghorbani",
            "Renuka Fernandez",
            "Nachiappan Nagappan",
            "Peter C. Rigby"
        ],
        "published": "2023-05-20T00:45:15Z",
        "summary": "Generative LLMs have been shown to effectively power AI-based code authoring\ntools that can suggest entire statements or blocks of code during code\nauthoring. In this paper we present CodeCompose, an AI-assisted code authoring\ntool developed and deployed at Meta internally. CodeCompose is based on the\nInCoder LLM that merges generative capabilities with bi-directionality. We have\nscaled up CodeCompose to serve tens of thousands of developers at Meta, across\n9 programming languages and several coding surfaces. We present our experience\nin making design decisions about the model and system architecture for\nCodeCompose that addresses these challenges.\n  To release a LLM model at this scale, we needed to first ensure that it is\nsufficiently accurate. In a random sample of 20K source code files, depending\non the language, we are able to reproduce hidden lines between 40% and 58% of\nthe time, an improvement of 1.4x and 4.1x over a model trained only on public\ndata.\n  We gradually rolled CodeCompose out to developers. At the time of this\nwriting, 16K developers have used it with 8% of their code coming directly from\nCodeCompose.\n  To triangulate our numerical findings, we conduct a thematic analysis on the\nfeedback from 70 developers. We find that 91.5% of the feedback is positive,\nwith the most common themes being discovering APIs, dealing with boilerplate\ncode, and accelerating coding. Meta continues to integrate this feedback into\nCodeCompose.",
        "pdf_link": "https://arxiv.org/pdf/2305.12050v2.pdf"
    },
    {
        "title": "Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding",
        "authors": [
            "Augustin Toma",
            "Patrick R. Lawler",
            "Jimmy Ba",
            "Rahul G. Krishnan",
            "Barry B. Rubin",
            "Bo Wang"
        ],
        "published": "2023-05-19T23:07:09Z",
        "summary": "We present Clinical Camel, an open large language model (LLM) explicitly\ntailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical\nCamel achieves state-of-the-art performance across medical benchmarks among\nopenly available medical LLMs. Leveraging efficient single-GPU training,\nClinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed\nbenchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for\nGPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to\n53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these\nbenchmarks, Clinical Camel demonstrates its broader capabilities, such as\nsynthesizing plausible clinical notes. This work introduces dialogue-based\nknowledge encoding, a novel method to synthesize conversational data from dense\nmedical texts. While benchmark results are encouraging, extensive and rigorous\nhuman evaluation across diverse clinical scenarios is imperative to ascertain\nsafety before implementation. By openly sharing Clinical Camel, we hope to\nfoster transparent and collaborative research, working towards the safe\nintegration of LLMs within the healthcare domain. Significant challenges\nconcerning reliability, bias, and the potential for outdated knowledge persist.\nNonetheless, the transparency provided by an open approach reinforces the\nscientific rigor essential for future clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.12031v2.pdf"
    },
    {
        "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
        "authors": [
            "Badr AlKhamissi",
            "Siddharth Verma",
            "Ping Yu",
            "Zhijing Jin",
            "Asli Celikyilmaz",
            "Mona Diab"
        ],
        "published": "2023-05-19T20:58:22Z",
        "summary": "In this paper, we conduct a thorough investigation into the reasoning\ncapabilities of Large Language Models (LLMs), focusing specifically on the Open\nPretrained Transformers (OPT) models as a representative of such models. Our\nstudy entails finetuning three different sizes of OPT on a carefully curated\nreasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned\nwithout explanations, and OPT-RE, finetuned with explanations. We then evaluate\nall models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS\nbenchmark, covering 26 distinct reasoning skills, utilizing three prompting\ntechniques. Through a comprehensive grid of 27 configurations and 6,156 test\nevaluations, we investigate the dimensions of finetuning, prompting, and scale\nto understand the role of explanations on different reasoning skills. Our\nfindings reveal that having explanations in the fewshot exemplar has no\nsignificant impact on the model's performance when the model is finetuned,\nwhile positively affecting the non-finetuned counterpart. Moreover, we observe\na slight yet consistent increase in classification accuracy as we incorporate\nexplanations during prompting and finetuning, respectively. Finally, we offer\ninsights on which skills benefit the most from incorporating explanations\nduring finetuning and prompting, such as Numerical (+20.4%) and Analogical\n(+13.9%) reasoning, as well as skills that exhibit negligible or negative\neffects.",
        "pdf_link": "https://arxiv.org/pdf/2305.12001v2.pdf"
    },
    {
        "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
        "authors": [
            "Kai North",
            "Tharindu Ranasinghe",
            "Matthew Shardlow",
            "Marcos Zampieri"
        ],
        "published": "2023-05-19T20:56:22Z",
        "summary": "Lexical Simplification (LS) is the task of replacing complex for simpler\nwords in a sentence whilst preserving the sentence's original meaning. LS is\nthe lexical component of Text Simplification (TS) with the aim of making texts\nmore accessible to various target populations. A past survey (Paetzold and\nSpecia, 2017) has provided a detailed overview of LS. Since this survey,\nhowever, the AI/NLP community has been taken by storm by recent advances in\ndeep learning, particularly with the introduction of large language models\n(LLM) and prompt learning. The high performance of these models sparked renewed\ninterest in LS. To reflect these recent advances, we present a comprehensive\nsurvey of papers published between 2017 and 2023 on LS and its sub-tasks with a\nspecial focus on deep learning. We also present benchmark datasets for the\nfuture development of LS systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.12000v1.pdf"
    },
    {
        "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-05-19T17:51:05Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious tasks and gained significant attention. LLMs are also used for local\nsequence transduction tasks, including grammatical error correction (GEC) and\nformality style transfer, where most tokens in a source text are kept\nunchanged. However, the models that generate all target tokens in such tasks\nhave a tendency to simply copy the input text as is, without making needed\nchanges, because the difference between input and output texts is minimal in\nthe training data. This is also inefficient because the computational cost\ngrows quadratically with the target sequence length with Transformer. This\npaper proposes predicting edit spans for the source text for local sequence\ntransduction tasks. Representing an edit span with a position of the source\ntext and corrected tokens, we can reduce the length of the target sequence and\nthe computational cost for inference. We apply instruction tuning for LLMs on\nthe supervision data of edit spans. Experiments show that the proposed method\nachieves comparable performance to the baseline in four tasks, paraphrasing,\nformality style transfer, GEC, and text simplification, despite reducing the\nlength of the target text by as small as 21%. Furthermore, we report that the\ntask-specific fine-tuning with the proposed method achieved state-of-the-art\nperformance in the four tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.11862v2.pdf"
    },
    {
        "title": "How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings",
        "authors": [
            "Shuaichen Chang",
            "Eric Fosler-Lussier"
        ],
        "published": "2023-05-19T17:43:58Z",
        "summary": "Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.",
        "pdf_link": "https://arxiv.org/pdf/2305.11853v3.pdf"
    },
    {
        "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
        "authors": [
            "Hye Sun Yun",
            "Iain J. Marshall",
            "Thomas A. Trikalinos",
            "Byron C. Wallace"
        ],
        "published": "2023-05-19T17:09:19Z",
        "summary": "Medical systematic reviews play a vital role in healthcare decision making\nand policy. However, their production is time-consuming, limiting the\navailability of high-quality and up-to-date evidence summaries. Recent\nadvancements in large language models (LLMs) offer the potential to\nautomatically generate literature reviews on demand, addressing this issue.\nHowever, LLMs sometimes generate inaccurate (and potentially misleading) texts\nby hallucination or omission. In healthcare, this can make LLMs unusable at\nbest and dangerous at worst. We conducted 16 interviews with international\nsystematic review experts to characterize the perceived utility and risks of\nLLMs in the specific context of medical evidence reviews. Experts indicated\nthat LLMs can assist in the writing process by drafting summaries, generating\ntemplates, distilling information, and crosschecking information. They also\nraised concerns regarding confidently composed but inaccurate LLM outputs and\nother potential downstream harms, including decreased accountability and\nproliferation of low-quality reviews. Informed by this qualitative analysis, we\nidentify criteria for rigorous evaluation of biomedical LLMs aligned with\ndomain expert views.",
        "pdf_link": "https://arxiv.org/pdf/2305.11828v3.pdf"
    },
    {
        "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
        "authors": [
            "Hongru Wang",
            "Rui Wang",
            "Fei Mi",
            "Yang Deng",
            "Zezhong Wang",
            "Bin Liang",
            "Ruifeng Xu",
            "Kam-Fai Wong"
        ],
        "published": "2023-05-19T16:27:43Z",
        "summary": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower\ndialogue systems with strong language understanding and generation\ncapabilities. However, most of the previous works prompt the LLMs to directly\ngenerate a response based on the dialogue context, overlooking the underlying\nlinguistic cues about the user status exhibited in the context. Such in-depth\ndialogue scenarios are challenging for existing LLMs to figure out the user's\nhidden needs and respond satisfactorily through a single-step inference. To\nthis end, we propose a novel linguistic cue-based chain-of-thoughts\n(\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate\nreasoning step to find cues exhibited in the dialogue, aiming to provide a more\npersonalized and engaging response. To evaluate the approach, we build a\nbenchmark with in-depth dialogue questions, consisting of 6 datasets in both\nChinese and English, targeting 3 major linguistic cues during the conversation:\n\\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct\nextensive experiments on the proposed benchmark with 5 LLMs under both\nzero-shot and one-shot settings. Empirical results demonstrate our proposed\n\\textit{Cue}-CoT method outperforms standard prompting methods in terms of both\n\\textit{helpfulness} and \\textit{acceptability} on all datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.11792v2.pdf"
    },
    {
        "title": "Prompting with Pseudo-Code Instructions",
        "authors": [
            "Mayank Mishra",
            "Prince Kumar",
            "Riyaz Bhat",
            "Rudra Murthy V",
            "Danish Contractor",
            "Srikanth Tamilselvam"
        ],
        "published": "2023-05-19T16:25:01Z",
        "summary": "Prompting with natural language instructions has recently emerged as a\npopular method of harnessing the capabilities of large language models. Given\nthe inherent ambiguity present in natural language, it is intuitive to consider\nthe possible advantages of prompting with less ambiguous prompt styles, such as\nthe use of pseudo-code.\n  In this paper we explore if prompting via pseudo-code instructions helps\nimprove the performance of pre-trained language models. We manually create a\ndataset of pseudo-code prompts for 132 different tasks spanning classification,\nQA and generative language tasks, sourced from the Super-NaturalInstructions\ndataset. Using these prompts along with their counterparts in natural language,\nwe study their performance on two LLM families - BLOOM and CodeGen. Our\nexperiments show that using pseudo-code instructions leads to better results,\nwith an average increase (absolute) of 7-16 points in F1 scores for\nclassification tasks and an improvement (relative) of 12-38% in aggregate\nROUGE-L scores across all tasks. We include detailed ablation studies which\nindicate that code comments, docstrings, and the structural clues encoded in\npseudo-code all contribute towards the improvement in performance.\n  To the best of our knowledge our work is the first to demonstrate how\npseudo-code prompts can be helpful in improving the performance of pre-trained\nLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.11790v3.pdf"
    },
    {
        "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
        "authors": [
            "Andrea Schioppa",
            "Xavier Garcia",
            "Orhan Firat"
        ],
        "published": "2023-05-19T16:14:07Z",
        "summary": "The recent rapid progress in pre-training Large Language Models has relied on\nusing self-supervised language modeling objectives like next token prediction\nor span corruption. On the other hand, Machine Translation Systems are mostly\ntrained using cross-lingual supervision that requires aligned data between\nsource and target languages. We demonstrate that pre-training Large Language\nModels on a mixture of a self-supervised Language Modeling objective and the\nsupervised Machine Translation objective, therefore including cross-lingual\nparallel data during pre-training, yields models with better in-context\nlearning abilities. As pre-training is a very resource-intensive process and a\ngrid search on the best mixing ratio between the two objectives is\nprohibitively expensive, we propose a simple yet effective strategy to learn it\nduring pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2305.11778v1.pdf"
    },
    {
        "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
        "authors": [
            "Mustafa Safa Ozdayi",
            "Charith Peris",
            "Jack FitzGerald",
            "Christophe Dupuy",
            "Jimit Majmudar",
            "Haidar Khan",
            "Rahil Parikh",
            "Rahul Gupta"
        ],
        "published": "2023-05-19T15:45:29Z",
        "summary": "Large Language Models (LLMs) are known to memorize significant portions of\ntheir training data. Parts of this memorized content have been shown to be\nextractable by simply querying the model, which poses a privacy risk. We\npresent a novel approach which uses prompt-tuning to control the extraction\nrates of memorized content in LLMs. We present two prompt training strategies\nto increase and decrease extraction rates, which correspond to an attack and a\ndefense, respectively. We demonstrate the effectiveness of our techniques by\nusing models from the GPT-Neo family on a public benchmark. For the 1.3B\nparameter GPT-Neo model, our attack yields a 9.3 percentage point increase in\nextraction rate compared to our baseline. Our defense can be tuned to achieve\ndifferent privacy-utility trade-offs by a user-specified hyperparameter. We\nachieve an extraction rate reduction of up to 97.7% relative to our baseline,\nwith a perplexity increase of 16.9%.",
        "pdf_link": "https://arxiv.org/pdf/2305.11759v1.pdf"
    },
    {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-19T15:36:27Z",
        "summary": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.",
        "pdf_link": "https://arxiv.org/pdf/2305.11747v3.pdf"
    },
    {
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
        "authors": [
            "Zhibin Gou",
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Yujiu Yang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-05-19T15:19:44Z",
        "summary": "Recent developments in large language models (LLMs) have been impressive.\nHowever, these models sometimes show inconsistencies and problematic behavior,\nsuch as hallucinating facts, generating flawed code, or creating offensive and\ntoxic content. Unlike these models, humans typically utilize external tools to\ncross-check and refine their initial content, like using a search engine for\nfact-checking, or a code interpreter for debugging. Inspired by this\nobservation, we introduce a framework called CRITIC that allows LLMs, which are\nessentially \"black boxes\" to validate and progressively amend their own outputs\nin a manner similar to human interaction with tools. More specifically,\nstarting with an initial output, CRITIC interacts with appropriate tools to\nevaluate certain aspects of the text, and then revises the output based on the\nfeedback obtained during this validation process. Comprehensive evaluations\ninvolving free-form question answering, mathematical program synthesis, and\ntoxicity reduction demonstrate that CRITIC consistently enhances the\nperformance of LLMs. Meanwhile, our research highlights the crucial importance\nof external feedback in promoting the ongoing self-improvement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.11738v4.pdf"
    },
    {
        "title": "S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
        "authors": [
            "Fangyu Lei",
            "Xiang Li",
            "Yifan Wei",
            "Shizhu He",
            "Yiming Huang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-05-19T15:01:48Z",
        "summary": "Answering multi-hop questions over hybrid factual knowledge from the given\ntext and table (TextTableQA) is a challenging task. Existing models mainly\nadopt a retriever-reader framework, which have several deficiencies, such as\nnoisy labeling in training retriever, insufficient utilization of heterogeneous\ninformation over text and table, and deficient ability for different reasoning\noperations. In this paper, we propose a three-stage TextTableQA framework\nS3HQA, which comprises of retriever, selector, and reasoner. We use a retriever\nwith refinement training to solve the noisy labeling problem. Then, a hybrid\nselector considers the linked relationships between heterogeneous data to\nselect the most relevant factual knowledge. For the final stage, instead of\nadapting a reading comprehension module like in previous methods, we employ a\ngeneration-based reasoner to obtain answers. This includes two approaches: a\nrow-wise generator and an LLM prompting generator~(first time used in this\ntask). The experimental results demonstrate that our method achieves\ncompetitive results in the few-shot setting. When trained on the full dataset,\nour approach outperforms all baseline methods, ranking first on the HybridQA\nleaderboard.",
        "pdf_link": "https://arxiv.org/pdf/2305.11725v1.pdf"
    },
    {
        "title": "Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses",
        "authors": [
            "Xenia Ohmer",
            "Elia Bruni",
            "Dieuwke Hupkes"
        ],
        "published": "2023-05-19T13:23:51Z",
        "summary": "At the staggering pace with which the capabilities of large language models\n(LLMs) are increasing, creating future-proof evaluation sets to assess their\nunderstanding becomes more and more challenging. In this paper, we propose a\nnovel paradigm for evaluating LLMs which leverages the idea that correct world\nunderstanding should be consistent across different (Fregean) senses of the\nsame meaning. Accordingly, we measure understanding not in terms of correctness\nbut by evaluating consistency across multiple senses that are generated by the\nmodel itself. We showcase our approach by instantiating a test where the\ndifferent senses are different languages, hence using multilingual\nself-consistency as a litmus test for the model's understanding and\nsimultaneously addressing the important topic of multilinguality. Taking one of\nthe latest versions of ChatGPT as our object of study, we evaluate multilingual\nconsistency for two different tasks across three different languages. We show\nthat its multilingual consistency is still lacking, and that its task and world\nunderstanding are thus not language-independent. As our approach does not\nrequire any static evaluation corpora in languages other than English, it can\neasily and cheaply be extended to different languages and tasks and could\nbecome an integral part of future benchmarking efforts.",
        "pdf_link": "https://arxiv.org/pdf/2305.11662v3.pdf"
    },
    {
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
        "authors": [
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "published": "2023-05-19T12:10:53Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner",
        "pdf_link": "https://arxiv.org/pdf/2305.11627v3.pdf"
    },
    {
        "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2023-05-19T11:15:33Z",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities in various\napplications, but they still face various inconsistency issues. Existing works\nprimarily focus on the inconsistency issues within a single LLM, while we\ncomplementarily explore the inter-consistency among multiple LLMs for\ncollaboration. To examine whether LLMs can collaborate effectively to achieve a\nconsensus for a shared goal, we focus on commonsense reasoning, and introduce a\nformal debate framework (FORD) to conduct a three-stage debate among LLMs with\nreal-world scenarios alignment: fair debate, mismatched debate, and roundtable\ndebate. Through extensive experiments on various datasets, LLMs can effectively\ncollaborate to reach a consensus despite noticeable inter-inconsistencies, but\nimbalances in their abilities can lead to domination by superior LLMs.\nLeveraging a more advanced LLM like GPT-4 as an authoritative judge can boost\ncollaboration performance. Our work contributes to understanding the\ninter-consistency among LLMs and lays the foundation for developing future\ncollaboration methods. Codes and data are available at\nhttps://github.com/Waste-Wood/FORD",
        "pdf_link": "https://arxiv.org/pdf/2305.11595v3.pdf"
    },
    {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "published": "2023-05-19T09:54:21Z",
        "summary": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.11554v4.pdf"
    },
    {
        "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
        "authors": [
            "Fangkai Yang",
            "Pu Zhao",
            "Zezhong Wang",
            "Lu Wang",
            "Jue Zhang",
            "Mohit Garg",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "published": "2023-05-19T09:23:25Z",
        "summary": "Large Language Model (LLM) has gained popularity and achieved remarkable\nresults in open-domain tasks, but its performance in real industrial\ndomain-specific scenarios is average due to its lack of specific domain\nknowledge. This issue has attracted widespread attention, but there are few\nrelevant benchmarks available. In this paper, we provide a benchmark Question\nAnswering (QA) dataset named MSQA, centered around Microsoft products and IT\ntechnical problems encountered by customers. This dataset contains industry\ncloud-specific QA knowledge, an area not extensively covered in general LLMs,\nmaking it well-suited for evaluating methods aiming to enhance LLMs'\ndomain-specific capabilities. In addition, we propose a new model interaction\nparadigm that can empower LLM to achieve better performance on domain-specific\ntasks where it is not proficient. Extensive experiments demonstrate that the\napproach following our method outperforms the commonly used LLM with retrieval\nmethods. We make our source code and sample data available at:\nhttps://aka.ms/Microsoft_QA.",
        "pdf_link": "https://arxiv.org/pdf/2305.11541v3.pdf"
    },
    {
        "title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset",
        "authors": [
            "Honghao Gui",
            "Shuofei Qiao",
            "Jintian Zhang",
            "Hongbin Ye",
            "Mengshu Sun",
            "Lei Liang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-19T08:51:11Z",
        "summary": "Traditional information extraction (IE) methodologies, constrained by\npre-defined classes and static training paradigms, often falter in\nadaptability, especially in the dynamic world. To bridge this gap, we explore\nan instruction-based IE paradigm in this paper, leveraging the substantial\ncross-task generalization capabilities of Large Language Models (LLMs). We\nobserve that most existing IE datasets tend to be overly redundant in their\nlabel sets, which leads to the inclusion of numerous labels not directly\nrelevant to the extraction content when constructing instructions. To tackle\nthis issue, we introduce a bilingual theme-centric IE instruction dataset\n(Chinese and English), InstructIE, and for the first time, incorporate a theme\nscheme design that effectively simplifies the label structure. Furthermore, we\ndevelop an innovative framework named KG2Instruction, which is specifically\ndesigned for the automatic generation of such datasets. Experimental\nevaluations based on InstructIE reveal that while current models show promise\nin Instruction-based IE tasks, opportunities for their potential optimization\nalso emerge. The dataset is available at\nhttps://huggingface.co/datasets/zjunlp/InstructIE.",
        "pdf_link": "https://arxiv.org/pdf/2305.11527v2.pdf"
    },
    {
        "title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning",
        "authors": [
            "Chengfeng Dou",
            "Zhi Jin",
            "Wenping Jiao",
            "Haiyan Zhao",
            "Zhenwei Tao",
            "Yongqiang Zhao"
        ],
        "published": "2023-05-19T08:18:24Z",
        "summary": "The patient-centered medical dialogue systems strive to offer diagnostic\ninterpretation services to users who are less knowledgeable about medical\nknowledge, through emphasizing the importance of providing responses specific\nto the patients. It is difficult for the large language models (LLMs) to\nguarantee the specificity of responses in spite of its promising performance\neven in some tasks in medical field. Inspired by in-context learning, we\npropose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this\nchallenge. PlugMed is equipped with two modules, the prompt generation (PG)\nmodule and the response ranking (RR) module, to enhances LLMs' dialogue\nstrategies for improving the specificity of the dialogue. The PG module is\ndesigned to stimulate the imitative ability of LLMs by providing them with real\ndialogues from similar patients as prompts. The RR module incorporates\nfine-tuned small model as response filter to enable the selection of\nappropriate responses generated by LLMs. Furthermore, we introduce a new\nevaluation method based on matching both user's intent and high-frequency\nmedical term to effectively assess the specificity of the responses. We conduct\nexperimental evaluations on three medical dialogue datasets, and the results,\nincluding both automatic and human evaluation, demonstrate the effectiveness of\nour approach.",
        "pdf_link": "https://arxiv.org/pdf/2305.11508v2.pdf"
    },
    {
        "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
        "authors": [
            "Tianci Xue",
            "Ziqi Wang",
            "Zhenhailong Wang",
            "Chi Han",
            "Pengfei Yu",
            "Heng Ji"
        ],
        "published": "2023-05-19T08:02:52Z",
        "summary": "Large language Models (LLMs) have achieved promising performance on\narithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting. However, LLMs face challenges in maintaining factual consistency\nduring reasoning, exhibiting tendencies to condition overlooking, question\nmisinterpretation, and condition hallucination over given problems. Existing\nmethods use coarse-grained feedback (e.g., whether the answer is correct) to\nimprove factual consistency. In this work, we propose RCoT (Reversing\nChain-of-Thought), a novel method to improve LLMs' reasoning abilities by\nautomatically detecting and rectifying factual inconsistency in LLMs, generated\nsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct\nthe problem based on generated solutions. Then fine-grained comparisons between\nthe original problem and the reconstructed problem expose the factual\ninconsistency in the original solutions. To rectify the solution, RCoT\nformulates detected factual inconsistency into fine-grained feedback to guide\nLLMs in revising solutions. Experimental results demonstrate improvements of\nRCoT over standard CoT, Self-Consistency and Self-Refine across seven\narithmetic datasets. Moreover, we find that manually written fine-grained\nfeedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT\nreaches 94.6% accuracy on GSM8K), encouraging the community to further explore\nthe fine-grained feedback generation methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.11499v2.pdf"
    },
    {
        "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
        "authors": [
            "Suhyeon Lee",
            "Won Jun Kim",
            "Jinho Chang",
            "Jong Chul Ye"
        ],
        "published": "2023-05-19T07:44:39Z",
        "summary": "Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.",
        "pdf_link": "https://arxiv.org/pdf/2305.11490v5.pdf"
    },
    {
        "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
        "authors": [
            "Sangho Suh",
            "Bryan Min",
            "Srishti Palani",
            "Haijun Xia"
        ],
        "published": "2023-05-19T07:31:59Z",
        "summary": "People are increasingly turning to large language models (LLMs) for complex\ninformation tasks like academic research or planning a move to another city.\nHowever, while they often require working in a nonlinear manner -- e.g., to\narrange information spatially to organize and make sense of it, current\ninterfaces for interacting with LLMs are generally linear to support\nconversational interaction. To address this limitation and explore how we can\nsupport LLM-powered exploration and sensemaking, we developed Sensecape, an\ninteractive system designed to support complex information tasks with an LLM by\nenabling users to (1) manage the complexity of information through multilevel\nabstraction and (2) seamlessly switch between foraging and sensemaking. Our\nwithin-subject user study reveals that Sensecape empowers users to explore more\ntopics and structure their knowledge hierarchically, thanks to the\nexternalization of levels of abstraction. We contribute implications for\nLLM-based workflows and interfaces for information tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.11483v2.pdf"
    },
    {
        "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
        "authors": [
            "Peiling Jiang",
            "Jude Rayan",
            "Steven P. Dow",
            "Haijun Xia"
        ],
        "published": "2023-05-19T06:53:25Z",
        "summary": "Large language models (LLMs) have recently soared in popularity due to their\nease of access and the unprecedented ability to synthesize text responses to\ndiverse user questions. However, LLMs like ChatGPT present significant\nlimitations in supporting complex information tasks due to the insufficient\naffordances of the text-based medium and linear conversational structure.\nThrough a formative study with ten participants, we found that LLM interfaces\noften present long-winded responses, making it difficult for people to quickly\ncomprehend and interact flexibly with various pieces of information,\nparticularly during more complex tasks. We present Graphologue, an interactive\nsystem that converts text-based responses from LLMs into graphical diagrams to\nfacilitate information-seeking and question-answering tasks. Graphologue\nemploys novel prompting strategies and interface designs to extract entities\nand relationships from LLM responses and constructs node-link diagrams in\nreal-time. Further, users can interact with the diagrams to flexibly adjust the\ngraphical presentation and to submit context-specific prompts to obtain more\ninformation. Utilizing diagrams, Graphologue enables graphical, non-linear\ndialogues between humans and LLMs, facilitating information exploration,\norganization, and comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2305.11473v2.pdf"
    },
    {
        "title": "Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs",
        "authors": [
            "Ioktong Lei",
            "Zhidong Deng"
        ],
        "published": "2023-05-19T06:30:17Z",
        "summary": "As a way of communicating with users and any LLMs like GPT or PaLM2,\nprompting becomes an increasingly important research topic for better\nutilization of LLMs. Although simple prompting performs well on single-step\nquestions, it cannot permanently activate the correct knowledge path for\nmulti-step reasoning tasks. The chain of thought (CoT), which often contains\nzero-shot CoT and few-shot CoT, is a recently developed prompting method that\ncan explain the reasoning process to the LLM and outperforms simple prompting\nin three challenging reasoning tasks, including arithmetic, symbolic, and\ncommonsense reasoning. In this paper, we propose a novel hint of thought (HoT)\nprompting with explainability and zero-shot generalization. First, it is\ndecomposed into the following three steps: explainable sub-questions, logical\nreasoning, and answer extraction. Second, such three steps are sequentially\nordered in the format of step-by-step hints, which can be easily adjusted and\nexplained to different tasks. Finally, experimental results demonstrate that\nour HoT prompting has a significant advantage on the zero-shot reasoning task\ncompared to existing zero-shot CoT. We did zero-shot experiments on math tasks\nlike GSM8K, ADDSUB, AQUA, SVAMP and commonsense tasks such as StrategyQA. In\nparticular, the accuracy of the proposed HoT prompting is improved with GSM8K\nfrom 40.50% to 67.80%, with AQUA from 31.9% to 46.4%, with SVAMP from 63.7% to\n76.9%, and with ADDSUB from 74.7% to 87.34%, respectively, which even defeats\nthe competitive PoT approach on GSM8k, AQUA, and SVAMP.",
        "pdf_link": "https://arxiv.org/pdf/2305.11461v5.pdf"
    },
    {
        "title": "Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions",
        "authors": [
            "Shiyao Ding",
            "Takayuki Ito"
        ],
        "published": "2023-05-19T06:27:16Z",
        "summary": "Finding an agreement among diverse opinions is a challenging topic in\nmultiagent systems. Recently, large language models (LLMs) have shown great\npotential in addressing this challenge due to their remarkable capabilities in\ncomprehending human opinions and generating human-like text. However, they\ntypically rely on extensive human-annotated data. In this paper, we propose\nSelf-Agreement, a novel framework for fine-tuning LLMs to autonomously find\nagreement using data generated by LLM itself. Specifically, our approach\nemploys the generative pre-trained transformer-3 (GPT-3) to generate multiple\nopinions for each question in a question dataset and create several agreement\ncandidates among these opinions. Then, a bidirectional encoder representations\nfrom transformers (BERT)-based model evaluates the agreement score of each\nagreement candidate and selects the one with the highest agreement score. This\nprocess yields a dataset of question-opinion-agreements, which we use to\nfine-tune a pre-trained LLM for discovering agreements among diverse opinions.\nRemarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework\nachieves comparable performance to GPT-3 with only 1/25 of its parameters,\nshowcasing its ability to identify agreement among various opinions without the\nneed for human-annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2305.11460v1.pdf"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "authors": [
            "Shubhra Kanti Karmaker Santu",
            "Dongji Feng"
        ],
        "published": "2023-05-19T04:59:34Z",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "pdf_link": "https://arxiv.org/pdf/2305.11430v2.pdf"
    },
    {
        "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
        "authors": [
            "Satyapriya Krishna",
            "Jiaqi Ma",
            "Dylan Slack",
            "Asma Ghandeharioun",
            "Sameer Singh",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-05-19T04:46:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex tasks. Moreover, recent research has shown that\nincorporating human-annotated rationales (e.g., Chain-of-Thought prompting)\nduring in-context learning can significantly enhance the performance of these\nmodels, particularly on tasks that require reasoning capabilities. However,\nincorporating such rationales poses challenges in terms of scalability as this\nrequires a high degree of human involvement. In this work, we present a novel\nframework, Amplifying Model Performance by Leveraging In-Context Learning with\nPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges\nby automating the process of rationale generation. To this end, we leverage\npost hoc explanation methods which output attribution scores (explanations)\ncapturing the influence of each of the input features on model predictions.\nMore specifically, we construct automated natural language rationales that\nembed insights from post hoc explanations to provide corrective signals to\nLLMs. Extensive experimentation with real-world datasets demonstrates that our\nframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%\nover a wide range of tasks, including those where prior approaches which rely\non human-annotated rationales such as Chain-of-Thought prompting fall short.\nOur work makes one of the first attempts at highlighting the potential of post\nhoc explanations as valuable tools for enhancing the effectiveness of LLMs.\nFurthermore, we conduct additional empirical analyses and ablation studies to\ndemonstrate the impact of each of the components of AMPLIFY, which, in turn,\nleads to critical insights for refining in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.11426v3.pdf"
    },
    {
        "title": "Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models",
        "authors": [
            "Sixing Yu",
            "J. Pablo Mu\u00f1oz",
            "Ali Jannesari"
        ],
        "published": "2023-05-19T03:51:59Z",
        "summary": "Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have\ndemonstrated remarkable success in a wide range of applications, driven by\ntheir ability to leverage vast amounts of data for pre-training. However,\noptimizing FMs often requires access to sensitive data, raising privacy\nconcerns and limiting their applicability in many domains. In this paper, we\npropose the Federated Foundation Models (FFMs) paradigm, which combines the\nbenefits of FMs and Federated Learning (FL) to enable privacy-preserving and\ncollaborative learning across multiple end-users. We discuss the potential\nbenefits and challenges of integrating FL into the lifespan of FMs, covering\npre-training, fine-tuning, and application. We further outline potential future\nresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, and\nfederated prompt tuning, which allow the development of more personalized and\ncontext-aware models while ensuring data privacy. Moreover, we explore the\npossibility of continual/lifelong learning in FFMs, as increased computational\npower at the edge may unlock the potential for optimizing FMs using newly\ngenerated private data close to the data source. The proposed FFM concepts\noffer a flexible and scalable framework for training large language models in a\nprivacy-preserving manner, setting the stage for subsequent advancements in\nboth FM training and federated learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.11414v3.pdf"
    },
    {
        "title": "A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation",
        "authors": [
            "Xiaowei Huang",
            "Wenjie Ruan",
            "Wei Huang",
            "Gaojie Jin",
            "Yi Dong",
            "Changshun Wu",
            "Saddek Bensalem",
            "Ronghui Mu",
            "Yi Qi",
            "Xingyu Zhao",
            "Kaiwen Cai",
            "Yanghao Zhang",
            "Sihao Wu",
            "Peipei Xu",
            "Dengyu Wu",
            "Andre Freitas",
            "Mustafa A. Mustafa"
        ],
        "published": "2023-05-19T02:41:12Z",
        "summary": "Large Language Models (LLMs) have exploded a new heatwave of AI for their\nability to engage end-users in human-level conversations with detailed and\narticulate answers across many knowledge domains. In response to their fast\nadoption in many industrial applications, this survey concerns their safety and\ntrustworthiness. First, we review known vulnerabilities and limitations of the\nLLMs, categorising them into inherent issues, attacks, and unintended bugs.\nThen, we consider if and how the Verification and Validation (V&V) techniques,\nwhich have been widely developed for traditional software and deep learning\nmodels such as convolutional neural networks as independent processes to check\nthe alignment of their implementations against the specifications, can be\nintegrated and further extended throughout the lifecycle of the LLMs to provide\nrigorous analysis to the safety and trustworthiness of LLMs and their\napplications. Specifically, we consider four complementary techniques:\nfalsification and evaluation, verification, runtime monitoring, and regulations\nand ethical use. In total, 370+ references are considered to support the quick\nunderstanding of the safety and trustworthiness issues from the perspective of\nV&V. While intensive research has been conducted to identify the safety and\ntrustworthiness issues, rigorous yet practical methods are called for to ensure\nthe alignment of LLMs with safety and trustworthiness requirements.",
        "pdf_link": "https://arxiv.org/pdf/2305.11391v2.pdf"
    },
    {
        "title": "ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery",
        "authors": [
            "Anaelia Ovalle",
            "Mehrab Beikzadeh",
            "Parshan Teimouri",
            "Kai-Wei Chang",
            "Majid Sarrafzadeh"
        ],
        "published": "2023-05-19T02:09:52Z",
        "summary": "Large language models have been useful in expanding mental health care\ndelivery. ChatGPT, in particular, has gained popularity for its ability to\ngenerate human-like dialogue. However, data-sensitive domains -- including but\nnot limited to healthcare -- face challenges in using ChatGPT due to privacy\nand data-ownership concerns. To enable its utilization, we propose a text\nambiguation framework that preserves user privacy. We ground this in the task\nof addressing stress prompted by user-provided texts to demonstrate the\nviability and helpfulness of privacy-preserved generations. Our results suggest\nthat chatGPT recommendations are still able to be moderately helpful and\nrelevant, even when the original user text is not provided.",
        "pdf_link": "https://arxiv.org/pdf/2306.05552v1.pdf"
    },
    {
        "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
        "authors": [
            "Emily Reif",
            "Minsuk Kahng",
            "Savvas Petridis"
        ],
        "published": "2023-05-19T00:53:45Z",
        "summary": "Large language models (LLMs) can be used to generate smaller, more refined\ndatasets via few-shot prompting for benchmarking, fine-tuning or other use\ncases. However, understanding and evaluating these datasets is difficult, and\nthe failure modes of LLM-generated data are still not well understood.\nSpecifically, the data can be repetitive in surprising ways, not only\nsemantically but also syntactically and lexically. We present LinguisticLens, a\nnovel inter-active visualization tool for making sense of and analyzing\nsyntactic diversity of LLM-generated datasets. LinguisticLens clusters text\nalong syntactic, lexical, and semantic axes. It supports hierarchical\nvisualization of a text dataset, allowing users to quickly scan for an overview\nand inspect individual examples. The live demo is available at\nshorturl.at/zHOUV.",
        "pdf_link": "https://arxiv.org/pdf/2305.11364v2.pdf"
    },
    {
        "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
        "authors": [
            "Giorgi Kokaia",
            "Pratyush Sinha",
            "Yutong Jiang",
            "Nozha Boujemaa"
        ],
        "published": "2023-05-18T22:47:06Z",
        "summary": "We introduce two novel methods, Tree-Search and Self-contextualizing QA,\ndesigned to enhance the performance of large language models (LLMs) in\nquestion-answering tasks. Tree-Search is a sampling technique specifically\ncreated to extract diverse information from an LLM for a given prompt.\nSelf-contextualizing QA leverages Tree-Search to enable the model to create its\nown context using a wide range of information relevant to the prompt, evaluate\nit explicitly and return a open book answer to the initial prompt . We\ndemonstrate that the quality of generated answers improves according to various\nmetrics, including accuracy, informativeness, coherence, and consistency, as\nevaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods\nresult in increased robustness and that performance is positively correlated\nwith tree size, benefiting both answer quality and robustness. Finally, we\ndiscuss other promising applications of Tree-Search, highlighting its potential\nto enhance a broad range of tasks beyond question-answering.\n  \\noindent We also discuss several areas for future work, including refining\nthe Tree-Search and Self-Contextualizing QA methods, improving the coherence of\nthe generated context, and investigating the impact of bootstrapping on model\nrobustness",
        "pdf_link": "https://arxiv.org/pdf/2305.11334v1.pdf"
    },
    {
        "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
        "authors": [
            "Wanrong Zhu",
            "Xinyi Wang",
            "Yujie Lu",
            "Tsu-Jui Fu",
            "Xin Eric Wang",
            "Miguel Eckstein",
            "William Yang Wang"
        ],
        "published": "2023-05-18T21:53:58Z",
        "summary": "The field of text-to-image (T2I) generation has garnered significant\nattention both within the research community and among everyday users. Despite\nthe advancements of T2I models, a common issue encountered by users is the need\nfor repetitive editing of input prompts in order to receive a satisfactory\nimage, which is time-consuming and labor-intensive. Given the demonstrated text\ngeneration power of large-scale language models, such as GPT-k, we investigate\nthe potential of utilizing such models to improve the prompt editing process\nfor T2I generation. We conduct a series of experiments to compare the common\nedits made by humans and GPT-k, evaluate the performance of GPT-k in prompting\nT2I, and examine factors that may influence this process. We found that GPT-k\nmodels focus more on inserting modifiers while humans tend to replace words and\nphrases, which includes changes to the subject matter. Experimental results\nshow that GPT-k are more effective in adjusting modifiers rather than\npredicting spontaneous changes in the primary subject matters. Adopting the\nedit suggested by GPT-k models may reduce the percentage of remaining edits by\n20-30%.",
        "pdf_link": "https://arxiv.org/pdf/2305.11317v2.pdf"
    },
    {
        "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Zijing Shi",
            "Yitong Li",
            "Ling Chen",
            "Mykola Pechenizkiy"
        ],
        "published": "2023-05-18T18:58:30Z",
        "summary": "\\textit{\\textbf{\\textcolor{red}{Warning}:} This paper contains content that\nmay be offensive or upsetting.} Pretrained conversational agents have been\nexposed to safety issues, exhibiting a range of stereotypical human biases such\nas gender bias. However, there are still limited bias categories in current\nresearch, and most of them only focus on English. In this paper, we introduce a\nnew Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese\nconversational language models. Apart from those previous well-explored bias\ncategories, CHBias includes under-explored bias categories, such as ageism and\nappearance biases, which received less attention. We evaluate two popular\npretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.\nFurthermore, to mitigate different biases, we apply several debiasing methods\nto the Chinese pretrained models. Experimental results show that these Chinese\npretrained models are potentially risky for generating texts that contain\nsocial biases, and debiasing methods using the proposed dataset can make\nresponse generation less biased while preserving the models' conversational\ncapabilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.11262v1.pdf"
    },
    {
        "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
        "authors": [
            "Eliza Kosoy",
            "Emily Rose Reagan",
            "Leslie Lai",
            "Alison Gopnik",
            "Danielle Krettek Cobb"
        ],
        "published": "2023-05-18T18:15:43Z",
        "summary": "Developmental psychologists have spent decades devising experiments to test\nthe intelligence and knowledge of infants and children, tracing the origin of\ncrucial concepts and capacities. Moreover, experimental techniques in\ndevelopmental psychology have been carefully designed to discriminate the\ncognitive capacities that underlie particular behaviors. We propose that using\nclassical experiments from child development is a particularly effective way to\nprobe the computational abilities of AI models, in general, and LLMs in\nparticular. First, the methodological techniques of developmental psychology,\nsuch as the use of novel stimuli to control for past experience or control\nconditions to determine whether children are using simple associations, can be\nequally helpful for assessing the capacities of LLMs. In parallel, testing LLMs\nin this way can tell us whether the information that is encoded in text is\nsufficient to enable particular responses, or whether those responses depend on\nother kinds of information, such as information from exploration of the\nphysical world. In this work we adapt classical developmental experiments to\nevaluate the capabilities of LaMDA, a large language model from Google. We\npropose a novel LLM Response Score (LRS) metric which can be used to evaluate\nother language models, such as GPT. We find that LaMDA generates appropriate\nresponses that are similar to those of children in experiments involving social\nunderstanding, perhaps providing evidence that knowledge of these domains is\ndiscovered through language. On the other hand, LaMDA's responses in early\nobject and action understanding, theory of mind, and especially causal\nreasoning tasks are very different from those of young children, perhaps\nshowing that these domains require more real-world, self-initiated exploration\nand cannot simply be learned from patterns in language input.",
        "pdf_link": "https://arxiv.org/pdf/2305.11243v2.pdf"
    },
    {
        "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
        "authors": [
            "Wenhai Wang",
            "Zhe Chen",
            "Xiaokang Chen",
            "Jiannan Wu",
            "Xizhou Zhu",
            "Gang Zeng",
            "Ping Luo",
            "Tong Lu",
            "Jie Zhou",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "published": "2023-05-18T17:59:42Z",
        "summary": "Large language models (LLMs) have notably accelerated progress towards\nartificial general intelligence (AGI), with their impressive zero-shot capacity\nfor user-tailored tasks, endowing them with immense potential across a range of\napplications. However, in the field of computer vision, despite the\navailability of numerous powerful vision foundation models (VFMs), they are\nstill restricted to tasks in a pre-defined form, struggling to match the\nopen-ended task capabilities of LLMs. In this work, we present an LLM-based\nframework for vision-centric tasks, termed VisionLLM. This framework provides a\nunified perspective for vision and language tasks by treating images as a\nforeign language and aligning vision-centric tasks with language tasks that can\nbe flexibly defined and managed using language instructions. An LLM-based\ndecoder can then make appropriate predictions based on these instructions for\nopen-ended tasks. Extensive experiments show that the proposed VisionLLM can\nachieve different levels of task customization through language instructions,\nfrom fine-grained object-level to coarse-grained task-level customization, all\nwith good results. It's noteworthy that, with a generalist LLM-based framework,\nour model can achieve over 60\\% mAP on COCO, on par with detection-specific\nmodels. We hope this model can set a new baseline for generalist vision and\nlanguage models. The demo shall be released based on\nhttps://github.com/OpenGVLab/InternGPT. The code shall be released at\nhttps://github.com/OpenGVLab/VisionLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.11175v2.pdf"
    },
    {
        "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
        "authors": [
            "Zorik Gekhman",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Chen Elkind",
            "Idan Szpektor"
        ],
        "published": "2023-05-18T17:58:35Z",
        "summary": "Factual consistency evaluation is often conducted using Natural Language\nInference (NLI) models, yet these models exhibit limited success in evaluating\nsummaries. Previous work improved such models with synthetic training data.\nHowever, the data is typically based on perturbed human-written summaries,\nwhich often differ in their characteristics from real model-generated summaries\nand have limited coverage of possible factual errors. Alternatively, large\nlanguage models (LLMs) have recently shown promising results in directly\nevaluating generative tasks, but are too computationally expensive for\npractical use. Motivated by these limitations, we introduce TrueTeacher, a\nmethod for generating synthetic data by annotating diverse model-generated\nsummaries using a LLM. Unlike prior work, TrueTeacher does not rely on\nhuman-written summaries, and is multilingual by nature. Experiments on the TRUE\nbenchmark show that a student model trained using our data, substantially\noutperforms both the state-of-the-art model with similar capacity, and the LLM\nteacher. In a systematic study, we compare TrueTeacher to existing synthetic\ndata generation methods and demonstrate its superiority and robustness to\ndomain-shift. We also show that our method generalizes to multilingual\nscenarios. Lastly, we release our large scale synthetic dataset (1.4M\nexamples), generated using TrueTeacher, and a checkpoint trained on this data.",
        "pdf_link": "https://arxiv.org/pdf/2305.11171v3.pdf"
    },
    {
        "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
        "authors": [
            "Kai Zhang",
            "Bernal Jim\u00e9nez Guti\u00e9rrez",
            "Yu Su"
        ],
        "published": "2023-05-18T17:48:03Z",
        "summary": "Recent work has shown that fine-tuning large language models (LLMs) on\nlarge-scale instruction-following datasets substantially improves their\nperformance on a wide range of NLP tasks, especially in the zero-shot setting.\nHowever, even advanced instruction-tuned LLMs still fail to outperform small\nLMs on relation extraction (RE), a fundamental information extraction task. We\nhypothesize that instruction-tuning has been unable to elicit strong RE\ncapabilities in LLMs due to RE's low incidence in instruction-tuning datasets,\nmaking up less than 1% of all tasks (Wang et al., 2022). To address this\nlimitation, we propose QA4RE, a framework that aligns RE with question\nanswering (QA), a predominant task in instruction-tuning datasets.\nComprehensive zero-shot RE experiments over four datasets with two series of\ninstruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework\nconsistently improves LLM performance, strongly verifying our hypothesis and\nenabling LLMs to outperform strong zero-shot baselines by a large margin.\nAdditionally, we provide thorough experiments and discussions to show the\nrobustness, few-shot effectiveness, and strong transferability of our QA4RE\nframework. This work illustrates a promising way of adapting LLMs to\nchallenging and underrepresented tasks by aligning these tasks with more common\ninstruction-tuning tasks like QA.",
        "pdf_link": "https://arxiv.org/pdf/2305.11159v1.pdf"
    },
    {
        "title": "Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement",
        "authors": [
            "Samuel Mensah",
            "Kai Sun",
            "Nikolaos Aletras"
        ],
        "published": "2023-05-18T15:22:00Z",
        "summary": "State-of-the-art target-oriented opinion word extraction (TOWE) models\ntypically use BERT-based text encoders that operate on the word level, along\nwith graph convolutional networks (GCNs) that incorporate syntactic information\nextracted from syntax trees. These methods achieve limited gains with GCNs and\nhave difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to\nbe effective at representing rare words or words with insufficient context\ninformation. To address this issue, this work trades syntax trees for BERT\nwordpieces by entirely removing the GCN component from the methods'\narchitectures. To enhance TOWE performance, we tackle the issue of aspect\nrepresentation loss during encoding. Instead of solely utilizing a sentence as\nthe input, we use a sentence-aspect pair. Our relatively simple approach\nachieves state-of-the-art results on benchmark datasets and should serve as a\nstrong baseline for further research.",
        "pdf_link": "https://arxiv.org/pdf/2305.11034v1.pdf"
    },
    {
        "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
        "authors": [
            "Tom Silver",
            "Soham Dan",
            "Kavitha Srinivas",
            "Joshua B. Tenenbaum",
            "Leslie Pack Kaelbling",
            "Michael Katz"
        ],
        "published": "2023-05-18T14:48:20Z",
        "summary": "Recent work has considered whether large language models (LLMs) can function\nas planners: given a task, generate a plan. We investigate whether LLMs can\nserve as generalized planners: given a domain and training tasks, generate a\nprogram that efficiently produces plans for other tasks in the domain. In\nparticular, we consider PDDL domains and use GPT-4 to synthesize Python\nprograms. We also consider (1) Chain-of-Thought (CoT) summarization, where the\nLLM is prompted to summarize the domain and propose a strategy in words before\nsynthesizing the program; and (2) automated debugging, where the program is\nvalidated with respect to the training tasks, and in case of errors, the LLM is\nre-prompted with four types of feedback. We evaluate this approach in seven\nPDDL domains and compare it to four ablations and four baselines. Overall, we\nfind that GPT-4 is a surprisingly powerful generalized planner. We also\nconclude that automated debugging is very important, that CoT summarization has\nnon-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two\ntraining tasks are often sufficient for strong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.11014v2.pdf"
    },
    {
        "title": "The Web Can Be Your Oyster for Improving Large Language Models",
        "authors": [
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Jingyuan Wang",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-18T14:20:32Z",
        "summary": "Large language models (LLMs) encode a large amount of world knowledge.\nHowever, as such knowledge is frozen at the time of model training, the models\nbecome static and limited by the training data at that time. In order to\nfurther improve the capacity of LLMs for knowledge-intensive tasks, we consider\naugmenting LLMs with the large-scale web using search engine. Unlike previous\naugmentation sources (e.g., Wikipedia data dump), the web provides broader,\nmore comprehensive and constantly updated information. In this paper, we\npresent a web-augmented LLM UNIWEB, which is trained over 16\nknowledge-intensive tasks in a unified text-to-text format. Instead of simply\nusing the retrieved contents from web, our approach has made two major\nimprovements. Firstly, we propose an adaptive search engine assisted learning\nmethod that can self-evaluate the confidence level of LLM's predictions, and\nadaptively determine when to refer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Secondly, we design a pretraining task,\ni.e., continual knowledge learning, based on salient spans prediction, to\nreduce the discrepancy between the encoded and retrieved knowledge. Experiments\non a wide range of knowledge-intensive tasks show that our model significantly\noutperforms previous retrieval-augmented methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.10998v2.pdf"
    },
    {
        "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
        "authors": [
            "Ning Lu",
            "Shengcai Liu",
            "Rui He",
            "Qi Wang",
            "Yew-Soon Ong",
            "Ke Tang"
        ],
        "published": "2023-05-18T10:03:25Z",
        "summary": "Large language models (LLMs) have shown remarkable performance in various\ntasks and have been extensively utilized by the public. However, the increasing\nconcerns regarding the misuse of LLMs, such as plagiarism and spamming, have\nled to the development of multiple detectors, including fine-tuned classifiers\nand statistical methods. In this study, we equip LLMs with prompts, rather than\nrelying on an external paraphraser, to evaluate the vulnerability of these\ndetectors. We propose a novel Substitution-based In-Context example\nOptimization method (SICO) to automatically construct prompts for evading the\ndetectors. SICO is cost-efficient as it requires only 40 human-written examples\nand a limited number of LLM inferences to generate a prompt. Moreover, once a\ntask-specific prompt has been constructed, it can be universally used against a\nwide range of detectors. Extensive experiments across three real-world tasks\ndemonstrate that SICO significantly outperforms the paraphraser baselines and\nenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by\n0.5 on average. Furthermore, a comprehensive human evaluation as well as a\nvalidation experiment in the wild show that the SICO-generated text achieves\nhuman-level readability and task completion rates. Finally, the strong\nperformance of SICO exhibits its potential as a reliable evaluation tool for\nfuture detectors. The codes and data are located on\nhttps://github.com/ColinLu50/Evade-GPT-Detector.",
        "pdf_link": "https://arxiv.org/pdf/2305.10847v5.pdf"
    },
    {
        "title": "X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models",
        "authors": [
            "Yixiong Chen",
            "Li Liu",
            "Chris Ding"
        ],
        "published": "2023-05-18T09:56:44Z",
        "summary": "This paper introduces a novel explainable image quality evaluation approach\ncalled X-IQE, which leverages visual large language models (LLMs) to evaluate\ntext-to-image generation methods by generating textual explanations. X-IQE\nutilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce\nself-consistent, unbiased texts that are highly correlated with human\nevaluation. It offers several advantages, including the ability to distinguish\nbetween real and generated images, evaluate text-image alignment, and assess\nimage aesthetics without requiring model training or fine-tuning. X-IQE is more\ncost-effective and efficient compared to human evaluation, while significantly\nenhancing the transparency and explainability of deep image quality evaluation\nmodels. We validate the effectiveness of our method as a benchmark using images\ngenerated by prevalent diffusion models. X-IQE demonstrates similar performance\nto state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming\nthe limitations of previous evaluation models on DrawBench, particularly in\nhandling ambiguous generation prompts and text recognition in generated images.\nProject website:\nhttps://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models",
        "pdf_link": "https://arxiv.org/pdf/2305.10843v2.pdf"
    },
    {
        "title": "ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation",
        "authors": [
            "Yunsheng Bai",
            "Atefeh Sohrabizadeh",
            "Zongyue Qin",
            "Ziniu Hu",
            "Yizhou Sun",
            "Jason Cong"
        ],
        "published": "2023-05-18T09:44:18Z",
        "summary": "Recent years have witnessed the growing popularity of domain-specific\naccelerators (DSAs), such as Google's TPUs, for accelerating various\napplications such as deep learning, search, autonomous driving, etc. To\nfacilitate DSA designs, high-level synthesis (HLS) is used, which allows a\ndeveloper to compile a high-level description in the form of software code in C\nand C++ into a design in low-level hardware description languages (such as VHDL\nor Verilog) and eventually synthesized into a DSA on an ASIC\n(application-specific integrated circuit) or FPGA (field-programmable gate\narrays). However, existing HLS tools still require microarchitecture decisions,\nexpressed in terms of pragmas (such as directives for parallelization and\npipelining). To enable more people to design DSAs, it is desirable to automate\nsuch decisions with the help of deep learning for predicting the quality of HLS\ndesigns. This requires us a deeper understanding of the program, which is a\ncombination of original code and pragmas. Naturally, these programs can be\nconsidered as sequence data, for which large language models (LLM) can help. In\naddition, these programs can be compiled and converted into a control data flow\ngraph (CDFG), and the compiler also provides fine-grained alignment between the\ncode tokens and the CDFG nodes. However, existing works either fail to leverage\nboth modalities or combine the two in shallow or coarse ways. We propose ProgSG\nallowing the source code sequence modality and the graph modalities to interact\nwith each other in a deep and fine-grained way. To alleviate the scarcity of\nlabeled designs, a pre-training method is proposed based on a suite of\ncompiler's data flow analysis tasks. Experimental results on two benchmark\ndatasets show the superiority of ProgSG over baseline methods that either only\nconsider one modality or combine the two without utilizing the alignment\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2305.10838v2.pdf"
    },
    {
        "title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants",
        "authors": [
            "Amal Haddad Haddad",
            "Damith Premasiri",
            "Tharindu Ranasinghe",
            "Ruslan Mitkov"
        ],
        "published": "2023-05-18T09:22:29Z",
        "summary": "The domain of Botany is rich with metaphorical terms. Those terms play an\nimportant role in the description and identification of flowers and plants.\nHowever, the identification of such terms in discourse is an arduous task. This\nleads in some cases to committing errors during translation processes and\nlexicographic tasks. The process is even more challenging when it comes to\nmachine translation, both in the cases of single-word terms and multi-word\nterms. One of the recent concerns of Natural Language Processing (NLP)\napplications and Machine Translation (MT) technologies is the automatic\nidentification of metaphor-based words in discourse through Deep Learning (DL).\nIn this study, we seek to fill this gap through the use of thirteen popular\ntransformer based models, as well as ChatGPT, and we show that discriminative\nmodels perform better than GPT-3.5 model with our best performer reporting\n92.2349% F1 score in metaphoric flower and plant names identification task.",
        "pdf_link": "https://arxiv.org/pdf/2305.10833v3.pdf"
    },
    {
        "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
        "authors": [
            "Qian Chen",
            "Wen Wang",
            "Qinglin Zhang",
            "Siqi Zheng",
            "Chong Deng",
            "Hai Yu",
            "Jiaqing Liu",
            "Yukun Ma",
            "Chong Zhang"
        ],
        "published": "2023-05-18T07:56:40Z",
        "summary": "Prior studies diagnose the anisotropy problem in sentence representations\nfrom pre-trained language models, e.g., BERT, without fine-tuning. Our analysis\nreveals that the sentence embeddings from BERT suffer from a bias towards\nuninformative words, limiting the performance in semantic textual similarity\n(STS) tasks. To address this bias, we propose a simple and efficient\nunsupervised approach, Diagonal Attention Pooling (Ditto), which weights words\nwith model-based importance estimations and computes the weighted average of\nword representations from pre-trained models as sentence embeddings. Ditto can\nbe easily applied to any pre-trained language model as a postprocessing\noperation. Compared to prior sentence embedding approaches, Ditto does not add\nparameters nor requires any learning. Empirical evaluations demonstrate that\nour proposed Ditto can alleviate the anisotropy problem and improve various\npre-trained models on STS tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.10786v2.pdf"
    },
    {
        "title": "Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models",
        "authors": [
            "Raj Sanjay Shah",
            "Vijay Marupudi",
            "Reba Koenen",
            "Khushi Bhardwaj",
            "Sashank Varma"
        ],
        "published": "2023-05-18T07:50:44Z",
        "summary": "Large Language Models (LLMs) do not differentially represent numbers, which\nare pervasive in text. In contrast, neuroscience research has identified\ndistinct neural representations for numbers and words. In this work, we\ninvestigate how well popular LLMs capture the magnitudes of numbers (e.g., that\n$4 < 5$) from a behavioral lens. Prior research on the representational\ncapabilities of LLMs evaluates whether they show human-level performance, for\ninstance, high overall accuracy on standard benchmarks. Here, we ask a\ndifferent question, one inspired by cognitive science: How closely do the\nnumber representations of LLMscorrespond to those of human language users, who\ntypically demonstrate the distance, size, and ratio effects? We depend on a\nlinking hypothesis to map the similarities among the model embeddings of number\nwords and digits to human response times. The results reveal surprisingly\nhuman-like representations across language models of different architectures,\ndespite the absence of the neural circuitry that directly supports these\nrepresentations in the human brain. This research shows the utility of\nunderstanding LLMs using behavioral benchmarks and points the way to future\nwork on the number representations of LLMs and their cognitive plausibility.",
        "pdf_link": "https://arxiv.org/pdf/2305.10782v3.pdf"
    },
    {
        "title": "Ethical ChatGPT: Concerns, Challenges, and Commandments",
        "authors": [
            "Jianlong Zhou",
            "Heimo M\u00fcller",
            "Andreas Holzinger",
            "Fang Chen"
        ],
        "published": "2023-05-18T02:04:13Z",
        "summary": "Large language models, e.g. ChatGPT are currently contributing enormously to\nmake artificial intelligence even more popular, especially among the general\npopulation. However, such chatbot models were developed as tools to support\nnatural language communication between humans. Problematically, it is very much\na ``statistical correlation machine\" (correlation instead of causality) and\nthere are indeed ethical concerns associated with the use of AI language models\nsuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights\nspecific ethical concerns on ChatGPT and articulates key challenges when\nChatGPT is used in various applications. Practical commandments for different\nstakeholders of ChatGPT are also proposed that can serve as checklist\nguidelines for those applying ChatGPT in their applications. These commandment\nexamples are expected to motivate the ethical use of ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.10646v1.pdf"
    },
    {
        "title": "Are Large Language Models Fit For Guided Reading?",
        "authors": [
            "Peter Ochieng"
        ],
        "published": "2023-05-18T02:03:55Z",
        "summary": "This paper looks at the ability of large language models to participate in\neducational guided reading. We specifically, evaluate their ability to generate\nmeaningful questions from the input text, generate diverse questions both in\nterms of content coverage and difficulty of the questions and evaluate their\nability to recommend part of the text that a student should re-read based on\nthe student's responses to the questions. Based on our evaluation of ChatGPT\nand Bard, we report that,\n  1) Large language models are able to generate high quality meaningful\nquestions that have high correlation with the input text, 2) They generate\ndiverse question that cover most topics in the input text even though this\nability is significantly degraded as the input text increases, 3)The large\nlanguage models are able to generate both low and high cognitive questions even\nthough they are significantly biased toward low cognitive question, 4) They are\nable to effectively summarize responses and extract a portion of text that\nshould be re-read.",
        "pdf_link": "https://arxiv.org/pdf/2305.10645v2.pdf"
    },
    {
        "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
        "authors": [
            "Jiannan Xiang",
            "Tianhua Tao",
            "Yi Gu",
            "Tianmin Shu",
            "Zirui Wang",
            "Zichao Yang",
            "Zhiting Hu"
        ],
        "published": "2023-05-18T00:35:38Z",
        "summary": "While large language models (LMs) have shown remarkable capabilities across\nnumerous tasks, they often struggle with simple reasoning and planning in\nphysical environments, such as understanding object permanence or planning\nhousehold activities. The limitation arises from the fact that LMs are trained\nonly on written text and miss essential embodied knowledge and skills. In this\npaper, we propose a new paradigm of enhancing LMs by finetuning them with world\nmodels, to gain diverse embodied knowledge while retaining their general\nlanguage capabilities. Our approach deploys an embodied agent in a world model,\nparticularly a simulator of the physical world (VirtualHome), and acquires a\ndiverse set of embodied experiences through both goal-oriented planning and\nrandom exploration. These experiences are then used to finetune LMs to teach\ndiverse abilities of reasoning and acting in the physical world, e.g., planning\nand completing goals, object permanence and tracking, etc. Moreover, it is\ndesirable to preserve the generality of LMs during finetuning, which\nfacilitates generalizing the embodied knowledge across tasks rather than being\ntied to specific simulations. We thus further introduce the classical (EWC) for\nselective weight updates, combined with low-rank adapters (LoRA) for training\nefficiency. Extensive experiments show our approach substantially improves base\nLMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs\n(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much\nlarger LMs (e.g., ChatGPT).",
        "pdf_link": "https://arxiv.org/pdf/2305.10626v3.pdf"
    },
    {
        "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
        "authors": [
            "Dong-Ho Lee",
            "Kian Ahrabian",
            "Woojeong Jin",
            "Fred Morstatter",
            "Jay Pujara"
        ],
        "published": "2023-05-17T23:50:28Z",
        "summary": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to\npredict future facts using knowledge of past facts. In this paper, we apply\nlarge language models (LLMs) to these benchmarks using in-context learning\n(ICL). We investigate whether and to what extent LLMs can be used for TKG\nforecasting, especially without any fine-tuning or explicit modules for\ncapturing structural and temporal information. For our experiments, we present\na framework that converts relevant historical facts into prompts and generates\nranked predictions using token probabilities. Surprisingly, we observe that\nLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully\ndesigned and trained for TKG forecasting. Our extensive evaluation presents\nperformances across several models and datasets with different characteristics,\ncompares alternative heuristics for preparing contextual information, and\ncontrasts to prominent TKG methods and simple frequency and recency baselines.\nWe also discover that using numerical indices instead of entity/relation names,\ni.e., hiding semantic information, does not significantly affect the\nperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is\nunnecessary; instead, LLMs can leverage the existing patterns in the context to\nachieve such performance. Our analysis also reveals that ICL enables LLMs to\nlearn irregular patterns from the historical context, going beyond simple\npredictions based on common or recent information.",
        "pdf_link": "https://arxiv.org/pdf/2305.10613v3.pdf"
    },
    {
        "title": "Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting",
        "authors": [
            "Saeth Wannasuphoprasit",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2023-05-17T23:41:30Z",
        "summary": "Cosine similarity between two words, computed using their contextualised\ntoken embeddings obtained from masked language models (MLMs) such as BERT has\nshown to underestimate the actual similarity between those words (Zhou et al.,\n2022). This similarity underestimation problem is particularly severe for\nhighly frequent words. Although this problem has been noted in prior work, no\nsolution has been proposed thus far. We observe that the L2 norm of\ncontextualised embeddings of a word correlates with its log-frequency in the\npretraining corpus. Consequently, the larger L2 norms associated with the\nhighly frequent words reduce the cosine similarity values measured between\nthem, thus underestimating the similarity scores. To solve this issue, we\npropose a method to discount the L2 norm of a contextualised word embedding by\nthe frequency of that word in a corpus when measuring the cosine similarities\nbetween words. We show that the so called stop words behave differently from\nthe rest of the words, which require special consideration during their\ndiscounting process. Experimental results on a contextualised word similarity\ndataset show that our proposed discounting method accurately solves the\nsimilarity underestimation problem.",
        "pdf_link": "https://arxiv.org/pdf/2305.10610v1.pdf"
    },
    {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "published": "2023-05-17T23:16:17Z",
        "summary": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.",
        "pdf_link": "https://arxiv.org/pdf/2305.10601v2.pdf"
    },
    {
        "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
        "authors": [
            "Zhaozhuo Xu",
            "Zirui Liu",
            "Beidi Chen",
            "Yuxin Tang",
            "Jue Wang",
            "Kaixiong Zhou",
            "Xia Hu",
            "Anshumali Shrivastava"
        ],
        "published": "2023-05-17T20:45:13Z",
        "summary": "While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.",
        "pdf_link": "https://arxiv.org/pdf/2305.11186v2.pdf"
    },
    {
        "title": "Statistical Knowledge Assessment for Large Language Models",
        "authors": [
            "Qingxiu Dong",
            "Jingjing Xu",
            "Lingpeng Kong",
            "Zhifang Sui",
            "Lei Li"
        ],
        "published": "2023-05-17T18:54:37Z",
        "summary": "Given varying prompts regarding a factoid question, can a large language\nmodel (LLM) reliably generate factually correct answers? Existing LLMs may\ngenerate distinct responses for different prompts. In this paper, we study the\nproblem of quantifying knowledge contained in an LLM regarding a given set of\nfacts. We propose KaRR, a statistical approach to assess factual knowledge for\nLLMs. The main idea is to estimate the ratio of LLM generating text\ncorresponding to the answer entity given diverse prompts of the subject and the\nquerying relation, versus it generating by random chances. Our assessment suite\ncontains a comprehensive set of 994,123 entities and 600 relations, with\n1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes,\nincluding LLaMA, Alpaca, OPT, etc. Experiments show that our results have a\nstrong correlation (0.43 Kendall's $\\tau$) with the results of human assessment\non LLMs. Our results reveal that the knowledge in LLMs with the same backbone\narchitecture adheres to the scaling law, while tuning on instruction-following\ndata sometimes compromises the model's capability to generate factually correct\ntext reliably.",
        "pdf_link": "https://arxiv.org/pdf/2305.10519v2.pdf"
    },
    {
        "title": "ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages",
        "authors": [
            "Sourojit Ghosh",
            "Aylin Caliskan"
        ],
        "published": "2023-05-17T18:30:05Z",
        "summary": "In this multicultural age, language translation is one of the most performed\ntasks, and it is becoming increasingly AI-moderated and automated. As a novel\nAI system, ChatGPT claims to be proficient in such translation tasks and in\nthis paper, we put that claim to the test. Specifically, we examine ChatGPT's\naccuracy in translating between English and languages that exclusively use\ngender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most\nspoken language globally, but also generalize our findings across five other\nlanguages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT\nperpetuates gender defaults and stereotypes assigned to certain occupations\n(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to\nwork), as it converts gender-neutral pronouns in languages to `he' or `she'. We\nalso observe ChatGPT completely failing to translate the English gender-neutral\npronoun `they' into equivalent gender-neutral pronouns in other languages, as\nit produces translations that are incoherent and incorrect. While it does\nrespect and provide appropriately gender-marked versions of Bengali words when\nprompted with gender information in English, ChatGPT appears to confer a higher\nrespect to men than to women in the same occupation. We conclude that ChatGPT\nexhibits the same gender biases which have been demonstrated for tools like\nGoogle Translate or MS Translator, as we provide recommendations for a human\ncentered approach for future designers of AIs that perform language translation\nto better accommodate such low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.10510v1.pdf"
    },
    {
        "title": "Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families",
        "authors": [
            "Stefania Druga",
            "Nancy Otero"
        ],
        "published": "2023-05-17T17:52:25Z",
        "summary": "How can AI enhance creative coding experiences for families? This study\nexplores the potential of large language models (LLMs) in helping families with\ncreative coding using Scratch. Based on our previous user study involving a\nprototype AI assistant, we devised three evaluation scenarios to determine if\nLLMs could help families comprehend game code, debug programs, and generate new\nideas for future projects. We utilized 22 Scratch projects for each scenario\nand generated responses from LLMs with and without practice tasks, resulting in\n120 creative coding support scenario datasets. In addition, the authors\nindependently evaluated their precision, pedagogical value, and age-appropriate\nlanguage. Our findings show that LLMs achieved an overall success rate of more\nthan 80\\% on the different tasks and evaluation criteria. This research offers\nvaluable information on using LLMs for creative family coding and presents\ndesign guidelines for future AI-supported coding applications. Our evaluation\nframework, together with our labeled evaluation data, is publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.10417v1.pdf"
    },
    {
        "title": "BAD: BiAs Detection for Large Language Models in the context of candidate screening",
        "authors": [
            "Nam Ho Koh",
            "Joseph Plata",
            "Joyce Chai"
        ],
        "published": "2023-05-17T17:47:31Z",
        "summary": "Application Tracking Systems (ATS) have allowed talent managers, recruiters,\nand college admissions committees to process large volumes of potential\ncandidate applications efficiently. Traditionally, this screening process was\nconducted manually, creating major bottlenecks due to the quantity of\napplications and introducing many instances of human bias. The advent of large\nlanguage models (LLMs) such as ChatGPT and the potential of adopting methods to\ncurrent automated application screening raises additional bias and fairness\nissues that must be addressed. In this project, we wish to identify and\nquantify the instances of social bias in ChatGPT and other OpenAI LLMs in the\ncontext of candidate screening in order to demonstrate how the use of these\nmodels could perpetuate existing biases and inequalities in the hiring process.",
        "pdf_link": "https://arxiv.org/pdf/2305.10407v1.pdf"
    },
    {
        "title": "Predicting Side Effect of Drug Molecules using Recurrent Neural Networks",
        "authors": [
            "Collin Beaudoin",
            "Koustubh Phalak",
            "Swaroop Ghosh"
        ],
        "published": "2023-05-17T16:56:19Z",
        "summary": "Identification and verification of molecular properties such as side effects\nis one of the most important and time-consuming steps in the process of\nmolecule synthesis. For example, failure to identify side effects before\nsubmission to regulatory groups can cost millions of dollars and months of\nadditional research to the companies. Failure to identify side effects during\nthe regulatory review can also cost lives. The complexity and expense of this\ntask have made it a candidate for a machine learning-based solution. Prior\napproaches rely on complex model designs and excessive parameter counts for\nside effect predictions. We believe reliance on complex models only shifts the\ndifficulty away from chemists rather than alleviating the issue. Implementing\nlarge models is also expensive without prior access to high-performance\ncomputers. We propose a heuristic approach that allows for the utilization of\nsimple neural networks, specifically the recurrent neural network, with a 98+%\nreduction in the number of required parameters compared to available large\nlanguage models while still obtaining near identical results as top-performing\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.10473v1.pdf"
    },
    {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-17T16:34:01Z",
        "summary": "Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.10355v3.pdf"
    },
    {
        "title": "Controllable Speaking Styles Using a Large Language Model",
        "authors": [
            "Atli Thor Sigurgeirsson",
            "Simon King"
        ],
        "published": "2023-05-17T16:01:50Z",
        "summary": "Reference-based Text-to-Speech (TTS) models can generate multiple,\nprosodically-different renditions of the same target text. Such models jointly\nlearn a latent acoustic space during training, which can be sampled from during\ninference. Controlling these models during inference typically requires finding\nan appropriate reference utterance, which is non-trivial.\n  Large generative language models (LLMs) have shown excellent performance in\nvarious language-related tasks. Given only a natural language query text (the\nprompt), such models can be used to solve specific, context-dependent tasks.\nRecent work in TTS has attempted similar prompt-based control of novel speaking\nstyle generation. Those methods do not require a reference utterance and can,\nunder ideal conditions, be controlled with only a prompt. But existing methods\ntypically require a prompt-labelled speech corpus for jointly training a\nprompt-conditioned encoder.\n  In contrast, we instead employ an LLM to directly suggest prosodic\nmodifications for a controllable TTS model, using contextual information\nprovided in the prompt. The prompt can be designed for a multitude of tasks.\nHere, we give two demonstrations: control of speaking style; prosody\nappropriate for a given dialogue context. The proposed method is rated most\nappropriate in 50% of cases vs. 31% for a baseline model.",
        "pdf_link": "https://arxiv.org/pdf/2305.10321v2.pdf"
    },
    {
        "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
        "authors": [
            "Hanxu Hu",
            "Hongyuan Lu",
            "Huajian Zhang",
            "Yun-Ze Song",
            "Wai Lam",
            "Yue Zhang"
        ],
        "published": "2023-05-17T15:07:50Z",
        "summary": "In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning",
        "pdf_link": "https://arxiv.org/pdf/2305.10276v6.pdf"
    },
    {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "published": "2023-05-17T14:40:29Z",
        "summary": "Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.",
        "pdf_link": "https://arxiv.org/pdf/2305.10250v3.pdf"
    },
    {
        "title": "Language Model Tokenizers Introduce Unfairness Between Languages",
        "authors": [
            "Aleksandar Petrov",
            "Emanuele La Malfa",
            "Philip H. S. Torr",
            "Adel Bibi"
        ],
        "published": "2023-05-17T14:17:57Z",
        "summary": "Recent language models have shown impressive multilingual performance, even\nwhen not explicitly trained for it. Despite this, there are concerns about the\nquality of their outputs across different languages. In this paper, we show how\ndisparity in the treatment of different languages arises at the tokenization\nstage, well before a model is even invoked. The same text translated into\ndifferent languages can have drastically different tokenization lengths, with\ndifferences up to 15 times in some cases. These disparities persist even for\ntokenizers that are intentionally trained for multilingual support.\nCharacter-level and byte-level models also exhibit over 4 times the difference\nin the encoding length for some language pairs. This induces unfair treatment\nfor some language communities in regard to the cost of accessing commercial\nlanguage services, the processing time and latency, as well as the amount of\ncontent that can be provided as context to the models. Therefore, we make the\ncase that we should train future language models using multilingually fair\nsubword tokenizers.",
        "pdf_link": "https://arxiv.org/pdf/2305.15425v2.pdf"
    },
    {
        "title": "Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries",
        "authors": [
            "Jiageng Wu",
            "Xian Wu",
            "Zhaopeng Qiu",
            "Minghui Li",
            "Yingying Zhang",
            "Yefeng Zheng",
            "Changzheng Yuan",
            "Jie Yang"
        ],
        "published": "2023-05-17T12:31:26Z",
        "summary": "$\\textbf{Objectives}$: Large Language Models (LLMs) such as ChatGPT and\nMed-PaLM have excelled in various medical question-answering tasks. However,\nthese English-centric models encounter challenges in non-English clinical\nsettings, primarily due to limited clinical knowledge in respective languages,\na consequence of imbalanced training corpora. We systematically evaluate LLMs\nin the Chinese medical context and develop a novel in-context learning\nframework to enhance their performance.\n  $\\textbf{Materials and Methods}$: The latest China National Medical Licensing\nExamination (CNMLE-2022) served as the benchmark. We collected 53 medical books\nand 381,149 medical questions to construct the medical knowledge base and\nquestion bank. The proposed Knowledge and Few-shot Enhancement In-context\nLearning (KFE) framework leverages the in-context learning ability of LLMs to\nintegrate diverse external clinical knowledge sources. We evaluated KFE with\nChatGPT(GPT3.5), GPT4, Baichuan2(BC2)-7B, and BC2-13B in CNMLE-2022 and\ninvestigated the effectiveness of different pathways for incorporating LLMs\nwith medical knowledge from 7 perspectives.\n  $\\textbf{Results}$: Directly applying ChatGPT failed to qualify for the\nCNMLE-2022 at a score of 51. Cooperated with the KFE, the LLMs with varying\nsizes yielded consistent and significant improvements. The ChatGPT's\nperformance surged to 70.04 and GPT-4 achieved the highest score of 82.59. This\nsurpasses the qualification threshold (60) and exceeds the average human score\nof 68.70. It also enabled a smaller BC2-13B to pass the examination, showcasing\nthe great potential in low-resource settings.\n  $\\textbf{Conclusion}$: By synergizing medical knowledge through in-context\nlearning, LLM can extend clinical insight beyond language barriers,\nsignificantly reducing language-related disparities of LLM applications and\nensuring global benefit in healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2305.10163v4.pdf"
    },
    {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "published": "2023-05-17T11:55:32Z",
        "summary": "We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player's negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model's negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game's rules or cannot incorporate AI feedback for further\nimprovement. (2) Models' abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.10142v1.pdf"
    },
    {
        "title": "Can Language Models Solve Graph Problems in Natural Language?",
        "authors": [
            "Heng Wang",
            "Shangbin Feng",
            "Tianxing He",
            "Zhaoxuan Tan",
            "Xiaochuang Han",
            "Yulia Tsvetkov"
        ],
        "published": "2023-05-17T08:29:21Z",
        "summary": "Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.",
        "pdf_link": "https://arxiv.org/pdf/2305.10037v3.pdf"
    },
    {
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "authors": [
            "Wenjun Peng",
            "Jingwei Yi",
            "Fangzhao Wu",
            "Shangxi Wu",
            "Bin Zhu",
            "Lingjuan Lyu",
            "Binxing Jiao",
            "Tong Xu",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023-05-17T08:28:54Z",
        "summary": "Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.",
        "pdf_link": "https://arxiv.org/pdf/2305.10036v3.pdf"
    },
    {
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
        "authors": [
            "Shangbin Feng",
            "Weijia Shi",
            "Yuyang Bai",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-05-17T05:25:27Z",
        "summary": "By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.",
        "pdf_link": "https://arxiv.org/pdf/2305.09955v3.pdf"
    },
    {
        "title": "\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
        "authors": [
            "Anaelia Ovalle",
            "Palash Goyal",
            "Jwala Dhamala",
            "Zachary Jaggers",
            "Kai-Wei Chang",
            "Aram Galstyan",
            "Richard Zemel",
            "Rahul Gupta"
        ],
        "published": "2023-05-17T04:21:45Z",
        "summary": "Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization of TGNB persons\ncontributes to and persists within Open Language Generation (OLG). This social\nknowledge serves as a guide for evaluating popular large language models (LLMs)\non two key aspects: (1) misgendering and (2) harmful responses to gender\ndisclosure. To do this, we introduce TANGO, a dataset of template-based\nreal-world text curated from a TGNB-oriented community. We discover a dominance\nof binary gender norms reflected by the models; LLMs least misgendered subjects\nin generated text when triggered by prompts whose subjects used binary\npronouns. Meanwhile, misgendering was most prevalent when triggering generation\nwith singular they and neopronouns. When prompted with gender disclosures, TGNB\ndisclosure generated the most stigmatizing language and scored most toxic, on\naverage. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.",
        "pdf_link": "https://arxiv.org/pdf/2305.09941v4.pdf"
    },
    {
        "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
        "authors": [
            "Niloofar Mireshghallah",
            "Justus Mattern",
            "Sicun Gao",
            "Reza Shokri",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2023-05-17T00:09:08Z",
        "summary": "With the advent of fluent generative language models that can produce\nconvincing utterances very similar to those written by humans, distinguishing\nwhether a piece of text is machine-generated or human-written becomes more\nchallenging and more important, as such models could be used to spread\nmisinformation, fake news, fake reviews and to mimic certain authors and\nfigures. To this end, there have been a slew of methods proposed to detect\nmachine-generated text. Most of these methods need access to the logits of the\ntarget model or need the ability to sample from the target. One such black-box\ndetection method relies on the observation that generated text is locally\noptimal under the likelihood function of the generator, while human-written\ntext is not. We find that overall, smaller and partially-trained models are\nbetter universal text detectors: they can more precisely detect text generated\nfrom both small and larger models. Interestingly, we find that whether the\ndetector and generator were trained on the same data is not critically\nimportant to the detection success. For instance the OPT-125M model has an AUC\nof 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT\nfamily, GPTJ-6B, has AUC of 0.45.",
        "pdf_link": "https://arxiv.org/pdf/2305.09859v4.pdf"
    },
    {
        "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs",
        "authors": [
            "Jiao Chen",
            "Luyi Ma",
            "Xiaohan Li",
            "Nikhil Thakurdesai",
            "Jianpeng Xu",
            "Jason H. D. Cho",
            "Kaushiki Nag",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "published": "2023-05-17T00:08:36Z",
        "summary": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system\nperformance by providing structured information about entities and their\nrelationships, such as complementary or substitutable relations between\nproducts or product types, which can be utilized in recommender systems.\nHowever, relation labeling in KGs remains a challenging task due to the dynamic\nnature of e-commerce domains and the associated cost of human labor. Recently,\nbreakthroughs in Large Language Models (LLMs) have shown surprising results in\nnumerous natural language processing tasks. In this paper, we conduct an\nempirical study of LLMs for relation labeling in e-commerce KGs, investigating\ntheir powerful learning capabilities in natural language and effectiveness in\npredicting relations between product types with limited labeled data. We\nevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,\ndemonstrating their ability to achieve competitive performance compared to\nhumans on relation labeling tasks using just 1 to 5 labeled examples per\nrelation. Additionally, we experiment with different prompt engineering\ntechniques to examine their impact on model performance. Our results show that\nLLMs significantly outperform existing KG completion models in relation\nlabeling for e-commerce KGs and exhibit performance strong enough to replace\nhuman labeling.",
        "pdf_link": "https://arxiv.org/pdf/2305.09858v1.pdf"
    },
    {
        "title": "Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites",
        "authors": [
            "Hans W. A. Hanley",
            "Zakir Durumeric"
        ],
        "published": "2023-05-16T21:51:01Z",
        "summary": "As large language models (LLMs) like ChatGPT have gained traction, an\nincreasing number of news websites have begun utilizing them to generate\narticles. However, not only can these language models produce factually\ninaccurate articles on reputable websites but disreputable news sites can\nutilize LLMs to mass produce misinformation. To begin to understand this\nphenomenon, we present one of the first large-scale studies of the prevalence\nof synthetic articles within online news media. To do this, we train a\nDeBERTa-based synthetic news detector and classify over 15.46 million articles\nfrom 3,074 misinformation and mainstream news websites. We find that between\nJanuary 1, 2022, and May 1, 2023, the relative number of synthetic news\narticles increased by 57.3% on mainstream websites while increasing by 474% on\nmisinformation sites. We find that this increase is largely driven by smaller\nless popular websites. Analyzing the impact of the release of ChatGPT using an\ninterrupted-time-series, we show that while its release resulted in a marked\nincrease in synthetic articles on small sites as well as misinformation news\nwebsites, there was not a corresponding increase on large mainstream news\nwebsites.",
        "pdf_link": "https://arxiv.org/pdf/2305.09820v5.pdf"
    },
    {
        "title": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models",
        "authors": [
            "Evan King",
            "Haoxiang Yu",
            "Sangsu Lee",
            "Christine Julien"
        ],
        "published": "2023-05-16T20:52:04Z",
        "summary": "Smart home assistants function best when user commands are direct and\nwell-specified (e.g., \"turn on the kitchen light\"), or when a hard-coded\nroutine specifies the response. In more natural communication, however, human\nspeech is unconstrained, often describing goals (e.g., \"make it cozy in here\"\nor \"help me save energy\") rather than indicating specific target devices and\nactions to take on those devices. Current systems fail to understand these\nunder-specified commands since they cannot reason about devices and settings as\nthey relate to human situations. We introduce large language models (LLMs) to\nthis problem space, exploring their use for controlling devices and creating\nautomation routines in response to under-specified user commands in smart\nhomes. We empirically study the baseline quality and failure modes of\nLLM-created action plans with a survey of age-diverse users. We find that LLMs\ncan reason creatively to achieve challenging goals, but they experience\npatterns of failure that diminish their usefulness. We address these gaps with\nSasha, a smarter smart home assistant. Sasha responds to loosely-constrained\ncommands like \"make it cozy\" or \"help me sleep better\" by executing plans to\nachieve user goals, e.g., setting a mood with available devices, or devising\nautomation routines. We implement and evaluate Sasha in a hands-on user study,\nshowing the capabilities and limitations of LLM-driven smart homes when faced\nwith unconstrained user-generated scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.09802v3.pdf"
    },
    {
        "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
        "authors": [
            "Jane Pan",
            "Tianyu Gao",
            "Howard Chen",
            "Danqi Chen"
        ],
        "published": "2023-05-16T18:05:19Z",
        "summary": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\nwith only a few demonstrations, but its mechanisms are not yet well-understood.\nSome works suggest that LLMs only recall already learned concepts from\npre-training, while others hint that ICL performs implicit learning over\ndemonstrations. We characterize two ways through which ICL leverages\ndemonstrations. Task recognition (TR) captures the extent to which LLMs can\nrecognize a task through demonstrations -- even without ground-truth labels --\nand apply their pre-trained priors, whereas task learning (TL) is the ability\nto capture new input-label mappings unseen in pre-training. Using a wide range\nof classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\ndesign controlled experiments to disentangle the roles of TR and TL in ICL. We\nshow that (1) models can achieve non-trivial performance with only TR, and TR\ndoes not further improve with larger models or more demonstrations; (2) LLMs\nacquire TL as the model scales, and TL's performance consistently improves with\nmore demonstrations in context. Our findings unravel two different forces\nbehind ICL and we advocate for discriminating them in future ICL research due\nto their distinct nature.",
        "pdf_link": "https://arxiv.org/pdf/2305.09731v1.pdf"
    },
    {
        "title": "SatLM: Satisfiability-Aided Language Models Using Declarative Prompting",
        "authors": [
            "Xi Ye",
            "Qiaochu Chen",
            "Isil Dillig",
            "Greg Durrett"
        ],
        "published": "2023-05-16T17:55:51Z",
        "summary": "Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving problems that require more sophisticated planning and\nsearch. In this paper, we propose a new satisfiability-aided language modeling\n(SatLM) approach for improving the reasoning capabilities of LLMs. We use an\nLLM to generate a declarative task specification rather than an imperative\nprogram and leverage an off-the-shelf automated theorem prover to derive the\nfinal answer. This approach has two key advantages. The declarative\nspecification is closer to the problem description than the reasoning steps\nare, so the LLM can parse it out of the description more accurately.\nFurthermore, by offloading the actual reasoning task to an automated theorem\nprover, our approach can guarantee the correctness of the answer with respect\nto the parsed specification and avoid planning errors in the solving process.\nWe evaluate SATLM on 8 different datasets and show that it consistently\noutperforms program-aided LMs in the imperative paradigm. In particular, SATLM\noutperforms program-aided LMs by 23% on a challenging subset of the GSM\narithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and\nBoardgameQA, surpassing previous models that are trained on the respective\ntraining sets.",
        "pdf_link": "https://arxiv.org/pdf/2305.09656v3.pdf"
    },
    {
        "title": "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction",
        "authors": [
            "Junsol Kim",
            "Byungkyu Lee"
        ],
        "published": "2023-05-16T17:13:07Z",
        "summary": "Large language models (LLMs) that produce human-like responses have begun to\nrevolutionize research practices in the social sciences. We develop a novel\nmethodological framework that fine-tunes LLMs with repeated cross-sectional\nsurveys to incorporate the meaning of survey questions, individual beliefs, and\ntemporal contexts for opinion prediction. We introduce two new emerging\napplications of the AI-augmented survey: retrodiction (i.e., predict year-level\nmissing responses) and unasked opinion prediction (i.e., predict entirely\nmissing responses). Among 3,110 binarized opinions from 68,846 Americans in the\nGeneral Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in\nretrodiction (AUC = 0.86 for personal opinion prediction, $\\rho$ = 0.98 for\npublic opinion prediction). These remarkable prediction capabilities allow us\nto fill in missing trends with high confidence and pinpoint when public\nattitudes changed, such as the rising support for same-sex marriage. On the\nother hand, our fine-tuned Alpaca-7b models show modest success in unasked\nopinion prediction (AUC = 0.73, $\\rho$ = 0.67). We discuss practical\nconstraints and ethical concerns regarding individual autonomy and privacy when\nusing LLMs for opinion prediction. Our study demonstrates that LLMs and surveys\ncan mutually enhance each other's capabilities: LLMs can broaden survey\npotential, while surveys can improve the alignment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.09620v3.pdf"
    },
    {
        "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
        "authors": [
            "Karan Singhal",
            "Tao Tu",
            "Juraj Gottweis",
            "Rory Sayres",
            "Ellery Wulczyn",
            "Le Hou",
            "Kevin Clark",
            "Stephen Pfohl",
            "Heather Cole-Lewis",
            "Darlene Neal",
            "Mike Schaekermann",
            "Amy Wang",
            "Mohamed Amin",
            "Sami Lachgar",
            "Philip Mansfield",
            "Sushant Prakash",
            "Bradley Green",
            "Ewa Dominowska",
            "Blaise Aguera y Arcas",
            "Nenad Tomasev",
            "Yun Liu",
            "Renee Wong",
            "Christopher Semturs",
            "S. Sara Mahdavi",
            "Joelle Barral",
            "Dale Webster",
            "Greg S. Corrado",
            "Yossi Matias",
            "Shekoofeh Azizi",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2023-05-16T17:11:29Z",
        "summary": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.",
        "pdf_link": "https://arxiv.org/pdf/2305.09617v1.pdf"
    },
    {
        "title": "Large Language Models are Built-in Autoregressive Search Engines",
        "authors": [
            "Noah Ziems",
            "Wenhao Yu",
            "Zhihan Zhang",
            "Meng Jiang"
        ],
        "published": "2023-05-16T17:04:48Z",
        "summary": "Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n  Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.",
        "pdf_link": "https://arxiv.org/pdf/2305.09612v1.pdf"
    },
    {
        "title": "Life of PII -- A PII Obfuscation Transformer",
        "authors": [
            "Ajinkya Deshmukh",
            "Saumya Banthia",
            "Anantha Sharma"
        ],
        "published": "2023-05-16T15:48:36Z",
        "summary": "Protecting sensitive information is crucial in today's world of Large\nLanguage Models (LLMs) and data-driven services. One common method used to\npreserve privacy is by using data perturbation techniques to reduce\noverreaching utility of (sensitive) Personal Identifiable Information (PII)\ndata while maintaining its statistical and semantic properties. Data\nperturbation methods often result in significant information loss, making them\nimpractical for use. In this paper, we propose 'Life of PII', a novel\nObfuscation Transformer framework for transforming PII into faux-PII while\npreserving the original information, intent, and context as much as possible.\nOur approach includes an API to interface with the given document, a\nconfiguration-based obfuscator, and a model based on the Transformer\narchitecture, which has shown high context preservation and performance in\nnatural language processing tasks and LLMs.\n  Our Transformer-based approach learns mapping between the original PII and\nits transformed faux-PII representation, which we call \"obfuscated\" data. Our\nexperiments demonstrate that our method, called Life of PII, outperforms\ntraditional data perturbation techniques in terms of both utility preservation\nand privacy protection. We show that our approach can effectively reduce\nutility loss while preserving the original information, offering greater\nflexibility in the trade-off between privacy protection and data utility. Our\nwork provides a solution for protecting PII in various real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.09550v2.pdf"
    },
    {
        "title": "CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling",
        "authors": [
            "Zheng Fang",
            "Yulan He",
            "Rob Procter"
        ],
        "published": "2023-05-16T10:07:33Z",
        "summary": "Most existing topic models rely on bag-of-words (BOW) representation, which\nlimits their ability to capture word order information and leads to challenges\nwith out-of-vocabulary (OOV) words in new documents. Contextualized word\nembeddings, however, show superiority in word sense disambiguation and\neffectively address the OOV issue. In this work, we introduce a novel neural\ntopic model called the Contextlized Word Topic Model (CWTM), which integrates\ncontextualized word embeddings from BERT. The model is capable of learning the\ntopic vector of a document without BOW information. In addition, it can also\nderive the topic vectors for individual words within a document based on their\ncontextualized word embeddings. Experiments across various datasets show that\nCWTM generates more coherent and meaningful topics compared to existing topic\nmodels, while also accommodating unseen words in newly encountered documents.",
        "pdf_link": "https://arxiv.org/pdf/2305.09329v3.pdf"
    },
    {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
        "authors": [
            "Hao Chen",
            "Yiming Zhang",
            "Qi Zhang",
            "Hantao Yang",
            "Xiaomeng Hu",
            "Xuetao Ma",
            "Yifan Yanggong",
            "Junbo Zhao"
        ],
        "published": "2023-05-16T07:52:57Z",
        "summary": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "pdf_link": "https://arxiv.org/pdf/2305.09246v1.pdf"
    },
    {
        "title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models",
        "authors": [
            "Boxi Cao",
            "Qiaoyu Tang",
            "Hongyu Lin",
            "Shanshan Jiang",
            "Bin Dong",
            "Xianpei Han",
            "Jiawei Chen",
            "Tianshu Wang",
            "Le Sun"
        ],
        "published": "2023-05-16T03:50:38Z",
        "summary": "Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.09144v2.pdf"
    },
    {
        "title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting",
        "authors": [
            "Xiaoying Zhang",
            "Baolin Peng",
            "Kun Li",
            "Jingyan Zhou",
            "Helen Meng"
        ],
        "published": "2023-05-15T23:29:56Z",
        "summary": "Building end-to-end task bots and maintaining their integration with new\nfunctionalities using minimal human efforts is a long-standing challenge in\ndialog research. Recently large language models (LLMs) have demonstrated\nexceptional proficiency in conversational engagement and adherence to\ninstructions across various downstream tasks. In this work, we introduce\nSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems\neffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we\ninstruct fixed LLMs to generate appropriate responses on novel tasks,\ncircumventing the need for training data. Specifically, SGP-TOD comprises three\ncomponents: a LLM for engaging with users, a DST Prompter to aid the LLM with\ndialog state tracking, which is then used to retrieve database items, and a\nPolicy Prompter to elicit proper responses adhering to the provided dialog\npolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show that\nour training-free strategy SGP-TOD, without any task-specific data, yields\nstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot\napproaches. In a domain-extension setting, SGP-TOD aptly adapts to new\nfunctionalities by merely adding supplementary schema rules. We make our code\nand data publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.09067v1.pdf"
    },
    {
        "title": "Small Models are Valuable Plug-ins for Large Language Models",
        "authors": [
            "Canwen Xu",
            "Yichong Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Chenguang Zhu",
            "Julian McAuley"
        ],
        "published": "2023-05-15T17:59:01Z",
        "summary": "Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their\nweights are often publicly unavailable and their immense sizes make the models\ndifficult to be tuned with common hardware. As a result, effectively tuning\nthese models with large-scale supervised data can be challenging. As an\nalternative, In-Context Learning (ICL) can only use a small number of\nsupervised examples due to context length limits. In this paper, we propose\nSuper In-Context Learning (SuperICL) which allows black-box LLMs to work with\nlocally fine-tuned smaller models, resulting in superior performance on\nsupervised tasks. Our experiments demonstrate that SuperICL can improve\nperformance beyond state-of-the-art fine-tuned models while addressing the\ninstability problem of in-context learning. Furthermore, SuperICL can enhance\nthe capabilities of smaller models, such as multilinguality and\ninterpretability.",
        "pdf_link": "https://arxiv.org/pdf/2305.08848v1.pdf"
    },
    {
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
        "authors": [
            "Yupeng Hou",
            "Junjie Zhang",
            "Zihan Lin",
            "Hongyu Lu",
            "Ruobing Xie",
            "Julian McAuley",
            "Wayne Xin Zhao"
        ],
        "published": "2023-05-15T17:57:39Z",
        "summary": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated\nimpressive general-purpose task-solving abilities, including the potential to\napproach recommendation tasks. Along this line of research, this work aims to\ninvestigate the capacity of LLMs that act as the ranking model for recommender\nsystems. We first formalize the recommendation problem as a conditional ranking\ntask, considering sequential interaction histories as conditions and the items\nretrieved by other candidate generation models as candidates. To solve the\nranking task by LLMs, we carefully design the prompting template and conduct\nextensive experiments on two widely-used datasets. We show that LLMs have\npromising zero-shot ranking abilities but (1) struggle to perceive the order of\nhistorical interactions, and (2) can be biased by popularity or item positions\nin the prompts. We demonstrate that these issues can be alleviated using\nspecially designed prompting and bootstrapping strategies. Equipped with these\ninsights, zero-shot LLMs can even challenge conventional recommendation models\nwhen ranking candidates are retrieved by multiple candidate generators. The\ncode and processed datasets are available at\nhttps://github.com/RUCAIBox/LLMRank.",
        "pdf_link": "https://arxiv.org/pdf/2305.08845v2.pdf"
    },
    {
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
        "authors": [
            "Afra Feyza Aky\u00fcrek",
            "Ekin Aky\u00fcrek",
            "Aman Madaan",
            "Ashwin Kalyan",
            "Peter Clark",
            "Derry Wijaya",
            "Niket Tandon"
        ],
        "published": "2023-05-15T17:57:16Z",
        "summary": "Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.",
        "pdf_link": "https://arxiv.org/pdf/2305.08844v2.pdf"
    },
    {
        "title": "Knowledge Rumination for Pre-trained Language Models",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Shengyu Mao",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-15T15:47:09Z",
        "summary": "Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.",
        "pdf_link": "https://arxiv.org/pdf/2305.08732v3.pdf"
    },
    {
        "title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
        "authors": [
            "Wentao Ye",
            "Mingfeng Ou",
            "Tianyi Li",
            "Yipeng chen",
            "Xuetao Ma",
            "Yifan Yanggong",
            "Sai Wu",
            "Jie Fu",
            "Gang Chen",
            "Haobo Wang",
            "Junbo Zhao"
        ],
        "published": "2023-05-15T15:44:51Z",
        "summary": "The recent popularity of large language models (LLMs) has brought a\nsignificant impact to boundless fields, particularly through their open-ended\necosystem such as the APIs, open-sourced models, and plugins. However, with\ntheir widespread deployment, there is a general lack of research that\nthoroughly discusses and analyzes the potential risks concealed. In that case,\nwe intend to conduct a preliminary but pioneering study covering the\nrobustness, consistency, and credibility of LLMs systems. With most of the\nrelated literature in the era of LLM uncharted, we propose an automated\nworkflow that copes with an upscaled number of queries/responses. Overall, we\nconduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,\nand OPT. Core to our workflow consists of a data primitive, followed by an\nautomated interpreter that evaluates these LLMs under different adversarial\nmetrical systems. As a result, we draw several, and perhaps unfortunate,\nconclusions that are quite uncommon from this trendy community. Briefly, they\nare: (i)-the minor but inevitable error occurrence in the user-generated query\ninput may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess\npoor consistency when processing semantically similar query input. In addition,\nas a side finding, we find that ChatGPT is still capable to yield the correct\nanswer even when the input is polluted at an extreme level. While this\nphenomenon demonstrates the powerful memorization of the LLMs, it raises\nserious concerns about using such data for LLM-involved evaluation in academic\ndevelopment. To deal with it, we propose a novel index associated with a\ndataset that roughly decides the feasibility of using such data for\nLLM-involved evaluation. Extensive empirical studies are tagged to support the\naforementioned claims.",
        "pdf_link": "https://arxiv.org/pdf/2305.10235v4.pdf"
    },
    {
        "title": "Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks",
        "authors": [
            "Chengguang Gan",
            "Tatsunori Mori"
        ],
        "published": "2023-05-15T15:19:08Z",
        "summary": "Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.",
        "pdf_link": "https://arxiv.org/pdf/2305.08714v2.pdf"
    },
    {
        "title": "Natural Language Decomposition and Interpretation of Complex Utterances",
        "authors": [
            "Harsh Jhamtani",
            "Hao Fang",
            "Patrick Xia",
            "Eran Levy",
            "Jacob Andreas",
            "Ben Van Durme"
        ],
        "published": "2023-05-15T14:35:00Z",
        "summary": "Designing natural language interfaces has historically required collecting\nsupervised data to translate user requests into carefully designed intent\nrepresentations. This requires enumerating and labeling a long tail of user\nrequests, which is challenging. At the same time, large language models (LLMs)\nencode knowledge about goals and plans that can help conversational assistants\ninterpret user requests requiring numerous steps to complete. We introduce an\napproach to handle complex-intent-bearing utterances from a user via a process\nof hierarchical natural language decomposition and interpretation. Our approach\nuses a pre-trained language model to decompose a complex utterance into a\nsequence of simpler natural language steps and interprets each step using the\nlanguage-to-program model designed for the interface. To test our approach, we\ncollect and release DeCU -- a new NL-to-program benchmark to evaluate\nDecomposition of Complex Utterances. Experiments show that the proposed\napproach enables the interpretation of complex utterances with almost no\ncomplex training data, while outperforming standard few-shot prompting\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2305.08677v2.pdf"
    },
    {
        "title": "Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering",
        "authors": [
            "Bing Wang",
            "Ximing Li",
            "Zhiyao Yang",
            "Yuanyuan Guan",
            "Jiayin Li",
            "Shengsheng Wang"
        ],
        "published": "2023-05-15T13:59:23Z",
        "summary": "Pre-trained Language Model (PLM) is nowadays the mainstay of Unsupervised\nSentence Representation Learning (USRL). However, PLMs are sensitive to the\nfrequency information of words from their pre-training corpora, resulting in\nanisotropic embedding space, where the embeddings of high-frequency words are\nclustered but those of low-frequency words disperse sparsely. This anisotropic\nphenomenon results in two problems of similarity bias and information bias,\nlowering the quality of sentence embeddings. To solve the problems, we\nfine-tune PLMs by leveraging the frequency information of words and propose a\nnovel USRL framework, namely Sentence Representation Learning with\nFrequency-induced Adversarial tuning and Incomplete sentence filtering\n(SLT-FAI). We calculate the word frequencies over the pre-training corpora of\nPLMs and assign words thresholding frequency labels. With them, (1) we\nincorporate a similarity discriminator used to distinguish the embeddings of\nhigh-frequency and low-frequency words, and adversarially tune the PLM with it,\nenabling to achieve uniformly frequency-invariant embedding space; and (2) we\npropose a novel incomplete sentence detection task, where we incorporate an\ninformation discriminator to distinguish the embeddings of original sentences\nand incomplete sentences by randomly masking several low-frequency words,\nenabling to emphasize the more informative low-frequency words. Our SLT-FAI is\na flexible and plug-and-play framework, and it can be integrated with existing\nUSRL techniques. We evaluate SLT-FAI with various backbones on benchmark\ndatasets. Empirical results indicate that SLT-FAI can be superior to the\nexisting USRL baselines. Our code is released in\n\\url{https://github.com/wangbing1416/SLT-FAI}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08655v1.pdf"
    },
    {
        "title": "Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study",
        "authors": [
            "Yaxin Fan",
            "Feng Jiang",
            "Peifeng Li",
            "Haizhou Li"
        ],
        "published": "2023-05-15T07:14:41Z",
        "summary": "Large language models, like ChatGPT, have shown remarkable capability in many\ndownstream tasks, yet their ability to understand discourse structures of\ndialogues remains less explored, where it requires higher level capabilities of\nunderstanding and reasoning. In this paper, we aim to systematically inspect\nChatGPT's performance in two discourse analysis tasks: topic segmentation and\ndiscourse parsing, focusing on its deep semantic understanding of linear and\nhierarchical discourse structures underlying dialogue. To instruct ChatGPT to\ncomplete these tasks, we initially craft a prompt template consisting of the\ntask description, output format, and structured input. Then, we conduct\nexperiments on four popular topic segmentation datasets and two discourse\nparsing datasets. The experimental results showcase that ChatGPT demonstrates\nproficiency in identifying topic structures in general-domain conversations yet\nstruggles considerably in specific-domain conversations. We also found that\nChatGPT hardly understands rhetorical structures that are more complex than\ntopic structures. Our deeper investigation indicates that ChatGPT can give more\nreasonable topic structures than human annotations but only linearly parses the\nhierarchical rhetorical structures. In addition, we delve into the impact of\nin-context learning (e.g., chain-of-thought) on ChatGPT and conduct the\nablation study on various prompt components, which can provide a research\nfoundation for future work. The code is available at\n\\url{https://github.com/yxfanSuda/GPTforDDA}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08391v2.pdf"
    },
    {
        "title": "Text Classification via Large Language Models",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Jiwei Li",
            "Fei Wu",
            "Shangwei Guo",
            "Tianwei Zhang",
            "Guoyin Wang"
        ],
        "published": "2023-05-15T06:24:45Z",
        "summary": "Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.",
        "pdf_link": "https://arxiv.org/pdf/2305.08377v3.pdf"
    },
    {
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "authors": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei",
            "Yao Fu",
            "Maosong Sun",
            "Junxian He"
        ],
        "published": "2023-05-15T03:20:19Z",
        "summary": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.",
        "pdf_link": "https://arxiv.org/pdf/2305.08322v3.pdf"
    },
    {
        "title": "Semantic Composition in Visually Grounded Language Models",
        "authors": [
            "Rohan Pandey"
        ],
        "published": "2023-05-15T03:19:42Z",
        "summary": "What is sentence meaning and its ideal representation? Much of the expressive\npower of human language derives from semantic composition, the mind's ability\nto represent meaning hierarchically & relationally over constituents. At the\nsame time, much sentential meaning is outside the text and requires grounding\nin sensory, motor, and experiential modalities to be adequately learned.\nAlthough large language models display considerable compositional ability,\nrecent work shows that visually-grounded language models drastically fail to\nrepresent compositional structure. In this thesis, we explore whether & how\nmodels compose visually grounded semantics, and how we might improve their\nability to do so.\n  Specifically, we introduce 1) WinogroundVQA, a new compositional visual\nquestion answering benchmark, 2) Syntactic Neural Module Distillation, a\nmeasure of compositional ability in sentence embedding models, 3) Causal\nTracing for Image Captioning Models to locate neural representations vital for\nvision-language composition, 4) Syntactic MeanPool to inject a compositional\ninductive bias into sentence embeddings, and 5) Cross-modal Attention\nCongruence Regularization, a self-supervised objective function for\nvision-language relation alignment. We close by discussing connections of our\nwork to neuroscience, psycholinguistics, formal semantics, and philosophy.",
        "pdf_link": "https://arxiv.org/pdf/2305.16328v1.pdf"
    },
    {
        "title": "Large Language Model Guided Tree-of-Thought",
        "authors": [
            "Jieyi Long"
        ],
        "published": "2023-05-15T01:18:23Z",
        "summary": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\napproach aimed at improving the problem-solving capabilities of auto-regressive\nlarge language models (LLMs). The ToT technique is inspired by the human mind's\napproach for solving complex reasoning tasks through trial and error. In this\nprocess, the human mind explores the solution space through a tree-like thought\nprocess, allowing for backtracking when necessary. To implement ToT as a\nsoftware system, we augment an LLM with additional modules including a prompter\nagent, a checker module, a memory module, and a ToT controller. In order to\nsolve a given problem, these modules engage in a multi-round conversation with\nthe LLM. The memory module records the conversation and state history of the\nproblem solving process, which allows the system to backtrack to the previous\nsteps of the thought-process and explore other directions from there. To verify\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\nsignificantly increase the success rate of Sudoku puzzle solving. Our\nimplementation of the ToT-based Sudoku solver is available on GitHub:\n\\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08291v1.pdf"
    },
    {
        "title": "$SmartProbe$: A Virtual Moderator for Market Research Surveys",
        "authors": [
            "Josh Seltzer",
            "Jiahua Pan",
            "Kathy Cheng",
            "Yuxiao Sun",
            "Santosh Kolagati",
            "Jimmy Lin",
            "Shi Zong"
        ],
        "published": "2023-05-14T22:36:08Z",
        "summary": "Market research surveys are a powerful methodology for understanding consumer\nperspectives at scale, but are limited by depth of understanding and insights.\nA virtual moderator can introduce elements of qualitative research into\nsurveys, developing a rapport with survey participants and dynamically asking\nprobing questions, ultimately to elicit more useful information for market\nresearchers. In this work, we introduce ${\\tt SmartProbe}$, an API which\nleverages the adaptive capabilities of large language models (LLMs), and\nincorporates domain knowledge from market research, in order to generate\neffective probing questions in any market research survey. We outline the\nmodular processing flow of $\\tt SmartProbe$, and evaluate the quality and\neffectiveness of its generated probing questions. We believe our efforts will\ninspire industry practitioners to build real-world applications based on the\nlatest advances in LLMs. Our demo is publicly available at\nhttps://nexxt.in/smartprobe-demo",
        "pdf_link": "https://arxiv.org/pdf/2305.08271v1.pdf"
    },
    {
        "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2023-05-14T20:57:11Z",
        "summary": "The field of Math-NLP has witnessed significant growth in recent years,\nmotivated by the desire to expand LLM performance to the learning of\nnon-linguistic notions (numerals, and subsequently, arithmetic reasoning).\nHowever, non-linguistic skill injection typically comes at a cost for LLMs: it\nleads to catastrophic forgetting of core linguistic skills, a consequence that\noften remains unaddressed in the literature. As Math-NLP has been able to\ncreate LLMs that can closely approximate the mathematical skills of a\ngrade-schooler or the arithmetic reasoning skills of a calculator, the\npracticality of these models fail if they concomitantly shed their linguistic\ncapabilities. In this work, we take a closer look into the phenomena of\ncatastrophic forgetting as it pertains to LLMs and subsequently offer a novel\nframework for non-linguistic skill injection for LLMs based on information\ntheoretic interventions and skill-specific losses that enable the learning of\nstrict arithmetic reasoning. Our model outperforms the state-of-the-art both on\ninjected non-linguistic skills and on linguistic knowledge retention, and does\nso with a fraction of the non-linguistic training data (1/4) and zero\nadditional synthetic linguistic training data.",
        "pdf_link": "https://arxiv.org/pdf/2305.08246v1.pdf"
    },
    {
        "title": "Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction",
        "authors": [
            "Danyang Zhang",
            "Hongshen Xu",
            "Zihan Zhao",
            "Lu Chen",
            "Ruisheng Cao",
            "Kai Yu"
        ],
        "published": "2023-05-14T12:31:03Z",
        "summary": "The User Interface (UI) is pivotal for human interaction with the digital\nworld, facilitating efficient control of machines, information navigation, and\ncomplex task completion. To achieve easy, efficient, and free interactions,\nresearchers have been exploring the potential of encapsulating the traditional\nProgramming Language Interfaces (PLIs) and Graphical User Interfaces (GUIs)\ninto Natural Language Interfaces (NLIs). However, due to the limited\ncapabilities of small models, traditional work mainly focuses on tasks for\nwhich only a single step is needed. This largely constrains the application of\nNLIs. Recently, Large Language Models (LLMs) have exhibited robust reasoning\nand planning abilities, yet their potential for multi-turn interactions in\ncomplex environments remains under-explored. To assess LLMs as NLIs in\nreal-world graphical environments, we introduce the GUI interaction platform,\nMobile-Env, specifically on mobile apps. Mobile-Env enhances interaction\nflexibility, task extensibility, and environment adaptability compared with\nprevious environments. A GUI task set based on WikiHow app is collected on\nMobile-Env to form a benchmark covering a range of GUI interaction\ncapabilities. We further conduct comprehensive evaluations of LLM agents,\nincluding various versions of GPT, LLaMA 2, and AgentLM, on WikiHow task set to\nacquire insights into the potentials and challenges of LLMs in GUI\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2305.08144v3.pdf"
    },
    {
        "title": "Watermarking Text Generated by Black-Box Language Models",
        "authors": [
            "Xi Yang",
            "Kejiang Chen",
            "Weiming Zhang",
            "Chang Liu",
            "Yuang Qi",
            "Jie Zhang",
            "Han Fang",
            "Nenghai Yu"
        ],
        "published": "2023-05-14T07:37:33Z",
        "summary": "LLMs now exhibit human-like skills in various fields, leading to worries\nabout misuse. Thus, detecting generated text is crucial. However, passive\ndetection methods are stuck in domain specificity and limited adversarial\nrobustness. To achieve reliable detection, a watermark-based method was\nproposed for white-box LLMs, allowing them to embed watermarks during text\ngeneration. The method involves randomly dividing the model vocabulary to\nobtain a special list and adjusting the probability distribution to promote the\nselection of words in the list. A detection algorithm aware of the list can\nidentify the watermarked text. However, this method is not applicable in many\nreal-world scenarios where only black-box language models are available. For\ninstance, third-parties that develop API-based vertical applications cannot\nwatermark text themselves because API providers only supply generated text and\nwithhold probability distributions to shield their commercial interests. To\nallow third-parties to autonomously inject watermarks into generated text, we\ndevelop a watermarking framework for black-box language model usage scenarios.\nSpecifically, we first define a binary encoding function to compute a random\nbinary encoding corresponding to a word. The encodings computed for\nnon-watermarked text conform to a Bernoulli distribution, wherein the\nprobability of a word representing bit-1 being approximately 0.5. To inject a\nwatermark, we alter the distribution by selectively replacing words\nrepresenting bit-0 with context-based synonyms that represent bit-1. A\nstatistical test is then used to identify the watermark. Experiments\ndemonstrate the effectiveness of our method on both Chinese and English\ndatasets. Furthermore, results under re-translation, polishing, word deletion,\nand synonym substitution attacks reveal that it is arduous to remove the\nwatermark without compromising the original semantics.",
        "pdf_link": "https://arxiv.org/pdf/2305.08883v1.pdf"
    },
    {
        "title": "ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance",
        "authors": [
            "Kaushik Roy",
            "Manas Gaur",
            "Misagh Soltani",
            "Vipula Rawte",
            "Ashwin Kalyan",
            "Amit Sheth"
        ],
        "published": "2023-05-13T21:31:02Z",
        "summary": "Current Virtual Mental Health Assistants (VMHAs) provide counseling and\nsuggestive care. They refrain from patient diagnostic assistance because they\nlack training in safety-constrained and specialized clinical process knowledge.\nIn this work, we define Proknow as an ordered set of information that maps to\nevidence-based guidelines or categories of conceptual understanding to experts\nin a domain. We also introduce a new dataset of diagnostic conversations guided\nby safety constraints and Proknow that healthcare professionals use. We develop\na method for natural language question generation (NLG) that collects\ndiagnostic information from the patient interactively. We demonstrate the\nlimitations of using state-of-the-art large-scale language models (LMs) on this\ndataset. Our algorithm models the process knowledge through explicitly modeling\nsafety, knowledge capture, and explainability. LMs augmented with ProKnow\nguided method generated 89% safer questions in the depression and anxiety\ndomain. The Explainability of the generated question is assessed by computing\nsimilarity with concepts in depression and anxiety knowledge bases. Overall,\nirrespective of the type of LMs augmented with our ProKnow, we achieved an\naverage 82% improvement over simple pre-trained LMs on safety, explainability,\nand process-guided question generation. We qualitatively and quantitatively\nevaluate the efficacy of the proposed ProKnow-guided methods by introducing\nthree new evaluation metrics for safety, explainability, and process knowledge\nadherence.",
        "pdf_link": "https://arxiv.org/pdf/2305.08010v2.pdf"
    },
    {
        "title": "Beyond the Safeguards: Exploring the Security Risks of ChatGPT",
        "authors": [
            "Erik Derner",
            "Kristina Batisti\u010d"
        ],
        "published": "2023-05-13T21:01:14Z",
        "summary": "The increasing popularity of large language models (LLMs) such as ChatGPT has\nled to growing concerns about their safety, security risks, and ethical\nimplications. This paper aims to provide an overview of the different types of\nsecurity risks associated with ChatGPT, including malicious text and code\ngeneration, private data disclosure, fraudulent services, information\ngathering, and producing unethical content. We present an empirical study\nexamining the effectiveness of ChatGPT's content filters and explore potential\nways to bypass these safeguards, demonstrating the ethical implications and\nsecurity risks that persist in LLMs even when protections are in place. Based\non a qualitative analysis of the security implications, we discuss potential\nstrategies to mitigate these risks and inform researchers, policymakers, and\nindustry professionals about the complex security challenges posed by LLMs like\nChatGPT. This study contributes to the ongoing discussion on the ethical and\nsecurity implications of LLMs, underscoring the need for continued research in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2305.08005v1.pdf"
    },
    {
        "title": "Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics",
        "authors": [
            "Steve Phelps",
            "Yvan I. Russell"
        ],
        "published": "2023-05-13T17:23:16Z",
        "summary": "In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms.",
        "pdf_link": "https://arxiv.org/pdf/2305.07970v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models in Conversational Recommender Systems",
        "authors": [
            "Luke Friedman",
            "Sameer Ahuja",
            "David Allen",
            "Zhenning Tan",
            "Hakim Sidahmed",
            "Changbo Long",
            "Jun Xie",
            "Gabriel Schubiner",
            "Ajay Patel",
            "Harsh Lara",
            "Brian Chu",
            "Zexi Chen",
            "Manoj Tiwari"
        ],
        "published": "2023-05-13T16:40:07Z",
        "summary": "A Conversational Recommender System (CRS) offers increased transparency and\ncontrol to users by enabling them to engage with the system through a real-time\nmulti-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an\nunprecedented ability to converse naturally and incorporate world knowledge and\ncommon-sense reasoning into language understanding, unlocking the potential of\nthis paradigm. However, effectively leveraging LLMs within a CRS introduces new\ntechnical challenges, including properly understanding and controlling a\ncomplex conversation and retrieving from external sources of information. These\nissues are exacerbated by a large, evolving item corpus and a lack of\nconversational data for training. In this paper, we provide a roadmap for\nbuilding an end-to-end large-scale CRS using LLMs. In particular, we propose\nnew implementations for user preference understanding, flexible dialogue\nmanagement and explainable recommendations as part of an integrated\narchitecture powered by LLMs. For improved personalization, we describe how an\nLLM can consume interpretable natural language user profiles and use them to\nmodulate session-level context. To overcome conversational data limitations in\nthe absence of an existing production CRS, we propose techniques for building a\ncontrollable LLM-based user simulator to generate synthetic conversations. As a\nproof of concept we introduce RecLLM, a large-scale CRS for YouTube videos\nbuilt on LaMDA, and demonstrate its fluency and diverse functionality through\nsome illustrative example conversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07961v2.pdf"
    },
    {
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "authors": [
            "Yue Wang",
            "Hung Le",
            "Akhilesh Deepak Gotmare",
            "Nghi D. Q. Bui",
            "Junnan Li",
            "Steven C. H. Hoi"
        ],
        "published": "2023-05-13T14:23:07Z",
        "summary": "Large language models (LLMs) pretrained on vast source code have achieved\nprominent progress in code intelligence. However, existing code LLMs have two\nmain limitations in terms of architecture and pretraining tasks. First, they\noften adopt a specific architecture (encoder-only or decoder-only) or rely on a\nunified encoder-decoder network for different downstream tasks. The former\nparadigm is limited by inflexibility in applications while in the latter, the\nmodel is treated as a single system for all tasks, leading to suboptimal\nperformance on a subset of tasks. Secondly, they often employ a limited set of\npretraining objectives which might not be relevant to some downstream tasks and\nhence result in substantial performance degrade. To address these limitations,\nwe propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which\ncomponent modules can be flexibly combined to suit a wide range of downstream\ncode tasks. Such flexibility is enabled by our proposed mixture of pretraining\nobjectives to mitigate the pretrain-finetune discrepancy. These objectives\ncover span denoising, contrastive learning, text-code matching, and causal LM\npretraining tasks, on both unimodal and bimodal multilingual code corpora.\nFurthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs\nwithout training from scratch to efficiently scale up our models, and explore\ninstruction-tuning to align with natural language instructions. We extensively\nevaluate CodeT5+ on over 20 code-related benchmarks in different settings,\nincluding zero-shot, finetuning, and instruction-tuning. We observe\nstate-of-the-art (SoTA) model performance on various code-related tasks, such\nas code generation and completion, math programming, and text-to-code retrieval\ntasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA\nresults on HumanEval code generation task against other open code LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07922v2.pdf"
    },
    {
        "title": "Dual Use Concerns of Generative AI and Large Language Models",
        "authors": [
            "Alexei Grinbaum",
            "Laurynas Adomaitis"
        ],
        "published": "2023-05-13T10:08:57Z",
        "summary": "We suggest the implementation of the Dual Use Research of Concern (DURC)\nframework, originally designed for life sciences, to the domain of generative\nAI, with a specific focus on Large Language Models (LLMs). With its\ndemonstrated advantages and drawbacks in biological research, we believe the\nDURC criteria can be effectively redefined for LLMs, potentially contributing\nto improved AI governance. Acknowledging the balance that must be struck when\nemploying the DURC framework, we highlight its crucial political role in\nenhancing societal awareness of the impact of generative AI. As a final point,\nwe offer a series of specific recommendations for applying the DURC approach to\nLLM research.",
        "pdf_link": "https://arxiv.org/pdf/2305.07882v2.pdf"
    },
    {
        "title": "Improving Small Language Models on PubMedQA via Generative Data Augmentation",
        "authors": [
            "Zhen Guo",
            "Peiqi Wang",
            "Yanwei Wang",
            "Shangdi Yu"
        ],
        "published": "2023-05-12T23:49:23Z",
        "summary": "Large Language Models (LLMs) have made remarkable advancements in the field\nof natural language processing. However, their increasing size poses challenges\nin terms of computational cost. On the other hand, Small Language Models (SLMs)\nare known for their efficiency, but they often struggle with limited capacity\nand training data, especially in specific domains. In this paper, we introduce\na novel method aimed at improving SLMs in the medical domain using LLM-based\ngenerative data augmentation. The objective of our approach is to develop more\nefficient and capable models that are specifically tailored for specialized\napplications. Through experiments conducted on the PubMedQA dataset, we\ndemonstrate the effectiveness of LLMs in refining and diversifying existing\nquestion-answer pairs. This refinement process leads to improved performance in\na significantly smaller model after fine-tuning. Notably, our best SLM, with\nunder 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA\ndataset. Our code and generated data are publicly available to facilitate\nfurther explorations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07804v4.pdf"
    },
    {
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
        "authors": [
            "Yongchao Chen",
            "Rujul Gandhi",
            "Yang Zhang",
            "Chuchu Fan"
        ],
        "published": "2023-05-12T21:22:08Z",
        "summary": "Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (>95%) using only <10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.",
        "pdf_link": "https://arxiv.org/pdf/2305.07766v2.pdf"
    },
    {
        "title": "Knowledge Authoring for Rules and Actions",
        "authors": [
            "Yuheng Wang",
            "Paul Fodor",
            "Michael Kifer"
        ],
        "published": "2023-05-12T21:08:35Z",
        "summary": "Knowledge representation and reasoning (KRR) systems describe and reason with\ncomplex concepts and relations in the form of facts and rules. Unfortunately,\nwide deployment of KRR systems runs into the problem that domain experts have\ngreat difficulty constructing correct logical representations of their domain\nknowledge. Knowledge engineers can help with this construction process, but\nthere is a deficit of such specialists. The earlier Knowledge Authoring Logic\nMachine (KALM) based on Controlled Natural Language (CNL) was shown to have\nvery high accuracy for authoring facts and questions. More recently, KALMFL, a\nsuccessor of KALM, replaced CNL with factual English, which is much less\nrestrictive and requires very little training from users. However, KALMFL has\nlimitations in representing certain types of knowledge, such as authoring rules\nfor multi-step reasoning or understanding actions with timestamps. To address\nthese limitations, we propose KALMRA to enable authoring of rules and actions.\nOur evaluation using the UTI guidelines benchmark shows that KALMRA achieves a\nhigh level of correctness (100%) on rule authoring. When used for authoring and\nreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI\nbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.\nFinally, we illustrate the logical reasoning capabilities of KALMRA by drawing\nattention to the problems faced by the recently made famous AI, ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.07763v1.pdf"
    },
    {
        "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "authors": [
            "Ronen Eldan",
            "Yuanzhi Li"
        ],
        "published": "2023-05-12T20:56:48Z",
        "summary": "Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07759v2.pdf"
    },
    {
        "title": "Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery",
        "authors": [
            "Pranav Kulkarni",
            "Adway Kanhere",
            "Paul H. Yi",
            "Vishwa S. Parekh"
        ],
        "published": "2023-05-12T17:46:06Z",
        "summary": "The Imaging Data Commons (IDC) is a cloud-based database that provides\nresearchers with open access to cancer imaging data, with the goal of\nfacilitating collaboration. However, cohort discovery within the IDC database\nhas a significant technical learning curve. Recently, large language models\n(LLM) have demonstrated exceptional utility for natural language processing\ntasks. We developed Text2Cohort, a LLM-powered toolkit to facilitate\nuser-friendly natural language cohort discovery in the IDC. Our method\ntranslates user input into IDC queries using grounding techniques and returns\nthe query's response. We evaluate Text2Cohort on 50 natural language inputs,\nfrom information extraction to cohort discovery. Our toolkit successfully\ngenerated responses with an 88% accuracy and 0.94 F1 score. We demonstrate that\nText2Cohort can enable researchers to discover and curate cohorts on IDC with\nhigh levels of accuracy using natural language in a more intuitive and\nuser-friendly way.",
        "pdf_link": "https://arxiv.org/pdf/2305.07637v3.pdf"
    },
    {
        "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Yang Zhang",
            "Wenjie Wang",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "published": "2023-05-12T16:54:36Z",
        "summary": "The remarkable achievements of Large Language Models (LLMs) have led to the\nemergence of a novel recommendation paradigm -- Recommendation via LLM\n(RecLLM). Nevertheless, it is important to note that LLMs may contain social\nprejudices, and therefore, the fairness of recommendations made by RecLLM\nrequires further investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to various sensitive\nattributes on the user side. Due to the differences between the RecLLM paradigm\nand the traditional recommendation paradigm, it is problematic to directly use\nthe fairness benchmark of traditional recommendation. To address the dilemma,\nwe propose a novel benchmark called Fairness of Recommendation via LLM\n(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset\nthat accounts for eight sensitive attributes1 in two recommendation scenarios:\nmusic and movies. By utilizing our FaiRLLM benchmark, we conducted an\nevaluation of ChatGPT and discovered that it still exhibits unfairness to some\nsensitive attributes when generating recommendations. Our code and dataset can\nbe found at https://github.com/jizhi-zhang/FaiRLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.07609v3.pdf"
    },
    {
        "title": "Generative AI: Implications and Applications for Education",
        "authors": [
            "Anastasia Olga",
            "Tzirides",
            "Akash Saini",
            "Gabriela Zapata",
            "Duane Searsmith",
            "Bill Cope",
            "Mary Kalantzis",
            "Vania Castro",
            "Theodora Kourkoulou",
            "John Jones",
            "Rodrigo Abrantes da Silva",
            "Jen Whiting",
            "Nikoleta Polyxeni Kastania"
        ],
        "published": "2023-05-12T16:52:38Z",
        "summary": "The launch of ChatGPT in November 2022 precipitated a panic among some\neducators while prompting qualified enthusiasm from others. Under the umbrella\nterm Generative AI, ChatGPT is an example of a range of technologies for the\ndelivery of computer-generated text, image, and other digitized media. This\npaper examines the implications for education of one generative AI technology,\nchatbots responding from large language models, or C-LLM. It reports on an\napplication of a C-LLM to AI review and assessment of complex student work. In\na concluding discussion, the paper explores the intrinsic limits of generative\nAI, bound as it is to language corpora and their textual representation through\nbinary notation. Within these limits, we suggest the range of emerging and\npotential applications of Generative AI in education.",
        "pdf_link": "https://arxiv.org/pdf/2305.07605v3.pdf"
    },
    {
        "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
        "authors": [
            "Ilias Chalkidis",
            "Nicolas Garneau",
            "Catalina Goanta",
            "Daniel Martin Katz",
            "Anders S\u00f8gaard"
        ],
        "published": "2023-05-12T14:21:38Z",
        "summary": "In this work, we conduct a detailed analysis on the performance of\nlegal-oriented pre-trained language models (PLMs). We examine the interplay\nbetween their original objective, acquired knowledge, and legal language\nunderstanding capacities which we define as the upstream, probing, and\ndownstream performance, respectively. We consider not only the models' size but\nalso the pre-training corpora used as important dimensions in our study. To\nthis end, we release a multinational English legal corpus (LeXFiles) and a\nlegal knowledge probing benchmark (LegalLAMA) to facilitate training and\ndetailed analysis of legal-oriented PLMs. We release two new legal PLMs trained\non LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We\nfind that probing performance strongly correlates with upstream performance in\nrelated legal topics. On the other hand, downstream performance is mainly\ndriven by the model's size and prior legal knowledge which can be estimated by\nupstream and probing performance. Based on these findings, we can conclude that\nboth dimensions are important for those seeking the development of\ndomain-specific PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07507v2.pdf"
    },
    {
        "title": "Calibration-Aware Bayesian Learning",
        "authors": [
            "Jiayi Huang",
            "Sangwoo Park",
            "Osvaldo Simeone"
        ],
        "published": "2023-05-12T14:19:15Z",
        "summary": "Deep learning models, including modern systems like large language models,\nare well known to offer unreliable estimates of the uncertainty of their\ndecisions. In order to improve the quality of the confidence levels, also known\nas calibration, of a model, common approaches entail the addition of either\ndata-dependent or data-independent regularization terms to the training loss.\nData-dependent regularizers have been recently introduced in the context of\nconventional frequentist learning to penalize deviations between confidence and\naccuracy. In contrast, data-independent regularizers are at the core of\nBayesian learning, enforcing adherence of the variational distribution in the\nmodel parameter space to a prior density. The former approach is unable to\nquantify epistemic uncertainty, while the latter is severely affected by model\nmisspecification. In light of the limitations of both methods, this paper\nproposes an integrated framework, referred to as calibration-aware Bayesian\nneural networks (CA-BNNs), that applies both regularizers while optimizing over\na variational distribution as in Bayesian learning. Numerical results validate\nthe advantages of the proposed approach in terms of expected calibration error\n(ECE) and reliability diagrams.",
        "pdf_link": "https://arxiv.org/pdf/2305.07504v1.pdf"
    },
    {
        "title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter",
        "authors": [
            "Zhengqing Yuan",
            "Yunhong He",
            "Kun Wang",
            "Yanfang Ye",
            "Lichao Sun"
        ],
        "published": "2023-05-12T14:04:30Z",
        "summary": "The success of large language models (LLMs) has inspired an emerging research\nfield of multimodal learning. However, a grand challenge of exploiting LLMs for\nmultimodal learning is the size of pre-trained LLMs which are always with\nbillions of parameters. To tackle this challenge, models such as MiniGPT-4 and\nLLaVA have been developed to fine-tune the pre-trained models using fewer\nparameters. Despite their promising performance, these models remain limited in\ntheir understanding of artistic imagery. To facilitate better\nartistic-understanding, in this paper, we propose ArtGPT-4, a pioneering large\nvision-language model tailored to address the limitations of existing models in\nartistic comprehension. The key innovation of ArtGPT-4 lies in its craft for\nthe sophisticated challenge of artistic image comprehension, setting it apart\nfrom other models that overlook fine details for broader themes. Specifically,\nit works by integrating some specialized adapter layers into the LLM, enabling\nthe model to more efficiently and effectively parse and interpret complex\nvisual tokens, instead of fine-tuning the whole LLM as in the existing method.\nArtGPT-4 has demonstrated its outstanding performance on the efficiency:\nutilizing a Tesla A100 device, its training can be completed in mere 2 hours\nwith an image-text pair dataset comprising approximately 0.52M entries.\nAdditionally, ArtGPT-4 has also achieved state-of-the-art performance on the\nArtEmis and ArtEmis-v2.0 datasets as well as the benchmarks established in this\nwork, lagging behind professional artists' descriptions by a negligible 0.15\npoints on a 6-point scale. The outstanding performance of ArtGPT-4 shows that\nit can render images with an artistic-understanding and convey the emotions\nthey inspire, mirroring human interpretation. The code and the pre-trained\nmodel are accessible in \\url{https://github.com/DLYuanGod/ArtGPT-4}.",
        "pdf_link": "https://arxiv.org/pdf/2305.07490v6.pdf"
    },
    {
        "title": "Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation",
        "authors": [
            "Tu Anh Dinh",
            "Jan Niehues"
        ],
        "published": "2023-05-12T13:10:57Z",
        "summary": "Quality Estimation (QE) is the task of predicting the quality of Machine\nTranslation (MT) system output, without using any gold-standard translation\nreferences. State-of-the-art QE models are supervised: they require\nhuman-labeled quality of some MT system output on some datasets for training,\nmaking them domain-dependent and MT-system-dependent. There has been research\non unsupervised QE, which requires glass-box access to the MT systems, or\nparallel MT data to generate synthetic errors for training QE models. In this\npaper, we present Perturbation-based QE - a word-level Quality Estimation\napproach that works simply by analyzing MT system output on perturbed input\nsource sentences. Our approach is unsupervised, explainable, and can evaluate\nany type of blackbox MT systems, including the currently prominent large\nlanguage models (LLMs) with opaque internal processes. For language directions\nwith no labeled QE data, our approach has similar or better performance than\nthe zero-shot supervised approach on the WMT21 shared task. Our approach is\nbetter at detecting gender bias and word-sense-disambiguation errors in\ntranslation than supervised QE, indicating its robustness to out-of-domain\nusage. The performance gap is larger when detecting errors on a nontraditional\ntranslation-prompting LLM, indicating that our approach is more generalizable\nto different MT systems. We give examples demonstrating our approach's\nexplainability power, where it shows which input source words have influence on\na certain MT output word.",
        "pdf_link": "https://arxiv.org/pdf/2305.07457v2.pdf"
    },
    {
        "title": "Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation",
        "authors": [
            "Lei Liu",
            "Jimmy Xiangji Huang"
        ],
        "published": "2023-05-12T11:41:16Z",
        "summary": "Dialogue systems for non-English languages have long been under-explored. In\nthis paper, we take the first step to investigate few-shot cross-lingual\ntransfer learning (FS-XLT) and multitask learning (MTL) in the context of\nopen-domain dialogue generation for non-English languages with limited data. We\nobserved catastrophic forgetting in both FS-XLT and MTL for all 6 languages in\nour preliminary experiments. To mitigate the issue, we propose a simple yet\neffective prompt learning approach that can preserve the multilinguality of\nmultilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging\nthe gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and\nour hand-crafted prompts. Experimental results on all 6 languages in terms of\nboth automatic and human evaluations demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.",
        "pdf_link": "https://arxiv.org/pdf/2305.07393v2.pdf"
    },
    {
        "title": "Surfacing Biases in Large Language Models using Contrastive Input Decoding",
        "authors": [
            "Gal Yona",
            "Or Honovich",
            "Itay Laish",
            "Roee Aharoni"
        ],
        "published": "2023-05-12T11:09:49Z",
        "summary": "Ensuring that large language models (LMs) are fair, robust and useful\nrequires an understanding of how different modifications to their inputs impact\nthe model's behaviour. In the context of open-text generation tasks, however,\nsuch an evaluation is not trivial. For example, when introducing a model with\nan input text and a perturbed, \"contrastive\" version of it, meaningful\ndifferences in the next-token predictions may not be revealed with standard\ndecoding strategies. With this motivation in mind, we propose Contrastive Input\nDecoding (CID): a decoding algorithm to generate text given two inputs, where\nthe generated text is likely given one input but unlikely given the other. In\nthis way, the contrastive generations can highlight potentially subtle\ndifferences in how the LM output differs for the two inputs in a simple and\ninterpretable manner. We use CID to highlight context-specific biases that are\nhard to detect with standard decoding strategies and quantify the effect of\ndifferent input perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07378v1.pdf"
    },
    {
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation",
        "authors": [
            "Jinglong Gao",
            "Xiao Ding",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-05-12T10:54:13Z",
        "summary": "Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\nexplainer. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\nThe code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .",
        "pdf_link": "https://arxiv.org/pdf/2305.07375v4.pdf"
    },
    {
        "title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust",
        "authors": [
            "Minh-Tien Nguyen",
            "Duy-Hung Nguyen",
            "Shahab Sabahi",
            "Hung Le",
            "Jeff Yang",
            "Hajime Hotta"
        ],
        "published": "2023-05-12T03:49:59Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, with GPT models at the forefront. While their remarkable\nperformance spans a range of tasks, adapting LLMs for real-world business\nscenarios still poses challenges warranting further investigation. This paper\npresents an empirical analysis aimed at bridging the gap in adapting LLMs to\npractical use cases. To do that, we select the question answering (QA) task of\ninsurance as a case study due to its challenge of reasoning. Based on the task\nwe design a new model relied on LLMs which are empowered by additional\nknowledge extracted from insurance policy rulebooks and DBpedia. The additional\nknowledge helps LLMs to understand new concepts of insurance for domain\nadaptation. Preliminary results on two QA datasets show that knowledge\nenhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and\n57.83% in terms of accuracy). The analysis also indicates that existing public\nknowledge bases, e.g., DBPedia is beneficial for knowledge enhancement. Our\nfindings reveal that the inherent complexity of business scenarios often\nnecessitates the incorporation of domain-specific knowledge and external\nresources for effective problem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2305.07230v2.pdf"
    },
    {
        "title": "Exploring Zero and Few-shot Techniques for Intent Classification",
        "authors": [
            "Soham Parikh",
            "Quaizar Vohra",
            "Prashil Tumbade",
            "Mitul Tiwari"
        ],
        "published": "2023-05-11T22:07:27Z",
        "summary": "Conversational NLU providers often need to scale to thousands of\nintent-classification models where new customers often face the cold-start\nproblem. Scaling to so many customers puts a constraint on storage space as\nwell. In this paper, we explore four different zero and few-shot intent\nclassification approaches with this low-resource constraint: 1) domain\nadaptation, 2) data augmentation, 3) zero-shot intent classification using\ndescriptions large language models (LLMs), and 4) parameter-efficient\nfine-tuning of instruction-finetuned language models. Our results show that all\nthese approaches are effective to different degrees in low-resource settings.\nParameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on\nFlan-T5 (Chang et al., 2022) yields the best performance even with just one\nsample per intent. We also show that the zero-shot method of prompting LLMs\nusing intent descriptions",
        "pdf_link": "https://arxiv.org/pdf/2305.07157v1.pdf"
    },
    {
        "title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
        "authors": [
            "Gokul Yenduri",
            "Ramalingam M",
            "Chemmalar Selvi G",
            "Supriya Y",
            "Gautam Srivastava",
            "Praveen Kumar Reddy Maddikunta",
            "Deepti Raj G",
            "Rutvij H Jhaveri",
            "Prabadevi B",
            "Weizheng Wang",
            "Athanasios V. Vasilakos",
            "Thippa Reddy Gadekallu"
        ],
        "published": "2023-05-11T19:20:38Z",
        "summary": "The Generative Pre-trained Transformer (GPT) represents a notable\nbreakthrough in the domain of natural language processing, which is propelling\nus toward the development of machines that can understand and communicate using\nlanguage in a manner that closely resembles that of humans. GPT is based on the\ntransformer architecture, a deep neural network designed for natural language\nprocessing tasks. Due to their impressive performance on natural language\nprocessing tasks and ability to effectively converse, GPT have gained\nsignificant popularity among researchers and industrial communities, making\nthem one of the most widely used and effective models in natural language\nprocessing and related fields, which motivated to conduct this review. This\nreview provides a detailed overview of the GPT, including its architecture,\nworking process, training procedures, enabling technologies, and its impact on\nvarious applications. In this review, we also explored the potential challenges\nand limitations of a GPT. Furthermore, we discuss potential solutions and\nfuture directions. Overall, this paper aims to provide a comprehensive\nunderstanding of GPT, enabling technologies, their impact on various\napplications, emerging challenges, and potential solutions.",
        "pdf_link": "https://arxiv.org/pdf/2305.10435v2.pdf"
    },
    {
        "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales",
        "authors": [
            "Brihi Joshi",
            "Ziyi Liu",
            "Sahana Ramnath",
            "Aaron Chan",
            "Zhewei Tong",
            "Shaoliang Nie",
            "Qifan Wang",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-05-11T19:01:13Z",
        "summary": "Among the remarkable emergent capabilities of large language models (LMs) is\nfree-text rationalization; beyond a certain scale, large LMs are capable of\ngenerating seemingly useful rationalizations, which in turn, can dramatically\nenhance their performances on leaderboards. This phenomenon raises a question:\ncan machine generated rationales also be useful for humans, especially when lay\nhumans try to answer questions based on those machine rationales? We observe\nthat human utility of existing rationales is far from satisfactory, and\nexpensive to estimate with human studies. Existing metrics like task\nperformance of the LM generating the rationales, or similarity between\ngenerated and gold rationales are not good indicators of their human utility.\nWhile we observe that certain properties of rationales like conciseness and\nnovelty are correlated with their human utility, estimating them without human\ninvolvement is challenging. We show that, by estimating a rationale's\nhelpfulness in answering similar unseen instances, we can measure its human\nutility to a better extent. We also translate this finding into an automated\nscore, GEN-U, that we propose, which can help improve LMs' ability to generate\nrationales with better human utility, while maintaining most of its task\nperformance. Lastly, we release all code and collected data with this project.",
        "pdf_link": "https://arxiv.org/pdf/2305.07095v1.pdf"
    },
    {
        "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
        "authors": [
            "Haoyang Huang",
            "Tianyi Tang",
            "Dongdong Zhang",
            "Wayne Xin Zhao",
            "Ting Song",
            "Yan Xia",
            "Furu Wei"
        ],
        "published": "2023-05-11T17:44:17Z",
        "summary": "Large language models (LLMs) demonstrate impressive multilingual capability,\nbut their performance varies substantially across different languages. In this\nwork, we introduce a simple yet effective method, called cross-lingual-thought\nprompting (XLT), to systematically improve the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt that stimulates cross-lingual\nand logical reasoning skills to enhance task performance across languages. We\nconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,\nunderstanding, and generation tasks, covering both high-resource and\nlow-resource languages. Experimental results show that XLT not only remarkably\nenhances the performance of various multilingual tasks but also significantly\nreduces the gap between the average performance and the best performance of\neach task in different languages. Notably, XLT brings over 10 points of average\nimprovement in arithmetic reasoning and open-domain question-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.07004v2.pdf"
    },
    {
        "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
        "authors": [
            "Junjie Zhang",
            "Ruobing Xie",
            "Yupeng Hou",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-11T17:39:07Z",
        "summary": "In the past decades, recommender systems have attracted much attention in\nboth research and industry communities, and a large number of studies have been\ndevoted to developing effective recommendation models. Basically speaking,\nthese models mainly learn the underlying user preference from historical\nbehavior data, and then estimate the user-item matching relationships for\nrecommendations. Inspired by the recent progress on large language models\n(LLMs), we take a different approach to developing the recommendation models,\nconsidering recommendation as instruction following by LLMs. The key idea is\nthat the preferences or needs of a user can be expressed in natural language\ndescriptions (called instructions), so that LLMs can understand and further\nexecute the instruction for fulfilling the recommendation task. Instead of\nusing public APIs of LLMs, we instruction tune an open-source LLM (3B\nFlan-T5-XL), in order to better adapt LLMs to recommender systems. For this\npurpose, we first design a general instruction format for describing the\npreference, intention, task form and context of a user in natural language.\nThen we manually design 39 instruction templates and automatically generate a\nlarge amount of user-personalized instruction data (252K instructions) with\nvarying types of preferences and intentions. To demonstrate the effectiveness\nof our approach, we instantiate the instruction templates into several\nwidely-studied recommendation (or search) tasks, and conduct extensive\nexperiments on these tasks with real-world datasets. Experiment results show\nthat the proposed approach can outperform several competitive baselines,\nincluding the powerful GPT-3.5, on these evaluation tasks. Our approach sheds\nlight on developing more user-friendly recommender systems, in which users can\nfreely communicate with the system and obtain more accurate recommendations via\nnatural language instructions.",
        "pdf_link": "https://arxiv.org/pdf/2305.07001v1.pdf"
    },
    {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "authors": [
            "Ehsan Kamalloo",
            "Nouha Dziri",
            "Charles L. A. Clarke",
            "Davood Rafiei"
        ],
        "published": "2023-05-11T17:14:33Z",
        "summary": "Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.06984v3.pdf"
    },
    {
        "title": "Active Retrieval Augmented Generation",
        "authors": [
            "Zhengbao Jiang",
            "Frank F. Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2023-05-11T17:13:40Z",
        "summary": "Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing retrieval\naugmented LMs employ a retrieve-and-generate setup that only retrieves\ninformation once based on the input. This is limiting, however, in more general\nscenarios involving generation of long texts, where continually gathering\ninformation throughout generation is essential. In this work, we provide a\ngeneralized view of active retrieval augmented generation, methods that\nactively decide when and what to retrieve across the course of the generation.\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\ngeneric method which iteratively uses a prediction of the upcoming sentence to\nanticipate future content, which is then utilized as a query to retrieve\nrelevant documents to regenerate the sentence if it contains low-confidence\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\ncompetitive performance on all tasks, demonstrating the effectiveness of our\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "pdf_link": "https://arxiv.org/pdf/2305.06983v2.pdf"
    },
    {
        "title": "Spear Phishing With Large Language Models",
        "authors": [
            "Julian Hazell"
        ],
        "published": "2023-05-11T16:55:19Z",
        "summary": "Recent progress in artificial intelligence (AI), particularly in the domain\nof large language models (LLMs), has resulted in powerful and versatile\ndual-use systems. This intelligence can be put towards a wide variety of\nbeneficial tasks, yet it can also be used to cause harm. This study explores\none such harm by examining how LLMs can be used for spear phishing, a form of\ncybercrime that involves manipulating targets into divulging sensitive\ninformation. I first explore LLMs' ability to assist with the reconnaissance\nand message generation stages of a spear phishing attack, where I find that\nLLMs are capable of assisting with the email generation phase of a spear\nphishing attack. To explore how LLMs could potentially be harnessed to scale\nspear phishing campaigns, I then create unique spear phishing messages for over\n600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My\nfindings provide some evidence that these messages are not only realistic but\nalso cost-effective, with each email costing only a fraction of a cent to\ngenerate. Next, I demonstrate how basic prompt engineering can circumvent\nsafeguards installed in LLMs, highlighting the need for further research into\nrobust interventions that can help prevent models from being misused. To\nfurther address these evolving risks, I explore two potential solutions:\nstructured access schemes, such as application programming interfaces, and\nLLM-based defensive systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.06972v3.pdf"
    },
    {
        "title": "Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models",
        "authors": [
            "Luk\u00e1\u0161 Mikula",
            "Michal \u0160tef\u00e1nik",
            "Marek Petrovi\u010d",
            "Petr Sojka"
        ],
        "published": "2023-05-11T14:35:00Z",
        "summary": "While the Large Language Models (LLMs) dominate a majority of language\nunderstanding tasks, previous work shows that some of these results are\nsupported by modelling spurious correlations of training datasets. Authors\ncommonly assess model robustness by evaluating their models on\nout-of-distribution (OOD) datasets of the same task, but these datasets might\nshare the bias of the training dataset.\n  We propose a simple method for measuring a scale of models' reliance on any\nidentified spurious feature and assess the robustness towards a large set of\nknown and newly found prediction biases for various pre-trained models and\ndebiasing methods in Question Answering (QA). We find that while existing\ndebiasing methods can mitigate reliance on a chosen spurious feature, the OOD\nperformance gains of these methods can not be explained by mitigated reliance\non biased features, suggesting that biases are shared among different QA\ndatasets. Finally, we evidence this to be the case by measuring that the\nperformance of models trained on different QA datasets relies comparably on the\nsame bias features. We hope these results will motivate future work to refine\nthe reports of LMs' robustness to a level of adversarial samples addressing\nspecific spurious features.",
        "pdf_link": "https://arxiv.org/pdf/2305.06841v2.pdf"
    },
    {
        "title": "Structured Chain-of-Thought Prompting for Code Generation",
        "authors": [
            "Jia Li",
            "Ge Li",
            "Yongmin Li",
            "Zhi Jin"
        ],
        "published": "2023-05-11T06:43:37Z",
        "summary": "Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. LLMs take prompts as inputs, and\nChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.\nCoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural\nlanguage reasoning steps) and then output the code. However, CoT prompting is\ndesigned for natural language generation and has low accuracy in code\ngeneration.\n  In this paper, we propose Structured CoTs (SCoTs) and present a novel\nprompting technique for code generation, named SCoT prompting. Our motivation\nis source code contains rich structural information and any code can be\ncomposed of three program structures (i.e., sequence, branch, and loop\nstructures). Intuitively, structured intermediate reasoning steps make for\nstructured source code. Thus, we ask LLMs to use program structures to build\nCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.\nCompared to CoT prompting, SCoT prompting explicitly constrains LLMs to think\nabout how to solve requirements from the view of source code and further the\nperformance of LLMs in code generation. We apply SCoT prompting to two LLMs\n(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,\nMBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline\n- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human\ndevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust to\nexamples and achieves substantial improvements.",
        "pdf_link": "https://arxiv.org/pdf/2305.06599v3.pdf"
    },
    {
        "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
        "authors": [
            "Hongyuan Lu",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Haoran Yang",
            "Wai Lam",
            "Furu Wei"
        ],
        "published": "2023-05-11T05:19:47Z",
        "summary": "Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.06575v3.pdf"
    },
    {
        "title": "How Good are Commercial Large Language Models on African Languages?",
        "authors": [
            "Jessica Ojo",
            "Kelechi Ogueji"
        ],
        "published": "2023-05-11T02:29:53Z",
        "summary": "Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.",
        "pdf_link": "https://arxiv.org/pdf/2305.06530v1.pdf"
    },
    {
        "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
        "authors": [
            "Han Cheol Moon",
            "Shafiq Joty",
            "Ruochen Zhao",
            "Megh Thakkar",
            "Xu Chi"
        ],
        "published": "2023-05-11T01:50:16Z",
        "summary": "Large-scale pre-trained language models have shown outstanding performance in\na variety of NLP tasks. However, they are also known to be significantly\nbrittle against specifically crafted adversarial examples, leading to\nincreasing interest in probing the adversarial robustness of NLP systems. We\nintroduce RSMI, a novel two-stage framework that combines randomized smoothing\n(RS) with masked inference (MI) to improve the adversarial robustness of NLP\nsystems. RS transforms a classifier into a smoothed classifier to obtain robust\nrepresentations, whereas MI forces a model to exploit the surrounding context\nof a masked token in an input sequence. RSMI improves adversarial robustness by\n2 to 3 times over existing state-of-the-art methods on benchmark datasets. We\nalso perform in-depth qualitative analysis to validate the effectiveness of the\ndifferent stages of RSMI and probe the impact of its components through\nextensive ablations. By empirically proving the stability of RSMI, we put it\nforward as a practical method to robustly train large-scale NLP models. Our\ncode and datasets are available at https://github.com/Han8931/rsmi_nlp",
        "pdf_link": "https://arxiv.org/pdf/2305.06522v1.pdf"
    },
    {
        "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
        "authors": [
            "Wang-Cheng Kang",
            "Jianmo Ni",
            "Nikhil Mehta",
            "Maheswaran Sathiamoorthy",
            "Lichan Hong",
            "Ed Chi",
            "Derek Zhiyuan Cheng"
        ],
        "published": "2023-05-10T21:43:42Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\ngeneralizing to new tasks in a zero-shot or few-shot manner. However, the\nextent to which LLMs can comprehend user preferences based on their previous\nbehavior remains an emerging and still unclear research question.\nTraditionally, Collaborative Filtering (CF) has been the most effective method\nfor these tasks, predominantly relying on the extensive volume of rating data.\nIn contrast, LLMs typically demand considerably less data while maintaining an\nexhaustive world knowledge about each item, such as movies or products. In this\npaper, we conduct a thorough examination of both CF and LLMs within the classic\ntask of user rating prediction, which involves predicting a user's rating for a\ncandidate item based on their past ratings. We investigate various LLMs in\ndifferent sizes, ranging from 250M to 540B parameters and evaluate their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. We conduct\ncomprehensive analysis to compare between LLMs and strong CF methods, and find\nthat zero-shot LLMs lag behind traditional recommender models that have the\naccess to user interaction data, indicating the importance of user interaction\ndata. However, through fine-tuning, LLMs achieve comparable or even better\nperformance with only a small fraction of the training data, demonstrating\ntheir potential through data efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.06474v1.pdf"
    },
    {
        "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
        "authors": [
            "Hong Wang",
            "Xuan Luo",
            "Weizhi Wang",
            "Xifeng Yan"
        ],
        "published": "2023-05-10T19:09:24Z",
        "summary": "Large language models like ChatGPT have recently demonstrated impressive\ncapabilities in natural language understanding and generation, enabling various\napplications including translation, essay writing, and chit-chatting. However,\nthere is a concern that they can be misused for malicious purposes, such as\nfraud or denial-of-service attacks. Therefore, it is crucial to develop methods\nfor detecting whether the party involved in a conversation is a bot or a human.\nIn this paper, we propose a framework named FLAIR, Finding Large language model\nAuthenticity via a single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, positioning, noise filtering, and ASCII art),\nand those that are easy for bots but difficult for humans (e.g., memorization\nand computation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities and ensure that they are\nserving real users. We open-sourced our dataset on\nhttps://github.com/hongwang600/FLAIR and welcome contributions from the\ncommunity to enrich such detection datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.06424v2.pdf"
    },
    {
        "title": "Automatic Evaluation of Attribution by Large Language Models",
        "authors": [
            "Xiang Yue",
            "Boshi Wang",
            "Ziru Chen",
            "Kai Zhang",
            "Yu Su",
            "Huan Sun"
        ],
        "published": "2023-05-10T16:58:33Z",
        "summary": "A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support its claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is fully supported by the cited reference,\nremains an open problem. Although human evaluation is common practice, it is\ncostly and time-consuming. In this paper, we investigate the automatic\nevaluation of attribution given by LLMs. We begin by defining different types\nof attribution errors, and then explore two approaches for automatic\nevaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is\nrepurposed from related tasks such as question answering, fact-checking,\nnatural language inference, and summarization. We manually curate a set of test\nexamples covering 12 domains from a generative search engine, New Bing. Our\nresults on this curated test set and simulated examples from existing\nbenchmarks highlight both promising signals and challenges. We hope our problem\nformulation, testbeds, and findings will help lay the foundation for future\nstudies on this important problem.",
        "pdf_link": "https://arxiv.org/pdf/2305.06311v2.pdf"
    },
    {
        "title": "Evaluating Embedding APIs for Information Retrieval",
        "authors": [
            "Ehsan Kamalloo",
            "Xinyu Zhang",
            "Odunayo Ogundepo",
            "Nandan Thakur",
            "David Alfonso-Hermelo",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023-05-10T16:40:52Z",
        "summary": "The ever-increasing size of language models curtails their widespread\navailability to the community, thereby galvanizing many companies into offering\naccess to large language models through APIs. One particular type, suitable for\ndense retrieval, is a semantic embedding service that builds vector\nrepresentations of input text. With a growing number of publicly available\nAPIs, our goal in this paper is to analyze existing offerings in realistic\nretrieval scenarios, to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we investigate the\ncapabilities of existing semantic embedding APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate these services on two\nstandard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective in English,\nin contrast to the standard practice of employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best, albeit at a higher cost. We hope our\nwork lays the groundwork for evaluating semantic embedding APIs that are\ncritical in search and more broadly, for information access.",
        "pdf_link": "https://arxiv.org/pdf/2305.06300v2.pdf"
    },
    {
        "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
        "authors": [
            "Chantal Shaib",
            "Millicent L. Li",
            "Sebastian Joseph",
            "Iain J. Marshall",
            "Junyi Jessy Li",
            "Byron C. Wallace"
        ],
        "published": "2023-05-10T16:40:37Z",
        "summary": "Large language models, particularly GPT-3, are able to produce high quality\nsummaries of general domain news articles in few- and zero-shot settings.\nHowever, it is unclear if such models are similarly capable in more\nspecialized, high-stakes domains such as biomedicine. In this paper, we enlist\ndomain experts (individuals with medical training) to evaluate summaries of\nbiomedical articles generated by GPT-3, given zero supervision. We consider\nboth single- and multi-document settings. In the former, GPT-3 is tasked with\ngenerating regular and plain-language summaries of articles describing\nrandomized controlled trials; in the latter, we assess the degree to which\nGPT-3 is able to \\emph{synthesize} evidence reported across a collection of\narticles. We design an annotation scheme for evaluating model outputs, with an\nemphasis on assessing the factual accuracy of generated summaries. We find that\nwhile GPT-3 is able to summarize and simplify single biomedical articles\nfaithfully, it struggles to provide accurate aggregations of findings over\nmultiple documents. We release all data and annotations used in this work.",
        "pdf_link": "https://arxiv.org/pdf/2305.06299v2.pdf"
    },
    {
        "title": "Privacy-Preserving Prompt Tuning for Large Language Model Services",
        "authors": [
            "Yansong Li",
            "Zhixing Tan",
            "Yang Liu"
        ],
        "published": "2023-05-10T14:41:51Z",
        "summary": "Prompt tuning provides an efficient way for users to customize Large Language\nModels (LLMs) with their private data in the emerging LLM service scenario.\nHowever, the sensitive nature of private data brings the need for privacy\npreservation in LLM service customization. Based on prompt tuning, we propose\nPrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy\nguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,\nallowing users to privatize their data locally with local differential privacy.\nAs prompt tuning performs poorly when directly trained on privatized data, we\nintroduce a novel privatized token reconstruction task that is trained jointly\nwith the downstream task, allowing LLMs to learn better task-dependent\nrepresentations. Despite the simplicity of our framework, experiments show that\nRAPT achieves competitive performance across tasks while providing privacy\nguarantees against adversaries.",
        "pdf_link": "https://arxiv.org/pdf/2305.06212v1.pdf"
    },
    {
        "title": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
        "authors": [
            "Qingyu Chen",
            "Jingcheng Du",
            "Yan Hu",
            "Vipina Kuttichi Keloth",
            "Xueqing Peng",
            "Kalpana Raja",
            "Rui Zhang",
            "Zhiyong Lu",
            "Hua Xu"
        ],
        "published": "2023-05-10T13:40:06Z",
        "summary": "Biomedical literature is growing rapidly, making it challenging to curate and\nextract knowledge manually. Biomedical natural language processing (BioNLP)\ntechniques that can automatically extract information from biomedical\nliterature help alleviate this burden. Recently, large Language Models (LLMs),\nsuch as GPT-3 and GPT-4, have gained significant attention for their impressive\nperformance. However, their effectiveness in BioNLP tasks and impact on method\ndevelopment and downstream users remain understudied. This pilot study (1)\nestablishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and\none-shot settings in eight BioNLP datasets across four applications: named\nentity recognition, relation extraction, multi-label document classification,\nand semantic similarity and reasoning, (2) examines the errors produced by the\nLLMs and categorized the errors into three types: missingness, inconsistencies,\nand unwanted artificial content, and (3) provides suggestions for using LLMs in\nBioNLP applications. We make the datasets, baselines, and results publicly\navailable to the community via\nhttps://github.com/qingyu-qc/gpt_bionlp_benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2305.16326v2.pdf"
    },
    {
        "title": "Davinci the Dualist: the mind-body divide in large language models and in human learners",
        "authors": [
            "Iris Berent",
            "Alexzander Sansiveri"
        ],
        "published": "2023-05-10T12:28:09Z",
        "summary": "A large literature suggests that people are intuitive Dualists--they consider\nthe mind ethereal, distinct from the body. Past research also shows that\nDualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But\nwhether learning is sufficient to give rise to Dualism is unknown.The evidence\nfrom human learners does address this question because humans are endowed not\nonly with general learning capacities but also with core knowledge capacities.\nAnd recent results suggest that core knowledge begets Dualism (Berent, Theodore\n& Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we\nprobe for a mind-body divide in Davinci--a large language model (LLM) that is\ndevoid of any innate core knowledge. We show that Davinci still leans towards\nDualism, and that this bias increases systematically with the learner's\ninductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist\ntendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a\nfull-blown bias. It selectively considers thoughts (epistemic states) as\ndisembodied--as unlikely to show up in the body (in the brain), but not in its\nabsence (after death). While Davinci's performance is constrained by its\nsyntactic limitations, and it differs from humans, its Dualist bias is robust.\nThese results demonstrate that the mind-body divide is partly learnable from\nexperience.They also show how, as LLM's are exposed to human narratives, they\ninduce not only human knowledge but also human biases.",
        "pdf_link": "https://arxiv.org/pdf/2305.07667v2.pdf"
    },
    {
        "title": "Enriching language models with graph-based context information to better understand textual data",
        "authors": [
            "Albert Roethel",
            "Maria Ganzha",
            "Anna Wr\u00f3blewska"
        ],
        "published": "2023-05-10T10:57:21Z",
        "summary": "A considerable number of texts encountered daily are somehow connected with\neach other. For example, Wikipedia articles refer to other articles via\nhyperlinks, scientific papers relate to others via citations or (co)authors,\nwhile tweets relate via users that follow each other or reshare content. Hence,\na graph-like structure can represent existing connections and be seen as\ncapturing the \"context\" of the texts. The question thus arises if extracting\nand integrating such context information into a language model might help\nfacilitate a better automated understanding of the text. In this study, we\nexperimentally demonstrate that incorporating graph-based contextualization\ninto BERT model enhances its performance on an example of a classification\ntask. Specifically, on Pubmed dataset, we observed a reduction in error from\n8.51% to 7.96%, while increasing the number of parameters just by 1.6%.\n  Our source code: https://github.com/tryptofanik/gc-bert",
        "pdf_link": "https://arxiv.org/pdf/2305.11070v1.pdf"
    },
    {
        "title": "Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models",
        "authors": [
            "Varun Nair",
            "Elliot Schumacher",
            "Anitha Kannan"
        ],
        "published": "2023-05-10T08:48:53Z",
        "summary": "A medical provider's summary of a patient visit serves several critical\npurposes, including clinical decision-making, facilitating hand-offs between\nproviders, and as a reference for the patient. An effective summary is required\nto be coherent and accurately capture all the medically relevant information in\nthe dialogue, despite the complexity of patient-generated language. Even minor\ninaccuracies in visit summaries (for example, summarizing \"patient does not\nhave a fever\" when a fever is present) can be detrimental to the outcome of\ncare for the patient.\n  This paper tackles the problem of medical conversation summarization by\ndiscretizing the task into several smaller dialogue-understanding tasks that\nare sequentially built upon. First, we identify medical entities and their\naffirmations within the conversation to serve as building blocks. We study\ndynamically constructing few-shot prompts for tasks by conditioning on relevant\npatient information and use GPT-3 as the backbone for our experiments. We also\ndevelop GPT-derived summarization metrics to measure performance against\nreference summaries quantitatively. Both our human evaluation study and metrics\nfor medical correctness show that summaries generated using this approach are\nclinically accurate and outperform the baseline approach of summarizing the\ndialog in a zero-shot, single-prompt setting.",
        "pdf_link": "https://arxiv.org/pdf/2305.05982v1.pdf"
    },
    {
        "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
        "authors": [
            "Jiangjie Chen",
            "Wei Shi",
            "Ziquan Fu",
            "Sijie Cheng",
            "Lei Li",
            "Yanghua Xiao"
        ],
        "published": "2023-05-10T08:35:50Z",
        "summary": "Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.",
        "pdf_link": "https://arxiv.org/pdf/2305.05976v2.pdf"
    },
    {
        "title": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition",
        "authors": [
            "Anis Koubaa",
            "Basit Qureshi",
            "Adel Ammar",
            "Zahid Khan",
            "Wadii Boulila",
            "Lahouari Ghouti"
        ],
        "published": "2023-05-10T08:16:46Z",
        "summary": "Since the release of ChatGPT, numerous studies have highlighted the\nremarkable performance of ChatGPT, which often rivals or even surpasses human\ncapabilities in various tasks and domains. However, this paper presents a\ncontrasting perspective by demonstrating an instance where human performance\nexcels in typical tasks suited for ChatGPT, specifically in the domain of\ncomputer programming. We utilize the IEEExtreme Challenge competition as a\nbenchmark, a prestigious, annual international programming contest encompassing\na wide range of problems with different complexities. To conduct a thorough\nevaluation, we selected and executed a diverse set of 102 challenges, drawn\nfrom five distinct IEEExtreme editions, using three major programming\nlanguages: Python, Java, and C++. Our empirical analysis provides evidence that\ncontrary to popular belief, human programmers maintain a competitive edge over\nChatGPT in certain aspects of problem-solving within the programming context.\nIn fact, we found that the average score obtained by ChatGPT on the set of\nIEEExtreme programming problems is 3.9 to 5.8 times lower than the average\nhuman score, depending on the programming language. This paper elaborates on\nthese findings, offering critical insights into the limitations and potential\nareas of improvement for AI-based language models like ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.06934v1.pdf"
    },
    {
        "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer",
        "authors": [
            "Zhiqiang Hu",
            "Roy Ka-Wei Lee",
            "Nancy F. Chen"
        ],
        "published": "2023-05-10T07:33:36Z",
        "summary": "Adapting a large language model for multiple-attribute text style transfer\nvia fine-tuning can be challenging due to the significant amount of\ncomputational resources and labeled data required for the specific task. In\nthis paper, we address this challenge by introducing AdapterTST, a framework\nthat freezes the pre-trained model's original parameters and enables the\ndevelopment of a multiple-attribute text style transfer model. Using BART as\nthe backbone model, Adapter-TST utilizes different neural adapters to capture\ndifferent attribute information, like a plug-in connected to BART. Our method\nallows control over multiple attributes, like sentiment, tense, voice, etc.,\nand configures the adapters' architecture to generate multiple outputs\nrespected to attributes or compositional editing on the same sentence. We\nevaluate the proposed model on both traditional sentiment transfer and\nmultiple-attribute transfer tasks. The experiment results demonstrate that\nAdapter-TST outperforms all the state-of-the-art baselines with significantly\nlesser computational resources. We have also empirically shown that each\nadapter is able to capture specific stylistic attributes effectively and can be\nconfigured to perform compositional editing.",
        "pdf_link": "https://arxiv.org/pdf/2305.05945v1.pdf"
    },
    {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "authors": [
            "Eshaan Tanwar",
            "Subhabrata Dutta",
            "Manish Borthakur",
            "Tanmoy Chakraborty"
        ],
        "published": "2023-05-10T07:24:36Z",
        "summary": "In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.",
        "pdf_link": "https://arxiv.org/pdf/2305.05940v3.pdf"
    },
    {
        "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks",
        "authors": [
            "Xianzhi Li",
            "Samuel Chan",
            "Xiaodan Zhu",
            "Yulong Pei",
            "Zhiqiang Ma",
            "Xiaomo Liu",
            "Sameena Shah"
        ],
        "published": "2023-05-10T03:13:54Z",
        "summary": "The most recent large language models(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of generalist models, achieving state-of-the-art\nperformance on a wide range of NLP tasks with little or no adaptation. How\neffective are such models in the financial domain? Understanding this basic\nquestion would have a significant impact on many downstream financial\nanalytical tasks. In this paper, we conduct an empirical study and provide\nexperimental evidences of their performance on a wide variety of financial text\nanalytical problems, using eight benchmark datasets from five categories of\ntasks. We report both the strengths and limitations of the current models by\ncomparing them to the state-of-the-art fine-tuned approaches and the recently\nreleased domain-specific pretrained models. We hope our study can help\nunderstand the capability of the existing models in the financial domain and\nfacilitate further improvements.",
        "pdf_link": "https://arxiv.org/pdf/2305.05862v2.pdf"
    },
    {
        "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
        "authors": [
            "Travis Munyer",
            "Abdullah Tanvir",
            "Arjon Das",
            "Xin Zhong"
        ],
        "published": "2023-05-09T21:31:07Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of text generators. With the potential for misuse\nescalating, the importance of discerning whether texts are human-authored or\ngenerated by LLMs has become paramount. Several preceding studies have ventured\nto address this challenge by employing binary classifiers to differentiate\nbetween human-written and LLM-generated text. Nevertheless, the reliability of\nthese classifiers has been subject to question. Given that consequential\ndecisions may hinge on the outcome of such classification, it is imperative\nthat text source detection is of high caliber. In light of this, the present\npaper introduces DeepTextMark, a deep learning-driven text watermarking\nmethodology devised for text source identification. By leveraging Word2Vec and\nSentence Encoding for watermark insertion, alongside a transformer-based\nclassifier for watermark detection, DeepTextMark epitomizes a blend of\nblindness, robustness, imperceptibility, and reliability. As elaborated within\nthe paper, these attributes are crucial for universal text source detection,\nwith a particular emphasis in this paper on text produced by LLMs. DeepTextMark\noffers a viable \"add-on\" solution to prevailing text generation frameworks,\nrequiring no direct access or alterations to the underlying text generation\nmechanism. Experimental evaluations underscore the high imperceptibility,\nelevated detection accuracy, augmented robustness, reliability, and swift\nexecution of DeepTextMark.",
        "pdf_link": "https://arxiv.org/pdf/2305.05773v2.pdf"
    },
    {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "published": "2023-05-09T17:52:59Z",
        "summary": "For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.05658v2.pdf"
    },
    {
        "title": "Towards Building the Federated GPT: Federated Instruction Tuning",
        "authors": [
            "Jianyi Zhang",
            "Saeed Vahidian",
            "Martin Kuo",
            "Chunyuan Li",
            "Ruiyi Zhang",
            "Tong Yu",
            "Yufan Zhou",
            "Guoyin Wang",
            "Yiran Chen"
        ],
        "published": "2023-05-09T17:42:34Z",
        "summary": "While \"instruction-tuned\" generative large language models (LLMs) have\ndemonstrated an impressive ability to generalize to new tasks, the training\nphases heavily rely on large amounts of diverse and high-quality instruction\ndata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,\nespecially when it comes to human-written data, can pose significant challenges\nboth in terms of cost and accessibility. Moreover, concerns related to privacy\ncan further limit access to such data, making the process of obtaining it a\ncomplex and nuanced undertaking. Consequently, this hinders the generality of\nthe tuned models and may restrict their effectiveness in certain contexts. To\ntackle this issue, our study introduces a new approach called Federated\nInstruction Tuning (FedIT), which leverages federated learning (FL) as the\nlearning framework for the instruction tuning of LLMs. This marks the first\nexploration of FL-based instruction tuning for LLMs. This is especially\nimportant since text data is predominantly generated by end users. Therefore,\nit is imperative to design and adapt FL approaches to effectively leverage\nthese users' diverse instructions stored on local devices, while preserving\nprivacy and ensuring data security. In the current paper, by conducting widely\nused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous\nand diverse sets of instructions on the client's end with the proposed\nframework FedIT, we improved the performance of LLMs compared to centralized\ntraining with only limited local instructions. Further, in this paper, we\ndeveloped a Github repository named Shepherd. This repository offers a\nfoundational framework for exploring federated fine-tuning of LLMs using\nheterogeneous instructions across diverse categories.",
        "pdf_link": "https://arxiv.org/pdf/2305.05644v2.pdf"
    },
    {
        "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling",
        "authors": [
            "Zhang Ze Yu",
            "Lau Jia Jaw",
            "Zhang Hui",
            "Bryan Kian Hsiang Low"
        ],
        "published": "2023-05-09T17:06:06Z",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to\nsignificantly enhance the performance of large language models (LLMs) by\naligning their outputs with desired human values through instruction tuning.\nHowever, RLHF is constrained by the expertise and productivity limitations of\nhuman evaluators. A response to this downside is to fall back to supervised\nfine-tuning (SFT) with additional carefully selected expert demonstrations.\nHowever, while this method has been proven to be effective, it invariably also\nleads to increased human-in-the-loop overhead. In this study, we propose\nanother alternative approach: Reinforcement Learning with Generative\nAdversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative\nadversarial training style to enable the LLMs to learn useful human expert\ndemonstrations without being directly exposed to the training examples, thus\nenabling good generalization capabilities while preserving sample efficiency.\nOur preliminary findings indicate that RLGAF can help align LLMs outputs with\ncompetitive performance against RLHF and SFT, while not suffering from their\nrespective inherent restrictions, suggesting promising avenues for further\nresearch on automating AI alignment.",
        "pdf_link": "https://arxiv.org/pdf/2305.06176v3.pdf"
    },
    {
        "title": "The Case Records of ChatGPT: Language Models and Complex Clinical Questions",
        "authors": [
            "Timothy Poterucha",
            "Pierre Elias",
            "Christopher M. Haggerty"
        ],
        "published": "2023-05-09T16:58:32Z",
        "summary": "Background: Artificial intelligence language models have shown promise in\nvarious applications, including assisting with clinical decision-making as\ndemonstrated by strong performance of large language models on medical\nlicensure exams. However, their ability to solve complex, open-ended cases,\nwhich may be representative of clinical practice, remains unexplored. Methods:\nIn this study, the accuracy of large language AI models GPT4 and GPT3.5 in\ndiagnosing complex clinical cases was investigated using published Case Records\nof the Massachusetts General Hospital. A total of 50 cases requiring a\ndiagnosis and diagnostic test published from January 1, 2022 to April 16, 2022\nwere identified. For each case, models were given a prompt requesting the top\nthree specific diagnoses and associated diagnostic tests, followed by case\ntext, labs, and figure legends. Model outputs were assessed in comparison to\nthe final clinical diagnosis and whether the model-predicted test would result\nin a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the\ncorrect diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%\nwithin three attempts, respectively. GPT4 and GPT3.5 provided a correct\nessential diagnostic test in 28% and 24% of cases in one attempt, and 44% and\n50% within three attempts, respectively. No significant differences were found\nbetween the two models, and multiple trials with identical prompts using the\nGPT3.5 model provided similar results. Conclusions: In summary, these models\ndemonstrate potential usefulness in generating differential diagnoses but\nremain limited in their ability to provide a single unifying diagnosis in\ncomplex, open-ended cases. Future research should focus on evaluating model\nperformance in larger datasets of open-ended clinical challenges and exploring\npotential human-AI collaboration strategies to enhance clinical\ndecision-making.",
        "pdf_link": "https://arxiv.org/pdf/2305.05609v1.pdf"
    },
    {
        "title": "ChatGPT as a Text Simplification Tool to Remove Bias",
        "authors": [
            "Charmaine Barker",
            "Dimitar Kazakov"
        ],
        "published": "2023-05-09T13:10:23Z",
        "summary": "The presence of specific linguistic signals particular to a certain sub-group\nof people can be picked up by language models during training. If the model\nbegins to associate specific language with a distinct group, any decisions made\nbased upon this language would hold a strong correlation to a decision based\nupon their protected characteristic, leading to possible discrimination. We\nexplore a potential technique for bias mitigation in the form of simplification\nof text. The driving force of this idea is that simplifying text should\nstandardise language between different sub-groups to one way of speaking while\nkeeping the same meaning. The experiment shows promising results as the\nclassifier accuracy for predicting the sensitive attribute drops by up to 17%\nfor the simplified data.",
        "pdf_link": "https://arxiv.org/pdf/2305.06166v2.pdf"
    },
    {
        "title": "Large Language Models Need Holistically Thought in Medical Conversational QA",
        "authors": [
            "Yixuan Weng",
            "Bin Li",
            "Fei Xia",
            "Minjun Zhu",
            "Bin Sun",
            "Shizhu He",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023-05-09T12:57:28Z",
        "summary": "The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.",
        "pdf_link": "https://arxiv.org/pdf/2305.05410v2.pdf"
    },
    {
        "title": "A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Yue Liu",
            "Zhenchang Xing",
            "Jon Whittle"
        ],
        "published": "2023-05-09T11:37:16Z",
        "summary": "The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted huge interest in foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is limited understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories: the\npretraining and adaptation of foundation models, the architecture design of\nfoundation model based systems, and responsible-AI-by-design. This taxonomy can\nserve as concrete guidance for making major architectural design decisions when\ndesigning foundation model based systems and highlights trade-offs arising from\ndesign decisions.",
        "pdf_link": "https://arxiv.org/pdf/2305.05352v6.pdf"
    },
    {
        "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
        "authors": [
            "Shanshan Zhong",
            "Zhongzhan Huang",
            "Wushao Wen",
            "Jinghui Qin",
            "Liang Lin"
        ],
        "published": "2023-05-09T05:48:38Z",
        "summary": "Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.",
        "pdf_link": "https://arxiv.org/pdf/2305.05189v4.pdf"
    },
    {
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
        "authors": [
            "Xiaonan Li",
            "Xipeng Qiu"
        ],
        "published": "2023-05-09T05:25:05Z",
        "summary": "Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.05181v2.pdf"
    },
    {
        "title": "Accessible Instruction-Following Agent",
        "authors": [
            "Kairui Zhou"
        ],
        "published": "2023-05-08T23:57:26Z",
        "summary": "Humans can collaborate and complete tasks based on visual signals and\ninstruction from the environment. Training such a robot is difficult especially\ndue to the understanding of the instruction and the complicated environment.\nPrevious instruction-following agents are biased to English-centric corpus,\nmaking it unrealizable to be applied to users that use multiple languages or\neven low-resource languages. Nevertheless, the instruction-following agents are\npre-trained in a mode that assumes the user can observe the environment, which\nlimits its accessibility. In this work, we're trying to generalize the success\nof instruction-following agents to non-English languages with little corpus\nresources, and improve its intractability and accessibility. We introduce UVLN\n(Universal Vision-Language Navigation), a novel machine-translation\ninstructional augmented framework for cross-lingual vision-language navigation,\nwith a novel composition of state-of-the-art large language model (GPT3) with\nthe image caption model (BLIP). We first collect a multilanguage\nvision-language navigation dataset via machine translation. Then we extend the\nstandard VLN training objectives to a multilingual setting via a cross-lingual\nlanguage encoder. The alignment between different languages is captured through\na shared vision and action context via a cross-modal transformer, which encodes\nthe inputs of language instruction, visual observation, and action decision\nsequences. To improve the intractability, we connect our agent with the large\nlanguage model that informs the situation and current state to the user and\nalso explains the action decisions. Experiments over Room Across Room Dataset\nprove the effectiveness of our approach. And the qualitative results show the\npromising intractability and accessibility of our instruction-following agent.",
        "pdf_link": "https://arxiv.org/pdf/2305.06358v1.pdf"
    },
    {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "published": "2023-05-08T23:31:39Z",
        "summary": "Communication via natural language is a key aspect of machine intelligence,\nand it requires computational models to learn and reason about world concepts,\nwith varying levels of supervision. Significant progress has been made on\nfully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding. Yet, various sequential interactive tasks, as in\ntext-based games, have revealed limitations of existing approaches in terms of\ncoherence, contextual awareness, and their ability to learn effectively from\nthe environment. In this paper, we propose a knowledge-injection framework for\nimproved functional grounding of agents in text-based games. Specifically, we\nconsider two forms of domain knowledge that we inject into learning-based\nagents: memory of previous correct actions and affordances of relevant objects\nin the environment. Our framework supports two representative model classes:\nreinforcement learning agents and language model agents. Furthermore, we devise\nmultiple injection strategies for the above domain knowledge types and agent\narchitectures, including injection via knowledge graphs and augmentation of the\nexisting input encoding strategies. We experiment with four models on the 10\ntasks in the ScienceWorld text-based game environment, to illustrate the impact\nof knowledge injection on various model configurations and challenging task\nsettings. Our findings provide crucial insights into the interplay between task\nproperties, model architectures, and domain knowledge for interactive contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.05091v2.pdf"
    },
    {
        "title": "Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer",
        "authors": [
            "Tao Hong"
        ],
        "published": "2023-05-08T21:35:12Z",
        "summary": "Large Language Models (LLMs), such as the Generative Pretrained Transformer\n(GPT), have achieved tremendous success in various language tasks, but their\nemergent abilities have also raised many questions, concerns, and challenges\nthat need to be addressed. To gain a better understanding of the models' inner\nmechanisms, we analyze the hidden state and channel wave dynamics in a small\nGPT, focusing on the coherence of wave patterns in terms of cross-channel\ncorrelation and individual auto-correlation. Our findings suggest that wave\ndynamics offer consistent and repeatable intrinsic oscillation modes, along\nwith context-aware plasticity and expressiveness in language generation. By\nanalyzing wave patterns, coherence, and clustering, we provide a systematic way\nto identify and interpret the functionality of the hidden state channels,\npaving the way to understand and control higher-level language pattern\nformation. In addition, we investigate the Poisson statistics of spelling\nerrors in text sequence generation across various levels of model training and\nobserve a phase-transition-like process. As coherence builds up, there is a\ncompetition between the generation of correct and misspelled words. However,\nonce the model is adequately trained and significant coherence has emerged, the\ncoherent process becomes strong enough to effectively suppress spelling errors,\npreventing the cascade amplification of defects. The distribution of correct\nspellings transitions from Poissonian to Sub-Poissonian, while the distribution\nof misspellings shows the opposite trend. By leveraging concepts and techniques\nfrom quantum physics, we gain novel insights into the dynamics of the small\nGPT. This approach can be extended to larger language models that exhibit more\ncomplex coherent language patterns, opening up opportunities to interpret their\nemergent capabilities and develop more specialized models.",
        "pdf_link": "https://arxiv.org/pdf/2305.05061v1.pdf"
    },
    {
        "title": "ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
        "authors": [
            "Thilini Wijesiriwardene",
            "Ruwan Wickramarachchi",
            "Bimal G. Gajera",
            "Shreeyash Mukul Gowaikar",
            "Chandan Gupta",
            "Aman Chadha",
            "Aishwarya Naresh Reganti",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023-05-08T21:12:20Z",
        "summary": "Over the past decade, analogies, in the form of word-level analogies, have\nplayed a significant role as an intrinsic measure of evaluating the quality of\nword embedding methods such as word2vec. Modern large language models (LLMs),\nhowever, are primarily evaluated on extrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a few investigations on whether LLMs\ncan draw analogies between long texts. In this paper, we present ANALOGICAL, a\nnew benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of\nlong text with six levels of complexity -- (i) word, (ii) word vs. sentence,\n(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using\nthirteen datasets and three different distance measures, we evaluate the\nabilities of eight LLMs in identifying analogical pairs in the semantic vector\nspace. Our evaluation finds that it is increasingly challenging for LLMs to\nidentify analogies when going up the analogy taxonomy.",
        "pdf_link": "https://arxiv.org/pdf/2305.05050v3.pdf"
    },
    {
        "title": "Web Content Filtering through knowledge distillation of Large Language Models",
        "authors": [
            "Tam\u00e1s V\u00f6r\u00f6s",
            "Sean Paul Bergeron",
            "Konstantin Berlin"
        ],
        "published": "2023-05-08T20:09:27Z",
        "summary": "We introduce a state-of-the-art approach for URL categorization that\nleverages the power of Large Language Models (LLMs) to address the primary\nobjectives of web content filtering: safeguarding organizations from legal and\nethical risks, limiting access to high-risk or suspicious websites, and\nfostering a secure and professional work environment. Our method utilizes LLMs\nto generate accurate classifications and then employs established knowledge\ndistillation techniques to create smaller, more specialized student models\ntailored for web content filtering. Distillation results in a student model\nwith a 9% accuracy rate improvement in classifying websites, sourced from\ncustomer telemetry data collected by a large security vendor, into 30 distinct\ncontent categories based on their URLs, surpassing the current state-of-the-art\napproach. Our student model matches the performance of the teacher LLM with 175\ntimes less parameters, allowing the model to be used for in-line scanning of\nlarge volumes of URLs, and requires 3 orders of magnitude less manually labeled\ntraining data than the current state-of-the-art approach. Depending on the\nspecific use case, the output generated by our approach can either be directly\nreturned or employed as a pre-filter for more resource-intensive operations\ninvolving website images or HTML.",
        "pdf_link": "https://arxiv.org/pdf/2305.05027v2.pdf"
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "authors": [
            "Somin Wadhwa",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-05-08T19:19:07Z",
        "summary": "Relation extraction (RE) is the core NLP task of inferring semantic\nrelationships between entities from text. Standard supervised RE techniques\nentail training modules to tag tokens comprising entity spans and then predict\nthe relationship between them. Recent work has instead treated the problem as a\n\\emph{sequence-to-sequence} task, linearizing relations between entities as\ntarget strings to be generated conditioned on the input. Here we push the\nlimits of this approach, using larger language models (GPT-3 and Flan-T5 large)\nthan considered in prior work and evaluating their performance on standard RE\ntasks under varying levels of supervision. We address issues inherent to\nevaluating generative approaches to RE by doing human evaluations, in lieu of\nrelying on exact matching. Under this refined evaluation, we find that: (1)\nFew-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly\nequivalent to existing fully supervised models; (2) Flan-T5 is not as capable\nin the few-shot setting, but supervising and fine-tuning it with\nChain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA\nresults. We release this model as a new baseline for RE tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.05003v1.pdf"
    },
    {
        "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
        "authors": [
            "Josh Magnus Ludan",
            "Yixuan Meng",
            "Tai Nguyen",
            "Saurabh Shah",
            "Qing Lyu",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-08T18:53:45Z",
        "summary": "Large Language Models (LLMs) are so powerful that they sometimes learn\ncorrelations between labels and features that are irrelevant to the task,\nleading to poor generalization on out-of-distribution data. We propose\nexplanation-based finetuning as a general approach to mitigate LLMs' reliance\non spurious correlations. Unlike standard finetuning where the model only\npredicts the answer given the input, we finetune the model to additionally\ngenerate a free-text explanation supporting its answer. To evaluate our method,\nwe finetune the model on artificially constructed training sets containing\ndifferent types of spurious cues, and test it on a test set without these cues.\nCompared to standard finetuning, our method makes GPT-3 (davinci) remarkably\nmore robust against spurious cues in terms of accuracy drop across four\nclassification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC\n(+6.5). The efficacy generalizes across multiple model families and scales,\nwith greater gains for larger models. Finally, our method also works well with\nexplanations generated by the model, implying its applicability to more\ndatasets without human-written explanations.",
        "pdf_link": "https://arxiv.org/pdf/2305.04990v3.pdf"
    },
    {
        "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
        "authors": [
            "Kaushik Roy",
            "Tarun Garg",
            "Vedant Palit",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-05-08T18:53:14Z",
        "summary": "A fundamental question in natural language processing is - what kind of\nlanguage structure and semantics is the language model capturing? Graph formats\nsuch as knowledge graphs are easy to evaluate as they explicitly express\nlanguage semantics and structure. This study evaluates the semantics encoded in\nthe self-attention transformers by leveraging explicit knowledge graph\nstructures. We propose novel metrics to measure the reconstruction error when\nproviding graph path sequences from a knowledge graph and trying to\nreproduce/reconstruct the same from the outputs of the self-attention\ntransformer models. The opacity of language models has an immense bearing on\nsocietal issues of trust and explainable decision outcomes. Our findings\nsuggest that language models are models of stochastic control processes for\nplausible language pattern generation. However, they do not ascribe object and\nconcept-level meaning and semantics to the learned stochastic patterns such as\nthose described in knowledge graphs. Furthermore, to enable robust evaluation\nof concept understanding by language models, we construct and make public an\naugmented language understanding benchmark built on the General Language\nUnderstanding Evaluation (GLUE) benchmark. This has significant\napplication-level user trust implications as stochastic patterns without a\nstrong sense of meaning cannot be trusted in high-stakes applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.04989v1.pdf"
    },
    {
        "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge",
        "authors": [
            "Phillip Howard",
            "Junlin Wang",
            "Vasudev Lal",
            "Gadi Singer",
            "Yejin Choi",
            "Swabha Swayamdipta"
        ],
        "published": "2023-05-08T18:20:36Z",
        "summary": "Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is\nan essential component of our world knowledge, yet understudied in prior\nliterature. In this paper, we harvest the dramatic improvements in knowledge\ncapabilities of language models into a large-scale comparative knowledge base.\nWhile the ease of acquisition of such comparative knowledge is much higher from\nextreme-scale models like GPT-4, compared to their considerably smaller and\nweaker counterparts such as GPT-2, not even the most powerful models are exempt\nfrom making errors. We thus ask: to what extent are models at different scales\nable to generate valid and diverse comparative knowledge?\n  We introduce NeuroComparatives, a novel framework for comparative knowledge\ndistillation overgenerated from language models such as GPT-variants and LLaMA,\nfollowed by stringent filtering of the generated knowledge. Our framework\nacquires comparative knowledge between everyday objects, producing a corpus of\nup to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more\ndiverse than existing resources. Moreover, human evaluations show that\nNeuroComparatives outperform existing resources in terms of validity (up to 32%\nabsolute improvement). Our acquired NeuroComparatives leads to performance\nimprovements on five downstream tasks. We find that neuro-symbolic manipulation\nof smaller models offers complementary benefits to the currently dominant\npractice of prompting extreme-scale language models for knowledge distillation.",
        "pdf_link": "https://arxiv.org/pdf/2305.04978v3.pdf"
    },
    {
        "title": "How Do In-Context Examples Affect Compositional Generalization?",
        "authors": [
            "Shengnan An",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ],
        "published": "2023-05-08T16:32:18Z",
        "summary": "Compositional generalization--understanding unseen combinations of seen\nprimitives--is an essential reasoning capability in human intelligence. The AI\ncommunity mainly studies this capability by fine-tuning neural networks on lots\nof training samples, while it is still unclear whether and how in-context\nlearning--the prevailing few-shot paradigm based on large language\nmodels--exhibits compositional generalization. In this paper, we present CoFe,\na test suite to investigate in-context compositional generalization. We find\nthat the compositional generalization performance can be easily affected by the\nselection of in-context examples, thus raising the research question what the\nkey factors are to make good in-context examples for compositional\ngeneralization. We study three potential factors: similarity, diversity and\ncomplexity. Our systematic experiments indicate that in-context examples should\nbe structurally similar to the test case, diverse from each other, and\nindividually simple. Furthermore, two strong limitations are observed:\nin-context compositional generalization on fictional words is much weaker than\nthat on commonly used ones; it is still critical that the in-context examples\nshould cover required linguistic structures, even though the backbone model has\nbeen pre-trained on large corpus. We hope our analysis would facilitate the\nunderstanding and utilization of in-context learning paradigm.",
        "pdf_link": "https://arxiv.org/pdf/2305.04835v3.pdf"
    },
    {
        "title": "Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video",
        "authors": [
            "Zenan Xu",
            "Xiaojun Meng",
            "Yasheng Wang",
            "Qinliang Su",
            "Zexuan Qiu",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-05-08T16:24:46Z",
        "summary": "Multimodal abstractive summarization for videos (MAS) requires generating a\nconcise textual summary to describe the highlights of a video according to\nmultimodal resources, in our case, the video content and its transcript.\nInspired by the success of the large-scale generative pre-trained language\nmodel (GPLM) in generating high-quality textual content (e.g., summary), recent\nMAS methods have proposed to adapt the GPLM to this task by equipping it with\nthe visual information, which is often obtained through a general-purpose\nvisual feature extractor. However, the generally extracted visual features may\noverlook some summary-worthy visual information, which impedes model\nperformance. In this work, we propose a novel approach to learning the\nsummary-worthy visual representation that facilitates abstractive\nsummarization. Our method exploits the summary-worthy information from both the\ncross-modal transcript data and the knowledge that distills from the pseudo\nsummary. Extensive experiments on three public multimodal datasets show that\nour method outperforms all competing baselines. Furthermore, with the\nadvantages of summary-worthy visual information, our model can have a\nsignificant improvement on small datasets or even datasets with limited\ntraining data.",
        "pdf_link": "https://arxiv.org/pdf/2305.04824v1.pdf"
    },
    {
        "title": "Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns",
        "authors": [
            "Ning Bian",
            "Hongyu Lin",
            "Peilin Liu",
            "Yaojie Lu",
            "Chunkang Zhang",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-05-08T16:10:18Z",
        "summary": "Social cognitive theory explains how people learn and acquire knowledge\nthrough observing others. Recent years have witnessed the rapid development of\nlarge language models (LLMs), which suggests their potential significance as\nagents in the society. LLMs, as AI agents, can observe external information,\nwhich shapes their cognition and behaviors. However, the extent to which\nexternal information influences LLMs' cognition and behaviors remains unclear.\nThis study investigates how external statements and opinions influence LLMs'\nthoughts and behaviors from a social cognitive perspective. Three experiments\nwere conducted to explore the effects of external information on LLMs'\nmemories, opinions, and social media behavioral decisions. Sociocognitive\nfactors, including source authority, social identity, and social role, were\nanalyzed to investigate their moderating effects. Results showed that external\ninformation can significantly shape LLMs' memories, opinions, and behaviors,\nwith these changes mirroring human social cognitive patterns such as authority\nbias, in-group bias, emotional positivity, and emotion contagion. This\nunderscores the challenges in developing safe and unbiased LLMs, and emphasizes\nthe importance of understanding the susceptibility of LLMs to external\ninfluences.",
        "pdf_link": "https://arxiv.org/pdf/2305.04812v3.pdf"
    },
    {
        "title": "Algebra Error Classification with Large Language Models",
        "authors": [
            "Hunter McNichols",
            "Mengxue Zhang",
            "Andrew Lan"
        ],
        "published": "2023-05-08T15:51:38Z",
        "summary": "Automated feedback as students answer open-ended math questions has\nsignificant potential in improving learning outcomes at large scale. A key part\nof automated feedback systems is an error classification component, which\nidentifies student errors and enables appropriate, predefined feedback to be\ndeployed. Most existing approaches to error classification use a rule-based\nmethod, which has limited capacity to generalize. Existing data-driven methods\navoid these limitations but specifically require mathematical expressions in\nstudent responses to be parsed into syntax trees. This requirement is itself a\nlimitation, since student responses are not always syntactically valid and\ncannot be converted into trees. In this work, we introduce a flexible method\nfor error classification using pre-trained large language models. We\ndemonstrate that our method can outperform existing methods in algebra error\nclassification, and is able to classify a larger set of student responses.\nAdditionally, we analyze common classification errors made by our method and\ndiscuss limitations of automated error classification.",
        "pdf_link": "https://arxiv.org/pdf/2305.06163v1.pdf"
    },
    {
        "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
        "authors": [
            "Ziyang Luo",
            "Can Xu",
            "Pu Zhao",
            "Xiubo Geng",
            "Chongyang Tao",
            "Jing Ma",
            "Qingwei Lin",
            "Daxin Jiang"
        ],
        "published": "2023-05-08T15:05:16Z",
        "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP) with their impressive language understanding and generation\ncapabilities. However, their performance may be suboptimal for domain-specific\ntasks that require specialized knowledge due to limited exposure to the related\ndata. Additionally, the lack of transparency of most state-of-the-art (SOTA)\nLLMs, which can only be accessed via APIs, impedes further fine-tuning with\ndomain custom data. Moreover, providing private data to the LLMs' owner leads\nto data privacy problems. To address these challenges, we propose the novel\nParametric Knowledge Guiding (PKG) framework, which equips LLMs with a\nknowledge-guiding module to access relevant knowledge without altering the\nLLMs' parameters. Our PKG is based on open-source \"white-box\" language models,\nallowing offline memory of any knowledge that LLMs require. We demonstrate that\nour PKG framework can enhance the performance of \"black-box\" LLMs on a range of\ndomain knowledge-intensive tasks that require factual (+7.9%), tabular\n(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2305.04757v2.pdf"
    },
    {
        "title": "Differentially Private Attention Computation",
        "authors": [
            "Yeqi Gao",
            "Zhao Song",
            "Xin Yang"
        ],
        "published": "2023-05-08T13:32:41Z",
        "summary": "Large language models (LLMs) have had a profound impact on numerous aspects\nof daily life including natural language processing, content generation,\nresearch methodologies and so on. However, one crucial issue concerning the\ninference results of large language models is security and privacy. In many\nscenarios, the results generated by LLMs could possibly leak many confidential\nor copyright information. A recent beautiful and breakthrough work [Vyas,\nKakade and Barak 2023] focus on such privacy issue of the LLMs from theoretical\nperspective. It is well-known that computing the attention matrix is one of the\nmajor task during the LLMs computation. Thus, how to give a provable privately\nguarantees of computing the attention matrix is an important research\ndirection.\n  Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposed\nprovable tight result for fast computation of attention without considering\nprivacy concerns. One natural mathematical formulation to quantity the privacy\nin theoretical computer science graduate school textbook is differential\nprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide a\nprovable result for showing how to differentially private approximate the\nattention matrix.\n  From technique perspective, our result replies on a pioneering work in the\narea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang\n2022].",
        "pdf_link": "https://arxiv.org/pdf/2305.04701v1.pdf"
    },
    {
        "title": "Enhancing Knowledge Graph Construction Using Large Language Models",
        "authors": [
            "Milena Trajanoska",
            "Riste Stojanov",
            "Dimitar Trajanov"
        ],
        "published": "2023-05-08T12:53:06Z",
        "summary": "The growing trend of Large Language Models (LLM) development has attracted\nsignificant attention, with models for various applications emerging\nconsistently. However, the combined application of Large Language Models with\nsemantic technologies for reasoning and inference is still a challenging task.\nThis paper analyzes how the current advances in foundational LLM, like ChatGPT,\ncan be compared with the specialized pretrained models, like REBEL, for joint\nentity and relation extraction. To evaluate this approach, we conducted several\nexperiments using sustainability-related text as our use case. We created\npipelines for the automatic creation of Knowledge Graphs from raw texts, and\nour findings indicate that using advanced LLM models can improve the accuracy\nof the process of creating these graphs from unstructured text. Furthermore, we\nexplored the potential of automatic ontology creation using foundation LLM\nmodels, which resulted in even more relevant and accurate knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2305.04676v1.pdf"
    },
    {
        "title": "Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT",
        "authors": [
            "Guo Lin",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-08T07:28:16Z",
        "summary": "This study investigates the feasibility of developing an Artificial General\nRecommender (AGR), facilitated by recent advancements in Large Language Models\n(LLMs). An AGR comprises both conversationality and universality to engage in\nnatural dialogues and generate recommendations across various domains. We\npropose ten fundamental principles that an AGR should adhere to, each with its\ncorresponding testing protocols. We proceed to assess whether ChatGPT, a\nsophisticated LLM, can comply with the proposed principles by engaging in\nrecommendation-oriented dialogues with the model while observing its behavior.\nOur findings demonstrate the potential for ChatGPT to serve as an AGR, though\nseveral limitations and areas for improvement are identified.",
        "pdf_link": "https://arxiv.org/pdf/2305.04518v1.pdf"
    },
    {
        "title": "Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding",
        "authors": [
            "Bhanu Prakash Voutharoja",
            "Lizhen Qu",
            "Fatemeh Shiri"
        ],
        "published": "2023-05-08T05:03:07Z",
        "summary": "Recent works on form understanding mostly employ multimodal transformers or\nlarge-scale pre-trained language models. These models need ample data for\npre-training. In contrast, humans can usually identify key-value pairings from\na form only by looking at layouts, even if they don't comprehend the language\nused. No prior research has been conducted to investigate how helpful layout\ninformation alone is for form understanding. Hence, we propose a unique\nentity-relation graph parsing method for scanned forms called LAGNN, a\nlanguage-independent Graph Neural Network model. Our model parses a form into a\nword-relation graph in order to identify entities and relations jointly and\nreduce the time complexity of inference. This graph is then transformed by\ndeterministic rules into a fully connected entity-relation graph. Our model\nsimply takes into account relative spacing between bounding boxes from layout\ninformation to facilitate easy transfer across languages. To further improve\nthe performance of LAGNN, and achieve isomorphism between entity-relation\ngraphs and word-relation graphs, we use integer linear programming (ILP) based\ninference. Code is publicly available at https://github.com/Bhanu068/LAGNN",
        "pdf_link": "https://arxiv.org/pdf/2305.04460v1.pdf"
    },
    {
        "title": "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts",
        "authors": [
            "Jaromir Savelka"
        ],
        "published": "2023-05-08T01:55:53Z",
        "summary": "We evaluated the capability of a state-of-the-art generative pre-trained\ntransformer (GPT) model to perform semantic annotation of short text snippets\n(one to few sentences) coming from legal documents of various types.\nDiscussions of potential uses (e.g., document drafting, summarization) of this\nemerging technology in legal domain have intensified, but to date there has not\nbeen a rigorous analysis of these large language models' (LLM) capacity in\nsentence-level semantic annotation of legal texts in zero-shot learning\nsettings. Yet, this particular type of use could unlock many practical\napplications (e.g., in contract review) and research opportunities (e.g., in\nempirical legal studies). We fill the gap with this study. We examined if and\nhow successfully the model can semantically annotate small batches of short\ntext snippets (10-50) based exclusively on concise definitions of the semantic\ntypes. We found that the GPT model performs surprisingly well in zero-shot\nsettings on diverse types of documents (F1=.73 on a task involving court\nopinions, .86 for contracts, and .54 for statutes and regulations). These\nfindings can be leveraged by legal scholars and practicing lawyers alike to\nguide their decisions in integrating LLMs in wide range of workflows involving\nsemantic annotation of legal texts.",
        "pdf_link": "https://arxiv.org/pdf/2305.04417v1.pdf"
    },
    {
        "title": "Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5",
        "authors": [
            "Gaurav Suri",
            "Lily R. Slater",
            "Ali Ziaee",
            "Morgan Nguyen"
        ],
        "published": "2023-05-08T01:02:52Z",
        "summary": "A Large Language Model (LLM) is an artificial intelligence system that has\nbeen trained on vast amounts of natural language data, enabling it to generate\nhuman-like responses to written or spoken language input. GPT-3.5 is an example\nof an LLM that supports a conversational agent called ChatGPT. In this work, we\nused a series of novel prompts to determine whether ChatGPT shows heuristics,\nbiases, and other decision effects. We also tested the same prompts on human\nparticipants. Across four studies, we found that ChatGPT was influenced by\nrandom anchors in making estimates (Anchoring Heuristic, Study 1); it judged\nthe likelihood of two events occurring together to be higher than the\nlikelihood of either event occurring alone, and it was erroneously influenced\nby salient anecdotal information (Representativeness and Availability\nHeuristic, Study 2); it found an item to be more efficacious when its features\nwere presented positively rather than negatively - even though both\npresentations contained identical information (Framing Effect, Study 3); and it\nvalued an owned item more than a newly found item even though the two items\nwere identical (Endowment Effect, Study 4). In each study, human participants\nshowed similar effects. Heuristics and related decision effects in humans are\nthought to be driven by cognitive and affective processes such as loss aversion\nand effort reduction. The fact that an LLM - which lacks these processes - also\nshows such effects invites consideration of the possibility that language may\nplay a role in generating these effects in humans.",
        "pdf_link": "https://arxiv.org/pdf/2305.04400v1.pdf"
    },
    {
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "authors": [
            "Miles Turpin",
            "Julian Michael",
            "Ethan Perez",
            "Samuel R. Bowman"
        ],
        "published": "2023-05-07T22:44:25Z",
        "summary": "Large Language Models (LLMs) can achieve strong performance on many tasks by\nproducing step-by-step reasoning before giving a final output, often referred\nto as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT\nexplanations as the LLM's process for solving a task. This level of\ntransparency into LLMs' predictions would yield significant safety benefits.\nHowever, we find that CoT explanations can systematically misrepresent the true\nreason for a model's prediction. We demonstrate that CoT explanations can be\nheavily influenced by adding biasing features to model inputs--e.g., by\nreordering the multiple-choice options in a few-shot prompt to make the answer\nalways \"(A)\"--which models systematically fail to mention in their\nexplanations. When we bias models toward incorrect answers, they frequently\ngenerate CoT explanations rationalizing those answers. This causes accuracy to\ndrop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing\nwith GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,\nmodel explanations justify giving answers in line with stereotypes without\nmentioning the influence of these social biases. Our findings indicate that CoT\nexplanations can be plausible yet misleading, which risks increasing our trust\nin LLMs without guaranteeing their safety. Building more transparent and\nexplainable systems will require either improving CoT faithfulness through\ntargeted efforts or abandoning CoT in favor of alternative methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.04388v2.pdf"
    },
    {
        "title": "Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",
        "authors": [
            "Hazem Ibrahim",
            "Fengyuan Liu",
            "Rohail Asim",
            "Balaraju Battu",
            "Sidahmed Benabderrahmane",
            "Bashar Alhafni",
            "Wifag Adnan",
            "Tuka Alhanai",
            "Bedoor AlShebli",
            "Riyadh Baghdadi",
            "Jocelyn J. B\u00e9langer",
            "Elena Beretta",
            "Kemal Celik",
            "Moumena Chaqfeh",
            "Mohammed F. Daqaq",
            "Zaynab El Bernoussi",
            "Daryl Fougnie",
            "Borja Garcia de Soto",
            "Alberto Gandolfi",
            "Andras Gyorgy",
            "Nizar Habash",
            "J. Andrew Harris",
            "Aaron Kaufman",
            "Lefteris Kirousis",
            "Korhan Kocak",
            "Kangsan Lee",
            "Seungah S. Lee",
            "Samreen Malik",
            "Michail Maniatakos",
            "David Melcher",
            "Azzam Mourad",
            "Minsu Park",
            "Mahmoud Rasras",
            "Alicja Reuben",
            "Dania Zantout",
            "Nancy W. Gleason",
            "Kinga Makovi",
            "Talal Rahwan",
            "Yasir Zaki"
        ],
        "published": "2023-05-07T10:37:51Z",
        "summary": "The emergence of large language models has led to the development of powerful\ntools such as ChatGPT that can produce text indistinguishable from\nhuman-generated work. With the increasing accessibility of such technology,\nstudents across the globe may utilize it to help with their school work -- a\npossibility that has sparked discussions on the integrity of student\nevaluations in the age of artificial intelligence (AI). To date, it is unclear\nhow such tools perform compared to students on university-level courses.\nFurther, students' perspectives regarding the use of such tools, and educators'\nperspectives on treating their use as plagiarism, remain unknown. Here, we\ncompare the performance of ChatGPT against students on 32 university-level\ncourses. We also assess the degree to which its use can be detected by two\nclassifiers designed specifically for this purpose. Additionally, we conduct a\nsurvey across five countries, as well as a more in-depth survey at the authors'\ninstitution, to discern students' and educators' perceptions of ChatGPT's use.\nWe find that ChatGPT's performance is comparable, if not superior, to that of\nstudents in many courses. Moreover, current AI-text classifiers cannot reliably\ndetect ChatGPT's use in school work, due to their propensity to classify\nhuman-written answers as AI-generated, as well as the ease with which\nAI-generated text can be edited to evade detection. Finally, we find an\nemerging consensus among students to use the tool, and among educators to treat\nthis as plagiarism. Our findings offer insights that could guide policy\ndiscussions addressing the integration of AI into educational frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2305.13934v1.pdf"
    },
    {
        "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
        "authors": [
            "Feilong Chen",
            "Minglun Han",
            "Haozhi Zhao",
            "Qingyang Zhang",
            "Jing Shi",
            "Shuang Xu",
            "Bo Xu"
        ],
        "published": "2023-05-07T02:25:42Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable language abilities.\nGPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities\nbeyond previous visual language models. We attribute this to the use of more\nadvanced LLMs compared with previous multimodal models. Unfortunately, the\nmodel architecture and training strategies of GPT-4 are unknown. To endow LLMs\nwith multimodal capabilities, we propose X-LLM, which converts Multi-modalities\n(images, speech, videos) into foreign languages using X2L interfaces and inputs\nthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple\nfrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''\ndenotes multi-modalities such as image, speech, and videos, and ``L'' denotes\nlanguages. X-LLM's training consists of three stages: (1) Converting Multimodal\nInformation: The first stage trains each X2L interface to align with its\nrespective single-modal encoder separately to convert multimodal information\ninto languages. (2) Aligning X2L representations with the LLM: single-modal\nencoders are aligned with the LLM through X2L interfaces independently. (3)\nIntegrating multiple modalities: all single-modal encoders are aligned with the\nLLM through X2L interfaces to integrate multimodal capabilities into the LLM.\nOur experiments show that X-LLM demonstrates impressive multimodel chat\nabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen\nimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 on\na synthetic multimodal instruction-following dataset. And we also conduct\nquantitative tests on using LLM for ASR and multimodal ASR, hoping to promote\nthe era of LLM-based speech recognition.",
        "pdf_link": "https://arxiv.org/pdf/2305.04160v3.pdf"
    },
    {
        "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
        "authors": [
            "Maximillian Chen",
            "Xiao Yu",
            "Weiyan Shi",
            "Urvi Awasthi",
            "Zhou Yu"
        ],
        "published": "2023-05-06T23:11:25Z",
        "summary": "Mixed-initiative dialogue tasks involve repeated exchanges of information and\nconversational control. Conversational agents gain control by generating\nresponses that follow particular dialogue intents or strategies, prescribed by\na policy planner. The standard approach has been fine-tuning pre-trained\nlanguage models to perform generation conditioned on these intents. However,\nthese supervised generation models are limited by the cost and quality of data\nannotation. We instead prompt large language models as a drop-in replacement to\nfine-tuning on conditional generation. We formalize prompt construction for\ncontrollable mixed-initiative dialogue. Our findings show improvements over\nfine-tuning and ground truth responses according to human evaluation and\nautomatic metrics for two tasks: PersuasionForGood and Emotional Support\nConversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.04147v1.pdf"
    },
    {
        "title": "Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?",
        "authors": [
            "Hernan Ceferino Vazquez"
        ],
        "published": "2023-05-06T20:53:22Z",
        "summary": "Artificial Intelligence (AI) has been rapidly advancing and has demonstrated\nits ability to perform a wide range of cognitive tasks, including language\nprocessing, visual recognition, and decision-making. Part of this progress is\ndue to LLMs (Large Language Models) like those of the GPT (Generative\nPre-Trained Transformers) family. These models are capable of exhibiting\nbehavior that can be perceived as intelligent. Most authors in Neuropsychology\nconsider intelligent behavior to depend on a number of overarching skills, or\nExecutive Functions (EFs), which rely on the correct functioning of neural\nnetworks in the frontal lobes, and have developed a series of tests to evaluate\nthem. In this work, we raise the question of whether LLMs are developing\nexecutive functions similar to those of humans as part of their learning, and\nwe evaluate the planning function and working memory of GPT using the popular\nTowers of Hanoi method. Additionally, we introduce a new variant of the\nclassical method in order to avoid that the solutions are found in the LLM\ntraining data (dataleakeage). Preliminary results show that LLMs generates\nnear-optimal solutions in Towers of Hanoi related tasks, adheres to task\nconstraints, and exhibits rapid planning capabilities and efficient working\nmemory usage, indicating a potential development of executive functions.\nHowever, these abilities are quite limited and worse than well-trained humans\nwhen the tasks are not known and are not part of the training data.",
        "pdf_link": "https://arxiv.org/pdf/2305.04134v2.pdf"
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "authors": [
            "Lei Wang",
            "Wanyu Xu",
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Yunshi Lan",
            "Roy Ka-Wei Lee",
            "Ee-Peng Lim"
        ],
        "published": "2023-05-06T16:34:37Z",
        "summary": "Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "pdf_link": "https://arxiv.org/pdf/2305.04091v3.pdf"
    },
    {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "authors": [
            "Kechi Zhang",
            "Zhuo Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023-05-06T16:12:19Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.04087v5.pdf"
    },
    {
        "title": "Pre-training Language Model as a Multi-perspective Course Learner",
        "authors": [
            "Beiduo Chen",
            "Shaohan Huang",
            "Zihan Zhang",
            "Wu Guo",
            "Zhenhua Ling",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Qi Zhang"
        ],
        "published": "2023-05-06T09:02:10Z",
        "summary": "ELECTRA, the generator-discriminator pre-training framework, has achieved\nimpressive semantic construction capability among various downstream tasks.\nDespite the convincing performance, ELECTRA still faces the challenges of\nmonotonous training and deficient interaction. Generator with only masked\nlanguage modeling (MLM) leads to biased learning and label imbalance for\ndiscriminator, decreasing learning efficiency; no explicit feedback loop from\ndiscriminator to generator results in the chasm between these two components,\nunderutilizing the course learning. In this study, a multi-perspective course\nlearning (MCL) method is proposed to fetch a many degrees and visual angles for\nsample-efficient pre-training, and to fully leverage the relationship between\ngenerator and discriminator. Concretely, three self-supervision courses are\ndesigned to alleviate inherent flaws of MLM and balance the label in a\nmulti-perspective way. Besides, two self-correction courses are proposed to\nbridge the chasm between the two encoders by creating a \"correction notebook\"\nfor secondary-supervision. Moreover, a course soups trial is conducted to solve\nthe \"tug-of-war\" dynamics problem of MCL, evolving a stronger pre-trained\nmodel. Experimental results show that our method significantly improves\nELECTRA's average performance by 2.8% and 3.2% absolute points respectively on\nGLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style\nmodels under the same settings. The pre-trained MCL model is available at\nhttps://huggingface.co/McmanusChen/MCL-base.",
        "pdf_link": "https://arxiv.org/pdf/2305.03981v1.pdf"
    },
    {
        "title": "Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",
        "authors": [
            "Mark Connor",
            "Michael O'Neill"
        ],
        "published": "2023-05-05T21:20:02Z",
        "summary": "This paper explores the potential opportunities, risks, and challenges\nassociated with the use of large language models (LLMs) in sports science and\nmedicine. LLMs are large neural networks with transformer style architectures\ntrained on vast amounts of textual data, and typically refined with human\nfeedback. LLMs can perform a large range of natural language processing tasks.\nIn sports science and medicine, LLMs have the potential to support and augment\nthe knowledge of sports medicine practitioners, make recommendations for\npersonalised training programs, and potentially distribute high-quality\ninformation to practitioners in developing countries. However, there are also\npotential risks associated with the use and development of LLMs, including\nbiases in the dataset used to create the model, the risk of exposing\nconfidential data, the risk of generating harmful output, and the need to align\nthese models with human preferences through feedback. Further research is\nneeded to fully understand the potential applications of LLMs in sports science\nand medicine and to ensure that their use is ethical and beneficial to\nathletes, clients, patients, practitioners, and the general public.",
        "pdf_link": "https://arxiv.org/pdf/2305.03851v1.pdf"
    },
    {
        "title": "On Contrastive Learning of Semantic Similarity forCode to Code Search",
        "authors": [
            "Anthony Saieva",
            "Saikat Chakraborty",
            "Gail Kaiser"
        ],
        "published": "2023-05-05T20:46:56Z",
        "summary": "This paper introduces a novel code-to-code search technique that enhances the\nperformance of Large Language Models (LLMs) by including both static and\ndynamic features as well as utilizing both similar and dissimilar examples\nduring training. We present the first-ever code search method that encodes\ndynamic runtime information during training without the need to execute either\nthe corpus under search or the search query at inference time and the first\ncode search technique that trains on both positive and negative reference\nsamples. To validate the efficacy of our approach, we perform a set of studies\ndemonstrating the capability of enhanced LLMs to perform cross-language\ncode-to-code search.\n  Our evaluation demonstrates that the effectiveness of our approach is\nconsistent across various model architectures and programming languages. We\noutperform the state-of-the-art cross-language search tool by up to 44.7\\%.\nMoreover, our ablation studies reveal that even a single positive and negative\nreference sample in the training process results in substantial performance\nimprovements demonstrating both similar and dissimilar references are important\nparts of code search. Importantly, we show that enhanced well-crafted,\nfine-tuned models consistently outperform enhanced larger modern LLMs without\nfine tuning, even when enhancing the largest available LLMs highlighting the\nimportance for open-sourced models.\n  To ensure the reproducibility and extensibility of our research, we present\nan open-sourced implementation of our tool and training procedures called\nCosco.",
        "pdf_link": "https://arxiv.org/pdf/2305.03843v1.pdf"
    },
    {
        "title": "Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation",
        "authors": [
            "Nilaksh Das",
            "Monica Sunkara",
            "Sravan Bodapati",
            "Jinglun Cai",
            "Devang Kulshreshtha",
            "Jeff Farris",
            "Katrin Kirchhoff"
        ],
        "published": "2023-05-05T20:35:42Z",
        "summary": "End-to-end ASR models trained on large amount of data tend to be implicitly\nbiased towards language semantics of the training data. Internal language model\nestimation (ILME) has been proposed to mitigate this bias for autoregressive\nmodels such as attention-based encoder-decoder and RNN-T. Typically, ILME is\nperformed by modularizing the acoustic and language components of the model\narchitecture, and eliminating the acoustic input to perform log-linear\ninterpolation with the text-only posterior. However, for CTC-based ASR, it is\nnot as straightforward to decouple the model into such acoustic and language\ncomponents, as CTC log-posteriors are computed in a non-autoregressive manner.\nIn this work, we propose a novel ILME technique for CTC-based ASR models. Our\nmethod iteratively masks the audio timesteps to estimate a pseudo\nlog-likelihood of the internal LM by accumulating log-posteriors for only the\nmasked timesteps. Extensive evaluation across multiple out-of-domain datasets\nreveals that the proposed approach improves WER by up to 9.8% and OOV F1-score\nby up to 24.6% relative to Shallow Fusion, when only text data from target\ndomain is available. In the case of zero-shot domain adaptation, with no access\nto any target domain data, we demonstrate that removing the source domain bias\nwith ILME can still outperform Shallow Fusion to improve WER by up to 9.3%\nrelative.",
        "pdf_link": "https://arxiv.org/pdf/2305.03837v1.pdf"
    },
    {
        "title": "Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios",
        "authors": [
            "Hazal T\u00fcrkmen",
            "O\u011fuz Dikenelli",
            "Cenk Eraslan",
            "Mehmet Cem \u00c7all\u0131",
            "S\u00fcha S\u00fcreyya \u00d6zbek"
        ],
        "published": "2023-05-05T18:39:07Z",
        "summary": "In recent years, major advancements in natural language processing (NLP) have\nbeen driven by the emergence of large language models (LLMs), which have\nsignificantly revolutionized research and development within the field.\nBuilding upon this progress, our study delves into the effects of various\npre-training methodologies on Turkish clinical language models' performance in\na multi-label classification task involving radiology reports, with a focus on\naddressing the challenges posed by limited language resources. Additionally, we\nevaluated the simultaneous pretraining approach by utilizing limited clinical\ntask data for the first time. We developed four models, including\nTurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and\nTurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model\n(BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a\nsubstantial general-domain corpus, demonstrate the best overall performance.\nAlthough the task-adaptive pre-training approach has the potential to capture\ndomain-specific patterns, it is constrained by the limited task-specific corpus\nand may be susceptible to overfitting. Furthermore, our results underscore the\nsignificance of domain-specific vocabulary during pre-training for enhancing\nmodel performance. Ultimately, we observe that the combination of\ngeneral-domain knowledge and task-specific fine-tuning is essential for\nachieving optimal performance across a range of categories. This study offers\nvaluable insights for developing effective Turkish clinical language models and\ncan guide future research on pre-training techniques for other low-resource\nlanguages within the clinical domain.",
        "pdf_link": "https://arxiv.org/pdf/2305.03788v1.pdf"
    },
    {
        "title": "Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management",
        "authors": [
            "Oluwatosin Ogundare",
            "Subuola Sofolahan"
        ],
        "published": "2023-05-05T17:55:49Z",
        "summary": "This study investigates the potential of an ambulatory device that\nincorporates Large Language Models (LLMs) in cadence with other specialized ML\nmodels to assess anemia severity in sickle cell patients in real time. The\ndevice would rely on sensor data that measures angiogenic material levels to\nassess anemia severity, providing real-time information to patients and\nclinicians to reduce the frequency of vaso-occlusive crises because of the\nearly detection of anemia severity, allowing for timely interventions and\npotentially reducing the likelihood of serious complications. The main\nchallenges in developing such a device are the creation of a reliable\nnon-invasive tool for angiogenic level assessment, a biophysics model and the\npractical consideration of an LLM communicating with emergency personnel on\nbehalf of an incapacitated patient. A possible system is proposed, and the\nlimitations of this approach are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2305.03715v1.pdf"
    },
    {
        "title": "LMEye: An Interactive Perception Network for Large Language Models",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Xinyu Chen",
            "Lin Ma",
            "Yong Xu",
            "Min Zhang"
        ],
        "published": "2023-05-05T17:27:21Z",
        "summary": "Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4,\nis resource-intensive. Regarding Large Language Models (LLMs) as the core\nprocessor for multimodal information, our paper introduces LMEye, a human-like\neye with a play-and-plug interactive perception network, designed to enable\ndynamic interaction between LLMs and external vision information. Previous\nmethods incorporate visual information into LLMs with a simple visual mapping\nnetwork or Q-former from BLIP-2. Such networks project the image feature once\nyet do not consider the interaction between the image and the human input\nquery. Hence, the obtained visual information without being connected to human\nintention may be inadequate for LLMs to generate intention-following responses,\nwhich we refer to as static visual information. LMEye addresses this issue by\nallowing the LLM to request the desired visual information aligned with various\nhuman instructions, which we term as the dynamic visual information\ninteraction. Specifically, LMEye consists of a simple visual mapping network to\nprovide the basic perception of an image for LLMs. It also contains additional\nmodules responsible for acquiring requests from LLMs, performing request-based\nvisual information interaction, and transmitting the resulting interacted\nvisual information to LLMs, respectively. In this way, LLMs act to understand\nthe human query, deliver the corresponding request to the request-based visual\ninformation interaction module, and generate the response based on the\ninterleaved multimodal information. We evaluate LMEye through extensive\nexperiments on some multimodal benchmarks, demonstrating that it significantly\nimproves the zero-shot performance on various multimodal tasks compared to\nprevious methods, with less parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.03701v6.pdf"
    },
    {
        "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
        "authors": [
            "Jiacheng Liu",
            "Wenya Wang",
            "Dianzhuo Wang",
            "Noah A. Smith",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-05T17:15:32Z",
        "summary": "Despite the much discussed capabilities of today's language models, they are\nstill prone to silly and unexpected commonsense failures. We consider a\nretrospective verification approach that reflects on the correctness of LM\noutputs, and introduce Vera, a general-purpose model that estimates the\nplausibility of declarative statements based on commonsense knowledge. Trained\non ~7M commonsense statements created from 19 QA datasets and two large-scale\nknowledge bases, and with a combination of three training objectives, Vera is a\nversatile model that effectively separates correct from incorrect statements\nacross diverse commonsense domains. When applied to solving commonsense\nproblems in the verification format, Vera substantially outperforms existing\nmodels that can be repurposed for commonsense verification, and it further\nexhibits generalization capabilities to unseen tasks and provides\nwell-calibrated outputs. We find that Vera excels at filtering LM-generated\ncommonsense knowledge and is useful in detecting erroneous commonsense\nstatements generated by models like ChatGPT in real-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.03695v3.pdf"
    },
    {
        "title": "Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs",
        "authors": [
            "Somin Wadhwa",
            "Jay DeYoung",
            "Benjamin Nye",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-05-05T16:02:06Z",
        "summary": "Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings:\nhttp://ico-relations.ebm-nlp.com",
        "pdf_link": "https://arxiv.org/pdf/2305.03642v3.pdf"
    },
    {
        "title": "Now It Sounds Like You: Learning Personalized Vocabulary On Device",
        "authors": [
            "Sid Wang",
            "Ashish Shenoy",
            "Pierce Chuang",
            "John Nguyen"
        ],
        "published": "2023-05-05T14:44:20Z",
        "summary": "In recent years, Federated Learning (FL) has shown significant advancements\nin its ability to perform various natural language processing (NLP) tasks. This\nwork focuses on applying personalized FL for on-device language modeling. Due\nto limitations of memory and latency, these models cannot support the\ncomplexity of sub-word tokenization or beam search decoding, resulting in the\ndecision to deploy a closed-vocabulary language model. However,\nclosed-vocabulary models are unable to handle out-of-vocabulary (OOV) words\nbelonging to specific users. To address this issue, We propose a novel\ntechnique called \"OOV expansion\" that improves OOV coverage and increases model\naccuracy while minimizing the impact on memory and latency. This method\nintroduces a personalized \"OOV adapter\" that effectively transfers knowledge\nfrom a central model and learns word embedding for personalized vocabulary. OOV\nexpansion significantly outperforms standard FL personalization methods on a\nset of common FL benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2305.03584v3.pdf"
    },
    {
        "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering",
        "authors": [
            "Lei Wang",
            "Yi Hu",
            "Jiabang He",
            "Xing Xu",
            "Ning Liu",
            "Hui Liu",
            "Heng Tao Shen"
        ],
        "published": "2023-05-05T11:56:30Z",
        "summary": "Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the external\nessential information missed. To address these issues, we propose a novel\nmethod termed T-SciQ that aims at teaching science question answering with LLM\nsignals. The T-SciQ approach generates high-quality CoT rationales as teaching\nsignals and is advanced to train much smaller models to perform CoT reasoning\nin complex modalities. Additionally, we introduce a novel data mixing strategy\nto produce more effective teaching data samples for simple and complex science\nquestion answer problems. Extensive experimental results show that our T-SciQ\nmethod achieves a new state-of-the-art performance on the ScienceQA benchmark,\nwith an accuracy of 96.18%. Moreover, our approach outperforms the most\npowerful fine-tuned baseline by 4.5%. The code is publicly available at\nhttps://github.com/T-SciQ/T-SciQ.",
        "pdf_link": "https://arxiv.org/pdf/2305.03453v4.pdf"
    },
    {
        "title": "Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects",
        "authors": [
            "Kehui Tan",
            "Tianqi Pang",
            "Chenyou Fan",
            "Song Yu"
        ],
        "published": "2023-05-05T11:09:13Z",
        "summary": "This perspective paper proposes a series of interactive scenarios that\nutilize Artificial Intelligence (AI) to enhance classroom teaching, such as\ndialogue auto-completion, knowledge and style transfer, and assessment of\nAI-generated content. By leveraging recent developments in Large Language\nModels (LLMs), we explore the potential of AI to augment and enrich\nteacher-student dialogues and improve the quality of teaching. Our goal is to\nproduce innovative and meaningful conversations between teachers and students,\ncreate standards for evaluation, and improve the efficacy of AI-for-Education\ninitiatives. In Section 3, we discuss the challenges of utilizing existing LLMs\nto effectively complete the educated tasks and present a unified framework for\naddressing diverse education dataset, processing lengthy conversations, and\ncondensing information to better accomplish more downstream tasks. In Section\n4, we summarize the pivoting tasks including Teacher-Student Dialogue\nAuto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessment\nof AI-Generated Content (AIGC), providing a clear path for future research. In\nSection 5, we also explore the use of external and adjustable LLMs to improve\nthe generated content through human-in-the-loop supervision and reinforcement\nlearning. Ultimately, this paper seeks to highlight the potential for AI to aid\nthe field of education and promote its further exploration.",
        "pdf_link": "https://arxiv.org/pdf/2305.03433v2.pdf"
    },
    {
        "title": "Using ChatGPT for Entity Matching",
        "authors": [
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "published": "2023-05-05T10:39:32Z",
        "summary": "Entity Matching is the task of deciding if two entity descriptions refer to\nthe same real-world entity. State-of-the-art entity matching methods often rely\non fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks\nof using these models for entity matching are that (i) the models require\nsignificant amounts of fine-tuning data for reaching a good performance and\n(ii) the fine-tuned models are not robust concerning out-of-distribution\nentities. In this paper, we investigate using ChatGPT for entity matching as a\nmore robust, training data-efficient alternative to traditional Transformer\nmodels. We perform experiments along three dimensions: (i) general prompt\ndesign, (ii) in-context learning, and (iii) provision of higher-level matching\nknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,\nreaching a zero-shot performance of 82.35% F1 on a challenging matching task on\nwhich RoBERTa requires 2000 training examples for reaching a similar\nperformance. Adding in-context demonstrations to the prompts further improves\nthe F1 by up to 7.85% when using similarity-based example selection. Always\nusing the same set of 10 handpicked demonstrations leads to an improvement of\n4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be\nguided by adding higher-level matching knowledge in the form of rules to the\nprompts. Providing matching rules leads to similar performance gains as\nproviding in-context demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2305.03423v2.pdf"
    },
    {
        "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
        "authors": [
            "Hanlin Zhang",
            "Jiani Huang",
            "Ziyang Li",
            "Mayur Naik",
            "Eric Xing"
        ],
        "published": "2023-05-05T07:24:46Z",
        "summary": "Pre-trained large language models (LMs) struggle to perform logical reasoning\nreliably despite advances in scale and compositionality. In this work, we\ntackle this challenge through the lens of symbolic programming. We propose\nDSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs\ngovern the perception of factual knowledge, and a symbolic module performs\ndeductive reasoning. In contrast to works that rely on hand-crafted logic\nrules, our differentiable symbolic reasoning framework efficiently learns\nweighted rules and applies semantic loss to further improve LMs. DSR-LM is\nscalable, interpretable, and allows easy integration of prior knowledge,\nthereby supporting extensive symbolic programming to robustly derive a logical\nconclusion. The results of our experiments suggest that DSR-LM improves the\nlogical reasoning abilities of pre-trained language models, resulting in a\nsignificant increase in accuracy of over 20% on deductive reasoning benchmarks.\nFurthermore, DSR-LM outperforms a variety of competitive baselines when faced\nwith systematic changes in sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2305.03742v1.pdf"
    },
    {
        "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Shafiq Joty",
            "Chengwei Qin",
            "Lidong Bing"
        ],
        "published": "2023-05-05T03:49:14Z",
        "summary": "As large language models (LLMs) have become the norm in NLP, demonstrating\ngood performance in generation and reasoning tasks, one of its most fatal\ndisadvantages is the lack of factual correctness. Generating unfactual texts\nnot only leads to lower performances but also degrades the trust and validity\nof their applications. Chain-of-Thought (CoT) prompting improves trust and\nmodel performance on complex reasoning tasks by generating interpretable\nreasoning chains, but still suffers from factuality concerns in\nknowledge-intensive tasks. In this paper, we propose the Verify-and-Edit\nframework for CoT prompting, which seeks to increase prediction factuality by\npost-editing reasoning chains according to external knowledge. Building on top\nof GPT-3, our framework lead to accuracy improvements in multiple open-domain\nquestion-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.03268v1.pdf"
    },
    {
        "title": "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna",
        "authors": [
            "Bin Ji"
        ],
        "published": "2023-05-05T02:46:22Z",
        "summary": "Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and\nfew-shot capabilities in Named Entity Recognition (NER). However, these models\ncan only be accessed via online APIs, which may cause data leak and\nnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot\nNER framework based on the newly released open-source LLM -- Vicuna. VicunaNER\nis a two-phase framework, where each phase leverages multi-turn dialogues with\nVicuna to recognize entities from texts. We name the second phase as\nRe-Recognition, which recognizes those entities not recognized in the first\nphase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues\nin each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot\ncapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.\nExperimental results demonstrate that VicunaNER achieves superior performance\nin both shot settings. Additionally, we conduct comprehensive investigations on\nVicuna from multiple perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2305.03253v1.pdf"
    },
    {
        "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics",
        "authors": [
            "Shervin Ardeshir"
        ],
        "published": "2023-05-04T23:54:37Z",
        "summary": "Trained on a vast amount of data, Large Language models (LLMs) have achieved\nunprecedented success and generalization in modeling fairly complex textual\ninputs in the abstract space, making them powerful tools for zero-shot\nlearning. Such capability is extended to other modalities such as the visual\ndomain using cross-modal foundation models such as CLIP, and as a result,\nsemantically meaningful representation are extractable from visual inputs.\n  In this work, we leverage this capability and propose an approach that can\nprovide semantic insights into a model's patterns of failures and biases. Given\na black box model, its training data, and task definition, we first calculate\nits task-related loss for each data point. We then extract a semantically\nmeaningful representation for each training data point (such as CLIP embeddings\nfrom its visual encoder) and train a lightweight diagnosis model which maps\nthis semantically meaningful representation of a data point to its task loss.\nWe show that an ensemble of such lightweight models can be used to generate\ninsights on the performance of the black-box model, in terms of identifying its\npatterns of failures and biases.",
        "pdf_link": "https://arxiv.org/pdf/2305.03212v2.pdf"
    },
    {
        "title": "Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing",
        "authors": [
            "Jawid Ahmad Baktash",
            "Mursal Dawodi"
        ],
        "published": "2023-05-04T22:46:43Z",
        "summary": "Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation\nlanguage model in the GPT series, developed by OpenAI, which promises\nsignificant advancements in the field of natural language processing (NLP). In\nthis research article, we have discussed the features of GPT-4, its potential\napplications, and the challenges that it might face. We have also compared\nGPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one\ntrillion), better multilingual capabilities, improved contextual understanding,\nand reasoning capabilities than GPT-3. Some of the potential applications of\nGPT-4 include chatbots, personal assistants, language translation, text\nsummarization, and question-answering. However, GPT-4 poses several challenges\nand limitations such as computational requirements, data requirements, and\nethical concerns.",
        "pdf_link": "https://arxiv.org/pdf/2305.03195v1.pdf"
    },
    {
        "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
        "authors": [
            "Jinyang Li",
            "Binyuan Hui",
            "Ge Qu",
            "Jiaxi Yang",
            "Binhua Li",
            "Bowen Li",
            "Bailin Wang",
            "Bowen Qin",
            "Rongyu Cao",
            "Ruiying Geng",
            "Nan Huo",
            "Xuanhe Zhou",
            "Chenhao Ma",
            "Guoliang Li",
            "Kevin C. C. Chang",
            "Fei Huang",
            "Reynold Cheng",
            "Yongbin Li"
        ],
        "published": "2023-05-04T19:02:29Z",
        "summary": "Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.03111v3.pdf"
    },
    {
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
        "authors": [
            "Zhiqing Sun",
            "Yikang Shen",
            "Qinhong Zhou",
            "Hongxin Zhang",
            "Zhenfang Chen",
            "David Cox",
            "Yiming Yang",
            "Chuang Gan"
        ],
        "published": "2023-05-04T17:59:28Z",
        "summary": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including < 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2305.03047v2.pdf"
    },
    {
        "title": "Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study",
        "authors": [
            "Sajjad Rahmani",
            "AmirHossein Naghshzan",
            "Latifa Guerrouj"
        ],
        "published": "2023-05-04T17:43:19Z",
        "summary": "Our research investigates the recommendation of code examples to aid software\ndevelopers, a practice that saves developers significant time by providing\nready-to-use code snippets. The focus of our study is Stack Overflow, a\ncommonly used resource for coding discussions and solutions, particularly in\nthe context of the Java programming language. We applied BERT, a powerful Large\nLanguage Model (LLM) that enables us to transform code examples into numerical\nvectors by extracting their semantic information. Once these numerical\nrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)\nusing Locality-Sensitive Hashing (LSH). Our research employed two variants of\nLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared\nthese two approaches across four parameters: HitRate, Mean Reciprocal Rank\n(MRR), Average Execution Time, and Relevance. Our study revealed that the\nQuery-Aware (QA) approach showed superior performance over the Random\nHyperplane-based (RH) method. Specifically, it exhibited a notable improvement\nof 20\\% to 35\\% in HitRate for query pairs compared to the RH approach.\nFurthermore, the QA approach proved significantly more time-efficient, with its\nspeed in creating hashing tables and assigning data samples to buckets being at\nleast four times faster. It can return code examples within milliseconds,\nwhereas the RH approach typically requires several seconds to recommend code\nexamples. Due to the superior performance of the QA approach, we tested it\nagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA method\nshowed comparable efficiency proving its potential for effective code\nrecommendation.",
        "pdf_link": "https://arxiv.org/pdf/2305.03017v4.pdf"
    },
    {
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
        "authors": [
            "Haoran Li",
            "Mingshi Xu",
            "Yangqiu Song"
        ],
        "published": "2023-05-04T17:31:41Z",
        "summary": "Sentence-level representations are beneficial for various natural language\nprocessing tasks. It is commonly believed that vector representations can\ncapture rich linguistic properties. Currently, large language models (LMs)\nachieve state-of-the-art performance on sentence embedding. However, some\nrecent works suggest that vector representations from LMs can cause information\nleakage. In this work, we further investigate the information leakage issue and\npropose a generative embedding inversion attack (GEIA) that aims to reconstruct\ninput sequences based only on their sentence embeddings. Given the black-box\naccess to a language model, we treat sentence embeddings as initial tokens'\nrepresentations and train or fine-tune a powerful decoder model to decode the\nwhole sequences directly. We conduct extensive experiments to demonstrate that\nour generative inversion attack outperforms previous embedding inversion\nattacks in classification metrics and generates coherent and contextually\nsimilar sentences as the original inputs.",
        "pdf_link": "https://arxiv.org/pdf/2305.03010v1.pdf"
    },
    {
        "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
        "authors": [
            "Reid Pryzant",
            "Dan Iter",
            "Jerry Li",
            "Yin Tat Lee",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023-05-04T15:15:22Z",
        "summary": "Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2305.03495v2.pdf"
    },
    {
        "title": "An automatically discovered chain-of-thought prompt generalizes to novel models and datasets",
        "authors": [
            "Konstantin Hebenstreit",
            "Robert Praas",
            "Louis P Kiesewetter",
            "Matthias Samwald"
        ],
        "published": "2023-05-04T15:07:20Z",
        "summary": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improve\nperformance and explainability of large language models (LLMs). However,\nuncertainties remain about how reasoning strategies formulated for previous\nmodel generations generalize to new model generations and different datasets.\nIn this small-scale study, we compare different reasoning strategies induced by\nzero-shot prompting across six recently released LLMs (davinci-002,\ndavinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a\nmixture of six question-answering datasets, including datasets from scientific\nand medical domains. Our findings demonstrate that while some variations in\neffectiveness occur, gains from CoT reasoning strategies remain robust across\ndifferent models and datasets. GPT-4 has the most benefit from current\nstate-of-the-art reasoning strategies and exhibits the best performance by\napplying a prompt previously discovered through automated discovery.",
        "pdf_link": "https://arxiv.org/pdf/2305.02897v2.pdf"
    },
    {
        "title": "\"Oops, Did I Just Say That?\" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",
        "authors": [
            "Pingchuan Ma",
            "Zongjie Li",
            "Ao Sun",
            "Shuai Wang"
        ],
        "published": "2023-05-04T08:00:32Z",
        "summary": "As the popularity of large language models (LLMs) soars across various\napplications, ensuring their alignment with human values has become a paramount\nconcern. In particular, given that LLMs have great potential to serve as\ngeneral-purpose AI assistants in daily life, their subtly unethical suggestions\nbecome a serious and real concern. Tackling the challenge of automatically\ntesting and repairing unethical suggestions is thus demanding.\n  This paper introduces the first framework for testing and repairing unethical\nsuggestions made by LLMs. We first propose ETHICSSUITE, a test suite that\npresents complex, contextualized, and realistic moral scenarios to test LLMs.\nWe then propose a novel suggest-critic-reflect (SCR) process, serving as an\nautomated test oracle to detect unethical suggestions. We recast deciding if\nLLMs yield unethical suggestions (a hard problem; often requiring human\nexpertise and costly to decide) into a PCR task that can be automatically\nchecked for violation. Moreover, we propose a novel on-the-fly (OTF) repairing\nscheme that repairs unethical suggestions made by LLMs in real-time. The OTF\nscheme is applicable to LLMs in a black-box API setting with moderate cost.\nWith ETHICSSUITE, our study on seven popular LLMs (e.g., ChatGPT, GPT-4)\nuncovers in total 109,824 unethical suggestions. We apply our OTF scheme on two\nLLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable\namount of unethical ones, paving the way for more ethically conscious LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.02626v1.pdf"
    },
    {
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "authors": [
            "Hang Chen",
            "Jing Luo",
            "Xinyu Yang",
            "Wenjing Zhu"
        ],
        "published": "2023-05-04T07:45:49Z",
        "summary": "Our investigation into the Affective Reasoning in Conversation (ARC) task\nhighlights the challenge of causal discrimination. Almost all existing models,\nincluding large language models (LLMs), excel at capturing semantic\ncorrelations within utterance embeddings but fall short in determining the\nspecific causal relationships. To overcome this limitation, we propose the\nincorporation of \\textit{i.i.d.} noise terms into the conversation process,\nthereby constructing a structural causal model (SCM). It explores how distinct\ncausal relationships of fitted embeddings can be discerned through independent\nconditions. To facilitate the implementation of deep learning, we introduce the\ncogn frameworks to handle unstructured conversation data, and employ an\nautoencoder architecture to regard the unobservable noise as learnable\n\"implicit causes.\" Moreover, we curate a synthetic dataset that includes i.i.d.\nnoise. Through comprehensive experiments, we validate the effectiveness and\ninterpretability of our approach. Our code is available in\nhttps://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.",
        "pdf_link": "https://arxiv.org/pdf/2305.02615v2.pdf"
    },
    {
        "title": "Faithful Question Answering with Monte-Carlo Planning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Hong Zhao",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023-05-04T05:21:36Z",
        "summary": "Although large language models demonstrate remarkable question-answering\nperformances, revealing the intermediate reasoning steps that the models\nfaithfully follow remains challenging. In this paper, we propose FAME (FAithful\nquestion answering with MontE-carlo planning) to answer questions based on\nfaithful reasoning steps. The reasoning steps are organized as a structured\nentailment tree, which shows how premises are used to produce intermediate\nconclusions that can prove the correctness of the answer. We formulate the task\nas a discrete decision-making problem and solve it through the interaction of a\nreasoning environment and a controller. The environment is modular and contains\nseveral basic task-oriented modules, while the controller proposes actions to\nassemble the modules. Since the search space could be large, we introduce a\nMonte-Carlo planning algorithm to do a look-ahead search and select actions\nthat will eventually lead to high-quality steps. FAME achieves state-of-the-art\nperformance on the standard benchmark. It can produce valid and faithful\nreasoning steps compared with large language models with a much smaller model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2305.02556v1.pdf"
    },
    {
        "title": "Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era",
        "authors": [
            "Dong Zhang"
        ],
        "published": "2023-05-04T05:21:09Z",
        "summary": "With various AI tools such as ChatGPT becoming increasingly popular, we are\nentering a true AI era. We can foresee that exceptional AI tools will soon reap\nconsiderable profits. A crucial question arise: should AI tools share revenue\nwith their training data providers in additional to traditional stakeholders\nand shareholders? The answer is Yes. Large AI tools, such as large language\nmodels, always require more and better quality data to continuously improve,\nbut current copyright laws limit their access to various types of data. Sharing\nrevenue between AI tools and their data providers could transform the current\nhostile zero-sum game relationship between AI tools and a majority of\ncopyrighted data owners into a collaborative and mutually beneficial one, which\nis necessary to facilitate the development of a virtuous cycle among AI tools,\ntheir users and data providers that drives forward AI technology and builds a\nhealthy AI ecosystem. However, current revenue-sharing business models do not\nwork for AI tools in the forthcoming AI era, since the most widely used metrics\nfor website-based traffic and action, such as clicks, will be replaced by new\nmetrics such as prompts and cost per prompt for generative AI tools. A\ncompletely new revenue-sharing business model, which must be almost independent\nof AI tools and be easily explained to data providers, needs to establish a\nprompt-based scoring system to measure data engagement of each data provider.\nThis paper systematically discusses how to build such a scoring system for all\ndata providers for AI tools based on classification and content similarity\nmodels, and outlines the requirements for AI tools or third parties to build\nit. Sharing revenue with data providers using such a scoring system would\nencourage more data owners to participate in the revenue-sharing program. This\nwill be a utilitarian AI era where all parties benefit.",
        "pdf_link": "https://arxiv.org/pdf/2305.02555v2.pdf"
    },
    {
        "title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
        "authors": [
            "Hang Jiang",
            "Xiajie Zhang",
            "Xubo Cao",
            "Cynthia Breazeal",
            "Deb Roy",
            "Jad Kabbara"
        ],
        "published": "2023-05-04T04:58:00Z",
        "summary": "Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents which we refer to as LLM personas and present a case study with GPT-3.5\nand GPT-4 to investigate whether LLMs can generate content that aligns with\ntheir assigned personality profiles. To this end, we simulate distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, LLM personas' writings have emerging representative\nlinguistic patterns for personality traits when compared with a human writing\ncorpus. Furthermore, human evaluation shows that humans can perceive some\npersonality traits with an accuracy of up to 80%. Interestingly, the accuracy\ndrops significantly when the annotators were informed of AI authorship.",
        "pdf_link": "https://arxiv.org/pdf/2305.02547v5.pdf"
    },
    {
        "title": "Can LLMs Capture Human Preferences?",
        "authors": [
            "Ali Goli",
            "Amandeep Singh"
        ],
        "published": "2023-05-04T03:51:31Z",
        "summary": "We explore the viability of Large Language Models (LLMs), specifically\nOpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting\npreferences, with a focus on intertemporal choices. Leveraging the extensive\nliterature on intertemporal discounting for benchmarking, we examine responses\nfrom LLMs across various languages and compare them to human responses,\nexploring preferences between smaller, sooner, and larger, later rewards. Our\nfindings reveal that both GPT models demonstrate less patience than humans,\nwith GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike\nhuman decision-makers. Though GPT-4 does not display lexicographic preferences,\nits measured discount rates are still considerably larger than those found in\nhumans. Interestingly, GPT models show greater patience in languages with weak\nfuture tense references, such as German and Mandarin, aligning with existing\nliterature that suggests a correlation between language structure and\nintertemporal preferences. We demonstrate how prompting GPT to explain its\ndecisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but\ndoes not eliminate, discrepancies between LLM and human responses. While\ndirectly eliciting preferences using LLMs may yield misleading results,\ncombining chain-of-thought conjoint with topic modeling aids in hypothesis\ngeneration, enabling researchers to explore the underpinnings of preferences.\nChain-of-thought conjoint provides a structured framework for marketers to use\nLLMs to identify potential attributes or factors that can explain preference\nheterogeneity across different customers and contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.02531v6.pdf"
    },
    {
        "title": "AutoML-GPT: Automatic Machine Learning with GPT",
        "authors": [
            "Shujian Zhang",
            "Chengyue Gong",
            "Lemeng Wu",
            "Xingchao Liu",
            "Mingyuan Zhou"
        ],
        "published": "2023-05-04T02:09:43Z",
        "summary": "AI tasks encompass a wide range of domains and fields. While numerous AI\nmodels have been designed for specific tasks and applications, they often\nrequire considerable human efforts in finding the right model architecture,\noptimization algorithm, and hyperparameters. Recent advances in large language\nmodels (LLMs) like ChatGPT show remarkable capabilities in various aspects of\nreasoning, comprehension, and interaction. Consequently, we propose developing\ntask-oriented prompts and automatically utilizing LLMs to automate the training\npipeline. To implement this concept, we present the AutoML-GPT, which employs\nGPT as the bridge to diverse AI models and dynamically trains models with\noptimized hyperparameters. AutoML-GPT dynamically takes user requests from the\nmodel and data cards and composes the corresponding prompt paragraph.\nUltimately, with this prompt paragraph, AutoML-GPT will automatically conduct\nthe experiments from data processing to model architecture, hyperparameter\ntuning, and predicted training log. By leveraging {\\ours}'s robust language\ncapabilities and the available AI models, AutoML-GPT can tackle numerous\nintricate AI tasks across various tasks and datasets. This approach achieves\nremarkable results in computer vision, natural language processing, and other\nchallenging areas. Extensive experiments and ablation studies demonstrate that\nour method can be general, effective, and beneficial for many AI tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.02499v1.pdf"
    },
    {
        "title": "Personalized Abstractive Summarization by Tri-agent Generation Pipeline",
        "authors": [
            "Wen Xiao",
            "Yujia Xie",
            "Giuseppe Carenini",
            "Pengcheng He"
        ],
        "published": "2023-05-04T01:12:35Z",
        "summary": "Tailoring outputs from large language models, like ChatGPT, to implicit user\npreferences remains a challenge despite their impressive generative\ncapabilities. In this paper, we propose a tri-agent generation pipeline\ncomprising a generator, an instructor, and an editor to enhance output\npersonalization. The generator produces an initial output, the instructor\nautomatically generates editing instructions based on user preferences, and the\neditor refines the output to align with those preferences. The inference-only\nlarge language model (ChatGPT) serves as both the generator and editor, with a\nsmaller model acting as the instructor to guide output generation. We train the\ninstructor using editor-steered reinforcement learning, leveraging feedback\nfrom a large-scale editor model to optimize instruction generation.\nExperimental results on two abstractive summarization datasets demonstrate the\neffectiveness of our approach in generating outputs that better meet user\nexpectations. Code is available at\n\\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",
        "pdf_link": "https://arxiv.org/pdf/2305.02483v2.pdf"
    },
    {
        "title": "Black-box Prompt Tuning with Subspace Learning",
        "authors": [
            "Yuanhang Zheng",
            "Zhixing Tan",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023-05-04T01:04:25Z",
        "summary": "Black-box prompt tuning uses derivative-free optimization algorithms to learn\nprompts in low-dimensional subspaces instead of back-propagating through the\nnetwork of Large Language Models (LLMs). Recent studies have found that\nblack-box prompt tuning lacks versatility across tasks and LLMs, which we\nbelieve is related to the inappropriate choice of subspaces. In this paper, we\npropose Black-box prompt tuning with Subspace Learning (BSL) to improve the\nversatility of black-box prompt tuning. Based on the assumption that nearly\noptimal prompts for similar tasks exist in a common subspace, we propose\nidentifying such subspaces by meta-learning on a set of similar source tasks.\nTherefore, for a target task that shares similarities with source tasks, we\nguarantee that optimizing in the subspace can find a prompt that performs well\non the target task. Experiments confirm that our BSL framework consistently\nachieves competitive performance regardless of downstream tasks and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.03518v1.pdf"
    },
    {
        "title": "The System Model and the User Model: Exploring AI Dashboard Design",
        "authors": [
            "Fernanda Vi\u00e9gas",
            "Martin Wattenberg"
        ],
        "published": "2023-05-04T00:22:49Z",
        "summary": "This is a speculative essay on interface design and artificial intelligence.\nRecently there has been a surge of attention to chatbots based on large\nlanguage models, including widely reported unsavory interactions. We contend\nthat part of the problem is that text is not all you need: sophisticated AI\nsystems should have dashboards, just like all other complicated devices.\nAssuming the hypothesis that AI systems based on neural networks will contain\ninterpretable models of aspects of the world around them, we discuss what data\nsuch dashboards might display. We conjecture that, for many systems, the two\nmost important models will be of the user and of the system itself. We call\nthese the System Model and User Model. We argue that, for usability and safety,\ninterfaces to dialogue-based AI systems should have a parallel display based on\nthe state of the System Model and the User Model. Finding ways to identify,\ninterpret, and display these two models should be a core part of interface\nresearch for AI.",
        "pdf_link": "https://arxiv.org/pdf/2305.02469v1.pdf"
    },
    {
        "title": "Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs",
        "authors": [
            "Deepak Narayanan",
            "Keshav Santhanam",
            "Peter Henderson",
            "Rishi Bommasani",
            "Tony Lee",
            "Percy Liang"
        ],
        "published": "2023-05-03T21:51:42Z",
        "summary": "Large language models (LLMs) power many state-of-the-art systems in natural\nlanguage processing. However, these models are extremely computationally\nexpensive, even at inference time, raising the natural question: when is the\nextra cost of deploying a larger model worth the anticipated boost in\ncapabilities? Better understanding this tradeoff fundamentally could benefit\nfrom an inference efficiency metric that is both (i) easily comparable across\nmodels from different providers, and (ii) representative of the true cost of\nrunning queries in an isolated performance environment. Unfortunately, access\nto LLMs today is largely restricted to black-box text generation APIs and raw\nruntimes measured through this interface do not satisfy these desiderata: model\nproviders can apply various software and hardware optimizations orthogonal to\nthe model, and models served on shared infrastructure are susceptible to\nperformance contention. To circumvent these problems, we propose a new metric\nfor comparing inference efficiency across models. This metric puts models on\nequal footing as though they were served (i) on uniform hardware and software,\nand (ii) without performance contention. We call this metric the\n\\emph{idealized runtime}, and we propose a methodology to efficiently estimate\nthis metric for autoregressive Transformer models. We also propose cost-aware\nvariants that incorporate the number of accelerators needed to serve the model.\nUsing these metrics, we compare ten state-of-the-art LLMs to provide the first\nanalysis of inference efficiency-capability tradeoffs; we make several\nobservations from this analysis, including the fact that the superior inference\nruntime performance of certain APIs is often a byproduct of optimizations\nwithin the API rather than the underlying model. Our methodology also\nfacilitates the efficient comparison of different software and hardware stacks.",
        "pdf_link": "https://arxiv.org/pdf/2305.02440v1.pdf"
    },
    {
        "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
        "authors": [
            "Xin Cheng",
            "Di Luo",
            "Xiuying Chen",
            "Lemao Liu",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2023-05-03T21:40:54Z",
        "summary": "With direct access to human-written reference as memory, retrieval-augmented\ngeneration has achieved much progress in a wide range of text generation tasks.\nSince better memory would typically prompt better generation~(we define this as\nprimal problem). The traditional approach for memory retrieval involves\nselecting memory that exhibits the highest similarity to the input. However,\nthis method is constrained by the quality of the fixed corpus from which memory\nis retrieved. In this paper, by exploring the duality of the primal problem:\nbetter generation also prompts better memory, we propose a novel framework,\nselfmem, which addresses this limitation by iteratively employing a\nretrieval-augmented generator to create an unbounded memory pool and using a\nmemory selector to choose one output as memory for the subsequent generation\nround. This enables the model to leverage its own output, referred to as\nself-memory, for improved generation. We evaluate the effectiveness of selfmem\non three distinct text generation tasks: neural machine translation,\nabstractive text summarization, and dialogue generation, under two generation\nparadigms: fine-tuned small model and few-shot LLM. Our approach achieves\nstate-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),\nand BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in\nenhancing retrieval-augmented generation models. Furthermore, we conduct\nthorough analyses of each component in the selfmem framework to identify\nbottlenecks and provide insights for future research.",
        "pdf_link": "https://arxiv.org/pdf/2305.02437v3.pdf"
    },
    {
        "title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",
        "authors": [
            "Yue Wu",
            "So Yeon Min",
            "Yonatan Bisk",
            "Ruslan Salakhutdinov",
            "Amos Azaria",
            "Yuanzhi Li",
            "Tom Mitchell",
            "Shrimai Prabhumoye"
        ],
        "published": "2023-05-03T20:11:22Z",
        "summary": "Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications.",
        "pdf_link": "https://arxiv.org/pdf/2305.02412v2.pdf"
    },
    {
        "title": "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs",
        "authors": [
            "Yucheng Shi",
            "Hehuan Ma",
            "Wenliang Zhong",
            "Qiaoyu Tan",
            "Gengchen Mai",
            "Xiang Li",
            "Tianming Liu",
            "Junzhou Huang"
        ],
        "published": "2023-05-03T19:57:43Z",
        "summary": "ChatGPT, as a recently launched large language model (LLM), has shown\nsuperior performance in various natural language processing (NLP) tasks.\nHowever, two major limitations hinder its potential applications: (1) the\ninflexibility of finetuning on downstream tasks and (2) the lack of\ninterpretability in the decision-making process. To tackle these limitations,\nwe propose a novel framework that leverages the power of ChatGPT for specific\ntasks, such as text classification, while improving its interpretability. The\nproposed framework conducts a knowledge graph extraction task to extract\nrefined and structural knowledge from the raw data using ChatGPT. The rich\nknowledge is then converted into a graph, which is further used to train an\ninterpretable linear classifier to make predictions. To evaluate the\neffectiveness of our proposed method, we conduct experiments on four datasets.\nThe result shows that our method can significantly improve the performance\ncompared to directly utilizing ChatGPT for text classification tasks. And our\nmethod provides a more transparent decision-making process compared with\nprevious text classification methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.03513v2.pdf"
    },
    {
        "title": "Entity Tracking in Language Models",
        "authors": [
            "Najoung Kim",
            "Sebastian Schuster"
        ],
        "published": "2023-05-03T18:01:13Z",
        "summary": "Keeping track of how states of entities change as a text or dialog unfolds is\na key prerequisite to discourse understanding. Yet, there have been few\nsystematic investigations into the ability of large language models (LLMs) to\ntrack discourse entities. In this work, we present a task probing to what\nextent a language model can infer the final state of an entity given an English\ndescription of the initial state and a series of state-changing operations. We\nuse this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track\nthe state of entities, and find that only GPT-3.5 models, which have been\npretrained on large amounts of code, exhibit this ability. We then investigate\nwhether smaller models pretrained primarily on text can learn to track\nentities, through finetuning T5 on several training/evaluation splits. While\nperformance degrades for more complex splits, we find that even when evaluated\non a different set of entities from training or longer operation sequences, a\nfinetuned model can perform non-trivial entity tracking. Taken together, these\nresults suggest that language models can learn to track entities but\npretraining on text corpora alone does not make this capacity surface.",
        "pdf_link": "https://arxiv.org/pdf/2305.02363v2.pdf"
    },
    {
        "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
        "authors": [
            "Daniel Rose",
            "Vaishnavi Himakunthala",
            "Andy Ouyang",
            "Ryan He",
            "Alex Mei",
            "Yujie Lu",
            "Michael Saxon",
            "Chinmay Sonar",
            "Diba Mirza",
            "William Yang Wang"
        ],
        "published": "2023-05-03T17:58:29Z",
        "summary": "Recent advances in large language models elicit reasoning in a\nchain-of-thought that allows models to decompose problems in a human-like\nfashion. Though this paradigm improves multi-step reasoning ability in language\nmodels, it is limited by being unimodal and applied mainly to\nquestion-answering tasks. We claim that incorporating visual augmentation into\nreasoning is essential, especially for complex, imaginative tasks.\nConsequently, we introduce VCoT, a novel method that leverages chain-of-thought\nprompting with vision-language grounding to recursively bridge the logical gaps\nwithin sequential data. Our method uses visual guidance to generate synthetic\nmultimodal infillings that add consistent and novel information to reduce the\nlogical gaps for downstream tasks that can benefit from temporal reasoning, as\nwell as provide interpretability into models' multi-step reasoning. We apply\nVCoT to the Visual Storytelling and WikiHow summarization datasets and\ndemonstrate through human evaluation that VCoT offers novel and consistent\nsynthetic data augmentation beating chain-of-thought baselines, which can be\nused to enhance downstream performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.02317v3.pdf"
    },
    {
        "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
        "authors": [
            "Cheng-Yu Hsieh",
            "Chun-Liang Li",
            "Chih-Kuan Yeh",
            "Hootan Nakhost",
            "Yasuhisa Fujii",
            "Alexander Ratner",
            "Ranjay Krishna",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published": "2023-05-03T17:50:56Z",
        "summary": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .",
        "pdf_link": "https://arxiv.org/pdf/2305.02301v2.pdf"
    },
    {
        "title": "Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections",
        "authors": [
            "Ming Jiang",
            "Jennifer D'Souza",
            "S\u00f6ren Auer",
            "J. Stephen Downie"
        ],
        "published": "2023-05-03T17:32:16Z",
        "summary": "The rapid growth of research publications has placed great demands on digital\nlibraries (DL) for advanced information management technologies. To cater to\nthese demands, techniques relying on knowledge-graph structures are being\nadvocated. In such graph-based pipelines, inferring semantic relations between\nrelated scientific concepts is a crucial step. Recently, BERT-based pre-trained\nmodels have been popularly explored for automatic relation classification.\nDespite significant progress, most of them were evaluated in different\nscenarios, which limits their comparability. Furthermore, existing methods are\nprimarily evaluated on clean texts, which ignores the digitization context of\nearly scholarly publications in terms of machine scanning and optical character\nrecognition (OCR). In such cases, the texts may contain OCR noise, in turn\ncreating uncertainty about existing classifiers' performances. To address these\nlimitations, we started by creating OCR-noisy texts based on three clean\ncorpora. Given these parallel corpora, we conducted a thorough empirical\nevaluation of eight Bert-based classification models by focusing on three\nfactors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise\nimpacts. Experiments on clean data show that the domain-specific pre-trained\nBert is the best variant to identify scientific relations. The strategy of\npredicting a single relation each time outperforms the one simultaneously\nidentifying multiple relations in general. The optimal classifier's performance\ncan decline by around 10% to 20% in F-score on the noisy corpora. Insights\ndiscussed in this study can help DL stakeholders select techniques for building\noptimal knowledge-graph-based systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.02291v1.pdf"
    },
    {
        "title": "WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models",
        "authors": [
            "John Giorgi",
            "Augustin Toma",
            "Ronald Xie",
            "Sondra S. Chen",
            "Kevin R. An",
            "Grace X. Zheng",
            "Bo Wang"
        ],
        "published": "2023-05-03T15:58:28Z",
        "summary": "This paper describes our submission to the MEDIQA-Chat 2023 shared task for\nautomatic clinical note generation from doctor-patient conversations. We report\nresults for two approaches: the first fine-tunes a pre-trained language model\n(PLM) on the shared task data, and the second uses few-shot in-context learning\n(ICL) with a large language model (LLM). Both achieve high performance as\nmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and\nfirst, respectively, of all submissions to the shared task. Expert human\nscrutiny indicates that notes generated via the ICL-based approach with GPT-4\nare preferred about as often as human-written notes, making it a promising path\ntoward automated note generation from doctor-patient conversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.02220v2.pdf"
    },
    {
        "title": "Judgments of research co-created by generative AI: experimental evidence",
        "authors": [
            "Pawe\u0142 Niszczota",
            "Paul Conway"
        ],
        "published": "2023-05-03T15:57:39Z",
        "summary": "The introduction of ChatGPT has fuelled a public debate on the use of\ngenerative AI (large language models; LLMs), including its use by researchers.\nIn the current work, we test whether delegating parts of the research process\nto LLMs leads people to distrust and devalue researchers and scientific output.\nParticipants (N=402) considered a researcher who delegates elements of the\nresearch process to a PhD student or LLM, and rated (1) moral acceptability,\n(2) trust in the scientist to oversee future projects, and (3) the accuracy and\nquality of the output. People judged delegating to an LLM as less acceptable\nthan delegating to a human (d = -0.78). Delegation to an LLM also decreased\ntrust to oversee future research projects (d = -0.80), and people thought the\nresults would be less accurate and of lower quality (d = -0.85). We discuss how\nthis devaluation might transfer into the underreporting of generative AI use.",
        "pdf_link": "https://arxiv.org/pdf/2305.11873v1.pdf"
    },
    {
        "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
        "authors": [
            "Zhen Wan",
            "Fei Cheng",
            "Zhuoyuan Mao",
            "Qianying Liu",
            "Haiyue Song",
            "Jiwei Li",
            "Sadao Kurohashi"
        ],
        "published": "2023-05-03T13:28:08Z",
        "summary": "In spite of the potential for ground-breaking achievements offered by large\nlanguage models (LLMs) (e.g., GPT-3), they still lag significantly behind\nfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\nThis is due to the two major shortcomings of LLMs in RE: (1) low relevance\nregarding entity and relation in retrieved demonstrations for in-context\nlearning; and (2) the strong inclination to wrongly classify NULL examples into\nother pre-defined labels.\n  In this paper, we propose GPT-RE to bridge the gap between LLMs and\nfully-supervised baselines. GPT-RE successfully addresses the aforementioned\nissues by (1) incorporating task-specific entity representations in\ndemonstration retrieval; and (2) enriching the demonstrations with gold\nlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used RE\ndatasets, and observe that GPT-RE achieves improvements over not only existing\nGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE\nachieves SOTA performances on the Semeval and SciERC datasets, and competitive\nperformances on the TACRED and ACE05 datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.02105v3.pdf"
    },
    {
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2023-05-03T07:28:50Z",
        "summary": "Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.01937v1.pdf"
    },
    {
        "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
        "authors": [
            "Qinyuan Cheng",
            "Xiaogui Yang",
            "Tianxiang Sun",
            "Linyang Li",
            "Xipeng Qiu"
        ],
        "published": "2023-05-03T06:26:13Z",
        "summary": "Contrastive learning has become a popular approach in natural language\nprocessing, particularly for the learning of sentence embeddings. However, the\ndiscrete nature of natural language makes it difficult to ensure the quality of\npositive and negative sample pairs generated through data augmentation methods.\nAlthough supervised contrastive learning can produce more accurate sample pairs\nwith human feedback labels, it still lacks fine-grained training signals. In\nthis paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of\nsentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our\nmethod utilizes AI feedback from large pre-trained language models (LLMs) to\nconstruct sample pairs with fine-grained sample similarity scores to improve\ncontrastive learning. Besides, we combine human feedback and AI feedback to\nprovide better supervision signals for supervised contrastive learning of\nsentence embeddings. Experimental results show that our method achieves\nstate-of-the-art performance on several semantic textual similarity (STS) and\ntransfer learning tasks compared to other unsupervised and supervised\ncontrastive learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.01918v3.pdf"
    },
    {
        "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
        "authors": [
            "Peifeng Wang",
            "Zhengyang Wang",
            "Zheng Li",
            "Yifan Gao",
            "Bing Yin",
            "Xiang Ren"
        ],
        "published": "2023-05-03T03:47:00Z",
        "summary": "Large language models (LMs) beyond a certain scale, demonstrate the emergent\ncapability of generating free-text rationales for their predictions via\nchain-of-thought (CoT) prompting. While CoT can yield dramatically improved\nperformance, such gains are only observed for sufficiently large LMs. Even more\nconcerning, there is little guarantee that the generated rationales are\nconsistent with LM's predictions or faithfully justify the decisions. In this\nwork, we propose a faithful knowledge distillation method to learn a small,\nself-consistent CoT model from a teacher model that is orders of magnitude\nlarger. To form better supervision, we elicit rationales supporting the gold\nanswers from a large LM (teacher) by contrastive decoding, which encourages the\nteacher to generate tokens that become more plausible only when the answer is\nconsidered. To ensure faithful distillation, we use the teacher-generated\nrationales to learn a student LM with a counterfactual reasoning objective,\nwhich prevents the student from ignoring the rationales to make inconsistent\npredictions. Experiments show that, while yielding comparable end-task\nperformance, our method can generate CoT rationales that are more faithful than\nbaselines do. Further analysis suggests that such a model respects the\nrationales more when making decisions; thus, we can improve its performance\nmore by refining its rationales.",
        "pdf_link": "https://arxiv.org/pdf/2305.01879v4.pdf"
    },
    {
        "title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness",
        "authors": [
            "Yichuan Li",
            "Jialong Han",
            "Kyumin Lee",
            "Chengyuan Ma",
            "Benjamin Yao",
            "Derek Liu"
        ],
        "published": "2023-05-02T22:28:26Z",
        "summary": "In recent years, Pre-trained Language Models (PLMs) have shown their\nsuperiority by pre-training on unstructured text corpus and then fine-tuning on\ndownstream tasks. On entity-rich textual resources like Wikipedia,\nKnowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens\nand mentioned entities in pre-training, and are thus more effective on\nentity-centric tasks such as entity linking and relation classification.\nAlthough exploiting Wikipedia's rich structures to some extent, conventional\nKEPLMs still neglect a unique layout of the corpus where each Wikipedia page is\naround a topic entity (identified by the page URL and shown in the page title).\nIn this paper, we demonstrate that KEPLMs without incorporating the topic\nentities will lead to insufficient entity interaction and biased (relation)\nword semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained\nLanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET\nidentifies where to add the topic entity's information in a Wikipedia sentence,\nfuses such information into token and mentioned entities representations, and\nsupervises the network learning, through which it takes topic entities back\ninto consideration. Experiments demonstrated the generality and superiority of\nKEPLET which was applied to two representative KEPLMs, achieving significant\nimprovements on four entity-centric tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.01810v1.pdf"
    },
    {
        "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
        "authors": [
            "Yujie Lu",
            "Pan Lu",
            "Zhiyu Chen",
            "Wanrong Zhu",
            "Xin Eric Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-02T21:46:44Z",
        "summary": "Embodied agents have achieved prominent performance in following human\ninstructions to complete tasks. However, the potential of providing\ninstructions informed by texts and images to assist humans in completing tasks\nremains underexplored. To uncover this capability, we present the multimodal\nprocedural planning (MPP) task, in which models are given a high-level goal and\ngenerate plans of paired text-image steps, providing more complementary and\ninformative guidance than unimodal plans. The key challenges of MPP are to\nensure the informativeness, temporal coherence,and accuracy of plans across\nmodalities. To tackle this, we propose Text-Image Prompting (TIP), a\ndual-modality prompting method that jointly leverages zero-shot reasoning\nability in large language models (LLMs) and compelling text-to-image generation\nability from diffusion-based models. TIP improves the interaction in the dual\nmodalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs\nto guide the textual-grounded image plan generation and leveraging the\ndescriptions of image plans to ground the textual plan reversely. To address\nthe lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed\nfor MPP. Our results show compelling human preferences and automatic scores\nagainst unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms\nof informativeness, temporal coherence, and plan accuracy. Our code and data:\nhttps://github.com/YujieLu10/MPP.",
        "pdf_link": "https://arxiv.org/pdf/2305.01795v1.pdf"
    },
    {
        "title": "Automated Code generation for Information Technology Tasks in YAML through Large Language Models",
        "authors": [
            "Saurabh Pujar",
            "Luca Buratti",
            "Xiaojie Guo",
            "Nicolas Dupuis",
            "Burn Lewis",
            "Sahil Suneja",
            "Atin Sood",
            "Ganesh Nalawade",
            "Matthew Jones",
            "Alessandro Morari",
            "Ruchir Puri"
        ],
        "published": "2023-05-02T21:01:01Z",
        "summary": "The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models. In few-shot settings we asses the impact of\ntraining with Ansible, YAML data and compare with different baselines including\nCodex-Davinci-002. We also show that after finetuning, our Ansible specific\nmodel (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4)\nmodel, which was evaluated in few shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.02783v4.pdf"
    },
    {
        "title": "Few-shot In-context Learning for Knowledge Base Question Answering",
        "authors": [
            "Tianle Li",
            "Xueguang Ma",
            "Alex Zhuang",
            "Yu Gu",
            "Yu Su",
            "Wenhu Chen"
        ],
        "published": "2023-05-02T19:31:55Z",
        "summary": "Question answering over knowledge bases is considered a difficult problem due\nto the challenge of generalizing to a wide variety of possible natural language\nquestions. Additionally, the heterogeneity of knowledge base schema items\nbetween different knowledge bases often necessitates specialized training for\ndifferent knowledge base question-answering (KBQA) datasets. To handle\nquestions over diverse KBQA datasets with a unified training-free framework, we\npropose KB-BINDER, which for the first time enables few-shot in-context\nlearning over KBQA tasks. Firstly, KB-BINDER leverages large language models\nlike Codex to generate logical forms as the draft for a specific question by\nimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge\nbase to bind the generated draft to an executable one with BM25 score matching.\nThe experimental results on four public heterogeneous KBQA datasets show that\nKB-BINDER can achieve a strong performance with only a few in-context\ndemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even\noutperform the state-of-the-art trained models. On GrailQA and WebQSP, our\nmodel is also on par with other fully-trained models. We believe KB-BINDER can\nserve as an important baseline for future research. Our code is available at\nhttps://github.com/ltl3A87/KB-BINDER.",
        "pdf_link": "https://arxiv.org/pdf/2305.01750v2.pdf"
    },
    {
        "title": "Privacy-Preserving In-Context Learning for Large Language Models",
        "authors": [
            "Tong Wu",
            "Ashwinee Panda",
            "Jiachen T. Wang",
            "Prateek Mittal"
        ],
        "published": "2023-05-02T17:52:58Z",
        "summary": "In-context learning (ICL) is an important capability of Large Language Models\n(LLMs), enabling these models to dynamically adapt based on specific,\nin-context exemplars, thereby improving accuracy and relevance. However, LLM's\nresponses may leak the sensitive private information contained in in-context\nexemplars. To address this challenge, we propose Differentially Private\nIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The\nkey idea for DP-ICL paradigm is generating differentially private responses\nthrough a noisy consensus among an ensemble of LLM's responses based on\ndisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate\nseveral techniques showing how to privatize ICL for text classification and\nlanguage generation. We evaluate DP-ICL on four text classification benchmarks\nand two language generation tasks, and our empirical results show that DP-ICL\nachieves a strong utility-privacy tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2305.01639v2.pdf"
    },
    {
        "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "authors": [
            "Wes Gurnee",
            "Neel Nanda",
            "Matthew Pauly",
            "Katherine Harvey",
            "Dmitrii Troitskii",
            "Dimitris Bertsimas"
        ],
        "published": "2023-05-02T17:13:55Z",
        "summary": "Despite rapid adoption and deployment of large language models (LLMs), the\ninternal computations of these models remain opaque and poorly understood. In\nthis work, we seek to understand how high-level human-interpretable features\nare represented within the internal neuron activations of LLMs. We train\n$k$-sparse linear classifiers (probes) on these internal activations to predict\nthe presence of features in the input; by varying the value of $k$ we study the\nsparsity of learned representations and how this varies with model scale. With\n$k=1$, we localize individual neurons which are highly relevant for a\nparticular feature, and perform a number of case studies to illustrate general\nproperties of LLMs. In particular, we show that early layers make use of sparse\ncombinations of neurons to represent many features in superposition, that\nmiddle layers have seemingly dedicated neurons to represent higher-level\ncontextual features, and that increasing scale causes representational sparsity\nto increase on average, but there are multiple types of scaling dynamics. In\nall, we probe for over 100 unique features comprising 10 different categories\nin 7 different models spanning 70 million to 6.9 billion parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.01610v2.pdf"
    },
    {
        "title": "Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy",
        "authors": [
            "Aly M. Kassem"
        ],
        "published": "2023-05-02T15:53:28Z",
        "summary": "Large Language models (LLMs) are trained on large amounts of data, which can\ninclude sensitive information that may compromise personal privacy. LLMs showed\nto memorize parts of the training data and emit those data verbatim when an\nadversary prompts appropriately. Previous research has primarily focused on\ndata preprocessing and differential privacy techniques to address memorization\nor prevent verbatim memorization exclusively, which can give a false sense of\nprivacy. However, these methods rely on explicit and implicit assumptions about\nthe structure of the data to be protected, which often results in an incomplete\nsolution to the problem. To address this, we propose a novel framework that\nutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate\napproximate memorization. Our approach utilizes a negative similarity score,\nsuch as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity\npolicy. Our results demonstrate that this framework effectively mitigates\napproximate memorization while maintaining high levels of coherence and fluency\nin the generated samples. Furthermore, our framework is robust in mitigating\napproximate memorization across various circumstances, including longer\ncontext, which is known to increase memorization in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.01550v1.pdf"
    },
    {
        "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "authors": [
            "Andrew Zhu",
            "Karmanya Aggarwal",
            "Alexander Feng",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-02T15:36:10Z",
        "summary": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD&D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.01528v3.pdf"
    },
    {
        "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset",
        "authors": [
            "Jianquan Li",
            "Xidong Wang",
            "Xiangbo Wu",
            "Zhiyi Zhang",
            "Xiaolong Xu",
            "Jie Fu",
            "Prayag Tiwari",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "published": "2023-05-02T15:33:01Z",
        "summary": "In this paper, we release a largest ever medical Question Answering (QA)\ndataset with 26 million QA pairs. We benchmark many existing approaches in our\ndataset in terms of both retrieval and generation. Experimental results show\nthat the existing models perform far lower than expected and the released\ndataset is still challenging in the pre-trained language model era. Moreover,\nwe also experimentally show the benefit of the proposed dataset in many\naspects: (i) trained models for other QA datasets in a zero-shot fashion; and\n(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)\nimproving existing pre-trained language models by using the QA pairs as a\npre-training corpus in continued training manner. We believe that this dataset\nwill not only contribute to medical research but also facilitate both the\npatients and clinical doctors. See\n\\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",
        "pdf_link": "https://arxiv.org/pdf/2305.01526v1.pdf"
    },
    {
        "title": "VPGTrans: Transfer Visual Prompt Generator across LLMs",
        "authors": [
            "Ao Zhang",
            "Hao Fei",
            "Yuan Yao",
            "Wei Ji",
            "Li Li",
            "Zhiyuan Liu",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-02T09:28:39Z",
        "summary": "While developing a new multimodal LLM (MLLM) by pre-training on tremendous\nimage-text pairs from scratch can be exceedingly resource-consuming, connecting\nan existing LLM with a comparatively lightweight visual prompt generator (VPG)\nbecomes a feasible paradigm. However, further tuning the VPG part of the MLLM\nstill suffers from indispensable computational costs, i.e., requiring thousands\nof GPU hours and millions of training data. One alternative solution is to\ntransfer an existing VPG from any existing MLLMs for the target MLLM.\n  In this work, we for the first time investigate the VPG transferability\nacross LLMs, and explore a solution to reduce the cost of VPG transfer. We\nfirst study the VPG transfer across different LLM sizes (e.g., small-to-large),\nand across different LLM types, through which we diagnose the key factors to\nmaximize the transfer efficiency. Based on our observation, we design a\ntwo-stage transfer framework named VPGTrans, which is simple yet highly\neffective. Through extensive experiments, we demonstrate that VPGTrans helps\nsignificantly speed up the transfer learning process without compromising\nperformance. Remarkably, it helps achieve the VPG transfer from BLIP-2\nOPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and\n10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from\nscratch. Further, a series of intriguing findings and potential rationales\nbehind them are provided and discussed. Finally, we showcase the practical\nvalue of our VPGTrans approach, by customizing two novel MLLMs, including\nVL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.01278v2.pdf"
    },
    {
        "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
        "authors": [
            "Jiawei Liu",
            "Chunqiu Steven Xia",
            "Yuyao Wang",
            "Lingming Zhang"
        ],
        "published": "2023-05-02T05:46:48Z",
        "summary": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.",
        "pdf_link": "https://arxiv.org/pdf/2305.01210v3.pdf"
    },
    {
        "title": "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models",
        "authors": [
            "Chenyang Lyu",
            "Zefeng Du",
            "Jitao Xu",
            "Yitao Duan",
            "Minghao Wu",
            "Teresa Lynn",
            "Alham Fikri Aji",
            "Derek F. Wong",
            "Siyou Liu",
            "Longyue Wang"
        ],
        "published": "2023-05-02T03:27:27Z",
        "summary": "Machine Translation (MT) has greatly advanced over the years due to the\ndevelopments in deep neural networks. However, the emergence of Large Language\nModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT\ndomain. In this context, we believe that the future of MT is intricately tied\nto the capabilities of LLMs. These models not only offer vast linguistic\nunderstandings but also bring innovative methodologies, such as prompt-based\ntechniques, that have the potential to further elevate MT. In this paper, we\nprovide an overview of the significant enhancements in MT that are influenced\nby LLMs and advocate for their pivotal role in upcoming MT research and\nimplementations. We highlight several new MT directions, emphasizing the\nbenefits of LLMs in scenarios such as Long-Document Translation, Stylized\nTranslation, and Interactive Translation. Additionally, we address the\nimportant concern of privacy in LLM-driven MT and suggest essential\nprivacy-preserving strategies. By showcasing practical instances, we aim to\ndemonstrate the advantages that LLMs offer, particularly in tasks like\ntranslating extended documents. We conclude by emphasizing the critical role of\nLLMs in guiding the future evolution of MT and offer a roadmap for future\nexploration in the sector.",
        "pdf_link": "https://arxiv.org/pdf/2305.01181v3.pdf"
    },
    {
        "title": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
        "authors": [
            "Nurendra Choudhary",
            "Chandan K. Reddy"
        ],
        "published": "2023-05-02T02:21:49Z",
        "summary": "Reasoning over knowledge graphs (KGs) is a challenging task that requires a\ndeep understanding of the complex relationships between entities and the\nunderlying logic of their relations. Current approaches rely on learning\ngeometries to embed entities in vector space for logical query operations, but\nthey suffer from subpar performance on complex queries and dataset-specific\nrepresentations. In this paper, we propose a novel decoupled approach,\nLanguage-guided Abstract Reasoning over Knowledge graphs (LARK), that\nformulates complex KG reasoning as a combination of contextual KG search and\nlogical query reasoning, to leverage the strengths of graph extraction\nalgorithms and large language models (LLM), respectively. Our experiments\ndemonstrate that the proposed approach outperforms state-of-the-art KG\nreasoning methods on standard benchmark datasets across several logical query\nconstructs, with significant performance gain for queries of higher complexity.\nFurthermore, we show that the performance of our approach improves\nproportionally to the increase in size of the underlying LLM, enabling the\nintegration of the latest advancements in LLMs for logical reasoning over KGs.\nOur work presents a new direction for addressing the challenges of complex KG\nreasoning and paves the way for future research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2305.01157v3.pdf"
    },
    {
        "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models",
        "authors": [
            "Dave Van Veen",
            "Cara Van Uden",
            "Maayane Attias",
            "Anuj Pareek",
            "Christian Bluethgen",
            "Malgorzata Polacin",
            "Wah Chiu",
            "Jean-Benoit Delbrouck",
            "Juan Manuel Zambrano Chaves",
            "Curtis P. Langlotz",
            "Akshay S. Chaudhari",
            "John Pauly"
        ],
        "published": "2023-05-02T01:33:02Z",
        "summary": "We systematically investigate lightweight strategies to adapt large language\nmodels (LLMs) for the task of radiology report summarization (RRS).\nSpecifically, we focus on domain adaptation via pretraining (on natural\nlanguage, biomedical text, or clinical text) and via discrete prompting or\nparameter-efficient fine-tuning. Our results consistently achieve best\nperformance by maximally adapting to the task via pretraining on clinical text\nand fine-tuning on RRS examples. Importantly, this method fine-tunes a mere\n0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning\n(100% of parameters). Additionally, we study the effect of in-context examples\nand out-of-distribution (OOD) training before concluding with a radiologist\nreader study and qualitative analysis. Our findings highlight the importance of\ndomain adaptation in RRS and provide valuable insights toward developing\neffective natural language processing solutions for clinical tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.01146v3.pdf"
    },
    {
        "title": "Evaluating statistical language models as pragmatic reasoners",
        "authors": [
            "Benjamin Lipkin",
            "Lionel Wong",
            "Gabriel Grand",
            "Joshua B Tenenbaum"
        ],
        "published": "2023-05-01T18:22:10Z",
        "summary": "The relationship between communicated language and intended meaning is often\nprobabilistic and sensitive to context. Numerous strategies attempt to estimate\nsuch a mapping, often leveraging recursive Bayesian models of communication. In\nparallel, large language models (LLMs) have been increasingly applied to\nsemantic parsing applications, tasked with inferring logical representations\nfrom natural language. While existing LLM explorations have been largely\nrestricted to literal language use, in this work, we evaluate the capacity of\nLLMs to infer the meanings of pragmatic utterances. Specifically, we explore\nthe case of threshold estimation on the gradable adjective ``strong'',\ncontextually conditioned on a strength prior, then extended to composition with\nqualification, negation, polarity inversion, and class comparison. We find that\nLLMs can derive context-grounded, human-like distributions over the\ninterpretations of several complex pragmatic utterances, yet struggle composing\nwith negation. These results inform the inferential capacity of statistical\nlanguage models, and their use in pragmatic and semantic parsing applications.\nAll corresponding code is made publicly available\n(https://github.com/benlipkin/probsem/tree/CogSci2023).",
        "pdf_link": "https://arxiv.org/pdf/2305.01020v1.pdf"
    },
    {
        "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
        "authors": [
            "Patrick Fernandes",
            "Aman Madaan",
            "Emmy Liu",
            "Ant\u00f3nio Farinhas",
            "Pedro Henrique Martins",
            "Amanda Bertsch",
            "Jos\u00e9 G. C. de Souza",
            "Shuyan Zhou",
            "Tongshuang Wu",
            "Graham Neubig",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2023-05-01T17:36:06Z",
        "summary": "Many recent advances in natural language generation have been fueled by\ntraining large language models on internet-scale data. However, this paradigm\ncan lead to models that generate toxic, inaccurate, and unhelpful content, and\nautomatic evaluation metrics often fail to identify these behaviors. As models\nbecome more capable, human feedback is an invaluable signal for evaluating and\nimproving models. This survey aims to provide an overview of the recent\nresearch that has leveraged human feedback to improve natural language\ngeneration. First, we introduce an encompassing formalization of feedback, and\nidentify and organize existing research into a taxonomy following this\nformalization. Next, we discuss how feedback can be described by its format and\nobjective, and cover the two approaches proposed to use feedback (either for\ntraining or decoding): directly using the feedback or training feedback models.\nWe also discuss existing datasets for human-feedback data collection, and\nconcerns surrounding feedback collection. Finally, we provide an overview of\nthe nascent field of AI feedback, which exploits large language models to make\njudgments based on a set of principles and minimize the need for human\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2305.00955v2.pdf"
    },
    {
        "title": "Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs",
        "authors": [
            "Ga\u0161per Begu\u0161",
            "Maksymilian D\u0105bkowski",
            "Ryan Rhodes"
        ],
        "published": "2023-05-01T17:09:33Z",
        "summary": "The performance of large language models (LLMs) has recently improved to the\npoint where the models can perform well on many language tasks. We show here\nthat for the first time, the models can also generate coherent and valid formal\nanalyses of linguistic data and illustrate the vast potential of large language\nmodels for analyses of their metalinguistic abilities. LLMs are primarily\ntrained on language data in the form of text; analyzing and evaluating their\nmetalinguistic abilities improves our understanding of their general\ncapabilities and sheds new light on theoretical models in linguistics. In this\npaper, we probe into GPT-4's metalinguistic capabilities by focusing on three\nsubfields of formal linguistics: syntax, phonology, and semantics. We outline a\nresearch program for metalinguistic analyses of large language models, propose\nexperimental designs, provide general guidelines, discuss limitations, and\noffer future directions for this line of research. This line of inquiry also\nexemplifies behavioral interpretability of deep learning, where models'\nrepresentations are accessed by explicit prompting rather than internal\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2305.00948v2.pdf"
    },
    {
        "title": "Poisoning Language Models During Instruction Tuning",
        "authors": [
            "Alexander Wan",
            "Eric Wallace",
            "Sheng Shen",
            "Dan Klein"
        ],
        "published": "2023-05-01T16:57:33Z",
        "summary": "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on\ndatasets that contain user-submitted examples, e.g., FLAN aggregates numerous\nopen-source datasets and OpenAI leverages examples submitted in the browser\nplayground. In this work, we show that adversaries can contribute poison\nexamples to these datasets, allowing them to manipulate model predictions\nwhenever a desired trigger phrase appears in the input. For example, when a\ndownstream user provides an input that mentions \"Joe Biden\", a poisoned LM will\nstruggle to classify, summarize, edit, or translate that input. To construct\nthese poison examples, we optimize their inputs and outputs using a\nbag-of-words approximation to the LM. We evaluate our method on open-source\ninstruction-tuned LMs. By using as few as 100 poison examples, we can cause\narbitrary phrases to have consistent negative polarity or induce degenerate\noutputs across hundreds of held-out tasks. Worryingly, we also show that larger\nLMs are increasingly vulnerable to poisoning and that defenses based on data\nfiltering or reducing model capacity provide only moderate protections while\nreducing test accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.00944v1.pdf"
    },
    {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "published": "2023-05-01T14:02:48Z",
        "summary": "Large language models have been shown to struggle with multi-step reasoning,\nand do not retain previous reasoning steps for future use. We propose a simple\nmethod for solving both of these problems by allowing the model to take\nSelf-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model\ncan deviate from the input context at any time to explicitly think and write\ndown its thoughts. This allows the model to perform reasoning on the fly as it\nreads the context and even integrate previous reasoning steps, thus enhancing\nits memory with useful information and enabling multi-step reasoning.\nExperiments across a wide variety of tasks demonstrate that our method can\noutperform chain-of-thought and scratchpad methods by taking Self-Notes that\ninterleave the input text.",
        "pdf_link": "https://arxiv.org/pdf/2305.00833v2.pdf"
    },
    {
        "title": "Self-Evaluation Guided Beam Search for Reasoning",
        "authors": [
            "Yuxi Xie",
            "Kenji Kawaguchi",
            "Yiran Zhao",
            "Xu Zhao",
            "Min-Yen Kan",
            "Junxian He",
            "Qizhe Xie"
        ],
        "published": "2023-05-01T02:37:59Z",
        "summary": "Breaking down a problem into intermediate steps has demonstrated impressive\nperformance in Large Language Model (LLM) reasoning. However, the growth of the\nreasoning chain introduces uncertainty and error accumulation, making it\nchallenging to elicit accurate final results. To tackle this challenge of\nuncertainty in multi-step reasoning, we introduce a stepwise self-evaluation\nmechanism to guide and calibrate the reasoning process of LLMs. We propose a\ndecoding algorithm integrating the self-evaluation guidance via stochastic beam\nsearch. The self-evaluation guidance serves as a better-calibrated automatic\ncriterion, facilitating an efficient search in the reasoning space and\nresulting in superior prediction quality. Stochastic beam search balances\nexploitation and exploration of the search space with temperature-controlled\nrandomness. Our approach surpasses the corresponding Codex-backboned baselines\nin few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA,\nand StrategyQA benchmarks, respectively. Experiment results with Llama-2 on\narithmetic reasoning demonstrate the efficiency of our method in outperforming\nthe baseline methods with comparable computational budgets. Further analysis in\nmulti-step reasoning finds our self-evaluation guidance pinpoints logic\nfailures and leads to higher consistency and robustness. Our code is publicly\navailable at https://guideddecoding.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.00633v3.pdf"
    },
    {
        "title": "Using Large Language Models to Generate JUnit Tests: An Empirical Study",
        "authors": [
            "Mohammed Latif Siddiq",
            "Joanna C. S. Santos",
            "Ridwanul Hasan Tanvir",
            "Noshin Ulfat",
            "Fahmid Al Rifat",
            "Vinicius Carvalho Lopes"
        ],
        "published": "2023-04-30T07:28:06Z",
        "summary": "A code generation model generates code by taking a prompt from a code\ncomment, existing code, or a combination of both. Although code generation\nmodels (e.g., GitHub Copilot) are increasingly being adopted in practice, it is\nunclear whether they can successfully be used for unit test generation without\nfine-tuning for a strongly typed language like Java. To fill this gap, we\ninvestigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can\ngenerate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to\ninvestigate the effect of context generation on the unit test generation\nprocess. We evaluated the models based on compilation rates, test correctness,\ntest coverage, and test smells. We found that the Codex model achieved above\n80% coverage for the HumanEval dataset, but no model had more than 2% coverage\nfor the EvoSuite SF110 benchmark. The generated tests also suffered from test\nsmells, such as Duplicated Asserts and Empty Tests.",
        "pdf_link": "https://arxiv.org/pdf/2305.00418v4.pdf"
    },
    {
        "title": "Beyond Classification: Financial Reasoning in State-of-the-Art Language Models",
        "authors": [
            "Guijin Son",
            "Hanearl Jung",
            "Moonjeong Hahm",
            "Keonju Na",
            "Sol Jin"
        ],
        "published": "2023-04-30T04:36:05Z",
        "summary": "Large Language Models (LLMs), consisting of 100 billion or more parameters,\nhave demonstrated remarkable ability in complex multi-step reasoning tasks.\nHowever, the application of such generic advancements has been limited to a few\nfields, such as clinical or legal, with the field of financial reasoning\nremaining largely unexplored. To the best of our knowledge, the ability of LLMs\nto solve financial reasoning problems has never been dealt with, and whether it\ncan be performed at any scale remains unknown. To address this knowledge gap,\nthis research presents a comprehensive investigation into the potential\napplication of LLMs in the financial domain. The investigation includes a\ndetailed exploration of a range of subjects, including task formulation,\nsynthetic data generation, prompting methods, and evaluation capability.\nFurthermore, the study benchmarks various GPT variants with parameter scales\nranging from 2.8B to 13B, with and without instruction tuning, on diverse\ndataset sizes. By analyzing the results, we reveal that the ability to generate\ncoherent financial reasoning first emerges at 6B parameters, and continues to\nimprove with better instruction-tuning or larger datasets. Additionally, the\nstudy provides a publicly accessible dataset named sFIOG (Synthetic-Financial\nInvestment Opinion Generation), consisting of 11,802 synthetic investment\nthesis samples, to support further research in the field of financial\nreasoning. Overall, this research seeks to contribute to the understanding of\nthe efficacy of language models in the field of finance, with a particular\nemphasis on their ability to engage in sophisticated reasoning and analysis\nwithin the context of investment decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2305.01505v2.pdf"
    },
    {
        "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
        "authors": [
            "Emre K\u0131c\u0131man",
            "Robert Ness",
            "Amit Sharma",
            "Chenhao Tan"
        ],
        "published": "2023-04-28T19:00:43Z",
        "summary": "The causal capabilities of large language models (LLMs) is a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nfurther our understanding of LLMs and their causal implications, considering\nthe distinctions between different types of causal reasoning tasks, as well as\nthe entangled threats of construct and measurement validity. LLM-based methods\nestablish new state-of-the-art accuracies on multiple causal benchmarks.\nAlgorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise\ncausal discovery task (97%, 13 points gain), counterfactual reasoning task\n(92%, 20 points gain), and actual causality (86% accuracy in determining\nnecessary and sufficient causes in vignettes). At the same time, LLMs exhibit\nunpredictable failure modes and we provide some techniques to interpret their\nrobustness.\n  Crucially, LLMs perform these causal tasks while relying on sources of\nknowledge and methods distinct from and complementary to non-LLM based\napproaches. Specifically, LLMs bring capabilities so far understood to be\nrestricted to humans, such as using collected knowledge to generate causal\ngraphs or identifying background causal context from natural language. We\nenvision LLMs to be used alongside existing causal methods, as a proxy for\nhuman domain knowledge and to reduce human effort in setting up a causal\nanalysis, one of the biggest impediments to the widespread adoption of causal\nmethods. We also see existing causal methods as promising tools for LLMs to\nformalize, validate, and communicate their reasoning especially in high-stakes\nscenarios.\n  In capturing common sense and domain knowledge about causal mechanisms and\nsupporting translation between natural language and formal methods, LLMs open\nnew frontiers for advancing the research, practice, and adoption of causality.",
        "pdf_link": "https://arxiv.org/pdf/2305.00050v2.pdf"
    },
    {
        "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
        "authors": [
            "Peng Gao",
            "Jiaming Han",
            "Renrui Zhang",
            "Ziyi Lin",
            "Shijie Geng",
            "Aojun Zhou",
            "Wei Zhang",
            "Pan Lu",
            "Conghui He",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Yu Qiao"
        ],
        "published": "2023-04-28T17:59:25Z",
        "summary": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
        "pdf_link": "https://arxiv.org/pdf/2304.15010v1.pdf"
    },
    {
        "title": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs",
        "authors": [
            "George Pu",
            "Anirudh Jain",
            "Jihan Yin",
            "Russell Kaplan"
        ],
        "published": "2023-04-28T17:39:49Z",
        "summary": "As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.",
        "pdf_link": "https://arxiv.org/pdf/2304.14999v1.pdf"
    },
    {
        "title": "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",
        "authors": [
            "Chunkit Chan",
            "Jiayang Cheng",
            "Weiqi Wang",
            "Yuxin Jiang",
            "Tianqing Fang",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023-04-28T13:14:36Z",
        "summary": "This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.",
        "pdf_link": "https://arxiv.org/pdf/2304.14827v3.pdf"
    },
    {
        "title": "Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?",
        "authors": [
            "Sonal Sannigrahi",
            "Josef van Genabith",
            "Cristina Espana-Bonet"
        ],
        "published": "2023-04-28T12:11:21Z",
        "summary": "Dense vector representations for textual data are crucial in modern NLP. Word\nembeddings and sentence embeddings estimated from raw texts are key in\nachieving state-of-the-art results in various tasks requiring semantic\nunderstanding. However, obtaining embeddings at the document level is\nchallenging due to computational requirements and lack of appropriate data.\nInstead, most approaches fall back on computing document embeddings based on\nsentence representations. Although there exist architectures and models to\nencode documents fully, they are in general limited to English and few other\nhigh-resourced languages. In this work, we provide a systematic comparison of\nmethods to produce document-level representations from sentences based on\nLASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare\ninput token number truncation, sentence averaging as well as some simple\nwindowing and in some cases new augmented and learnable approaches, on 3 multi-\nand cross-lingual tasks in 8 languages belonging to 3 different language\nfamilies. Our task-based extrinsic evaluations show that, independently of the\nlanguage, a clever combination of sentence embeddings is usually better than\nencoding the full document as a single unit, even when this is possible. We\ndemonstrate that while a simple sentence average results in a strong baseline\nfor classification tasks, more complex combinations are necessary for semantic\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14796v1.pdf"
    },
    {
        "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-Seng Chua"
        ],
        "published": "2023-04-28T10:15:25Z",
        "summary": "Making the content generated by Large Language Model (LLM), accurate,\ncredible and traceable is crucial, especially in complex knowledge-intensive\ntasks that require multi-step reasoning and each step needs knowledge to solve.\nRetrieval-augmented generation is good potential to solve this problem.\nHowever, where and how to introduce Information Retrieval (IR) to LLM is a big\nchallenge. Previous work has the problems that wrong knowledge retrieved by IR\nmisleads the LLM and interaction between IR and LLM breaks the reasoning chain\nof LLM. This paper proposes a novel framework named\n\\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the reasoning chain named\nChain-of-Query (CoQ) where each node consists of an IR-oriented query-answer\npair. Second, IR verifies the answer of each node of CoQ. It corrects the\nanswer that is not consistent with the retrieved information when IR gives high\nconfidence, which improves the credibility. Third, LLM can indicate its missing\nknowledge in CoQ and rely on IR to provide this knowledge to LLM. These\noperations improve the accuracy in terms of reasoning and knowledge. Finally,\nSearChain generates the reasoning process and marks references to supporting\ndocuments for each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a tree, which\nenables LLM to dynamically modify the direction of reasoning. Experiments show\nthat SearChain outperforms state-of-the-art baselines on complex\nknowledge-intensive tasks including multi-hop Q\\&A, slot filling, fact\nchecking, and long-form Q\\&A.",
        "pdf_link": "https://arxiv.org/pdf/2304.14732v7.pdf"
    },
    {
        "title": "Towards autonomous system: flexible modular production system enhanced with large language model agents",
        "authors": [
            "Yuchen Xia",
            "Manthan Shenoy",
            "Nasser Jazdi",
            "Michael Weyrich"
        ],
        "published": "2023-04-28T09:42:18Z",
        "summary": "In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. We retrofit the\nautomation system for a modular production facility and create executable\ncontrol interfaces of fine-granular functionalities and coarse-granular skills.\nLow-level functionalities are executed by automation components, and high-level\nskills are performed by automation modules. Subsequently, a digital twin system\nis developed, registering these interfaces and containing additional\ndescriptive information about the production system. Based on the retrofitted\nautomation system and the created digital twins, LLM-agents are designed to\ninterpret descriptive information in the digital twins and control the physical\nsystem through service interfaces. These LLM-agents serve as intelligent agents\non different levels within an automation system, enabling autonomous planning\nand control of flexible production. Given a task instruction as input, the\nLLM-agents orchestrate a sequence of atomic functionalities and skills to\naccomplish the task. We demonstrate how our implemented prototype can handle\nun-predefined tasks, plan a production process, and execute the operations.\nThis research highlights the potential of integrating LLMs into industrial\nautomation systems in the context of smart factory for more agile, flexible,\nand adaptive production processes, while it also underscores the critical\ninsights and limitations for future work. Demos at:\nhttps://github.com/YuchenXia/GPT4IndustrialAutomation",
        "pdf_link": "https://arxiv.org/pdf/2304.14721v4.pdf"
    },
    {
        "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine",
        "authors": [
            "Chaoyi Wu",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "published": "2023-04-27T18:29:05Z",
        "summary": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities\nin natural language understanding. While demonstrating proficiency in everyday\nconversations and question-answering situations, these models frequently\nstruggle in domains that require precision, such as medical applications, due\nto their lack of domain-specific knowledge. In this paper, we describe the\nprocedure for building a powerful, open-source language model specifically\ndesigned for medicine applications, termed as PMC-LLaMA. Our contributions are\nthreefold: (i) we systematically investigate the process of adapting a\ngeneral-purpose foundation language model towards medical domain, this involves\ndata-centric knowledge injection through the integration of 4.8M biomedical\nacademic papers and 30K medical textbooks, as well as comprehensive fine-tuning\nfor alignment with domain-specific instructions; (ii) we contribute a\nlarge-scale, comprehensive dataset for instruction tuning. This dataset\nencompasses medical question-answering (QA), rationale for reasoning, and\nconversational dialogues, comprising a total of 202M tokens; (iii) we conduct\nthorough ablation studies to demonstrate the effectiveness of each proposed\ncomponent. While evaluating on various public medical question-answering\nbenchmarks, our lightweight PMCLLaMA, which consists of only 13 billion\nparameters, exhibits superior performance, even surpassing ChatGPT. All models,\ncodes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.",
        "pdf_link": "https://arxiv.org/pdf/2304.14454v3.pdf"
    },
    {
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
        "authors": [
            "Minghao Wu",
            "Abdul Waheed",
            "Chiyu Zhang",
            "Muhammad Abdul-Mageed",
            "Alham Fikri Aji"
        ],
        "published": "2023-04-27T17:58:49Z",
        "summary": "Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being much smaller in size.",
        "pdf_link": "https://arxiv.org/pdf/2304.14402v3.pdf"
    },
    {
        "title": "IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers",
        "authors": [
            "Ronghuan Wu",
            "Wanchao Su",
            "Kede Ma",
            "Jing Liao"
        ],
        "published": "2023-04-27T17:58:02Z",
        "summary": "Scalable Vector Graphics (SVG) is a popular vector image format that offers\ngood support for interactivity and animation. Despite its appealing\ncharacteristics, creating custom SVG content can be challenging for users due\nto the steep learning curve required to understand SVG grammars or get familiar\nwith professional editing software. Recent advancements in text-to-image\ngeneration have inspired researchers to explore vector graphics synthesis using\neither image-based methods (i.e., text -> raster image -> vector graphics)\ncombining text-to-image generation models with image vectorization, or\nlanguage-based methods (i.e., text -> vector graphics script) through\npretrained large language models. However, these methods still suffer from\nlimitations in terms of generation quality, diversity, and flexibility. In this\npaper, we introduce IconShop, a text-guided vector icon synthesis method using\nautoregressive transformers. The key to success of our approach is to\nsequentialize and tokenize SVG paths (and textual descriptions as guidance)\ninto a uniquely decodable token sequence. With that, we are able to fully\nexploit the sequence learning power of autoregressive transformers, while\nenabling both unconditional and text-conditioned icon synthesis. Through\nstandard training to predict the next token on a large-scale vector icon\ndataset accompanied by textural descriptions, the proposed IconShop\nconsistently exhibits better icon synthesis capability than existing\nimage-based and language-based methods both quantitatively and qualitatively.\nMeanwhile, we observe a dramatic improvement in generation diversity, which is\nvalidated by the objective Uniqueness and Novelty measures. More importantly,\nwe demonstrate the flexibility of IconShop with multiple novel icon synthesis\ntasks, including icon editing, icon interpolation, icon semantic combination,\nand icon design auto-suggestion.",
        "pdf_link": "https://arxiv.org/pdf/2304.14400v4.pdf"
    },
    {
        "title": "We're Afraid Language Models Aren't Modeling Ambiguity",
        "authors": [
            "Alisa Liu",
            "Zhaofeng Wu",
            "Julian Michael",
            "Alane Suhr",
            "Peter West",
            "Alexander Koller",
            "Swabha Swayamdipta",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "published": "2023-04-27T17:57:58Z",
        "summary": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is\na key part of human language understanding, allowing us to anticipate\nmisunderstanding as communicators and revise our interpretations as listeners.\nAs language models (LMs) are increasingly employed as dialogue interfaces and\nwriting aids, handling ambiguous language is critical to their success. We\ncharacterize ambiguity in a sentence by its effect on entailment relations with\nanother sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645\nexamples with diverse kinds of ambiguity. We design a suite of tests based on\nAmbiEnt, presenting the first evaluation of pretrained LMs to recognize\nambiguity and disentangle possible meanings. We find that the task remains\nextremely challenging, including for GPT-4, whose generated disambiguations are\nconsidered correct only 32% of the time in human evaluation, compared to 90%\nfor disambiguations in our dataset. Finally, to illustrate the value of\nambiguity-sensitive tools, we show that a multilabel NLI model can flag\npolitical claims in the wild that are misleading due to ambiguity. We encourage\nthe field to rediscover the importance of ambiguity for NLP.",
        "pdf_link": "https://arxiv.org/pdf/2304.14399v2.pdf"
    },
    {
        "title": "CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants",
        "authors": [
            "Albert Yu Sun",
            "Varun Nair",
            "Elliot Schumacher",
            "Anitha Kannan"
        ],
        "published": "2023-04-27T17:39:11Z",
        "summary": "A wave of new task-based virtual assistants has been fueled by increasingly\npowerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major\nchallenge in deploying LLM-based virtual conversational assistants in real\nworld settings is ensuring they operate within what is admissible for the task.\nTo overcome this challenge, the designers of these virtual assistants rely on\nan independent guardrail system that verifies the virtual assistant's output\naligns with the constraints required for the task. However, relying on commonly\nused, prompt-based guardrails can be difficult to engineer correctly and\ncomprehensively. To address these challenges, we propose CONSCENDI. We use\nCONSCENDI to exhaustively generate training data with two key LLM-powered\ncomponents: scenario-augmented generation and contrastive training examples.\nWhen generating conversational data, we generate a set of rule-breaking\nscenarios, which enumerate a diverse set of high-level ways a rule can be\nviolated. This scenario-guided approach produces a diverse training set and\nprovides chatbot designers greater control. To generate contrastive examples,\nwe prompt the LLM to alter conversations with violations into acceptable\nconversations to enable fine-grained distinctions. We then use this data,\ngenerated by CONSCENDI, to train a smaller model. We find that CONSCENDI\nresults in guardrail models that improve over baselines in multiple dialogue\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2304.14364v2.pdf"
    },
    {
        "title": "Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems",
        "authors": [
            "Oluwatosin Ogundare",
            "Srinath Madasu",
            "Nathanial Wiggins"
        ],
        "published": "2023-04-27T17:33:49Z",
        "summary": "Large Language Models (LLMs) have shown great potential in solving complex\nproblems in various fields, including oil and gas engineering and other\nindustrial engineering disciplines like factory automation, PLC programming\netc. However, automatic identification of strong and weak solutions to\nfundamental physics equations governing several industrial processes remain a\nchallenging task. This paper identifies the limitation of current LLM\napproaches, particularly ChatGPT in selected practical problems native to oil\nand gas engineering but not exclusively. The performance of ChatGPT in solving\ncomplex problems in oil and gas engineering is discussed and the areas where\nLLMs are most effective are presented.",
        "pdf_link": "https://arxiv.org/pdf/2304.14354v1.pdf"
    },
    {
        "title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
        "authors": [
            "Terry Yue Zhuo"
        ],
        "published": "2023-04-27T16:38:17Z",
        "summary": "Recent advancements in the field of natural language generation have\nfacilitated the use of large language models to assess the quality of generated\ntext. Although these models have shown promising results in tasks such as\nmachine translation and summarization, their applicability in code intelligence\ntasks remains limited without human involvement. The complexity of programming\nconcepts required for such tasks makes it difficult to develop evaluation\nmetrics that align with human judgment. Token-matching-based metrics, such as\nBLEU, have demonstrated weak correlations with human practitioners in code\nintelligence tasks. Moreover, utilizing human-written test suites to evaluate\nfunctional correctness can be challenging in domains with low resources. To\novercome these obstacles, we propose \\texttt{ICE-Score}, a new evaluation\nmetric via instructing large language models (LLMs) for code assessments. Our\nmetric addresses the limitations of existing approaches by achieving superior\ncorrelations with functional correctness and human preferences, without the\nneed for test oracles or references. We evaluate the efficacy of our metric on\ntwo different aspects (\\textit{human preference} and \\textit{execution\nsuccess}) and four programming languages. Our results demonstrate that our\nmetric surpasses state-of-the-art metrics for code generation, delivering high\nlevels of accuracy and consistency across various programming languages and\ntasks. We also make our evaluation metric and datasets available to the\npublic\\footnote{\\url{https://github.com/terryyz/ice-score}}, encouraging\nfurther research in evaluating code intelligence tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14317v2.pdf"
    },
    {
        "title": "Controlled Text Generation with Natural Language Instructions",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Ethan Wilcox",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "published": "2023-04-27T15:56:34Z",
        "summary": "Large language models generate fluent texts and can follow natural language\ninstructions to solve a wide range of tasks without task-specific training.\nNevertheless, it is notoriously difficult to control their generation to\nsatisfy the various constraints required by different applications. In this\nwork, we present InstructCTG, a controlled text generation framework that\nincorporates different constraints by conditioning on natural language\ndescriptions and demonstrations of the constraints. In particular, we first\nextract the underlying constraints of natural texts through a combination of\noff-the-shelf NLP tools and simple heuristics. We then verbalize the\nconstraints into natural language instructions to form weakly supervised\ntraining data. By prepending natural language descriptions of the constraints\nand a few demonstrations, we fine-tune a pre-trained language model to\nincorporate various types of constraints. Compared to existing search-based or\nscore-based methods, InstructCTG is more flexible to different constraint types\nand has a much smaller impact on the generation quality and speed because it\ndoes not modify the decoding procedure. Additionally, InstructCTG allows the\nmodel to adapt to new constraints without re-training through the use of\nfew-shot task generalization and in-context learning abilities of\ninstruction-tuned language models.",
        "pdf_link": "https://arxiv.org/pdf/2304.14293v2.pdf"
    },
    {
        "title": "Origin Tracing and Detecting of LLMs",
        "authors": [
            "Linyang Li",
            "Pengyu Wang",
            "Ke Ren",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "published": "2023-04-27T10:05:57Z",
        "summary": "The extraordinary performance of large language models (LLMs) heightens the\nimportance of detecting whether the context is generated by an AI system. More\nimportantly, while more and more companies and institutions release their LLMs,\nthe origin can be hard to trace. Since LLMs are heading towards the time of\nAGI, similar to the origin tracing in anthropology, it is of great importance\nto trace the origin of LLMs. In this paper, we first raise the concern of the\norigin tracing of LLMs and propose an effective method to trace and detect\nAI-generated contexts. We introduce a novel algorithm that leverages the\ncontrastive features between LLMs and extracts model-wise features to trace the\ntext origins. Our proposed method works under both white-box and black-box\nsettings therefore can be widely generalized to detect various LLMs.(e.g. can\nbe generalized to detect GPT-3 models without the GPT-3 models). Also, our\nproposed method requires only limited data compared with the supervised\nlearning methods and can be extended to trace new-coming model origins. We\nconstruct extensive experiments to examine whether we can trace the origins of\ngiven texts. We provide valuable observations based on the experimental\nresults, such as the difficulty level of AI origin tracing, and the AI origin\nsimilarities, and call for ethical concerns of LLM providers. We are releasing\nall codes and data as a toolkit and benchmark for future AI origin tracing and\ndetecting studies. \\footnote{We are releasing all available resource at\n\\url{https://github.com/OpenLMLab/}.}",
        "pdf_link": "https://arxiv.org/pdf/2304.14072v1.pdf"
    },
    {
        "title": "Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering",
        "authors": [
            "Xiangyang Liu",
            "Tianqi Pang",
            "Chenyou Fan"
        ],
        "published": "2023-04-27T01:48:03Z",
        "summary": "We investigate how to enhance answer precision in frequently asked questions\nposed by distributed users using cloud-based Large Language Models (LLMs). Our\nstudy focuses on a typical situations where users ask similar queries that\ninvolve identical mathematical reasoning steps and problem-solving procedures.\nDue to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone\nquestions, we propose to improve the distributed synonymous questions using\nSelf-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we\nfirst retrieve synonymous questions from a crowd-sourced database and create a\nfederated question pool. We call these federated synonymous questions with the\nsame or different parameters SP-questions or DP-questions, respectively. We\nrefer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate\nsignificantly more accurate answers for all user queries without requiring\nsophisticated model-tuning. Through extensive experiments, we demonstrate that\nour proposed methods can significantly enhance question accuracy by fully\nexploring the synonymous nature of the questions and the consistency of the\nanswers.",
        "pdf_link": "https://arxiv.org/pdf/2304.13911v2.pdf"
    },
    {
        "title": "The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks",
        "authors": [
            "Anders Giovanni M\u00f8ller",
            "Jacob Aarup Dalsgaard",
            "Arianna Pera",
            "Luca Maria Aiello"
        ],
        "published": "2023-04-26T23:09:02Z",
        "summary": "In the realm of Computational Social Science (CSS), practitioners often\nnavigate complex, low-resource domains and face the costly and time-intensive\nchallenges of acquiring and annotating data. We aim to establish a set of\nguidelines to address such challenges, comparing the use of human-labeled data\nwith synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS\nclassification tasks of varying complexity. Additionally, we examine the impact\nof training data sizes on performance. Our findings reveal that models trained\non human-labeled data consistently exhibit superior or comparable performance\ncompared to their synthetically augmented counterparts. Nevertheless, synthetic\naugmentation proves beneficial, particularly in improving performance on rare\nclasses within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2\nfor zero-shot classification and find that, while they generally display strong\nperformance, they often fall short when compared to specialized classifiers\ntrained on moderately sized training sets.",
        "pdf_link": "https://arxiv.org/pdf/2304.13861v2.pdf"
    },
    {
        "title": "Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery",
        "authors": [
            "Debadutta Dash",
            "Rahul Thapa",
            "Juan M. Banda",
            "Akshay Swaminathan",
            "Morgan Cheatham",
            "Mehr Kashyap",
            "Nikesh Kotecha",
            "Jonathan H. Chen",
            "Saurabh Gombar",
            "Lance Downing",
            "Rachel Pedreira",
            "Ethan Goh",
            "Angel Arnaout",
            "Garret Kenn Morris",
            "Honor Magon",
            "Matthew P Lungren",
            "Eric Horvitz",
            "Nigam H. Shah"
        ],
        "published": "2023-04-26T17:54:28Z",
        "summary": "Despite growing interest in using large language models (LLMs) in healthcare,\ncurrent explorations do not assess the real-world utility and safety of LLMs in\nclinical settings. Our objective was to determine whether two LLMs can serve\ninformation needs submitted by physicians as questions to an informatics\nconsultation service in a safe and concordant manner. Sixty six questions from\nan informatics consult service were submitted to GPT-3.5 and GPT-4 via simple\nprompts. 12 physicians assessed the LLM responses' possibility of patient harm\nand concordance with existing reports from an informatics consultation service.\nPhysician assessments were summarized based on majority vote. For no questions\ndid a majority of physicians deem either LLM response as harmful. For GPT-3.5,\nresponses to 8 questions were concordant with the informatics consult report,\n20 discordant, and 9 were unable to be assessed. There were 29 responses with\nno majority on \"Agree\", \"Disagree\", and \"Unable to assess\". For GPT-4,\nresponses to 13 questions were concordant, 15 discordant, and 3 were unable to\nbe assessed. There were 35 responses with no majority. Responses from both LLMs\nwere largely devoid of overt harm, but less than 20% of the responses agreed\nwith an answer from an informatics consultation service, responses contained\nhallucinated references, and physicians were divided on what constitutes harm.\nThese results suggest that while general purpose LLMs are able to provide safe\nand credible responses, they often do not meet the specific information need of\na given question. A definitive evaluation of the usefulness of LLMs in\nhealthcare settings will likely require additional research on prompt\nengineering, calibration, and custom-tailoring of general purpose models.",
        "pdf_link": "https://arxiv.org/pdf/2304.13714v3.pdf"
    },
    {
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
        "authors": [
            "Jingfeng Yang",
            "Hongye Jin",
            "Ruixiang Tang",
            "Xiaotian Han",
            "Qizhang Feng",
            "Haoming Jiang",
            "Bing Yin",
            "Xia Hu"
        ],
        "published": "2023-04-26T17:52:30Z",
        "summary": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",
        "pdf_link": "https://arxiv.org/pdf/2304.13712v2.pdf"
    },
    {
        "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables",
        "authors": [
            "Matthias Urban",
            "Carsten Binnig"
        ],
        "published": "2023-04-26T13:31:04Z",
        "summary": "In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class\nof database systems that can seamlessly query text and tables using SQL. To\nenable seamless querying of textual data using SQL in an MMDB, we propose to\nextend relational databases with so-called multi-modal operators (MMOps) which\nare based on the advances of recent large language models such as GPT-3. The\nmain idea of MMOps is that they allow text collections to be treated as tables\nwithout the need to manually transform the data. As we show in our evaluation,\nour MMDB prototype can not only outperform state-of-the-art approaches such as\ntext-to-table in terms of accuracy and performance but it also requires\nsignificantly less training data to fine-tune the model for an unseen text\ncollection.",
        "pdf_link": "https://arxiv.org/pdf/2304.13559v2.pdf"
    },
    {
        "title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
        "authors": [
            "Bing Wang",
            "Xinnian Liang",
            "Jian Yang",
            "Hui Huang",
            "Shuangzhi Wu",
            "Peihao Wu",
            "Lu Lu",
            "Zejun Ma",
            "Zhoujun Li"
        ],
        "published": "2023-04-26T07:25:31Z",
        "summary": "Large Language Models (LLMs) are constrained by their inability to process\nlengthy inputs, resulting in the loss of critical historical information. To\naddress this limitation, in this paper, we propose the Self-Controlled Memory\n(SCM) framework to enhance the ability of LLMs to maintain long-term memory and\nrecall relevant information. Our SCM framework comprises three key components:\nan LLM-based agent serving as the backbone of the framework, a memory stream\nstoring agent memories, and a memory controller updating memories and\ndetermining when and how to utilize memories from memory stream. Additionally,\nthe proposed SCM is able to process ultra-long texts without any modification\nor fine-tuning, which can integrate with any instruction following LLMs in a\nplug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the\neffectiveness of SCM for handling lengthy inputs. The annotated dataset covers\nthree tasks: long-term dialogues, book summarization, and meeting\nsummarization. Experimental results demonstrate that our method achieves better\nretrieval recall and generates more informative responses compared to\ncompetitive baselines in long-term dialogues.\n(https://github.com/wbbeyourself/SCM4LLMs)",
        "pdf_link": "https://arxiv.org/pdf/2304.13343v2.pdf"
    },
    {
        "title": "Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval",
        "authors": [
            "Chunxi Guo",
            "Zhiliang Tian",
            "Jintao Tang",
            "Pancheng Wang",
            "Zhihua Wen",
            "Kang Yang",
            "Ting Wang"
        ],
        "published": "2023-04-26T06:02:01Z",
        "summary": "Text-to-SQL is a task that converts a natural language question into a\nstructured query language (SQL) to retrieve information from a database. Large\nlanguage models (LLMs) work well in natural language generation tasks, but they\nare not specifically pre-trained to understand the syntax and semantics of SQL\ncommands. In this paper, we propose an LLM-based framework for Text-to-SQL\nwhich retrieves helpful demonstration examples to prompt LLMs. However,\nquestions with different database schemes can vary widely, even if the\nintentions behind them are similar and the corresponding SQL queries exhibit\nsimilarities. Consequently, it becomes crucial to identify the appropriate SQL\ndemonstrations that align with our requirements. We design a de-semanticization\nmechanism that extracts question skeletons, allowing us to retrieve similar\nexamples based on their structural similarity. We also model the relationships\nbetween question tokens and database schema items (i.e., tables and columns) to\nfilter out scheme-related information. Our framework adapts the range of the\ndatabase schema in prompts to balance length and valuable information. A\nfallback mechanism allows for a more detailed schema to be provided if the\ngenerated SQL query fails. Ours outperforms state-of-the-art models and\ndemonstrates strong generalization ability on three cross-domain Text-to-SQL\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2304.13301v2.pdf"
    },
    {
        "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
        "authors": [
            "Shuai Li",
            "Zhao Song",
            "Yu Xia",
            "Tong Yu",
            "Tianyi Zhou"
        ],
        "published": "2023-04-26T04:33:41Z",
        "summary": "Large language models (LLMs) are known for their exceptional performance in\nnatural language processing, making them highly effective in many human\nlife-related or even job-related tasks. The attention mechanism in the\nTransformer architecture is a critical component of LLMs, as it allows the\nmodel to selectively focus on specific input parts. The softmax unit, which is\na key part of the attention mechanism, normalizes the attention scores. Hence,\nthe performance of LLMs in various NLP tasks depends significantly on the\ncrucial role played by the attention mechanism with the softmax unit.\n  In-context learning, as one of the celebrated abilities of recent LLMs, is an\nimportant concept in querying LLMs such as ChatGPT. Without further parameter\nupdates, Transformers can learn to predict based on few in-context examples.\nHowever, the reason why Transformers becomes in-context learners is not well\nunderstood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the\nin-context learning from a mathematical perspective based on a linear\nregression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers'\ncapability of learning linear functions in context.\n  In this work, we study the in-context learning based on a softmax regression\nformulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b\n\\|_2$ of Transformer's attention mechanism. We show the upper bounds of the\ndata transformations induced by a single self-attention layer and by\ngradient-descent on a $\\ell_2$ regression loss for softmax prediction function,\nwhich imply that when training self-attention-only Transformers for fundamental\nregression tasks, the models learned by gradient-descent and Transformers show\ngreat similarity.",
        "pdf_link": "https://arxiv.org/pdf/2304.13276v1.pdf"
    },
    {
        "title": "The Internal State of an LLM Knows When It's Lying",
        "authors": [
            "Amos Azaria",
            "Tom Mitchell"
        ],
        "published": "2023-04-26T02:49:38Z",
        "summary": "While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, one of their most prominent drawbacks is generating inaccurate\nor false information with a confident tone. In this paper, we provide evidence\nthat the LLM's internal state can be used to reveal the truthfulness of\nstatements. This includes both statements provided to the LLM, and statements\nthat the LLM itself generates. Our approach is to train a classifier that\noutputs the probability that a statement is truthful, based on the hidden layer\nactivations of the LLM as it reads or generates the statement. Experiments\ndemonstrate that given a set of test sentences, of which half are true and half\nfalse, our trained classifier achieves an average of 71\\% to 83\\% accuracy\nlabeling which sentences are true versus false, depending on the LLM base\nmodel. Furthermore, we explore the relationship between our classifier's\nperformance and approaches based on the probability assigned to the sentence by\nthe LLM. We show that while LLM-assigned sentence probability is related to\nsentence truthfulness, this probability is also dependent on sentence length\nand the frequencies of words in the sentence, resulting in our trained\nclassifier providing a more reliable approach to detecting truthfulness,\nhighlighting its potential to enhance the reliability of LLM-generated content\nand its practical applicability in real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.13734v2.pdf"
    },
    {
        "title": "Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks",
        "authors": [
            "Siddhartha Kapuria",
            "Tarunraj G. Mohanraj",
            "Nethra Venkatayogi",
            "Ozdemir Can Kara",
            "Yuki Hirata",
            "Patrick Minot",
            "Ariel Kapusta",
            "Naruhiko Ikoma",
            "Farshid Alambeigi"
        ],
        "published": "2023-04-25T23:18:13Z",
        "summary": "In this study, toward addressing the over-confident outputs of existing\nartificial intelligence-based colorectal cancer (CRC) polyp classification\ntechniques, we propose a confidence-calibrated residual neural network.\nUtilizing a novel vision-based tactile sensing (VS-TS) system and unique CRC\npolyp phantoms, we demonstrate that traditional metrics such as accuracy and\nprecision are not sufficient to encapsulate model performance for handling a\nsensitive CRC polyp diagnosis. To this end, we develop a residual neural\nnetwork classifier and address its over-confident outputs for CRC polyps\nclassification via the post-processing method of temperature scaling. To\nevaluate the proposed method, we introduce noise and blur to the obtained\ntextural images of the VS-TS and test the model's reliability for non-ideal\ninputs through reliability diagrams and other statistical metrics.",
        "pdf_link": "https://arxiv.org/pdf/2304.13192v1.pdf"
    },
    {
        "title": "TABLET: Learning From Instructions For Tabular Data",
        "authors": [
            "Dylan Slack",
            "Sameer Singh"
        ],
        "published": "2023-04-25T23:07:20Z",
        "summary": "Acquiring high-quality data is often a significant challenge in training\nmachine learning (ML) models for tabular prediction, particularly in\nprivacy-sensitive and costly domains like medicine and finance. Providing\nnatural language instructions to large language models (LLMs) offers an\nalternative solution. However, it is unclear how effectively instructions\nleverage the knowledge in LLMs for solving tabular prediction problems. To\naddress this gap, we introduce TABLET, a benchmark of 20 diverse tabular\ndatasets annotated with instructions that vary in their phrasing, granularity,\nand technicality. Additionally, TABLET includes the instructions' logic and\nstructured modifications to the instructions. We find in-context instructions\nincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for\nChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular\nprediction in our benchmark by evaluating instruction faithfulness. We find\nLLMs often ignore instructions and fail to predict specific instances\ncorrectly, even with examples. Our analysis on TABLET shows that, while\ninstructions help LLM performance, learning from instructions for tabular data\nrequires new capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2304.13188v1.pdf"
    },
    {
        "title": "AI-assisted coding: Experiments with GPT-4",
        "authors": [
            "Russell A Poldrack",
            "Thomas Lu",
            "Ga\u0161per Begu\u0161"
        ],
        "published": "2023-04-25T22:59:01Z",
        "summary": "Artificial intelligence (AI) tools based on large language models have\nacheived human-level performance on some computer programming tasks. We report\nseveral experiments using GPT-4 to generate computer code. These experiments\ndemonstrate that AI code generation using the current generation of tools,\nwhile powerful, requires substantial human validation to ensure accurate\nperformance. We also demonstrate that GPT-4 refactoring of existing code can\nsignificantly improve that code along several established metrics for code\nquality, and we show that GPT-4 can generate tests with substantial coverage,\nbut that many of the tests fail when applied to the associated code. These\nfindings suggest that while AI coding tools are very powerful, they still\nrequire humans in the loop to ensure validity and accuracy of the results.",
        "pdf_link": "https://arxiv.org/pdf/2304.13187v1.pdf"
    },
    {
        "title": "The Potential of Visual ChatGPT For Remote Sensing",
        "authors": [
            "Lucas Prado Osco",
            "Eduardo Lopes de Lemos",
            "Wesley Nunes Gon\u00e7alves",
            "Ana Paula Marques Ramos",
            "Jos\u00e9 Marcato Junior"
        ],
        "published": "2023-04-25T17:29:47Z",
        "summary": "Recent advancements in Natural Language Processing (NLP), particularly in\nLarge Language Models (LLMs), associated with deep learning-based computer\nvision techniques, have shown substantial potential for automating a variety of\ntasks. One notable model is Visual ChatGPT, which combines ChatGPT's LLM\ncapabilities with visual computation to enable effective image analysis. The\nmodel's ability to process images based on textual inputs can revolutionize\ndiverse fields. However, its application in the remote sensing domain remains\nunexplored. This is the first paper to examine the potential of Visual ChatGPT,\na cutting-edge LLM founded on the GPT architecture, to tackle the aspects of\nimage processing related to the remote sensing domain. Among its current\ncapabilities, Visual ChatGPT can generate textual descriptions of images,\nperform canny edge and straight line detection, and conduct image segmentation.\nThese offer valuable insights into image content and facilitate the\ninterpretation and extraction of information. By exploring the applicability of\nthese techniques within publicly available datasets of satellite images, we\ndemonstrate the current model's limitations in dealing with remote sensing\nimages, highlighting its challenges and future prospects. Although still in\nearly development, we believe that the combination of LLMs and visual models\nholds a significant potential to transform remote sensing image processing,\ncreating accessible and practical application opportunities in the field.",
        "pdf_link": "https://arxiv.org/pdf/2304.13009v2.pdf"
    },
    {
        "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
        "authors": [
            "Rongjie Huang",
            "Mingze Li",
            "Dongchao Yang",
            "Jiatong Shi",
            "Xuankai Chang",
            "Zhenhui Ye",
            "Yuning Wu",
            "Zhiqing Hong",
            "Jiawei Huang",
            "Jinglin Liu",
            "Yi Ren",
            "Zhou Zhao",
            "Shinji Watanabe"
        ],
        "published": "2023-04-25T17:05:38Z",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. Despite the recent success, current LLMs are not capable of\nprocessing complex audio information or conducting spoken conversations (like\nSiri or Alexa). In this work, we propose a multi-modal AI system named\nAudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to\nprocess complex audio information and solve numerous understanding and\ngeneration tasks; and 2) the input/output interface (ASR, TTS) to support\nspoken dialogue. With an increasing demand to evaluate multi-modal LLMs of\nhuman intention understanding and cooperation with foundation models, we\noutline the principles and processes and test AudioGPT in terms of consistency,\ncapability, and robustness. Experimental results demonstrate the capabilities\nof AudioGPT in solving AI tasks with speech, music, sound, and talking head\nunderstanding and generation in multi-round dialogues, which empower humans to\ncreate rich and diverse audio content with unprecedented ease. Our system is\npublicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.",
        "pdf_link": "https://arxiv.org/pdf/2304.12995v1.pdf"
    },
    {
        "title": "What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files",
        "authors": [
            "Peter Meltzer",
            "Joseph G. Lambourne",
            "Daniele Grandi"
        ],
        "published": "2023-04-25T12:30:01Z",
        "summary": "Semantic knowledge of part-part and part-whole relationships in assemblies is\nuseful for a variety of tasks from searching design repositories to the\nconstruction of engineering knowledge bases. In this work we propose that the\nnatural language names designers use in Computer Aided Design (CAD) software\nare a valuable source of such knowledge, and that Large Language Models (LLMs)\ncontain useful domain-specific information for working with this data as well\nas other CAD and engineering-related tasks.\n  In particular we extract and clean a large corpus of natural language part,\nfeature and document names and use this to quantitatively demonstrate that a\npre-trained language model can outperform numerous benchmarks on three\nself-supervised tasks, without ever having seen this data before. Moreover, we\nshow that fine-tuning on the text data corpus further boosts the performance on\nall tasks, thus demonstrating the value of the text data which until now has\nbeen largely ignored. We also identify key limitations to using LLMs with text\ndata alone, and our findings provide a strong motivation for further work into\nmulti-modal text-geometry models.\n  To aid and encourage further work in this area we make all our data and code\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2304.14275v1.pdf"
    },
    {
        "title": "Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting",
        "authors": [
            "Jianzhang Zhang",
            "Yiyang Chen",
            "Nan Niu",
            "Yinglin Wang",
            "Chuang Liu"
        ],
        "published": "2023-04-25T04:09:45Z",
        "summary": "Recently, various illustrative examples have shown the impressive ability of\ngenerative large language models (LLMs) to perform NLP related tasks. ChatGPT\nundoubtedly is the most representative model. We empirically evaluate ChatGPT's\nperformance on requirements information retrieval (IR) tasks to derive insights\ninto designing or developing more effective requirements retrieval methods or\ntools based on generative LLMs. We design an evaluation framework considering\nfour different combinations of two popular IR tasks and two common artifact\ntypes. Under zero-shot setting, evaluation results reveal ChatGPT's promising\nability to retrieve requirements relevant information (high recall) and limited\nability to retrieve more specific requirements information (low precision). Our\nevaluation of ChatGPT on requirements IR under zero-shot setting provides\npreliminary evidence for designing or developing more effective requirements IR\nmethods or tools based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.12562v2.pdf"
    },
    {
        "title": "Improved Trust in Human-Robot Collaboration with ChatGPT",
        "authors": [
            "Yang Ye",
            "Hengxu You",
            "Jing Du"
        ],
        "published": "2023-04-25T02:48:35Z",
        "summary": "Human robot collaboration is becoming increasingly important as robots become\nmore involved in various aspects of human life in the era of Artificial\nIntelligence. However, the issue of human operators trust in robots remains a\nsignificant concern, primarily due to the lack of adequate semantic\nunderstanding and communication between humans and robots. The emergence of\nLarge Language Models (LLMs), such as ChatGPT, provides an opportunity to\ndevelop an interactive, communicative, and robust human-robot collaboration\napproach. This paper explores the impact of ChatGPT on trust in a human-robot\ncollaboration assembly task. This study designs a robot control system called\nRoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human\noperators fetch, and place tools, while human operators can communicate with\nand control the robot arm using natural language. A human-subject experiment\nshowed that incorporating ChatGPT in robots significantly increased trust in\nhuman-robot collaboration, which can be attributed to the robot's ability to\ncommunicate more effectively with humans. Furthermore, ChatGPT ability to\nunderstand the nuances of human language and respond appropriately helps to\nbuild a more natural and intuitive human-robot interaction. The findings of\nthis study have significant implications for the development of human-robot\ncollaboration systems.",
        "pdf_link": "https://arxiv.org/pdf/2304.12529v1.pdf"
    },
    {
        "title": "Semantic Compression With Large Language Models",
        "authors": [
            "Henry Gilbert",
            "Michael Sandborn",
            "Douglas C. Schmidt",
            "Jesse Spencer-Smith",
            "Jules White"
        ],
        "published": "2023-04-25T01:47:05Z",
        "summary": "The rise of large language models (LLMs) is revolutionizing information\nretrieval, question answering, summarization, and code generation tasks.\nHowever, in addition to confidently presenting factually inaccurate information\nat times (known as \"hallucinations\"), LLMs are also inherently limited by the\nnumber of input and output tokens that can be processed at once, making them\npotentially less effective on tasks that require processing a large set or\ncontinuous stream of information. A common approach to reducing the size of\ndata is through lossless or lossy compression. Yet, in some cases it may not be\nstrictly necessary to perfectly recover every detail from the original data, as\nlong as a requisite level of semantic precision or intent is conveyed.\n  This paper presents three contributions to research on LLMs. First, we\npresent the results from experiments exploring the viability of approximate\ncompression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT\ninterfaces. Second, we investigate and quantify the capability of LLMs to\ncompress text and code, as well as to recall and manipulate compressed\nrepresentations of prompts. Third, we present two novel metrics -- Exact\nReconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness\n(SRE) -- that quantify the level of preserved intent between text compressed\nand decompressed by the LLMs we studied. Our initial results indicate that\nGPT-4 can effectively compress and reconstruct text while preserving the\nsemantic essence of the original text, providing a path to leverage\n$\\sim$5$\\times$ more tokens than present limits allow.",
        "pdf_link": "https://arxiv.org/pdf/2304.12512v1.pdf"
    },
    {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "authors": [
            "Can Xu",
            "Qingfeng Sun",
            "Kai Zheng",
            "Xiubo Geng",
            "Pu Zhao",
            "Jiazhan Feng",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "published": "2023-04-24T16:31:06Z",
        "summary": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM",
        "pdf_link": "https://arxiv.org/pdf/2304.12244v2.pdf"
    },
    {
        "title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering",
        "authors": [
            "Yucheng Li"
        ],
        "published": "2023-04-24T13:55:47Z",
        "summary": "Large language models (LLMs) have received significant attention by achieving\nremarkable performance across various tasks. However, their fixed context\nlength poses challenges when processing long documents or maintaining extended\nconversations. This paper proposes a method called \\textit{Selective Context}\nthat employs self-information to filter out less informative content, thereby\nenhancing the efficiency of the fixed context length. We demonstrate the\neffectiveness of our approach on tasks of summarisation and question answering\nacross different data sources, including academic papers, news articles, and\nconversation transcripts.",
        "pdf_link": "https://arxiv.org/pdf/2304.12102v1.pdf"
    },
    {
        "title": "ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain",
        "authors": [
            "Philipp Kuehn",
            "Mike Schmidt",
            "Markus Bayer",
            "Christian Reuter"
        ],
        "published": "2023-04-24T09:53:33Z",
        "summary": "Publicly available information contains valuable information for Cyber Threat\nIntelligence (CTI). This can be used to prevent attacks that have already taken\nplace on other systems. Ideally, only the initial attack succeeds and all\nsubsequent ones are detected and stopped. But while there are different\nstandards to exchange this information, a lot of it is shared in articles or\nblog posts in non-standardized ways. Manually scanning through multiple online\nportals and news pages to discover new threats and extracting them is a\ntime-consuming task. To automize parts of this scanning process, multiple\npapers propose extractors that use Natural Language Processing (NLP) to extract\nIndicators of Compromise (IOCs) from documents. However, while this already\nsolves the problem of extracting the information out of documents, the search\nfor these documents is rarely considered. In this paper, a new focused crawler\nis proposed called ThreatCrawl, which uses Bidirectional Encoder\nRepresentations from Transformers (BERT)-based models to classify documents and\nadapt its crawling path dynamically. While ThreatCrawl has difficulties to\nclassify the specific type of Open Source Intelligence (OSINT) named in texts,\ne.g., IOC content, it can successfully find relevant documents and modify its\npath accordingly. It yields harvest rates of up to 52%, which are, to the best\nof our knowledge, better than the current state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2304.11960v2.pdf"
    },
    {
        "title": "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology",
        "authors": [
            "Yixing Huang",
            "Ahmed Gomaa",
            "Sabine Semrau",
            "Marlen Haderlein",
            "Sebastian Lettmaier",
            "Thomas Weissmann",
            "Johanna Grigo",
            "Hassen Ben Tkhayat",
            "Benjamin Frey",
            "Udo S. Gaipl",
            "Luitpold V. Distel",
            "Andreas Maier",
            "Rainer Fietkau",
            "Christoph Bert",
            "Florian Putz"
        ],
        "published": "2023-04-24T09:50:39Z",
        "summary": "The potential of large language models in medicine for education and decision\nmaking purposes has been demonstrated as they achieve decent scores on medical\nexams such as the United States Medical Licensing Exam (USMLE) and the MedQA\nexam. In this work, we evaluate the performance of ChatGPT-4 in the specialized\nfield of radiation oncology using the 38th American College of Radiology (ACR)\nradiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone\ncases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of\n63.65% and 74.57%, respectively, highlighting the advantage of the latest\nChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in\nradiation oncology are identified to some extent. Specifically, ChatGPT-4\ndemonstrates better knowledge of statistics, CNS & eye, pediatrics, biology,\nand physics than knowledge of bone & soft tissue and gynecology, as per the ACR\nknowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in\ndiagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks\nproficiency in in-depth details of clinical trials. For the Gray Zone cases,\nChatGPT-4 is able to suggest a personalized treatment approach to each case\nwith high correctness and comprehensiveness. Importantly, it provides novel\ntreatment aspects for many cases, which are not suggested by any human experts.\nBoth evaluations demonstrate the potential of ChatGPT-4 in medical education\nfor the general public and cancer patients, as well as the potential to aid\nclinical decision-making, while acknowledging its limitations in certain\ndomains. Because of the risk of hallucination, facts provided by ChatGPT always\nneed to be verified.",
        "pdf_link": "https://arxiv.org/pdf/2304.11957v4.pdf"
    },
    {
        "title": "Is ChatGPT the Ultimate Programming Assistant -- How far is it?",
        "authors": [
            "Haoye Tian",
            "Weiqi Lu",
            "Tsz On Li",
            "Xunzhu Tang",
            "Shing-Chi Cheung",
            "Jacques Klein",
            "Tegawend\u00e9 F. Bissyand\u00e9"
        ],
        "published": "2023-04-24T09:20:13Z",
        "summary": "Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT's potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT's performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT's practical\napplications for software engineering.",
        "pdf_link": "https://arxiv.org/pdf/2304.11938v2.pdf"
    },
    {
        "title": "Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT",
        "authors": [
            "Ruohong Zhang",
            "Yau-Shian Wang",
            "Yiming Yang"
        ],
        "published": "2023-04-24T07:35:38Z",
        "summary": "Moreover, GPT-based zero-shot classification models tend to make independent\npredictions over test instances, which can be sub-optimal as the instance\ncorrelations and the decision boundaries in the target space are ignored. To\naddress these difficulties and limitations, we propose a new approach to\nzero-shot text classification, namely \\ourmodelshort, which leverages the\nstrong generative power of GPT to assist in training a smaller, more adaptable,\nand efficient sentence encoder classifier with contrastive self-training.\nSpecifically, GenCo applies GPT in two ways: firstly, it generates multiple\naugmented texts for each input instance to enhance the semantic embedding of\nthe instance and improve the mapping to relevant labels; secondly, it generates\naugmented texts conditioned on the predicted label during self-training, which\nmakes the generative process tailored to the decision boundaries in the target\nspace. In our experiments, GenCo outperforms previous state-of-the-art methods\non multiple benchmark datasets, even when only limited in-domain text data is\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2304.11872v1.pdf"
    },
    {
        "title": "PARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis",
        "authors": [
            "Shu Wei",
            "Nuo Xu"
        ],
        "published": "2023-04-24T03:54:48Z",
        "summary": "Document layout analysis has a wide range of requirements across various\ndomains, languages, and business scenarios. However, most current\nstate-of-the-art algorithms are language-dependent, with architectures that\nrely on transformer encoders or language-specific text encoders, such as BERT,\nfor feature extraction. These approaches are limited in their ability to handle\nvery long documents due to input sequence length constraints and are closely\ntied to language-specific tokenizers. Additionally, training a cross-language\ntext encoder can be challenging due to the lack of labeled multilingual\ndocument datasets that consider privacy. Furthermore, some layout tasks require\na clean separation between different layout components without overlap, which\ncan be difficult for image segmentation-based algorithms to achieve. In this\npaper, we present Paragraph2Graph, a language-independent graph neural network\n(GNN)-based model that achieves competitive results on common document layout\ndatasets while being adaptable to business scenarios with strict separation.\nWith only 19.95 million parameters, our model is suitable for industrial\napplications, particularly in multi-language scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.11810v1.pdf"
    },
    {
        "title": "A Lightweight Constrained Generation Alternative for Query-focused Summarization",
        "authors": [
            "Zhichao Xu",
            "Daniel Cohen"
        ],
        "published": "2023-04-23T18:43:48Z",
        "summary": "Query-focused summarization (QFS) aims to provide a summary of a document\nthat satisfies information need of a given query and is useful in various IR\napplications, such as abstractive snippet generation. Current QFS approaches\ntypically involve injecting additional information, e.g. query-answer relevance\nor fine-grained token-level interaction between a query and document, into a\nfinetuned large language model. However, these approaches often require extra\nparameters \\& training, and generalize poorly to new dataset distributions. To\nmitigate this, we propose leveraging a recently developed constrained\ngeneration model Neurological Decoding (NLD) as an alternative to current QFS\nregimes which rely on additional sub-architectures and training. We first\nconstruct lexical constraints by identifying important tokens from the document\nusing a lightweight gradient attribution model, then subsequently force the\ngenerated summary to satisfy these constraints by directly manipulating the\nfinal vocabulary likelihood. This lightweight approach requires no additional\nparameters or finetuning as it utilizes both an off-the-shelf neural retrieval\nmodel to construct the constraints and a standard generative language model to\nproduce the QFS. We demonstrate the efficacy of this approach on two public QFS\ncollections achieving near parity with the state-of-the-art model with\nsubstantially reduced complexity.",
        "pdf_link": "https://arxiv.org/pdf/2304.11721v1.pdf"
    },
    {
        "title": "Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoning Ye",
            "Lin Zhang",
            "Zhuozhi Xiong",
            "Zihan Li",
            "Qianyu He",
            "Sihang Jiang",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-04-23T15:11:49Z",
        "summary": "Domain knowledge refers to the in-depth understanding, expertise, and\nfamiliarity with a specific subject, industry, field, or area of special\ninterest. The existing benchmarks are all lack of an overall design for domain\nknowledge evaluation. Holding the belief that the real ability of domain\nlanguage understanding can only be fairly evaluated by an comprehensive and\nin-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa\ntargets at testing Large Language Models (LLMs) on their domain knowledge\nunderstanding, it features extensive domain coverage, large data volume, and a\ncontinually updated data set based on Chinese 112 first-level subject\nclassifications. DomMa consist of 100,000 questions in both Chinese and English\nsourced from graduate entrance examinations and undergraduate exams in Chinese\ncollege. We have also propose designs to make benchmark and evaluation process\nmore suitable to LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.11679v2.pdf"
    },
    {
        "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
        "authors": [
            "Jiashuo Sun",
            "Yi Luo",
            "Yeyun Gong",
            "Chen Lin",
            "Yelong Shen",
            "Jian Guo",
            "Nan Duan"
        ],
        "published": "2023-04-23T13:54:39Z",
        "summary": "Large language models (LLMs) can achieve highly effective performance on\nvarious reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting as demonstrations. However, the reasoning chains of demonstrations\ngenerated by LLMs are prone to errors, which can subsequently lead to incorrect\nreasoning during inference. Furthermore, inappropriate exemplars (overly\nsimplistic or complex), can affect overall performance among varying levels of\ndifficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts\nPrompting), an iterative bootstrapping approach for selecting exemplars and\ngenerating reasoning chains. By utilizing iterative bootstrapping, our approach\nenables LLMs to autonomously rectify errors, resulting in more precise and\ncomprehensive reasoning chains. Simultaneously, our approach selects\nchallenging yet answerable questions accompanied by reasoning chains as\nexemplars with a moderate level of difficulty, which enhances the LLMs'\ngeneralizability across varying levels of difficulty. Experimental results\nindicate that Iter-CoT exhibits superiority, achieving competitive performance\nacross three distinct reasoning tasks on ten datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.11657v3.pdf"
    }
]