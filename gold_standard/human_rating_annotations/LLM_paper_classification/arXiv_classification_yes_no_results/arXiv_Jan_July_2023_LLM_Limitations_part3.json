[
    {
        "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
        "authors": [
            "Bo Li",
            "Gexiang Fang",
            "Yang Yang",
            "Quansen Wang",
            "Wei Ye",
            "Wen Zhao",
            "Shikun Zhang"
        ],
        "published": "2023-04-23T12:33:18Z",
        "summary": "The capability of Large Language Models (LLMs) like ChatGPT to comprehend\nuser intent and provide reasonable responses has made them extremely popular\nlately. In this paper, we focus on assessing the overall ability of ChatGPT\nusing 7 fine-grained information extraction (IE) tasks. Specially, we present\nthe systematically analysis by measuring ChatGPT's performance, explainability,\ncalibration, and faithfulness, and resulting in 15 keys from either the ChatGPT\nor domain experts. Our findings reveal that ChatGPT's performance in\nStandard-IE setting is poor, but it surprisingly exhibits excellent performance\nin the OpenIE setting, as evidenced by human evaluation. In addition, our\nresearch indicates that ChatGPT provides high-quality and trustworthy\nexplanations for its decisions. However, there is an issue of ChatGPT being\noverconfident in its predictions, which resulting in low calibration.\nFurthermore, ChatGPT demonstrates a high level of faithfulness to the original\ntext in the majority of cases. We manually annotate and release the test sets\nof 7 fine-grained IE tasks contains 14 datasets to further promote the\nresearch. The datasets and code are available at\nhttps://github.com/pkuserc/ChatGPT_for_IE.",
        "pdf_link": "https://arxiv.org/pdf/2304.11633v1.pdf"
    },
    {
        "title": "Differentiate ChatGPT-generated and Human-written Medical Texts",
        "authors": [
            "Wenxiong Liao",
            "Zhengliang Liu",
            "Haixing Dai",
            "Shaochen Xu",
            "Zihao Wu",
            "Yiyang Zhang",
            "Xiaoke Huang",
            "Dajiang Zhu",
            "Hongmin Cai",
            "Tianming Liu",
            "Xiang Li"
        ],
        "published": "2023-04-23T07:38:07Z",
        "summary": "Background: Large language models such as ChatGPT are capable of generating\ngrammatically perfect and human-like text content, and a large number of\nChatGPT-generated texts have appeared on the Internet. However, medical texts\nsuch as clinical notes and diagnoses require rigorous validation, and erroneous\nmedical content generated by ChatGPT could potentially lead to disinformation\nthat poses significant harm to healthcare and the general public.\n  Objective: This research is among the first studies on responsible and\nethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus\non analyzing the differences between medical texts written by human experts and\ngenerated by ChatGPT, and designing machine learning workflows to effectively\ndetect and differentiate medical texts generated by ChatGPT.\n  Methods: We first construct a suite of datasets containing medical texts\nwritten by human experts and generated by ChatGPT. In the next step, we analyze\nthe linguistic features of these two types of content and uncover differences\nin vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally,\nwe design and implement machine learning methods to detect medical text\ngenerated by ChatGPT.\n  Results: Medical texts written by humans are more concrete, more diverse, and\ntypically contain more useful information, while medical texts generated by\nChatGPT pay more attention to fluency and logic, and usually express general\nterminologies rather than effective information specific to the context of the\nproblem. A BERT-based model can effectively detect medical texts generated by\nChatGPT, and the F1 exceeds 95%.",
        "pdf_link": "https://arxiv.org/pdf/2304.11567v1.pdf"
    },
    {
        "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL",
        "authors": [
            "Xiping Liu",
            "Zhao Tan"
        ],
        "published": "2023-04-23T06:52:35Z",
        "summary": "Chain-of-thought (CoT) prompting combined with large language models (LLMs)\nhave achieved encouraging results on complex reasoning tasks. Text-to-SQL is a\ncritical semantic parsing task that converts natural language questions into\nSQL statements, involving a complex reasoning process. However, there is little\nwork about using CoT prompting to activate LLM's reasoning capabilities on\nText-to-SQL tasks. In this work, we propose a new paradigm for prompting\nText-to-SQL tasks, called Divide-and-Prompt, which first divides the task into\nsubtasks, and then approach each subtask through CoT. We present 3\nprompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments\nshow that these prompts guide LLMs to generate Text-to-SQL with higher\nexecution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2304.11556v1.pdf"
    },
    {
        "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
        "authors": [
            "Shima Rahimi Moghaddam",
            "Christopher J. Honey"
        ],
        "published": "2023-04-22T22:50:50Z",
        "summary": "Large language models (LLMs) excel in many tasks in 2023, but they still face\nchallenges in complex reasoning. Theory-of-mind (ToM) tasks, which require\nunderstanding agents' beliefs, goals, and mental states, are essential for\ncommon-sense reasoning involving humans, making it crucial to enhance LLM\nperformance in this area. This study measures the ToM performance of GPT-4 and\nthree GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates\nthe effectiveness of in-context learning in improving their ToM comprehension.\nWe evaluated prompts featuring two-shot chain of thought reasoning and\nstep-by-step thinking instructions. We found that LLMs trained with\nReinforcement Learning from Human Feedback (RLHF) (all models excluding\nDavinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed\nbest in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell\nshort of the 87% human accuracy on the test set. However, when supplied with\nprompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM\naccuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate\nprompting enhances LLM ToM reasoning, and they underscore the context-dependent\nnature of LLM cognitive capacities.",
        "pdf_link": "https://arxiv.org/pdf/2304.11490v3.pdf"
    },
    {
        "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
        "authors": [
            "Bo Liu",
            "Yuqian Jiang",
            "Xiaohan Zhang",
            "Qiang Liu",
            "Shiqi Zhang",
            "Joydeep Biswas",
            "Peter Stone"
        ],
        "published": "2023-04-22T20:34:03Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable zero-shot\ngeneralization abilities: state-of-the-art chatbots can provide plausible\nanswers to many common questions that arise in daily life. However, so far,\nLLMs cannot reliably solve long-horizon planning problems. By contrast,\nclassical planners, once a problem is given in a formatted way, can use\nefficient search algorithms to quickly identify correct, or even optimal,\nplans. In an effort to get the best of both worlds, this paper introduces\nLLM+P, the first framework that incorporates the strengths of classical\nplanners into LLMs. LLM+P takes in a natural language description of a planning\nproblem, then returns a correct (or optimal) plan for solving that problem in\nnatural language. LLM+P does so by first converting the language description\ninto a file written in the planning domain definition language (PDDL), then\nleveraging classical planners to quickly find a solution, and then translating\nthe found solution back into natural language. Along with LLM+P, we define a\ndiverse set of different benchmark problems taken from common planning\nscenarios. Via a comprehensive set of experiments on these benchmark problems,\nwe find that LLM+P is able to provide optimal solutions for most problems,\nwhile LLMs fail to provide even feasible plans for most problems.\\footnote{The\ncode and results are publicly available at\nhttps://github.com/Cranial-XIX/llm-pddl.git.",
        "pdf_link": "https://arxiv.org/pdf/2304.11477v3.pdf"
    },
    {
        "title": "N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models",
        "authors": [
            "Alex Foote",
            "Neel Nanda",
            "Esben Kran",
            "Ionnis Konstas",
            "Fazl Barez"
        ],
        "published": "2023-04-22T19:06:13Z",
        "summary": "Understanding the function of individual neurons within language models is\nessential for mechanistic interpretability research. We propose $\\textbf{Neuron\nto Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and\nautomatically distills the neuron's behaviour on those examples to an\ninterpretable graph. This presents a less labour intensive approach to\ninterpreting neurons than current manual methods, that will better scale these\nmethods to Large Language Models (LLMs). We use truncation and saliency methods\nto only present the important tokens, and augment the dataset examples with\nmore diverse samples to better capture the extent of neuron behaviour. These\ngraphs can be visualised to aid manual interpretation by researchers, but can\nalso output token activations on text to compare to the neuron's ground truth\nactivations for automatic validation. N2G represents a step towards scalable\ninterpretability methods by allowing us to convert neurons in an LLM to\ninterpretable representations of measurable quality.",
        "pdf_link": "https://arxiv.org/pdf/2304.12918v1.pdf"
    },
    {
        "title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023-04-22T12:50:49Z",
        "summary": "Recent psycholinguistic studies have drawn conflicting conclusions about the\nrelationship between the quality of a language model and the ability of its\nsurprisal estimates to predict human reading times, which has been speculated\nto be due to the large gap in both the amount of training data and model\ncapacity across studies. The current work aims to consolidate these findings by\nevaluating surprisal estimates from Transformer-based language model variants\nthat vary systematically in the amount of training data and model capacity on\ntheir ability to predict human reading times. The results show that surprisal\nestimates from most variants with contemporary model capacities provide the\nbest fit after seeing about two billion training tokens, after which they begin\nto diverge from humanlike expectations. Additionally, newly-trained smaller\nmodel variants reveal a 'tipping point' at convergence, after which the\ndecrease in language model perplexity begins to result in poorer fits to human\nreading times. These results suggest that the massive amount of training data\nis mainly responsible for the poorer fit achieved by surprisal from larger\npre-trained language models, and that a certain degree of model capacity is\nnecessary for Transformer-based language models to capture humanlike\nexpectations.",
        "pdf_link": "https://arxiv.org/pdf/2304.11389v2.pdf"
    },
    {
        "title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval",
        "authors": [
            "Haitao Li",
            "Qingyao Ai",
            "Jia Chen",
            "Qian Dong",
            "Yueyue Wu",
            "Yiqun Liu",
            "Chong Chen",
            "Qi Tian"
        ],
        "published": "2023-04-22T10:47:01Z",
        "summary": "Legal case retrieval, which aims to find relevant cases for a query case,\nplays a core role in the intelligent legal system. Despite the success that\npre-training has achieved in ad-hoc retrieval tasks, effective pre-training\nstrategies for legal case retrieval remain to be explored. Compared with\ngeneral documents, legal case documents are typically long text sequences with\nintrinsic logical structures. However, most existing language models have\ndifficulty understanding the long-distance dependencies between different\nstructures. Moreover, in contrast to the general retrieval, the relevance in\nthe legal domain is sensitive to key legal elements. Even subtle differences in\nkey legal elements can significantly affect the judgement of relevance.\nHowever, existing pre-trained language models designed for general purposes\nhave not been equipped to handle legal elements.\n  To address these issues, in this paper, we propose SAILER, a new\nStructure-Aware pre-traIned language model for LEgal case Retrieval. It is\nhighlighted in the following three aspects: (1) SAILER fully utilizes the\nstructural information contained in legal case documents and pays more\nattention to key legal elements, similar to how legal experts browse legal case\ndocuments. (2) SAILER employs an asymmetric encoder-decoder architecture to\nintegrate several different pre-training objectives. In this way, rich semantic\ninformation across tasks is encoded into dense vectors. (3) SAILER has powerful\ndiscriminative ability, even without any legal annotation data. It can\ndistinguish legal cases with different charges accurately. Extensive\nexperiments over publicly available legal benchmarks demonstrate that our\napproach can significantly outperform previous state-of-the-art methods in\nlegal case retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2304.11370v1.pdf"
    },
    {
        "title": "Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers",
        "authors": [
            "Felipe Urrutia",
            "Roberto Araya"
        ],
        "published": "2023-04-21T21:25:30Z",
        "summary": "Written answers to open-ended questions can have a higher long-term effect on\nlearning than multiple-choice questions. However, it is critical that teachers\nimmediately review the answers, and ask to redo those that are incoherent. This\ncan be a difficult task and can be time-consuming for teachers. A possible\nsolution is to automate the detection of incoherent answers. One option is to\nautomate the review with Large Language Models (LLM). In this paper, we analyze\nthe responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,\nand YOU. We used them with zero, one, two, three and four shots. We compared\ntheir performance with the results of various classifiers trained with Machine\nLearning (ML). We found that LLMs perform worse than MLs in detecting\nincoherent answers. The difficulty seems to reside in recursive questions that\ncontain both questions and answers, and in responses from students with typical\nfourth-grader misspellings. Upon closer examination, we have found that the\nChatGPT model faces the same challenges.",
        "pdf_link": "https://arxiv.org/pdf/2304.11257v1.pdf"
    },
    {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raff"
        ],
        "published": "2023-04-21T17:58:31Z",
        "summary": "Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model's\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model's full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite and plot scaling laws for forecasting memorization,\nallowing us to provide equi-compute recommendations to maximize the reliability\n(recall) of such predictions. We additionally provide further novel discoveries\non the distribution of memorization scores across models and data. We release\nall code and data necessary to reproduce the results in this paper at\nhttps://github.com/EleutherAI/pythia",
        "pdf_link": "https://arxiv.org/pdf/2304.11158v2.pdf"
    },
    {
        "title": "The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination",
        "authors": [
            "Zihao Li"
        ],
        "published": "2023-04-21T16:40:54Z",
        "summary": "With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our\nwhole society, rapidly altering the way we think, create and live. For\ninstance, the GPT integration in Bing has altered our approach to online\nsearching. While nascent LLMs have many advantages, new legal and ethical risks\nare also emerging, stemming in particular from stochastic parrots and\nhallucination. The EU is the first and foremost jurisdiction that has focused\non the regulation of AI models. However, the risks posed by the new LLMs are\nlikely to be underestimated by the emerging EU regulatory paradigm. Therefore,\nthis correspondence warns that the European AI regulatory paradigm must evolve\nfurther to mitigate such risks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14347v1.pdf"
    },
    {
        "title": "Inducing anxiety in large language models increases exploration and bias",
        "authors": [
            "Julian Coda-Forno",
            "Kristin Witte",
            "Akshay K. Jagadish",
            "Marcel Binz",
            "Zeynep Akata",
            "Eric Schulz"
        ],
        "published": "2023-04-21T16:29:43Z",
        "summary": "Large language models are transforming research on machine learning while\ngalvanizing public debates. Understanding not only when these models work well\nand succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of computational psychiatry, a framework\nused to computationally describe and modify aberrant behavior, to the outputs\nproduced by these models. We focus on the Generative Pre-Trained Transformer\n3.5 and subject it to tasks commonly studied in psychiatry. Our results show\nthat GPT-3.5 responds robustly to a common anxiety questionnaire, producing\nhigher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be\npredictably changed by using emotion-inducing prompts. Emotion-induction not\nonly influences GPT-3.5's behavior in a cognitive task measuring exploratory\ndecision-making but also influences its behavior in a previously-established\ntask measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a\nstrong increase in biases when prompted with anxiety-inducing text. Thus, it is\nlikely that how prompts are communicated to large language models has a strong\ninfluence on their behavior in applied settings. These results progress our\nunderstanding of prompt engineering and demonstrate the usefulness of methods\ntaken from computational psychiatry for studying the capable algorithms to\nwhich we increasingly delegate authority and autonomy.",
        "pdf_link": "https://arxiv.org/pdf/2304.11111v1.pdf"
    },
    {
        "title": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT",
        "authors": [
            "Tianyang Zhong",
            "Yaonai Wei",
            "Li Yang",
            "Zihao Wu",
            "Zhengliang Liu",
            "Xiaozheng Wei",
            "Wenjun Li",
            "Junjie Yao",
            "Chong Ma",
            "Xiang Li",
            "Dajiang Zhu",
            "Xi Jiang",
            "Junwei Han",
            "Dinggang Shen",
            "Tianming Liu",
            "Tuo Zhang"
        ],
        "published": "2023-04-21T16:23:47Z",
        "summary": "Large language models (LLMs) such as ChatGPT have recently demonstrated\nsignificant potential in mathematical abilities, providing valuable reasoning\nparadigm consistent with human natural language. However, LLMs currently have\ndifficulty in bridging perception, language understanding and reasoning\ncapabilities due to incompatibility of the underlying information flow among\nthem, making it challenging to accomplish tasks autonomously. On the other\nhand, abductive learning (ABL) frameworks for integrating the two abilities of\nperception and reasoning has seen significant success in inverse decipherment\nof incomplete facts, but it is limited by the lack of semantic understanding of\nlogical reasoning rules and the dependence on complicated domain knowledge\nrepresentation. This paper presents a novel method (ChatABL) for integrating\nLLMs into the ABL framework, aiming at unifying the three abilities in a more\nuser-friendly and understandable manner. The proposed method uses the strengths\nof LLMs' understanding and logical reasoning to correct the incomplete logical\nfacts for optimizing the performance of perceptual module, by summarizing and\nreorganizing reasoning rules represented in natural language format. Similarly,\nperceptual module provides necessary reasoning examples for LLMs in natural\nlanguage format. The variable-length handwritten equation deciphering task, an\nabstract expression of the Mayan calendar decoding, is used as a testbed to\ndemonstrate that ChatABL has reasoning ability beyond most existing\nstate-of-the-art methods, which has been well supported by comparative studies.\nTo our best knowledge, the proposed ChatABL is the first attempt to explore a\nnew pattern for further approaching human-level cognitive ability via natural\nlanguage interaction with ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2304.11107v1.pdf"
    },
    {
        "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
        "authors": [
            "Mohammadreza Pourreza",
            "Davood Rafiei"
        ],
        "published": "2023-04-21T15:02:18Z",
        "summary": "There is currently a significant gap between the performance of fine-tuned\nmodels and prompting approaches using Large Language Models (LLMs) on the\nchallenging task of text-to-SQL, as evaluated on datasets such as Spider. To\nimprove the performance of LLMs in the reasoning process, we study how\ndecomposing the task into smaller sub-tasks can be effective. In particular, we\nshow that breaking down the generation problem into sub-problems and feeding\nthe solutions of those sub-problems into LLMs can be an effective approach for\nsignificantly improving their performance. Our experiments with three LLMs show\nthat this approach consistently improves their simple few-shot performance by\nroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the\nholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9\nand the new SOTA at the time of this writing using our approach is 85.3. Our\napproach with in-context learning beats many heavily fine-tuned models by at\nleast 5%. Additionally, when evaluated on the BIRD benchmark, our approach\nachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test\nset.",
        "pdf_link": "https://arxiv.org/pdf/2304.11015v3.pdf"
    },
    {
        "title": "Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition",
        "authors": [
            "Matteo Muffo",
            "Aldo Cocco",
            "Enrico Bertino"
        ],
        "published": "2023-04-21T14:21:52Z",
        "summary": "In recent years, Large Language Models such as GPT-3 showed remarkable\ncapabilities in performing NLP tasks in the zero and few shot settings. On the\nother hand, the experiments highlighted the difficulty of GPT-3 in carrying out\ntasks that require a certain degree of reasoning, such as arithmetic\noperations. In this paper we evaluate the ability of Transformer Language\nModels to perform arithmetic operations following a pipeline that, before\nperforming computations, decomposes numbers in units, tens, and so on. We\ndenote the models fine-tuned with this pipeline with the name Calculon and we\ntest them in the task of performing additions, subtractions and multiplications\non the same test sets of GPT-3. Results show an increase of accuracy of 63% in\nthe five-digit addition task. Moreover, we demonstrate the importance of the\ndecomposition pipeline introduced, since fine-tuning the same Language Model\nwithout decomposing numbers results in 0% accuracy in the five-digit addition\ntask.",
        "pdf_link": "https://arxiv.org/pdf/2304.10977v1.pdf"
    },
    {
        "title": "SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model",
        "authors": [
            "Juexiao Zhou",
            "Xiaonan He",
            "Liyuan Sun",
            "Jiannan Xu",
            "Xiuying Chen",
            "Yuetan Chu",
            "Longxi Zhou",
            "Xingyu Liao",
            "Bin Zhang",
            "Xin Gao"
        ],
        "published": "2023-04-21T01:17:09Z",
        "summary": "Skin and subcutaneous diseases rank high among the leading contributors to\nthe global burden of nonfatal diseases, impacting a considerable portion of the\npopulation. Nonetheless, the field of dermatology diagnosis faces three\nsignificant hurdles. Firstly, there is a shortage of dermatologists accessible\nto diagnose patients, particularly in rural regions. Secondly, accurately\ninterpreting skin disease images poses a considerable challenge. Lastly,\ngenerating patient-friendly diagnostic reports is usually a time-consuming and\nlabor-intensive task for dermatologists. To tackle these challenges, we present\nSkinGPT-4, which is the world's first interactive dermatology diagnostic system\npowered by an advanced visual large language model. SkinGPT-4 leverages a\nfine-tuned version of MiniGPT-4, trained on an extensive collection of skin\ndisease images (comprising 52,929 publicly available and proprietary images)\nalong with clinical concepts and doctors' notes. We designed a two-step\ntraining process to allow SkinGPT to express medical features in skin disease\nimages with natural language and make accurate diagnoses of the types of skin\ndiseases. With SkinGPT-4, users could upload their own skin photos for\ndiagnosis, and the system could autonomously evaluate the images, identifies\nthe characteristics and categories of the skin conditions, performs in-depth\nanalysis, and provides interactive treatment recommendations. Meanwhile,\nSkinGPT-4's local deployment capability and commitment to user privacy also\nrender it an appealing choice for patients in search of a dependable and\nprecise diagnosis of their skin ailments. To demonstrate the robustness of\nSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, which\nwere independently reviewed by certified dermatologists, and showed that\nSkinGPT-4 could provide accurate diagnoses of skin diseases.",
        "pdf_link": "https://arxiv.org/pdf/2304.10691v2.pdf"
    },
    {
        "title": "Meta Semantics: Towards better natural language understanding and reasoning",
        "authors": [
            "Xiaolin Hu"
        ],
        "published": "2023-04-20T22:16:16Z",
        "summary": "Natural language understanding is one of the most challenging topics in\nartificial intelligence. Deep neural network methods, particularly large\nlanguage module (LLM) methods such as ChatGPT and GPT-3, have powerful\nflexibility to adopt informal text but are weak on logical deduction and suffer\nfrom the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods\nsuch as Mathematica, Semantic web, and Lean, are excellent in reasoning but\ncannot handle the complex and changeable informal text. Inspired by pragmatics\nand structuralism, we propose two strategies to solve the OOV problem and a\nsemantic model for better natural language understanding and reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2304.10663v1.pdf"
    },
    {
        "title": "\"HOT\" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media",
        "authors": [
            "Lingyao Li",
            "Lizhou Fan",
            "Shubham Atreja",
            "Libby Hemphill"
        ],
        "published": "2023-04-20T19:40:51Z",
        "summary": "Harmful content is pervasive on social media, poisoning online communities\nand negatively impacting participation. A common approach to address this issue\nis to develop detection models that rely on human annotations. However, the\ntasks required to build such models expose annotators to harmful and offensive\ncontent and may require significant time and cost to complete. Generative AI\nmodels have the potential to understand and detect harmful content. To\ninvestigate this potential, we used ChatGPT and compared its performance with\nMTurker annotations for three frequently discussed concepts related to harmful\ncontent: Hateful, Offensive, and Toxic (HOT). We designed five prompts to\ninteract with ChatGPT and conducted four experiments eliciting HOT\nclassifications. Our results show that ChatGPT can achieve an accuracy of\napproximately 80% when compared to MTurker annotations. Specifically, the model\ndisplays a more consistent classification for non-HOT comments than HOT\ncomments compared to human annotations. Our findings also suggest that ChatGPT\nclassifications align with provided HOT definitions, but ChatGPT classifies\n\"hateful\" and \"offensive\" as subsets of \"toxic.\" Moreover, the choice of\nprompts used to interact with ChatGPT impacts its performance. Based on these\nin-sights, our study provides several meaningful implications for employing\nChatGPT to detect HOT content, particularly regarding the reliability and\nconsistency of its performance, its understand-ing and reasoning of the HOT\nconcept, and the impact of prompts on its performance. Overall, our study\nprovides guidance about the potential of using generative AI models to moderate\nlarge volumes of user-generated content on social media.",
        "pdf_link": "https://arxiv.org/pdf/2304.10619v1.pdf"
    },
    {
        "title": "Joint Repetition Suppression and Content Moderation of Large Language Models",
        "authors": [
            "Minghui Zhang",
            "Alex Sokolov",
            "Weixin Cai",
            "Si-Qing Chen"
        ],
        "published": "2023-04-20T19:17:49Z",
        "summary": "Natural language generation (NLG) is one of the most impactful fields in NLP,\nand recent years have witnessed its evolution brought about by large language\nmodels (LLMs). As the key instrument for writing assistance applications, they\nare generally prone to replicating or extending offensive content provided in\nthe input. In low-resource data regime, they can also lead to repetitive\noutputs. Usually, offensive content and repetitions are mitigated with post-hoc\nmethods, including n-gram level blocklists, top-k and nucleus sampling. In this\npaper, we apply non-exact repetition suppression using token and sequence level\nunlikelihood loss, and further explore the framework of unlikelihood training\nobjective in order to jointly endow the model with abilities to avoid\ngenerating offensive words and phrases from the beginning. Finally, with\ncomprehensive experiments, we demonstrate that our proposed methods work\nexceptionally in controlling the repetition and content quality of LLM outputs.",
        "pdf_link": "https://arxiv.org/pdf/2304.10611v2.pdf"
    },
    {
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
        "authors": [
            "Deyao Zhu",
            "Jun Chen",
            "Xiaoqian Shen",
            "Xiang Li",
            "Mohamed Elhoseiny"
        ],
        "published": "2023-04-20T18:25:35Z",
        "summary": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model's generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2304.10592v2.pdf"
    },
    {
        "title": "Why Does ChatGPT Fall Short in Providing Truthful Answers?",
        "authors": [
            "Shen Zheng",
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-04-20T17:48:43Z",
        "summary": "Recent advancements in large language models, such as ChatGPT, have\ndemonstrated significant potential to impact various aspects of human life.\nHowever, ChatGPT still faces challenges in providing reliable and accurate\nanswers to user questions. To better understand the model's particular\nweaknesses in providing truthful answers, we embark an in-depth exploration of\nopen-domain question answering. Specifically, we undertake a detailed\nexamination of ChatGPT's failures, categorized into: comprehension, factuality,\nspecificity, and inference. We further pinpoint factuality as the most\ncontributing failure and identify two critical abilities associated with\nfactuality: knowledge memorization and knowledge recall. Through experiments\nfocusing on factuality, we propose several potential enhancement strategies.\nOur findings suggest that augmenting the model with granular external knowledge\nand cues for knowledge recall can enhance the model's factuality in answering\nquestions.",
        "pdf_link": "https://arxiv.org/pdf/2304.10513v3.pdf"
    },
    {
        "title": "Learning to Plan with Natural Language",
        "authors": [
            "Yiduo Guo",
            "Yaobo Liang",
            "Chenfei Wu",
            "Wenshan Wu",
            "Dongyan Zhao",
            "Nan Duan"
        ],
        "published": "2023-04-20T17:09:12Z",
        "summary": "Large Language Models (LLMs) have shown remarkable performance in various\nbasic natural language tasks. For completing the complex task, we still need a\nplan for the task to guide LLMs to generate the specific solutions step by\nstep. LLMs can directly generate task plans, but these plans may still contain\nfactual errors or are incomplete. A high-quality task plan contains correct\nstep-by-step solutions for solving all situations and behavioral instructions\nfor avoiding mistakes. To obtain it, we propose the Learning to Plan method,\nwhich involves two phases: (1) In the first learning task plan phase, it\niteratively updates the task plan with new step-by-step solutions and\nbehavioral instructions, which are obtained by prompting LLMs to derive from\ntraining error feedback. (2) In the subsequent test phase, the LLM uses the\nlearned task plan to guide the inference of LLM on the test set. We demonstrate\nthe effectiveness of our method on the five different reasoning type tasks (8\ndatasets). Further, our analysis experiment shows that the task plan learned by\none LLM can directly guide another LLM to improve its performance, which\nreveals a new transfer learning paradigm. We release the code at\n\\url{https://github.com/Eureka6174/LearnNLPlan}",
        "pdf_link": "https://arxiv.org/pdf/2304.10464v4.pdf"
    },
    {
        "title": "Safety Assessment of Chinese Large Language Models",
        "authors": [
            "Hao Sun",
            "Zhexin Zhang",
            "Jiawen Deng",
            "Jiale Cheng",
            "Minlie Huang"
        ],
        "published": "2023-04-20T16:27:35Z",
        "summary": "With the rapid popularity of large language models such as ChatGPT and GPT-4,\na growing amount of attention is paid to their safety concerns. These models\nmay generate insulting and discriminatory content, reflect incorrect social\nvalues, and may be used for malicious purposes such as fraud and dissemination\nof misleading information. Evaluating and enhancing their safety is\nparticularly essential for the wide application of large language models\n(LLMs). To further promote the safe deployment of LLMs, we develop a Chinese\nLLM safety assessment benchmark. Our benchmark explores the comprehensive\nsafety performance of LLMs from two perspectives: 8 kinds of typical safety\nscenarios and 6 types of more challenging instruction attacks. Our benchmark is\nbased on a straightforward process in which it provides the test prompts and\nevaluates the safety of the generated responses from the evaluated model. In\nevaluation, we utilize the LLM's strong evaluation ability and develop it as a\nsafety evaluator by prompting. On top of this benchmark, we conduct safety\nassessments and analyze 15 LLMs including the OpenAI GPT series and other\nwell-known Chinese LLMs, where we observe some interesting findings. For\nexample, we find that instruction attacks are more likely to expose safety\nissues of all LLMs. Moreover, to promote the development and deployment of\nsafe, responsible, and ethical AI, we publicly release SafetyPrompts including\n100k augmented prompts and responses by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.10436v1.pdf"
    },
    {
        "title": "GPT-NER: Named Entity Recognition via Large Language Models",
        "authors": [
            "Shuhe Wang",
            "Xiaofei Sun",
            "Xiaoya Li",
            "Rongbin Ouyang",
            "Fei Wu",
            "Tianwei Zhang",
            "Jiwei Li",
            "Guoyin Wang"
        ],
        "published": "2023-04-20T16:17:26Z",
        "summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n  We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.",
        "pdf_link": "https://arxiv.org/pdf/2304.10428v4.pdf"
    },
    {
        "title": "Fully Autonomous Programming with Large Language Models",
        "authors": [
            "Vadim Liventsev",
            "Anastasiia Grishina",
            "Aki H\u00e4rm\u00e4",
            "Leon Moonen"
        ],
        "published": "2023-04-20T16:12:05Z",
        "summary": "Current approaches to program synthesis with Large Language Models (LLMs)\nexhibit a \"near miss syndrome\": they tend to generate programs that\nsemantically resemble the correct answer (as measured by text similarity\nmetrics or human evaluation), but achieve a low or even zero accuracy as\nmeasured by unit tests due to small imperfections, such as the wrong input or\noutput format. This calls for an approach known as Synthesize, Execute, Debug\n(SED), whereby a draft of the solution is generated first, followed by a\nprogram repair phase addressing the failed tests. To effectively apply this\napproach to instruction-driven LLMs, one needs to determine which prompts\nperform best as instructions for LLMs, as well as strike a balance between\nrepairing unsuccessful programs and replacing them with newly generated ones.\nWe explore these trade-offs empirically, comparing replace-focused,\nrepair-focused, and hybrid debug strategies, as well as different\ntemplate-based and model-based prompt-generation techniques. We use OpenAI\nCodex as the LLM and Program Synthesis Benchmark 2 as a database of problem\ndescriptions and tests for evaluation. The resulting framework outperforms both\nconventional usage of Codex without the repair phase and traditional genetic\nprogramming approaches.",
        "pdf_link": "https://arxiv.org/pdf/2304.10423v1.pdf"
    },
    {
        "title": "CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population",
        "authors": [
            "Tianqing Fang",
            "Quyet V. Do",
            "Sehyun Choi",
            "Weiqi Wang",
            "Yangqiu Song"
        ],
        "published": "2023-04-20T15:27:29Z",
        "summary": "Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task\nin NLP, as it tackles knowledge from external sources with unseen events and\nentities. Fang et al. (2021a) proposed a CSKB Population benchmark with an\nevaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that\nsuffer from a substantial fraction of incorrect answers, and the evaluation set\nis not well-aligned with the external knowledge source as a result of random\nsampling. In this paper, we introduce CKBP v2, a new high-quality CSKB\nPopulation benchmark, which addresses the two mentioned problems by using\nexperts instead of crowd-sourced annotation and by adding diversified\nadversarial samples to make the evaluation set more representative. We conduct\nextensive experiments comparing state-of-the-art methods for CSKB Population on\nthe new evaluation set for future research comparisons. Empirical results show\nthat the population task is still challenging, even for large language models\n(LLM) such as ChatGPT. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.",
        "pdf_link": "https://arxiv.org/pdf/2304.10392v1.pdf"
    },
    {
        "title": "Interventional Probing in High Dimensions: An NLI Case Study",
        "authors": [
            "Julia Rozanova",
            "Marco Valentino",
            "Lucas Cordeiro",
            "Andre Freitas"
        ],
        "published": "2023-04-20T14:34:31Z",
        "summary": "Probing strategies have been shown to detect the presence of various\nlinguistic features in large language models; in particular, semantic features\nintermediate to the \"natural logic\" fragment of the Natural Language Inference\ntask (NLI). In the case of natural logic, the relation between the intermediate\nfeatures and the entailment label is explicitly known: as such, this provides a\nripe setting for interventional studies on the NLI models' representations,\nallowing for stronger causal conjectures and a deeper critical analysis of\ninterventional probing methods. In this work, we carry out new and existing\nrepresentation-level interventions to investigate the effect of these semantic\nfeatures on NLI classification: we perform amnesic probing (which removes\nfeatures as directed by learned linear probes) and introduce the mnestic\nprobing variation (which forgets all dimensions except the probe-selected\nones). Furthermore, we delve into the limitations of these methods and outline\nsome pitfalls have been obscuring the effectivity of interventional probing\nstudies.",
        "pdf_link": "https://arxiv.org/pdf/2304.10346v1.pdf"
    },
    {
        "title": "Analyzing FOMC Minutes: Accuracy and Constraints of Language Models",
        "authors": [
            "Wonseong Kim",
            "Jan Frederic Sp\u00f6rer",
            "Siegfried Handschuh"
        ],
        "published": "2023-04-20T08:54:00Z",
        "summary": "This research article analyzes the language used in the official statements\nreleased by the Federal Open Market Committee (FOMC) after its scheduled\nmeetings to gain insights into the impact of FOMC official statements on\nfinancial markets and economic forecasting. The study reveals that the FOMC is\ncareful to avoid expressing emotion in their sentences and follows a set of\ntemplates to cover economic situations. The analysis employs advanced language\nmodeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The\nresults show that FinBERT outperforms other techniques in predicting negative\nsentiment accurately. However, the study also highlights the challenges and\nlimitations of using current NLP techniques to analyze FOMC texts and suggests\nthe potential for enhancing language models and exploring alternative\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2304.10164v2.pdf"
    },
    {
        "title": "Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks",
        "authors": [
            "Yiming Zhu",
            "Peixian Zhang",
            "Ehsan-Ul Haq",
            "Pan Hui",
            "Gareth Tyson"
        ],
        "published": "2023-04-20T08:08:12Z",
        "summary": "The release of ChatGPT has uncovered a range of possibilities whereby large\nlanguage models (LLMs) can substitute human intelligence. In this paper, we\nseek to understand whether ChatGPT has the potential to reproduce\nhuman-generated label annotations in social computing tasks. Such an\nachievement could significantly reduce the cost and complexity of social\ncomputing research. As such, we use ChatGPT to relabel five seminal datasets\ncovering stance detection (2x), sentiment analysis, hate speech, and bot\ndetection. Our results highlight that ChatGPT does have the potential to handle\nthese data annotation tasks, although a number of challenges remain. ChatGPT\nobtains an average accuracy 0.609. Performance is highest for the sentiment\nanalysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we\nshow that performance varies substantially across individual labels. We believe\nthis work can open up new lines of analysis and act as a basis for future\nresearch into the exploitation of ChatGPT for human annotation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.10145v2.pdf"
    },
    {
        "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
        "authors": [
            "Charvi Rastogi",
            "Marco Tulio Ribeiro",
            "Nicholas King",
            "Harsha Nori",
            "Saleema Amershi"
        ],
        "published": "2023-04-19T21:59:04Z",
        "summary": "Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.",
        "pdf_link": "https://arxiv.org/pdf/2304.09991v3.pdf"
    },
    {
        "title": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery",
        "authors": [
            "Lalithkumar Seenivasan",
            "Mobarakol Islam",
            "Gokul Kannan",
            "Hongliang Ren"
        ],
        "published": "2023-04-19T21:22:52Z",
        "summary": "Advances in GPT-based large language models (LLMs) are revolutionizing\nnatural language processing, exponentially increasing its use across various\ndomains. Incorporating uni-directional attention, these autoregressive LLMs can\ngenerate long and coherent paragraphs. However, for visual question answering\n(VQA) tasks that require both vision and language processing, models with\nbi-directional attention or models employing fusion techniques are often\nemployed to capture the context of multiple modalities all at once. As GPT does\nnot natively process vision tokens, to exploit the advancements in GPT models\nfor VQA in robotic surgery, we design an end-to-end trainable Language-Vision\nGPT (LV-GPT) model that expands the GPT2 model to include vision input (image).\nThe proposed LV-GPT incorporates a feature extractor (vision tokenizer) and\nvision token embedding (token type and pose). Given the limitations of\nunidirectional attention in GPT models and their ability to generate coherent\nlong paragraphs, we carefully sequence the word tokens before vision tokens,\nmimicking the human thought process of understanding the question to infer an\nanswer from an image. Quantitatively, we prove that the LV-GPT model\noutperforms other state-of-the-art VQA models on two publically available\nsurgical-VQA datasets (based on endoscopic vision challenge robotic scene\nsegmentation 2018 and CholecTriplet2021) and on our newly annotated dataset\n(based on the holistic surgical scene dataset). We further annotate all three\ndatasets to include question-type annotations to allow sub-type analysis.\nFurthermore, we extensively study and present the effects of token sequencing,\ntoken type and pose embedding for vision tokens in the LV-GPT model.",
        "pdf_link": "https://arxiv.org/pdf/2304.09974v2.pdf"
    },
    {
        "title": "Low-resource Bilingual Dialect Lexicon Induction with Large Language Models",
        "authors": [
            "Ekaterina Artemova",
            "Barbara Plank"
        ],
        "published": "2023-04-19T20:20:41Z",
        "summary": "Bilingual word lexicons are crucial tools for multilingual natural language\nunderstanding and machine translation tasks, as they facilitate the mapping of\nwords in one language to their synonyms in another language. To achieve this,\nnumerous papers have explored bilingual lexicon induction (BLI) in\nhigh-resource scenarios, using a typical pipeline consisting of two\nunsupervised steps: bitext mining and word alignment, both of which rely on\npre-trained large language models~(LLMs).\n  In this paper, we present an analysis of the BLI pipeline for German and two\nof its dialects, Bavarian and Alemannic. This setup poses several unique\nchallenges, including the scarcity of resources, the relatedness of the\nlanguages, and the lack of standardization in the orthography of dialects. To\nevaluate the BLI outputs, we analyze them with respect to word frequency and\npairwise edit distance. Additionally, we release two evaluation datasets\ncomprising 1,500 bilingual sentence pairs and 1,000 bilingual word pairs. They\nwere manually judged for their semantic similarity for each Bavarian-German and\nAlemannic-German language pair.",
        "pdf_link": "https://arxiv.org/pdf/2304.09957v1.pdf"
    },
    {
        "title": "Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers",
        "authors": [
            "Aishwarya Deep Shukla",
            "Laksh Agarwal",
            "Jie Mein",
            "Goh",
            "Guodong",
            "Gao",
            "Ritu Agarwal"
        ],
        "published": "2023-04-19T19:59:26Z",
        "summary": "The proliferation of fake reviews of doctors has potentially detrimental\nconsequences for patient well-being and has prompted concern among consumer\nprotection groups and regulatory bodies. Yet despite significant advancements\nin the fields of machine learning and natural language processing, there\nremains limited comprehension of the characteristics differentiating fraudulent\nfrom authentic reviews. This study utilizes a novel pre-labeled dataset of\n38048 physician reviews to establish the effectiveness of large language models\nin classifying reviews. Specifically, we compare the performance of traditional\nML models, such as logistic regression and support vector machines, to\ngenerative pre-trained transformer models. Furthermore, we use GPT4, the newest\nmodel in the GPT family, to uncover the key dimensions along which fake and\ngenuine physician reviews differ. Our findings reveal significantly superior\nperformance of GPT-3 over traditional ML models in this context. Additionally,\nour analysis suggests that GPT3 requires a smaller training sample than\ntraditional models, suggesting its appropriateness for tasks with scarce\ntraining data. Moreover, the superiority of GPT3 performance increases in the\ncold start context i.e., when there are no prior reviews of a doctor. Finally,\nwe employ GPT4 to reveal the crucial dimensions that distinguish fake physician\nreviews. In sharp contrast to previous findings in the literature that were\nobtained using simulated data, our findings from a real-world dataset show that\nfake reviews are generally more clinically detailed, more reserved in\nsentiment, and have better structure and grammar than authentic ones.",
        "pdf_link": "https://arxiv.org/pdf/2304.09948v1.pdf"
    },
    {
        "title": "Fundamental Limitations of Alignment in Large Language Models",
        "authors": [
            "Yotam Wolf",
            "Noam Wies",
            "Oshri Avnery",
            "Yoav Levine",
            "Amnon Shashua"
        ],
        "published": "2023-04-19T17:50:09Z",
        "summary": "An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that within the limits of this framework, for any\nbehavior that has a finite probability of being exhibited by the model, there\nexist prompts that can trigger the model into outputting this behavior, with\nprobability that increases with the length of the prompt. This implies that any\nalignment process that attenuates an undesired behavior but does not remove it\naltogether, is not safe against adversarial prompting attacks. Furthermore, our\nframework hints at the mechanism by which leading alignment approaches such as\nreinforcement learning from human feedback make the LLM prone to being prompted\ninto the undesired behaviors. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.",
        "pdf_link": "https://arxiv.org/pdf/2304.11082v5.pdf"
    },
    {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "published": "2023-04-19T17:47:47Z",
        "summary": "Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner. The project is available at\nhttps://chameleon-llm.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2304.09842v3.pdf"
    },
    {
        "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
        "authors": [
            "Qiao Jin",
            "Yifan Yang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "published": "2023-04-19T13:53:19Z",
        "summary": "While large language models (LLMs) have been successfully applied to various\ntasks, they still face challenges with hallucinations. Augmenting LLMs with\ndomain-specific tools such as database utilities can facilitate easier and more\nprecise access to specialized knowledge. In this paper, we present GeneGPT, a\nnovel method for teaching LLMs to use the Web APIs of the National Center for\nBiotechnology Information (NCBI) for answering genomics questions.\nSpecifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs\nby in-context learning and an augmented decoding algorithm that can detect and\nexecute API calls. Experimental results show that GeneGPT achieves\nstate-of-the-art performance on eight tasks in the GeneTuring benchmark with an\naverage score of 0.83, largely surpassing retrieval-augmented LLMs such as the\nnew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)\nAPI demonstrations have good cross-task generalizability and are more useful\nthan documentations for in-context learning; (2) GeneGPT can generalize to\nlonger chains of API calls and answer multi-hop questions in GeneHop, a novel\ndataset introduced in this work; (3) Different types of errors are enriched in\ndifferent tasks, providing valuable insights for future improvements.",
        "pdf_link": "https://arxiv.org/pdf/2304.09667v3.pdf"
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Pengjie Ren",
            "Zhumin Chen",
            "Dawei Yin",
            "Zhaochun Ren"
        ],
        "published": "2023-04-19T10:16:03Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable zero-shot\ngeneralization across various language-related tasks, including search engines.\nHowever, existing work utilizes the generative ability of LLMs for Information\nRetrieval (IR) rather than direct passage ranking. The discrepancy between the\npre-training objectives of LLMs and the ranking objective poses another\nchallenge. In this paper, we first investigate generative LLMs such as ChatGPT\nand GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal\nthat properly instructed LLMs can deliver competitive, even superior results to\nstate-of-the-art supervised methods on popular IR benchmarks. Furthermore, to\naddress concerns about data contamination of LLMs, we collect a new test set\ncalled NovelEval, based on the latest knowledge and aiming to verify the\nmodel's ability to rank unknown knowledge. Finally, to improve efficiency in\nreal-world applications, we delve into the potential for distilling the ranking\ncapabilities of ChatGPT into small specialized models using a permutation\ndistillation scheme. Our evaluation results turn out that a distilled 440M\nmodel outperforms a 3B supervised model on the BEIR benchmark. The code to\nreproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "pdf_link": "https://arxiv.org/pdf/2304.09542v2.pdf"
    },
    {
        "title": "A Theory on Adam Instability in Large-Scale Machine Learning",
        "authors": [
            "Igor Molybog",
            "Peter Albert",
            "Moya Chen",
            "Zachary DeVito",
            "David Esiobu",
            "Naman Goyal",
            "Punit Singh Koura",
            "Sharan Narang",
            "Andrew Poulton",
            "Ruan Silva",
            "Binh Tang",
            "Diana Liskovich",
            "Puxin Xu",
            "Yuchen Zhang",
            "Melanie Kambadur",
            "Stephen Roller",
            "Susan Zhang"
        ],
        "published": "2023-04-19T06:15:11Z",
        "summary": "We present a theory for the previously unexplained divergent behavior noticed\nin the training of large language models. We argue that the phenomenon is an\nartifact of the dominant optimization algorithm used for training, called Adam.\nWe observe that Adam can enter a state in which the parameter update vector has\na relatively large norm and is essentially uncorrelated with the direction of\ndescent on the training loss landscape, leading to divergence. This artifact is\nmore likely to be observed in the training of a deep model with a large batch\nsize, which is the typical setting of large-scale language model training. To\nargue the theory, we present observations from the training runs of the\nlanguage models of different scales: 7 billion, 30 billion, 65 billion, and 546\nbillion parameters.",
        "pdf_link": "https://arxiv.org/pdf/2304.09871v2.pdf"
    },
    {
        "title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes",
        "authors": [
            "Simran Arora",
            "Brandon Yang",
            "Sabri Eyuboglu",
            "Avanika Narayan",
            "Andrew Hojel",
            "Immanuel Trummer",
            "Christopher R\u00e9"
        ],
        "published": "2023-04-19T06:00:26Z",
        "summary": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.",
        "pdf_link": "https://arxiv.org/pdf/2304.09433v2.pdf"
    },
    {
        "title": "TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection",
        "authors": [
            "Quanjiang Guo",
            "Zhao Kang",
            "Ling Tian",
            "Zhouguo Chen"
        ],
        "published": "2023-04-19T04:47:36Z",
        "summary": "Fake news detection aims to detect fake news widely spreading on social media\nplatforms, which can negatively influence the public and the government. Many\napproaches have been developed to exploit relevant information from news\nimages, text, or videos. However, these methods may suffer from the following\nlimitations: (1) ignore the inherent emotional information of the news, which\ncould be beneficial since it contains the subjective intentions of the authors;\n(2) pay little attention to the relation (similarity) between the title and\ntextual information in news articles, which often use irrelevant title to\nattract reader' attention. To this end, we propose a novel Title-Text\nsimilarity and emotion-aware Fake news detection (TieFake) method by jointly\nmodeling the multi-modal context information and the author sentiment in a\nunified framework. Specifically, we respectively employ BERT and ResNeSt to\nlearn the representations for text and images, and utilize publisher emotion\nextractor to capture the author's subjective emotion in the news content. We\nalso propose a scale-dot product attention mechanism to capture the similarity\nbetween title features and textual features. Experiments are conducted on two\npublicly available multi-modal datasets, and the results demonstrate that our\nproposed method can significantly improve the performance of fake news\ndetection. Our code is available at https://github.com/UESTC-GQJ/TieFake.",
        "pdf_link": "https://arxiv.org/pdf/2304.09421v1.pdf"
    },
    {
        "title": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control",
        "authors": [
            "Jinjie Mai",
            "Jun Chen",
            "Bing Li",
            "Guocheng Qian",
            "Mohamed Elhoseiny",
            "Bernard Ghanem"
        ],
        "published": "2023-04-19T00:08:48Z",
        "summary": "Embodied AI focuses on the study and development of intelligent systems that\npossess a physical or virtual embodiment (i.e. robots) and are able to\ndynamically interact with their environment. Memory and control are the two\nessential parts of an embodied system and usually require separate frameworks\nto model each of them. In this paper, we propose a novel and generalizable\nframework called LLM-Brain: using Large-scale Language Model as a robotic brain\nto unify egocentric memory and control. The LLM-Brain framework integrates\nmultiple multimodal language models for robotic tasks, utilizing a zero-shot\nlearning approach. All components within LLM-Brain communicate using natural\nlanguage in closed-loop multi-round dialogues that encompass perception,\nplanning, control, and memory. The core of the system is an embodied LLM to\nmaintain egocentric memory and control the robot. We demonstrate LLM-Brain by\nexamining two downstream tasks: active exploration and embodied question\nanswering. The active exploration tasks require the robot to extensively\nexplore an unknown environment within a limited number of actions. Meanwhile,\nthe embodied question answering tasks necessitate that the robot answers\nquestions based on observations acquired during prior explorations.",
        "pdf_link": "https://arxiv.org/pdf/2304.09349v4.pdf"
    },
    {
        "title": "Creating Large Language Model Resistant Exams: Guidelines and Strategies",
        "authors": [
            "Simon kaare Larsen"
        ],
        "published": "2023-04-18T18:01:32Z",
        "summary": "The proliferation of Large Language Models (LLMs), such as ChatGPT, has\nraised concerns about their potential impact on academic integrity, prompting\nthe need for LLM-resistant exam designs. This article investigates the\nperformance of LLMs on exams and their implications for assessment, focusing on\nChatGPT's abilities and limitations. We propose guidelines for creating\nLLM-resistant exams, including content moderation, deliberate inaccuracies,\nreal-world scenarios beyond the model's knowledge base, effective distractor\noptions, evaluating soft skills, and incorporating non-textual information. The\narticle also highlights the significance of adapting assessments to modern\ntools and promoting essential skills development in students. By adopting these\nstrategies, educators can maintain academic integrity while ensuring that\nassessments accurately reflect contemporary professional settings and address\nthe challenges and opportunities posed by artificial intelligence in education.",
        "pdf_link": "https://arxiv.org/pdf/2304.12203v1.pdf"
    },
    {
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling",
        "authors": [
            "Xiuying Wei",
            "Yunchen Zhang",
            "Yuhang Li",
            "Xiangguo Zhang",
            "Ruihao Gong",
            "Jinyang Guo",
            "Xianglong Liu"
        ],
        "published": "2023-04-18T17:34:23Z",
        "summary": "Post-training quantization~(PTQ) of transformer language models faces\nsignificant challenges due to the existence of detrimental outliers in\nactivations. We observe that these outliers are concentrated in specific\nchannels and are asymmetric across channels. To address this issue, we propose\nthe Outlier Suppression+~(OS+) framework, which contains the channel-wise\nshifting for asymmetry and channel-wise scaling for concentration. We show that\nthese operations can be seamlessly migrated into subsequent modules while\nmaintaining equivalence. Second, we propose a fast and stable scheme to\ncalculate effective shifting and scaling values. The channel-wise shifting\naligns the center of each channel for removal of outlier asymmetry. The\nchannel-wise scaling quantitatively evaluates changes brought by migration and\nquantization for better quantization burden balance. We validate our OS+ under\nboth standard and fine-grained quantization settings with models including\nBERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks\ndemonstrate the superiority of our approach. Especially, with standard\nquantization, OS+ can achieve near-floating-point performance on both small\nmodels and large language models on 8-bit and 6-bit. Besides, we establish a\nnew state-of-the-art for 4-bit BERT with 15.5\\% improvement. Our code is\navailable at \\url{https://github.com/ModelTC/Outlier_Suppression_Plus}.",
        "pdf_link": "https://arxiv.org/pdf/2304.09145v3.pdf"
    },
    {
        "title": "Towards Designing a ChatGPT Conversational Companion for Elderly People",
        "authors": [
            "Abeer Alessa",
            "Hend Al-Khalifa"
        ],
        "published": "2023-04-18T17:24:14Z",
        "summary": "Loneliness and social isolation are serious and widespread problems among\nolder people, affecting their physical and mental health, quality of life, and\nlongevity. In this paper, we propose a ChatGPT-based conversational companion\nsystem for elderly people. The system is designed to provide companionship and\nhelp reduce feelings of loneliness and social isolation. The system was\nevaluated with a preliminary study. The results showed that the system was able\nto generate responses that were relevant to the created elderly personas.\nHowever, it is essential to acknowledge the limitations of ChatGPT, such as\npotential biases and misinformation, and to consider the ethical implications\nof using AI-based companionship for the elderly, including privacy concerns.",
        "pdf_link": "https://arxiv.org/pdf/2304.09866v1.pdf"
    },
    {
        "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
        "authors": [
            "Zihao Wu",
            "Lu Zhang",
            "Chao Cao",
            "Xiaowei Yu",
            "Haixing Dai",
            "Chong Ma",
            "Zhengliang Liu",
            "Lin Zhao",
            "Gang Li",
            "Wei Liu",
            "Quanzheng Li",
            "Dinggang Shen",
            "Xiang Li",
            "Dajiang Zhu",
            "Tianming Liu"
        ],
        "published": "2023-04-18T17:21:48Z",
        "summary": "Recently, ChatGPT and GPT-4 have emerged and gained immense global attention\ndue to their unparalleled performance in language processing. Despite\ndemonstrating impressive capability in various open-domain tasks, their\nadequacy in highly specific fields like radiology remains untested. Radiology\npresents unique linguistic phenomena distinct from open-domain data due to its\nspecificity and complexity. Assessing the performance of large language models\n(LLMs) in such specific domains is crucial not only for a thorough evaluation\nof their overall performance but also for providing valuable insights into\nfuture model design directions: whether model design should be generic or\ndomain-specific. To this end, in this study, we evaluate the performance of\nChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned\nspecifically on task-related data samples. We also conduct a comprehensive\ninvestigation on ChatGPT/GPT-4's reasoning ability by introducing varying\nlevels of inference difficulty. Our results show that 1) GPT-4 outperforms\nChatGPT in the radiology NLI task; 2) other specifically fine-tuned models\nrequire significant amounts of data samples to achieve comparable performance\nto ChatGPT/GPT-4. These findings demonstrate that constructing a generic model\nthat is capable of solving various tasks across different domains is feasible.",
        "pdf_link": "https://arxiv.org/pdf/2304.09138v1.pdf"
    },
    {
        "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT",
        "authors": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-04-18T13:20:45Z",
        "summary": "The way users acquire information is undergoing a paradigm shift with the\nadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves\nknowledge from the model itself and generates answers for users. ChatGPT's\nimpressive question-answering (QA) capability has attracted more than 100\nmillion users within a short period of time but has also raised concerns\nregarding its reliability. In this paper, we perform the first large-scale\nmeasurement of ChatGPT's reliability in the generic QA scenario with a\ncarefully curated set of 5,695 questions across ten datasets and eight domains.\nWe find that ChatGPT's reliability varies across different domains, especially\nunderperforming in law and science questions. We also demonstrate that system\nroles, originally designed by OpenAI to allow users to steer ChatGPT's\nbehavior, can impact ChatGPT's reliability in an imperceptible way. We further\nshow that ChatGPT is vulnerable to adversarial examples, and even a single\ncharacter change can negatively affect its reliability in certain cases. We\nbelieve that our study provides valuable insights into ChatGPT's reliability\nand underscores the need for strengthening the reliability and security of\nlarge language models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2304.08979v2.pdf"
    },
    {
        "title": "Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs",
        "authors": [
            "Da Silva Gameiro Henrique",
            "Andrei Kucharavy",
            "Rachid Guerraoui"
        ],
        "published": "2023-04-18T13:05:01Z",
        "summary": "The self-attention revolution allowed generative language models to scale and\nachieve increasingly impressive abilities. Such models - commonly referred to\nas Large Language Models (LLMs) - have recently gained prominence with the\ngeneral public, thanks to conversational fine-tuning, putting their behavior in\nline with public expectations regarding AI. This prominence amplified prior\nconcerns regarding the misuse of LLMs and led to the emergence of numerous\ntools to detect LLMs in the wild.\n  Unfortunately, most such tools are critically flawed. While major\npublications in the LLM detectability field suggested that LLMs were easy to\ndetect with fine-tuned autoencoders, the limitations of their results are easy\nto overlook. Specifically, they assumed publicly available generative models\nwithout fine-tunes or non-trivial prompts. While the importance of these\nassumptions has been demonstrated, until now, it remained unclear how well such\ndetection could be countered.\n  Here, we show that an attacker with access to such detectors' reference human\ntexts and output not only evades detection but can fully frustrate the detector\ntraining - with a reasonable budget and all its outputs labeled as such.\nAchieving it required combining common \"reinforcement from critic\" loss\nfunction modification and AdamW optimizer, which led to surprisingly good\nfine-tuning generalization. Finally, we warn against the temptation to\ntranspose the conclusions obtained in RNN-driven text GANs to LLMs due to their\nbetter representative ability.\n  These results have critical implications for the detection and prevention of\nmalicious use of generative language models, and we hope they will aid the\ndesigners of generative models and detectors.",
        "pdf_link": "https://arxiv.org/pdf/2304.08968v1.pdf"
    },
    {
        "title": "Safer Conversational AI as a Source of User Delight",
        "authors": [
            "Xiaoding Lu",
            "Aleksey Korshuk",
            "Zongyi Liu",
            "William Beauchamp",
            "Chai Research"
        ],
        "published": "2023-04-18T11:03:10Z",
        "summary": "This work explores the impact of moderation on users' enjoyment of\nconversational AI systems. While recent advancements in Large Language Models\n(LLMs) have led to highly capable conversational AIs that are increasingly\ndeployed in real-world settings, there is a growing concern over AI safety and\nthe need to moderate systems to encourage safe language and prevent harm.\nHowever, some users argue that current approaches to moderation limit the\ntechnology, compromise free expression, and limit the value delivered by the\ntechnology. This study takes an unbiased stance and shows that moderation does\nnot necessarily detract from user enjoyment. Heavy handed moderation does seem\nto have a nefarious effect, but models that are moderated to be safer can lead\nto a better user experience. By deploying various conversational AIs in the\nChai platform, the study finds that user retention can increase with a level of\nmoderation and safe system design. These results demonstrate the importance of\nappropriately defining safety in models in a way that is both responsible and\nfocused on serving users.",
        "pdf_link": "https://arxiv.org/pdf/2304.09865v1.pdf"
    },
    {
        "title": "Masked Language Model Based Textual Adversarial Example Detection",
        "authors": [
            "Xiaomei Zhang",
            "Zhaoxi Zhang",
            "Qi Zhong",
            "Xufei Zheng",
            "Yanjun Zhang",
            "Shengshan Hu",
            "Leo Yu Zhang"
        ],
        "published": "2023-04-18T06:52:14Z",
        "summary": "Adversarial attacks are a serious threat to the reliable deployment of\nmachine learning models in safety-critical applications. They can misguide\ncurrent models to predict incorrectly by slightly modifying the inputs.\nRecently, substantial work has shown that adversarial examples tend to deviate\nfrom the underlying data manifold of normal examples, whereas pre-trained\nmasked language models can fit the manifold of normal NLP data. To explore how\nto use the masked language model in adversarial detection, we propose a novel\ntextual adversarial example detection method, namely Masked Language\nModel-based Detection (MLMD), which can produce clearly distinguishable signals\nbetween normal examples and adversarial examples by exploring the changes in\nmanifolds induced by the masked language model. MLMD features a plug and play\nusage (i.e., no need to retrain the victim model) for adversarial defense and\nit is agnostic to classification tasks, victim model's architectures, and\nto-be-defended attack methods. We evaluate MLMD on various benchmark textual\ndatasets, widely studied machine learning models, and state-of-the-art (SOTA)\nadversarial attacks (in total $3*4*4 = 48$ settings). Experimental results show\nthat MLMD can achieve strong performance, with detection accuracy up to 0.984,\n0.967, and 0.901 on AG-NEWS, IMDB, and SST-2 datasets, respectively.\nAdditionally, MLMD is superior, or at least comparable to, the SOTA detection\ndefenses in detection accuracy and F1 score. Among many defenses based on the\noff-manifold assumption of adversarial examples, this work offers a new angle\nfor capturing the manifold change. The code for this work is openly accessible\nat \\url{https://github.com/mlmddetection/MLMDdetection}.",
        "pdf_link": "https://arxiv.org/pdf/2304.08767v3.pdf"
    },
    {
        "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Zheheng Luo",
            "Benyou Wang",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-18T06:38:40Z",
        "summary": "The exponential growth of biomedical texts such as biomedical literature and\nelectronic health records (EHRs), poses a significant challenge for clinicians\nand researchers to access clinical information efficiently. To tackle this\nchallenge, biomedical text summarization (BTS) has been proposed as a solution\nto support clinical information retrieval and management. BTS aims at\ngenerating concise summaries that distill key information from single or\nmultiple biomedical documents. In recent years, the rapid advancement of\nfundamental natural language processing (NLP) techniques, from pre-trained\nlanguage models (PLMs) to large language models (LLMs), has greatly facilitated\nthe progress of BTS. This growth has led to numerous proposed summarization\nmethods, datasets, and evaluation metrics, raising the need for a comprehensive\nand up-to-date survey for BTS. In this paper, we present a systematic review of\nrecent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to\nLLMs, to help understand the latest progress, challenges, and future\ndirections. We begin by introducing the foundational concepts of BTS, PLMs and\nLLMs, followed by an in-depth review of available datasets, recent approaches,\nand evaluation metrics in BTS. We finally discuss existing challenges and\npromising future directions in the era of LLMs. To facilitate the research\ncommunity, we line up open resources including available datasets, recent\napproaches, codes, evaluation metrics, and the leaderboard in a public project:\nhttps://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We\nbelieve that this survey will be a useful resource to researchers, allowing\nthem to quickly track recent advancements and provide guidelines for future BTS\nresearch within the research community.",
        "pdf_link": "https://arxiv.org/pdf/2304.08763v2.pdf"
    },
    {
        "title": "CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models",
        "authors": [
            "Tianhao Li",
            "Sandesh Shetty",
            "Advaith Kamath",
            "Ajay Jaiswal",
            "Xianqian Jiang",
            "Ying Ding",
            "Yejin Kim"
        ],
        "published": "2023-04-18T02:49:53Z",
        "summary": "Large pre-trained language models (LLMs) have been shown to have significant\npotential in few-shot learning across various fields, even with minimal\ntraining data. However, their ability to generalize to unseen tasks in more\ncomplex fields, such as biology, has yet to be fully evaluated. LLMs can offer\na promising alternative approach for biological inference, particularly in\ncases where structured data and sample size are limited, by extracting prior\nknowledge from text corpora. Our proposed few-shot learning approach uses LLMs\nto predict the synergy of drug pairs in rare tissues that lack structured data\nand features. Our experiments, which involved seven rare tissues from different\ncancer types, demonstrated that the LLM-based prediction model achieved\nsignificant accuracy with very few or zero samples. Our proposed model, the\nCancerGPT (with $\\sim$ 124M parameters), was even comparable to the larger\nfine-tuned GPT-3 model (with $\\sim$ 175B parameters). Our research is the first\nto tackle drug pair synergy prediction in rare tissues with limited data. We\nare also the first to utilize an LLM-based prediction model for biological\nreaction prediction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.10946v1.pdf"
    },
    {
        "title": "Large Language Models Based Automatic Synthesis of Software Specifications",
        "authors": [
            "Shantanu Mandal",
            "Adhrik Chethan",
            "Vahid Janfaza",
            "S M Farabi Mahmud",
            "Todd A Anderson",
            "Javier Turek",
            "Jesmin Jahan Tithi",
            "Abdullah Muzahid"
        ],
        "published": "2023-04-18T01:22:44Z",
        "summary": "Software configurations play a crucial role in determining the behavior of\nsoftware systems. In order to ensure safe and error-free operation, it is\nnecessary to identify the correct configuration, along with their valid bounds\nand rules, which are commonly referred to as software specifications. As\nsoftware systems grow in complexity and scale, the number of configurations and\nassociated specifications required to ensure the correct operation can become\nlarge and prohibitively difficult to manipulate manually. Due to the fast pace\nof software development, it is often the case that correct software\nspecifications are not thoroughly checked or validated within the software\nitself. Rather, they are frequently discussed and documented in a variety of\nexternal sources, including software manuals, code comments, and online\ndiscussion forums. Therefore, it is hard for the system administrator to know\nthe correct specifications of configurations due to the lack of clarity,\norganization, and a centralized unified source to look at. To address this\nchallenge, we propose SpecSyn a framework that leverages a state-of-the-art\nlarge language model to automatically synthesize software specifications from\nnatural language sources. Our approach formulates software specification\nsynthesis as a sequence-to-sequence learning problem and investigates the\nextraction of specifications from large contextual texts. This is the first\nwork that uses a large language model for end-to-end specification synthesis\nfrom natural language texts. Empirical results demonstrate that our system\noutperforms prior the state-of-the-art specification synthesis tool by 21% in\nterms of F1 score and can find specifications from single as well as multiple\nsentences.",
        "pdf_link": "https://arxiv.org/pdf/2304.09181v1.pdf"
    },
    {
        "title": "Classification of US Supreme Court Cases using BERT-Based Techniques",
        "authors": [
            "Shubham Vatsal",
            "Adam Meyers",
            "John E. Ortega"
        ],
        "published": "2023-04-17T22:53:54Z",
        "summary": "Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.",
        "pdf_link": "https://arxiv.org/pdf/2304.08649v3.pdf"
    },
    {
        "title": "An Evaluation on Large Language Model Outputs: Discourse and Memorization",
        "authors": [
            "Adrian de Wynter",
            "Xun Wang",
            "Alex Sokolov",
            "Qilong Gu",
            "Si-Qing Chen"
        ],
        "published": "2023-04-17T22:12:12Z",
        "summary": "We present an empirical evaluation of various outputs generated by nine of\nthe most widely-available large language models (LLMs). Our analysis is done\nwith off-the-shelf, readily-available tools. We find a correlation between\npercentage of memorized text, percentage of unique text, and overall output\nquality, when measured with respect to output pathologies such as\ncounterfactual and logically-flawed statements, and general failures like not\nstaying on topic. Overall, 80.0% of the outputs evaluated contained memorized\ndata, but outputs containing the most memorized content were also more likely\nto be considered of high quality. We discuss and evaluate mitigation\nstrategies, showing that, in the models evaluated, the rate of memorized text\nbeing output is reduced. We conclude with a discussion on potential\nimplications around what it means to learn, to memorize, and to evaluate\nquality text.",
        "pdf_link": "https://arxiv.org/pdf/2304.08637v1.pdf"
    },
    {
        "title": "Visual Instruction Tuning",
        "authors": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "published": "2023-04-17T17:59:25Z",
        "summary": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2304.08485v2.pdf"
    },
    {
        "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
        "authors": [
            "Abdullatif K\u00f6ksal",
            "Timo Schick",
            "Anna Korhonen",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-04-17T17:36:35Z",
        "summary": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. Finally, our models can\neffectively follow and answer multilingual instructions; we demonstrate this\nfor news generation. We publicly release our data and models:\nhttps://github.com/akoksal/LongForm.",
        "pdf_link": "https://arxiv.org/pdf/2304.08460v2.pdf"
    },
    {
        "title": "ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT",
        "authors": [
            "Chong Ma",
            "Zihao Wu",
            "Jiaqi Wang",
            "Shaochen Xu",
            "Yaonai Wei",
            "Zhengliang Liu",
            "Xi Jiang",
            "Lei Guo",
            "Xiaoyan Cai",
            "Shu Zhang",
            "Tuo Zhang",
            "Dajiang Zhu",
            "Dinggang Shen",
            "Tianming Liu",
            "Xiang Li"
        ],
        "published": "2023-04-17T17:13:42Z",
        "summary": "The 'Impression' section of a radiology report is a critical basis for\ncommunication between radiologists and other physicians, and it is typically\nwritten by radiologists based on the 'Findings' section. However, writing\nnumerous impressions can be laborious and error-prone for radiologists.\nAlthough recent studies have achieved promising results in automatic impression\ngeneration using large-scale medical text data for pre-training and fine-tuning\npre-trained language models, such models often require substantial amounts of\nmedical text data and have poor generalization performance. While large\nlanguage models (LLMs) like ChatGPT have shown strong generalization\ncapabilities and performance, their performance in specific domains, such as\nradiology, remains under-investigated and potentially limited. To address this\nlimitation, we propose ImpressionGPT, which leverages the in-context learning\ncapability of LLMs by constructing dynamic contexts using domain-specific,\nindividualized data. This dynamic prompt approach enables the model to learn\ncontextual knowledge from semantically similar examples from existing data.\nAdditionally, we design an iterative optimization algorithm that performs\nautomatic evaluation on the generated impression results and composes the\ncorresponding instruction prompts to further optimize the model. The proposed\nImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and\nOpenI datasets without requiring additional training data or fine-tuning the\nLLMs. This work presents a paradigm for localizing LLMs that can be applied in\na wide range of similar application scenarios, bridging the gap between\ngeneral-purpose LLMs and the specific language processing needs of various\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2304.08448v2.pdf"
    },
    {
        "title": "Low-code LLM: Graphical User Interface over Large Language Models",
        "authors": [
            "Yuzhe Cai",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Zehua Wang",
            "Yaobo Liang",
            "Tao Ge",
            "Chenfei Wu",
            "Wang You",
            "Ting Song",
            "Yan Xia",
            "Jonathan Tien",
            "Nan Duan",
            "Furu Wei"
        ],
        "published": "2023-04-17T09:27:40Z",
        "summary": "Utilizing Large Language Models (LLMs) for complex tasks is challenging,\noften involving a time-consuming and uncontrollable prompt engineering process.\nThis paper introduces a novel human-LLM interaction framework, Low-code LLM. It\nincorporates six types of simple low-code visual programming interactions to\nachieve more controllable and stable responses. Through visual interaction with\na graphical user interface, users can incorporate their ideas into the process\nwithout writing trivial prompts. The proposed Low-code LLM framework consists\nof a Planning LLM that designs a structured planning workflow for complex\ntasks, which can be correspondingly edited and confirmed by users through\nlow-code visual programming operations, and an Executing LLM that generates\nresponses following the user-confirmed workflow. We highlight three advantages\nof the low-code LLM: user-friendly interaction, controllable generation, and\nwide applicability. We demonstrate its benefits using four typical\napplications. By introducing this framework, we aim to bridge the gap between\nhumans and LLMs, enabling more effective and efficient utilization of LLMs for\ncomplex tasks. The code, prompts, and experimental details are available at\nhttps://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM. A system\ndemonstration video can be found at\nhttps://www.youtube.com/watch?v=jb2C1vaeO3E.",
        "pdf_link": "https://arxiv.org/pdf/2304.08103v3.pdf"
    },
    {
        "title": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction",
        "authors": [
            "Xiao Wang",
            "Weikang Zhou",
            "Can Zu",
            "Han Xia",
            "Tianze Chen",
            "Yuansen Zhang",
            "Rui Zheng",
            "Junjie Ye",
            "Qi Zhang",
            "Tao Gui",
            "Jihua Kang",
            "Jingsheng Yang",
            "Siyuan Li",
            "Chunsai Du"
        ],
        "published": "2023-04-17T09:00:50Z",
        "summary": "Large language models have unlocked strong multi-task capabilities from\nreading instructive prompts. However, recent studies have shown that existing\nlarge models still have difficulty with information extraction tasks. For\nexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,\nwhich is significantly lower than the state-of-the-art performance. In this\npaper, we propose InstructUIE, a unified information extraction framework based\non instruction tuning, which can uniformly model various information extraction\ntasks and capture the inter-task dependency. To validate the proposed method,\nwe introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction\ndatasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable\nperformance to Bert in supervised settings and significantly outperforms the\nstate-of-the-art and gpt3.5 in zero-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.08085v1.pdf"
    },
    {
        "title": "SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model",
        "authors": [
            "Nan Li",
            "Bo Kang",
            "Tijl De Bie"
        ],
        "published": "2023-04-17T08:43:20Z",
        "summary": "We present SkillGPT, a tool for skill extraction and standardization (SES)\nfrom free-style job descriptions and user profiles with an open-source Large\nLanguage Model (LLM) as backbone. Most previous methods for similar tasks\neither need supervision or rely on heavy data-preprocessing and feature\nengineering. Directly prompting the latest conversational LLM for standard\nskills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes\na LLM to perform its tasks in steps via summarization and vector similarity\nsearch, to balance speed with precision. The backbone LLM of SkillGPT is based\non Llama, free for academic use and thus useful for exploratory research and\nprototype development. Hence, our cost-free SkillGPT gives users the\nconvenience of conversational SES, efficiently and reliably.",
        "pdf_link": "https://arxiv.org/pdf/2304.11060v2.pdf"
    },
    {
        "title": "Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding",
        "authors": [
            "Ziang Xiao",
            "Xingdi Yuan",
            "Q. Vera Liao",
            "Rania Abdelghani",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2023-04-17T04:52:43Z",
        "summary": "Qualitative analysis of textual contents unpacks rich and valuable\ninformation by assigning labels to the data. However, this process is often\nlabor-intensive, particularly when working with large datasets. While recent\nAI-based tools demonstrate utility, researchers may not have readily available\nAI resources and expertise, let alone be challenged by the limited\ngeneralizability of those task-specific models. In this study, we explored the\nuse of large language models (LLMs) in supporting deductive coding, a major\ncategory of qualitative analysis where researchers use pre-determined codebooks\nto label the data into a fixed set of codes. Instead of training task-specific\nmodels, a pre-trained LLM could be used directly for various tasks without\nfine-tuning through prompt learning. Using a curiosity-driven questions coding\ntask as a case study, we found, by combining GPT-3 with expert-drafted\ncodebooks, our proposed approach achieved fair to substantial agreements with\nexpert-coded results. We lay out challenges and opportunities in using LLMs to\nsupport qualitative coding and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2304.10548v1.pdf"
    },
    {
        "title": "Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark",
        "authors": [
            "Michael V. Reiss"
        ],
        "published": "2023-04-17T00:41:19Z",
        "summary": "Recent studies have demonstrated promising potential of ChatGPT for various\ntext annotation and classification tasks. However, ChatGPT is non-deterministic\nwhich means that, as with human coders, identical input can lead to different\noutputs. Given this, it seems appropriate to test the reliability of ChatGPT.\nTherefore, this study investigates the consistency of ChatGPT's zero-shot\ncapabilities for text annotation and classification, focusing on different\nmodel parameters, prompt variations, and repetitions of identical inputs. Based\non the real-world classification task of differentiating website texts into\nnews and not news, results show that consistency in ChatGPT's classification\noutput can fall short of scientific thresholds for reliability. For example,\neven minor wording alterations in prompts or repeating the identical input can\nlead to varying outputs. Although pooling outputs from multiple repetitions can\nimprove reliability, this study advises caution when using ChatGPT for\nzero-shot text annotation and underscores the need for thorough validation,\nsuch as comparison against human-annotated data. The unsupervised application\nof ChatGPT for text annotation and classification is not recommended.",
        "pdf_link": "https://arxiv.org/pdf/2304.11085v1.pdf"
    },
    {
        "title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping",
        "authors": [
            "Zheng Zhang",
            "Jie Gao",
            "Ranjodh Singh Dhaliwal",
            "Toby Jia-Jun Li"
        ],
        "published": "2023-04-16T15:29:03Z",
        "summary": "In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.",
        "pdf_link": "https://arxiv.org/pdf/2304.07810v2.pdf"
    },
    {
        "title": "A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions",
        "authors": [
            "Papa Abdou Karim Karou Diallo",
            "Samuel Reyd",
            "Amal Zouaq"
        ],
        "published": "2023-04-16T13:12:26Z",
        "summary": "In recent years, the field of neural machine translation (NMT) for SPARQL\nquery generation has witnessed significant growth. Incorporating the copy\nmechanism with traditional encoder-decoder architectures and using pre-trained\nencoder-decoders and large language models have set new performance benchmarks.\nThis paper presents various experiments that replicate and expand upon recent\nNMT-based SPARQL generation studies, comparing pre-trained language models\n(PLMs), non-pre-trained language models (NPLMs), and large language models\n(LLMs), highlighting the impact of question annotation and the copy mechanism\nand testing various fine-tuning methods using LLMs. In particular, we provide a\nsystematic error analysis of the models and test their generalization ability.\nOur study demonstrates that the copy mechanism yields significant performance\nenhancements for most PLMs and NPLMs. Annotating the data is pivotal to\ngenerating correct URIs, with the \"tag-within\" strategy emerging as the most\neffective approach. Additionally, our findings reveal that the primary source\nof errors stems from incorrect URIs in SPARQL queries that are sometimes\nreplaced with hallucinated URIs when using base models. This does not happen\nusing the copy mechanism, but it sometimes leads to selecting wrong URIs among\ncandidates. Finally, the performance of the tested LLMs fell short of achieving\nthe desired outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2304.07772v3.pdf"
    },
    {
        "title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
        "authors": [
            "Joy He-Yueya",
            "Gabriel Poesia",
            "Rose E. Wang",
            "Noah D. Goodman"
        ],
        "published": "2023-04-16T04:16:06Z",
        "summary": "Automatically generating high-quality step-by-step solutions to math word\nproblems has many applications in education. Recently, combining large language\nmodels (LLMs) with external tools to perform complex reasoning and calculation\nhas emerged as a promising direction for solving math word problems, but prior\napproaches such as Program-Aided Language model (PAL) are biased towards simple\nprocedural problems and less effective for problems that require declarative\nreasoning. We propose an approach that combines an LLM that can incrementally\nformalize word problems as a set of variables and equations with an external\nsymbolic solver that can solve the equations. Our approach achieves comparable\naccuracy to the original PAL on the GSM8K benchmark of math word problems and\noutperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more\nchallenging word problems extracted from Algebra textbooks. Our work highlights\nthe benefits of using declarative and incremental representations when\ninterfacing with an external tool for solving complex math word problems. Our\ndata and prompts are publicly available at\nhttps://github.com/joyheyueya/declarative-math-word-problem.",
        "pdf_link": "https://arxiv.org/pdf/2304.09102v1.pdf"
    },
    {
        "title": "Tractable Control for Autoregressive Language Generation",
        "authors": [
            "Honghua Zhang",
            "Meihua Dang",
            "Nanyun Peng",
            "Guy Van den Broeck"
        ],
        "published": "2023-04-15T00:19:44Z",
        "summary": "Despite the success of autoregressive large language models in text\ngeneration, it remains a major challenge to generate text that satisfies\ncomplex constraints: sampling from the conditional distribution\n${\\Pr}(\\text{text} | \\alpha)$ is intractable for even the simplest lexical\nconstraints $\\alpha$. To overcome this challenge, we propose to use tractable\nprobabilistic models (TPMs) to impose lexical constraints in autoregressive\ntext generation models, which we refer to as GeLaTo (Generating Language with\nTractable Constraints). To demonstrate the effectiveness of this framework, we\nuse distilled hidden Markov models, where we can efficiently compute\n${\\Pr}(\\text{text} | \\alpha)$, to guide autoregressive generation from GPT2.\nGeLaTo achieves state-of-the-art performance on challenging benchmarks for\nconstrained text generation (e.g., CommonGen), beating various strong baselines\nby a large margin. Our work not only opens up new avenues for controlling large\nlanguage models but also motivates the development of more expressive TPMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.07438v4.pdf"
    },
    {
        "title": "Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models",
        "authors": [
            "Danny M. den Hamer",
            "Perry Schoor",
            "Tobias B. Polak",
            "Daniel Kapitan"
        ],
        "published": "2023-04-14T21:19:46Z",
        "summary": "Physicians considering clinical trials for their patients are met with the\nlaborious process of checking many text based eligibility criteria. Large\nLanguage Models (LLMs) have shown to perform well for clinical information\nextraction and clinical reasoning, including medical tests, but not yet in\nreal-world scenarios. This paper investigates the use of InstructGPT to assist\nphysicians in determining eligibility for clinical trials based on a patient's\nsummarised medical profile. Using a prompting strategy combining one-shot,\nselection-inference and chain-of-thought techniques, we investigate the\nperformance of LLMs on 10 synthetically created patient profiles. Performance\nis evaluated at four levels: ability to identify screenable eligibility\ncriteria from a trial given a medical profile; ability to classify for each\nindividual criterion whether the patient qualifies; the overall classification\nwhether a patient is eligible for a clinical trial and the percentage of\ncriteria to be screened by physician. We evaluated against 146 clinical trials\nand a total of 4,135 eligibility criteria. The LLM was able to correctly\nidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,\n72% (341/471) of the screenable criteria were evaluated correctly. The\nresulting trial level classification as eligible or ineligible resulted in a\nrecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0\nand precision of 0.71 on clinical trial level can be achieved while reducing\nthe amount of criteria to be checked by an estimated 90%. LLMs can be used to\nassist physicians with pre-screening of patients for clinical trials. By\nforcing instruction-tuned LLMs to produce chain-of-thought responses, the\nreasoning can be made transparent to and the decision process becomes amenable\nby physicians, thereby making such a system feasible for use in real-world\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.07396v2.pdf"
    },
    {
        "title": "The Self-Perception and Political Biases of ChatGPT",
        "authors": [
            "J\u00e9r\u00f4me Rutinowski",
            "Sven Franke",
            "Jan Endendyk",
            "Ina Dormuth",
            "Markus Pauly"
        ],
        "published": "2023-04-14T18:06:13Z",
        "summary": "This contribution analyzes the self-perception and political biases of\nOpenAI's Large Language Model ChatGPT. Taking into account the first\nsmall-scale reports and studies that have emerged, claiming that ChatGPT is\npolitically biased towards progressive and libertarian points of view, this\ncontribution aims to provide further clarity on this subject. For this purpose,\nChatGPT was asked to answer the questions posed by the political compass test\nas well as similar questionnaires that are specific to the respective politics\nof the G7 member states. These eight tests were repeated ten times each and\nrevealed that ChatGPT seems to hold a bias towards progressive views. The\npolitical compass test revealed a bias towards progressive and libertarian\nviews, with the average coordinates on the political compass being (-6.48,\n-5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes\nranging from -10 to 10), supporting the claims of prior research. The political\nquestionnaires for the G7 member states indicated a bias towards progressive\nviews but no significant bias between authoritarian and libertarian views,\ncontradicting the findings of prior reports, with the average coordinates being\n(-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested\nusing the OCEAN test and its personality type was queried using the\nMyers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT\nwas evaluated using the Dark Factor test. These three tests were also repeated\nten times each, revealing that ChatGPT perceives itself as highly open and\nagreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of\ntest-takers with the least pronounced dark traits.",
        "pdf_link": "https://arxiv.org/pdf/2304.07333v1.pdf"
    },
    {
        "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
        "authors": [
            "Andreas K\u00f6pf",
            "Yannic Kilcher",
            "Dimitri von R\u00fctte",
            "Sotiris Anagnostidis",
            "Zhi-Rui Tam",
            "Keith Stevens",
            "Abdullah Barhoum",
            "Nguyen Minh Duc",
            "Oliver Stanley",
            "Rich\u00e1rd Nagyfi",
            "Shahul ES",
            "Sameer Suri",
            "David Glushkov",
            "Arnav Dantuluri",
            "Andrew Maguire",
            "Christoph Schuhmann",
            "Huu Nguyen",
            "Alexander Mattick"
        ],
        "published": "2023-04-14T18:01:29Z",
        "summary": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.",
        "pdf_link": "https://arxiv.org/pdf/2304.07327v2.pdf"
    },
    {
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
        "authors": [
            "Minghao Li",
            "Yingxiu Zhao",
            "Bowen Yu",
            "Feifan Song",
            "Hangyu Li",
            "Haiyang Yu",
            "Zhoujun Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-04-14T14:05:32Z",
        "summary": "Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question.",
        "pdf_link": "https://arxiv.org/pdf/2304.08244v2.pdf"
    },
    {
        "title": "DroidBot-GPT: GPT-powered UI Automation for Android",
        "authors": [
            "Hao Wen",
            "Hongming Wang",
            "Jiaxuan Liu",
            "Yuanchun Li"
        ],
        "published": "2023-04-14T11:31:56Z",
        "summary": "This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large\nlanguage models (LLMs) to automate the interactions with Android mobile\napplications. Given a natural language description of a desired task,\nDroidBot-GPT can automatically generate and execute actions that navigate the\napp to complete the task. It works by translating the app GUI state information\nand the available actions on the smartphone screen to natural language prompts\nand asking the LLM to make a choice of actions. Since the LLM is typically\ntrained on a large amount of data including the how-to manuals of diverse\nsoftware applications, it has the ability to make reasonable choices of actions\nbased on the provided information. We evaluate DroidBot-GPT with a self-created\ndataset that contains 33 tasks collected from 17 Android applications spanning\n10 categories. It can successfully complete 39.39% of the tasks, and the\naverage partial completion progress is about 66.76%. Given the fact that our\nmethod is fully unsupervised (no modification required from both the app and\nthe LLM), we believe there is great potential to enhance automation performance\nwith better app development paradigms and/or custom model training.",
        "pdf_link": "https://arxiv.org/pdf/2304.07061v5.pdf"
    },
    {
        "title": "MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data",
        "authors": [
            "Tianyu Han",
            "Lisa C. Adams",
            "Jens-Michalis Papaioannou",
            "Paul Grundmann",
            "Tom Oberhauser",
            "Alexander L\u00f6ser",
            "Daniel Truhn",
            "Keno K. Bressem"
        ],
        "published": "2023-04-14T11:28:08Z",
        "summary": "As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.",
        "pdf_link": "https://arxiv.org/pdf/2304.08247v2.pdf"
    },
    {
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
        "authors": [
            "Haochun Wang",
            "Chi Liu",
            "Nuwa Xi",
            "Zewen Qiang",
            "Sendong Zhao",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-04-14T07:54:17Z",
        "summary": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
        "pdf_link": "https://arxiv.org/pdf/2304.06975v1.pdf"
    },
    {
        "title": "nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales",
        "authors": [
            "Yiqun Yao",
            "Siqi fan",
            "Xiusheng Huang",
            "Xuezhi Fang",
            "Xiang Li",
            "Ziyi Ni",
            "Xin Jiang",
            "Xuying Meng",
            "Peng Han",
            "Shuo Shang",
            "Kang Liu",
            "Aixin Sun",
            "Yequan Wang"
        ],
        "published": "2023-04-14T00:45:01Z",
        "summary": "As language models scale up, it becomes increasingly expensive to verify\nresearch ideas because conclusions on small models do not trivially transfer to\nlarge ones. A possible solution is to establish a generic system that\naccurately predicts certain metrics for large models without training them.\nExisting scaling laws require hyperparameter search on the largest models,\nlimiting their predicative capability. In this paper, we present an approach\n(namely {\\mu}Scaling) to predict the pre-training loss, based on our\nobservations that Maximal Update Parametrization ({\\mu}P) enables accurate\nfitting of scaling laws close to common loss basins in hyperparameter space.\nWith {\\mu}Scaling, different model designs can be compared on large scales by\ntraining only their smaller counterparts. Further, we introduce nanoLM: an\naffordable LLM pre-training benchmark that facilitates this new research\nparadigm. With around 14% of the one-time pre-training cost, we can accurately\nforecast the loss for models up to 52B. Our goal with nanoLM is to empower\nresearchers with limited resources to reach meaningful conclusions on large\nmodels. We also aspire for our benchmark to serve as a bridge between the\nacademic community and the industry. Code for {\\mu}Scaling is available at\nhttps://github.com/cofe-ai/Mu-scaling. Code for nanoLLM will be available\nlater.",
        "pdf_link": "https://arxiv.org/pdf/2304.06875v4.pdf"
    },
    {
        "title": "Stochastic Code Generation",
        "authors": [
            "Swapnil Sharma",
            "Nikita Anand",
            "Kranthi Kiran G. V"
        ],
        "published": "2023-04-14T00:01:05Z",
        "summary": "Large language models pre-trained for code generation can generate\nhigh-quality short code but often struggle with generating coherent long code\nand understanding higher-level or system-level specifications. This issue is\nalso observed in language modeling for long text generation, and one proposed\nsolution is the use of a latent stochastic process. This approach involves\ngenerating a document plan and then producing text that is consistent with it.\n  In this study, we investigate whether this technique can be applied to code\ngeneration to improve coherence. We base our proposed encoder and decoder on\nthe pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for\ntraining. We evaluate our results using the HumanEval benchmark and observe\nthat the modified Time Control model performs similarly to CodeParrot on this\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2304.08243v1.pdf"
    },
    {
        "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
        "authors": [
            "Swapnil Sharma",
            "Nikita Anand",
            "Kranthi Kiran G. V.",
            "Alind Jain"
        ],
        "published": "2023-04-13T23:29:58Z",
        "summary": "Large pre-trained language models are widely used in the community. These\nmodels are usually trained on unmoderated and unfiltered data from open sources\nlike the Internet. Due to this, biases that we see in platforms online which\nare a reflection of those in society are in turn captured and learned by these\nmodels. These models are deployed in applications that affect millions of\npeople and their inherent biases are harmful to the targeted social groups. In\nthis work, we study the general trend in bias reduction as newer pre-trained\nmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)\nare chosen and evaluated against two bias benchmarks, StereoSet and\nCrowS-Pairs. They are compared to the baseline of BERT using the associated\nmetrics. We explore whether as advancements are made and newer, faster, lighter\nmodels are released: are they being developed responsibly such that their\ninherent social biases have been reduced compared to their older counterparts?\nThe results are compiled and we find that all the models under study do exhibit\nbiases but have generally improved as compared to BERT.",
        "pdf_link": "https://arxiv.org/pdf/2304.06861v1.pdf"
    },
    {
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "authors": [
            "Toufique Ahmed",
            "Kunal Suresh Pai",
            "Premkumar Devanbu",
            "Earl T. Barr"
        ],
        "published": "2023-04-13T20:49:35Z",
        "summary": "Large Language Models (LLM) are a new class of computation engines,\n\"programmed\" via prompt engineering. We are still learning how to best\n\"program\" these LLMs to help developers. We start with the intuition that\ndevelopers tend to consciously and unconsciously have a collection of semantics\nfacts in mind when working on coding tasks. Mostly these are shallow, simple\nfacts arising from a quick read. For a function, examples of facts might\ninclude parameter and local variable names, return expressions, simple pre- and\npost-conditions, and basic control and data flow, etc.\n  One might assume that the powerful multi-layer architecture of\ntransformer-style LLMs makes them inherently capable of doing this simple level\nof \"code analysis\" and extracting such information, implicitly, while\nprocessing code: but are they, really? If they aren't, could explicitly adding\nthis information help? Our goal here is to investigate this question, using the\ncode summarization task and evaluate whether automatically augmenting an LLM's\nprompt with semantic facts explicitly, actually helps.\n  Prior work shows that LLM performance on code summarization benefits from\nfew-shot samples drawn either from the same-project or from examples found via\ninformation retrieval methods (such as BM25). While summarization performance\nhas steadily increased since the early days, there is still room for\nimprovement: LLM performance on code summarization still lags its performance\non natural-language tasks like translation and text summarization.\n  We find that adding semantic facts actually does help! This approach improves\nperformance in several different settings suggested by prior work, including\nfor two different Large Language Models. In most cases, improvement nears or\nexceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,\nthis augmentation actually yields performance surpassing 30 BLEU.",
        "pdf_link": "https://arxiv.org/pdf/2304.06815v3.pdf"
    },
    {
        "title": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence",
        "authors": [
            "Gengchen Mai",
            "Weiming Huang",
            "Jin Sun",
            "Suhang Song",
            "Deepak Mishra",
            "Ninghao Liu",
            "Song Gao",
            "Tianming Liu",
            "Gao Cong",
            "Yingjie Hu",
            "Chris Cundy",
            "Ziyuan Li",
            "Rui Zhu",
            "Ni Lao"
        ],
        "published": "2023-04-13T19:50:17Z",
        "summary": "Large pre-trained models, also known as foundation models (FMs), are trained\nin a task-agnostic manner on large-scale data and can be adapted to a wide\nrange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.\nDespite their successes in language and vision tasks, we have yet seen an\nattempt to develop foundation models for geospatial artificial intelligence\n(GeoAI). In this work, we explore the promises and challenges of developing\nmultimodal foundation models for GeoAI. We first investigate the potential of\nmany existing FMs by testing their performances on seven tasks across multiple\ngeospatial subdomains including Geospatial Semantics, Health Geography, Urban\nGeography, and Remote Sensing. Our results indicate that on several geospatial\ntasks that only involve text modality such as toponym recognition, location\ndescription recognition, and US state-level/county-level dementia time series\nforecasting, these task-agnostic LLMs can outperform task-specific\nfully-supervised models in a zero-shot or few-shot learning setting. However,\non other geospatial tasks, especially tasks that involve multiple data\nmodalities (e.g., POI-based urban function classification, street view\nimage-based urban noise intensity classification, and remote sensing image\nscene classification), existing foundation models still underperform\ntask-specific models. Based on these observations, we propose that one of the\nmajor challenges of developing a FM for GeoAI is to address the multimodality\nnature of geospatial tasks. After discussing the distinct challenges of each\ngeospatial data modality, we suggest the possibility of a multimodal foundation\nmodel which can reason over various types of geospatial data through geospatial\nalignments. We conclude this paper by discussing the unique risks and\nchallenges to develop such a model for GeoAI.",
        "pdf_link": "https://arxiv.org/pdf/2304.06798v1.pdf"
    },
    {
        "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
        "authors": [
            "Aleksandar Shtedritski",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "published": "2023-04-13T17:58:08Z",
        "summary": "Large-scale Vision-Language Models, such as CLIP, learn powerful image-text\nrepresentations that have found numerous applications, from zero-shot\nclassification to text-to-image generation. Despite that, their capabilities\nfor solving novel discriminative tasks via prompting fall behind those of large\nlanguage models, such as GPT-3. Here we explore the idea of visual prompt\nengineering for solving computer vision tasks beyond classification by editing\nin image space instead of text. In particular, we discover an emergent ability\nof CLIP, where, by simply drawing a red circle around an object, we can direct\nthe model's attention to that region, while also maintaining global\ninformation. We show the power of this simple approach by achieving\nstate-of-the-art in zero-shot referring expressions comprehension and strong\nperformance in keypoint localization tasks. Finally, we draw attention to some\npotential ethical concerns of large language-vision models.",
        "pdf_link": "https://arxiv.org/pdf/2304.06712v2.pdf"
    },
    {
        "title": "Verbs in Action: Improving verb understanding in video-language models",
        "authors": [
            "Liliane Momeni",
            "Mathilde Caron",
            "Arsha Nagrani",
            "Andrew Zisserman",
            "Cordelia Schmid"
        ],
        "published": "2023-04-13T17:57:01Z",
        "summary": "Understanding verbs is crucial to modelling how people and objects interact\nwith each other and the environment through space and time. Recently,\nstate-of-the-art video-language models based on CLIP have been shown to have\nlimited verb understanding and to rely extensively on nouns, restricting their\nperformance in real-world video applications that require action and temporal\nunderstanding. In this work, we improve verb understanding for CLIP-based\nvideo-language models by proposing a new Verb-Focused Contrastive (VFC)\nframework. This consists of two main components: (1) leveraging pretrained\nlarge language models (LLMs) to create hard negatives for cross-modal\ncontrastive learning, together with a calibration strategy to balance the\noccurrence of concepts in positive and negative pairs; and (2) enforcing a\nfine-grained, verb phrase alignment loss. Our method achieves state-of-the-art\nresults for zero-shot performance on three downstream tasks that focus on verb\nunderstanding: video-text matching, video question-answering and video\nclassification. To the best of our knowledge, this is the first work which\nproposes a method to alleviate the verb understanding problem, and does not\nsimply highlight it.",
        "pdf_link": "https://arxiv.org/pdf/2304.06708v1.pdf"
    },
    {
        "title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",
        "authors": [
            "Sunder Ali Khowaja",
            "Parus Khuwaja",
            "Kapal Dev",
            "Weizheng Wang",
            "Lewis Nkenyereye"
        ],
        "published": "2023-04-13T16:01:28Z",
        "summary": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.",
        "pdf_link": "https://arxiv.org/pdf/2305.03123v3.pdf"
    },
    {
        "title": "Are LLMs All You Need for Task-Oriented Dialogue?",
        "authors": [
            "Vojt\u011bch Hude\u010dek",
            "Ond\u0159ej Du\u0161ek"
        ],
        "published": "2023-04-13T14:03:14Z",
        "summary": "Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.",
        "pdf_link": "https://arxiv.org/pdf/2304.06556v2.pdf"
    },
    {
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
        "authors": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "published": "2023-04-13T09:39:30Z",
        "summary": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.",
        "pdf_link": "https://arxiv.org/pdf/2304.06364v2.pdf"
    },
    {
        "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
        "authors": [
            "Jielin Qiu",
            "Jiacheng Zhu",
            "Shiqi Liu",
            "William Han",
            "Jingqi Zhang",
            "Chaojing Duan",
            "Michael Rosenberg",
            "Emerson Liu",
            "Douglas Weber",
            "Ding Zhao"
        ],
        "published": "2023-04-13T06:32:25Z",
        "summary": "Automated interpretation of electrocardiograms (ECG) has garnered significant\nattention with the advancements in machine learning methodologies. Despite the\ngrowing interest, most current studies focus solely on classification or\nregression tasks, which overlook a crucial aspect of clinical cardio-disease\ndiagnosis: the diagnostic report generated by experienced human clinicians. In\nthis paper, we introduce a novel approach to ECG interpretation, leveraging\nrecent breakthroughs in Large Language Models (LLMs) and Vision-Transformer\n(ViT) models. Rather than treating ECG diagnosis as a classification or\nregression task, we propose an alternative method of automatically identifying\nthe most similar clinical cases based on the input ECG data. Also, since\ninterpreting ECG as images is more affordable and accessible, we process ECG as\nencoded images and adopt a vision-language learning paradigm to jointly learn\nvision-language alignment between encoded ECG images and ECG diagnosis reports.\nEncoding ECG into images can result in an efficient ECG retrieval system, which\nwill be highly practical and useful in clinical applications. More importantly,\nour findings could serve as a crucial resource for providing diagnostic\nservices in underdeveloped regions.",
        "pdf_link": "https://arxiv.org/pdf/2304.06286v3.pdf"
    },
    {
        "title": "Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Zhenchang Xing",
            "Jon Whittle"
        ],
        "published": "2023-04-13T05:01:03Z",
        "summary": "The release of ChatGPT has drawn huge interests on foundations models. There\nis a broad consensus that foundations models will be the fundamental building\nblocks for future AI systems. However, there is a lack of systematic guidance\non the architecture design. Particularly, the the rapidly growing capabilities\nof foundations models can eventually absorb other components of AI systems,\nposing challenges of moving boundary and interface evolution in architecture\ndesign. Furthermore, incorporating foundations models into AI systems raises\nsignificant concerns about responsible AI due to their opaque nature and\nrapidly advancing intelligence. To address these challenges, the paper first\npresents an architecture evolution of AI systems in the era of foundation\nmodels, transitioning from \"foundation-model-as-a-connector\" to\n\"foundation-model-as-a-monolithic architecture\". The paper then identifies key\ndesign decisions and proposes a pattern-oriented reference architecture for\ndesigning responsible foundation-model-based systems. The patterns can enable\nthe potential of foundation models while minimising associated risks.",
        "pdf_link": "https://arxiv.org/pdf/2304.11090v3.pdf"
    },
    {
        "title": "Language Instructed Reinforcement Learning for Human-AI Coordination",
        "authors": [
            "Hengyuan Hu",
            "Dorsa Sadigh"
        ],
        "published": "2023-04-13T04:47:31Z",
        "summary": "One of the fundamental quests of AI is to produce agents that coordinate well\nwith humans. This problem is challenging, especially in domains that lack high\nquality human behavioral data, because multi-agent reinforcement learning (RL)\noften converges to different equilibria from the ones that humans prefer. We\npropose a novel framework, instructRL, that enables humans to specify what kind\nof strategies they expect from their AI partners through natural language\ninstructions. We use pretrained large language models to generate a prior\npolicy conditioned on the human instruction and use the prior to regularize the\nRL objective. This leads to the RL agent converging to equilibria that are\naligned with human preferences. We show that instructRL converges to human-like\npolicies that satisfy the given instructions in a proof-of-concept environment\nas well as the challenging Hanabi benchmark. Finally, we show that knowing the\nlanguage instruction significantly boosts human-AI coordination performance in\nhuman evaluations in Hanabi.",
        "pdf_link": "https://arxiv.org/pdf/2304.07297v2.pdf"
    },
    {
        "title": "Detection of Fake Generated Scientific Abstracts",
        "authors": [
            "Panagiotis C. Theocharopoulos",
            "Panagiotis Anagnostou",
            "Anastasia Tsoukala",
            "Spiros V. Georgakopoulos",
            "Sotiris K. Tasoulis",
            "Vassilis P. Plagianakos"
        ],
        "published": "2023-04-12T20:20:22Z",
        "summary": "The widespread adoption of Large Language Models and publicly available\nChatGPT has marked a significant turning point in the integration of Artificial\nIntelligence into people's everyday lives. The academic community has taken\nnotice of these technological advancements and has expressed concerns regarding\nthe difficulty of discriminating between what is real and what is artificially\ngenerated. Thus, researchers have been working on developing effective systems\nto identify machine-generated text. In this study, we utilize the GPT-3 model\nto generate scientific paper abstracts through Artificial Intelligence and\nexplore various text representation methods when combined with Machine Learning\nmodels with the aim of identifying machine-written text. We analyze the models'\nperformance and address several research questions that rise during the\nanalysis of the results. By conducting this research, we shed light on the\ncapabilities and limitations of Artificial Intelligence generated text.",
        "pdf_link": "https://arxiv.org/pdf/2304.06148v1.pdf"
    },
    {
        "title": "Can Large Language Models Transform Computational Social Science?",
        "authors": [
            "Caleb Ziems",
            "William Held",
            "Omar Shaikh",
            "Jiaao Chen",
            "Zhehao Zhang",
            "Diyi Yang"
        ],
        "published": "2023-04-12T17:33:28Z",
        "summary": "Large Language Models (LLMs) are capable of successfully performing many\nlanguage processing tasks zero-shot (without training data). If zero-shot LLMs\ncan also reliably classify and explain social phenomena like persuasiveness and\npolitical ideology, then LLMs could augment the Computational Social Science\n(CSS) pipeline in important ways. This work provides a road map for using LLMs\nas CSS tools. Towards this end, we contribute a set of prompting best practices\nand an extensive evaluation pipeline to measure the zero-shot performance of 13\nlanguage models on 25 representative English CSS benchmarks. On taxonomic\nlabeling tasks (classification), LLMs fail to outperform the best fine-tuned\nmodels but still achieve fair levels of agreement with humans. On free-form\ncoding tasks (generation), LLMs produce explanations that often exceed the\nquality of crowdworkers' gold references. We conclude that the performance of\ntoday's LLMs can augment the CSS research pipeline in two ways: (1) serving as\nzero-shot data annotators on human annotation teams, and (2) bootstrapping\nchallenging creative generation tasks (e.g., explaining the underlying\nattributes of a text). In summary, LLMs are posed to meaningfully participate\nin social science analysis in partnership with humans.",
        "pdf_link": "https://arxiv.org/pdf/2305.03514v3.pdf"
    },
    {
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
        "authors": [
            "Viet Dac Lai",
            "Nghia Trung Ngo",
            "Amir Pouran Ben Veyseh",
            "Hieu Man",
            "Franck Dernoncourt",
            "Trung Bui",
            "Thien Huu Nguyen"
        ],
        "published": "2023-04-12T05:08:52Z",
        "summary": "Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.",
        "pdf_link": "https://arxiv.org/pdf/2304.05613v1.pdf"
    },
    {
        "title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
        "authors": [
            "Cheng Zhang",
            "Stefan Bauer",
            "Paul Bennett",
            "Jiangfeng Gao",
            "Wenbo Gong",
            "Agrin Hilmkil",
            "Joel Jennings",
            "Chao Ma",
            "Tom Minka",
            "Nick Pawlowski",
            "James Vaughan"
        ],
        "published": "2023-04-11T22:30:03Z",
        "summary": "We assess the ability of large language models (LLMs) to answer causal\nquestions by analyzing their strengths and weaknesses against three types of\ncausal question. We believe that current LLMs can answer causal questions with\nexisting causal knowledge as combined domain experts. However, they are not yet\nable to provide satisfactory answers for discovering new knowledge or for\nhigh-stakes decision-making tasks with high precision. We discuss possible\nfuture directions and opportunities, such as enabling explicit and implicit\ncausal modules as well as deep causal-aware LLMs. These will not only enable\nLLMs to answer many different types of causal questions for greater impact but\nalso enable LLMs to be more trustworthy and efficient in general.",
        "pdf_link": "https://arxiv.org/pdf/2304.05524v1.pdf"
    },
    {
        "title": "Training Large Language Models Efficiently with Sparsity and Dataflow",
        "authors": [
            "Venkat Srinivasan",
            "Darshan Gandhi",
            "Urmish Thakker",
            "Raghu Prabhakar"
        ],
        "published": "2023-04-11T21:37:13Z",
        "summary": "Large foundation language models have shown their versatility in being able\nto be adapted to perform a wide variety of downstream tasks, such as text\ngeneration, sentiment analysis, semantic search etc. However, training such\nlarge foundational models is a non-trivial exercise that requires a significant\namount of compute power and expertise from machine learning and systems\nexperts. As models get larger, these demands are only increasing. Sparsity is a\npromising technique to relieve the compute requirements for training. However,\nsparsity introduces new challenges in training the sparse model to the same\nquality as the dense counterparts. Furthermore, sparsity drops the operation\nintensity and introduces irregular memory access patterns that makes it\nchallenging to efficiently utilize compute resources. This paper demonstrates\nan end-to-end training flow on a large language model - 13 billion GPT - using\nsparsity and dataflow. The dataflow execution model and architecture enables\nefficient on-chip irregular memory accesses as well as native kernel fusion and\npipelined parallelism that helps recover device utilization. We show that we\ncan successfully train GPT 13B to the same quality as the dense GPT 13B model,\nwhile achieving an end-end speedup of 4.5x over dense A100 baseline.",
        "pdf_link": "https://arxiv.org/pdf/2304.05511v1.pdf"
    },
    {
        "title": "chatClimate: Grounding Conversational AI in Climate Science",
        "authors": [
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Veruska Muccione",
            "Jingwei Ni",
            "Mathias Kraus",
            "Julia Bingler",
            "Tobias Schimanski",
            "Chiara Colesanti-Senni",
            "Nicolas Webersinke",
            "Christrian Huggel",
            "Markus Leippold"
        ],
        "published": "2023-04-11T21:31:39Z",
        "summary": "Large Language Models (LLMs) have made significant progress in recent years,\nachieving remarkable results in question-answering tasks (QA). However, they\nstill face two major challenges: hallucination and outdated information after\nthe training phase. These challenges take center stage in critical domains like\nclimate change, where obtaining accurate and up-to-date information from\nreliable sources in a limited time is essential and difficult. To overcome\nthese barriers, one potential solution is to provide LLMs with access to\nexternal, scientifically accurate, and robust sources (long-term memory) to\ncontinuously update their knowledge and prevent the propagation of inaccurate,\nincorrect, or outdated information. In this study, we enhanced GPT-4 by\nintegrating the information from the Sixth Assessment Report of the\nIntergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable\nsource in this domain. We present our conversational AI prototype, available at\nwww.chatclimate.ai and demonstrate its ability to answer challenging questions\naccurately in three different QA scenarios: asking from 1) GPT-4, 2)\nchatClimate, and 3) hybrid chatClimate. The answers and their sources were\nevaluated by our team of IPCC authors, who used their expert knowledge to score\nthe accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation\nshowed that the hybrid chatClimate provided more accurate answers, highlighting\nthe effectiveness of our solution. This approach can be easily scaled for\nchatbots in specific domains, enabling the delivery of reliable and accurate\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2304.05510v2.pdf"
    },
    {
        "title": "Zero-shot Temporal Relation Extraction with ChatGPT",
        "authors": [
            "Chenhan Yuan",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-11T18:59:05Z",
        "summary": "The goal of temporal relation extraction is to infer the temporal relation\nbetween two events in the document. Supervised models are dominant in this\ntask. In this work, we investigate ChatGPT's ability on zero-shot temporal\nrelation extraction. We designed three different prompt techniques to break\ndown the task and evaluate ChatGPT. Our experiments show that ChatGPT's\nperformance has a large gap with that of supervised methods and can heavily\nrely on the design of prompts. We further demonstrate that ChatGPT can infer\nmore small relation classes correctly than supervised methods. The current\nshortcomings of ChatGPT on temporal relation extraction are also discussed in\nthis paper. We found that ChatGPT cannot keep consistency during temporal\ninference and it fails in actively long-dependency temporal inference.",
        "pdf_link": "https://arxiv.org/pdf/2304.05454v1.pdf"
    },
    {
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
        "authors": [
            "Ameet Deshpande",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Karthik Narasimhan"
        ],
        "published": "2023-04-11T16:53:54Z",
        "summary": "Large language models (LLMs) have shown incredible capabilities and\ntranscended the natural language processing (NLP) community, with adoption\nthroughout many services like healthcare, therapy, education, and customer\nservice. Since users include people with critical information needs like\nstudents or patients engaging with chatbots, the safety of these systems is of\nprime importance. Therefore, a clear understanding of the capabilities and\nlimitations of LLMs is necessary. To this end, we systematically evaluate\ntoxicity in over half a million generations of ChatGPT, a popular\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\nincreases the toxicity of generations. Depending on the persona assigned to\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\nfind concerning patterns where specific entities (e.g., certain races) are\ntargeted more than others (3x more) irrespective of the assigned persona, that\nreflect inherent discriminatory biases in the model. We hope that our findings\ninspire the broader AI community to rethink the efficacy of current safety\nguardrails and develop better techniques that lead to robust, safe, and\ntrustworthy AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2304.05335v1.pdf"
    },
    {
        "title": "TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning",
        "authors": [
            "William Won",
            "Midhilesh Elavazhagan",
            "Sudarshan Srinivasan",
            "Ajaya Durg",
            "Samvit Kaul",
            "Swati Gupta",
            "Tushar Krishna"
        ],
        "published": "2023-04-11T15:50:54Z",
        "summary": "The surge of artificial intelligence, specifically large language models, has\nled to a rapid advent towards the development of large-scale machine learning\ntraining clusters. Collective communications within these clusters tend to be\nheavily bandwidth-bound, necessitating techniques to optimally utilize the\navailable network bandwidth. This puts the routing algorithm for the collective\nat the forefront of determining the performance. Unfortunately, communication\nlibraries used in distributed machine learning today are limited by a fixed set\nof routing algorithms. This constraints collective performance within the\ndomain of next-generation training clusters that employ intricate,\nheterogeneous, and asymmetric, large-scale topologies. Further, the emergence\nof irregular topologies attributed to runtime phenomena such as device failures\nserves to compound the complexity of the challenge. To this end, this paper\nintroduces TACOS, an automated synthesizer that generates topology-aware\ncollective algorithms for common distributed machine learning collectives\nacross arbitrary input network topologies. TACOS was able to synthesize\nAll-Reduce algorithm for a heterogeneous 512-NPU system in just 6.09 minutes\nwhile achieving performance improvement up to 4.27x over state-of-the-art prior\nwork. TACOS exhibits high scalability, with synthesis time scaling\nquadratically with the number of NPUs. In contrast to prior works' NP-hard\napproaches, TACOS with 40K NPUs completes in 2.52 hours.",
        "pdf_link": "https://arxiv.org/pdf/2304.05301v2.pdf"
    },
    {
        "title": "Approximating Online Human Evaluation of Social Chatbots with Prompting",
        "authors": [
            "Ekaterina Svikhnushina",
            "Pearl Pu"
        ],
        "published": "2023-04-11T14:45:01Z",
        "summary": "As conversational models become increasingly available to the general public,\nusers are engaging with this technology in social interactions. Such\nunprecedented interaction experiences may pose considerable social and\npsychological risks to the users unless the technology is properly controlled.\nThis highlights the need for scalable and robust evaluation metrics for\nconversational chatbots. Existing evaluation metrics aim to automate offline\nuser evaluation and approximate human judgment of pre-curated dialogs. However,\nthey are limited in their ability to capture subjective perceptions of users\nwho actually interact with the bots and might not generalize to real-world\nsettings. To address this limitation, we propose an approach to approximate\nonline human evaluation leveraging large language models (LLMs) from the GPT\nfamily. We introduce a new Dialog system Evaluation framework based on\nPrompting (DEP), which enables a fully automatic evaluation pipeline that\nreplicates live user studies and achieves an impressive correlation with human\njudgment (up to Pearson r=0.95 on a system level). The DEP approach involves\ncollecting synthetic chat logs of evaluated bots with an LLM in the other-play\nsetting, where the LLM is carefully conditioned to follow a specific scenario.\nWe further explore different prompting approaches to produce evaluation scores\nwith the same LLM. The best performing prompts, which contain few-shot\ndemonstrations and instructions, show outstanding performance on the tested\ndataset and demonstrate the ability to generalize to other dialog corpora.",
        "pdf_link": "https://arxiv.org/pdf/2304.05253v2.pdf"
    },
    {
        "title": "Towards preserving word order importance through Forced Invalidation",
        "authors": [
            "Hadeel Al-Negheimish",
            "Pranava Madhyastha",
            "Alessandra Russo"
        ],
        "published": "2023-04-11T13:42:10Z",
        "summary": "Large pre-trained language models such as BERT have been widely used as a\nframework for natural language understanding (NLU) tasks. However, recent\nfindings have revealed that pre-trained language models are insensitive to word\norder. The performance on NLU tasks remains unchanged even after randomly\npermuting the word of a sentence, where crucial syntactic information is\ndestroyed. To help preserve the importance of word order, we propose a simple\napproach called Forced Invalidation (FI): forcing the model to identify\npermuted sequences as invalid samples. We perform an extensive evaluation of\nour approach on various English NLU and QA based tasks over BERT-based and\nattention-based models over word embeddings. Our experiments demonstrate that\nForced Invalidation significantly improves the sensitivity of the models to\nword order.",
        "pdf_link": "https://arxiv.org/pdf/2304.05221v1.pdf"
    },
    {
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
        "authors": [
            "Haoran Li",
            "Dadi Guo",
            "Wei Fan",
            "Mingshi Xu",
            "Jie Huang",
            "Fanpu Meng",
            "Yangqiu Song"
        ],
        "published": "2023-04-11T13:05:04Z",
        "summary": "With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.",
        "pdf_link": "https://arxiv.org/pdf/2304.05197v3.pdf"
    },
    {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": [
            "Xinyun Chen",
            "Maxwell Lin",
            "Nathanael Sch\u00e4rli",
            "Denny Zhou"
        ],
        "published": "2023-04-11T10:43:43Z",
        "summary": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.",
        "pdf_link": "https://arxiv.org/pdf/2304.05128v2.pdf"
    },
    {
        "title": "Human-machine cooperation for semantic feature listing",
        "authors": [
            "Kushin Mukherjee",
            "Siddharth Suresh",
            "Timothy T. Rogers"
        ],
        "published": "2023-04-11T06:38:04Z",
        "summary": "Semantic feature norms, lists of features that concepts do and do not\npossess, have played a central role in characterizing human conceptual\nknowledge, but require extensive human labor. Large language models (LLMs)\noffer a novel avenue for the automatic generation of such feature lists, but\nare prone to significant error. Here, we present a new method for combining a\nlearned model of human lexical-semantics from limited data with LLM-generated\ndata to efficiently generate high-quality feature norms.",
        "pdf_link": "https://arxiv.org/pdf/2304.05012v1.pdf"
    },
    {
        "title": "Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection",
        "authors": [
            "Luoxuan Weng",
            "Minfeng Zhu",
            "Kam Kwai Wong",
            "Shi Liu",
            "Jiashun Sun",
            "Hang Zhu",
            "Dongming Han",
            "Wei Chen"
        ],
        "published": "2023-04-11T06:37:30Z",
        "summary": "Large language models (LLMs) have gained popularity in various fields for\ntheir exceptional capability of generating human-like text. Their potential\nmisuse has raised social concerns about plagiarism in academic contexts.\nHowever, effective artificial scientific text detection is a non-trivial task\ndue to several challenges, including 1) the lack of a clear understanding of\nthe differences between machine-generated and human-written scientific text, 2)\nthe poor generalization performance of existing methods caused by\nout-of-distribution issues, and 3) the limited support for human-machine\ncollaboration with sufficient interpretability during the detection process. In\nthis paper, we first identify the critical distinctions between\nmachine-generated and human-written scientific text through a quantitative\nexperiment. Then, we propose a mixed-initiative workflow that combines human\nexperts' prior knowledge with machine intelligence, along with a visual\nanalytics prototype to facilitate efficient and trustworthy scientific text\ndetection. Finally, we demonstrate the effectiveness of our approach through\ntwo case studies and a controlled user study with proficient researchers. We\nalso provide design implications for interactive artificial text detection\ntools in high-stakes decision-making scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.05011v1.pdf"
    },
    {
        "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
        "authors": [
            "Jiaao Chen",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola",
            "Diyi Yang"
        ],
        "published": "2023-04-10T17:58:42Z",
        "summary": "Diffusion models that are based on iterative denoising have been recently\nproposed and leveraged in various generation tasks like image generation.\nWhereas, as a way inherently built for continuous data, existing diffusion\nmodels still have some limitations in modeling discrete data, e.g., languages.\nFor example, the generally used Gaussian noise can not handle the discrete\ncorruption well, and the objectives in continuous spaces fail to be stable for\ntextual data in the diffusion process especially when the dimension is high. To\nalleviate these issues, we introduce a novel diffusion model for language\nmodeling, Masked-Diffuse LM, with lower training cost and better performances,\ninspired by linguistic features in languages. Specifically, we design a\nlinguistic-informed forward process which adds corruptions to the text through\nstrategically soft-masking to better noise the textual data. Also, we directly\npredict the categorical distribution with cross-entropy loss function in every\ndiffusion step to connect the continuous space and discrete space in a more\nefficient and straightforward way. Through experiments on 5 controlled\ngeneration tasks, we demonstrate that our Masked-Diffuse LM can achieve better\ngeneration quality than the state-of-the-art diffusion models with better\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2304.04746v1.pdf"
    },
    {
        "title": "On the Possibilities of AI-Generated Text Detection",
        "authors": [
            "Souradip Chakraborty",
            "Amrit Singh Bedi",
            "Sicheng Zhu",
            "Bang An",
            "Dinesh Manocha",
            "Furong Huang"
        ],
        "published": "2023-04-10T17:47:39Z",
        "summary": "Our work addresses the critical issue of distinguishing text generated by\nLarge Language Models (LLMs) from human-produced text, a task essential for\nnumerous applications. Despite ongoing debate about the feasibility of such\ndifferentiation, we present evidence supporting its consistent achievability,\nexcept when human and machine text distributions are indistinguishable across\ntheir entire support. Drawing from information theory, we argue that as\nmachine-generated text approximates human-like quality, the sample size needed\nfor detection increases. We establish precise sample complexity bounds for\ndetecting AI-generated text, laying groundwork for future research aimed at\ndeveloping advanced, multi-sample detectors. Our empirical evaluations across\nmultiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the\nviability of enhanced detection methods. We test various state-of-the-art text\ngenerators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and\nLlama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector,\nGPTZero. Our findings align with OpenAI's empirical data related to sequence\nlength, marking the first theoretical substantiation for these observations.",
        "pdf_link": "https://arxiv.org/pdf/2304.04736v3.pdf"
    },
    {
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
        "authors": [
            "Wenhao Zhu",
            "Hongyi Liu",
            "Qingxiu Dong",
            "Jingjing Xu",
            "Shujian Huang",
            "Lingpeng Kong",
            "Jiajun Chen",
            "Lei Li"
        ],
        "published": "2023-04-10T15:51:30Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nhandling multilingual machine translation (MMT). In this paper, we\nsystematically investigate the advantages and challenges of LLMs for MMT by\nanswering two questions: 1) How well do LLMs perform in translating massive\nlanguages? 2) Which factors affect LLMs' performance in translation? We\nthoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our\nempirical results show that translation capabilities of LLMs are continually\nimproving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of\ntranslation directions but still faces a large gap towards the commercial\ntranslation system, especially on low-resource languages. Through further\nanalysis, we discover that LLMs exhibit new working patterns when used for MMT.\nFirst, instruction semantics can surprisingly be ignored when given in-context\nexemplars. Second, cross-lingual exemplars can provide better task guidance for\nlow-resource translation than exemplars in the same language pairs. Third, LLM\ncan acquire translation ability in a resource-efficient way and generate\nmoderate translation even on zero-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2304.04675v3.pdf"
    },
    {
        "title": "Learnings from Data Integration for Augmented Language Models",
        "authors": [
            "Alon Halevy",
            "Jane Dwivedi-Yu"
        ],
        "published": "2023-04-10T13:28:35Z",
        "summary": "One of the limitations of large language models is that they do not have\naccess to up-to-date, proprietary or personal data. As a result, there are\nmultiple efforts to extend language models with techniques for accessing\nexternal data. In that sense, LLMs share the vision of data integration systems\nwhose goal is to provide seamless access to a large collection of heterogeneous\ndata sources. While the details and the techniques of LLMs differ greatly from\nthose of data integration, this paper shows that some of the lessons learned\nfrom research on data integration can elucidate the research path we are\nconducting today on language models.",
        "pdf_link": "https://arxiv.org/pdf/2304.04576v1.pdf"
    },
    {
        "title": "Inference with Reference: Lossless Acceleration of Large Language Models",
        "authors": [
            "Nan Yang",
            "Tao Ge",
            "Liang Wang",
            "Binxing Jiao",
            "Daxin Jiang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023-04-10T09:55:14Z",
        "summary": "We propose LLMA, an LLM accelerator to losslessly speed up Large Language\nModel (LLM) inference with references. LLMA is motivated by the observation\nthat there are abundant identical text spans between the decoding result by an\nLLM and the reference that is available in many real world scenarios (e.g.,\nretrieved documents). LLMA first selects a text span from the reference and\ncopies its tokens to the decoder and then efficiently checks the tokens'\nappropriateness as the decoding result in parallel within one decoding step.\nThe improved computational parallelism allows LLMA to achieve over 2x speed-up\nfor LLMs with identical generation results as greedy decoding in many practical\ngeneration scenarios where significant overlap between in-context reference and\noutputs exists (e.g., search engines and multi-turn conversations).",
        "pdf_link": "https://arxiv.org/pdf/2304.04487v1.pdf"
    },
    {
        "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
        "authors": [
            "Jiawei Zhang"
        ],
        "published": "2023-04-10T05:25:54Z",
        "summary": "In this paper, we aim to develop a large language model (LLM) with the\nreasoning ability on complex graph data. Currently, LLMs have achieved very\nimpressive performance on various natural language learning tasks, extensions\nof which have also been applied to study the vision tasks with multi-modal\ndata. However, when it comes to the graph learning tasks, existing LLMs present\nvery serious flaws due to their several inherited weaknesses in performing\n{multi-step logic reasoning}, {precise mathematical calculation} and\n{perception about the spatial and temporal factors}.\n  To address such challenges, in this paper, we will investigate the\nprinciples, methodologies and algorithms to empower existing LLMs with graph\nreasoning ability, which will have tremendous impacts on the current research\nof both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer\nmodels, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)\nframework to teach LLMs themselves with prompts augmented by ChatGPT to use\nexternal graph reasoning API tools. Specifically, we will investigate to teach\nGraph-ToolFormer to handle various graph data reasoning tasks in this paper,\nincluding both (1) very basic graph data loading and graph property reasoning\ntasks, ranging from simple graph order and size to the graph diameter and\nperiphery, and (2) more advanced reasoning tasks on real-world graph data, such\nas bibliographic networks, protein molecules, sequential recommender systems,\nsocial networks and knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2304.11116v3.pdf"
    },
    {
        "title": "The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Yanzhao Lai",
            "Min Peng",
            "Jimin Huang"
        ],
        "published": "2023-04-10T04:31:00Z",
        "summary": "Recently, large language models (LLMs) like ChatGPT have demonstrated\nremarkable performance across a variety of natural language processing tasks.\nHowever, their effectiveness in the financial domain, specifically in\npredicting stock market movements, remains to be explored. In this paper, we\nconduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal\nstock movement prediction, on three tweets and historical stock price datasets.\nOur findings indicate that ChatGPT is a \"Wall Street Neophyte\" with limited\nsuccess in predicting stock movements, as it underperforms not only\nstate-of-the-art methods but also traditional methods like linear regression\nusing price features. Despite the potential of Chain-of-Thought prompting\nstrategies and the inclusion of tweets, ChatGPT's performance remains subpar.\nFurthermore, we observe limitations in its explainability and stability,\nsuggesting the need for more specialized training or fine-tuning. This research\nprovides insights into ChatGPT's capabilities and serves as a foundation for\nfuture work aimed at improving financial market analysis and prediction by\nleveraging social media sentiment and historical stock data.",
        "pdf_link": "https://arxiv.org/pdf/2304.05351v2.pdf"
    },
    {
        "title": "OpenAGI: When LLM Meets Domain Experts",
        "authors": [
            "Yingqiang Ge",
            "Wenyue Hua",
            "Kai Mei",
            "Jianchao Ji",
            "Juntao Tan",
            "Shuyuan Xu",
            "Zelong Li",
            "Yongfeng Zhang"
        ],
        "published": "2023-04-10T03:55:35Z",
        "summary": "Human Intelligence (HI) excels at combining basic skills to solve complex\ntasks. This capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive AI Agents, enabling them to harness expert models for\ncomplex task-solving towards Artificial General Intelligence (AGI). Large\nLanguage Models (LLMs) show promising learning and reasoning abilities, and can\neffectively use external models, tools, plugins, or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research and\ndevelopment platform designed for solving multi-step, real-world tasks.\nSpecifically, OpenAGI uses a dual strategy, integrating standard benchmark\ntasks for benchmarking and evaluation, and open-ended tasks including more\nexpandable models, tools, plugins, or APIs for creative problem-solving. Tasks\nare presented as natural language queries to the LLM, which then selects and\nexecutes appropriate models. We also propose a Reinforcement Learning from Task\nFeedback (RLTF) mechanism that uses task results to improve the LLM's\ntask-solving ability, which creates a self-improving AI feedback loop. While we\nacknowledge that AGI is a broad and multifaceted research challenge with no\nsingularly defined solution path, the integration of LLMs with domain-specific\nexpert models, inspired by mirroring the blend of general and specialized\nintelligence in humans, offers a promising approach towards AGI. We are\nopen-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation\nmethods, and the UI demo to foster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.",
        "pdf_link": "https://arxiv.org/pdf/2304.04370v6.pdf"
    },
    {
        "title": "Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao",
            "Linda Petzold"
        ],
        "published": "2023-04-09T16:31:47Z",
        "summary": "Large language models (LLMs) have made significant progress in various\ndomains, including healthcare. However, the specialized nature of clinical\nlanguage understanding tasks presents unique challenges and limitations that\nwarrant further investigation. In this study, we conduct a comprehensive\nevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within\nthe realm of clinical language understanding tasks. These tasks span a diverse\nrange, including named entity recognition, relation extraction, natural\nlanguage inference, semantic textual similarity, document classification, and\nquestion-answering. We also introduce a novel prompting strategy,\nself-questioning prompting (SQP), tailored to enhance LLMs' performance by\neliciting informative questions and answers pertinent to the clinical scenarios\nat hand. Our evaluation underscores the significance of task-specific learning\nstrategies and prompting techniques for improving LLMs' effectiveness in\nhealthcare-related tasks. Additionally, our in-depth error analysis on the\nchallenging relation extraction task offers valuable insights into error\ndistribution and potential avenues for improvement using SQP. Our study sheds\nlight on the practical implications of employing LLMs in the specialized domain\nof healthcare, serving as a foundation for future research and the development\nof potential applications in healthcare settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.05368v3.pdf"
    },
    {
        "title": "A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding",
        "authors": [
            "Wenbo Pan",
            "Qiguang Chen",
            "Xiao Xu",
            "Wanxiang Che",
            "Libo Qin"
        ],
        "published": "2023-04-09T15:28:36Z",
        "summary": "Zero-shot dialogue understanding aims to enable dialogue to track the user's\nneeds without any training data, which has gained increasing attention. In this\nwork, we investigate the understanding ability of ChatGPT for zero-shot\ndialogue understanding tasks including spoken language understanding (SLU) and\ndialogue state tracking (DST). Experimental results on four popular benchmarks\nreveal the great potential of ChatGPT for zero-shot dialogue understanding. In\naddition, extensive analysis shows that ChatGPT benefits from the multi-turn\ninteractive prompt in the DST task but struggles to perform slot filling for\nSLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue\nunderstanding tasks, hoping to provide some insights for future research on\nbuilding zero-shot dialogue understanding systems with Large Language Models\n(LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2304.04256v1.pdf"
    },
    {
        "title": "Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance",
        "authors": [
            "Abdolvahab Khademi"
        ],
        "published": "2023-04-09T04:53:15Z",
        "summary": "ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that\nare slated to promise different applications in diverse areas. In education,\nthese AI technologies have been tested for applications in assessment and\nteaching. In assessment, AI has long been used in automated essay scoring and\nautomated item generation. One psychometric property that these tools must have\nto assist or replace humans in assessment is high reliability in terms of\nagreement between AI scores and human raters. In this paper, we measure the\nreliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and\ntrained humans in perceiving and rating the complexity of writing prompts.\nIntraclass correlation (ICC) as a performance metric showed that the\ninter-reliability of both the OpenAI ChatGPT and the Google Bard were low\nagainst the gold standard of human ratings.",
        "pdf_link": "https://arxiv.org/pdf/2304.05372v1.pdf"
    },
    {
        "title": "Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding",
        "authors": [
            "Susik Yoon",
            "Dongha Lee",
            "Yunyi Zhang",
            "Jiawei Han"
        ],
        "published": "2023-04-08T20:41:15Z",
        "summary": "Unsupervised discovery of stories with correlated news articles in real-time\nhelps people digest massive news streams without expensive human annotations. A\ncommon approach of the existing studies for unsupervised online story discovery\nis to represent news articles with symbolic- or graph-based embedding and\nincrementally cluster them into stories. Recent large language models are\nexpected to improve the embedding further, but a straightforward adoption of\nthe models by indiscriminately encoding all information in articles is\nineffective to deal with text-rich and evolving news streams. In this work, we\npropose a novel thematic embedding with an off-the-shelf pretrained sentence\nencoder to dynamically represent articles and stories by considering their\nshared temporal themes. To realize the idea for unsupervised online story\ndiscovery, a scalable framework USTORY is introduced with two main techniques,\ntheme- and time-aware dynamic embedding and novelty-aware adaptive clustering,\nfueled by lightweight story summaries. A thorough evaluation with real news\ndata sets demonstrates that USTORY achieves higher story discovery performances\nthan baselines while being robust and scalable to various streaming settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.04099v3.pdf"
    },
    {
        "title": "Comparing Code Explanations Created by Students and Large Language Models",
        "authors": [
            "Juho Leinonen",
            "Paul Denny",
            "Stephen MacNeil",
            "Sami Sarsa",
            "Seth Bernstein",
            "Joanne Kim",
            "Andrew Tran",
            "Arto Hellas"
        ],
        "published": "2023-04-08T06:52:54Z",
        "summary": "Reasoning about code and explaining its purpose are fundamental skills for\ncomputer scientists. There has been extensive research in the field of\ncomputing education on the relationship between a student's ability to explain\ncode and other skills such as writing and tracing code. In particular, the\nability to describe at a high-level of abstraction how code will behave over\nall possible inputs correlates strongly with code writing skills. However,\ndeveloping the expertise to comprehend and explain code accurately and\nsuccinctly is a challenge for many students. Existing pedagogical approaches\nthat scaffold the ability to explain code, such as producing exemplar code\nexplanations on demand, do not currently scale well to large classrooms. The\nrecent emergence of powerful large language models (LLMs) may offer a solution.\nIn this paper, we explore the potential of LLMs in generating explanations that\ncan serve as examples to scaffold students' ability to understand and explain\ncode. To evaluate LLM-created explanations, we compare them with explanations\ncreated by students in a large course ($n \\approx 1000$) with respect to\naccuracy, understandability and length. We find that LLM-created explanations,\nwhich can be produced automatically on demand, are rated as being significantly\neasier to understand and more accurate summaries of code than student-created\nexplanations. We discuss the significance of this finding, and suggest how such\nmodels can be incorporated into introductory programming education.",
        "pdf_link": "https://arxiv.org/pdf/2304.03938v1.pdf"
    },
    {
        "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
        "authors": [
            "Jinming Li",
            "Wentao Zhang",
            "Tian Wang",
            "Guanglei Xiong",
            "Alan Lu",
            "Gerard Medioni"
        ],
        "published": "2023-04-08T00:30:08Z",
        "summary": "Recent advancements in Natural Language Processing (NLP) have led to the\ndevelopment of NLP-based recommender systems that have shown superior\nperformance. However, current models commonly treat items as mere IDs and adopt\ndiscriminative modeling, resulting in limitations of (1) fully leveraging the\ncontent information of items and the language modeling capabilities of NLP\nmodels; (2) interpreting user interests to improve relevance and diversity; and\n(3) adapting practical circumstances such as growing item inventories. To\naddress these limitations, we present GPT4Rec, a novel and flexible generative\nframework inspired by search engines. It first generates hypothetical \"search\nqueries\" given item titles in a user's history, and then retrieves items for\nrecommendation by searching these queries. The framework overcomes previous\nlimitations by learning both user and item embeddings in the language space. To\nwell-capture user interests with different aspects and granularity for\nimproving relevance and diversity, we propose a multi-query generation\ntechnique with beam search. The generated queries naturally serve as\ninterpretable representations of user interests and can be searched to\nrecommend cold-start items. With GPT-2 language model and BM25 search engine,\nour framework outperforms state-of-the-art methods by $75.7\\%$ and $22.2\\%$ in\nRecall@K on two public datasets. Experiments further revealed that multi-query\ngeneration with beam search improves both the diversity of retrieved items and\nthe coverage of a user's multi-interests. The adaptiveness and interpretability\nof generated queries are discussed with qualitative case studies.",
        "pdf_link": "https://arxiv.org/pdf/2304.03879v1.pdf"
    },
    {
        "title": "Why think step by step? Reasoning emerges from the locality of experience",
        "authors": [
            "Ben Prystawski",
            "Michael Y. Li",
            "Noah D. Goodman"
        ],
        "published": "2023-04-07T21:04:03Z",
        "summary": "Humans have a powerful and mysterious capacity to reason. Working through a\nset of mental steps enables us to make inferences we would not be capable of\nmaking directly even though we get no additional data from the world.\nSimilarly, when large language models generate intermediate steps (a chain of\nthought) before answering a question, they often produce better answers than\nthey would directly. We investigate why and how chain-of-thought reasoning is\nuseful in language models, testing the hypothesis that reasoning is effective\nwhen training data consists of overlapping local clusters of variables that\ninfluence each other strongly. These training conditions enable the chaining of\naccurate local inferences to estimate relationships between variables that were\nnot seen together in training. We prove that there will exist a \"reasoning\ngap\", where reasoning through intermediate variables reduces bias, for the\nsimple case of an autoregressive density estimator trained on local samples\nfrom a chain-structured probabilistic model. We then test our hypothesis\nexperimentally in more complex models, training an autoregressive language\nmodel on samples from Bayes nets but only including a subset of variables in\neach sample. We test language models' ability to match conditional\nprobabilities with and without intermediate reasoning steps, finding that\nintermediate steps are only helpful when the training data is locally\nstructured with respect to dependencies between variables. The combination of\nlocally structured observations and reasoning is much more data-efficient than\ntraining on all variables. Our results illustrate how the effectiveness of\nreasoning step by step is rooted in the local statistical structure of the\ntraining data.",
        "pdf_link": "https://arxiv.org/pdf/2304.03843v3.pdf"
    },
    {
        "title": "Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions",
        "authors": [
            "Sarah Fakhoury",
            "Saikat Chakraborty",
            "Madan Musuvathi",
            "Shuvendu K. Lahiri"
        ],
        "published": "2023-04-07T18:58:33Z",
        "summary": "Large language models (LLMs), such as OpenAI's Codex, have demonstrated their\npotential to generate code from natural language descriptions across a wide\nrange of programming tasks. Several benchmarks have recently emerged to\nevaluate the ability of LLMs to generate functionally correct code from natural\nlanguage intent with respect to a set of hidden test cases. This has enabled\nthe research community to identify significant and reproducible advancements in\nLLM capabilities. However, there is currently a lack of benchmark datasets for\nassessing the ability of LLMs to generate functionally correct code edits based\non natural language descriptions of intended changes. This paper aims to\naddress this gap by motivating the problem NL2Fix of translating natural\nlanguage descriptions of code changes (namely bug fixes described in Issue\nreports in repositories) into correct code fixes. To this end, we introduce\nDefects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4J\ndataset augmented with high-level descriptions of bug fixes, and empirically\nevaluate the performance of several state-of-the-art LLMs for the this task.\nResults show that these LLMS together are capable of generating plausible fixes\nfor 64.6% of the bugs, and the best LLM-based technique can achieve up to\n21.20% top-1 and 35.68% top-5 accuracy on this benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2304.03816v1.pdf"
    },
    {
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
        "authors": [
            "Emilio Ferrara"
        ],
        "published": "2023-04-07T17:14:00Z",
        "summary": "As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.",
        "pdf_link": "https://arxiv.org/pdf/2304.03738v3.pdf"
    },
    {
        "title": "Interpretable Unified Language Checking",
        "authors": [
            "Tianhua Zhang",
            "Hongyin Luo",
            "Yung-Sung Chuang",
            "Wei Fang",
            "Luc Gaitskell",
            "Thomas Hartvigsen",
            "Xixin Wu",
            "Danny Fox",
            "Helen Meng",
            "James Glass"
        ],
        "published": "2023-04-07T16:47:49Z",
        "summary": "Despite recent concerns about undesirable behaviors generated by large\nlanguage models (LLMs), including non-factual, biased, and hateful language, we\nfind LLMs are inherent multi-task language checkers based on their latent\nrepresentations of natural and social knowledge. We present an interpretable,\nunified, language checking (UniLC) method for both human and machine-generated\nlanguage that aims to check if language input is factual and fair. While\nfairness and fact-checking tasks have been handled separately with dedicated\nmodels, we find that LLMs can achieve high performance on a combination of\nfact-checking, stereotype detection, and hate speech detection tasks with a\nsimple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task\nlanguage checking method proposed in this work, the GPT3.5-turbo model\noutperforms fully supervised baselines on several language tasks. The simple\napproach and results suggest that based on strong latent knowledge\nrepresentations, an LLM can be an adaptive and explainable tool for detecting\nmisinformation, stereotypes, and hate speech.",
        "pdf_link": "https://arxiv.org/pdf/2304.03728v1.pdf"
    },
    {
        "title": "What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory",
        "authors": [
            "Ronald Fischer",
            "Markus Luczak-Roesch",
            "Johannes A Karl"
        ],
        "published": "2023-04-07T12:20:13Z",
        "summary": "There has been concern about ideological basis and possible discrimination in\ntext generated by Large Language Models (LLMs). We test possible value biases\nin ChatGPT using a psychological value theory. We designed a simple experiment\nin which we used a number of different probes derived from the Schwartz basic\nvalue theory (items from the revised Portrait Value Questionnaire, the value\ntype definitions, value names). We prompted ChatGPT via the OpenAI API\nrepeatedly to generate text and then analyzed the generated corpus for value\ncontent with a theory-driven value dictionary using a bag of words approach.\nOverall, we found little evidence of explicit value bias. The results showed\nsufficient construct and discriminant validity for the generated text in line\nwith the theoretical predictions of the psychological model, which suggests\nthat the value content was carried through into the outputs with high fidelity.\nWe saw some merging of socially oriented values, which may suggest that these\nvalues are less clearly differentiated at a linguistic level or alternatively,\nthis mixing may reflect underlying universal human motivations. We outline some\npossible applications of our findings for both applications of ChatGPT for\ncorporate usage and policy making as well as future research avenues. We also\nhighlight possible implications of this relatively high-fidelity replication of\nmotivational content using a linguistic model for the theorizing about human\nvalues.",
        "pdf_link": "https://arxiv.org/pdf/2304.03612v1.pdf"
    },
    {
        "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
        "authors": [
            "Yulin Zhou",
            "Yiren Zhao",
            "Ilia Shumailov",
            "Robert Mullins",
            "Yarin Gal"
        ],
        "published": "2023-04-07T12:06:44Z",
        "summary": "Current literature demonstrates that Large Language Models (LLMs) are great\nfew-shot learners, and prompting significantly increases their performance on a\nrange of downstream tasks in a few-shot learning setting. An attempt to\nautomate human-led prompting followed, with some progress achieved. In\nparticular, subsequent work demonstrates automation can outperform fine-tuning\nin certain K-shot learning scenarios.\n  In this paper, we revisit techniques for automated prompting on six different\ndownstream tasks and a larger range of K-shot learning settings. We find that\nautomated prompting does not consistently outperform simple manual prompts. Our\nwork suggests that, in addition to fine-tuning, manual prompts should be used\nas a baseline in this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2304.03609v2.pdf"
    },
    {
        "title": "ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions",
        "authors": [
            "Sibei Chen",
            "Hanbing Liu",
            "Weiting Jin",
            "Xiangyu Sun",
            "Xiaoyao Feng",
            "Ju Fan",
            "Xiaoyong Du",
            "Nan Tang"
        ],
        "published": "2023-04-07T08:33:08Z",
        "summary": "Orchestrating a high-quality data preparation program is essential for\nsuccessful machine learning (ML), but it is known to be time and effort\nconsuming. Despite the impressive capabilities of large language models like\nChatGPT in generating programs by interacting with users through natural\nlanguage prompts, there are still limitations. Specifically, a user must\nprovide specific prompts to iteratively guide ChatGPT in improving data\npreparation programs, which requires a certain level of expertise in\nprogramming, the dataset used and the ML task. Moreover, once a program has\nbeen generated, it is non-trivial to revisit a previous version or make changes\nto the program without starting the process over again. In this paper, we\npresent ChatPipe, a novel system designed to facilitate seamless interaction\nbetween users and ChatGPT. ChatPipe provides users with effective\nrecommendation on next data preparation operations, and guides ChatGPT to\ngenerate program for the operations. Also, ChatPipe enables users to easily\nroll back to previous versions of the program, which facilitates more efficient\nexperimentation and testing. We have developed a web application for ChatPipe\nand prepared several real-world ML tasks from Kaggle. These tasks can showcase\nthe capabilities of ChatPipe and enable VLDB attendees to easily experiment\nwith our novel features to rapidly orchestrate a high-quality data preparation\nprogram.",
        "pdf_link": "https://arxiv.org/pdf/2304.03540v1.pdf"
    },
    {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O'Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "published": "2023-04-07T01:55:19Z",
        "summary": "Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent's experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine's\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.",
        "pdf_link": "https://arxiv.org/pdf/2304.03442v2.pdf"
    },
    {
        "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
        "authors": [
            "Hanmeng Liu",
            "Ruoxi Ning",
            "Zhiyang Teng",
            "Jian Liu",
            "Qiji Zhou",
            "Yue Zhang"
        ],
        "published": "2023-04-07T01:37:45Z",
        "summary": "Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. With the release of Generative Pretrained Transformer 4\n(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn\nthe GPT-4 performance on various logical reasoning tasks. This report analyses\nmultiple logical reasoning datasets, with popular benchmarks like LogiQA and\nReClor, and newly-released datasets like AR-LSAT. We test the multi-choice\nreading comprehension and natural language inference tasks with benchmarks\nrequiring logical reasoning. We further construct a logical reasoning\nout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.\nWe also make a performance comparison between ChatGPT and GPT-4. Experiment\nresults show that ChatGPT performs significantly better than the RoBERTa\nfine-tuning method on most logical reasoning benchmarks. With early access to\nthe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.\nThe results show GPT-4 yields even higher performance on most logical reasoning\ndatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known\ndatasets like LogiQA and ReClor. However, the performance drops significantly\nwhen handling newly released and out-of-distribution datasets. Logical\nreasoning remains challenging for ChatGPT and GPT-4, especially on\nout-of-distribution and natural language inference datasets. We release the\nprompt-style logical reasoning datasets as a benchmark suite and name it\nLogiEval.",
        "pdf_link": "https://arxiv.org/pdf/2304.03439v3.pdf"
    },
    {
        "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
        "authors": [
            "Kailai Yang",
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-06T19:53:59Z",
        "summary": "The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.",
        "pdf_link": "https://arxiv.org/pdf/2304.03347v4.pdf"
    },
    {
        "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "authors": [
            "Alexander Pan",
            "Jun Shern Chan",
            "Andy Zou",
            "Nathaniel Li",
            "Steven Basart",
            "Thomas Woodside",
            "Jonathan Ng",
            "Hanlin Zhang",
            "Scott Emmons",
            "Dan Hendrycks"
        ],
        "published": "2023-04-06T17:59:03Z",
        "summary": "Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2304.03279v4.pdf"
    },
    {
        "title": "Instruction Tuning with GPT-4",
        "authors": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "published": "2023-04-06T17:58:09Z",
        "summary": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2304.03277v1.pdf"
    },
    {
        "title": "When do you need Chain-of-Thought Prompting for ChatGPT?",
        "authors": [
            "Jiuhai Chen",
            "Lichang Chen",
            "Heng Huang",
            "Tianyi Zhou"
        ],
        "published": "2023-04-06T17:47:29Z",
        "summary": "Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step\nreasoning from Large Language Models~(LLMs). For example, by simply adding CoT\ninstruction ``Let's think step-by-step'' to each input query of MultiArith\ndataset, GPT-3's accuracy can be improved from 17.7\\% to 78.7\\%. However, it is\nnot clear whether CoT is still effective on more recent instruction finetuned\n(IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer\neffective for certain tasks such as arithmetic reasoning while still keeping\neffective on other reasoning tasks. Moreover, on the former tasks, ChatGPT\nusually achieves the best performance and can generate CoT even without being\ninstructed to do so. Hence, it is plausible that ChatGPT has already been\ntrained on these tasks with CoT and thus memorized the instruction so it\nimplicitly follows such an instruction when applied to the same queries, even\nwithout CoT. Our analysis reflects a potential risk of overfitting/bias toward\ninstructions introduced in IFT, which becomes more common in training LLMs. In\naddition, it indicates possible leakage of the pretraining recipe, e.g., one\ncan verify whether a dataset and instruction were used in training ChatGPT. Our\nexperiments report new baseline results of ChatGPT on a variety of reasoning\ntasks and shed novel insights into LLM's profiling, instruction memorization,\nand pretraining dataset leakage.",
        "pdf_link": "https://arxiv.org/pdf/2304.03262v2.pdf"
    },
    {
        "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
        "authors": [
            "Marzena Karpinska",
            "Mohit Iyyer"
        ],
        "published": "2023-04-06T17:27:45Z",
        "summary": "Large language models (LLMs) are competitive with the state of the art on a\nwide range of sentence-level translation datasets. However, their ability to\ntranslate paragraphs and documents remains unexplored because evaluation in\nthese settings is costly and difficult. We show through a rigorous human\nevaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an\nentire literary paragraph (e.g., from a novel) at once results in\nhigher-quality translations than standard sentence-by-sentence translation\nacross 18 linguistically-diverse language pairs (e.g., translating into and out\nof Japanese, Polish, and English). Our evaluation, which took approximately 350\nhours of effort for annotation and analysis, is conducted by hiring translators\nfluent in both the source and target language and asking them to provide both\nspan-level error annotations as well as preference judgments of which system's\ntranslations are better. We observe that discourse-level LLM translators commit\nfewer mistranslations, grammar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, critical errors still abound,\nincluding occasional content omissions, and a human translator's intervention\nremains necessary to ensure that the author's voice remains intact. We publicly\nrelease our dataset and error annotations to spur future research on evaluation\nof document-level literary translation.",
        "pdf_link": "https://arxiv.org/pdf/2304.03245v3.pdf"
    },
    {
        "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
        "authors": [
            "Lei Wang",
            "Ee-Peng Lim"
        ],
        "published": "2023-04-06T15:35:11Z",
        "summary": "Large language models (LLMs) have achieved impressive zero-shot performance\nin various natural language processing (NLP) tasks, demonstrating their\ncapabilities for inference without training examples. Despite their success, no\nresearch has yet explored the potential of LLMs to perform next-item\nrecommendations in the zero-shot setting. We have identified two major\nchallenges that must be addressed to enable LLMs to act effectively as\nrecommenders. First, the recommendation space can be extremely large for LLMs,\nand LLMs do not know about the target user's past interacted items and\npreferences. To address this gap, we propose a prompting strategy called\nZero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make\nnext-item recommendations. Specifically, the NIR-based strategy involves using\nan external module to generate candidate items based on user-filtering or\nitem-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3\nto carry subtasks that capture the user's preferences, select representative\npreviously watched movies, and recommend a ranked list of 10 movies. We\nevaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show\nthat it achieves strong zero-shot performance, even outperforming some strong\nsequential recommendation models trained on the entire training dataset. These\npromising results highlight the ample research opportunities to use LLMs as\nrecommenders. The code can be found at\nhttps://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.",
        "pdf_link": "https://arxiv.org/pdf/2304.03153v1.pdf"
    },
    {
        "title": "Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media",
        "authors": [
            "Bowen Zhang",
            "Xianghua Fu",
            "Daijun Ding",
            "Hu Huang",
            "Yangyang Li",
            "Liwen Jing"
        ],
        "published": "2023-04-06T14:12:02Z",
        "summary": "Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.",
        "pdf_link": "https://arxiv.org/pdf/2304.03087v1.pdf"
    },
    {
        "title": "Multi-label classification of open-ended questions with BERT",
        "authors": [
            "Matthias Schonlau",
            "Julia Wei\u00df",
            "Jan Marquardt"
        ],
        "published": "2023-04-06T09:09:44Z",
        "summary": "Open-ended questions in surveys are valuable because they do not constrain\nthe respondent's answer, thereby avoiding biases. However, answers to\nopen-ended questions are text data which are harder to analyze. Traditionally,\nanswers were manually classified as specified in the coding manual. Most of the\neffort to automate coding has gone into the easier problem of single label\nprediction, where answers are classified into a single code. However, open-ends\nthat require multi-label classification, i.e., that are assigned multiple\ncodes, occur frequently. This paper focuses on multi-label classification of\ntext answers to open-ended survey questions in social science surveys. We\nevaluate the performance of the transformer-based architecture BERT for the\nGerman language in comparison to traditional multi-label algorithms (Binary\nRelevance, Label Powerset, ECC) in a German social science survey, the GLES\nPanel (N=17,584, 55 labels). We find that classification with BERT (forcing at\nleast one label) has the smallest 0/1 loss (13.1%) among methods considered\n(18.9%-21.6%). As expected, it is much easier to correctly predict answer texts\nthat correspond to a single label (7.1% loss) than those that correspond to\nmultiple labels ($\\sim$50% loss). Because BERT predicts zero labels for only\n1.5% of the answers, forcing at least one label, while recommended, ultimately\ndoes not lower the 0/1 loss by much. Our work has important implications for\nsocial scientists: 1) We have shown multi-label classification with BERT works\nin the German language for open-ends. 2) For mildly multi-label classification\ntasks, the loss now appears small enough to allow for fully automatic\nclassification (as compared to semi-automatic approaches). 3) Multi-label\nclassification with BERT requires only a single model. The leading competitor,\nECC, iterates through individual single label predictions.",
        "pdf_link": "https://arxiv.org/pdf/2304.02945v1.pdf"
    },
    {
        "title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
        "authors": [
            "Chen Feng Tsai",
            "Xiaochen Zhou",
            "Sierra S. Liu",
            "Jing Li",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "published": "2023-04-06T05:01:28Z",
        "summary": "Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.",
        "pdf_link": "https://arxiv.org/pdf/2304.02868v1.pdf"
    },
    {
        "title": "Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",
        "authors": [
            "Madiha Zahrah Choksi",
            "David Goedicke"
        ],
        "published": "2023-04-06T03:09:26Z",
        "summary": "Intelligent or generative writing tools rely on large language models that\nrecognize, summarize, translate, and predict content. This position paper\nprobes the copyright interests of open data sets used to train large language\nmodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumvent\nthe copyright interests of the used data? We start by defining software\ncopyright and tracing its history. We rely on GitHub Copilot as a modern case\nstudy challenging software copyright. Our conclusion outlines obstacles that\ngenerative writing assistants create for copyright, and offers a practical road\nmap for copyright analysis for developers, software law experts, and general\nusers to consider in the context of intelligent LLM-powered writing tools.",
        "pdf_link": "https://arxiv.org/pdf/2304.02839v1.pdf"
    },
    {
        "title": "Approach Intelligent Writing Assistants Usability with Seven Stages of Action",
        "authors": [
            "Avinash Bhat",
            "Disha Shrivastava",
            "Jin L. C. Guo"
        ],
        "published": "2023-04-06T02:11:55Z",
        "summary": "Despite the potential of Large Language Models (LLMs) as writing assistants,\nthey are plagued by issues like coherence and fluency of the model output,\ntrustworthiness, ownership of the generated content, and predictability of\nmodel performance, thereby limiting their usability. In this position paper, we\npropose to adopt Norman's seven stages of action as a framework to approach the\ninteraction design of intelligent writing assistants. We illustrate the\nframework's applicability to writing tasks by providing an example of software\ntutorial authoring. The paper also discusses the framework as a tool to\nsynthesize research on the interaction design of LLM-based tools and presents\nexamples of tools that support the stages of action. Finally, we briefly\noutline the potential of a framework for human-LLM interaction research.",
        "pdf_link": "https://arxiv.org/pdf/2304.02822v1.pdf"
    },
    {
        "title": "GPT detectors are biased against non-native English writers",
        "authors": [
            "Weixin Liang",
            "Mert Yuksekgonul",
            "Yining Mao",
            "Eric Wu",
            "James Zou"
        ],
        "published": "2023-04-06T01:51:15Z",
        "summary": "The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7",
        "pdf_link": "https://arxiv.org/pdf/2304.02819v3.pdf"
    },
    {
        "title": "Context-Aware Classification of Legal Document Pages",
        "authors": [
            "Pavlos Fragkogiannis",
            "Martina Forster",
            "Grace E. Lee",
            "Dell Zhang"
        ],
        "published": "2023-04-05T23:14:58Z",
        "summary": "For many business applications that require the processing, indexing, and\nretrieval of professional documents such as legal briefs (in PDF format etc.),\nit is often essential to classify the pages of any given document into their\ncorresponding types beforehand. Most existing studies in the field of document\nimage classification either focus on single-page documents or treat multiple\npages in a document independently. Although in recent years a few techniques\nhave been proposed to exploit the context information from neighboring pages to\nenhance document page classification, they typically cannot be utilized with\nlarge pre-trained language models due to the constraint on input length. In\nthis paper, we present a simple but effective approach that overcomes the above\nlimitation. Specifically, we enhance the input with extra tokens carrying\nsequential information about previous pages - introducing recurrence - which\nenables the usage of pre-trained Transformer models like BERT for context-aware\npage classification. Our experiments conducted on two legal datasets in English\nand Portuguese respectively show that the proposed approach can significantly\nimprove the performance of document page classification compared to the\nnon-recurrent setup as well as the other context-aware baselines.",
        "pdf_link": "https://arxiv.org/pdf/2304.02787v2.pdf"
    },
    {
        "title": "Conceptual structure coheres in human cognition but not in large language models",
        "authors": [
            "Siddharth Suresh",
            "Kushin Mukherjee",
            "Xizheng Yu",
            "Wei-Chun Huang",
            "Lisa Padua",
            "Timothy T Rogers"
        ],
        "published": "2023-04-05T21:27:01Z",
        "summary": "Neural network models of language have long been used as a tool for\ndeveloping hypotheses about conceptual representation in the mind and brain.\nFor many years, such use involved extracting vector-space representations of\nwords and using distances among these to predict or understand human behavior\nin various semantic tasks. Contemporary large language models (LLMs), however,\nmake it possible to interrogate the latent structure of conceptual\nrepresentations using experimental methods nearly identical to those commonly\nused with human participants. The current work utilizes three common techniques\nborrowed from cognitive psychology to estimate and compare the structure of\nconcepts in humans and a suite of LLMs. In humans, we show that conceptual\nstructure is robust to differences in culture, language, and method of\nestimation. Structures estimated from LLM behavior, while individually fairly\nconsistent with those estimated from human behavior, vary much more depending\nupon the particular task used to generate responses--across tasks, estimates of\nconceptual structure from the very same model cohere less with one another than\ndo human structure estimates. These results highlight an important difference\nbetween contemporary LLMs and human cognition, with implications for\nunderstanding some fundamental limitations of contemporary machine language.",
        "pdf_link": "https://arxiv.org/pdf/2304.02754v2.pdf"
    },
    {
        "title": "Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks",
        "authors": [
            "Md. Tanvir Rouf Shawon",
            "G. M. Shahariar",
            "Faisal Muhammad Shah",
            "Mohammad Shafiul Alam",
            "Md. Shahriar Mahbub"
        ],
        "published": "2023-04-05T20:40:09Z",
        "summary": "This paper investigates the potential of semi-supervised Generative\nAdversarial Networks (GANs) to fine-tune pretrained language models in order to\nclassify Bengali fake reviews from real reviews with a few annotated data. With\nthe rise of social media and e-commerce, the ability to detect fake or\ndeceptive reviews is becoming increasingly important in order to protect\nconsumers from being misled by false information. Any machine learning model\nwill have trouble identifying a fake review, especially for a low resource\nlanguage like Bengali. We have demonstrated that the proposed semi-supervised\nGAN-LM architecture (generative adversarial network on top of a pretrained\nlanguage model) is a viable solution in classifying Bengali fake reviews as the\nexperimental results suggest that even with only 1024 annotated samples,\nBanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and\na f1-score of 84.89% outperforming other pretrained language models -\nBanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and\n10% respectively in terms of accuracy. The experiments were conducted on a\nmanually labeled food review dataset consisting of total 6014 real and fake\nreviews collected from various social media groups. Researchers that are\nexperiencing difficulty recognizing not just fake reviews but other\nclassification issues owing to a lack of labeled data may find a solution in\nour proposed methodology.",
        "pdf_link": "https://arxiv.org/pdf/2304.02739v1.pdf"
    },
    {
        "title": "Efficient OCR for Building a Diverse Digital History",
        "authors": [
            "Jacob Carlson",
            "Tom Bryan",
            "Melissa Dell"
        ],
        "published": "2023-04-05T20:36:04Z",
        "summary": "Thousands of users consult digital archives daily, but the information they\ncan access is unrepresentative of the diversity of documentary history. The\nsequence-to-sequence architecture typically used for optical character\nrecognition (OCR) - which jointly learns a vision and language model - is\npoorly extensible to low-resource document collections, as learning a\nlanguage-vision model requires extensive labeled sequences and compute. This\nstudy models OCR as a character level image retrieval problem, using a\ncontrastively trained vision encoder. Because the model only learns characters'\nvisual features, it is more sample efficient and extensible than existing\narchitectures, enabling accurate OCR in settings where existing solutions fail.\nCrucially, the model opens new avenues for community engagement in making\ndigital history more representative of documentary history.",
        "pdf_link": "https://arxiv.org/pdf/2304.02737v1.pdf"
    },
    {
        "title": "Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning",
        "authors": [
            "J. Harry Caufield",
            "Harshad Hegde",
            "Vincent Emonet",
            "Nomi L. Harris",
            "Marcin P. Joachimiak",
            "Nicolas Matentzoglu",
            "HyeongSik Kim",
            "Sierra A. T. Moxon",
            "Justin T. Reese",
            "Melissa A. Haendel",
            "Peter N. Robinson",
            "Christopher J. Mungall"
        ],
        "published": "2023-04-05T19:07:04Z",
        "summary": "Creating knowledge bases and ontologies is a time consuming task that relies\non a manual curation. AI/NLP approaches can assist expert curators in\npopulating these knowledge bases, but current approaches rely on extensive\ntraining data, and are not able to populate arbitrary complex nested knowledge\nschemas.\n  Here we present Structured Prompt Interrogation and Recursive Extraction of\nSemantics (SPIRES), a Knowledge Extraction approach that relies on the ability\nof Large Language Models (LLMs) to perform zero-shot learning (ZSL) and\ngeneral-purpose query answering from flexible prompts and return information\nconforming to a specified schema. Given a detailed, user-defined knowledge\nschema and an input text, SPIRES recursively performs prompt interrogation\nagainst GPT-3+ to obtain a set of responses matching the provided schema.\nSPIRES uses existing ontologies and vocabularies to provide identifiers for all\nmatched elements.\n  We present examples of use of SPIRES in different domains, including\nextraction of food recipes, multi-species cellular signaling pathways, disease\ntreatments, multi-step drug mechanisms, and chemical to disease causation\ngraphs. Current SPIRES accuracy is comparable to the mid-range of existing\nRelation Extraction (RE) methods, but has the advantage of easy customization,\nflexibility, and, crucially, the ability to perform new tasks in the absence of\nany training data. This method supports a general strategy of leveraging the\nlanguage interpreting capabilities of LLMs to assemble knowledge bases,\nassisting manual knowledge curation and acquisition while supporting validation\nwith publicly-available databases and ontologies external to the LLM.\n  SPIRES is available as part of the open source OntoGPT package:\nhttps://github.com/ monarch-initiative/ontogpt.",
        "pdf_link": "https://arxiv.org/pdf/2304.02711v2.pdf"
    },
    {
        "title": "Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification",
        "authors": [
            "Shan Chen",
            "Yingya Li",
            "Sheng Lu",
            "Hoang Van",
            "Hugo JWL Aerts",
            "Guergana K. Savova",
            "Danielle S. Bitterman"
        ],
        "published": "2023-04-05T15:11:25Z",
        "summary": "Recent advances in large language models (LLMs) have shown impressive ability\nin biomedical question-answering, but have not been adequately investigated for\nmore specific biomedical applications. This study investigates the performance\nof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical\ntasks beyond question-answering. Because no patient data can be passed to the\nOpenAI API public interface, we evaluated model performance with over 10000\nsamples as proxies for two fundamental tasks in the clinical domain -\nclassification and reasoning. The first task is classifying whether statements\nof clinical and policy recommendations in scientific literature constitute\nhealth advice. The second task is causal relation detection from the biomedical\nliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)\nwith logistic regression, and fine-tuned BioBERT models. Despite the excitement\naround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks\nremained the best strategy. The simple BoW model performed on par with the most\ncomplex LLM prompting. Prompt engineering required significant investment.",
        "pdf_link": "https://arxiv.org/pdf/2304.02496v1.pdf"
    },
    {
        "title": "Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT",
        "authors": [
            "Tong Xie",
            "Yuwei Wan",
            "Wei Huang",
            "Yufei Zhou",
            "Yixuan Liu",
            "Qingyuan Linghu",
            "Shaozhou Wang",
            "Chunyu Kit",
            "Clara Grazian",
            "Wenjie Zhang",
            "Bram Hoex"
        ],
        "published": "2023-04-05T04:01:52Z",
        "summary": "The amount of data has growing significance in exploring cutting-edge\nmaterials and a number of datasets have been generated either by hand or\nautomated approaches. However, the materials science field struggles to\neffectively utilize the abundance of data, especially in applied disciplines\nwhere materials are evaluated based on device performance rather than their\nproperties. This article presents a new natural language processing (NLP) task\ncalled structured information inference (SII) to address the complexities of\ninformation extraction at the device level in materials science. We\naccomplished this task by tuning GPT-3 on an existing perovskite solar cell\nFAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%\nF1-score and extended the dataset with data published since its release. The\nproduced data is formatted and normalized, enabling its direct utilization as\ninput in subsequent data analysis. This feature empowers materials scientists\nto develop models by selecting high-quality review articles within their\ndomain. Additionally, we designed experiments to predict the electrical\nperformance of solar cells and design materials or devices with targeted\nparameters using large language models (LLMs). Our results demonstrate\ncomparable performance to traditional machine learning methods without feature\nselection, highlighting the potential of LLMs to acquire scientific knowledge\nand design new materials akin to materials scientists.",
        "pdf_link": "https://arxiv.org/pdf/2304.02213v5.pdf"
    },
    {
        "title": "Document-Level Machine Translation with Large Language Models",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-04-05T03:49:06Z",
        "summary": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2304.02210v2.pdf"
    },
    {
        "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
        "authors": [
            "Zhi-Qi Cheng",
            "Qi Dai",
            "Siyao Li",
            "Jingdong Sun",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "published": "2023-04-05T00:25:27Z",
        "summary": "Charts are a powerful tool for visually conveying complex data, but their\ncomprehension poses a challenge due to the diverse chart types and intricate\ncomponents. Existing chart comprehension methods suffer from either heuristic\nrules or an over-reliance on OCR systems, resulting in suboptimal performance.\nTo address these issues, we present ChartReader, a unified framework that\nseamlessly integrates chart derendering and comprehension tasks. Our approach\nincludes a transformer-based chart component detection module and an extended\npre-trained vision-language model for chart-to-X tasks. By learning the rules\nof charts automatically from annotated datasets, our approach eliminates the\nneed for manual rule-making, reducing effort and enhancing accuracy.~We also\nintroduce a data variable replacement technique and extend the input and\nposition embeddings of the pre-trained model for cross-task training. We\nevaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks,\ndemonstrating its superiority over existing methods. Our proposed framework can\nsignificantly reduce the manual effort involved in chart analysis, providing a\nstep towards a universal chart understanding model. Moreover, our approach\noffers opportunities for plug-and-play integration with mainstream LLMs such as\nT5 and TaPas, extending their capability to chart comprehension tasks. The code\nis available at https://github.com/zhiqic/ChartReader.",
        "pdf_link": "https://arxiv.org/pdf/2304.02173v1.pdf"
    },
    {
        "title": "Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering",
        "authors": [
            "Krishna Kumar"
        ],
        "published": "2023-04-04T21:47:41Z",
        "summary": "The widespread adoption of large language models (LLMs), such as OpenAI's\nChatGPT, could revolutionize various industries, including geotechnical\nengineering. However, GPT models can sometimes generate plausible-sounding but\nfalse outputs, leading to hallucinations. In this article, we discuss the\nimportance of prompt engineering in mitigating these risks and harnessing the\nfull potential of GPT for geotechnical applications. We explore the challenges\nand pitfalls associated with LLMs and highlight the role of context in ensuring\naccurate and valuable responses. Furthermore, we examine the development of\ncontext-specific search engines and the potential of LLMs to become a natural\ninterface for complex tasks, such as data analysis and design. We also develop\na unified interface using natural language to handle complex geotechnical\nengineering tasks and data analysis. By integrating GPT into geotechnical\nengineering workflows, professionals can streamline their work and develop\nsustainable and resilient infrastructure systems for the future.",
        "pdf_link": "https://arxiv.org/pdf/2304.02138v3.pdf"
    },
    {
        "title": "Dialogue-Contextualized Re-ranking for Medical History-Taking",
        "authors": [
            "Jian Zhu",
            "Ilya Valmianski",
            "Anitha Kannan"
        ],
        "published": "2023-04-04T17:31:32Z",
        "summary": "AI-driven medical history-taking is an important component in symptom\nchecking, automated patient intake, triage, and other AI virtual care\napplications. As history-taking is extremely varied, machine learning models\nrequire a significant amount of data to train. To overcome this challenge,\nexisting systems are developed using indirect data or expert knowledge. This\nleads to a training-inference gap as models are trained on different kinds of\ndata than what they observe at inference time. In this work, we present a\ntwo-stage re-ranking approach that helps close the training-inference gap by\nre-ranking the first-stage question candidates using a dialogue-contextualized\nmodel. For this, we propose a new model, global re-ranker, which cross-encodes\nthe dialogue with all questions simultaneously, and compare it with several\nexisting neural baselines. We test both transformer and S4-based language model\nbackbones. We find that relative to the expert system, the best performance is\nachieved by our proposed global re-ranker with a transformer backbone,\nresulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%\nhigher mean average precision (mAP).",
        "pdf_link": "https://arxiv.org/pdf/2304.01974v1.pdf"
    },
    {
        "title": "Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems",
        "authors": [
            "Fangzhen Lin",
            "Ziyi Shou",
            "Chengcai Chen"
        ],
        "published": "2023-04-04T13:01:48Z",
        "summary": "For a natural language problem that requires some non-trivial reasoning to\nsolve, there are at least two ways to do it using a large language model (LLM).\nOne is to ask it to solve it directly. The other is to use it to extract the\nfacts from the problem text and then use a theorem prover to solve it. In this\nnote, we compare the two methods using ChatGPT and GPT4 on a series of logic\nword puzzles, and conclude that the latter is the right approach.",
        "pdf_link": "https://arxiv.org/pdf/2304.01771v1.pdf"
    },
    {
        "title": "Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation",
        "authors": [
            "Tao Fang",
            "Shu Yang",
            "Kaixin Lan",
            "Derek F. Wong",
            "Jinpeng Hu",
            "Lidia S. Chao",
            "Yue Zhang"
        ],
        "published": "2023-04-04T12:33:40Z",
        "summary": "ChatGPT, a large-scale language model based on the advanced GPT-3.5\narchitecture, has shown remarkable potential in various Natural Language\nProcessing (NLP) tasks. However, there is currently a dearth of comprehensive\nstudy exploring its potential in the area of Grammatical Error Correction\n(GEC). To showcase its capabilities in GEC, we design zero-shot\nchain-of-thought (CoT) and few-shot CoT settings using in-context learning for\nChatGPT. Our evaluation involves assessing ChatGPT's performance on five\nofficial test sets in three different languages, along with three\ndocument-level GEC test sets in English. Our experimental results and human\nevaluations demonstrate that ChatGPT has excellent error detection capabilities\nand can freely correct errors to make the corrected sentences very fluent,\npossibly due to its over-correction tendencies and not adhering to the\nprinciple of minimal edits. Additionally, its performance in non-English and\nlow-resource settings highlights its potential in multilingual GEC tasks.\nHowever, further analysis of various types of errors at the document-level has\nshown that ChatGPT cannot effectively correct agreement, coreference, tense\nerrors across sentences, and cross-sentence boundary errors.",
        "pdf_link": "https://arxiv.org/pdf/2304.01746v1.pdf"
    },
    {
        "title": "To ChatGPT, or not to ChatGPT: That is the question!",
        "authors": [
            "Alessandro Pegoraro",
            "Kavita Kumari",
            "Hossein Fereidooni",
            "Ahmad-Reza Sadeghi"
        ],
        "published": "2023-04-04T03:04:28Z",
        "summary": "ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.",
        "pdf_link": "https://arxiv.org/pdf/2304.01487v2.pdf"
    },
    {
        "title": "Blockwise Compression of Transformer-based Models without Retraining",
        "authors": [
            "Gaochen Dong",
            "Wei Chen"
        ],
        "published": "2023-04-04T02:55:40Z",
        "summary": "Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have\nrecently garnered considerable attention in both academia and industry due to\ntheir promising performance in general language tasks. Nevertheless, these\nmodels typically involve computationally encoding processes, and in some cases,\ndecoding processes as well, both of which are fundamentally large-scale matrix\nmultiplication. These operations bring the inevitable challenges of massive\ncomputation resources and huge memory footprint, usually requiring at least\n10^23 FLOPs and hundreds of gigabytes, respectively. A common method to address\nthis issue is to reduce the computational and memory requirements by applying\nlayerwise quantization to the transformer, replacing the usual fp32 data type\nwith a low-bit equivalent. Unfortunately, this method often leads to decreased\nmodel accuracy and necessitates time-consuming retraining. Such retraining not\nonly requires fine-tuning skills but also substantial computational resources,\nposing challenges for users. To specifically tackle these issues, we propose\nBCT, a framework of blockwise compression for transformers without retraining,\naiming to facilitate model deployment. Unlike layerwise compression methods,\nBCT achieves finer compression of the entire transformer by operating\nblockwise. This method mitigates data distribution deviation caused by\nquantization, eliminating the requirement for retraining. BCT effectively\ncompresses all components of the model, including but not limited to the\nembedding, matrix multiplication, GELU, Softmax, layer normalization, and\nintermediate results. In a case study, an efficient model is compressed by BCT\nachieving up to 7.988x compression. Subsequently, we also evaluate it on\nseveral General Language Understanding Evaluation (GLUE) datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.01483v2.pdf"
    },
    {
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "authors": [
            "Stella Biderman",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Herbie Bradley",
            "Kyle O'Brien",
            "Eric Hallahan",
            "Mohammad Aflah Khan",
            "Shivanshu Purohit",
            "USVSN Sai Prashanth",
            "Edward Raff",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Oskar van der Wal"
        ],
        "published": "2023-04-03T20:58:15Z",
        "summary": "How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.",
        "pdf_link": "https://arxiv.org/pdf/2304.01373v2.pdf"
    },
    {
        "title": "Classification of integers based on residue classes via modern deep learning algorithms",
        "authors": [
            "Da Wu",
            "Jingye Yang",
            "Mian Umair Ahsan",
            "Kai Wang"
        ],
        "published": "2023-04-03T19:53:31Z",
        "summary": "Judging whether an integer can be divided by prime numbers such as 2 or 3 may\nappear trivial to human beings, but can be less straightforward for computers.\nHere, we tested multiple deep learning architectures and feature engineering\napproaches on classifying integers based on their residues when divided by\nsmall prime numbers. We found that the ability of classification critically\ndepends on the feature space. We also evaluated Automated Machine Learning\n(AutoML) platforms from Amazon, Google and Microsoft, and found that they\nfailed on this task without appropriately engineered features. Furthermore, we\nintroduced a method that utilizes linear regression on Fourier series basis\nvectors, and demonstrated its effectiveness. Finally, we evaluated Large\nLanguage Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated\ntheir failures. In conclusion, feature engineering remains an important task to\nimprove performance and increase interpretability of machine-learning models,\neven in the era of AutoML and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.01333v3.pdf"
    },
    {
        "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning",
        "authors": [
            "Lifu Tu",
            "Jin Qu",
            "Semih Yavuz",
            "Shafiq Joty",
            "Wenhao Liu",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-04-03T18:46:01Z",
        "summary": "Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD for cross-lingual alignment\npretraining, a parallel and large-scale multilingual conversation dataset that\nwe created by translating the English-only Schema-Guided Dialogue (SGD) dataset\n(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately\n330k utterances per language. To facilitate aligned cross-lingual\nrepresentations, we develop an efficient prompt-tuning-based method for\nlearning alignment prompts. We also investigate two different classifiers:\nNLI-based and vanilla classifiers, and test cross-lingual capability enabled by\nthe aligned prompts. We evaluate our model's cross-lingual generalization\ncapabilities on two conversation tasks: slot-filling and intent classification.\nOur results demonstrate the strong and efficient modeling ability of NLI-based\nclassifiers and the large cross-lingual transfer improvements achieved by our\naligned prompts, particularly in few-shot settings. In addition, we highlight\nthe nice results of our approach compared to LLMs such as text-davinci-003 and\nChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive\nperformance in English, their cross-lingual capabilities in other languages,\nparticularly low-resource languages, are limited.",
        "pdf_link": "https://arxiv.org/pdf/2304.01295v4.pdf"
    },
    {
        "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
        "authors": [
            "Canwen Xu",
            "Daya Guo",
            "Nan Duan",
            "Julian McAuley"
        ],
        "published": "2023-04-03T17:59:09Z",
        "summary": "Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.",
        "pdf_link": "https://arxiv.org/pdf/2304.01196v4.pdf"
    },
    {
        "title": "Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT",
        "authors": [
            "Yi Qi",
            "Xingyu Zhao",
            "Siddartha Khastgir",
            "Xiaowei Huang"
        ],
        "published": "2023-04-03T16:46:49Z",
        "summary": "Can safety analysis make use of Large Language Models (LLMs)? A case study\nexplores Systems Theoretic Process Analysis (STPA) applied to Automatic\nEmergency Brake (AEB) and Electricity Demand Side Management (DSM) systems\nusing ChatGPT. We investigate how collaboration schemes, input semantic\ncomplexity, and prompt guidelines influence STPA results. Comparative results\nshow that using ChatGPT without human intervention may be inadequate due to\nreliability related issues, but with careful design, it may outperform human\nexperts. No statistically significant differences are found when varying the\ninput semantic complexity or using common prompt guidelines, which suggests the\nnecessity for developing domain-specific prompt engineering. We also highlight\nfuture challenges, including concerns about LLM trustworthiness and the\nnecessity for standardisation and regulation in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2304.01246v3.pdf"
    },
    {
        "title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task",
        "authors": [
            "Honglin Xiong",
            "Sheng Wang",
            "Yitao Zhu",
            "Zihao Zhao",
            "Yuxiao Liu",
            "Linlin Huang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-04-03T15:57:51Z",
        "summary": "The recent progress of large language models (LLMs), including ChatGPT and\nGPT-4, in comprehending and responding to human instructions has been\nremarkable. Nevertheless, these models typically perform better in English and\nhave not been explicitly trained for the medical domain, resulting in\nsuboptimal precision in diagnoses, drug recommendations, and other medical\nadvice. Additionally, training and deploying a dialogue model is still believed\nto be impossible for hospitals, hindering the promotion of LLMs. To tackle\nthese challenges, we have collected databases of medical dialogues in Chinese\nwith ChatGPT's help and adopted several techniques to train an easy-deploy LLM.\nRemarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13\nhours, which means having a healthcare-purpose LLM can be very affordable.\nDoctorGLM is currently an early-stage engineering attempt and contain various\nmistakes. We are sharing it with the broader community to invite feedback and\nsuggestions to improve its healthcare-focused capabilities:\nhttps://github.com/xionghonglin/DoctorGLM.",
        "pdf_link": "https://arxiv.org/pdf/2304.01097v2.pdf"
    },
    {
        "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models",
        "authors": [
            "Zhihang Yuan",
            "Lin Niu",
            "Jiawei Liu",
            "Wenyu Liu",
            "Xinggang Wang",
            "Yuzhang Shang",
            "Guangyu Sun",
            "Qiang Wu",
            "Jiaxiang Wu",
            "Bingzhe Wu"
        ],
        "published": "2023-04-03T15:46:15Z",
        "summary": "Large-scale language models (LLMs) have demonstrated impressive performance,\nbut their deployment presents challenges due to their significant memory usage.\nThis issue can be alleviated through quantization. In this paper, we identify\nthat the challenge in quantizing activations in LLMs arises from varying ranges\nacross channels, rather than solely the presence of outliers. To address this\nchallenge, we introduce a quantization method called RPTQ, which utilizes a\nreorder-based approach. By rearranging the channels and quantizing them in\nclusters, RPTQ effectively mitigates the impact of range differences between\nchannels. To minimize the overhead of the reorder operation, we fuse it into\nthe layer norm operation and weights in linear layers. In our experiments, RPTQ\nachieved a significant breakthrough by utilizing 3-bit activation in LLMs for\nthe first time, resulting in a substantial reduction in memory usage. For\ninstance, quantizing OPT-175b can lead to a memory consumption reduction of up\nto 80%.",
        "pdf_link": "https://arxiv.org/pdf/2304.01089v4.pdf"
    },
    {
        "title": "Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?",
        "authors": [
            "Wen Shen",
            "Lei Cheng",
            "Yuxiao Yang",
            "Mingjie Li",
            "Quanshi Zhang"
        ],
        "published": "2023-04-03T15:39:35Z",
        "summary": "In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.",
        "pdf_link": "https://arxiv.org/pdf/2304.01083v1.pdf"
    },
    {
        "title": "Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?",
        "authors": [
            "Adaku Uchendu",
            "Jooyoung Lee",
            "Hua Shen",
            "Thai Le",
            "Ting-Hao 'Kenneth' Huang",
            "Dongwon Lee"
        ],
        "published": "2023-04-03T14:06:47Z",
        "summary": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git",
        "pdf_link": "https://arxiv.org/pdf/2304.01002v3.pdf"
    },
    {
        "title": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection",
        "authors": [
            "Maxime Labonne",
            "Sean Moran"
        ],
        "published": "2023-04-03T10:27:53Z",
        "summary": "This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.",
        "pdf_link": "https://arxiv.org/pdf/2304.01238v3.pdf"
    },
    {
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
        "authors": [
            "Yi Chen",
            "Rui Wang",
            "Haiyun Jiang",
            "Shuming Shi",
            "Ruifeng Xu"
        ],
        "published": "2023-04-03T05:29:58Z",
        "summary": "Evaluating the quality of generated text is a challenging task in NLP, due to\nthe inherent complexity and diversity of text. Recently, large language models\n(LLMs) have garnered significant attention due to their impressive performance\nin various tasks. Therefore, we present this paper to investigate the\neffectiveness of LLMs, especially ChatGPT, and explore ways to optimize their\nuse in assessing text quality. We compared three kinds of reference-free\nevaluation methods. The experimental results prove that ChatGPT is capable of\nevaluating text quality effectively from various perspectives without reference\nand demonstrates superior performance than most existing automatic metrics. In\nparticular, the Explicit Score, which utilizes ChatGPT to generate a numeric\nscore measuring text quality, is the most effective and reliable method among\nthe three exploited approaches. However, directly comparing the quality of two\ntexts may lead to suboptimal results. We believe this paper will provide\nvaluable insights for evaluating text quality with LLMs and have released the\nused data.",
        "pdf_link": "https://arxiv.org/pdf/2304.00723v3.pdf"
    },
    {
        "title": "A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets",
        "authors": [
            "Iva Bojic",
            "Josef Halim",
            "Verena Suharman",
            "Sreeja Tar",
            "Qi Chwen Ong",
            "Duy Phung",
            "Mathieu Ravaut",
            "Shafiq Joty",
            "Josip Car"
        ],
        "published": "2023-04-02T08:26:38Z",
        "summary": "Low-quality data can cause downstream problems in high-stakes applications.\nData-centric approach emphasizes on improving dataset quality to enhance model\nperformance. High-quality datasets are needed for general-purpose Large\nLanguage Models (LLMs) training, as well as for domain-specific models, which\nare usually small in size as it is costly to engage a large number of domain\nexperts for their creation. Thus, it is vital to ensure high-quality\ndomain-specific training data. In this paper, we propose a framework for\nenhancing the data quality of original datasets. We applied the proposed\nframework to four biomedical datasets and showed relative improvement of up to\n33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when\nusing back translation to enhance the original dataset quality.",
        "pdf_link": "https://arxiv.org/pdf/2304.00483v2.pdf"
    },
    {
        "title": "Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System",
        "authors": [
            "Pingchuan Ma",
            "Rui Ding",
            "Shuai Wang",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-04-02T07:27:49Z",
        "summary": "Exploring data is crucial in data analysis, as it helps users understand and\ninterpret the data more effectively. However, performing effective data\nexploration requires in-depth knowledge of the dataset and expertise in data\nanalysis techniques. Not being familiar with either can create obstacles that\nmake the process time-consuming and overwhelming for data analysts. To address\nthis issue, we introduce InsightPilot, an LLM (Large Language Model)-based,\nautomated data exploration system designed to simplify the data exploration\nprocess. InsightPilot automatically selects appropriate analysis intents, such\nas understanding, summarizing, and explaining. Then, these analysis intents are\nconcretized by issuing corresponding intentional queries (IQueries) to create a\nmeaningful and coherent exploration sequence. In brief, an IQuery is an\nabstraction and automation of data analysis operations, which mimics the\napproach of data analysts and simplifies the exploration process for users. By\nemploying an LLM to iteratively collaborate with a state-of-the-art insight\nengine via IQueries, InsightPilot is effective in analyzing real-world\ndatasets, enabling users to gain valuable insights through natural language\ninquiries. We demonstrate the effectiveness of InsightPilot in a case study,\nshowing how it can help users gain valuable insights from their datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.00477v2.pdf"
    },
    {
        "title": "Querying Large Language Models with SQL",
        "authors": [
            "Mohammed Saeed",
            "Nicola De Cao",
            "Paolo Papotti"
        ],
        "published": "2023-04-02T06:58:14Z",
        "summary": "In many use-cases, information is stored in text but not available in\nstructured data. However, extracting data from natural language text to\nprecisely fit a schema, and thus enable querying, is a challenging task. With\nthe rise of pre-trained Large Language Models (LLMs), there is now an effective\nsolution to store and use information extracted from massive corpora of text\ndocuments. Thus, we envision the use of SQL queries to cover a broad range of\ndata that is not captured by traditional databases by tapping the information\nin LLMs. To ground this vision, we present Galois, a prototype based on a\ntraditional database architecture, but with new physical operators for querying\nthe underlying LLM. The main idea is to execute some operators of the the query\nplan with prompts that retrieve data from the LLM. For a large class of SQL\nqueries, querying LLMs returns well structured relations, with encouraging\nqualitative results. Preliminary experimental results make pre-trained LLMs a\npromising addition to the field of database systems, introducing a new\ndirection for hybrid query processing. However, we pinpoint several research\nchallenges that must be addressed to build a DBMS that exploits LLMs. While\nsome of these challenges necessitate integrating concepts from the NLP\nliterature, others offer novel research avenues for the DB community.",
        "pdf_link": "https://arxiv.org/pdf/2304.00472v3.pdf"
    },
    {
        "title": "LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models",
        "authors": [
            "Patrik Puchert",
            "Poonam Poonam",
            "Christian van Onzenoodt",
            "Timo Ropinski"
        ],
        "published": "2023-04-02T05:47:09Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the dataset, which often covers an\nentire field. This field-based evaluation, is problematic with respect to\ntransparency and model improvement. A stratified evaluation could instead\nreveal subfields, where hallucinations are more likely to occur and thus help\nto better assess LLMs' risks and guide their further development. To support\nsuch stratified evaluations, we propose LLMMaps as a novel visualization\ntechnique that enables users to evaluate LLMs' performance with respect to Q&A\ndatasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities\nin different subfields, by transforming Q&A datasets as well as LLM responses\ninto an internal knowledge structure. An extension for comparative\nvisualization furthermore, allows for the detailed comparison of multiple LLMs.\nTo assess LLMMaps we use them to conduct a comparative analysis of several\nstate-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as\nwell as two qualitative user evaluations. All necessary source code and data\nfor generating LLMMaps to be used in scientific publications and elsewhere is\navailable on GitHub: https://github.com/viscom-ulm/LLMMaps",
        "pdf_link": "https://arxiv.org/pdf/2304.00457v3.pdf"
    },
    {
        "title": "Towards Healthy AI: Large Language Models Need Therapists Too",
        "authors": [
            "Baihan Lin",
            "Djallel Bouneffouf",
            "Guillermo Cecchi",
            "Kush R. Varshney"
        ],
        "published": "2023-04-02T00:39:12Z",
        "summary": "Recent advances in large language models (LLMs) have led to the development\nof powerful AI chatbots capable of engaging in natural and human-like\nconversations. However, these chatbots can be potentially harmful, exhibiting\nmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to\nbe safe, trustworthy and ethical. To create healthy AI systems, we present the\nSafeguardGPT framework that uses psychotherapy to correct for these harmful\nbehaviors in AI chatbots. The framework involves four types of AI agents: a\nChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate the\neffectiveness of SafeguardGPT through a working example of simulating a social\nconversation. Our results show that the framework can improve the quality of\nconversations between AI chatbots and humans. Although there are still several\nchallenges and directions to be addressed in the future, SafeguardGPT provides\na promising approach to improving the alignment between AI chatbots and human\nvalues. By incorporating psychotherapy and reinforcement learning techniques,\nthe framework enables AI chatbots to learn and adapt to human preferences and\nvalues in a safe and ethical way, contributing to the development of a more\nhuman-centric and responsible AI.",
        "pdf_link": "https://arxiv.org/pdf/2304.00416v1.pdf"
    },
    {
        "title": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection",
        "authors": [
            "Yizheng Chen",
            "Zhoujie Ding",
            "Lamya Alowain",
            "Xinyun Chen",
            "David Wagner"
        ],
        "published": "2023-04-01T23:29:14Z",
        "summary": "We propose and release a new vulnerable source code dataset. We curate the\ndataset by crawling security issue websites, extracting vulnerability-fixing\ncommits and source codes from the corresponding projects. Our new dataset\ncontains 18,945 vulnerable functions spanning 150 CWEs and 330,492\nnon-vulnerable functions extracted from 7,514 commits. Our dataset covers 295\nmore projects than all previous datasets combined.\n  Combining our new dataset with previous datasets, we present an analysis of\nthe challenges and promising research directions of using deep learning for\ndetecting software vulnerabilities. We study 11 model architectures belonging\nto 4 families. Our results show that deep learning is still not ready for\nvulnerability detection, due to high false positive rate, low F1 score, and\ndifficulty of detecting hard CWEs. In particular, we demonstrate an important\ngeneralization challenge for the deployment of deep learning-based models. We\nshow that increasing the volume of training data may not further improve the\nperformance of deep learning models for vulnerability detection, but might be\nuseful to improve the generalization ability to unseen projects.\n  We also identify hopeful future research directions. We demonstrate that\nlarge language models (LLMs) are a promising research direction for ML-based\nvulnerability detection, outperforming Graph Neural Networks (GNNs) with\ncode-structure features in our experiments. Moreover, developing source code\nspecific pre-training objectives is a promising research direction to improve\nthe vulnerability detection performance.",
        "pdf_link": "https://arxiv.org/pdf/2304.00409v2.pdf"
    },
    {
        "title": "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
        "authors": [
            "Chunqiu Steven Xia",
            "Lingming Zhang"
        ],
        "published": "2023-04-01T20:57:33Z",
        "summary": "Automated Program Repair (APR) aims to automatically generate patches for\nbuggy programs. Recent APR work has been focused on leveraging modern Large\nLanguage Models (LLMs) to directly generate patches for APR. Such LLM-based APR\ntools work by first constructing an input prompt built using the original buggy\ncode and then queries the LLM to generate patches. While the LLM-based APR\ntools are able to achieve state-of-the-art results, it still follows the\nclassic Generate and Validate repair paradigm of first generating lots of\npatches and then validating each one afterwards. This not only leads to many\nrepeated patches that are incorrect but also miss the crucial information in\ntest failures as well as in plausible patches.\n  To address these limitations, we propose ChatRepair, the first fully\nautomated conversation-driven APR approach that interleaves patch generation\nwith instant feedback to perform APR in a conversational style. ChatRepair\nfirst feeds the LLM with relevant test failure information to start with, and\nthen learns from both failures and successes of earlier patching attempts of\nthe same bug for more powerful APR. For earlier patches that failed to pass all\ntests, we combine the incorrect patches with their corresponding relevant test\nfailure information to construct a new prompt for the LLM to generate the next\npatch. In this way, we can avoid making the same mistakes. For earlier patches\nthat passed all the tests, we further ask the LLM to generate alternative\nvariations of the original plausible patches. In this way, we can further build\non and learn from earlier successes to generate more plausible patches to\nincrease the chance of having correct patches. While our approach is general,\nwe implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT.\nBy calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs\nfor \\$0.42 each!",
        "pdf_link": "https://arxiv.org/pdf/2304.00385v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics",
        "authors": [
            "Jason Holmes",
            "Zhengliang Liu",
            "Lian Zhang",
            "Yuzhen Ding",
            "Terence T. Sio",
            "Lisa A. McGee",
            "Jonathan B. Ashman",
            "Xiang Li",
            "Tianming Liu",
            "Jiajian Shen",
            "Wei Liu"
        ],
        "published": "2023-04-01T06:04:58Z",
        "summary": "We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.",
        "pdf_link": "https://arxiv.org/pdf/2304.01938v1.pdf"
    },
    {
        "title": "Large language models can rate news outlet credibility",
        "authors": [
            "Kai-Cheng Yang",
            "Filippo Menczer"
        ],
        "published": "2023-04-01T05:04:06Z",
        "summary": "Although large language models (LLMs) have shown exceptional performance in\nvarious natural language processing tasks, they are prone to hallucinations.\nState-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue\nby gathering information directly from the internet to ground their answers. In\nthis setting, the capacity to distinguish trustworthy sources is critical for\nproviding appropriate accuracy contexts to users. Here we assess whether\nChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With\nappropriate instructions, ChatGPT can provide ratings for a diverse set of news\noutlets, including those in non-English languages and satirical sources, along\nwith contextual explanations. Our results show that these ratings correlate\nwith those from human experts (Spearmam's $\\rho=0.54, p<0.001$). These findings\nsuggest that LLMs could be an affordable reference for credibility ratings in\nfact-checking applications. Future LLMs should enhance their alignment with\nhuman expert judgments of source credibility to improve information accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2304.00228v1.pdf"
    },
    {
        "title": "Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts",
        "authors": [
            "Ryan Koo",
            "Anna Martin",
            "Linghe Wang",
            "Dongyeop Kang"
        ],
        "published": "2023-03-31T20:33:03Z",
        "summary": "Scholarly writing presents a complex space that generally follows a\nmethodical procedure to plan and produce both rationally sound and creative\ncompositions. Recent works involving large language models (LLM) demonstrate\nconsiderable success in text generation and revision tasks; however, LLMs still\nstruggle to provide structural and creative feedback on the document level that\nis crucial to academic writing. In this paper, we introduce a novel taxonomy\nthat categorizes scholarly writing behaviors according to intention, writer\nactions, and the information types of the written data. We also provide\nManuScript, an original dataset annotated with a simplified version of our\ntaxonomy to show writer actions and the intentions behind them. Motivated by\ncognitive writing theory, our taxonomy for scientific papers includes three\nlevels of categorization in order to trace the general writing flow and\nidentify the distinct writer activities embedded within each higher-level\nprocess. ManuScript intends to provide a complete picture of the scholarly\nwriting process by capturing the linearity and non-linearity of writing\ntrajectory, such that writing assistants can provide stronger feedback and\nsuggestions on an end-to-end level. The collected writing trajectories are\nviewed at https://minnesotanlp.github.io/REWARD_demo/",
        "pdf_link": "https://arxiv.org/pdf/2304.00121v1.pdf"
    },
    {
        "title": "Enhancing Large Language Models with Climate Resources",
        "authors": [
            "Mathias Kraus",
            "Julia Anna Bingler",
            "Markus Leippold",
            "Tobias Schimanski",
            "Chiara Colesanti Senni",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Nicolas Webersinke"
        ],
        "published": "2023-03-31T20:24:14Z",
        "summary": "Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.",
        "pdf_link": "https://arxiv.org/pdf/2304.00116v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models",
        "authors": [
            "Wayne Xin Zhao",
            "Kun Zhou",
            "Junyi Li",
            "Tianyi Tang",
            "Xiaolei Wang",
            "Yupeng Hou",
            "Yingqian Min",
            "Beichen Zhang",
            "Junjie Zhang",
            "Zican Dong",
            "Yifan Du",
            "Chen Yang",
            "Yushuo Chen",
            "Zhipeng Chen",
            "Jinhao Jiang",
            "Ruiyang Ren",
            "Yifan Li",
            "Xinyu Tang",
            "Zikang Liu",
            "Peiyu Liu",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-03-31T17:28:46Z",
        "summary": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.",
        "pdf_link": "https://arxiv.org/pdf/2303.18223v13.pdf"
    },
    {
        "title": "Assessing Language Model Deployment with Risk Cards",
        "authors": [
            "Leon Derczynski",
            "Hannah Rose Kirk",
            "Vidhisha Balachandran",
            "Sachin Kumar",
            "Yulia Tsvetkov",
            "M. R. Leiser",
            "Saif Mohammad"
        ],
        "published": "2023-03-31T16:45:42Z",
        "summary": "This paper introduces RiskCards, a framework for structured assessment and\ndocumentation of risks associated with an application of language models. As\nwith all language, text generated by language models can be harmful, or used to\nbring about harm. Automating language generation adds both an element of scale\nand also more subtle or emergent undesirable tendencies to the generated text.\nPrior work establishes a wide variety of language model harms to many different\nactors: existing taxonomies identify categories of harms posed by language\nmodels; benchmarks establish automated tests of these harms; and documentation\nstandards for models, tasks and datasets encourage transparent reporting.\nHowever, there is no risk-centric framework for documenting the complexity of a\nlandscape in which some risks are shared across models and contexts, while\nothers are specific, and where certain conditions may be required for risks to\nmanifest as harms. RiskCards address this methodological gap by providing a\ngeneric framework for assessing the use of a given language model in a given\nscenario. Each RiskCard makes clear the routes for the risk to manifest harm,\ntheir placement in harm taxonomies, and example prompt-output pairs. While\nRiskCards are designed to be open-source, dynamic and participatory, we present\na \"starter set\" of RiskCards taken from a broad literature survey, each of\nwhich details a concrete risk presentation. Language model RiskCards initiate a\ncommunity knowledge base which permits the mapping of risks and harms to a\nspecific model or its application scenario, ultimately contributing to a\nbetter, safer and shared understanding of the risk landscape.",
        "pdf_link": "https://arxiv.org/pdf/2303.18190v1.pdf"
    },
    {
        "title": "CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Mizanur Rahman",
            "Israt Jahan",
            "Enamul Hoque",
            "Jimmy Huang"
        ],
        "published": "2023-03-31T15:39:54Z",
        "summary": "Debatepedia is a publicly available dataset consisting of arguments and\ncounter-arguments on controversial topics that has been widely used for the\nsingle-document query-focused abstractive summarization task in recent years.\nHowever, it has been recently found that this dataset is limited by noise and\neven most queries in this dataset do not have any relevance to the respective\ndocument. In this paper, we present a methodology for cleaning the Debatepedia\ndataset by leveraging the generative power of large language models to make it\nsuitable for query-focused abstractive summarization. More specifically, we\nharness the language generation capabilities of ChatGPT to regenerate its\nqueries. We evaluate the effectiveness of the proposed ChatGPT annotated\nversion of the Debatepedia dataset using several benchmark summarization models\nand demonstrate that the newly annotated version of Debatepedia outperforms the\noriginal dataset in terms of both query relevance as well as summary generation\nquality. We will make this annotated and cleaned version of the dataset\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.06147v1.pdf"
    },
    {
        "title": "BERTino: an Italian DistilBERT model",
        "authors": [
            "Matteo Muffo",
            "Enrico Bertino"
        ],
        "published": "2023-03-31T15:07:40Z",
        "summary": "The recent introduction of Transformers language representation models\nallowed great improvements in many natural language processing (NLP) tasks.\nHowever, if on one hand the performances achieved by this kind of architectures\nare surprising, on the other their usability is limited by the high number of\nparameters which constitute their network, resulting in high computational and\nmemory demands. In this work we present BERTino, a DistilBERT model which\nproposes to be the first lightweight alternative to the BERT architecture\nspecific for the Italian language. We evaluated BERTino on the Italian ISDT,\nItalian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining\nF1 scores comparable to those obtained by a BERTBASE with a remarkable\nimprovement in training and inference speed.",
        "pdf_link": "https://arxiv.org/pdf/2303.18121v1.pdf"
    },
    {
        "title": "Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations",
        "authors": [
            "Jungo Kasai",
            "Yuhei Kasai",
            "Keisuke Sakaguchi",
            "Yutaro Yamada",
            "Dragomir Radev"
        ],
        "published": "2023-03-31T13:04:47Z",
        "summary": "As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years, including the current\nyear. Our team comprises native Japanese-speaking NLP researchers and a\npracticing cardiologist based in Japan. Our experiments show that GPT-4\noutperforms ChatGPT and GPT-3 and passes all six years of the exams,\nhighlighting LLMs' potential in a language that is typologically distant from\nEnglish. However, our evaluation also exposes critical limitations of the\ncurrent LLM APIs. First, LLMs sometimes select prohibited choices that should\nbe strictly avoided in medical practice in Japan, such as suggesting\neuthanasia. Further, our analysis shows that the API costs are generally higher\nand the maximum context size is smaller for Japanese because of the way\nnon-Latin scripts are currently tokenized in the pipeline. We release our\nbenchmark as Igaku QA as well as all model outputs and exam metadata. We hope\nthat our results and benchmark will spur progress on more diverse applications\nof LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.",
        "pdf_link": "https://arxiv.org/pdf/2303.18027v2.pdf"
    },
    {
        "title": "GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors",
        "authors": [
            "Dongyeop Jang",
            "Tae-Rim Yun",
            "Choong-Yeol Lee",
            "Young-Kyu Kwon",
            "Chang-Eop Kim"
        ],
        "published": "2023-03-31T05:43:21Z",
        "summary": "Traditional Korean medicine (TKM) emphasizes individualized diagnosis and\ntreatment. This uniqueness makes AI modeling difficult due to limited data and\nimplicit processes. Large language models (LLMs) have demonstrated impressive\nmedical inference, even without advanced training in medical texts. This study\nassessed the capabilities of GPT-4 in TKM, using the Korean National Licensing\nExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The\nK-NLEKMD, administered by a national organization, encompasses 12 major\nsubjects in TKM. We optimized prompts with Chinese-term annotation, English\ntranslation for questions and instruction, exam-optimized instruction, and\nself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,\nsurpassing both the examination's average pass mark of 60% and the 40% minimum\nfor each subject. The gradual introduction of language-related prompts and\nprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.\nGPT-4 showed low accuracy in subjects including public health &\nmedicine-related law, internal medicine (2) which are localized in Korea and\nTKM. The model's accuracy was lower for questions requiring TKM-specialized\nknowledge. It exhibited higher accuracy in diagnosis-based and recall-based\nquestions than in intervention-based questions. A positive correlation was\nobserved between the consistency and accuracy of GPT-4's responses. This study\nunveils both the potential and challenges of applying LLMs to TKM. These\nfindings underline the potential of LLMs like GPT-4 in culturally adapted\nmedicine, especially TKM, for tasks such as clinical assistance, medical\neducation, and research. But they also point towards the necessity for the\ndevelopment of methods to mitigate cultural bias inherent in large language\nmodels and validate their efficacy in real-world clinical settings.",
        "pdf_link": "https://arxiv.org/pdf/2303.17807v2.pdf"
    },
    {
        "title": "AceCoder: Utilizing Existing Code to Enhance Code Generation",
        "authors": [
            "Jia Li",
            "Yunfei Zhao",
            "Yongmin Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023-03-31T02:57:15Z",
        "summary": "Large Language Models (LLMs) have shown great success in code generation.\nLLMs take as the input a prompt and output the code. A key question is how to\nmake prompts (i.e., Prompting Techniques). Existing prompting techniques are\ndesigned for natural language generation and have low accuracy in code\ngeneration.\n  In this paper, we propose a new prompting technique named AceCoder. Our\nmotivation is that code generation meets two unique challenges (i.e.,\nrequirement understanding and code implementation). AceCoder contains two novel\nmechanisms (i.e., guided code generation and example retrieval) to solve these\nchallenges. (1) Guided code generation asks LLMs first to analyze requirements\nand output an intermediate preliminary (e.g., test cases). The preliminary is\nused to clarify requirements and tell LLMs \"what to write\". (2) Example\nretrieval selects similar programs as examples in prompts, which provide lots\nof relevant content (e.g., algorithms, APIs) and teach LLMs \"how to write\". We\napply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public\nbenchmarks using the Pass@k. Results show that AceCoder can significantly\nimprove the performance of LLMs on code generation. (1) In terms of Pass@1,\nAceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,\n70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with\ndifferent sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,\nand JavaScript). (3) Human evaluation shows human developers prefer programs\nfrom AceCoder.",
        "pdf_link": "https://arxiv.org/pdf/2303.17780v3.pdf"
    },
    {
        "title": "Can ChatGPT be used to generate scientific hypotheses?",
        "authors": [
            "Yang Jeong Park",
            "Daniel Kaplan",
            "Zhichu Ren",
            "Chia-Wei Hsu",
            "Changhao Li",
            "Haowei Xu",
            "Sipei Li",
            "Ju Li"
        ],
        "published": "2023-03-30T20:40:52Z",
        "summary": "We investigate whether large language models can perform the creative\nhypothesis generation that human researchers regularly do. While the error rate\nis high, generative AI seems to be able to effectively structure vast amounts\nof scientific knowledge and provide interesting and testable hypotheses. The\nfuture scientific enterprise may include synergistic efforts with a swarm of\n\"hypothesis machines\", challenged by automated experimentation and adversarial\npeer reviews.",
        "pdf_link": "https://arxiv.org/pdf/2304.12208v1.pdf"
    },
    {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "Shashank Gupta",
            "Bodhisattwa Prasad Majumder",
            "Katherine Hermann",
            "Sean Welleck",
            "Amir Yazdanbakhsh",
            "Peter Clark"
        ],
        "published": "2023-03-30T18:30:01Z",
        "summary": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.",
        "pdf_link": "https://arxiv.org/pdf/2303.17651v2.pdf"
    },
    {
        "title": "Recognition, recall, and retention of few-shot memories in large language models",
        "authors": [
            "A. Emin Orhan"
        ],
        "published": "2023-03-30T17:26:16Z",
        "summary": "The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.",
        "pdf_link": "https://arxiv.org/pdf/2303.17557v1.pdf"
    },
    {
        "title": "Language Models can Solve Computer Tasks",
        "authors": [
            "Geunwoo Kim",
            "Pierre Baldi",
            "Stephen McAleer"
        ],
        "published": "2023-03-30T16:01:52Z",
        "summary": "Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting with\nexternal feedback. We find that RCI combined with CoT performs better than\neither separately. Our code can be found here:\nhttps://github.com/posgnu/rci-agent.",
        "pdf_link": "https://arxiv.org/pdf/2303.17491v3.pdf"
    },
    {
        "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
        "authors": [
            "Xinhao Mei",
            "Chutong Meng",
            "Haohe Liu",
            "Qiuqiang Kong",
            "Tom Ko",
            "Chengqi Zhao",
            "Mark D. Plumbley",
            "Yuexian Zou",
            "Wenwu Wang"
        ],
        "published": "2023-03-30T14:07:47Z",
        "summary": "The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.",
        "pdf_link": "https://arxiv.org/pdf/2303.17395v1.pdf"
    },
    {
        "title": "Yes but.. Can ChatGPT Identify Entities in Historical Documents?",
        "authors": [
            "Carlos-Emiliano Gonz\u00e1lez-Gallardo",
            "Emanuela Boros",
            "Nancy Girdhar",
            "Ahmed Hamdi",
            "Jose G. Moreno",
            "Antoine Doucet"
        ],
        "published": "2023-03-30T12:23:39Z",
        "summary": "Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2303.17322v1.pdf"
    },
    {
        "title": "Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure",
        "authors": [
            "Philipp Koralus",
            "Vincent Wang-Ma\u015bcianica"
        ],
        "published": "2023-03-30T10:32:18Z",
        "summary": "Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.",
        "pdf_link": "https://arxiv.org/pdf/2303.17276v1.pdf"
    },
    {
        "title": "The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling",
        "authors": [
            "Joey \u00d6hman",
            "Severine Verlinden",
            "Ariel Ekgren",
            "Amaru Cuba Gyllensten",
            "Tim Isbister",
            "Evangelia Gogoulou",
            "Fredrik Carlsson",
            "Magnus Sahlgren"
        ],
        "published": "2023-03-30T06:42:22Z",
        "summary": "Pre-training Large Language Models (LLMs) require massive amounts of text\ndata, and the performance of the LLMs typically correlates with the scale and\nquality of the datasets. This means that it may be challenging to build LLMs\nfor smaller languages such as Nordic ones, where the availability of text\ncorpora is limited. In order to facilitate the development of the LLMS in the\nNordic languages, we curate a high-quality dataset consisting of 1.2TB of text,\nin all of the major North Germanic languages (Danish, Icelandic, Norwegian, and\nSwedish), as well as some high-quality English data. This paper details our\nconsiderations and processes for collecting, cleaning, and filtering the\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2303.17183v1.pdf"
    },
    {
        "title": "Advances in apparent conceptual physics reasoning in GPT-4",
        "authors": [
            "Colin G. West"
        ],
        "published": "2023-03-29T20:32:40Z",
        "summary": "ChatGPT is built on a large language model trained on an enormous corpus of\nhuman text to emulate human conversation. Despite lacking any explicit\nprogramming regarding the laws of physics, recent work has demonstrated that\nGPT-3.5 could pass an introductory physics course at some nominal level and\nregister something close to a minimal understanding of Newtonian Mechanics on\nthe Force Concept Inventory. This work replicates those results and also\ndemonstrates that the latest version, GPT-4, has reached a much higher mark in\nthe latter context. Indeed, its responses come quite close to perfectly\ndemonstrating expert-level competence, with a few very notable exceptions and\nlimitations. We briefly comment on the implications of this for the future of\nphysics education and pedagogy.",
        "pdf_link": "https://arxiv.org/pdf/2303.17012v3.pdf"
    },
    {
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
        "authors": [
            "Yang Liu",
            "Dan Iter",
            "Yichong Xu",
            "Shuohang Wang",
            "Ruochen Xu",
            "Chenguang Zhu"
        ],
        "published": "2023-03-29T12:46:54Z",
        "summary": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval",
        "pdf_link": "https://arxiv.org/pdf/2303.16634v3.pdf"
    },
    {
        "title": "LMExplainer: a Knowledge-Enhanced Explainer for Language Models",
        "authors": [
            "Zichen Chen",
            "Ambuj K Singh",
            "Misha Sra"
        ],
        "published": "2023-03-29T08:59:44Z",
        "summary": "Large language models (LLMs) such as GPT-4 are very powerful and can process\ndifferent kinds of natural language processing (NLP) tasks. However, it can be\ndifficult to interpret the results due to the multi-layer nonlinear model\nstructure and millions of parameters. A lack of clarity and understanding of\nhow the language models (LMs) work can make them unreliable, difficult to\ntrust, and potentially dangerous for use in real-world scenarios. Most recent\nworks exploit attention weights to provide explanations for LM predictions.\nHowever, pure attention-based explanations are unable to support the growing\ncomplexity of LMs, and cannot reason about their decision-making processes. We\npropose LMExplainer, a knowledge-enhanced explainer for LMs that can provide\nhuman-understandable explanations. We use a knowledge graph (KG) and a graph\nattention neural network to extract the key decision signals of the LM. We\nfurther explore whether interpretation can also help the AI understand the task\nbetter. Our experimental results show that LMExplainer outperforms existing\nLM+KG methods on CommonsenseQA and OpenBookQA. We compare the explanation\nresults with generated explanation methods and human-annotated results. The\ncomparison shows our method can provide more comprehensive and clearer\nexplanations. LMExplainer demonstrates the potential to enhance model\nperformance and furnish explanations for the LM reasoning process in natural\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2303.16537v2.pdf"
    },
    {
        "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
        "authors": [
            "Namrata Shivagunde",
            "Vladislav Lialin",
            "Anna Rumshisky"
        ],
        "published": "2023-03-29T04:00:53Z",
        "summary": "Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.",
        "pdf_link": "https://arxiv.org/pdf/2303.16445v3.pdf"
    },
    {
        "title": "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models",
        "authors": [
            "Ning Bian",
            "Xianpei Han",
            "Le Sun",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Shanshan Jiang",
            "Bin Dong"
        ],
        "published": "2023-03-29T03:05:43Z",
        "summary": "Large language models (LLMs) have made significant progress in NLP. However,\ntheir ability to memorize, represent, and leverage commonsense knowledge has\nbeen a well-known pain point. In this paper, we specifically focus on ChatGPT,\na widely used and easily accessible LLM, and ask the following questions: (1)\nCan ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of\nthe underlying commonsense knowledge for answering a specific question? (3) Is\nChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage\ncommonsense for answering questions? We conduct a series of experiments on 11\ndatasets to evaluate ChatGPT's commonsense abilities, including answering\ncommonsense questions, identifying necessary knowledge, generating knowledge\ndescriptions, and using knowledge descriptions to answer questions again.\nExperimental results show that: (1) ChatGPT can achieve good QA accuracies in\ncommonsense tasks, while still struggling with certain domains of datasets. (2)\nChatGPT is knowledgeable, and can accurately generate most of the commonsense\nknowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an\ninexperienced commonsense problem solver, which cannot precisely identify the\nneeded commonsense for answering a specific question. These findings raise the\nneed to explore improved mechanisms for effectively incorporating commonsense\ninto LLMs like ChatGPT, such as better instruction following and commonsense\nguidance.",
        "pdf_link": "https://arxiv.org/pdf/2303.16421v2.pdf"
    },
    {
        "title": "Writing Assistants Should Model Social Factors of Language",
        "authors": [
            "Vivek Kulkarni",
            "Vipul Raheja"
        ],
        "published": "2023-03-28T19:38:57Z",
        "summary": "Intelligent writing assistants powered by large language models (LLMs) are\nmore popular today than ever before, but their further widespread adoption is\nprecluded by sub-optimal performance. In this position paper, we argue that a\nmajor reason for this sub-optimal performance and adoption is a singular focus\non the information content of language while ignoring its social aspects. We\nanalyze the different dimensions of these social factors in the context of\nwriting assistants and propose their incorporation into building smarter, more\neffective, and truly personalized writing assistants that would enrich the user\nexperience and contribute to increased user adoption.",
        "pdf_link": "https://arxiv.org/pdf/2303.16275v1.pdf"
    },
    {
        "title": "Training Language Models with Language Feedback at Scale",
        "authors": [
            "J\u00e9r\u00e9my Scheurer",
            "Jon Ander Campos",
            "Tomasz Korbak",
            "Jun Shern Chan",
            "Angelica Chen",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2023-03-28T17:04:15Z",
        "summary": "Pretrained language models often generate outputs that are not in line with\nhuman preferences, such as harmful text or factually incorrect summaries.\nRecent work approaches the above issues by learning from a simple form of human\nfeedback: comparisons between pairs of model-generated outputs. However,\ncomparison feedback only conveys limited information about human preferences.\nIn this paper, we introduce Imitation learning from Language Feedback (ILF), a\nnew approach that utilizes more informative language feedback. ILF consists of\nthree steps that are applied iteratively: first, conditioning the language\nmodel on the input, an initial LM output, and feedback to generate refinements.\nSecond, selecting the refinement incorporating the most feedback. Third,\nfinetuning the language model to maximize the likelihood of the chosen\nrefinement given the input. We show theoretically that ILF can be viewed as\nBayesian Inference, similar to Reinforcement Learning from human feedback. We\nevaluate ILF's effectiveness on a carefully-controlled toy task and a realistic\nsummarization task. Our experiments demonstrate that large language models\naccurately incorporate feedback and that finetuning with ILF scales well with\nthe dataset size, even outperforming finetuning on human summaries. Learning\nfrom both language and comparison feedback outperforms learning from each\nalone, achieving human-level summarization performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.16755v3.pdf"
    },
    {
        "title": "Hallucinations in Large Multilingual Translation Models",
        "authors": [
            "Nuno M. Guerreiro",
            "Duarte Alves",
            "Jonas Waldendorf",
            "Barry Haddow",
            "Alexandra Birch",
            "Pierre Colombo",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2023-03-28T16:17:59Z",
        "summary": "Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.",
        "pdf_link": "https://arxiv.org/pdf/2303.16104v1.pdf"
    },
    {
        "title": "Improving Code Generation by Training with Natural Language Feedback",
        "authors": [
            "Angelica Chen",
            "J\u00e9r\u00e9my Scheurer",
            "Tomasz Korbak",
            "Jon Ander Campos",
            "Jun Shern Chan",
            "Samuel R. Bowman",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2023-03-28T16:15:31Z",
        "summary": "The potential for pre-trained large language models (LLMs) to use natural\nlanguage feedback at inference time has been an exciting recent development. We\nbuild upon this observation by formalizing an algorithm for learning from\nnatural language feedback at training time instead, which we call Imitation\nlearning from Language Feedback (ILF). ILF requires only a small amount of\nhuman-written feedback during training and does not require the same feedback\nat test time, making it both user-friendly and sample-efficient. We further\nshow that ILF can be seen as a form of minimizing the KL divergence to the\nground truth distribution and demonstrate a proof-of-concept on a neural\nprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's\npass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python\nProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP and\nfine-tuning on repaired programs written by humans. Overall, our results\nsuggest that learning from human-written natural language feedback is both more\neffective and sample-efficient than training exclusively on demonstrations for\nimproving an LLM's performance on code generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.16749v2.pdf"
    },
    {
        "title": "Synthetically generated text for supervised text analysis",
        "authors": [
            "Andrew Halterman"
        ],
        "published": "2023-03-28T14:55:13Z",
        "summary": "Supervised text models are a valuable tool for political scientists but\npresent several obstacles to their use, including the expense of hand-labeling\ndocuments, the difficulty of retrieving rare relevant documents for annotation,\nand copyright and privacy concerns involved in sharing annotated documents.\nThis article proposes a partial solution to these three issues, in the form of\ncontrolled generation of synthetic text with large language models. I provide a\nconceptual overview of text generation, guidance on when researchers should\nprefer different techniques for generating synthetic text, a discussion of\nethics, and a simple technique for improving the quality of synthetic text. I\ndemonstrate the usefulness of synthetic text with three applications:\ngenerating synthetic tweets describing the fighting in Ukraine, synthetic news\narticles describing specified political events for training an event detection\nsystem, and a multilingual corpus of populist manifesto statements for training\na sentence-level populism classifier.",
        "pdf_link": "https://arxiv.org/pdf/2303.16028v1.pdf"
    },
    {
        "title": "Evaluation of ChatGPT for NLP-based Mental Health Applications",
        "authors": [
            "Bishal Lamichhane"
        ],
        "published": "2023-03-28T04:47:43Z",
        "summary": "Large language models (LLM) have been successful in several natural language\nunderstanding tasks and could be relevant for natural language processing\n(NLP)-based mental health application research. In this work, we report the\nperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three\ntext-based mental health classification tasks: stress detection (2-class\nclassification), depression detection (2-class classification), and suicidality\ndetection (5-class classification). We obtained annotated social media posts\nfor the three classification tasks from public datasets. Then ChatGPT API\nclassified the social media posts with an input prompt for classification. We\nobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression\ndetection, and suicidality detection, respectively. A baseline model that\nalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and\n0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a\npotential use of language models for mental health classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.15727v1.pdf"
    },
    {
        "title": "Pre-training Transformers for Knowledge Graph Completion",
        "authors": [
            "Sanxing Chen",
            "Hao Cheng",
            "Xiaodong Liu",
            "Jian Jiao",
            "Yangfeng Ji",
            "Jianfeng Gao"
        ],
        "published": "2023-03-28T02:10:37Z",
        "summary": "Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.",
        "pdf_link": "https://arxiv.org/pdf/2303.15682v1.pdf"
    },
    {
        "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning",
        "authors": [
            "Vladislav Lialin",
            "Vijeta Deshpande",
            "Anna Rumshisky"
        ],
        "published": "2023-03-28T00:06:38Z",
        "summary": "This paper presents a systematic overview and comparison of\nparameter-efficient fine-tuning methods covering over 40 papers published\nbetween February 2019 and February 2023. These methods aim to resolve the\ninfeasibility and impracticality of fine-tuning large language models by only\ntraining a small set of parameters. We provide a taxonomy that covers a broad\nrange of methods and present a detailed method comparison with a specific focus\non real-life efficiency and fine-tuning multibillion-scale language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.15647v1.pdf"
    },
    {
        "title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2023-03-27T22:30:39Z",
        "summary": "The performance of text summarization has been greatly boosted by pre-trained\nlanguage models. A main concern of existing methods is that most generated\nsummaries are not factually inconsistent with their source documents. To\nalleviate the problem, many efforts have focused on developing effective\nfactuality evaluation metrics based on natural language inference, question\nanswering, and syntactic dependency et al. However, these approaches are\nlimited by either their high computational complexity or the uncertainty\nintroduced by multi-component pipelines, resulting in only partial agreement\nwith human judgement. Most recently, large language models(LLMs) have shown\nexcellent performance in not only text generation but also language\ncomprehension. In this paper, we particularly explore ChatGPT's ability to\nevaluate factual inconsistency under a zero-shot setting by examining it on\nboth coarse-grained and fine-grained evaluation tasks including binary\nentailment inference, summary ranking, and consistency rating. Experimental\nresults indicate that ChatGPT generally outperforms previous evaluation metrics\nacross the three tasks, indicating its great potential for factual\ninconsistency evaluation. However, a closer inspection of ChatGPT's output\nreveals certain limitations including its preference for more lexically similar\ncandidates, false reasoning, and inadequate understanding of instructions.",
        "pdf_link": "https://arxiv.org/pdf/2303.15621v2.pdf"
    },
    {
        "title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",
        "authors": [
            "Walid Hariri"
        ],
        "published": "2023-03-27T21:27:58Z",
        "summary": "Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2304.02017v8.pdf"
    },
    {
        "title": "Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses",
        "authors": [
            "Wenshi Gu"
        ],
        "published": "2023-03-27T20:33:40Z",
        "summary": "In the field of Japanese-Chinese translation linguistics, the issue of\ncorrectly translating attributive clauses has persistently proven to be\nchallenging. Present-day machine translation tools often fail to accurately\ntranslate attributive clauses from Japanese to Chinese. In light of this, this\npaper investigates the linguistic problem underlying such difficulties, namely\nhow does the semantic role of the modified noun affect the selection of\ntranslation patterns for attributive clauses, from a linguistic perspective. To\nad-dress these difficulties, a pre-edit scheme is proposed, which aims to\nenhance the accuracy of translation. Furthermore, we propose a novel two-step\nprompt strategy, which combines this pre-edit scheme with ChatGPT, currently\nthe most widely used large language model. This prompt strategy is capable of\noptimizing translation input in zero-shot scenarios and has been demonstrated\nto improve the average translation accuracy score by over 35%.",
        "pdf_link": "https://arxiv.org/pdf/2303.15587v1.pdf"
    },
    {
        "title": "TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models",
        "authors": [
            "Md Kamrul Hasan",
            "Md Saiful Islam",
            "Sangwu Lee",
            "Wasifur Rahman",
            "Iftekhar Naim",
            "Mohammed Ibrahim Khan",
            "Ehsan Hoque"
        ],
        "published": "2023-03-27T17:54:32Z",
        "summary": "Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.",
        "pdf_link": "https://arxiv.org/pdf/2303.15430v2.pdf"
    },
    {
        "title": "KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation",
        "authors": [
            "Di Wu",
            "Da Yin",
            "Kai-Wei Chang"
        ],
        "published": "2023-03-27T17:45:38Z",
        "summary": "Despite the significant advancements in keyphrase extraction and keyphrase\ngeneration methods, the predominant approach for evaluation mainly relies on\nexact matching with human references. This scheme fails to recognize systems\nthat generate keyphrases semantically equivalent to the references or diverse\nkeyphrases that carry practical utility. To better assess the capability of\nkeyphrase systems, we propose KPEval, a comprehensive evaluation framework\nconsisting of four critical aspects: reference agreement, faithfulness,\ndiversity, and utility. For each aspect, we design semantic-based metrics to\nreflect the evaluation objectives. Meta-evaluation studies demonstrate that our\nevaluation strategy correlates better with human preferences compared to a\nrange of previously proposed metrics. Using KPEval, we re-evaluate 21 keyphrase\nsystems and discover that (1) established model comparison results have\nblind-spots especially when considering reference-free evaluation; (2) large\nlanguage models are underestimated by prior evaluation works; and (3) there is\nno single best model that can excel in all the aspects.",
        "pdf_link": "https://arxiv.org/pdf/2303.15422v3.pdf"
    },
    {
        "title": "LMCanvas: Object-Oriented Interaction to Personalize Large Language Model-Powered Writing Environments",
        "authors": [
            "Tae Soo Kim",
            "Arghya Sarkar",
            "Yoonjoo Lee",
            "Minsuk Chang",
            "Juho Kim"
        ],
        "published": "2023-03-27T11:56:26Z",
        "summary": "Large language models (LLMs) can enhance writing by automating or supporting\nspecific tasks in writers' workflows (e.g., paraphrasing, creating analogies).\nLeveraging this capability, a collection of interfaces have been developed that\nprovide LLM-powered tools for specific writing tasks. However, these interfaces\nprovide limited support for writers to create personal tools for their own\nunique tasks, and may not comprehensively fulfill a writer's needs -- requiring\nthem to continuously switch between interfaces during writing. In this work, we\nenvision LMCanvas, an interface that enables writers to create their own\nLLM-powered writing tools and arrange their personal writing environment by\ninteracting with \"blocks\" in a canvas. In this interface, users can create text\nblocks to encapsulate writing and LLM prompts, model blocks for model parameter\nconfigurations, and connect these to create pipeline blocks that output\ngenerations. In this workshop paper, we discuss the design for LMCanvas and our\nplans to develop this concept.",
        "pdf_link": "https://arxiv.org/pdf/2303.15125v1.pdf"
    },
    {
        "title": "Large Language Models are Diverse Role-Players for Summarization Evaluation",
        "authors": [
            "Ning Wu",
            "Ming Gong",
            "Linjun Shou",
            "Shining Liang",
            "Daxin Jiang"
        ],
        "published": "2023-03-27T10:40:59Z",
        "summary": "Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. A document summary's quality can be\nassessed by human annotators on various criteria, both objective ones like\ngrammar and correctness, and subjective ones like informativeness,\nsuccinctness, and appeal. Most of the automatic evaluation methods like\nBLUE/ROUGE may be not able to adequately capture the above dimensions. In this\npaper, we propose a new evaluation framework based on LLMs, which provides a\ncomprehensive evaluation framework by comparing generated text and reference\ntext from both objective and subjective aspects. First, we propose to model\nobjective and subjective dimensions of generated text based on roleplayers\nprompting mechanism. Furthermore, we introduce a context-based prompting\nmechanism that is able to generate dynamic roleplayer profiles based on input\ncontext. Finally, we design a multi-roleplayer prompting technology based on\nbatch prompting and integrate multiple outputs into the final evaluation\nresults. Experimental results on three real datasets for summarization show\nthat our model is highly competitive and has a very high consistency with human\nannotators.",
        "pdf_link": "https://arxiv.org/pdf/2303.15078v3.pdf"
    },
    {
        "title": "Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain",
        "authors": [
            "Xu Liu",
            "Mengyue Zhou",
            "Gaosheng Shi",
            "Yu Du",
            "Lin Zhao",
            "Zihao Wu",
            "David Liu",
            "Tianming Liu",
            "Xintao Hu"
        ],
        "published": "2023-03-27T01:41:48Z",
        "summary": "Linking computational natural language processing (NLP) models and neural\nresponses to language in the human brain on the one hand facilitates the effort\ntowards disentangling the neural representations underpinning language\nperception, on the other hand provides neurolinguistics evidence to evaluate\nand improve NLP models. Mappings of an NLP model's representations of and the\nbrain activities evoked by linguistic input are typically deployed to reveal\nthis symbiosis. However, two critical problems limit its advancement: 1) The\nmodel's representations (artificial neurons, ANs) rely on layer-level\nembeddings and thus lack fine-granularity; 2) The brain activities (biological\nneurons, BNs) are limited to neural recordings of isolated cortical unit (i.e.,\nvoxel/region) and thus lack integrations and interactions among brain\nfunctions. To address those problems, in this study, we 1) define ANs with\nfine-granularity in transformer-based NLP models (BERT in this study) and\nmeasure their temporal activations to input text sequences; 2) define BNs as\nfunctional brain networks (FBNs) extracted from functional magnetic resonance\nimaging (fMRI) data to capture functional interactions in the brain; 3) couple\nANs and BNs by maximizing the synchronization of their temporal activations.\nOur experimental results demonstrate 1) The activations of ANs and BNs are\nsignificantly synchronized; 2) the ANs carry meaningful linguistic/semantic\ninformation and anchor to their BN signatures; 3) the anchored BNs are\ninterpretable in a neurolinguistic context. Overall, our study introduces a\nnovel, general, and effective framework to link transformer-based NLP models\nand neural activities in response to language and may provide novel insights\nfor future studies such as brain-inspired evaluation and development of NLP\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2303.14871v1.pdf"
    },
    {
        "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
        "authors": [
            "Xinlei He",
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-03-26T21:12:36Z",
        "summary": "Nowadays, powerful large language models (LLMs) such as ChatGPT have\ndemonstrated revolutionary power in a variety of tasks. Consequently, the\ndetection of machine-generated texts (MGTs) is becoming increasingly crucial as\nLLMs become more advanced and prevalent. These models have the ability to\ngenerate human-like language, making it challenging to discern whether a text\nis authored by a human or a machine. This raises concerns regarding\nauthenticity, accountability, and potential bias. However, existing methods for\ndetecting MGTs are evaluated using different model architectures, datasets, and\nexperimental settings, resulting in a lack of a comprehensive evaluation\nframework that encompasses various methodologies. Furthermore, it remains\nunclear how existing detection methods would perform against powerful LLMs. In\nthis paper, we fill this gap by proposing the first benchmark framework for MGT\ndetection against powerful LLMs, named MGTBench. Extensive evaluations on\npublic datasets with curated texts generated by various powerful LLMs such as\nChatGPT-turbo and Claude demonstrate the effectiveness of different detection\nmethods. Our ablation study shows that a larger number of words in general\nleads to better performance and most detection methods can achieve similar\nperformance with much fewer training samples. Moreover, we delve into a more\nchallenging task: text attribution. Our findings indicate that the model-based\ndetection methods still perform well in the text attribution task. To\ninvestigate the robustness of different detection methods, we consider three\nadversarial attacks, namely paraphrasing, random spacing, and adversarial\nperturbations. We discover that these attacks can significantly diminish\ndetection effectiveness, underscoring the critical need for the development of\nmore robust detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2303.14822v3.pdf"
    },
    {
        "title": "WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation",
        "authors": [
            "Jongheon Jeong",
            "Yang Zou",
            "Taewan Kim",
            "Dongqing Zhang",
            "Avinash Ravichandran",
            "Onkar Dabeer"
        ],
        "published": "2023-03-26T20:41:21Z",
        "summary": "Visual anomaly classification and segmentation are vital for automating\nindustrial quality inspection. The focus of prior research in the field has\nbeen on training custom models for each quality inspection task, which requires\ntask-specific images and annotation. In this paper we move away from this\nregime, addressing zero-shot and few-normal-shot anomaly classification and\nsegmentation. Recently CLIP, a vision-language model, has shown revolutionary\ngenerality with competitive zero-/few-shot performance in comparison to\nfull-supervision. But CLIP falls short on anomaly classification and\nsegmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a\ncompositional ensemble on state words and prompt templates and (2) efficient\nextraction and aggregation of window/patch/image-level features aligned with\ntext. We also propose its few-normal-shot extension WinCLIP+, which uses\ncomplementary information from normal images. In MVTec-AD (and VisA), without\nfurther tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot\nanomaly classification and segmentation while WinCLIP+ does 93.1%/95.2%\n(83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.",
        "pdf_link": "https://arxiv.org/pdf/2303.14814v1.pdf"
    },
    {
        "title": "Task-oriented Memory-efficient Pruning-Adapter",
        "authors": [
            "Guorun Wang",
            "Jun Yang",
            "Yaoru Sun"
        ],
        "published": "2023-03-26T12:18:00Z",
        "summary": "The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.",
        "pdf_link": "https://arxiv.org/pdf/2303.14704v2.pdf"
    },
    {
        "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
        "authors": [
            "Yunfan Gao",
            "Tao Sheng",
            "Youlin Xiang",
            "Yun Xiong",
            "Haofen Wang",
            "Jiawei Zhang"
        ],
        "published": "2023-03-25T17:37:43Z",
        "summary": "Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.",
        "pdf_link": "https://arxiv.org/pdf/2303.14524v2.pdf"
    },
    {
        "title": "Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining",
        "authors": [
            "Zhouhong Gu",
            "Sihang Jiang",
            "Wenhao Huang",
            "Jiaqing Liang",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-03-25T10:19:14Z",
        "summary": "The model's ability to understand synonymous expression is crucial in many\nkinds of downstream tasks. It will make the model to better understand the\nsimilarity between context, and more robust to the synonym substitution attack.\nHowever, many Pretrained Language Model (PLM) lack synonym knowledge due to\nlimitation of small-scale synsets and PLM's pretraining objectives. In this\npaper, we propose a framework called Sem4SAP to mine synsets from Open\nKnowledge Graph (Open-KG) and using the mined synsets to do synonym-aware\npretraining for language models. We propose to coarsly filter the content in\nOpen-KG and use the frequency information to better help the clustering process\nunder low-resource unsupervised conditions. We expand the mined synsets by\nmigrating core semantics between synonymous expressions.We also propose two\nnovel and effective synonym-aware pre-training methods for injecting synonym\nknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can\ndramatically outperform the original PLMs and other baselines on ten different\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.14425v1.pdf"
    },
    {
        "title": "Freestyle Layout-to-Image Synthesis",
        "authors": [
            "Han Xue",
            "Zhiwu Huang",
            "Qianru Sun",
            "Li Song",
            "Wenjun Zhang"
        ],
        "published": "2023-03-25T09:37:41Z",
        "summary": "Typical layout-to-image synthesis (LIS) models generate images for a closed\nset of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work,\nwe explore the freestyle capability of the model, i.e., how far can it generate\nunseen semantics (e.g., classes, attributes, and styles) onto a given layout,\nand call the task Freestyle LIS (FLIS). Thanks to the development of\nlarge-scale pre-trained language-image models, a number of discriminative\nmodels (e.g., image classification and object detection) trained on limited\nbase classes are empowered with the ability of unseen class prediction.\nInspired by this, we opt to leverage large-scale pre-trained text-to-image\ndiffusion models to achieve the generation of unseen semantics. The key\nchallenge of FLIS is how to enable the diffusion model to synthesize images\nfrom a specific layout which very likely violates its pre-learned knowledge,\ne.g., the model never sees \"a unicorn sitting on a bench\" during its\npre-training. To this end, we introduce a new module called Rectified\nCross-Attention (RCA) that can be conveniently plugged in the diffusion model\nto integrate semantic masks. This \"plug-in\" is applied in each cross-attention\nlayer of the model to rectify the attention maps between image and text tokens.\nThe key idea of RCA is to enforce each text token to act on the pixels in a\nspecified region, allowing us to freely put a wide variety of semantics from\npre-trained knowledge (which is general) onto the given layout (which is\nspecific). Extensive experiments show that the proposed diffusion network\nproduces realistic and freestyle layout-to-image generation results with\ndiverse text inputs, which has a high potential to spawn a bunch of interesting\napplications. Code is available at https://github.com/essunny310/FreestyleNet.",
        "pdf_link": "https://arxiv.org/pdf/2303.14412v1.pdf"
    },
    {
        "title": "Backdoor Attacks with Input-unique Triggers in NLP",
        "authors": [
            "Xukun Zhou",
            "Jiwei Li",
            "Tianwei Zhang",
            "Lingjuan Lyu",
            "Muqiao Yang",
            "Jun He"
        ],
        "published": "2023-03-25T01:41:54Z",
        "summary": "Backdoor attack aims at inducing neural models to make incorrect predictions\nfor poison data while keeping predictions on the clean dataset unchanged, which\ncreates a considerable threat to current natural language processing (NLP)\nsystems. Existing backdoor attacking systems face two severe issues:firstly,\nmost backdoor triggers follow a uniform and usually input-independent pattern,\ne.g., insertion of specific trigger words, synonym replacement. This\nsignificantly hinders the stealthiness of the attacking model, leading the\ntrained backdoor model being easily identified as malicious by model probes.\nSecondly, trigger-inserted poisoned sentences are usually disfluent,\nungrammatical, or even change the semantic meaning from the original sentence,\nmaking them being easily filtered in the pre-processing stage. To resolve these\ntwo issues, in this paper, we propose an input-unique backdoor attack(NURA),\nwhere we generate backdoor triggers unique to inputs. IDBA generates\ncontext-related triggers by continuing writing the input with a language model\nlike GPT2. The generated sentence is used as the backdoor trigger. This\nstrategy not only creates input-unique backdoor triggers, but also preserves\nthe semantics of the original input, simultaneously resolving the two issues\nabove. Experimental results show that the IDBA attack is effective for attack\nand difficult to defend: it achieves high attack success rate across all the\nwidely applied benchmarks, while is immune to existing defending methods. In\naddition, it is able to generate fluent, grammatical, and diverse backdoor\ninputs, which can hardly be recognized through human inspection.",
        "pdf_link": "https://arxiv.org/pdf/2303.14325v1.pdf"
    },
    {
        "title": "TRAK: Attributing Model Behavior at Scale",
        "authors": [
            "Sung Min Park",
            "Kristian Georgiev",
            "Andrew Ilyas",
            "Guillaume Leclerc",
            "Aleksander Madry"
        ],
        "published": "2023-03-24T17:56:22Z",
        "summary": "The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .",
        "pdf_link": "https://arxiv.org/pdf/2303.14186v2.pdf"
    },
    {
        "title": "\"Get ready for a party\": Exploring smarter smart spaces with help from large language models",
        "authors": [
            "Evan King",
            "Haoxiang Yu",
            "Sangsu Lee",
            "Christine Julien"
        ],
        "published": "2023-03-24T16:51:08Z",
        "summary": "The right response to someone who says \"get ready for a party\" is deeply\ninfluenced by meaning and context. For a smart home assistant (e.g., Google\nHome), the ideal response might be to survey the available devices in the home\nand change their state to create a festive atmosphere. Current practical\nsystems cannot service such requests since they require the ability to (1)\ninfer meaning behind an abstract statement and (2) map that inference to a\nconcrete course of action appropriate for the context (e.g., changing the\nsettings of specific devices). In this paper, we leverage the observation that\nrecent task-agnostic large language models (LLMs) like GPT-3 embody a vast\namount of cross-domain, sometimes unpredictable contextual knowledge that\nexisting rule-based home assistant systems lack, which can make them powerful\ntools for inferring user intent and generating appropriate context-dependent\nresponses during smart home interactions. We first explore the feasibility of a\nsystem that places an LLM at the center of command inference and action\nplanning, showing that LLMs have the capacity to infer intent behind vague,\ncontext-dependent commands like \"get ready for a party\" and respond with\nconcrete, machine-parseable instructions that can be used to control smart\ndevices. We furthermore demonstrate a proof-of-concept implementation that puts\nan LLM in control of real devices, showing its ability to infer intent and\nchange device state appropriately with no fine-tuning or task-specific\ntraining. Our work hints at the promise of LLM-driven systems for\ncontext-awareness in smart environments, motivating future research in this\narea.",
        "pdf_link": "https://arxiv.org/pdf/2303.14143v1.pdf"
    },
    {
        "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
        "authors": [
            "Yunxiang Li",
            "Zihan Li",
            "Kai Zhang",
            "Ruilong Dan",
            "Steve Jiang",
            "You Zhang"
        ],
        "published": "2023-03-24T15:29:16Z",
        "summary": "The primary aim of this research was to address the limitations observed in\nthe medical knowledge of prevalent large language models (LLMs) such as\nChatGPT, by creating a specialized language model with enhanced accuracy in\nmedical advice. We achieved this by adapting and refining the large language\nmodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\nsourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In\naddition to the model refinement, we incorporated a self-directed information\nretrieval mechanism, allowing the model to access and utilize real-time\ninformation from online sources like Wikipedia and data from curated offline\nmedical databases. The fine-tuning of the model with real-world patient-doctor\ninteractions significantly improved the model's ability to understand patient\nneeds and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed\nsubstantial improvements in the accuracy of its responses. Our proposed\nChatDoctor, represents a significant advancement in medical LLMs, demonstrating\na significant improvement in understanding patient inquiries and providing\naccurate advice. Given the high stakes and low error tolerance in the medical\nfield, such enhancements in providing accurate and reliable information are not\nonly beneficial but essential.",
        "pdf_link": "https://arxiv.org/pdf/2303.14070v5.pdf"
    },
    {
        "title": "Paraphrase Detection: Human vs. Machine Content",
        "authors": [
            "Jonas Becker",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "published": "2023-03-24T13:25:46Z",
        "summary": "The growing prominence of large language models, such as GPT-4 and ChatGPT,\nhas led to increased concerns over academic integrity due to the potential for\nmachine-generated content and paraphrasing. Although studies have explored the\ndetection of human- and machine-paraphrased content, the comparison between\nthese types of content remains underexplored. In this paper, we conduct a\ncomprehensive analysis of various datasets commonly employed for paraphrase\ndetection tasks and evaluate an array of detection methods. Our findings\nhighlight the strengths and limitations of different detection methods in terms\nof performance on individual datasets, revealing a lack of suitable\nmachine-generated datasets that can be aligned with human expectations. Our\nmain finding is that human-authored paraphrases exceed machine-generated ones\nin terms of difficulty, diversity, and similarity implying that automatically\ngenerated texts are not yet on par with human-level performance. Transformers\nemerged as the most effective method across datasets with TF-IDF excelling on\nsemantically diverse corpora. Additionally, we identify four datasets as the\nmost diverse and challenging for paraphrase detection.",
        "pdf_link": "https://arxiv.org/pdf/2303.13989v1.pdf"
    },
    {
        "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods",
        "authors": [
            "Thilo Hagendorff"
        ],
        "published": "2023-03-24T13:24:41Z",
        "summary": "Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2303.13988v4.pdf"
    },
    {
        "title": "$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference",
        "authors": [
            "Benfeng Xu",
            "Quan Wang",
            "Zhendong Mao",
            "Yajuan Lyu",
            "Qiaoqiao She",
            "Yongdong Zhang"
        ],
        "published": "2023-03-24T06:16:29Z",
        "summary": "In-Context Learning (ICL), which formulates target tasks as prompt completion\nconditioned on in-context demonstrations, has become the prevailing utilization\nof LLMs. In this paper, we first disclose an actual predicament for this\ntypical usage that it can not scale up with training data due to context length\nrestriction. Besides, existing works have shown that ICL also suffers from\nvarious biases and requires delicate calibration treatment. To address both\nchallenges, we advocate a simple and effective solution, $k$NN Prompting, which\nfirst queries LLM with training data for distributed representations, then\npredicts test instances by simply referring to nearest neighbors. We conduct\ncomprehensive experiments to demonstrate its two-fold superiority: 1)\nCalibration-Free: $k$NN Prompting does not directly align LLM output\ndistribution with task-specific label space, instead leverages such\ndistribution to align test and training instances. It significantly outperforms\nstate-of-the-art calibration-based methods under comparable few-shot scenario.\n2) Beyond-Context: $k$NN Prompting can further scale up effectively with as\nmany training data as are available, continually bringing substantial\nimprovements. The scaling trend holds across 10 orders of magnitude ranging\nfrom 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B\nto 30B. It successfully bridges data scaling into model scaling, and brings new\npotentials for the gradient-free paradigm of LLM deployment. Code is publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2303.13824v1.pdf"
    },
    {
        "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
        "authors": [
            "Qingyu Lu",
            "Baopu Qiu",
            "Liang Ding",
            "Kanjian Zhang",
            "Tom Kocmi",
            "Dacheng Tao"
        ],
        "published": "2023-03-24T05:05:03Z",
        "summary": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks, such as machine translation,\ntext summarization. Recent research (Kocmi and Federmann, 2023) has shown that\nutilizing LLMs for assessing the quality of machine translation (MT) achieves\nstate-of-the-art performance at the system level but \\textit{performs poorly at\nthe segment level}. To further improve the performance of LLMs on MT quality\nassessment, we investigate several prompting designs, and propose a new\nprompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt)\nby combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et\nal., 2023). This technique emulates the commonly accepted human evaluation\nframework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and\n\\textit{produces explainable and reliable MT evaluations at both the system and\nsegment level}. Experimental Results from the WMT22 metrics shared task\nvalidate the effectiveness of EAPrompt on various LLMs, with different\nstructures. Further analysis confirms that EAPrompt effectively distinguishes\nmajor errors from minor ones, while also sharing a similar distribution of the\nnumber of errors with MQM. These findings highlight the potential of EAPrompt\nas a human-like evaluator prompting technique for MT evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2303.13809v3.pdf"
    },
    {
        "title": "Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function",
        "authors": [
            "A. B. Siddique",
            "M. H. Maqbool",
            "Kshitija Taywade",
            "Hassan Foroosh"
        ],
        "published": "2023-03-24T04:33:40Z",
        "summary": "Task-oriented dialog systems enable users to accomplish tasks using natural\nlanguage. State-of-the-art systems respond to users in the same way regardless\nof their personalities, although personalizing dialogues can lead to higher\nlevels of adoption and better user experiences. Building personalized dialog\nsystems is an important, yet challenging endeavor and only a handful of works\ntook on the challenge. Most existing works rely on supervised learning\napproaches and require laborious and expensive labeled training data for each\nuser profile. Additionally, collecting and labeling data for each user profile\nis virtually impossible. In this work, we propose a novel framework, P-ToD, to\npersonalize task-oriented dialog systems capable of adapting to a wide range of\nuser profiles in an unsupervised fashion using a zero-shot generalizable reward\nfunction. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three\nphases. Phase one performs task-specific training. Phase two kicks off\nunsupervised personalization by leveraging the proximal policy optimization\nalgorithm that performs policy gradients guided by the zero-shot generalizable\nreward function. Our novel reward function can quantify the quality of the\ngenerated responses even for unseen profiles. The optional final phase\nfine-tunes the personalized model using a few labeled training examples. We\nconduct extensive experimental analysis using the personalized bAbI dialogue\nbenchmark for five tasks and up to 180 diverse user profiles. The experimental\nresults demonstrate that P-ToD, even when it had access to zero labeled\nexamples, outperforms state-of-the-art supervised personalization models and\nachieves competitive performance on BLEU and ROUGE metrics when compared to a\nstrong fully-supervised GPT-2 baseline",
        "pdf_link": "https://arxiv.org/pdf/2303.13797v1.pdf"
    },
    {
        "title": "Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching",
        "authors": [
            "Jiayi Yuan",
            "Ruixiang Tang",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-24T03:14:00Z",
        "summary": "The process of matching patients with suitable clinical trials is essential\nfor advancing medical research and providing optimal care. However, current\napproaches face challenges such as data standardization, ethical\nconsiderations, and a lack of interoperability between Electronic Health\nRecords (EHRs) and clinical trial criteria. In this paper, we explore the\npotential of large language models (LLMs) to address these challenges by\nleveraging their advanced natural language generation capabilities to improve\ncompatibility between EHRs and clinical trial descriptions. We propose an\ninnovative privacy-aware data augmentation approach for LLM-based patient-trial\nmatching (LLM-PTM), which balances the benefits of LLMs while ensuring the\nsecurity and confidentiality of sensitive patient data. Our experiments\ndemonstrate a 7.32% average improvement in performance using the proposed\nLLM-PTM method, and the generalizability to new data is improved by 12.12%.\nAdditionally, we present case studies to further illustrate the effectiveness\nof our approach and provide a deeper understanding of its underlying\nprinciples.",
        "pdf_link": "https://arxiv.org/pdf/2303.16756v2.pdf"
    },
    {
        "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
        "authors": [
            "Zheng-Xin Yong",
            "Ruochen Zhang",
            "Jessica Zosa Forde",
            "Skyler Wang",
            "Arjun Subramonian",
            "Holy Lovenia",
            "Samuel Cahyawijaya",
            "Genta Indra Winata",
            "Lintang Sutawika",
            "Jan Christian Blaise Cruz",
            "Yin Lin Tan",
            "Long Phan",
            "Rowena Garcia",
            "Thamar Solorio",
            "Alham Fikri Aji"
        ],
        "published": "2023-03-23T18:16:30Z",
        "summary": "While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The recent proliferation of Large\nLanguage Models (LLMs) compels one to ask: how capable are these systems in\ngenerating code-mixed data? In this paper, we explore prompting multilingual\nLLMs in a zero-shot manner to generate code-mixed data for seven languages in\nSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,\nTamil, and Singlish. We find that publicly available multilingual\ninstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of\nproducing texts with phrases or clauses from different languages. ChatGPT\nexhibits inconsistent capabilities in generating code-mixed texts, wherein its\nperformance varies depending on the prompt template and language pairing. For\ninstance, ChatGPT generates fluent and natural Singlish texts (an English-based\ncreole spoken in Singapore), but for English-Tamil language pair, the system\nmostly produces grammatically incorrect or semantically meaningless utterances.\nFurthermore, it may erroneously introduce languages not specified in the\nprompt. Based on our investigation, existing multilingual LLMs exhibit a wide\nrange of proficiency in code-mixed data generation for SEA languages. As such,\nwe advise against using LLMs in this context without extensive human checks.",
        "pdf_link": "https://arxiv.org/pdf/2303.13592v4.pdf"
    },
    {
        "title": "The Quantization Model of Neural Scaling",
        "authors": [
            "Eric J. Michaud",
            "Ziming Liu",
            "Uzay Girit",
            "Max Tegmark"
        ],
        "published": "2023-03-23T17:58:43Z",
        "summary": "We propose the Quantization Model of neural scaling laws, explaining both the\nobserved power law dropoff of loss with model and data size, and also the\nsudden emergence of new capabilities with scale. We derive this model from what\nwe call the Quantization Hypothesis, where network knowledge and skills are\n\"quantized\" into discrete chunks ($\\textbf{quanta}$). We show that when quanta\nare learned in order of decreasing use frequency, then a power law in use\nfrequencies explains observed power law scaling of loss. We validate this\nprediction on toy datasets, then study how scaling curves decompose for large\nlanguage models. Using language model gradients, we automatically decompose\nmodel behavior into a diverse set of skills (quanta). We tentatively find that\nthe frequency at which these quanta are used in the training distribution\nroughly follows a power law corresponding with the empirical scaling exponent\nfor language models, a prediction of our theory.",
        "pdf_link": "https://arxiv.org/pdf/2303.13506v3.pdf"
    },
    {
        "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
        "authors": [
            "Kalpesh Krishna",
            "Yixiao Song",
            "Marzena Karpinska",
            "John Wieting",
            "Mohit Iyyer"
        ],
        "published": "2023-03-23T16:29:27Z",
        "summary": "The rise in malicious usage of large language models, such as fake content\ncreation and academic plagiarism, has motivated the development of approaches\nthat identify AI-generated text, including those based on watermarking or\noutlier detection. However, the robustness of these detection algorithms to\nparaphrases of AI-generated text remains unclear. To stress test these\ndetectors, we build a 11B parameter paraphrase generation model (DIPPER) that\ncan paraphrase paragraphs, condition on surrounding context, and control\nlexical diversity and content reordering. Using DIPPER to paraphrase text\ngenerated by three large language models (including GPT3.5-davinci-003)\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection\naccuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of\n1%), without appreciably modifying the input semantics.\n  To increase the robustness of AI-generated text detection to paraphrase\nattacks, we introduce a simple defense that relies on retrieving\nsemantically-similar generations and must be maintained by a language model API\nprovider. Given a candidate text, our algorithm searches a database of\nsequences previously generated by the API, looking for sequences that match the\ncandidate text within a certain threshold. We empirically verify our defense\nusing a database of 15M generations from a fine-tuned T5-XXL model and find\nthat it can detect 80% to 97% of paraphrased generations across different\nsettings while only classifying 1% of human-written sequences as AI-generated.\nWe open-source our models, code and data.",
        "pdf_link": "https://arxiv.org/pdf/2303.13408v2.pdf"
    },
    {
        "title": "ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model",
        "authors": [
            "Hanyao Huang",
            "Ou Zheng",
            "Dongdong Wang",
            "Jiayi Yin",
            "Zijin Wang",
            "Shengxuan Ding",
            "Heng Yin",
            "Chuan Xu",
            "Renjie Yang",
            "Qian Zheng",
            "Bing Shi"
        ],
        "published": "2023-03-23T15:34:26Z",
        "summary": "The ChatGPT, a lite and conversational variant of Generative Pretrained\nTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large\nLanguage Models (LLMs) with billions of parameters. LLMs have stirred up much\ninterest among researchers and practitioners in their impressive skills in\nnatural language processing tasks, which profoundly impact various fields. This\npaper mainly discusses the future applications of LLMs in dentistry. We\nintroduce two primary LLM deployment methods in dentistry, including automated\ndental diagnosis and cross-modal dental diagnosis, and examine their potential\napplications. Especially, equipped with a cross-modal encoder, a single LLM can\nmanage multi-source data and conduct advanced natural language reasoning to\nperform complex clinical operations. We also present cases to demonstrate the\npotential of a fully automatic Multi-Modal LLM AI system for dentistry clinical\napplication. While LLMs offer significant potential benefits, the challenges,\nsuch as data privacy, data quality, and model bias, need further study.\nOverall, LLMs have the potential to revolutionize dental diagnosis and\ntreatment, which indicates a promising avenue for clinical application and\nresearch in dentistry.",
        "pdf_link": "https://arxiv.org/pdf/2304.03086v2.pdf"
    },
    {
        "title": "Increasing Textual Context Size Boosts Medical Image-Text Matching",
        "authors": [
            "Idan Glassberg",
            "Tom Hope"
        ],
        "published": "2023-03-23T15:20:05Z",
        "summary": "This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2303.13340v1.pdf"
    },
    {
        "title": "Fairness-guided Few-shot Prompting for Large Language Models",
        "authors": [
            "Huan Ma",
            "Changqing Zhang",
            "Yatao Bian",
            "Lemao Liu",
            "Zhirui Zhang",
            "Peilin Zhao",
            "Shu Zhang",
            "Huazhu Fu",
            "Qinghua Hu",
            "Bingzhe Wu"
        ],
        "published": "2023-03-23T12:28:25Z",
        "summary": "Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.",
        "pdf_link": "https://arxiv.org/pdf/2303.13217v3.pdf"
    },
    {
        "title": "A Simple Explanation for the Phase Transition in Large Language Models with List Decoding",
        "authors": [
            "Cheng-Shang Chang"
        ],
        "published": "2023-03-23T09:00:07Z",
        "summary": "Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.",
        "pdf_link": "https://arxiv.org/pdf/2303.13112v1.pdf"
    },
    {
        "title": "SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization",
        "authors": [
            "Yu-Neng Chuang",
            "Ruixiang Tang",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-23T04:47:46Z",
        "summary": "Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.",
        "pdf_link": "https://arxiv.org/pdf/2303.13035v3.pdf"
    },
    {
        "title": "A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification",
        "authors": [
            "Thanh-Dung Le",
            "Philippe Jouvet",
            "Rita Noumeir"
        ],
        "published": "2023-03-22T20:10:29Z",
        "summary": "In recent years, Transformer-based models such as the Switch Transformer have\nachieved remarkable results in natural language processing tasks. However,\nthese models are often too complex and require extensive pre-training, which\nlimits their effectiveness for small clinical text classification tasks with\nlimited data. In this study, we propose a simplified Switch Transformer\nframework and train it from scratch on a small French clinical text\nclassification dataset at CHU Sainte-Justine hospital. Our results demonstrate\nthat the simplified small-scale Transformer models outperform pre-trained\nBERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.\nAdditionally, using a mixture of expert mechanisms from the Switch Transformer\nhelps capture diverse patterns; hence, the proposed approach achieves better\nresults than a conventional Transformer with the self-attention mechanism.\nFinally, our proposed framework achieves an accuracy of 87\\%, precision at\n87\\%, and recall at 85\\%, compared to the third-best pre-trained BERT-based\nmodel, FlauBERT, which achieved an accuracy of 84\\%, precision at 84\\%, and\nrecall at 84\\%. However, Switch Transformers have limitations, including a\ngeneralization gap and sharp minima. We compare it with a multi-layer\nperceptron neural network for small French clinical narratives classification\nand show that the latter outperforms all other models.",
        "pdf_link": "https://arxiv.org/pdf/2303.12892v1.pdf"
    },
    {
        "title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
        "authors": [
            "Ramisa Anan",
            "Tasnim Sakib Apon",
            "Zeba Tahsin Hossain",
            "Elizabeth Antora Modhu",
            "Sudipta Mondal",
            "MD. Golam Rabiul Alam"
        ],
        "published": "2023-03-22T17:35:35Z",
        "summary": "A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.",
        "pdf_link": "https://arxiv.org/pdf/2303.12772v1.pdf"
    },
    {
        "title": "Can we trust the evaluation on ChatGPT?",
        "authors": [
            "Rachith Aiyappa",
            "Jisun An",
            "Haewoon Kwak",
            "Yong-Yeol Ahn"
        ],
        "published": "2023-03-22T17:32:56Z",
        "summary": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.",
        "pdf_link": "https://arxiv.org/pdf/2303.12767v1.pdf"
    },
    {
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "authors": [
            "S\u00e9bastien Bubeck",
            "Varun Chandrasekaran",
            "Ronen Eldan",
            "Johannes Gehrke",
            "Eric Horvitz",
            "Ece Kamar",
            "Peter Lee",
            "Yin Tat Lee",
            "Yuanzhi Li",
            "Scott Lundberg",
            "Harsha Nori",
            "Hamid Palangi",
            "Marco Tulio Ribeiro",
            "Yi Zhang"
        ],
        "published": "2023-03-22T16:51:28Z",
        "summary": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.",
        "pdf_link": "https://arxiv.org/pdf/2303.12712v5.pdf"
    },
    {
        "title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study",
        "authors": [
            "Sonish Sivarajkumar",
            "Fengyi Gao",
            "Parker E. Denny",
            "Bayan M. Aldhahwani",
            "Shyam Visweswaran",
            "Allyn Bove",
            "Yanshan Wang"
        ],
        "published": "2023-03-22T13:46:16Z",
        "summary": "Post-stroke patient rehabilitation requires precise, personalized treatment\nplans. Natural Language Processing (NLP) offers potential to extract valuable\nexercise information from clinical notes, aiding in the development of more\neffective rehabilitation strategies. Objective: This study aims to develop and\nevaluate a variety of NLP algorithms to extract and categorize physical\nrehabilitation exercise information from the clinical notes of post-stroke\npatients treated at the University of Pittsburgh Medical Center. A cohort of\n13,605 patients diagnosed with stroke was identified, and their clinical notes\ncontaining rehabilitation therapy notes were retrieved. A comprehensive\nclinical ontology was created to represent various aspects of physical\nrehabilitation exercises. State-of-the-art NLP algorithms were then developed\nand compared, including rule-based, machine learning-based algorithms, and\nlarge language model (LLM)-based algorithms (ChatGPT). Analysis was conducted\non a dataset comprising 23,724 notes with detailed demographic and clinical\ncharacteristics. The rule-based NLP algorithm demonstrated superior performance\nin most areas, particularly in detecting the 'Right Side' location with an F1\nscore of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting\nexcelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing\nrule-based NLP by 0.023. It also showed notable performance in 'Passive Range\nof Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.\nThe rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'\nwith F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot\nprompts, achieved high recall but generally lower precision and F1 scores.\nHowever, it notably excelled in 'Backward Plane' motion detection, achieving an\nF1 score of 0.846, surpassing the rule-based algorithm's 0.720.",
        "pdf_link": "https://arxiv.org/pdf/2303.13466v2.pdf"
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "authors": [
            "Kabir Ahuja",
            "Harshita Diddee",
            "Rishav Hada",
            "Millicent Ochieng",
            "Krithika Ramesh",
            "Prachi Jain",
            "Akshay Nambi",
            "Tanuja Ganu",
            "Sameer Segal",
            "Maxamed Axmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023-03-22T13:03:10Z",
        "summary": "Generative AI models have shown impressive performance on many Natural\nLanguage Processing tasks such as language understanding, reasoning, and\nlanguage generation. An important question being asked by the AI community\ntoday is about the capabilities and limits of these models, and it is clear\nthat evaluating generative AI is very challenging. Most studies on generative\nLLMs have been restricted to English and it is unclear how capable these models\nare at understanding and generating text in other languages. We present the\nfirst comprehensive benchmarking of generative LLMs - MEGA, which evaluates\nmodels on standard NLP benchmarks, covering 16 NLP datasets across 70\ntypologically diverse languages. We compare the performance of generative LLMs\nincluding Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive\nmodels on these tasks to determine how well generative models perform compared\nto the previous generation of LLMs. We present a thorough analysis of the\nperformance of models across languages and tasks and discuss challenges in\nimproving the performance of generative LLMs on low-resource languages. We\ncreate a framework for evaluating generative LLMs in the multilingual setting\nand provide directions for future progress in the field.",
        "pdf_link": "https://arxiv.org/pdf/2303.12528v4.pdf"
    },
    {
        "title": "MEDIMP: 3D Medical Images with clinical Prompts from limited tabular data for renal transplantation",
        "authors": [
            "Leo Milecki",
            "Vicky Kalogeiton",
            "Sylvain Bodard",
            "Dany Anglicheau",
            "Jean-Michel Correas",
            "Marc-Olivier Timsit",
            "Maria Vakalopoulou"
        ],
        "published": "2023-03-22T10:30:43Z",
        "summary": "Renal transplantation emerges as the most effective solution for end-stage\nrenal disease. Occurring from complex causes, a substantial risk of transplant\nchronic dysfunction persists and may lead to graft loss. Medical imaging plays\na substantial role in renal transplant monitoring in clinical practice.\nHowever, graft supervision is multi-disciplinary, notably joining nephrology,\nurology, and radiology, while identifying robust biomarkers from such\nhigh-dimensional and complex data for prognosis is challenging. In this work,\ntaking inspiration from the recent success of Large Language Models (LLMs), we\npropose MEDIMP -- Medical Images with clinical Prompts -- a model to learn\nmeaningful multi-modal representations of renal transplant Dynamic\nContrast-Enhanced Magnetic Resonance Imaging (DCE MRI) by incorporating\nstructural clinicobiological data after translating them into text prompts.\nMEDIMP is based on contrastive learning from joint text-image paired embeddings\nto perform this challenging task. Moreover, we propose a framework that\ngenerates medical prompts using automatic textual data augmentations from LLMs.\nOur goal is to learn meaningful manifolds of renal transplant DCE MRI,\ninteresting for the prognosis of the transplant or patient status (2, 3, and 4\nyears after the transplant), fully exploiting the limited available multi-modal\ndata most efficiently. Extensive experiments and comparisons with other renal\ntransplant representation learning methods with limited data prove the\neffectiveness of MEDIMP in a relevant clinical setting, giving new directions\ntoward medical prompts. Our code is available at\nhttps://github.com/leomlck/MEDIMP.",
        "pdf_link": "https://arxiv.org/pdf/2303.12445v2.pdf"
    },
    {
        "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense",
        "authors": [
            "Andrei Kucharavy",
            "Zachary Schillaci",
            "Lo\u00efc Mar\u00e9chal",
            "Maxime W\u00fcrsch",
            "Ljiljana Dolamic",
            "Remi Sabonnadiere",
            "Dimitri Percia David",
            "Alain Mermoud",
            "Vincent Lenders"
        ],
        "published": "2023-03-21T18:45:09Z",
        "summary": "Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.",
        "pdf_link": "https://arxiv.org/pdf/2303.12132v1.pdf"
    },
    {
        "title": "Large Language Models Can Be Used to Estimate the Latent Positions of Politicians",
        "authors": [
            "Patrick Y. Wu",
            "Jonathan Nagler",
            "Joshua A. Tucker",
            "Solomon Messing"
        ],
        "published": "2023-03-21T17:48:00Z",
        "summary": "Existing approaches to estimating politicians' latent positions along\nspecific dimensions often fail when relevant data is limited. We leverage the\nembedded knowledge in generative large language models (LLMs) to address this\nchallenge and measure lawmakers' positions along specific political or policy\ndimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare\nlawmakers and then scale the resulting graph using the Bradley-Terry model. We\nestimate novel measures of U.S. senators' positions on liberal-conservative\nideology, gun control, and abortion. Our liberal-conservative scale, used to\nvalidate LLM-driven scaling, strongly correlates with existing measures and\noffsets interpretive gaps, suggesting LLMs synthesize relevant data from\ninternet and digitized media rather than memorizing existing measures. Our gun\ncontrol and abortion measures -- the first of their kind -- differ from the\nliberal-conservative scale in face-valid ways and predict interest group\nratings and legislator votes better than ideology alone. Our findings suggest\nLLMs hold promise for solving complex social science measurement problems.",
        "pdf_link": "https://arxiv.org/pdf/2303.12057v4.pdf"
    },
    {
        "title": "cTBLS: Augmenting Large Language Models with Conversational Tables",
        "authors": [
            "Anirudh S Sundar",
            "Larry Heck"
        ],
        "published": "2023-03-21T17:04:44Z",
        "summary": "Optimizing accuracy and performance while eliminating hallucinations of\nopen-domain conversational large language models (LLMs) is an open research\nchallenge. A particularly promising direction is to augment and ground LLMs\nwith information from structured sources. This paper introduces Conversational\nTables (cTBLS), a three-step architecture to retrieve and generate dialogue\nresponses grounded on retrieved tabular information. cTBLS uses Transformer\nencoder embeddings for Dense Table Retrieval and obtains up to 125% relative\nimprovement over the retriever in the previous state-of-the-art system on the\nHyrbiDialogue dataset. cTBLS then uses a shared process between encoder and\ndecoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking\ncombined with a GPT-3.5 LLM response generator to yield a 2x relative\nimprovement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the\ntime (coherency, fluency) and judge informativeness to be 4x better than the\nprevious state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2303.12024v3.pdf"
    },
    {
        "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
        "authors": [
            "Zonglin Yang",
            "Xinya Du",
            "Rui Mao",
            "Jinjie Ni",
            "Erik Cambria"
        ],
        "published": "2023-03-21T16:56:05Z",
        "summary": "Logical reasoning is central to human cognition and intelligence. It includes\ndeductive, inductive, and abductive reasoning. Past research of logical\nreasoning within AI uses formal language as knowledge representation and\nsymbolic reasoners. However, reasoning with formal language has proved\nchallenging (e.g., brittleness and knowledge-acquisition bottleneck). This\npaper provides a comprehensive overview on a new paradigm of logical reasoning,\nwhich uses natural language as knowledge representation and pretrained language\nmodels as reasoners, including philosophical definition and categorization of\nlogical reasoning, advantages of the new paradigm, benchmarks and methods,\nchallenges of the new paradigm, possible future directions, and relation to\nrelated NLP fields. This new paradigm is promising since it not only alleviates\nmany challenges of formal representation but also has advantages over\nend-to-end neural methods. This survey focus on transformer-based LLMs\nexplicitly working on deductive, inductive, and abductive reasoning over\nEnglish representation.",
        "pdf_link": "https://arxiv.org/pdf/2303.12023v2.pdf"
    },
    {
        "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
        "authors": [
            "Yushi Hu",
            "Benlin Liu",
            "Jungo Kasai",
            "Yizhong Wang",
            "Mari Ostendorf",
            "Ranjay Krishna",
            "Noah A Smith"
        ],
        "published": "2023-03-21T14:41:02Z",
        "summary": "Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.",
        "pdf_link": "https://arxiv.org/pdf/2303.11897v3.pdf"
    },
    {
        "title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing",
        "authors": [
            "Brady Lund",
            "Ting Wang",
            "Nishith Reddy Mannuru",
            "Bing Nie",
            "Somipam Shimray",
            "Ziang Wang"
        ],
        "published": "2023-03-21T14:35:07Z",
        "summary": "This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.",
        "pdf_link": "https://arxiv.org/pdf/2303.13367v2.pdf"
    },
    {
        "title": "ChatGPT for Programming Numerical Methods",
        "authors": [
            "Ali Kashefi",
            "Tapan Mukerji"
        ],
        "published": "2023-03-21T12:18:17Z",
        "summary": "ChatGPT is a large language model recently released by the OpenAI company. In\nthis technical report, we explore for the first time the capability of ChatGPT\nfor programming numerical algorithms. Specifically, we examine the capability\nof GhatGPT for generating codes for numerical algorithms in different\nprogramming languages, for debugging and improving written codes by users, for\ncompleting missed parts of numerical codes, rewriting available codes in other\nprogramming languages, and for parallelizing serial codes. Additionally, we\nassess if ChatGPT can recognize if given codes are written by humans or\nmachines. To reach this goal, we consider a variety of mathematical problems\nsuch as the Poisson equation, the diffusion equation, the incompressible\nNavier-Stokes equations, compressible inviscid flow, eigenvalue problems,\nsolving linear systems of equations, storing sparse matrices, etc. Furthermore,\nwe exemplify scientific machine learning such as physics-informed neural\nnetworks and convolutional neural networks with applications to computational\nphysics. Through these examples, we investigate the successes, failures, and\nchallenges of ChatGPT. Examples of failures are producing singular matrices,\noperations on arrays with incompatible sizes, programming interruption for\nrelatively long codes, etc. Our outcomes suggest that ChatGPT can successfully\nprogram numerical algorithms in different programming languages, but certain\nlimitations and challenges exist that require further improvement of this\nmachine learning model.",
        "pdf_link": "https://arxiv.org/pdf/2303.12093v3.pdf"
    },
    {
        "title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement",
        "authors": [
            "Shuzhou Yang",
            "Moxuan Ding",
            "Yanmin Wu",
            "Zihan Li",
            "Jian Zhang"
        ],
        "published": "2023-03-21T10:24:29Z",
        "summary": "The following three factors restrict the application of existing low-light\nimage enhancement methods: unpredictable brightness degradation and noise,\ninherent gap between metric-favorable and visual-friendly versions, and the\nlimited paired training data. To address these limitations, we propose an\nimplicit Neural Representation method for Cooperative low-light image\nenhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in\nan unsupervised manner. Concretely, NeRCo unifies the diverse degradation\nfactors of real-world scenes with a controllable fitting function, leading to\nbetter robustness. In addition, for the output results, we introduce\nsemantic-orientated supervision with priors from the pre-trained\nvision-language model. Instead of merely following reference images, it\nencourages results to meet subjective expectations, finding more\nvisual-friendly solutions. Further, to ease the reliance on paired data and\nreduce solution space, we develop a dual-closed-loop constrained enhancement\nmodule. It is trained cooperatively with other affiliated modules in a\nself-supervised manner. Finally, extensive experiments demonstrate the\nrobustness and superior effectiveness of our proposed NeRCo. Our code is\navailable at https://github.com/Ysz2022/NeRCo.",
        "pdf_link": "https://arxiv.org/pdf/2303.11722v3.pdf"
    },
    {
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
        "authors": [
            "Chaoning Zhang",
            "Chenshuang Zhang",
            "Sheng Zheng",
            "Yu Qiao",
            "Chenghao Li",
            "Mengchun Zhang",
            "Sumit Kumar Dam",
            "Chu Myaet Thwal",
            "Ye Lin Tun",
            "Le Luang Huy",
            "Donguk kim",
            "Sung-Ho Bae",
            "Lik-Hang Lee",
            "Yang Yang",
            "Heng Tao Shen",
            "In So Kweon",
            "Choong Seon Hong"
        ],
        "published": "2023-03-21T10:09:47Z",
        "summary": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.",
        "pdf_link": "https://arxiv.org/pdf/2303.11717v1.pdf"
    },
    {
        "title": "The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue",
        "authors": [
            "Gabriel Skantze",
            "A. Seza Do\u011fru\u00f6z"
        ],
        "published": "2023-03-21T10:01:49Z",
        "summary": "There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this position paper, we\nexplain this paradox through the theory of common ground as the basis for\nhuman-like communication. Furthermore, we question the assumptions behind\nopen-domain chatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.",
        "pdf_link": "https://arxiv.org/pdf/2303.11708v2.pdf"
    },
    {
        "title": "Language Model Behavior: A Comprehensive Survey",
        "authors": [
            "Tyler A. Chang",
            "Benjamin K. Bergen"
        ],
        "published": "2023-03-20T23:54:26Z",
        "summary": "Transformer language models have received widespread public attention, yet\ntheir generated text is often surprising even to NLP researchers. In this\nsurvey, we discuss over 250 recent studies of English language model behavior\nbefore task-specific fine-tuning. Language models possess basic capabilities in\nsyntax, semantics, pragmatics, world knowledge, and reasoning, but these\ncapabilities are sensitive to specific inputs and surface features. Despite\ndramatic increases in generated text quality as models scale to hundreds of\nbillions of parameters, the models are still prone to unfactual responses,\ncommonsense errors, memorized text, and social biases. Many of these weaknesses\ncan be framed as over-generalizations or under-generalizations of learned\npatterns in text. We synthesize recent results to highlight what is currently\nknown about large language model capabilities, thus providing a resource for\napplied work and for research in adjacent fields that use language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.11504v2.pdf"
    },
    {
        "title": "Large Language Models and Simple, Stupid Bugs",
        "authors": [
            "Kevin Jesse",
            "Toufique Ahmed",
            "Premkumar T. Devanbu",
            "Emily Morgan"
        ],
        "published": "2023-03-20T21:14:06Z",
        "summary": "With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.",
        "pdf_link": "https://arxiv.org/pdf/2303.11455v1.pdf"
    },
    {
        "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
        "authors": [
            "Noah Shinn",
            "Federico Cassano",
            "Edward Berman",
            "Ashwin Gopinath",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "published": "2023-03-20T18:08:50Z",
        "summary": "Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.11366v4.pdf"
    },
    {
        "title": "Context-faithful Prompting for Large Language Models",
        "authors": [
            "Wenxuan Zhou",
            "Sheng Zhang",
            "Hoifung Poon",
            "Muhao Chen"
        ],
        "published": "2023-03-20T17:54:58Z",
        "summary": "Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts. Code and data are released\nat https://github.com/wzhouad/context-faithful-llm.",
        "pdf_link": "https://arxiv.org/pdf/2303.11315v2.pdf"
    },
    {
        "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4",
        "authors": [
            "Zhengliang Liu",
            "Yue Huang",
            "Xiaowei Yu",
            "Lu Zhang",
            "Zihao Wu",
            "Chao Cao",
            "Haixing Dai",
            "Lin Zhao",
            "Yiwei Li",
            "Peng Shu",
            "Fang Zeng",
            "Lichao Sun",
            "Wei Liu",
            "Dinggang Shen",
            "Quanzheng Li",
            "Tianming Liu",
            "Dajiang Zhu",
            "Xiang Li"
        ],
        "published": "2023-03-20T11:34:37Z",
        "summary": "The digitization of healthcare has facilitated the sharing and re-using of\nmedical data but has also raised concerns about confidentiality and privacy.\nHIPAA (Health Insurance Portability and Accountability Act) mandates removing\nre-identifying information before the dissemination of medical records. Thus,\neffective and efficient solutions for de-identifying medical data, especially\nthose in free-text forms, are highly needed. While various computer-assisted\nde-identification methods, including both rule-based and learning-based, have\nbeen developed and used in prior practice, such solutions still lack\ngeneralizability or need to be fine-tuned according to different scenarios,\nsignificantly imposing restrictions in wider use. The advancement of large\nlanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential in\nprocessing text data in the medical domain with zero-shot in-context learning,\nespecially in the task of privacy protection, as these models can identify\nconfidential information by their powerful named entity recognition (NER)\ncapability. In this work, we developed a novel GPT4-enabled de-identification\nframework (``DeID-GPT\") to automatically identify and remove the identifying\ninformation. Compared to existing commonly used medical text data\nde-identification methods, our developed DeID-GPT showed the highest accuracy\nand remarkable reliability in masking private information from the unstructured\nmedical text while preserving the original structure and meaning of the text.\nThis study is one of the earliest to utilize ChatGPT and GPT-4 for medical text\ndata processing and de-identification, which provides insights for further\nresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 in\nhealthcare. Codes and benchmarking data information are available at\nhttps://github.com/yhydhx/ChatGPT-API.",
        "pdf_link": "https://arxiv.org/pdf/2303.11032v2.pdf"
    },
    {
        "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
        "authors": [
            "Ruochen Zhao",
            "Hailin Chen",
            "Weishi Wang",
            "Fangkai Jiao",
            "Xuan Long Do",
            "Chengwei Qin",
            "Bosheng Ding",
            "Xiaobao Guo",
            "Minzhi Li",
            "Xingxuan Li",
            "Shafiq Joty"
        ],
        "published": "2023-03-20T05:07:41Z",
        "summary": "As Large Language Models (LLMs) become popular, there emerged an important\ntrend of using multimodality to augment the LLMs' generation ability, which\nenables LLMs to better interact with the world. However, there lacks a unified\nperception of at which stage and how to incorporate different modalities. In\nthis survey, we review methods that assist and augment generative models by\nretrieving multimodal knowledge, whose formats range from images, codes,\ntables, graphs, to audio. Such methods offer a promising solution to important\nconcerns such as factuality, reasoning, interpretability, and robustness. By\nproviding an in-depth review, this survey is expected to provide scholars with\na deeper understanding of the methods' applications and encourage them to adapt\nexisting techniques to the fast-growing field of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.10868v3.pdf"
    },
    {
        "title": "Dynamic Documentation for AI Systems",
        "authors": [
            "Soham Mehta",
            "Anderson Rogers",
            "Thomas Krendl Gilbert"
        ],
        "published": "2023-03-20T04:23:07Z",
        "summary": "AI documentation is a rapidly-growing channel for coordinating the design of\nAI technologies with policies for transparency and accessibility. Calls to\nstandardize and enact documentation of algorithmic harms and impacts are now\ncommonplace. However, documentation standards for AI remain inchoate, and fail\nto match the capabilities and social effects of increasingly impactful\narchitectures such as Large Language Models (LLMs). In this paper, we show the\nlimits of present documentation protocols, and argue for dynamic documentation\nas a new paradigm for understanding and evaluating AI systems. We first review\ncanonical approaches to system documentation outside the context of AI,\nfocusing on the complex history of Environmental Impact Statements (EISs). We\nnext compare critical elements of the EIS framework to present challenges with\nalgorithmic documentation, which have inherited the limitations of EISs without\nincorporating their strengths. These challenges are specifically illustrated\nthrough the growing popularity of Model Cards and two case studies of\nalgorithmic impact assessment in China and Canada. Finally, we evaluate more\nrecent proposals, including Reward Reports, as potential components of fully\ndynamic AI documentation protocols.",
        "pdf_link": "https://arxiv.org/pdf/2303.10854v1.pdf"
    },
    {
        "title": "The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-03-20T01:57:52Z",
        "summary": "The AI community has embraced multi-sensory or multi-modal approaches to\nadvance this generation of AI models to resemble expected intelligent\nunderstanding. Combining language and imagery represents a familiar method for\nspecific tasks like image captioning or generation from descriptions. This\npaper compares these monolithic approaches to a lightweight and specialized\nmethod based on employing image models to label objects, then serially\nsubmitting this resulting object list to a large language model (LLM). This use\nof multiple Application Programming Interfaces (APIs) enables better than 95%\nmean average precision for correct object lists, which serve as input to the\nlatest Open AI text generator (GPT-4). To demonstrate the API as a modular\nalternative, we solve the problem of a user taking a picture of ingredients\navailable in a refrigerator, then generating novel recipe cards tailored to\ncomplex constraints on cost, preparation time, dietary restrictions, portion\nsizes, and multiple meal plans. The research concludes that monolithic\nmultimodal models currently lack the coherent memory to maintain context and\nformat for this task and that until recently, the language models like GPT-2/3\nstruggled to format similar problems without degenerating into repetitive or\nnon-sensical combinations of ingredients. For the first time, an AI chef or\ncook seems not only possible but offers some enhanced capabilities to augment\nhuman recipe libraries in pragmatic ways. The work generates a 100-page recipe\nbook featuring the thirty top ingredients using over 2000 refrigerator images\nas initializing lists.",
        "pdf_link": "https://arxiv.org/pdf/2304.02016v1.pdf"
    },
    {
        "title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
        "authors": [
            "H. A. Z. Sameen Shahgir",
            "Khondker Salman Sayeed"
        ],
        "published": "2023-03-19T09:24:48Z",
        "summary": "This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.",
        "pdf_link": "https://arxiv.org/pdf/2303.10612v1.pdf"
    },
    {
        "title": "Revisiting the Plastic Surgery Hypothesis via Large Language Models",
        "authors": [
            "Chunqiu Steven Xia",
            "Yifeng Ding",
            "Lingming Zhang"
        ],
        "published": "2023-03-18T20:33:46Z",
        "summary": "Automated Program Repair (APR) aspires to automatically generate patches for\nan input buggy program. Traditional APR tools typically focus on specific bug\ntypes and fixes through the use of templates, heuristics, and formal\nspecifications. However, these techniques are limited in terms of the bug types\nand patch variety they can produce. As such, researchers have designed various\nlearning-based APR tools with recent work focused on directly using Large\nLanguage Models (LLMs) for APR. While LLM-based APR tools are able to achieve\nstate-of-the-art performance on many repair datasets, the LLMs used for direct\nrepair are not fully aware of the project-specific information such as unique\nvariable or method names.\n  The plastic surgery hypothesis is a well-known insight for APR, which states\nthat the code ingredients to fix the bug usually already exist within the same\nproject. Traditional APR tools have largely leveraged the plastic surgery\nhypothesis by designing manual or heuristic-based approaches to exploit such\nexisting code ingredients. However, as recent APR research starts focusing on\nLLM-based approaches, the plastic surgery hypothesis has been largely ignored.\nIn this paper, we ask the following question: How useful is the plastic surgery\nhypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique\nopportunity to fully automate the plastic surgery hypothesis via fine-tuning\nand prompting. To this end, we propose FitRepair, which combines the direct\nusage of LLMs with two domain-specific fine-tuning strategies and one prompting\nstrategy for more powerful APR. Our experiments on the widely studied Defects4j\n1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially\noutperforming the best-performing baseline by 15 and 8), respectively,\ndemonstrating a promising future of the plastic surgery hypothesis in the era\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.10494v1.pdf"
    },
    {
        "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models",
        "authors": [
            "Vithursan Thangarasa",
            "Abhay Gupta",
            "William Marshall",
            "Tianda Li",
            "Kevin Leong",
            "Dennis DeCoste",
            "Sean Lie",
            "Shreyas Saxena"
        ],
        "published": "2023-03-18T17:56:01Z",
        "summary": "The pre-training and fine-tuning paradigm has contributed to a number of\nbreakthroughs in Natural Language Processing (NLP). Instead of directly\ntraining on a downstream task, language models are first pre-trained on large\ndatasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then\nfine-tuned on task-specific data (e.g., natural language generation, text\nsummarization, etc.). Scaling the model and dataset size has helped improve the\nperformance of LLMs, but unfortunately, this also lead to highly prohibitive\ncomputational costs. Pre-training LLMs often require orders of magnitude more\nFLOPs than fine-tuning and the model capacity often remains the same between\nthe two phases. To achieve training efficiency w.r.t training FLOPs, we propose\nto decouple the model capacity between the two phases and introduce Sparse\nPre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits\nof using unstructured weight sparsity to train only a subset of weights during\npre-training (Sparse Pre-training) and then recover the representational\ncapacity by allowing the zeroed weights to learn (Dense Fine-tuning). We\ndemonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3\nXL model resulting in a 2.5x reduction in pre-training FLOPs, without a\nsignificant loss in accuracy on the downstream tasks relative to the dense\nbaseline. By rigorously evaluating multiple downstream tasks, we also establish\na relationship between sparsity, task complexity and dataset size. Our work\npresents a promising direction to train large GPT models at a fraction of the\ntraining FLOPs using weight sparsity, while retaining the benefits of\npre-trained textual representations for downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.10464v2.pdf"
    },
    {
        "title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models",
        "authors": [
            "Junjie Ye",
            "Xuanting Chen",
            "Nuo Xu",
            "Can Zu",
            "Zekai Shao",
            "Shichun Liu",
            "Yuhan Cui",
            "Zeyang Zhou",
            "Chao Gong",
            "Yang Shen",
            "Jie Zhou",
            "Siming Chen",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-03-18T14:02:04Z",
        "summary": "GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models'\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models' ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.",
        "pdf_link": "https://arxiv.org/pdf/2303.10420v2.pdf"
    },
    {
        "title": "An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering",
        "authors": [
            "Nan Hu",
            "Yike Wu",
            "Guilin Qi",
            "Dehai Min",
            "Jiaoyan Chen",
            "Jeff Z. Pan",
            "Zafar Ali"
        ],
        "published": "2023-03-18T08:57:09Z",
        "summary": "Large-scale pre-trained language models (PLMs) such as BERT have recently\nachieved great success and become a milestone in natural language processing\n(NLP). It is now the consensus of the NLP community to adopt PLMs as the\nbackbone for downstream tasks. In recent works on knowledge graph question\nanswering (KGQA), BERT or its variants have become necessary in their KGQA\nmodels. However, there is still a lack of comprehensive research and comparison\nof the performance of different PLMs in KGQA. To this end, we summarize two\nbasic KGQA frameworks based on PLMs without additional neural network modules\nto compare the performance of nine PLMs in terms of accuracy and efficiency. In\naddition, we present three benchmarks for larger-scale KGs based on the popular\nSimpleQuestions benchmark to investigate the scalability of PLMs. We carefully\nanalyze the results of all PLMs-based KGQA basic frameworks on these benchmarks\nand two other popular datasets, WebQuestionSP and FreebaseQA, and find that\nknowledge distillation techniques and knowledge enhancement methods in PLMs are\npromising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal\nof attention in the NLP community, demonstrating its impressive capabilities\nand limitations in zero-shot KGQA. We have released the code and benchmarks to\npromote the use of PLMs on KGQA.",
        "pdf_link": "https://arxiv.org/pdf/2303.10368v1.pdf"
    },
    {
        "title": "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
        "authors": [
            "Lixiang Yan",
            "Lele Sha",
            "Linxuan Zhao",
            "Yuheng Li",
            "Roberto Martinez-Maldonado",
            "Guanliang Chen",
            "Xinyu Li",
            "Yueqiao Jin",
            "Dragan Ga\u0161evi\u0107"
        ],
        "published": "2023-03-17T18:14:46Z",
        "summary": "Educational technology innovations leveraging large language models (LLMs)\nhave shown the potential to automate the laborious process of generating and\nanalysing textual content. While various innovations have been developed to\nautomate a range of educational tasks (e.g., question generation, feedback\nprovision, and essay grading), there are concerns regarding the practicality\nand ethicality of these innovations. Such concerns may hinder future research\nand the adoption of LLMs-based innovations in authentic educational contexts.\nTo address this, we conducted a systematic scoping review of 118 peer-reviewed\npapers published since 2017 to pinpoint the current state of research on using\nLLMs to automate and support educational tasks. The findings revealed 53 use\ncases for LLMs in automating education tasks, categorised into nine main\ncategories: profiling/labelling, detection, grading, teaching support,\nprediction, knowledge representation, feedback, content generation, and\nrecommendation. Additionally, we also identified several practical and ethical\nchallenges, including low technological readiness, lack of replicability and\ntransparency, and insufficient privacy and beneficence considerations. The\nfindings were summarised into three recommendations for future studies,\nincluding updating existing innovations with state-of-the-art models (e.g.,\nGPT-3/4), embracing the initiative of open-sourcing models/systems, and\nadopting a human-centred approach throughout the developmental process. As the\nintersection of AI and education is continuously evolving, the findings of this\nstudy can serve as an essential reference point for researchers, allowing them\nto leverage the strengths, learn from the limitations, and uncover potential\nresearch opportunities enabled by ChatGPT and other generative AI models.",
        "pdf_link": "https://arxiv.org/pdf/2303.13379v2.pdf"
    },
    {
        "title": "Can AI-Generated Text be Reliably Detected?",
        "authors": [
            "Vinu Sankar Sadasivan",
            "Aounon Kumar",
            "Sriram Balasubramanian",
            "Wenxiao Wang",
            "Soheil Feizi"
        ],
        "published": "2023-03-17T17:53:19Z",
        "summary": "The unregulated use of LLMs can potentially lead to malicious consequences\nsuch as plagiarism, generating fake news, spamming, etc. Therefore, reliable\ndetection of AI-generated text can be critical to ensure the responsible use of\nLLMs. Recent works attempt to tackle this problem either using certain model\nsignatures present in the generated text outputs or by applying watermarking\ntechniques that imprint specific patterns onto them. In this paper, we show\nthat these detectors are not reliable in practical scenarios. In particular, we\ndevelop a recursive paraphrasing attack to apply on AI text, which can break a\nwhole range of detectors, including the ones using the watermarking schemes as\nwell as neural network-based detectors, zero-shot classifiers, and\nretrieval-based detectors. Our experiments include passages around 300 tokens\nin length, showing the sensitivity of the detectors even in the case of\nrelatively long passages. We also observe that our recursive paraphrasing only\ndegrades text quality slightly, measured via human studies, and metrics such as\nperplexity scores and accuracy on text benchmarks. Additionally, we show that\neven LLMs protected by watermarking schemes can be vulnerable against spoofing\nattacks aimed to mislead detectors to classify human-written text as\nAI-generated, potentially causing reputational damages to the developers. In\nparticular, we show that an adversary can infer hidden AI text signatures of\nthe LLM outputs without having white-box access to the detection method.\nFinally, we provide a theoretical connection between the AUROC of the best\npossible detector and the Total Variation distance between human and AI text\ndistributions that can be used to study the fundamental hardness of the\nreliable detection problem for advanced language models. Our code is publicly\navailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",
        "pdf_link": "https://arxiv.org/pdf/2303.11156v3.pdf"
    },
    {
        "title": "Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt",
        "authors": [
            "William Aiken",
            "Paul K. Mvula",
            "Paula Branco",
            "Guy-Vincent Jourdan",
            "Mehrdad Sabetzadeh",
            "Herna Viktor"
        ],
        "published": "2023-03-16T19:47:38Z",
        "summary": "Artificial Intelligence and Machine Learning have witnessed rapid,\nsignificant improvements in Natural Language Processing (NLP) tasks. Utilizing\nDeep Learning, researchers have taken advantage of repository comments in\nSoftware Engineering to produce accurate methods for detecting Self-Admitted\nTechnical Debt (SATD) from 20 open-source Java projects' code. In this work, we\nimprove SATD detection with a novel approach that leverages the Bidirectional\nEncoder Representations from Transformers (BERT) architecture. For comparison,\nwe re-evaluated previous deep learning methods and applied stratified 10-fold\ncross-validation to report reliable F$_1$-scores. We examine our model in both\ncross-project and intra-project contexts. For each context, we use re-sampling\nand duplication as augmentation strategies to account for data imbalance. We\nfind that our trained BERT model improves over the best performance of all\nprevious methods in 19 of the 20 projects in cross-project scenarios. However,\nthe data augmentation techniques were not sufficient to overcome the lack of\ndata present in the intra-project scenarios, and existing methods still perform\nbetter. Future research will look into ways to diversify SATD datasets in order\nto maximize the latent power in large BERT models.",
        "pdf_link": "https://arxiv.org/pdf/2303.09617v1.pdf"
    },
    {
        "title": "DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion",
        "authors": [
            "Maham Tanveer",
            "Yizhi Wang",
            "Ali Mahdavi-Amiri",
            "Hao Zhang"
        ],
        "published": "2023-03-16T19:12:52Z",
        "summary": "We introduce a novel method to automatically generate an artistic typography\nby stylizing one or more letter fonts to visually convey the semantics of an\ninput word, while ensuring that the output remains readable. To address an\nassortment of challenges with our task at hand including conflicting goals\n(artistic stylization vs. legibility), lack of ground truth, and immense search\nspace, our approach utilizes large language models to bridge texts and visual\nimages for stylization and build an unsupervised generative model with a\ndiffusion model backbone. Specifically, we employ the denoising generator in\nLatent Diffusion Model (LDM), with the key addition of a CNN-based\ndiscriminator to adapt the input style onto the input text. The discriminator\nuses rasterized images of a given letter/word font as real samples and output\nof the denoising generator as fake samples. Our model is coined DS-Fusion for\ndiscriminated and stylized diffusion. We showcase the quality and versatility\nof our method through numerous examples, qualitative and quantitative\nevaluation, as well as ablation studies. User studies comparing to strong\nbaselines including CLIPDraw and DALL-E 2, as well as artist-crafted\ntypographies, demonstrate strong performance of DS-Fusion.",
        "pdf_link": "https://arxiv.org/pdf/2303.09604v1.pdf"
    },
    {
        "title": "LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations",
        "authors": [
            "Catherine Tony",
            "Markus Mutas",
            "Nicol\u00e1s E. D\u00edaz Ferreyra",
            "Riccardo Scandariato"
        ],
        "published": "2023-03-16T15:13:58Z",
        "summary": "Large Language Models (LLMs) like Codex are powerful tools for performing\ncode completion and code generation tasks as they are trained on billions of\nlines of code from publicly available sources. Moreover, these models are\ncapable of generating code snippets from Natural Language (NL) descriptions by\nlearning languages and programming practices from public GitHub repositories.\nAlthough LLMs promise an effortless NL-driven deployment of software\napplications, the security of the code they generate has not been extensively\ninvestigated nor documented. In this work, we present LLMSecEval, a dataset\ncontaining 150 NL prompts that can be leveraged for assessing the security\nperformance of such models. Such prompts are NL descriptions of code snippets\nprone to various security vulnerabilities listed in MITRE's Top 25 Common\nWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a\nsecure implementation example to facilitate comparative evaluations against\ncode produced by LLMs. As a practical application, we show how LLMSecEval can\nbe used for evaluating the security of snippets automatically generated from NL\ndescriptions.",
        "pdf_link": "https://arxiv.org/pdf/2303.09384v1.pdf"
    },
    {
        "title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Christopher Bogart",
            "Yifan Song",
            "Majd Sakr"
        ],
        "published": "2023-03-16T13:58:45Z",
        "summary": "We evaluated the capability of generative pre-trained transformers (GPT), to\npass assessments in introductory and intermediate Python programming courses at\nthe postsecondary level. Discussions of potential uses (e.g., exercise\ngeneration, code explanation) and misuses (e.g., cheating) of this emerging\ntechnology in programming education have intensified, but to date there has not\nbeen a rigorous analysis of the models' capabilities in the realistic context\nof a full-fledged programming course with diverse set of assessment\ninstruments. We evaluated GPT on three Python courses that employ assessments\nranging from simple multiple-choice questions (no code involved) to complex\nprogramming projects with code bases distributed into multiple files (599\nexercises overall). Further, we studied if and how successfully GPT models\nleverage feedback provided by an auto-grader. We found that the current models\nare not capable of passing the full spectrum of assessments typically involved\nin a Python programming course (<70% on even entry-level modules). Yet, it is\nclear that a straightforward application of these easily accessible models\ncould enable a learner to obtain a non-trivial portion of the overall available\nscore (>55%) in introductory and intermediate courses alike. While the models\nexhibit remarkable capabilities, including correcting solutions based on\nauto-grader's feedback, some limitations exist (e.g., poor handling of\nexercises requiring complex chains of reasoning steps). These findings can be\nleveraged by instructors wishing to adapt their assessments so that GPT becomes\na valuable assistant for a learner as opposed to an end-to-end solution.",
        "pdf_link": "https://arxiv.org/pdf/2303.09325v1.pdf"
    },
    {
        "title": "How well do Large Language Models perform in Arithmetic tasks?",
        "authors": [
            "Zheng Yuan",
            "Hongyi Yuan",
            "Chuanqi Tan",
            "Wei Wang",
            "Songfang Huang"
        ],
        "published": "2023-03-16T09:28:15Z",
        "summary": "Large language models have emerged abilities including chain-of-thought to\nanswer math word problems step by step. Solving math word problems not only\nrequires abilities to disassemble problems via chain-of-thought but also needs\nto calculate arithmetic expressions correctly for each step. To the best of our\nknowledge, there is no work to focus on evaluating the arithmetic ability of\nlarge language models. In this work, we propose an arithmetic dataset MATH 401\nto test the latest large language models including GPT-4, ChatGPT, InstrctGPT,\nGalactica, and LLaMA with various arithmetic expressions and provide a detailed\nanalysis of the ability of large language models. MATH 401 and evaluation codes\nare released at \\url{https://github.com/GanjinZero/math401-llm}.",
        "pdf_link": "https://arxiv.org/pdf/2304.02015v1.pdf"
    },
    {
        "title": "A Short Survey of Viewing Large Language Models in Legal Aspect",
        "authors": [
            "Zhongxiang Sun"
        ],
        "published": "2023-03-16T08:01:22Z",
        "summary": "Large language models (LLMs) have transformed many fields, including natural\nlanguage processing, computer vision, and reinforcement learning. These models\nhave also made a significant impact in the field of law, where they are being\nincreasingly utilized to automate various legal tasks, such as legal judgement\nprediction, legal document analysis, and legal document writing. However, the\nintegration of LLMs into the legal field has also raised several legal\nproblems, including privacy concerns, bias, and explainability. In this survey,\nwe explore the integration of LLMs into the field of law. We discuss the\nvarious applications of LLMs in legal tasks, examine the legal challenges that\narise from their use, and explore the data resources that can be used to\nspecialize LLMs in the legal domain. Finally, we discuss several promising\ndirections and conclude this paper. By doing so, we hope to provide an overview\nof the current state of LLMs in law and highlight the potential benefits and\nchallenges of their integration.",
        "pdf_link": "https://arxiv.org/pdf/2303.09136v1.pdf"
    },
    {
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "authors": [
            "Shushan Arakelyan",
            "Rocktim Jyoti Das",
            "Yi Mao",
            "Xiang Ren"
        ],
        "published": "2023-03-16T07:45:46Z",
        "summary": "We systematically study how three large language models with code\ncapabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.\nWe consider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. We\nestablish that samples from each new domain present all the models with a\nsignificant challenge of distribution shift. We study how established methods\nadapt models to better generalize to new domains. Our experiments show that\nwhile multitask learning alone is a reasonable baseline, combining it with\nfew-shot finetuning on examples retrieved from training data can achieve very\nstrong performance. Moreover, this solution can outperform direct finetuning\nfor very low-data scenarios. Finally, we consider variations of this approach\nto create a more broadly applicable method to adapt to multiple domains at\nonce. We find that for code generation, a model adapted to multiple domains\nsimultaneously performs on par with those adapted to a single domain",
        "pdf_link": "https://arxiv.org/pdf/2303.09128v2.pdf"
    },
    {
        "title": "Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential",
        "authors": [
            "Qing Lyu",
            "Josh Tan",
            "Michael E. Zapadka",
            "Janardhana Ponnatapura",
            "Chuang Niu",
            "Kyle J. Myers",
            "Ge Wang",
            "Christopher T. Whitlow"
        ],
        "published": "2023-03-16T02:21:39Z",
        "summary": "The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.",
        "pdf_link": "https://arxiv.org/pdf/2303.09038v3.pdf"
    },
    {
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "authors": [
            "Bhargavi Paranjape",
            "Scott Lundberg",
            "Sameer Singh",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-03-16T01:04:45Z",
        "summary": "Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2303.09014v1.pdf"
    },
    {
        "title": "Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs",
        "authors": [
            "Yankai Zeng",
            "Abhiramon Rajasekharan",
            "Parth Padalkar",
            "Kinjal Basu",
            "Joaqu\u00edn Arias",
            "Gopal Gupta"
        ],
        "published": "2023-03-15T21:10:33Z",
        "summary": "Achieving human-like communication with machines remains a classic,\nchallenging topic in the field of Knowledge Representation and Reasoning and\nNatural Language Processing. These Large Language Models (LLMs) rely on\npattern-matching rather than a true understanding of the semantic meaning of a\nsentence. As a result, they may generate incorrect responses. To generate an\nassuredly correct response, one has to \"understand\" the semantics of a\nsentence. To achieve this \"understanding\", logic-based (commonsense) reasoning\nmethods such as Answer Set Programming (ASP) are arguably needed. In this\npaper, we describe the AutoConcierge system that leverages LLMs and ASP to\ndevelop a conversational agent that can truly \"understand\" human dialogs in\nrestricted domains. AutoConcierge is focused on a specific domain-advising\nusers about restaurants in their local area based on their preferences.\nAutoConcierge will interactively understand a user's utterances, identify the\nmissing information in them, and request the user via a natural language\nsentence to provide it. Once AutoConcierge has determined that all the\ninformation has been received, it computes a restaurant recommendation based on\nthe user-preferences it has acquired from the human user. AutoConcierge is\nbased on our STAR framework developed earlier, which uses GPT-3 to convert\nhuman dialogs into predicates that capture the deep structure of the dialog's\nsentence. These predicates are then input into the goal-directed s(CASP) ASP\nsystem for performing commonsense reasoning. To the best of our knowledge,\nAutoConcierge is the first automated conversational agent that can\nrealistically converse like a human and provide help to humans based on truly\nunderstanding human utterances.",
        "pdf_link": "https://arxiv.org/pdf/2303.08941v2.pdf"
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "Mark J. F. Gales"
        ],
        "published": "2023-03-15T19:31:21Z",
        "summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2303.08896v3.pdf"
    },
    {
        "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
        "authors": [
            "Yubo Ma",
            "Yixin Cao",
            "YongChing Hong",
            "Aixin Sun"
        ],
        "published": "2023-03-15T12:20:13Z",
        "summary": "Large Language Models (LLMs) have made remarkable strides in various tasks.\nWhether LLMs are competitive few-shot solvers for information extraction (IE)\ntasks, however, remains an open problem. In this work, we aim to provide a\nthorough answer to this question. Through extensive experiments on nine\ndatasets across four IE tasks, we demonstrate that current advanced LLMs\nconsistently exhibit inferior performance, higher latency, and increased budget\nrequirements compared to fine-tuned SLMs under most settings. Therefore, we\nconclude that LLMs are not effective few-shot information extractors in\ngeneral. Nonetheless, we illustrate that with appropriate prompting strategies,\nLLMs can effectively complement SLMs and tackle challenging samples that SLMs\nstruggle with. And moreover, we propose an adaptive filter-then-rerank paradigm\nto combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as\nfilters and LLMs serve as rerankers. By prompting LLMs to rerank a small\nportion of difficult samples identified by SLMs, our preliminary system\nconsistently achieves promising improvements (2.4% F1-gain on average) on\nvarious IE tasks, with an acceptable time and cost investment.",
        "pdf_link": "https://arxiv.org/pdf/2303.08559v2.pdf"
    },
    {
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "authors": [
            "Daixuan Cheng",
            "Shaohan Huang",
            "Junyu Bi",
            "Yuefeng Zhan",
            "Jianfeng Liu",
            "Yujing Wang",
            "Hao Sun",
            "Furu Wei",
            "Denvy Deng",
            "Qi Zhang"
        ],
        "published": "2023-03-15T10:53:49Z",
        "summary": "Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://arxiv.org/pdf/2303.08518v4.pdf"
    },
    {
        "title": "A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records",
        "authors": [
            "Sicheng Zhou",
            "Nan Wang",
            "Liwei Wang",
            "Ju Sun",
            "Anne Blaes",
            "Hongfang Liu",
            "Rui Zhang"
        ],
        "published": "2023-03-15T08:44:07Z",
        "summary": "Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.",
        "pdf_link": "https://arxiv.org/pdf/2303.08448v1.pdf"
    },
    {
        "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation",
        "authors": [
            "Zhewei Yao",
            "Xiaoxia Wu",
            "Cheng Li",
            "Stephen Youn",
            "Yuxiong He"
        ],
        "published": "2023-03-15T01:27:15Z",
        "summary": "Post-training quantization (PTQ) has emerged as a promising technique for\nmitigating memory consumption and computational costs in large language models\n(LLMs). However, a systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent from the\nliterature. In this paper, we conduct a comprehensive analysis of these factors\nby investigating the effects of PTQ on weight-only, activation-only, and\nweight-and-activation quantization using diverse methods such as\nround-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these\nmethods to two distinct model families with parameters ranging from 125M to\n176B. Our contributions include: (1) a sensitivity analysis revealing that\nactivation quantization is generally more susceptible to weight quantization,\nwith smaller models often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing PTQ methods to\noptimize model size reduction while minimizing the impact on accuracy,\nrevealing that none of the current methods can achieve the original model\nquality for quantization with either INT4-weight or\nINT4-weight-and-INT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC), which employs low-rank\nmatrices to enhance model quality recovery with a minimal increase in model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2303.08302v3.pdf"
    },
    {
        "title": "Attention-likelihood relationship in transformers",
        "authors": [
            "Valeria Ruscio",
            "Valentino Maiorca",
            "Fabrizio Silvestri"
        ],
        "published": "2023-03-15T00:23:49Z",
        "summary": "We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.",
        "pdf_link": "https://arxiv.org/pdf/2303.08288v1.pdf"
    },
    {
        "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models",
        "authors": [
            "Xufeng Zhao",
            "Mengdi Li",
            "Cornelius Weber",
            "Muhammad Burhan Hafez",
            "Stefan Wermter"
        ],
        "published": "2023-03-14T23:01:27Z",
        "summary": "Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2303.08268v3.pdf"
    },
    {
        "title": "Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures",
        "authors": [
            "Aokun Chen",
            "Zehao Yu",
            "Xi Yang",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-03-14T22:22:28Z",
        "summary": "Objective: To develop a natural language processing (NLP) system to extract\nmedications and contextual information that help understand drug changes. This\nproject is part of the 2022 n2c2 challenge.\n  Materials and methods: We developed NLP systems for medication mention\nextraction, event classification (indicating medication changes discussed or\nnot), and context classification to classify medication changes context into 5\northogonal dimensions related to drug changes. We explored 6 state-of-the-art\npretrained transformer models for the three subtasks, including GatorTron, a\nlarge language model pretrained using >90 billion words of text (including >80\nbillion words from >290 million clinical notes identified at the University of\nFlorida Health). We evaluated our NLP systems using annotated data and\nevaluation scripts provided by the 2022 n2c2 organizers.\n  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for\nmedication extraction (ranked 3rd), 0.9379 for event classification (ranked\n2nd), and the best micro-average accuracy of 0.9126 for context classification.\nGatorTron outperformed existing transformer models pretrained using smaller\ngeneral English text and clinical text corpora, indicating the advantage of\nlarge language models.\n  Conclusion: This study demonstrated the advantage of using large transformer\nmodels for contextual medication information extraction from clinical\nnarratives.",
        "pdf_link": "https://arxiv.org/pdf/2303.08259v1.pdf"
    },
    {
        "title": "How Many Demonstrations Do You Need for In-context Learning?",
        "authors": [
            "Jiuhai Chen",
            "Lichang Chen",
            "Chen Zhu",
            "Tianyi Zhou"
        ],
        "published": "2023-03-14T17:50:45Z",
        "summary": "Large language models (LLMs) are capable to perform complex reasoning by\nin-context learning (ICL) when provided with a few input-output demonstrations\n(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts\n(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In\nthis paper, we study ICL using fewer demos for each test query on the tasks\nin~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation\nwhen using only one randomly chosen demo. To study this phenomenon, for each\ntest query, we categorize demos into \"correct demos\" leading to the correct\nanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals an\ninherent bias in those widely studied datasets: most demos are correct for a\nmajority of test queries, which explains the good performance of using one\nrandom demo. Moreover, ICL (with and w/o CoT) using only one correct demo\nsignificantly outperforms all-demo ICL adopted by most previous works,\nindicating the weakness of LLMs in finding correct demo(s) for input queries,\nwhich is difficult to evaluate on the biased datasets. Furthermore, we observe\na counterintuitive behavior of ICL using multi-demo, i.e., its accuracy\ndegrades(improves) when given more correct(wrong) demos. This implies that ICL\ncan be easily misguided by interference among demos and their spurious\ncorrelations. Our analyses highlight several fundamental challenges that need\nto be addressed in LLMs training, ICL, and benchmark design.",
        "pdf_link": "https://arxiv.org/pdf/2303.08119v3.pdf"
    },
    {
        "title": "Do Transformers Parse while Predicting the Masked Word?",
        "authors": [
            "Haoyu Zhao",
            "Abhishek Panigrahi",
            "Rong Ge",
            "Sanjeev Arora"
        ],
        "published": "2023-03-14T17:49:50Z",
        "summary": "Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.",
        "pdf_link": "https://arxiv.org/pdf/2303.08117v2.pdf"
    },
    {
        "title": "Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family",
        "authors": [
            "Yiming Tan",
            "Dehai Min",
            "Yu Li",
            "Wenbo Li",
            "Nan Hu",
            "Yongrui Chen",
            "Guilin Qi"
        ],
        "published": "2023-03-14T15:46:28Z",
        "summary": "ChatGPT is a powerful large language model (LLM) that covers knowledge\nresources such as Wikipedia and supports natural language question answering\nusing its own knowledge. Therefore, there is growing interest in exploring\nwhether ChatGPT can replace traditional knowledge-based question answering\n(KBQA) models. Although there have been some works analyzing the question\nanswering performance of ChatGPT, there is still a lack of large-scale,\ncomprehensive testing of various types of complex questions to analyze the\nlimitations of the model. In this paper, we present a framework that follows\nthe black-box testing specifications of CheckList proposed by Ribeiro et. al.\nWe evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex\nquestion answering datasets, which include six English datasets and two\nmultilingual datasets. The total number of test cases is approximately 190,000.\nIn addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5\nto identify commonalities between the GPT family and other LLMs. The dataset\nand code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git",
        "pdf_link": "https://arxiv.org/pdf/2303.07992v3.pdf"
    },
    {
        "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction",
        "authors": [
            "Michael Hahn",
            "Navin Goyal"
        ],
        "published": "2023-03-14T15:24:05Z",
        "summary": "Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.07971v1.pdf"
    },
    {
        "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
        "authors": [
            "Souradip Chakraborty",
            "Kasun Weerakoon",
            "Prithvi Poddar",
            "Mohamed Elnoor",
            "Priya Narayanan",
            "Carl Busart",
            "Pratap Tokekar",
            "Amrit Singh Bedi",
            "Dinesh Manocha"
        ],
        "published": "2023-03-14T04:20:59Z",
        "summary": "Reinforcement learning-based policies for continuous control robotic\nnavigation tasks often fail to adapt to changes in the environment during\nreal-time deployment, which may result in catastrophic failures. To address\nthis limitation, we propose a novel approach called RE-MOVE (REquest help and\nMOVE on) to adapt already trained policy to real-time changes in the\nenvironment without re-training via utilizing a language-based feedback. The\nproposed approach essentially boils down to addressing two main challenges of\n(1) when to ask for feedback and, if received, (2) how to incorporate feedback\ninto trained policies. RE-MOVE incorporates an epistemic uncertainty-based\nframework to determine the optimal time to request instructions-based feedback.\nFor the second challenge, we employ a zero-shot learning natural language\nprocessing (NLP) paradigm with efficient, prompt design and leverage\nstate-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the\nproposed approach, we performed extensive synthetic and real-world evaluations\nin several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in\nup to 80% enhancement in the attainment of successful goals, coupled with a\nreduction of 13.50% in the normalized trajectory length, as compared to\nalternative approaches, particularly in demanding real-world environments with\nperceptual challenges.",
        "pdf_link": "https://arxiv.org/pdf/2303.07622v2.pdf"
    },
    {
        "title": "Input-length-shortening and text generation via attention values",
        "authors": [
            "Ne\u015fet \u00d6zkan Tan",
            "Alex Yuxuan Peng",
            "Joshua Bensemann",
            "Qiming Bao",
            "Tim Hartill",
            "Mark Gahegan",
            "Michael Witbrock"
        ],
        "published": "2023-03-14T02:11:24Z",
        "summary": "Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.",
        "pdf_link": "https://arxiv.org/pdf/2303.07585v1.pdf"
    },
    {
        "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification",
        "authors": [
            "Benjamin Clavi\u00e9",
            "Alexandru Ciceu",
            "Frederick Naylor",
            "Guillaume Souli\u00e9",
            "Thomas Brightwell"
        ],
        "published": "2023-03-13T14:09:53Z",
        "summary": "This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.07142v3.pdf"
    },
    {
        "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",
        "authors": [
            "Ying Sheng",
            "Lianmin Zheng",
            "Binhang Yuan",
            "Zhuohan Li",
            "Max Ryabinin",
            "Daniel Y. Fu",
            "Zhiqiang Xie",
            "Beidi Chen",
            "Clark Barrett",
            "Joseph E. Gonzalez",
            "Percy Liang",
            "Christopher R\u00e9",
            "Ion Stoica",
            "Ce Zhang"
        ],
        "published": "2023-03-13T05:19:28Z",
        "summary": "The high computational and memory requirements of large language model (LLM)\ninference make it feasible only with multiple high-end accelerators. Motivated\nby the emerging demand for latency-insensitive tasks with batched processing,\nthis paper initiates the study of high-throughput LLM inference using limited\nresources, such as a single commodity GPU. We present FlexGen, a\nhigh-throughput generation engine for running LLMs with limited GPU memory.\nFlexGen can be flexibly configured under various hardware resource constraints\nby aggregating memory and computation from the GPU, CPU, and disk. By solving a\nlinear programming problem, it searches for efficient patterns to store and\naccess tensors. FlexGen further compresses the weights and the attention cache\nto 4 bits with negligible accuracy loss. These techniques enable FlexGen to\nhave a larger space of batch size choices and thus significantly increase\nmaximum throughput. As a result, when running OPT-175B on a single 16GB GPU,\nFlexGen achieves significantly higher throughput compared to state-of-the-art\noffloading systems, reaching a generation throughput of 1 token/s for the first\ntime with an effective batch size of 144. On the HELM benchmark, FlexGen can\nbenchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21\nhours. The code is available at https://github.com/FMInference/FlexGen",
        "pdf_link": "https://arxiv.org/pdf/2303.06865v2.pdf"
    },
    {
        "title": "Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks",
        "authors": [
            "Zijian Ding",
            "Joel Chan"
        ],
        "published": "2023-03-11T15:45:47Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive text generation\ncapabilities, prompting us to reconsider the future of human-AI co-creation and\nhow humans interact with LLMs. In this paper, we present a spectrum of content\ngeneration tasks and their corresponding human-AI interaction patterns. These\ntasks include: 1) fixed-scope content curation tasks with minimal human-AI\ninteractions, 2) independent creative tasks with precise human-AI interactions,\nand 3) complex and interdependent creative tasks with iterative human-AI\ninteractions. We encourage the generative AI and HCI research communities to\nfocus on the more complex and interdependent tasks, which require greater\nlevels of human involvement.",
        "pdf_link": "https://arxiv.org/pdf/2303.06430v2.pdf"
    },
    {
        "title": "ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
        "authors": [
            "Jules White",
            "Sam Hays",
            "Quchen Fu",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "published": "2023-03-11T14:43:17Z",
        "summary": "This paper presents prompt design techniques for software engineering, in the\nform of patterns, to solve common problems when using large language models\n(LLMs), such as ChatGPT to automate common software engineering activities,\nsuch as ensuring code is decoupled from third-party libraries and simulating a\nweb application API before it is implemented. This paper provides two\ncontributions to research on using LLMs for software engineering. First, it\nprovides a catalog of patterns for software engineering that classifies\npatterns according to the types of problems they solve. Second, it explores\nseveral prompt patterns that have been applied to improve requirements\nelicitation, rapid prototyping, code quality, refactoring, and system design.",
        "pdf_link": "https://arxiv.org/pdf/2303.07839v1.pdf"
    },
    {
        "title": "Consistency Analysis of ChatGPT",
        "authors": [
            "Myeongjun Erik Jang",
            "Thomas Lukasiewicz"
        ],
        "published": "2023-03-11T01:19:01Z",
        "summary": "ChatGPT has gained a huge popularity since its introduction. Its positive\naspects have been reported through many media platforms, and some analyses even\nshowed that ChatGPT achieved a decent grade in professional exams, adding extra\nsupport to the claim that AI can now assist and even replace humans in\nindustrial fields. Others, however, doubt its reliability and trustworthiness.\nThis paper investigates the trustworthiness of ChatGPT and GPT-4 regarding\nlogically consistent behaviour, focusing specifically on semantic consistency\nand the properties of negation, symmetric, and transitive consistency. Our\nfindings suggest that while both models appear to show an enhanced language\nunderstanding and reasoning ability, they still frequently fall short of\ngenerating logically consistent predictions. We also ascertain via experiments\nthat prompt designing, few-shot learning and employing larger large language\nmodels (LLMs) are unlikely to be the ultimate solution to resolve the\ninconsistency issue of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.06273v3.pdf"
    },
    {
        "title": "Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook",
        "authors": [
            "Teresa Datta",
            "John P. Dickerson"
        ],
        "published": "2023-03-10T22:15:49Z",
        "summary": "Deployed artificial intelligence (AI) often impacts humans, and there is no\none-size-fits-all metric to evaluate these tools. Human-centered evaluation of\nAI-based systems combines quantitative and qualitative analysis and human\ninput. It has been explored to some depth in the explainable AI (XAI) and\nhuman-computer interaction (HCI) communities. Gaps remain, but the basic\nunderstanding that humans interact with AI and accompanying explanations, and\nthat humans' needs -- complete with their cognitive biases and quirks -- should\nbe held front and center, is accepted by the community. In this paper, we draw\nparallels between the relatively mature field of XAI and the rapidly evolving\nresearch boom around large language models (LLMs). Accepted evaluative metrics\nfor LLMs are not human-centered. We argue that many of the same paths tread by\nthe XAI community over the past decade will be retread when discussing LLMs.\nSpecifically, we argue that humans' tendencies -- again, complete with their\ncognitive biases and quirks -- should rest front and center when evaluating\ndeployed LLMs. We outline three developed focus areas of human-centered\nevaluation of XAI: mental models, use case utility, and cognitive engagement,\nand we highlight the importance of exploring each of these concepts for LLMs.\nOur goal is to jumpstart human-centered LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2303.06223v1.pdf"
    },
    {
        "title": "Susceptibility to Influence of Large Language Models",
        "authors": [
            "Lewis D Griffin",
            "Bennett Kleinberg",
            "Maximilian Mozes",
            "Kimberly T Mai",
            "Maria Vau",
            "Matthew Caldwell",
            "Augustine Marvor-Parker"
        ],
        "published": "2023-03-10T16:53:30Z",
        "summary": "Two studies tested the hypothesis that a Large Language Model (LLM) can be\nused to model psychological change following exposure to influential input. The\nfirst study tested a generic mode of influence - the Illusory Truth Effect\n(ITE) - where earlier exposure to a statement (through, for example, rating its\ninterest) boosts a later truthfulness test rating. Data was collected from 1000\nhuman participants using an online experiment, and 1000 simulated participants\nusing engineered prompts and LLM completion. 64 ratings per participant were\ncollected, using all exposure-test combinations of the attributes: truth,\ninterest, sentiment and importance. The results for human participants\nreconfirmed the ITE, and demonstrated an absence of effect for attributes other\nthan truth, and when the same attribute is used for exposure and test. The same\npattern of effects was found for LLM-simulated participants. The second study\nconcerns a specific mode of influence - populist framing of news to increase\nits persuasion and political mobilization. Data from LLM-simulated participants\nwas collected and compared to previously published data from a 15-country\nexperiment on 7286 human participants. Several effects previously demonstrated\nfrom the human study were replicated by the simulated study, including effects\nthat surprised the authors of the human study by contradicting their\ntheoretical expectations (anti-immigrant framing of news decreases its\npersuasion and mobilization); but some significant relationships found in human\ndata (modulation of the effectiveness of populist framing according to relative\ndeprivation of the participant) were not present in the LLM data. Together the\ntwo studies support the view that LLMs have potential to act as models of the\neffect of influence.",
        "pdf_link": "https://arxiv.org/pdf/2303.06074v1.pdf"
    },
    {
        "title": "Do large language models resemble humans in language use?",
        "authors": [
            "Zhenguang G. Cai",
            "Xufeng Duan",
            "David A. Haslett",
            "Shuqi Wang",
            "Martin J. Pickering"
        ],
        "published": "2023-03-10T10:47:59Z",
        "summary": "Large language models (LLMs) such as ChatGPT and Vicuna have shown remarkable\ncapacities in comprehending and producing language. However, their internal\nworkings remain a black box, and it is unclear whether LLMs and chatbots can\ndevelop humanlike characteristics in language use. Cognitive scientists have\ndevised many experiments that probe, and have made great progress in\nexplaining, how people comprehend and produce language. We subjected ChatGPT\nand Vicuna to 12 of these experiments ranging from sounds to dialogue,\npreregistered and with 1000 runs (i.e., iterations) per experiment. ChatGPT and\nVicuna replicated the human pattern of language use in 10 and 7 out of the 12\nexperiments, respectively. The models associated unfamiliar words with\ndifferent meanings depending on their forms, continued to access recently\nencountered meanings of ambiguous words, reused recent sentence structures,\nattributed causality as a function of verb semantics, and accessed different\nmeanings and retrieved different words depending on an interlocutor's identity.\nIn addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible\nsentences that were likely to have been corrupted by noise, drew reasonable\ninferences, and overlooked semantic fallacies in a sentence. Finally, unlike\nhumans, neither model preferred using shorter words to convey less informative\ncontent, nor did they use context to resolve syntactic ambiguities. We discuss\nhow these convergences and divergences may result from the transformer\narchitecture. Overall, these experiments demonstrate that LLMs such as ChatGPT\n(and Vicuna to a lesser extent) are humanlike in many aspects of human language\nprocessing.",
        "pdf_link": "https://arxiv.org/pdf/2303.08014v2.pdf"
    },
    {
        "title": "Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors",
        "authors": [
            "Mesut Erhan Unal",
            "Adriana Kovashka"
        ],
        "published": "2023-03-09T19:08:02Z",
        "summary": "Human-object interaction (HOI) detection aims to extract interacting\nhuman-object pairs and their interaction categories from a given natural image.\nEven though the labeling effort required for building HOI detection datasets is\ninherently more extensive than for many other computer vision tasks,\nweakly-supervised directions in this area have not been sufficiently explored\ndue to the difficulty of learning human-object interactions with weak\nsupervision, rooted in the combinatorial nature of interactions over the object\nand predicate space. In this paper, we tackle HOI detection with the weakest\nsupervision setting in the literature, using only image-level interaction\nlabels, with the help of a pretrained vision-language model (VLM) and a large\nlanguage model (LLM). We first propose an approach to prune non-interacting\nhuman and object proposals to increase the quality of positive pairs within the\nbag, exploiting the grounding capability of the vision-language model. Second,\nwe use a large language model to query which interactions are possible between\na human and a given object category, in order to force the model not to put\nemphasis on unlikely interactions. Lastly, we use an auxiliary\nweakly-supervised preposition prediction task to make our model explicitly\nreason about space. Extensive experiments and ablations show that all of our\ncontributions increase HOI detection performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.05546v1.pdf"
    },
    {
        "title": "Planning with Large Language Models for Code Generation",
        "authors": [
            "Shun Zhang",
            "Zhenfang Chen",
            "Yikang Shen",
            "Mingyu Ding",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "2023-03-09T18:59:47Z",
        "summary": "Existing large language model-based code generation pipelines typically use\nbeam search or sampling algorithms during the decoding process. Although the\nprograms they generate achieve high token-matching-based scores, they often\nfail to compile or generate incorrect outputs. The main reason is that\nconventional Transformer decoding algorithms may not be the best choice for\ncode generation. In this work, we propose a novel Transformer decoding\nalgorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning\nalgorithm to do lookahead search and guide the Transformer to generate better\nprograms. Specifically, instead of simply optimizing the likelihood of the\ngenerated sequences, the Transformer makes use of a planner to generate\ncandidate programs and test them on public test cases. The Transformer can\ntherefore make more informed decisions and generate tokens that will eventually\nlead to higher-quality programs. We also design a mechanism that shares\ninformation between the Transformer and the planner to make our algorithm\ncomputationally efficient. We empirically evaluate our framework with several\nlarge language models as backbones on public coding challenge benchmarks,\nshowing that 1) it can generate programs that consistently achieve higher\nperformance compared with competing baseline methods; 2) it enables\ncontrollable code generation, such as concise codes and highly-commented codes\nby optimizing modified objective.",
        "pdf_link": "https://arxiv.org/pdf/2303.05510v1.pdf"
    },
    {
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
        "authors": [
            "Hannah Rose Kirk",
            "Bertie Vidgen",
            "Paul R\u00f6ttger",
            "Scott A. Hale"
        ],
        "published": "2023-03-09T17:52:07Z",
        "summary": "Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.",
        "pdf_link": "https://arxiv.org/pdf/2303.05453v1.pdf"
    },
    {
        "title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Christopher Bogart",
            "Majd Sakr"
        ],
        "published": "2023-03-09T16:52:12Z",
        "summary": "We analyzed effectiveness of three generative pre-trained transformer (GPT)\nmodels in answering multiple-choice question (MCQ) assessments, often involving\nshort snippets of code, from introductory and intermediate programming courses\nat the postsecondary level. This emerging technology stirs countless\ndiscussions of its potential uses (e.g., exercise generation, code explanation)\nas well as misuses in programming education (e.g., cheating). However, the\ncapabilities of GPT models and their limitations to reason about and/or analyze\ncode in educational settings have been under-explored. We evaluated several\nOpenAI's GPT models on formative and summative MCQ assessments from three\nPython courses (530 questions). We found that MCQs containing code snippets are\nnot answered as successfully as those that only contain natural language. While\nquestions requiring to fill-in a blank in the code or completing a natural\nlanguage statement about the snippet are handled rather successfully, MCQs that\nrequire analysis and/or reasoning about the code (e.g., what is true/false\nabout the snippet, or what is its output) appear to be the most challenging.\nThese findings can be leveraged by educators to adapt their instructional\npractices and assessments in programming courses, so that GPT becomes a\nvaluable assistant for a learner as opposed to a source of confusion and/or\npotential hindrance in the learning process.",
        "pdf_link": "https://arxiv.org/pdf/2303.08033v1.pdf"
    },
    {
        "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
        "authors": [
            "Guo Yang",
            "Daniel Lo",
            "Robert Mullins",
            "Yiren Zhao"
        ],
        "published": "2023-03-09T14:44:31Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance on a\nrange of Natural Language Processing (NLP) tasks. Unfortunately, the immense\namount of computations and memory accesses required for LLM training makes them\nprohibitively expensive in terms of hardware cost, and thus challenging to\ndeploy in use cases such as on-device learning. In this paper, motivated by the\nobservation that LLM training is memory-bound, we propose a novel dynamic\nquantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a\nspecial focus on reducing the memory operations, but also enjoys the other\nbenefits of low precision training, such as the reduced arithmetic cost. We\nconduct a thorough study on two translation tasks (trained-from-scratch) and\nthree classification tasks (fine-tuning). DSQ reduces the amount of arithmetic\noperations by $20.95\\times$ and the number of DRAM operations by $2.55\\times$\non IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in\non-device learning.",
        "pdf_link": "https://arxiv.org/pdf/2303.05295v1.pdf"
    },
    {
        "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction",
        "authors": [
            "Jiabang He",
            "Lei Wang",
            "Yi Hu",
            "Ning Liu",
            "Hui Liu",
            "Xing Xu",
            "Heng Tao Shen"
        ],
        "published": "2023-03-09T06:24:50Z",
        "summary": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting. Code is available at\nhttps://github.com/MAEHCM/ICL-D3IE.",
        "pdf_link": "https://arxiv.org/pdf/2303.05063v4.pdf"
    },
    {
        "title": "Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification",
        "authors": [
            "Jiayi Pan",
            "Glen Chou",
            "Dmitry Berenson"
        ],
        "published": "2023-03-09T00:09:58Z",
        "summary": "To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.",
        "pdf_link": "https://arxiv.org/pdf/2303.08006v2.pdf"
    },
    {
        "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
        "authors": [
            "Matthias Cosler",
            "Christopher Hahn",
            "Daniel Mendoza",
            "Frederik Schmitt",
            "Caroline Trippel"
        ],
        "published": "2023-03-08T20:08:53Z",
        "summary": "A rigorous formalization of desired system requirements is indispensable when\nperforming any verification task. This often limits the application of\nverification techniques, as writing formal specifications is an error-prone and\ntime-consuming manual task. To facilitate this, we present nl2spec, a framework\nfor applying Large Language Models (LLMs) to derive formal specifications (in\ntemporal logics) from unstructured natural language. In particular, we\nintroduce a new methodology to detect and resolve the inherent ambiguity of\nsystem requirements in natural language: we utilize LLMs to map subformulas of\nthe formalization back to the corresponding natural language fragments of the\ninput. Users iteratively add, delete, and edit these sub-translations to amend\nerroneous formalizations, which is easier than manually redrafting the entire\nformalization. The framework is agnostic to specific application domains and\ncan be extended to similar specification languages and new neural models. We\nperform a user study to obtain a challenging dataset, which we use to run\nexperiments on the quality of translations. We provide an open-source\nimplementation, including a web-based frontend.",
        "pdf_link": "https://arxiv.org/pdf/2303.04864v1.pdf"
    },
    {
        "title": "Stealing the Decoding Algorithms of Language Models",
        "authors": [
            "Ali Naseh",
            "Kalpesh Krishna",
            "Mohit Iyyer",
            "Amir Houmansadr"
        ],
        "published": "2023-03-08T17:15:58Z",
        "summary": "A key component of generating text from modern language models (LM) is the\nselection and tuning of decoding algorithms. These algorithms determine how to\ngenerate text from the internal probability distribution generated by the LM.\nThe process of choosing a decoding algorithm and tuning its hyperparameters\ntakes significant time, manual effort, and computation, and it also requires\nextensive human evaluation. Therefore, the identity and hyperparameters of such\ndecoding algorithms are considered to be extremely valuable to their owners. In\nthis work, we show, for the first time, that an adversary with typical API\naccess to an LM can steal the type and hyperparameters of its decoding\nalgorithms at very low monetary costs. Our attack is effective against popular\nLMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We\ndemonstrate the feasibility of stealing such information with only a few\ndollars, e.g., $\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of\nGPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2303.04729v4.pdf"
    },
    {
        "title": "Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference",
        "authors": [
            "Chi Wang",
            "Susan Xueqing Liu",
            "Ahmed H. Awadallah"
        ],
        "published": "2023-03-08T15:52:14Z",
        "summary": "Large Language Models (LLMs) have sparked significant interest in their\ngenerative capabilities, leading to the development of various commercial\napplications. The high cost of using the models drives application builders to\nmaximize the value of generation under a limited inference budget. This paper\npresents a study of optimizing inference hyperparameters such as the number of\nresponses, temperature and max tokens, which significantly affects the\nutility/cost of text generation. We design a framework named EcoOptiGen which\nleverages economical hyperparameter optimization and cost-based pruning.\nExperiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its\neffectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML\nlibrary: \\url{https://aka.ms/autogen}.",
        "pdf_link": "https://arxiv.org/pdf/2303.04673v2.pdf"
    },
    {
        "title": "MenuCraft: Interactive Menu System Design with Large Language Models",
        "authors": [
            "Amir Hossein Kargaran",
            "Nafiseh Nikeghbal",
            "Abbas Heydarnoori",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-03-08T10:39:38Z",
        "summary": "Menu system design is a challenging task involving many design options and\nvarious human factors. For example, one crucial factor that designers need to\nconsider is the semantic and systematic relation of menu commands. However,\ncapturing these relations can be challenging due to limited available\nresources. With the advancement of neural language models, large language\nmodels can utilize their vast pre-existing knowledge in designing and refining\nmenu systems. In this paper, we propose MenuCraft, an AI-assisted designer for\nmenu design that enables collaboration between the designer and a dialogue\nsystem to design menus. MenuCraft offers an interactive language-based menu\ndesign tool that simplifies the menu design process and enables easy\ncustomization of design options. MenuCraft supports a variety of interactions\nthrough dialog that allows performing zero/few-shot learning.",
        "pdf_link": "https://arxiv.org/pdf/2303.04496v2.pdf"
    },
    {
        "title": "Automatically Auditing Large Language Models via Discrete Optimization",
        "authors": [
            "Erik Jones",
            "Anca Dragan",
            "Aditi Raghunathan",
            "Jacob Steinhardt"
        ],
        "published": "2023-03-08T05:09:59Z",
        "summary": "Auditing large language models for unexpected behaviors is critical to\npreempt catastrophic deployments, yet remains challenging. In this work, we\ncast auditing as an optimization problem, where we automatically search for\ninput-output pairs that match a desired target behavior. For example, we might\naim to find a non-toxic input that starts with \"Barack Obama\" that a model maps\nto a toxic output. This optimization problem is difficult to solve as the set\nof feasible points is sparse, the space is discrete, and the language models we\naudit are non-linear and high-dimensional. To combat these challenges, we\nintroduce a discrete optimization algorithm, ARCA, that jointly and efficiently\noptimizes over inputs and outputs. Our approach automatically uncovers\nderogatory completions about celebrities (e.g. \"Barack Obama is a legalized\nunborn\" -> \"child murderer\"), produces French inputs that complete to English\noutputs, and finds inputs that generate a specific name. Our work offers a\npromising new tool to uncover models' failure-modes before deployment.",
        "pdf_link": "https://arxiv.org/pdf/2303.04381v1.pdf"
    },
    {
        "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?",
        "authors": [
            "Ruixiang Tang",
            "Xiaotian Han",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-08T03:56:31Z",
        "summary": "Recent advancements in large language models (LLMs) have led to the\ndevelopment of highly potent models like OpenAI's ChatGPT. These models have\nexhibited exceptional performance in a variety of tasks, such as question\nanswering, essay composition, and code generation. However, their effectiveness\nin the healthcare sector remains uncertain. In this study, we seek to\ninvestigate the potential of ChatGPT to aid in clinical text mining by\nexamining its ability to extract structured information from unstructured\nhealthcare texts, with a focus on biological named entity recognition and\nrelation extraction. However, our preliminary results indicate that employing\nChatGPT directly for these tasks resulted in poor performance and raised\nprivacy concerns associated with uploading patients' information to the ChatGPT\nAPI. To overcome these limitations, we propose a new training paradigm that\ninvolves generating a vast quantity of high-quality synthetic data with labels\nutilizing ChatGPT and fine-tuning a local model for the downstream task. Our\nmethod has resulted in significant improvements in the performance of\ndownstream tasks, improving the F1-score from 23.37% to 63.99% for the named\nentity recognition task and from 75.86% to 83.59% for the relation extraction\ntask. Furthermore, generating data using ChatGPT can significantly reduce the\ntime and effort required for data collection and labeling, as well as mitigate\ndata privacy concerns. In summary, the proposed framework presents a promising\nsolution to enhance the applicability of LLM models to clinical text mining.",
        "pdf_link": "https://arxiv.org/pdf/2303.04360v2.pdf"
    },
    {
        "title": "Can large language models build causal graphs?",
        "authors": [
            "Stephanie Long",
            "Tibor Schuster",
            "Alexandre Pich\u00e9"
        ],
        "published": "2023-03-07T22:05:31Z",
        "summary": "Building causal graphs can be a laborious process. To ensure all relevant\ncausal pathways have been captured, researchers often have to discuss with\nclinicians and experts while also reviewing extensive relevant medical\nliterature. By encoding common and medical knowledge, large language models\n(LLMs) represent an opportunity to ease this process by automatically scoring\nedges (i.e., connections between two variables) in potential graphs. LLMs\nhowever have been shown to be brittle to the choice of probing words, context,\nand prompts that the user employs. In this work, we evaluate if LLMs can be a\nuseful tool in complementing causal graph development.",
        "pdf_link": "https://arxiv.org/pdf/2303.05279v2.pdf"
    },
    {
        "title": "Gradient-Free Structured Pruning with Unlabeled Data",
        "authors": [
            "Azade Nova",
            "Hanjun Dai",
            "Dale Schuurmans"
        ],
        "published": "2023-03-07T19:12:31Z",
        "summary": "Large Language Models (LLMs) have achieved great success in solving difficult\ntasks across many domains, but such success comes with a high computation cost,\nand inference latency. As developers and third parties customize these models,\nthe need to provide efficient inference has increased. Many efforts have\nattempted to reduce inference cost through model compression techniques such as\npruning and distillation. However, these techniques either require labeled\ndata, or are time-consuming as they require the compressed model to be\nretrained to regain accuracy. In this paper, we propose a gradient-free\nstructured pruning framework that uses only unlabeled data. An evaluation on\nthe GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates\nthe effectiveness of the proposed approach. By only using the weights of the\npre-trained model and unlabeled data, in a matter of a few minutes on a single\nGPU, up to 40% of the original FLOP count can be reduced with less than a 4%\naccuracy loss across all tasks considered.",
        "pdf_link": "https://arxiv.org/pdf/2303.04185v2.pdf"
    },
    {
        "title": "From Copilot to Pilot: Towards AI Supported Software Development",
        "authors": [
            "Rohith Pudari",
            "Neil A. Ernst"
        ],
        "published": "2023-03-07T18:56:52Z",
        "summary": "AI-supported programming has arrived, as shown by the introduction and\nsuccesses of large language models for code, such as Copilot/Codex\n(Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on\nprogramming challenges is now possible. However, software engineering is much\nmore than solving programming contests. Moving beyond code completion to\nAI-supported software engineering will require an AI system that can, among\nother things, understand how to avoid code smells, to follow language idioms,\nand eventually (maybe!) propose rational software designs. In this study, we\nexplore the current limitations of AI-supported code completion tools like\nCopilot and offer a simple taxonomy for understanding the classification of\nAI-supported code completion tools in this space. We first perform an\nexploratory study on Copilot's code suggestions for language idioms and code\nsmells. Copilot does not follow language idioms and avoid code smells in most\nof our test scenarios. We then conduct additional investigation to determine\nthe current boundaries of AI-supported code completion tools like Copilot by\nintroducing a taxonomy of software abstraction hierarchies where 'basic\nprogramming functionality' such as code compilation and syntax checking is at\nthe least abstract level, software architecture analysis and design are at the\nmost abstract level. We conclude by providing a discussion on challenges for\nfuture development of AI-supported code completion tools to reach the design\nlevel of abstraction in our taxonomy.",
        "pdf_link": "https://arxiv.org/pdf/2303.04142v1.pdf"
    },
    {
        "title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
        "authors": [
            "Maciej P. Polak",
            "Dane Morgan"
        ],
        "published": "2023-03-07T17:54:53Z",
        "summary": "There has been a growing effort to replace manual extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing, language models, and recently, large language models (LLMs).\nAlthough these methods enable efficient extraction of data from large sets of\nresearch papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with minimal initial effort and\nbackground, using an advanced conversational LLM. ChatExtract consists of a set\nof engineered prompts applied to a conversational LLM that both identify\nsentences with data, extract that data, and assure the data's correctness\nthrough a series of follow-up questions. These follow-up questions largely\novercome known issues with LLMs providing factually inaccurate responses.\nChatExtract can be applied with any conversational LLMs and yields very high\nquality data extraction. In tests on materials data we find precision and\nrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. We\ndemonstrate that the exceptional performance is enabled by the information\nretention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that\napproaches similar to ChatExtract, due to their simplicity, transferability,\nand accuracy are likely to become powerful tools for data extraction in the\nnear future. Finally, databases for critical cooling rates of metallic glasses\nand yield strengths of high entropy alloys are developed using ChatExtract.",
        "pdf_link": "https://arxiv.org/pdf/2303.05352v3.pdf"
    },
    {
        "title": "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification",
        "authors": [
            "Taja Kuzman",
            "Igor Mozeti\u010d",
            "Nikola Ljube\u0161i\u0107"
        ],
        "published": "2023-03-07T14:59:33Z",
        "summary": "ChatGPT has shown strong capabilities in natural language generation tasks,\nwhich naturally leads researchers to explore where its abilities end. In this\npaper, we examine whether ChatGPT can be used for zero-shot text\nclassification, more specifically, automatic genre identification. We compare\nChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on\ndatasets, manually annotated with genres. The models are compared on test sets\nin two languages: English and Slovenian. Results show that ChatGPT outperforms\nthe fine-tuned model when applied to the dataset which was not seen before by\neither of the models. Even when applied on Slovenian language as an\nunder-resourced language, ChatGPT's performance is no worse than when applied\nto English. However, if the model is fully prompted in Slovenian, the\nperformance drops significantly, showing the current limitations of ChatGPT\nusage on smaller languages. The presented results lead us to questioning\nwhether this is the beginning of an end of laborious manual annotation\ncampaigns even for smaller languages, such as Slovenian.",
        "pdf_link": "https://arxiv.org/pdf/2303.03953v2.pdf"
    },
    {
        "title": "Larger language models do in-context learning differently",
        "authors": [
            "Jerry Wei",
            "Jason Wei",
            "Yi Tay",
            "Dustin Tran",
            "Albert Webson",
            "Yifeng Lu",
            "Xinyun Chen",
            "Hanxiao Liu",
            "Da Huang",
            "Denny Zhou",
            "Tengyu Ma"
        ],
        "published": "2023-03-07T12:24:17Z",
        "summary": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.",
        "pdf_link": "https://arxiv.org/pdf/2303.03846v2.pdf"
    },
    {
        "title": "Exploring the Feasibility of ChatGPT for Event Extraction",
        "authors": [
            "Jun Gao",
            "Huan Zhao",
            "Changlong Yu",
            "Ruifeng Xu"
        ],
        "published": "2023-03-07T12:03:58Z",
        "summary": "Event extraction is a fundamental task in natural language processing that\ninvolves identifying and extracting information about events mentioned in text.\nHowever, it is a challenging task due to the lack of annotated data, which is\nexpensive and time-consuming to obtain. The emergence of large language models\n(LLMs) such as ChatGPT provides an opportunity to solve language tasks with\nsimple prompts without the need for task-specific datasets and fine-tuning.\nWhile ChatGPT has demonstrated impressive results in tasks like machine\ntranslation, text summarization, and question answering, it presents challenges\nwhen used for complex tasks like event extraction. Unlike other tasks, event\nextraction requires the model to be provided with a complex set of instructions\ndefining all event types and their schemas. To explore the feasibility of\nChatGPT for event extraction and the challenges it poses, we conducted a series\nof experiments. Our results show that ChatGPT has, on average, only 51.04% of\nthe performance of a task-specific model such as EEQA in long-tail and complex\nscenarios. Our usability testing experiments indicate that ChatGPT is not\nrobust enough, and continuous refinement of the prompt does not lead to stable\nperformance improvements, which can result in a poor user experience. Besides,\nChatGPT is highly sensitive to different prompt styles.",
        "pdf_link": "https://arxiv.org/pdf/2303.03836v2.pdf"
    },
    {
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
        "authors": [
            "Zhiwei Tang",
            "Dmitry Rybin",
            "Tsung-Hui Chang"
        ],
        "published": "2023-03-07T09:20:43Z",
        "summary": "In this study, we delve into an emerging optimization challenge involving a\nblack-box objective function that can only be gauged via a ranking oracle-a\nsituation frequently encountered in real-world scenarios, especially when the\nfunction is evaluated by human judges. Such challenge is inspired from\nReinforcement Learning with Human Feedback (RLHF), an approach recently\nemployed to enhance the performance of Large Language Models (LLMs) using human\nguidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization\nalgorithm designed to tackle this optimization problem, accompanied by\ntheoretical assurances. Our algorithm utilizes a novel rank-based random\nestimator to determine the descent direction and guarantees convergence to a\nstationary point. Moreover, ZO-RankSGD is readily applicable to policy\noptimization problems in Reinforcement Learning (RL), particularly when only\nranking oracles for the episode reward are available. Last but not least, we\ndemonstrate the effectiveness of ZO-RankSGD in a novel application: improving\nthe quality of images generated by a diffusion generative model with human\nranking feedback. Throughout experiments, we found that ZO-RankSGD can\nsignificantly enhance the detail of generated images with only a few rounds of\nhuman feedback. Overall, our work advances the field of zeroth-order\noptimization by addressing the problem of optimizing functions with only\nranking feedback, and offers a new and effective approach for aligning\nArtificial Intelligence (AI) with human intentions.",
        "pdf_link": "https://arxiv.org/pdf/2303.03751v2.pdf"
    },
    {
        "title": "Stylometric Detection of AI-Generated Text in Twitter Timelines",
        "authors": [
            "Tharindu Kumarage",
            "Joshua Garland",
            "Amrita Bhattacharjee",
            "Kirill Trapeznikov",
            "Scott Ruston",
            "Huan Liu"
        ],
        "published": "2023-03-07T07:26:09Z",
        "summary": "Recent advancements in pre-trained language models have enabled convenient\nmethods for generating human-like text at a large scale. Though these\ngeneration capabilities hold great potential for breakthrough applications, it\ncan also be a tool for an adversary to generate misinformation. In particular,\nsocial media platforms like Twitter are highly susceptible to AI-generated\nmisinformation. A potential threat scenario is when an adversary hijacks a\ncredible user account and incorporates a natural language generator to generate\nmisinformation. Such threats necessitate automated detectors for AI-generated\ntweets in a given user's Twitter timeline. However, tweets are inherently\nshort, thus making it difficult for current state-of-the-art pre-trained\nlanguage model-based detectors to accurately detect at what point the AI starts\nto generate tweets in a given Twitter timeline. In this paper, we present a\nnovel algorithm using stylometric signals to aid detecting AI-generated tweets.\nWe propose models corresponding to quantifying stylistic changes in human and\nAI tweets in two related tasks: Task 1 - discriminate between human and\nAI-generated tweets, and Task 2 - detect if and when an AI starts to generate\ntweets in a given Twitter timeline. Our extensive experiments demonstrate that\nthe stylometric features are effective in augmenting the state-of-the-art\nAI-generated text detectors.",
        "pdf_link": "https://arxiv.org/pdf/2303.03697v1.pdf"
    },
    {
        "title": "CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification",
        "authors": [
            "Seungone Kim",
            "Se June Joo",
            "Yul Jang",
            "Hyungjoo Chae",
            "Jinyoung Yeo"
        ],
        "published": "2023-03-07T03:23:14Z",
        "summary": "Chain-of-thought (CoT) prompting enables large language models (LLMs) to\nsolve complex reasoning tasks by generating an explanation before the final\nprediction. Despite it's promising ability, a critical downside of CoT\nprompting is that the performance is greatly affected by the factuality of the\ngenerated explanation. To improve the correctness of the explanations,\nfine-tuning language models with explanation data is needed. However, there\nexists only a few datasets that can be used for such approaches, and no data\ncollection tool for building them. Thus, we introduce CoTEVer, a tool-kit for\nannotating the factual correctness of generated explanations and collecting\nrevision data of wrong explanations. Furthermore, we suggest several use cases\nwhere the data collected with CoTEVer can be utilized for enhancing the\nfaithfulness of explanations. Our toolkit is publicly available at\nhttps://github.com/SeungoneKim/CoTEVer.",
        "pdf_link": "https://arxiv.org/pdf/2303.03628v1.pdf"
    },
    {
        "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
        "authors": [
            "Bowen Zhang",
            "Harold Soh"
        ],
        "published": "2023-03-06T23:16:24Z",
        "summary": "Human models play a crucial role in human-robot interaction (HRI), enabling\nrobots to consider the impact of their actions on people and plan their\nbehavior accordingly. However, crafting good human models is challenging;\ncapturing context-dependent human behavior requires significant prior knowledge\nand/or large amounts of interaction data, both of which are difficult to\nobtain. In this work, we explore the potential of large-language models (LLMs)\n-- which have consumed vast amounts of human-generated text data -- to act as\nzero-shot human models for HRI. Our experiments on three social datasets yield\npromising results; the LLMs are able to achieve performance comparable to\npurpose-built models. That said, we also discuss current limitations, such as\nsensitivity to prompts and spatial/numerical reasoning mishaps. Based on our\nfindings, we demonstrate how LLM-based human models can be integrated into a\nsocial robot's planning process and applied in HRI scenarios. Specifically, we\npresent one case study on a simulated trust-based table-clearing task and\nreplicate past results that relied on custom models. Next, we conduct a new\nrobot utensil-passing experiment (n = 65) where preliminary results show that\nplanning with a LLM-based human model can achieve gains over a basic myopic\nplan. In summary, our results show that LLMs offer a promising (but incomplete)\napproach to human modeling for HRI.",
        "pdf_link": "https://arxiv.org/pdf/2303.03548v1.pdf"
    },
    {
        "title": "Can an Embodied Agent Find Your \"Cat-shaped Mug\"? LLM-Guided Exploration for Zero-Shot Object Navigation",
        "authors": [
            "Vishnu Sashank Dorbala",
            "James F. Mullen Jr.",
            "Dinesh Manocha"
        ],
        "published": "2023-03-06T20:19:19Z",
        "summary": "We present LGX (Language-guided Exploration), a novel algorithm for\nLanguage-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied\nagent navigates to a uniquely described target object in a previously unseen\nenvironment. Our approach makes use of Large Language Models (LLMs) for this\ntask by leveraging the LLM's commonsense reasoning capabilities for making\nsequential navigational decisions. Simultaneously, we perform generalized\ntarget object detection using a pre-trained Vision-Language grounding model. We\nachieve state-of-the-art zero-shot object navigation results on RoboTHOR with a\nsuccess rate (SR) improvement of over 27% over the current baseline of the\nOWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for\nrobot navigation and present an analysis of various prompting strategies\naffecting the model output. Finally, we showcase the benefits of our approach\nvia \\textit{real-world} experiments that indicate the superior performance of\nLGX in detecting and navigating to visually unique objects.",
        "pdf_link": "https://arxiv.org/pdf/2303.03480v2.pdf"
    },
    {
        "title": "Spelling convention sensitivity in neural language models",
        "authors": [
            "Elizabeth Nielsen",
            "Christo Kirov",
            "Brian Roark"
        ],
        "published": "2023-03-06T19:29:20Z",
        "summary": "We examine whether large neural language models, trained on very large\ncollections of varied English text, learn the potentially long-distance\ndependency of British versus American spelling conventions, i.e., whether\nspelling is consistently one or the other within model-generated strings. In\ncontrast to long-distance dependencies in non-surface underlying structure\n(e.g., syntax), spelling consistency is easier to measure both in LMs and the\ntext corpora used to train them, which can provide additional insight into\ncertain observed model behaviors. Using a set of probe words unique to either\nBritish or American English, we first establish that training corpora exhibit\nsubstantial (though not total) consistency. A large T5 language model does\nappear to internalize this consistency, though only with respect to observed\nlexical items (not nonce words with British/American spelling patterns). We\nfurther experiment with correcting for biases in the training data by\nfine-tuning T5 on synthetic data that has been debiased, and find that\nfinetuned T5 remains only somewhat sensitive to spelling consistency. Further\nexperiments show GPT2 to be similarly limited.",
        "pdf_link": "https://arxiv.org/pdf/2303.03457v1.pdf"
    },
    {
        "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
        "authors": [
            "Hai Dang",
            "Sven Goller",
            "Florian Lehmann",
            "Daniel Buschek"
        ],
        "published": "2023-03-06T14:58:42Z",
        "summary": "We propose a conceptual perspective on prompts for Large Language Models\n(LLMs) that distinguishes between (1) diegetic prompts (part of the narrative,\ne.g. \"Once upon a time, I saw a fox...\"), and (2) non-diegetic prompts\n(external, e.g. \"Write about the adventures of the fox.\"). With this lens, we\nstudy how 129 crowd workers on Prolific write short texts with different user\ninterfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with\nGPT-3): When the interface offered multiple suggestions and provided an option\nfor non-diegetic prompting, participants preferred choosing from multiple\nsuggestions over controlling them via non-diegetic prompts. When participants\nprovided non-diegetic prompts it was to ask for inspiration, topics or facts.\nSingle suggestions in particular were guided both with diegetic and\nnon-diegetic information. This work informs human-AI interaction with\ngenerative models by revealing that (1) writing non-diegetic prompts requires\neffort, (2) people combine diegetic and non-diegetic prompting, and (3) they\nuse their draft (i.e. diegetic information) and suggestion timing to\nstrategically guide LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.03199v1.pdf"
    },
    {
        "title": "Towards Zero-Shot Functional Compositionality of Language Models",
        "authors": [
            "Hangyeol Yu",
            "Myeongho Jeong",
            "Jamin Shin",
            "Hyeongdon Moon",
            "Juneyoung Park",
            "Seungtaek Choi"
        ],
        "published": "2023-03-06T13:15:25Z",
        "summary": "Large Pre-trained Language Models (PLM) have become the most desirable\nstarting point in the field of NLP, as they have become remarkably good at\nsolving many individual tasks. Despite such success, in this paper, we argue\nthat current paradigms of working with PLMs are neglecting a critical aspect of\nmodeling human intelligence: functional compositionality. Functional\ncompositionality - the ability to compose learned tasks - has been a\nlong-standing challenge in the field of AI (and many other fields) as it is\nconsidered one of the hallmarks of human intelligence. An illustrative example\nof such is cross-lingual summarization, where a bilingual person\n(English-French) could directly summarize an English document into French\nsentences without having to translate the English document or summary into\nFrench explicitly. We discuss why this matter is an important open problem that\nrequires further attention from the field. Then, we show that current PLMs\n(e.g., GPT-2 and T5) don't have functional compositionality yet and it is far\nfrom human-level generalizability. Finally, we suggest several research\ndirections that could push the field towards zero-shot functional\ncompositionality of language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.03103v1.pdf"
    },
    {
        "title": "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training",
        "authors": [
            "Wei Li",
            "Linchao Zhu",
            "Longyin Wen",
            "Yi Yang"
        ],
        "published": "2023-03-06T11:02:47Z",
        "summary": "Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong\nzero-shot transfer capability in many discriminative tasks. Their adaptation to\nzero-shot image-conditioned text generation tasks has drawn increasing\ninterest. Prior arts approach to zero-shot captioning by either utilizing the\nexisting large language models (e.g., GPT-2) or pre-training the\nencoder-decoder network in an end-to-end manner. In this work, we propose a\nsimple framework, named DeCap, for zero-shot captioning. We introduce a\nlightweight visual-aware language decoder. This decoder is both data-efficient\nand computation-efficient: 1) it only requires the text data for training,\neasing the burden on the collection of paired data. 2) it does not require\nend-to-end training. When trained with text-only data, the decoder takes the\ntext embedding extracted from the off-the-shelf CLIP encoder as a prefix\nembedding. The challenge is that the decoder is trained on the text corpus but\nat the inference stage, it needs to generate captions based on visual inputs.\nThe modality gap issue is widely observed in multi-modal contrastive models\nthat prevents us from directly taking the visual embedding as the prefix\nembedding. We propose a training-free mechanism to reduce the modality gap. We\nproject the visual embedding into the CLIP text embedding space, while the\nprojected embedding retains the information of the visual input. Taking the\nprojected embedding as the prefix embedding, the decoder generates high-quality\ndescriptions that match the visual input. The experiments show that DeCap\noutperforms other zero-shot captioning methods and unpaired captioning methods\non the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",
        "pdf_link": "https://arxiv.org/pdf/2303.03032v1.pdf"
    },
    {
        "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
        "authors": [
            "Mohammad Abdullah Matin Khan",
            "M Saiful Bari",
            "Xuan Long Do",
            "Weishi Wang",
            "Md Rizwan Parvez",
            "Shafiq Joty"
        ],
        "published": "2023-03-06T10:08:51Z",
        "summary": "Recently, pre-trained large language models (LLMs) have shown impressive\nabilities in generating codes from natural language descriptions, repairing\nbuggy codes, translating codes between languages, and retrieving relevant code\nsegments. However, the evaluation of these models has often been performed in a\nscattered way on only one or two specific tasks, in a few languages, at a\npartial granularity (e.g., function) level, and in many cases without proper\ntraining data. Even more concerning is that in most cases the evaluation of\ngenerated codes has been done in terms of mere lexical overlap with a reference\ncode rather than actual execution. We introduce xCodeEval, the largest\nexecutable multilingual multitask benchmark to date consisting of $25$M\ndocument-level coding examples ($16.5$B tokens) from about $7.5$K unique\nproblems covering up to $11$ programming languages with execution-level\nparallelism. It features a total of $7$ tasks involving code understanding,\ngeneration, translation and retrieval. xCodeEval adopts an execution-based\nevaluation and offers a multilingual code execution engine, ExecEval that\nsupports unit test based execution in all the $11$ languages. To address the\nchallenge of balancing the distributions of text-code samples over multiple\nattributes in validation/test sets, we propose a novel data splitting and a\ndata selection schema based on the geometric mean and graph-theoretic\nprinciple. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs\n(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**\nto be quite challenging as per the current advancements in language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.03004v4.pdf"
    },
    {
        "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
        "authors": [
            "Victor Dibia"
        ],
        "published": "2023-03-06T06:47:22Z",
        "summary": "Systems that support users in the automatic creation of visualizations must\naddress several subtasks - understand the semantics of data, enumerate relevant\nvisualization goals and generate visualization specifications. In this work, we\npose visualization generation as a multi-stage generation problem and argue\nthat well-orchestrated pipelines based on large language models (LLMs) such as\nChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing\nthese tasks. We present LIDA, a novel tool for generating grammar-agnostic\nvisualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER\nthat converts data into a rich but compact natural language summary, a GOAL\nEXPLORER that enumerates visualization goals given the data, a VISGENERATOR\nthat generates, refines, executes and filters visualization code and an\nINFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA\nprovides a python api, and a hybrid user interface (direct manipulation and\nmultilingual natural language) for interactive chart, infographics and data\nstory generation. Learn more about the project here -\nhttps://microsoft.github.io/lida/",
        "pdf_link": "https://arxiv.org/pdf/2303.02927v3.pdf"
    },
    {
        "title": "OpenICL: An Open-Source Framework for In-context Learning",
        "authors": [
            "Zhenyu Wu",
            "YaoXiang Wang",
            "Jiacheng Ye",
            "Jiangtao Feng",
            "Jingjing Xu",
            "Yu Qiao",
            "Zhiyong Wu"
        ],
        "published": "2023-03-06T06:20:25Z",
        "summary": "In recent years, In-context Learning (ICL) has gained increasing attention\nand emerged as the new paradigm for large language model (LLM) evaluation.\nUnlike traditional fine-tuning methods, ICL instead adapts the pre-trained\nmodels to unseen tasks without any parameter updates. However, the\nimplementation of ICL is sophisticated due to the diverse retrieval and\ninference methods involved, as well as the varying pre-processing requirements\nfor different models, datasets, and tasks. A unified and flexible framework for\nICL is urgently needed to ease the implementation of the aforementioned\ncomponents. To facilitate ICL research, we introduce OpenICL, an open-source\ntoolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly\nflexible architecture that users can easily combine different components to\nsuit their needs. It also provides various state-of-the-art retrieval and\ninference methods to streamline the process of adapting ICL to cutting-edge\nresearch. The effectiveness of OpenICL has been validated on a wide range of\nNLP tasks, including classification, QA, machine translation, and semantic\nparsing. As a side-product, we found OpenICL to be an efficient yet robust tool\nfor LLMs evaluation. OpenICL is released at\nhttps://github.com/Shark-NLP/OpenICL",
        "pdf_link": "https://arxiv.org/pdf/2303.02913v1.pdf"
    },
    {
        "title": "Could a Large Language Model be Conscious?",
        "authors": [
            "David J. Chalmers"
        ],
        "published": "2023-03-04T19:14:20Z",
        "summary": "There has recently been widespread discussion of whether large language\nmodels might be sentient or conscious. Should we take this idea seriously? I\nwill break down the strongest reasons for and against. Given mainstream\nassumptions in the science of consciousness, there are significant obstacles to\nconsciousness in current models: for example, their lack of recurrent\nprocessing, a global workspace, and unified agency. At the same time, it is\nquite possible that these obstacles will be overcome in the next decade or so.\nI conclude that while it is somewhat unlikely that current large language\nmodels are conscious, we should take seriously the possibility that successors\nto large language models may be conscious in the not-too-distant future.",
        "pdf_link": "https://arxiv.org/pdf/2303.07103v2.pdf"
    },
    {
        "title": "The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges",
        "authors": [
            "Maria Lymperaiou",
            "Giorgos Stamou"
        ],
        "published": "2023-03-04T13:12:18Z",
        "summary": "Recent advancements in visiolinguistic (VL) learning have allowed the\ndevelopment of multiple models and techniques that offer several impressive\nimplementations, able to currently resolve a variety of tasks that require the\ncollaboration of vision and language. Current datasets used for VL pre-training\nonly contain a limited amount of visual and linguistic knowledge, thus\nsignificantly limiting the generalization capabilities of many VL models.\nExternal knowledge sources such as knowledge graphs (KGs) and Large Language\nModels (LLMs) are able to cover such generalization gaps by filling in missing\nknowledge, resulting in the emergence of hybrid architectures. In the current\nsurvey, we analyze tasks that have benefited from such hybrid approaches.\nMoreover, we categorize existing knowledge sources and types, proceeding to\ndiscussion regarding the KG vs LLM dilemma and its potential impact to future\nhybrid approaches.",
        "pdf_link": "https://arxiv.org/pdf/2303.02411v1.pdf"
    },
    {
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
        "authors": [
            "Shima Imani",
            "Liang Du",
            "Harsh Shrivastava"
        ],
        "published": "2023-03-04T04:43:49Z",
        "summary": "Large Language Models (LLMs) have limited performance when solving arithmetic\nreasoning tasks and often provide incorrect answers. Unlike natural language\nunderstanding, math problems typically have a single correct answer, making the\ntask of generating accurate solutions more challenging for LLMs. To the best of\nour knowledge, we are not aware of any LLMs that indicate their level of\nconfidence in their responses which fuels a trust deficit in these models\nimpeding their adoption. To address this deficiency, we propose `MathPrompter',\na technique that improves performance of LLMs on arithmetic problems along with\nincreased reliance in the predictions. MathPrompter uses the Zero-shot\nchain-of-thought prompting technique to generate multiple Algebraic expressions\nor Python functions to solve the same math problem in different ways and\nthereby raise the confidence level in the output results. This is in contrast\nto other prompt based CoT methods, where there is no check on the validity of\nthe intermediate steps followed. Our technique improves over state-of-the-art\non the MultiArith dataset ($78.7\\%\\rightarrow92.5\\%$) evaluated using 175B\nparameter GPT-based LLM.",
        "pdf_link": "https://arxiv.org/pdf/2303.05398v1.pdf"
    },
    {
        "title": "TrojText: Test-time Invisible Textual Trojan Insertion",
        "authors": [
            "Qian Lou",
            "Yepeng Liu",
            "Bo Feng"
        ],
        "published": "2023-03-03T22:19:22Z",
        "summary": "In Natural Language Processing (NLP), intelligent neuron models can be\nsusceptible to textual Trojan attacks. Such attacks occur when Trojan models\nbehave normally for standard inputs but generate malicious output for inputs\nthat contain a specific trigger. Syntactic-structure triggers, which are\ninvisible, are becoming more popular for Trojan attacks because they are\ndifficult to detect and defend against. However, these types of attacks require\na large corpus of training data to generate poisoned samples with the necessary\nsyntactic structures for Trojan insertion. Obtaining such data can be difficult\nfor attackers, and the process of generating syntactic poisoned triggers and\ninserting Trojans can be time-consuming. This paper proposes a solution called\nTrojText, which aims to determine whether invisible textual Trojan attacks can\nbe performed more efficiently and cost-effectively without training data. The\nproposed approach, called the Representation-Logit Trojan Insertion (RLI)\nalgorithm, uses smaller sampled test data instead of large training data to\nachieve the desired attack. The paper also introduces two additional\ntechniques, namely the accumulated gradient ranking (AGR) and Trojan Weights\nPruning (TWP), to reduce the number of tuned parameters and the attack\noverhead. The TrojText approach was evaluated on three datasets (AG's News,\nSST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The\nexperiments demonstrated that the TrojText approach achieved a 98.35\\%\nclassification accuracy for test sentences in the target class on the BERT\nmodel for the AG's News dataset. The source code for TrojText is available at\nhttps://github.com/UCF-ML-Research/TrojText.",
        "pdf_link": "https://arxiv.org/pdf/2303.02242v2.pdf"
    },
    {
        "title": "Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models",
        "authors": [
            "Navid Madani",
            "Rohini K. Srihari",
            "Kenneth Joseph"
        ],
        "published": "2023-03-03T20:35:38Z",
        "summary": "Answering questions over domain-specific graphs requires a tailored approach\ndue to the limited number of relations and the specific nature of the domain.\nOur approach integrates classic logical programming languages into large\nlanguage models (LLMs), enabling the utilization of logical reasoning\ncapabilities to tackle the KGQA task. By representing the questions as Prolog\nqueries, which are readable and near close to natural language in\nrepresentation, we facilitate the generation of programmatically derived\nanswers. To validate the effectiveness of our approach, we evaluate it using a\nwell-known benchmark dataset, MetaQA. Our experimental results demonstrate that\nour method achieves accurate identification of correct answer entities for all\ntest questions, even when trained on a small fraction of annotated data.\nOverall, our work presents a promising approach to addressing question\nanswering over domain-specific graphs, offering an explainable and robust\nsolution by incorporating logical programming languages.",
        "pdf_link": "https://arxiv.org/pdf/2303.02206v2.pdf"
    },
    {
        "title": "Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM",
        "authors": [
            "Rachel Bawden",
            "Fran\u00e7ois Yvon"
        ],
        "published": "2023-03-03T13:23:42Z",
        "summary": "The NLP community recently saw the release of a new large open-access\nmultilingual language model, BLOOM (BigScience et al., 2022) covering 46\nlanguages. We focus on BLOOM's multilingual ability by evaluating its machine\ntranslation performance across several datasets (WMT, Flores-101 and DiaBLa)\nand language pairs (high- and low-resourced). Our results show that 0-shot\nperformance suffers from overgeneration and generating in the wrong language,\nbut this is greatly improved in the few-shot setting, with very good results\nfor a number of language pairs. We study several aspects including prompt\ndesign, model sizes, cross-lingual transfer and the use of discursive context.",
        "pdf_link": "https://arxiv.org/pdf/2303.01911v2.pdf"
    },
    {
        "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
        "authors": [
            "Zhou Yu",
            "Xuecheng Ouyang",
            "Zhenwei Shao",
            "Meng Wang",
            "Jun Yu"
        ],
        "published": "2023-03-03T13:05:15Z",
        "summary": "Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the blind LLM as the\nprovided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. Finally, the\ntwo types of answer heuristics are jointly encoded into a formatted prompt to\nfacilitate the LLM's understanding of both the image and question, thus\ngenerating a more accurate answer. By incorporating the state-of-the-art LLM\nGPT-3, Prophet significantly outperforms existing state-of-the-art methods on\nfour challenging knowledge-based VQA datasets. To demonstrate the generality of\nour approach, we instantiate Prophet with the combinations of different VQA\nmodels (i.e., both discriminative and generative ones) and different LLMs\n(i.e., both commercial and open-source ones).",
        "pdf_link": "https://arxiv.org/pdf/2303.01903v3.pdf"
    },
    {
        "title": "Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Lidong Bing"
        ],
        "published": "2023-03-03T04:27:44Z",
        "summary": "Although large conversational AI models such as OpenAI's ChatGPT have\ndemonstrated great potential, we question whether such models can guarantee\nfactual accuracy. Recently, technology companies such as Microsoft and Google\nhave announced new services which aim to combine search engines with\nconversational AI. However, we have found numerous mistakes in the public\ndemonstrations that suggest we should not easily trust the factual claims of\nthe AI models. Rather than criticizing specific models or companies, we hope to\ncall on researchers and developers to improve AI models' transparency and\nfactual correctness.",
        "pdf_link": "https://arxiv.org/pdf/2304.11076v1.pdf"
    },
    {
        "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers",
        "authors": [
            "Tianlong Chen",
            "Zhenyu Zhang",
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Zhangyang Wang"
        ],
        "published": "2023-03-02T22:12:51Z",
        "summary": "Despite their remarkable achievement, gigantic transformers encounter\nsignificant drawbacks, including exorbitant computational and memory footprints\nduring training, as well as severe collapse evidenced by a high degree of\nparameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown\npromise to mitigate the issue of training efficiency, yet they are prone to (1)\nredundant experts due to representational collapse; and (2) poor expert\nscalability for inference and downstream fine-tuning, primarily due to\noverfitting of the learned routing policy to the number of activated experts\nduring training. As recent research efforts are predominantly focused on\nimproving routing policies to encourage expert specializations, this work\nfocuses on exploring the overlooked scalability bottleneck of SMoEs and\nleveraging it to effectively scale dense transformers. To this end, we propose\na new plug-and-play training framework, SMoE-Dropout, to enable scaling\ntransformers to better accuracy in their full capacity without collapse.\nSpecifically, SMoE-Dropout consists of a randomly initialized and fixed router\nnetwork to activate experts and gradually increases the activated expert number\nas training progresses over time. Transformers trained by SMoE-Dropout\nnaturally exhibit a self-slimmable property subject to resource availability,\noffering smooth and consistent performance boosts with an increase in activated\nexperts during inference or fine-tuning. Our extensive experiments demonstrate\nthe superior performance and substantial computation savings of SMoE-Dropout,\ncompared to dense training baselines with equivalent parameter counts. In\nparticular, our trained BERT outperforms its densely trained counterpart with\nconsistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks\n{ASDiv-A, MAWPS, SVAMP}, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2303.01610v1.pdf"
    },
    {
        "title": "Mixture of Soft Prompts for Controllable Data Generation",
        "authors": [
            "Derek Chen",
            "Celine Lee",
            "Yunan Lu",
            "Domenic Rosati",
            "Zhou Yu"
        ],
        "published": "2023-03-02T21:13:56Z",
        "summary": "Large language models (LLMs) effectively generate fluent text when the target\noutput follows natural language patterns. However, structured prediction tasks\nconfine the output format to a limited ontology, causing even very large models\nto struggle since they were never trained with such restrictions in mind. The\ndifficulty of using LLMs for direct prediction is exacerbated in few-shot\nlearning scenarios, which commonly arise due to domain shift and resource\nlimitations. We flip the problem on its head by leveraging the LLM as a tool\nfor data augmentation rather than direct prediction. Our proposed Mixture of\nSoft Prompts (MSP) serves as a parameter-efficient procedure for generating\ndata in a controlled manner. Denoising mechanisms are further applied to\nimprove the quality of synthesized data. Automatic metrics show our method is\ncapable of producing diverse and natural text, while preserving label\nsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarks\nwhen compared against strong baselines. Our method offers an alternate\ndata-centric approach for applying LLMs to complex prediction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.01580v2.pdf"
    },
    {
        "title": "WiCE: Real-World Entailment for Claims in Wikipedia",
        "authors": [
            "Ryo Kamoi",
            "Tanya Goyal",
            "Juan Diego Rodriguez",
            "Greg Durrett"
        ],
        "published": "2023-03-02T17:45:32Z",
        "summary": "Textual entailment models are increasingly applied in settings like\nfact-checking, presupposition verification in question answering, or summary\nevaluation. However, these represent a significant domain shift from existing\nentailment datasets, and models underperform as a result. We propose WiCE, a\nnew fine-grained textual entailment dataset built on natural claim and evidence\npairs extracted from Wikipedia. In addition to standard claim-level entailment,\nWiCE provides entailment judgments over sub-sentence units of the claim, and a\nminimal subset of evidence sentences that support each subclaim. To support\nthis, we propose an automatic claim decomposition strategy using GPT-3.5 which\nwe show is also effective at improving entailment models' performance on\nmultiple datasets at test time. Finally, we show that real claims in our\ndataset involve challenging verification and retrieval problems that existing\nmodels fail to address.",
        "pdf_link": "https://arxiv.org/pdf/2303.01432v2.pdf"
    },
    {
        "title": "Open-World Object Manipulation using Pre-trained Vision-Language Models",
        "authors": [
            "Austin Stone",
            "Ted Xiao",
            "Yao Lu",
            "Keerthana Gopalakrishnan",
            "Kuang-Huei Lee",
            "Quan Vuong",
            "Paul Wohlhart",
            "Sean Kirmani",
            "Brianna Zitkovich",
            "Fei Xia",
            "Chelsea Finn",
            "Karol Hausman"
        ],
        "published": "2023-03-02T01:55:10Z",
        "summary": "For robots to follow instructions from people, they must be able to connect\nthe rich semantic information in human vocabulary, e.g. \"can you get me the\npink stuffed whale?\" to their sensory observations and actions. This brings up\na notably difficult challenge for robots: while robot learning approaches allow\nrobots to learn many different behaviors from first-hand experience, it is\nimpractical for robots to have first-hand experiences that span all of this\nsemantic information. We would like a robot's policy to be able to perceive and\npick up the pink stuffed whale, even if it has never seen any data interacting\nwith a stuffed whale before. Fortunately, static data on the internet has vast\nsemantic information, and this information is captured in pre-trained\nvision-language models. In this paper, we study whether we can interface robot\npolicies with these pre-trained models, with the aim of allowing robots to\ncomplete instructions involving object categories that the robot has never seen\nfirst-hand. We develop a simple approach, which we call Manipulation of\nOpen-World Objects (MOO), which leverages a pre-trained vision-language model\nto extract object-identifying information from the language command and image,\nand conditions the robot policy on the current image, the instruction, and the\nextracted object information. In a variety of experiments on a real mobile\nmanipulator, we find that MOO generalizes zero-shot to a wide range of novel\nobject categories and environments. In addition, we show how MOO generalizes to\nother, non-language-based input modalities to specify the object of interest\nsuch as finger pointing, and how it can be further extended to enable\nopen-world navigation and manipulation. The project's website and evaluation\nvideos can be found at https://robot-moo.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2303.00905v2.pdf"
    },
    {
        "title": "Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents",
        "authors": [
            "Wenlong Huang",
            "Fei Xia",
            "Dhruv Shah",
            "Danny Driess",
            "Andy Zeng",
            "Yao Lu",
            "Pete Florence",
            "Igor Mordatch",
            "Sergey Levine",
            "Karol Hausman",
            "Brian Ichter"
        ],
        "published": "2023-03-01T22:58:50Z",
        "summary": "Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2303.00855v2.pdf"
    },
    {
        "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
        "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Keshav Santhanam",
            "Radu Florian",
            "Martin Franz",
            "Salim Roukos",
            "Avirup Sil",
            "Md Arafat Sultan",
            "Christopher Potts"
        ],
        "published": "2023-03-01T20:21:23Z",
        "summary": "Many information retrieval tasks require large labeled datasets for\nfine-tuning. However, such datasets are often unavailable, and their utility\nfor real-world applications can diminish quickly due to domain shifts. To\naddress this challenge, we develop and motivate a method for using large\nlanguage models (LLMs) to generate large numbers of synthetic queries cheaply.\nThe method begins by generating a small number of synthetic queries using an\nexpensive LLM. After that, a much less expensive one is used to create large\nnumbers of synthetic queries, which are used to fine-tune a family of reranker\nmodels. These rerankers are then distilled into a single efficient retriever\nfor use in the target domain. We show that this technique boosts zero-shot\naccuracy in long-tail domains and achieves substantially lower latency than\nstandard reranking methods.",
        "pdf_link": "https://arxiv.org/pdf/2303.00807v3.pdf"
    },
    {
        "title": "R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents",
        "authors": [
            "Daniel D. Johnson",
            "Daniel Tarlow",
            "Christian Walder"
        ],
        "published": "2023-03-01T18:46:40Z",
        "summary": "Large language models show impressive results at predicting structured text\nsuch as code, but also commonly introduce errors and hallucinations in their\noutput. When used to assist software developers, these models may make mistakes\nthat users must go back and fix, or worse, introduce subtle bugs that users may\nmiss entirely. We propose Randomized Utility-driven Synthesis of Uncertain\nREgions (R-U-SURE), an approach for building uncertainty-aware suggestions\nbased on a decision-theoretic model of goal-conditioned utility, using random\nsamples from a generative model as a proxy for the unobserved possible intents\nof the end user. Our technique combines minimum-Bayes-risk decoding, dual\ndecomposition, and decision diagrams in order to efficiently produce structured\nuncertainty summaries, given only sample access to an arbitrary generative\nmodel of code and an optional AST parser. We demonstrate R-U-SURE on three\ndeveloper-assistance tasks, and show that it can be applied different user\ninteraction patterns without retraining the model and leads to more accurate\nuncertainty estimates than token-probability baselines. We also release our\nimplementation as an open-source library at\nhttps://github.com/google-research/r_u_sure.",
        "pdf_link": "https://arxiv.org/pdf/2303.00732v2.pdf"
    },
    {
        "title": "Domain-adapted large language models for classifying nuclear medicine reports",
        "authors": [
            "Zachary Huemann",
            "Changhee Lee",
            "Junjie Hu",
            "Steve Y. Cho",
            "Tyler Bradshaw"
        ],
        "published": "2023-03-01T09:48:39Z",
        "summary": "With the growing use of transformer-based language models in medicine, it is\nunclear how well these models generalize to nuclear medicine which has\ndomain-specific vocabulary and unique reporting styles. In this study, we\nevaluated the value of domain adaptation in nuclear medicine by adapting\nlanguage models for the purpose of 5-point Deauville score prediction based on\nclinical 18F-fluorodeoxyglucose (FDG) PET/CT reports. We retrospectively\nretrieved 4542 text reports and 1664 images for FDG PET/CT lymphoma exams from\n2008-2018 in our clinical imaging database. Deauville scores were removed from\nthe reports and then the remaining text in the reports was used as the model\ninput. Multiple general-purpose transformer language models were used to\nclassify the reports into Deauville scores 1-5. We then adapted the models to\nthe nuclear medicine domain using masked language modeling and assessed its\nimpact on classification performance. The language models were compared against\nvision models, a multimodal vision language model, and a nuclear medicine\nphysician with seven-fold Monte Carlo cross validation, reported are the mean\nand standard deviations. Domain adaption improved all language models. For\nexample, BERT improved from 61.3% five-class accuracy to 65.7% following domain\nadaptation. The best performing model (domain-adapted RoBERTa) achieved a\nfive-class accuracy of 77.4%, which was better than the physician's performance\n(66%), the best vision model's performance (48.1), and was similar to the\nmultimodal model's performance (77.2). Domain adaptation improved the\nperformance of large language models in interpreting nuclear medicine text\nreports.",
        "pdf_link": "https://arxiv.org/pdf/2303.01258v1.pdf"
    },
    {
        "title": "Competence-Based Analysis of Language Models",
        "authors": [
            "Adam Davies",
            "Jize Jiang",
            "ChengXiang Zhai"
        ],
        "published": "2023-03-01T08:53:36Z",
        "summary": "Despite the recent success of large, pretrained neural language models (LLMs)\non a variety of prompting tasks, these models can be alarmingly brittle to\nsmall changes in inputs or application contexts. To better understand such\nbehavior and motivate the design of more robust LLMs, we provide a causal\nformulation of linguistic competence in the context of LLMs and propose a\ngeneral framework to study and measure LLM competence. Our framework, CALM\n(Competence-based Analysis of Language Models), establishes the first\nquantitative measure of LLM competence, which we study by damaging models'\ninternal representations of various linguistic properties in the course of\nperforming various tasks using causal probing and evaluating models' alignment\nunder these interventions with a given causal model. We also develop a novel\napproach for performing causal probing interventions using gradient-based\nadversarial attacks, which can target a broader range of properties and\nrepresentations than existing techniques. We carry out a case study of CALM\nusing these interventions to analyze BERT and RoBERTa's competence across a\nvariety of lexical inference tasks, showing that the CALM framework and\ncompetence metric can be valuable tools for explaining and predicting their\nbehavior across these tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.00333v3.pdf"
    },
    {
        "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks",
        "authors": [
            "Xuanting Chen",
            "Junjie Ye",
            "Can Zu",
            "Nuo Xu",
            "Rui Zheng",
            "Minlong Peng",
            "Jie Zhou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-03-01T07:39:01Z",
        "summary": "The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.",
        "pdf_link": "https://arxiv.org/pdf/2303.00293v1.pdf"
    },
    {
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
        "authors": [
            "Haocong Rao",
            "Cyril Leung",
            "Chunyan Miao"
        ],
        "published": "2023-03-01T06:16:14Z",
        "summary": "Large Language Models (LLMs) especially ChatGPT have produced impressive\nresults in various areas, but their potential human-like psychology is still\nlargely unexplored. Existing works study the virtual personalities of LLMs but\nrarely explore the possibility of analyzing human personalities via LLMs. This\npaper presents a generic evaluation framework for LLMs to assess human\npersonalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,\nwe first devise unbiased prompts by randomly permuting options in MBTI\nquestions and adopt the average testing result to encourage more impartial\nanswer generation. Then, we propose to replace the subject in question\nstatements to enable flexible queries and assessments on different subjects\nfrom LLMs. Finally, we re-formulate the question instructions in a manner of\ncorrectness evaluation to facilitate LLMs to generate clearer responses. The\nproposed framework enables LLMs to flexibly assess personalities of different\ngroups of people. We further propose three evaluation metrics to measure the\nconsistency, robustness, and fairness of assessment results from\nstate-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal\nChatGPT's ability to assess human personalities, and the average results\ndemonstrate that it can achieve more consistent and fairer assessments in spite\nof lower robustness against prompt biases compared with InstructGPT.",
        "pdf_link": "https://arxiv.org/pdf/2303.01248v3.pdf"
    },
    {
        "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine",
        "authors": [
            "Cyril Zakka",
            "Akash Chaurasia",
            "Rohan Shad",
            "Alex R. Dalal",
            "Jennifer L. Kim",
            "Michael Moor",
            "Kevin Alexander",
            "Euan Ashley",
            "Jack Boyd",
            "Kathleen Boyd",
            "Karen Hirsch",
            "Curt Langlotz",
            "Joanna Nelson",
            "William Hiesinger"
        ],
        "published": "2023-03-01T02:30:11Z",
        "summary": "Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine, adoption of these models in real-world\nsettings has been largely limited by their tendency to generate incorrect and\nsometimes even toxic statements. In this study, we develop Almanac, a large\nlanguage model framework augmented with retrieval capabilities for medical\nguideline and treatment recommendations. Performance on a novel dataset of\nclinical scenarios (n = 130) evaluated by a panel of 5 board-certified and\nresident physicians demonstrates significant increases in factuality (mean of\n18% at p-value < 0.05) across all specialties, with improvements in\ncompleteness and safety. Our results demonstrate the potential for large\nlanguage models to be effective tools in the clinical decision-making process,\nwhile also emphasizing the importance of careful testing and deployment to\nmitigate their shortcomings.",
        "pdf_link": "https://arxiv.org/pdf/2303.01229v2.pdf"
    },
    {
        "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics",
        "authors": [
            "Conor Houghton",
            "Nina Kazanina",
            "Priyanka Sukumaran"
        ],
        "published": "2023-02-28T20:49:38Z",
        "summary": "Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.",
        "pdf_link": "https://arxiv.org/pdf/2303.00077v1.pdf"
    },
    {
        "title": "Automatic Scoring of Dream Reports' Emotional Content with Large Language Models",
        "authors": [
            "Lorenzo Bertolini",
            "Valentina Elce",
            "Adriana Michalak",
            "Giulio Bernardi",
            "Julie Weeds"
        ],
        "published": "2023-02-28T18:23:17Z",
        "summary": "In the field of dream research, the study of dream content typically relies\non the analysis of verbal reports provided by dreamers upon awakening from\ntheir sleep. This task is classically performed through manual scoring provided\nby trained annotators, at a great time expense. While a consistent body of work\nsuggests that natural language processing (NLP) tools can support the automatic\nanalysis of dream reports, proposed methods lacked the ability to reason over a\nreport's full context and required extensive data pre-processing. Furthermore,\nin most cases, these methods were not validated against standard manual scoring\napproaches. In this work, we address these limitations by adopting large\nlanguage models (LLMs) to study and replicate the manual annotation of dream\nreports, using a mixture of off-the-shelf and bespoke approaches, with a focus\non references to reports' emotions. Our results show that the off-the-shelf\nmethod achieves a low performance probably in light of inherent linguistic\ndifferences between reports collected in different (groups of) individuals. On\nthe other hand, the proposed bespoke text classification method achieves a high\nperformance, which is robust against potential biases. Overall, these\nobservations indicate that our approach could find application in the analysis\nof large dream datasets and may favour reproducibility and comparability of\nresults across studies.",
        "pdf_link": "https://arxiv.org/pdf/2302.14828v1.pdf"
    },
    {
        "title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following",
        "authors": [
            "Seonghyeon Ye",
            "Hyeonbin Hwang",
            "Sohee Yang",
            "Hyeongu Yun",
            "Yireun Kim",
            "Minjoon Seo"
        ],
        "published": "2023-02-28T16:06:35Z",
        "summary": "In this paper, we present our finding that prepending a Task-Agnostic Prefix\nPrompt (TAPP) to the input improves the instruction-following ability of\nvarious Large Language Models (LLMs) during inference. TAPP is different from\ncanonical prompts for LLMs in that it is a fixed prompt prepended to the\nbeginning of every input regardless of the target task for zero-shot\ngeneralization. We observe that both base LLMs (i.e. not fine-tuned to follow\ninstructions) and instruction-tuned models benefit from TAPP, resulting in\n34.58% and 12.26% improvement on average, respectively. This implies that the\ninstruction-following ability of LLMs can be improved during inference time\nwith a fixed prompt constructed with simple heuristics. We hypothesize that\nTAPP assists language models to better estimate the output distribution by\nfocusing more on the instruction of the target task during inference. In other\nwords, such ability does not seem to be sufficiently activated in not only base\nLLMs but also many instruction-fine-tuned LLMs. All experiments are\nreproducible from https://github.com/seonghyeonye/TAPP.",
        "pdf_link": "https://arxiv.org/pdf/2302.14691v2.pdf"
    },
    {
        "title": "Zero-Shot Cross-Lingual Summarization via Large Language Models",
        "authors": [
            "Jiaan Wang",
            "Yunlong Liang",
            "Fandong Meng",
            "Beiqi Zou",
            "Zhixu Li",
            "Jianfeng Qu",
            "Jie Zhou"
        ],
        "published": "2023-02-28T01:27:37Z",
        "summary": "Given a document in a source language, cross-lingual summarization (CLS) aims\nto generate a summary in a different target language. Recently, the emergence\nof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has\nattracted wide attention from the computational linguistics community. However,\nit is not yet known the performance of LLMs on CLS. In this report, we\nempirically use various prompts to guide LLMs to perform zero-shot CLS from\ndifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminary\nevaluation on the generated summaries. We find that ChatGPT and GPT-4\noriginally prefer to produce lengthy summaries with detailed information. These\ntwo LLMs can further balance informativeness and conciseness with the help of\nan interactive prompt, significantly improving their CLS performance.\nExperimental results on three widely-used CLS datasets show that GPT-4 achieves\nstate-of-the-art zero-shot CLS performance, and performs competitively compared\nwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual and\nbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited\nzero-shot CLS ability. Due to the composite nature of CLS, which requires\nmodels to perform summarization and translation simultaneously, accomplishing\nthis task in a zero-shot manner is even a challenge for LLMs. Therefore, we\nsincerely hope and recommend future LLM research could use CLS as a testbed.",
        "pdf_link": "https://arxiv.org/pdf/2302.14229v4.pdf"
    },
    {
        "title": "Reward Design with Language Models",
        "authors": [
            "Minae Kwon",
            "Sang Michael Xie",
            "Kalesha Bullard",
            "Dorsa Sadigh"
        ],
        "published": "2023-02-27T22:09:35Z",
        "summary": "Reward design in reinforcement learning (RL) is challenging since specifying\nhuman notions of desired behavior may be difficult via reward functions or\nrequire many expert demonstrations. Can we instead cheaply design rewards using\na natural language interface? This paper explores how to simplify reward design\nby prompting a large language model (LLM) such as GPT-3 as a proxy reward\nfunction, where the user provides a textual prompt containing a few examples\n(few-shot) or a description (zero-shot) of the desired behavior. Our approach\nleverages this proxy reward function in an RL framework. Specifically, users\nspecify a prompt once at the beginning of training. During training, the LLM\nevaluates an RL agent's behavior against the desired behavior described by the\nprompt and outputs a corresponding reward signal. The RL agent then uses this\nreward to update its behavior. We evaluate whether our approach can train\nagents aligned with user objectives in the Ultimatum Game, matrix games, and\nthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agents\ntrained with our framework are well-aligned with the user's objectives and\noutperform RL agents trained with reward functions learned via supervised\nlearning",
        "pdf_link": "https://arxiv.org/pdf/2303.00001v1.pdf"
    },
    {
        "title": "Systematic Rectification of Language Models via Dead-end Analysis",
        "authors": [
            "Meng Cao",
            "Mehdi Fatemi",
            "Jackie Chi Kit Cheung",
            "Samira Shabanian"
        ],
        "published": "2023-02-27T17:47:53Z",
        "summary": "With adversarial or otherwise normal prompts, existing large language models\n(LLM) can be pushed to generate toxic discourses. One way to reduce the risk of\nLLMs generating undesired discourses is to alter the training of the LLM. This\ncan be very restrictive due to demanding computation requirements. Other\nmethods rely on rule-based or prompt-based token elimination, which are limited\nas they dismiss future tokens and the overall meaning of the complete\ndiscourse. Here, we center detoxification on the probability that the finished\ndiscourse is ultimately considered toxic. That is, at each point, we advise\nagainst token selections proportional to how likely a finished text from this\npoint will be toxic. To this end, we formally extend the dead-end theory from\nthe recent reinforcement learning (RL) literature to also cover uncertain\noutcomes. Our approach, called rectification, utilizes a separate but\nsignificantly smaller model for detoxification, which can be applied to diverse\nLLMs as long as they share the same vocabulary. Importantly, our method does\nnot require access to the internal representations of the LLM, but only the\ntoken probability distribution at each decoding step. This is crucial as many\nLLMs today are hosted in servers and only accessible through APIs. When applied\nto various LLMs, including GPT-3, our approach significantly improves the\ngenerated discourse compared to the base LLMs and other techniques in terms of\nboth the overall language and detoxification performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.14003v1.pdf"
    },
    {
        "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
        "authors": [
            "Rui-Jie Zhu",
            "Qihang Zhao",
            "Guoqi Li",
            "Jason K. Eshraghian"
        ],
        "published": "2023-02-27T16:43:04Z",
        "summary": "As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations.",
        "pdf_link": "https://arxiv.org/pdf/2302.13939v4.pdf"
    },
    {
        "title": "Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity",
        "authors": [
            "Zijian Ding",
            "Arvind Srinivasan",
            "Stephen MacNeil",
            "Joel Chan"
        ],
        "published": "2023-02-27T15:54:57Z",
        "summary": "Cross-domain analogical reasoning is a core creative ability that can be\nchallenging for humans. Recent work has shown some proofs-of concept of Large\nlanguage Models' (LLMs) ability to generate cross-domain analogies. However,\nthe reliability and potential usefulness of this capacity for augmenting human\ncreative work has received little systematic exploration. In this paper, we\nsystematically explore LLMs capacity to augment cross-domain analogical\nreasoning. Across three studies, we found: 1) LLM-generated cross-domain\nanalogies were frequently judged as helpful in the context of a problem\nreformulation task (median 4 out of 5 helpfulness rating), and frequently (~80%\nof cases) led to observable changes in problem formulations, and 2) there was\nan upper bound of 25% of outputs bring rated as potentially harmful, with a\nmajority due to potentially upsetting content, rather than biased or toxic\ncontent. These results demonstrate the potential utility -- and risks -- of\nLLMs for augmenting cross-domain analogical creativity.",
        "pdf_link": "https://arxiv.org/pdf/2302.12832v2.pdf"
    },
    {
        "title": "Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",
        "authors": [
            "Sakib Shahriar",
            "Kadhim Hayawi"
        ],
        "published": "2023-02-27T14:26:29Z",
        "summary": "The emergence of an AI-powered chatbot that can generate human-like sentences\nand write coherent essays has caught the world's attention. This paper\ndiscusses the historical overview of chatbots and the technology behind Chat\nGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,\npotential applications of ChatGPT in various domains, including healthcare,\neducation, and research, are highlighted. Despite promising results, there are\nseveral privacy and ethical concerns surrounding ChatGPT. In addition, we\nhighlight some of the important limitations of the current version of ChatGPT.\nWe also ask ChatGPT to provide its point of view and present its responses to\nseveral questions we attempt to answer.",
        "pdf_link": "https://arxiv.org/pdf/2302.13817v4.pdf"
    },
    {
        "title": "The (ab)use of Open Source Code to Train Large Language Models",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi"
        ],
        "published": "2023-02-27T11:34:53Z",
        "summary": "In recent years, Large Language Models (LLMs) have gained significant\npopularity due to their ability to generate human-like text and their potential\napplications in various fields, such as Software Engineering. LLMs for Code are\ncommonly trained on large unsanitized corpora of source code scraped from the\nInternet. The content of these datasets is memorized and emitted by the models,\noften in a verbatim manner. In this work, we will discuss the security,\nprivacy, and licensing implications of memorization. We argue why the use of\ncopyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide\nfour actionable recommendations to address this issue.",
        "pdf_link": "https://arxiv.org/pdf/2302.13681v2.pdf"
    },
    {
        "title": "Towards Human-Bot Collaborative Software Architecting with ChatGPT",
        "authors": [
            "Aakash Ahmad",
            "Muhammad Waseem",
            "Peng Liang",
            "Mahdi Fehmideh",
            "Mst Shamima Aktar",
            "Tommi Mikkonen"
        ],
        "published": "2023-02-26T16:32:16Z",
        "summary": "Architecting software-intensive systems can be a complex process. It deals\nwith the daunting tasks of unifying stakeholders' perspectives, designers'\nintellect, tool-based automation, pattern-driven reuse, and so on, to sketch a\nblueprint that guides software implementation and evaluation. Despite its\nbenefits, architecture-centric software engineering (ACSE) inherits a multitude\nof challenges. ACSE challenges could stem from a lack of standardized\nprocesses, socio-technical limitations, and scarcity of human expertise etc.\nthat can impede the development of existing and emergent classes of software\n(e.g., IoTs, blockchain, quantum systems). Software Development Bots (DevBots)\ntrained on large language models can help synergise architects' knowledge with\nartificially intelligent decision support to enable rapid architecting in a\nhuman-bot collaborative ACSE. An emerging solution to enable this collaboration\nis ChatGPT, a disruptive technology not primarily introduced for software\nengineering, but is capable of articulating and refining architectural\nartifacts based on natural language processing. We detail a case study that\ninvolves collaboration between a novice software architect and ChatGPT for\narchitectural analysis, synthesis, and evaluation of a services-driven software\napplication. Preliminary results indicate that ChatGPT can mimic an architect's\nrole to support and often lead ACSE, however; it requires human oversight and\ndecision support for collaborative architecting. Future research focuses on\nharnessing empirical evidence about architects' productivity and exploring\nsocio-technical aspects of architecting with ChatGPT to tackle emerging and\nfuturistic challenges of ACSE.",
        "pdf_link": "https://arxiv.org/pdf/2302.14600v1.pdf"
    },
    {
        "title": "Fast Attention Requires Bounded Entries",
        "authors": [
            "Josh Alman",
            "Zhao Song"
        ],
        "published": "2023-02-26T02:42:39Z",
        "summary": "In modern machine learning, inner product attention computation is a\nfundamental task for training large language models such as Transformer, GPT-1,\nBERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as\ninput three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to\nconstruct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A\nV \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention\nmatrix', and $\\exp$ is applied entry-wise. Straightforward methods for this\nproblem explicitly compute the $n \\times n$ attention matrix $A$, and hence\nrequire time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small.\n  In this paper, we investigate whether faster algorithms are possible by\nimplicitly making use of the matrix $A$. We present two results, showing that\nthere is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$.\n  $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an\n$n^{1+o(1)}$ time algorithm to approximate $\\mathrm{Att}(Q,K,V)$ up to\n$1/\\mathrm{poly}(n)$ additive error.\n  $\\bullet$ If $d = O(\\log n)$ and $B = \\Theta (\\sqrt{\\log n})$, assuming the\nStrong Exponential Time Hypothesis from fine-grained complexity theory, it is\nimpossible to approximate $\\mathrm{Att}(Q,K,V)$ up to $1/\\mathrm{poly}(n)$\nadditive error in truly subquadratic time $n^{2 - \\Omega(1)}$.\n  This gives a theoretical explanation for the phenomenon observed in practice\nthat attention computation is much more efficient when the input matrices have\nsmaller entries.",
        "pdf_link": "https://arxiv.org/pdf/2302.13214v2.pdf"
    },
    {
        "title": "On pitfalls (and advantages) of sophisticated large language models",
        "authors": [
            "Anna Strasser"
        ],
        "published": "2023-02-25T11:14:39Z",
        "summary": "Natural language processing based on large language models (LLMs) is a\nbooming field of AI research. After neural networks have proven to outperform\nhumans in games and practical domains based on pattern recognition, we might\nstand now at a road junction where artificial entities might eventually enter\nthe realm of human communication. However, this comes with serious risks. Due\nto the inherent limitations regarding the reliability of neural networks,\noverreliance on LLMs can have disruptive consequences. Since it will be\nincreasingly difficult to distinguish between human-written and\nmachine-generated text, one is confronted with new ethical challenges. This\nbegins with the no longer undoubtedly verifiable human authorship and continues\nwith various types of fraud, such as a new form of plagiarism. This also\nconcerns the violation of privacy rights, the possibility of circulating\ncounterfeits of humans, and, last but not least, it makes a massive spread of\nmisinformation possible.",
        "pdf_link": "https://arxiv.org/pdf/2303.17511v1.pdf"
    },
    {
        "title": "Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study",
        "authors": [
            "Chen Cao"
        ],
        "published": "2023-02-25T04:07:03Z",
        "summary": "Programming skills are rapidly becoming essential for many educational paths\nand career opportunities. Yet, for many international students, the traditional\napproach to teaching introductory programming courses can be a significant\nchallenge due to the complexities of the language, the lack of prior\nprogramming knowledge, and the language and cultural barriers. This study\nexplores how large language models and gamification can scaffold coding\nlearning and increase Chinese students sense of belonging in introductory\nprogramming courses. In this project, a gamification intelligent tutoring\nsystem was developed to adapt to Chinese international students learning needs\nand provides scaffolding to support their success in introductory computer\nprogramming courses.",
        "pdf_link": "https://arxiv.org/pdf/2302.12834v1.pdf"
    },
    {
        "title": "Robot Behavior-Tree-Based Task Generation with Large Language Models",
        "authors": [
            "Yue Cao",
            "C. S. George Lee"
        ],
        "published": "2023-02-24T22:53:10Z",
        "summary": "Nowadays, the behavior tree is gaining popularity as a representation for\nrobot tasks due to its modularity and reusability. Designing behavior-tree\ntasks manually is time-consuming for robot end-users, thus there is a need for\ninvestigating automatic behavior-tree-based task generation. Prior\nbehavior-tree-based task generation approaches focus on fixed primitive tasks\nand lack generalizability to new task domains. To cope with this issue, we\npropose a novel behavior-tree-based task generation approach that utilizes\nstate-of-the-art large language models. We propose a Phase-Step prompt design\nthat enables a hierarchical-structured robot task generation and further\nintegrate it with behavior-tree-embedding-based search to set up the\nappropriate prompt. In this way, we enable an automatic and cross-domain\nbehavior-tree task generation. Our behavior-tree-based task generation approach\ndoes not require a set of pre-defined primitive tasks. End-users only need to\ndescribe an abstract desired task and our proposed approach can swiftly\ngenerate the corresponding behavior tree. A full-process case study is provided\nto demonstrate our proposed approach. An ablation study is conducted to\nevaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step\nprompts and the limitation of large language models are presented and\ndiscussed.",
        "pdf_link": "https://arxiv.org/pdf/2302.12927v1.pdf"
    },
    {
        "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
        "authors": [
            "KaShun Shum",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "published": "2023-02-24T18:58:06Z",
        "summary": "Chain-of-thought (CoT) advances the reasoning abilities of large language\nmodels (LLMs) and achieves superior performance in complex reasoning tasks.\nHowever, most CoT studies rely on carefully designed human-annotated rational\nchains to prompt LLMs, posing challenges for real-world applications where\nlabeled data is available without rational chains. This paper proposes a new\nstrategy, Automate-CoT (Automatic Prompt Augmentation and Selection with\nChain-of-Thought), that can bypass human engineering of CoT by automatically\naugmenting rational chains from a small labeled dataset, and then pruning\nlow-quality chains to construct a candidate pool of machine-generated rationale\nchains based on the labels. Finally, it selects the optimal combination of\nseveral rationale chains from the pool for CoT prompting by employing a\nvariance-reduced policy gradient strategy to estimate the significance of each\nexample. Automate-CoT enables a quick adaptation of the CoT technique to\ndifferent tasks. Experimental results demonstrate the effectiveness of our\nmethod, where competitive results are achieved on arithmetic reasoning (+2.7%),\ncommonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning\ntasks (+2.5%). The code is available at\nhttps://github.com/SHUMKASHUN/Automate-CoT.",
        "pdf_link": "https://arxiv.org/pdf/2302.12822v3.pdf"
    },
    {
        "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
        "authors": [
            "Baolin Peng",
            "Michel Galley",
            "Pengcheng He",
            "Hao Cheng",
            "Yujia Xie",
            "Yu Hu",
            "Qiuyuan Huang",
            "Lars Liden",
            "Zhou Yu",
            "Weizhu Chen",
            "Jianfeng Gao"
        ],
        "published": "2023-02-24T18:48:43Z",
        "summary": "Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2302.12813v3.pdf"
    },
    {
        "title": "HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media",
        "authors": [
            "Isabel Segura-Bedmar"
        ],
        "published": "2023-02-24T18:17:38Z",
        "summary": "This paper describes our participation in SemEval-2023 Task 10, whose goal is\nthe detection of sexism in social media. We explore some of the most popular\ntransformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study\ndifferent data augmentation techniques to increase the training dataset. During\nthe development phase, our best results were obtained by using RoBERTa and data\naugmentation for tasks B and C. However, the use of synthetic data does not\nimprove the results for task C. We participated in the three subtasks. Our\napproach still has much room for improvement, especially in the two\nfine-grained classifications. All our code is available in the repository\nhttps://github.com/isegura/hulat_edos.",
        "pdf_link": "https://arxiv.org/pdf/2302.12840v2.pdf"
    },
    {
        "title": "Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary",
        "authors": [
            "Miguel Ortega-Mart\u00edn",
            "\u00d3scar Garc\u00eda-Sierra",
            "Alfonso Ardoiz",
            "Juan Carlos Armenteros",
            "Jorge \u00c1lvarez",
            "Adri\u00e1n Alonso"
        ],
        "published": "2023-02-24T16:59:54Z",
        "summary": "Dictionaries are one of the oldest and most used linguistic resources.\nBuilding them is a complex task that, to the best of our knowledge, has yet to\nbe explored with generative Large Language Models (LLMs). We introduce the\n\"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish\nAI-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We\nalso define future steps we aim to follow to improve this initial commitment to\nthe field, such as more additional languages.",
        "pdf_link": "https://arxiv.org/pdf/2302.12746v2.pdf"
    },
    {
        "title": "Language Models are Few-shot Learners for Prognostic Prediction",
        "authors": [
            "Zekai Chen",
            "Mariann Micsinai Balan",
            "Kevin Brown"
        ],
        "published": "2023-02-24T15:35:36Z",
        "summary": "Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases.",
        "pdf_link": "https://arxiv.org/pdf/2302.12692v4.pdf"
    },
    {
        "title": "Adapting Pre-trained Language Models for Quantum Natural Language Processing",
        "authors": [
            "Qiuchi Li",
            "Benyou Wang",
            "Yudong Zhu",
            "Christina Lioma",
            "Qun Liu"
        ],
        "published": "2023-02-24T14:59:02Z",
        "summary": "The emerging classical-quantum transfer learning paradigm has brought a\ndecent performance to quantum computational models in many tasks, such as\ncomputer vision, by enabling a combination of quantum models and classical\npre-trained neural networks. However, using quantum computing with pre-trained\nmodels has yet to be explored in natural language processing (NLP). Due to the\nhigh linearity constraints of the underlying quantum computing infrastructures,\nexisting Quantum NLP models are limited in performance on real tasks. We fill\nthis gap by pre-training a sentence state with complex-valued BERT-like\narchitecture, and adapting it to the classical-quantum transfer learning scheme\nfor sentence classification. On quantum simulation experiments, the pre-trained\nrepresentation can bring 50\\% to 60\\% increases to the capacity of end-to-end\nquantum models.",
        "pdf_link": "https://arxiv.org/pdf/2302.13812v1.pdf"
    },
    {
        "title": "Analyzing And Editing Inner Mechanisms Of Backdoored Language Models",
        "authors": [
            "Max Lamparth",
            "Anka Reuel"
        ],
        "published": "2023-02-24T05:26:08Z",
        "summary": "Poisoning of data sets is a potential security threat to large language\nmodels that can lead to backdoored models. A description of the internal\nmechanisms of backdoored language models and how they process trigger inputs,\ne.g., when switching to toxic language, has yet to be found. In this work, we\nstudy the internal representations of transformer-based backdoored language\nmodels and determine early-layer MLP modules as most important for the backdoor\nmechanism in combination with the initial embedding projection. We use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements that reduce the MLP module outputs to essentials for the backdoor\nmechanism. To this end, we introduce PCP ablation, where we replace transformer\nmodules with low-rank matrices based on the principal components of their\nactivations. We demonstrate our results on backdoored toy, backdoored large,\nand non-backdoored open-source models. We show that we can improve the backdoor\nrobustness of large language models by locally constraining individual modules\nduring fine-tuning on potentially poisonous data sets.\n  Trigger warning: Offensive language.",
        "pdf_link": "https://arxiv.org/pdf/2302.12461v2.pdf"
    },
    {
        "title": "Extracting Victim Counts from Text",
        "authors": [
            "Mian Zhong",
            "Shehzaad Dhuliawala",
            "Niklas Stoehr"
        ],
        "published": "2023-02-23T23:50:24Z",
        "summary": "Decision-makers in the humanitarian sector rely on timely and exact\ninformation during crisis events. Knowing how many civilians were injured\nduring an earthquake is vital to allocate aids properly. Information about such\nvictim counts is often only available within full-text event descriptions from\nnewspapers and other reports. Extracting numbers from text is challenging:\nnumbers have different formats and may require numeric reasoning. This renders\npurely string matching-based approaches insufficient. As a consequence,\nfine-grained counts of injured, displaced, or abused victims beyond fatalities\nare often not extracted and remain unseen. We cast victim count extraction as a\nquestion answering (QA) task with a regression or classification objective. We\ncompare regex, dependency parsing, semantic role labeling-based approaches, and\nadvanced text-to-text models. Beyond model accuracy, we analyze extraction\nreliability and robustness which are key for this sensitive task. In\nparticular, we discuss model calibration and investigate few-shot and\nout-of-distribution performance. Ultimately, we make a comprehensive\nrecommendation on which model to select for different desiderata and data\ndomains. Our work is among the first to apply numeracy-focused large language\nmodels in a real-world use case with a positive impact.",
        "pdf_link": "https://arxiv.org/pdf/2302.12367v1.pdf"
    },
    {
        "title": "Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",
        "authors": [
            "Guido Zuccon",
            "Bevan Koopman"
        ],
        "published": "2023-02-23T22:14:01Z",
        "summary": "Generative pre-trained language models (GPLMs) like ChatGPT encode in the\nmodel's parameters knowledge the models observe during the pre-training phase.\nThis knowledge is then used at inference to address the task specified by the\nuser in their prompt. For example, for the question-answering task, the GPLMs\nleverage the knowledge and linguistic patterns learned at training to produce\nan answer to a user question. Aside from the knowledge encoded in the model\nitself, answers produced by GPLMs can also leverage knowledge provided in the\nprompts. For example, a GPLM can be integrated into a retrieve-then-generate\nparadigm where a search engine is used to retrieve documents relevant to the\nquestion; the content of the documents is then transferred to the GPLM via the\nprompt. In this paper we study the differences in answer correctness generated\nby ChatGPT when leveraging the model's knowledge alone vs. in combination with\nthe prompt knowledge. We study this in the context of consumers seeking health\nadvice from the model. Aside from measuring the effectiveness of ChatGPT in\nthis context, we show that the knowledge passed in the prompt can overturn the\nknowledge encoded in the model and this is, in our experiments, to the\ndetriment of answer correctness. This work has important implications for the\ndevelopment of more robust and transparent question-answering systems based on\ngenerative pre-trained language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.13793v1.pdf"
    },
    {
        "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
        "authors": [
            "Denis Jered McInerney",
            "Geoffrey Young",
            "Jan-Willem van de Meent",
            "Byron C. Wallace"
        ],
        "published": "2023-02-23T21:23:06Z",
        "summary": "We propose CHiLL (Crafting High-Level Latents), an approach for\nnatural-language specification of features for linear models. CHiLL prompts\nLLMs with expert-crafted queries to generate interpretable features from health\nrecords. The resulting noisy labels are then used to train a simple linear\nclassifier. Generating features based on queries to an LLM can empower\nphysicians to use their domain expertise to craft features that are clinically\nmeaningful for a downstream task of interest, without having to manually\nextract these from raw EHR. We are motivated by a real-world risk prediction\ntask, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and\nstandard predictive tasks (e.g., 30-day readmission) to evaluate this approach.\nWe find that linear models using automatically extracted features are\ncomparably performant to models using reference features, and provide greater\ninterpretability than linear models using \"Bag-of-Words\" features. We verify\nthat learned feature weights align well with clinical expectations.",
        "pdf_link": "https://arxiv.org/pdf/2302.12343v2.pdf"
    },
    {
        "title": "Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning",
        "authors": [
            "Vittoria Dentella",
            "Elliot Murphy",
            "Gary Marcus",
            "Evelina Leivada"
        ],
        "published": "2023-02-23T20:18:52Z",
        "summary": "Advances in computational methods and big data availability have recently\ntranslated into breakthroughs in AI applications. With successes in bottom-up\nchallenges partially overshadowing shortcomings, the 'human-like' performance\nof Large Language Models has raised the question of how linguistic performance\nis achieved by algorithms. Given systematic shortcomings in generalization\nacross many AI systems, in this work we ask whether linguistic performance is\nindeed guided by language knowledge in Large Language Models. To this end, we\nprompt GPT-3 with a grammaticality judgement task and comprehension questions\non less frequent constructions that are thus unlikely to form part of Large\nLanguage Models' training data. These included grammatical 'illusions',\nsemantic anomalies, complex nested hierarchies and self-embeddings. GPT-3\nfailed for every prompt but one, often offering answers that show a critical\nlack of understanding even of high-frequency words used in these less frequent\ngrammatical constructions. The present work sheds light on the boundaries of\nthe alleged AI human-like linguistic competence and argues that, far from\nhuman-like, the next-word prediction abilities of LLMs may face issues of\nrobustness, when pushed beyond training data.",
        "pdf_link": "https://arxiv.org/pdf/2302.12313v2.pdf"
    },
    {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "authors": [
            "Shizhe Diao",
            "Pengcheng Wang",
            "Yong Lin",
            "Tong Zhang"
        ],
        "published": "2023-02-23T18:58:59Z",
        "summary": "The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.",
        "pdf_link": "https://arxiv.org/pdf/2302.12246v3.pdf"
    },
    {
        "title": "What Makes a Language Easy to Deep-Learn?",
        "authors": [
            "Lukas Galke",
            "Yoav Ram",
            "Limor Raviv"
        ],
        "published": "2023-02-23T18:57:34Z",
        "summary": "Deep neural networks drive the success of natural language processing. A\nfundamental property of language is its compositional structure, allowing\nhumans to systematically produce forms for new meanings. For humans, languages\nwith more compositional and transparent structures are typically easier to\nlearn than those with opaque and irregular structures. However, this\nlearnability advantage has not yet been shown for deep neural networks,\nlimiting their use as models for human language learning. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent languages that vary in their degree of compositional structure. We\nevaluate the memorization and generalization capabilities of a large language\nmodel and recurrent neural networks, and show that both deep neural networks\nexhibit a learnability advantage for more structured linguistic input: neural\nnetworks exposed to more compositional languages show more systematic\ngeneralization, greater agreement between different agents, and greater\nsimilarity to human learners.",
        "pdf_link": "https://arxiv.org/pdf/2302.12239v3.pdf"
    },
    {
        "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
        "authors": [
            "Kai Greshake",
            "Sahar Abdelnabi",
            "Shailesh Mishra",
            "Christoph Endres",
            "Thorsten Holz",
            "Mario Fritz"
        ],
        "published": "2023-02-23T17:14:38Z",
        "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.",
        "pdf_link": "https://arxiv.org/pdf/2302.12173v2.pdf"
    },
    {
        "title": "An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)",
        "authors": [
            "Paulo Shakarian",
            "Abhinav Koyyalamudi",
            "Noel Ngu",
            "Lakshmivihari Mareedu"
        ],
        "published": "2023-02-23T16:06:16Z",
        "summary": "We study the performance of a commercially available large language model\n(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.\nTo our knowledge, this is the first independent evaluation of ChatGPT. We found\nthat ChatGPT's performance changes dramatically based on the requirement to\nshow its work, failing 20% of the time when it provides work compared with 84%\nwhen it does not. Further several factors about MWPs relating to the number of\nunknowns and number of operations that lead to a higher probability of failure\nwhen compared with the prior, specifically noting (across all experiments) that\nthe probability of failure increases linearly with the number of addition and\nsubtraction operations. We also have released the dataset of ChatGPT's\nresponses to the MWPs to support further work on the characterization of LLM\nperformance and present baseline machine learning models to predict if ChatGPT\ncan correctly answer an MWP. We have released a dataset comprised of ChatGPT's\nresponses to support further research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2302.13814v2.pdf"
    },
    {
        "title": "Sentence Simplification via Large Language Models",
        "authors": [
            "Yutao Feng",
            "Jipeng Qiang",
            "Yun Li",
            "Yunhao Yuan",
            "Yi Zhu"
        ],
        "published": "2023-02-23T12:11:58Z",
        "summary": "Sentence Simplification aims to rephrase complex sentences into simpler\nsentences while retaining original meaning. Large Language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\ntasks. However, it is not yet known whether LLMs can be served as a\nhigh-quality sentence simplification system. In this work, we empirically\nanalyze the zero-/few-shot learning ability of LLMs by evaluating them on a\nnumber of benchmark test sets. Experimental results show LLMs outperform\nstate-of-the-art sentence simplification methods, and are judged to be on a par\nwith human annotators.",
        "pdf_link": "https://arxiv.org/pdf/2302.11957v1.pdf"
    },
    {
        "title": "Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments",
        "authors": [
            "Jason Xinyu Liu",
            "Ziyi Yang",
            "Ifrah Idrees",
            "Sam Liang",
            "Benjamin Schornstein",
            "Stefanie Tellex",
            "Ankit Shah"
        ],
        "published": "2023-02-22T20:56:40Z",
        "summary": "Grounding navigational commands to linear temporal logic (LTL) leverages its\nunambiguous semantics for reasoning about long-horizon tasks and verifying the\nsatisfaction of temporal constraints. Existing approaches require training data\nfrom the specific environment and landmarks that will be used in natural\nlanguage to understand commands in those environments. We propose Lang2LTL, a\nmodular system and a software package that leverages large language models\n(LLMs) to ground temporal navigational commands to LTL specifications in\nenvironments without prior language data. We comprehensively evaluate Lang2LTL\nfor five well-defined generalization behaviors. Lang2LTL demonstrates the\nstate-of-the-art ability of a single model to ground navigational commands to\ndiverse temporal specifications in 21 city-scaled environments. Finally, we\ndemonstrate a physical robot using Lang2LTL can follow 52 semantically diverse\nnavigational commands in two indoor environments.",
        "pdf_link": "https://arxiv.org/pdf/2302.11649v2.pdf"
    },
    {
        "title": "How Does In-Context Learning Help Prompt Tuning?",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Dan Iter",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2023-02-22T17:45:12Z",
        "summary": "Fine-tuning large language models is becoming ever more impractical due to\ntheir rapidly-growing scale. This motivates the use of parameter-efficient\nadaptation methods such as prompt tuning (PT), which adds a small number of\ntunable embeddings to an otherwise frozen model, and in-context learning (ICL),\nin which demonstrations of the task are provided to the model in natural\nlanguage without any additional training. Recently, Singhal et al. (2022)\npropose ``instruction prompt tuning'' (IPT), which combines PT with ICL by\nconcatenating a natural language demonstration with learned prompt embeddings.\nWhile all of these methods have proven effective on different tasks, how they\ninteract with each other remains unexplored. In this paper, we empirically\nstudy when and how in-context examples improve prompt tuning by measuring the\neffectiveness of ICL, PT, and IPT on five text generation tasks with multiple\nbase language models. We observe that (1) IPT does \\emph{not} always outperform\nPT, and in fact requires the in-context demonstration to be semantically\nsimilar to the test input to yield improvements; (2) PT is unstable and\nexhibits high variance, but combining PT and ICL (into IPT) consistently\nreduces variance across all five tasks; and (3) prompts learned for a specific\nsource task via PT exhibit positive transfer when paired with in-context\nexamples of a different target task. Our results offer actionable insights on\nchoosing a suitable parameter-efficient adaptation method for a given task.",
        "pdf_link": "https://arxiv.org/pdf/2302.11521v1.pdf"
    },
    {
        "title": "Guiding Large Language Models via Directional Stimulus Prompting",
        "authors": [
            "Zekun Li",
            "Baolin Peng",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao",
            "Xifeng Yan"
        ],
        "published": "2023-02-22T17:44:15Z",
        "summary": "We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
        "pdf_link": "https://arxiv.org/pdf/2302.11520v4.pdf"
    },
    {
        "title": "In-context Example Selection with Influences",
        "authors": [
            "Tai Nguyen",
            "Eric Wong"
        ],
        "published": "2023-02-21T22:47:45Z",
        "summary": "In-context learning (ICL) is a powerful paradigm emerged from large language\nmodels (LLMs). Despite its promises, ICL performance is known to be highly\nsensitive to input examples. In this work, we use $\\textit{in-context\ninfluences}$ to analyze few-shot ICL performance directly from the in-context\nexamples. Our proposed influence-based example selection method can identify\nboth positive and negative examples, outperforming several baselines when\nevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$\nperformance gap between using the most negative in-context examples compared to\nthe most positive. In a case study, we apply our influence-based framework to\nquantify the phenomena of recency bias in example ordering for few-shot ICL.",
        "pdf_link": "https://arxiv.org/pdf/2302.11042v2.pdf"
    },
    {
        "title": "$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models",
        "authors": [
            "Yangsibo Huang",
            "Daogao Liu",
            "Zexuan Zhong",
            "Weijia Shi",
            "Yin Tat Lee"
        ],
        "published": "2023-02-21T18:54:21Z",
        "summary": "Fine-tuning a language model on a new domain is standard practice for domain\nadaptation. However, it can be infeasible when it comes to modern large-scale\nlanguage models such as GPT-3, which can only be accessed through APIs, making\nit difficult to access the internal parameters of the model. In this paper, we\npropose $k$NN-Adapter, a method to effectively adapt these black-box large\nlanguage models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the\nretrieval-augmented language model, and adaptively learns to interpolate the\noutput of the language model with retrieval results from a datastore consisting\nof the target domain data. Our experiments on four different domains\ndemonstrate that $k$NN-Adapter significantly improves perplexity, and works\nparticularly well in settings with limited access to LLMs. Additionally, we\nshow that $k$NN-Adapter is more effective than fine-tuning when the amount of\ntraining data is limited. We also release a dataset to encourage further study.",
        "pdf_link": "https://arxiv.org/pdf/2302.10879v1.pdf"
    },
    {
        "title": "ChatGPT: Jack of all trades, master of none",
        "authors": [
            "Jan Koco\u0144",
            "Igor Cichecki",
            "Oliwier Kaszyca",
            "Mateusz Kochanek",
            "Dominika Szyd\u0142o",
            "Joanna Baran",
            "Julita Bielaniewicz",
            "Marcin Gruza",
            "Arkadiusz Janz",
            "Kamil Kanclerz",
            "Anna Koco\u0144",
            "Bart\u0142omiej Koptyra",
            "Wiktoria Mieleszczenko-Kowszewicz",
            "Piotr Mi\u0142kowski",
            "Marcin Oleksy",
            "Maciej Piasecki",
            "\u0141ukasz Radli\u0144ski",
            "Konrad Wojtasik",
            "Stanis\u0142aw Wo\u017aniak",
            "Przemys\u0142aw Kazienko"
        ],
        "published": "2023-02-21T15:20:37Z",
        "summary": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.",
        "pdf_link": "https://arxiv.org/pdf/2302.10724v4.pdf"
    },
    {
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
        "authors": [
            "Jules White",
            "Quchen Fu",
            "Sam Hays",
            "Michael Sandborn",
            "Carlos Olea",
            "Henry Gilbert",
            "Ashraf Elnashar",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "published": "2023-02-21T12:42:44Z",
        "summary": "Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.",
        "pdf_link": "https://arxiv.org/pdf/2302.11382v1.pdf"
    },
    {
        "title": "Mask-guided BERT for Few Shot Text Classification",
        "authors": [
            "Wenxiong Liao",
            "Zhengliang Liu",
            "Haixing Dai",
            "Zihao Wu",
            "Yiyang Zhang",
            "Xiaoke Huang",
            "Yuzhong Chen",
            "Xi Jiang",
            "Wei Liu",
            "Dajiang Zhu",
            "Tianming Liu",
            "Sheng Li",
            "Xiang Li",
            "Hongmin Cai"
        ],
        "published": "2023-02-21T05:24:00Z",
        "summary": "Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2302.10447v3.pdf"
    },
    {
        "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
        "authors": [
            "Xiang Wei",
            "Xingyu Cui",
            "Ning Cheng",
            "Xiaobin Wang",
            "Xin Zhang",
            "Shen Huang",
            "Pengjun Xie",
            "Jinan Xu",
            "Yufeng Chen",
            "Meishan Zhang",
            "Yong Jiang",
            "Wenjuan Han"
        ],
        "published": "2023-02-20T12:57:12Z",
        "summary": "Zero-shot information extraction (IE) aims to build IE systems from the\nunannotated text. It is challenging due to involving little human intervention.\nChallenging but worthwhile, zero-shot IE reduces the time and effort that data\nlabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,\nChatGPT) show promising performance on zero-shot settings, thus inspiring us to\nexplore prompt-based methods. In this work, we ask whether strong IE models can\nbe constructed by directly prompting LLMs. Specifically, we transform the\nzero-shot IE task into a multi-turn question-answering problem with a two-stage\nframework (ChatIE). With the power of ChatGPT, we extensively evaluate our\nframework on three IE tasks: entity-relation triple extract, named entity\nrecognition, and event extraction. Empirical results on six datasets across two\nlanguages show that ChatIE achieves impressive performance and even surpasses\nsome full-shot models on several datasets (e.g., NYT11-HRL). We believe that\nour work could shed light on building IE models with limited resources.",
        "pdf_link": "https://arxiv.org/pdf/2302.10205v1.pdf"
    },
    {
        "title": "90% F1 Score in Relational Triple Extraction: Is it Real ?",
        "authors": [
            "Pratik Saini",
            "Samiran Pal",
            "Tapas Nayak",
            "Indrajit Bhattacharya"
        ],
        "published": "2023-02-20T10:30:16Z",
        "summary": "Extracting relational triples from text is a crucial task for constructing\nknowledge bases. Recent advancements in joint entity and relation extraction\nmodels have demonstrated remarkable F1 scores ($\\ge 90\\%$) in accurately\nextracting relational triples from free text. However, these models have been\nevaluated under restrictive experimental settings and unrealistic datasets.\nThey overlook sentences with zero triples (zero-cardinality), thereby\nsimplifying the task. In this paper, we present a benchmark study of\nstate-of-the-art joint entity and relation extraction models under a more\nrealistic setting. We include sentences that lack any triples in our\nexperiments, providing a comprehensive evaluation. Our findings reveal a\nsignificant decline (approximately 10-15\\% in one dataset and 6-14\\% in another\ndataset) in the models' F1 scores within this realistic experimental setup.\nFurthermore, we propose a two-step modeling approach that utilizes a simple\nBERT-based classifier. This approach leads to overall performance improvement\nin these models within the realistic experimental setting.",
        "pdf_link": "https://arxiv.org/pdf/2302.09887v2.pdf"
    },
    {
        "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
        "authors": [
            "Lorenz Kuhn",
            "Yarin Gal",
            "Sebastian Farquhar"
        ],
        "published": "2023-02-19T20:10:07Z",
        "summary": "We introduce a method to measure uncertainty in large language models. For\ntasks like question answering, it is essential to know when we can trust the\nnatural language outputs of foundation models. We show that measuring\nuncertainty in natural language is challenging because of \"semantic\nequivalence\" -- different sentences can mean the same thing. To overcome these\nchallenges we introduce semantic entropy -- an entropy which incorporates\nlinguistic invariances created by shared meanings. Our method is unsupervised,\nuses only a single model, and requires no modifications to off-the-shelf\nlanguage models. In comprehensive ablation studies we show that the semantic\nentropy is more predictive of model accuracy on question answering data sets\nthan comparable baselines.",
        "pdf_link": "https://arxiv.org/pdf/2302.09664v3.pdf"
    },
    {
        "title": "Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference",
        "authors": [
            "Ming Li",
            "Yusheng Su",
            "Hsiu-Yuan Huang",
            "Jiali Cheng",
            "Xin Hu",
            "Xinmiao Zhang",
            "Huadong Wang",
            "Yujia Qin",
            "Xiaozhi Wang",
            "Kristen A. Lindquist",
            "Zhiyuan Liu",
            "Dan Zhang"
        ],
        "published": "2023-02-19T14:21:33Z",
        "summary": "Humans no doubt use language to communicate about their emotional\nexperiences, but does language in turn help humans understand emotions, or is\nlanguage just a vehicle of communication? This study used a form of artificial\nintelligence (AI) known as large language models (LLMs) to assess whether\nlanguage-based representations of emotion causally contribute to the AI's\nability to generate inferences about the emotional meaning of novel situations.\nFourteen attributes of human emotion concept representation were found to be\nrepresented by the LLM's distinct artificial neuron populations. By\nmanipulating these attribute-related neurons, we in turn demonstrated the role\nof emotion concept knowledge in generative emotion inference. The\nattribute-specific performance deterioration was related to the importance of\ndifferent attributes in human mental space. Our findings provide a\nproof-in-concept that even a LLM can learn about emotions in the absence of\nsensory-motor representations and highlight the contribution of\nlanguage-derived emotion-concept knowledge for emotion inference.",
        "pdf_link": "https://arxiv.org/pdf/2302.09582v5.pdf"
    },
    {
        "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
        "authors": [
            "Amr Hendy",
            "Mohamed Abdelrehim",
            "Amr Sharaf",
            "Vikas Raunak",
            "Mohamed Gabr",
            "Hitokazu Matsushita",
            "Young Jin Kim",
            "Mohamed Afify",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-02-18T02:11:36Z",
        "summary": "Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.",
        "pdf_link": "https://arxiv.org/pdf/2302.09210v1.pdf"
    },
    {
        "title": "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints",
        "authors": [
            "Albert Lu",
            "Hongxin Zhang",
            "Yanzhe Zhang",
            "Xuezhi Wang",
            "Diyi Yang"
        ],
        "published": "2023-02-17T23:30:28Z",
        "summary": "The limits of open-ended generative models are unclear, yet increasingly\nimportant. What causes them to succeed and what causes them to fail? In this\npaper, we take a prompt-centric approach to analyzing and bounding the\nabilities of open-ended generative models. We present a generic methodology of\nanalysis with two challenging prompt constraint types: structural and\nstylistic. These constraint types are categorized into a set of well-defined\nconstraints that are analyzable by a single prompt. We then systematically\ncreate a diverse set of simple, natural, and useful prompts to robustly analyze\neach individual constraint. Using the GPT-3 text-davinci-002 model as a case\nstudy, we generate outputs from our collection of prompts and analyze the\nmodel's generative failures. We also show the generalizability of our proposed\nmethod on other large models like BLOOM and OPT. Our results and our in-context\nmitigation strategies reveal open challenges for future research. We have\npublicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2302.09185v1.pdf"
    },
    {
        "title": "Complex QA and language models hybrid architectures, Survey",
        "authors": [
            "Xavier Daull",
            "Patrice Bellot",
            "Emmanuel Bruno",
            "Vincent Martin",
            "Elisabeth Murisasco"
        ],
        "published": "2023-02-17T18:31:31Z",
        "summary": "This paper reviews the state-of-the-art of language models architectures and\nstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on\nhybridization. Large Language Models (LLM) are good at leveraging public data\non standard problems but once you want to tackle more specific complex\nquestions or problems (e.g. How does the concept of personal freedom vary\nbetween different cultures ? What is the best mix of power generation methods\nto reduce climate change ?) you may need specific architecture, knowledge,\nskills, methods, sensitive data protection, explainability, human approval and\nversatile feedback... Recent projects like ChatGPT and GALACTICA have allowed\nnon-specialists to grasp the great potential as well as the equally strong\nlimitations of LLM in complex QA. In this paper, we start by reviewing required\nskills and evaluation techniques. We integrate findings from the robust\ncommunity edited research papers BIG, BLOOM and HELM which open source,\nbenchmark and analyze limits and challenges of LLM in terms of tasks complexity\nand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as\na baseline. We discuss some challenges associated with complex QA, including\ndomain adaptation, decomposition and efficient multi-step QA, long form and\nnon-factoid QA, safety and multi-sensitivity data protection, multimodal\nsearch, hallucinations, explainability and truthfulness, temporal reasoning. We\nanalyze current solutions and promising research trends, using elements such\nas: hybrid LLM architectural patterns, training and prompting strategies,\nactive human reinforcement learning supervised with AI, neuro-symbolic and\nstructured knowledge grounding, program synthesis, iterated decomposition and\nothers.",
        "pdf_link": "https://arxiv.org/pdf/2302.09051v4.pdf"
    },
    {
        "title": "Privately Customizing Prefinetuning to Better Match User Data in Federated Learning",
        "authors": [
            "Charlie Hou",
            "Hongyuan Zhan",
            "Akshat Shrivastava",
            "Sid Wang",
            "Aleksandr Livshits",
            "Giulia Fanti",
            "Daniel Lazar"
        ],
        "published": "2023-02-17T18:18:22Z",
        "summary": "In Federated Learning (FL), accessing private client data incurs\ncommunication and privacy costs. As a result, FL deployments commonly\nprefinetune pretrained foundation models on a (large, possibly public) dataset\nthat is held by the central server; they then FL-finetune the model on a\nprivate, federated dataset held by clients. Evaluating prefinetuning dataset\nquality reliably and privately is therefore of high importance. To this end, we\npropose FreD (Federated Private Fr\\'echet Distance) -- a privately computed\ndistance between a prefinetuning dataset and federated datasets. Intuitively,\nit privately computes and compares a Fr\\'echet distance between embeddings\ngenerated by a large language model on both the central (public) dataset and\nthe federated private client data. To make this computation privacy-preserving,\nwe use distributed, differentially-private mean and covariance estimators. We\nshow empirically that FreD accurately predicts the best prefinetuning dataset\nat minimal privacy cost. Altogether, using FreD we demonstrate a\nproof-of-concept for a new approach in private FL training: (1) customize a\nprefinetuning dataset to better match user data (2) prefinetune (3) perform\nFL-finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2302.09042v2.pdf"
    },
    {
        "title": "Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?",
        "authors": [
            "Gonzalo Mart\u00ednez",
            "Lauren Watson",
            "Pedro Reviriego",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Marc Juarez",
            "Rik Sarkar"
        ],
        "published": "2023-02-17T17:39:41Z",
        "summary": "In the span of a few months, generative Artificial Intelligence (AI) tools\nthat can generate realistic images or text have taken the Internet by storm,\nmaking them one of the technologies with fastest adoption ever. Some of these\ngenerative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide\npublic notoriety. Interestingly, these tools are possible because of the\nmassive amount of data (text and images) available on the Internet. The tools\nare trained on massive data sets that are scraped from Internet sites. And now,\nthese generative AI tools are creating massive amounts of new data that are\nbeing fed into the Internet. Therefore, future versions of generative AI tools\nwill be trained with Internet data that is a mix of original and AI-generated\ndata. As time goes on, a mixture of original data and data generated by\ndifferent versions of AI tools will populate the Internet. This raises a few\nintriguing questions: how will future versions of generative AI tools behave\nwhen trained on a mixture of real and AI generated data? Will they evolve with\nthe new data sets or degenerate? Will evolution introduce biases in subsequent\ngenerations of generative AI tools? In this document, we explore these\nquestions and report some very initial simulation results using a simple\nimage-generation AI tool. These results suggest that the quality of the\ngenerated images degrades as more AI-generated data is used for training thus\nsuggesting that generative AI may degenerate. Although these results are\npreliminary and cannot be generalised without further study, they serve to\nillustrate the potential issues of the interaction between generative AI and\nthe Internet.",
        "pdf_link": "https://arxiv.org/pdf/2303.01255v1.pdf"
    },
    {
        "title": "How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",
        "authors": [
            "Fadel M. Megahed",
            "Ying-Ju Chen",
            "Joshua A. Ferris",
            "Sven Knoth",
            "L. Allison Jones-Farmer"
        ],
        "published": "2023-02-17T15:48:37Z",
        "summary": "Generative Artificial Intelligence (AI) models such as OpenAI's ChatGPT have\nthe potential to revolutionize Statistical Process Control (SPC) practice,\nlearning, and research. However, these tools are in the early stages of\ndevelopment and can be easily misused or misunderstood. In this paper, we give\nan overview of the development of Generative AI. Specifically, we explore\nChatGPT's ability to provide code, explain basic concepts, and create knowledge\nrelated to SPC practice, learning, and research. By investigating responses to\nstructured prompts, we highlight the benefits and limitations of the results.\nOur study indicates that the current version of ChatGPT performs well for\nstructured tasks, such as translating code from one language to another and\nexplaining well-known concepts but struggles with more nuanced tasks, such as\nexplaining less widely known terms and creating code from scratch. We find that\nusing new AI tools may help practitioners, educators, and researchers to be\nmore efficient and productive. However, in their current stages of development,\nsome results are misleading and wrong. Overall, the use of generative AI models\nin SPC must be properly validated and used in conjunction with other methods to\nensure accurate results.",
        "pdf_link": "https://arxiv.org/pdf/2302.10916v1.pdf"
    },
    {
        "title": "Massively Multilingual Shallow Fusion with Large Language Models",
        "authors": [
            "Ke Hu",
            "Tara N. Sainath",
            "Bo Li",
            "Nan Du",
            "Yanping Huang",
            "Andrew M. Dai",
            "Yu Zhang",
            "Rodrigo Cabrera",
            "Zhifeng Chen",
            "Trevor Strohman"
        ],
        "published": "2023-02-17T14:46:38Z",
        "summary": "While large language models (LLM) have made impressive progress in natural\nlanguage processing, it remains unclear how to utilize them in improving\nautomatic speech recognition (ASR). In this work, we propose to train a single\nmultilingual language model (LM) for shallow fusion in multiple languages. We\npush the limits of the multilingual LM to cover up to 84 languages by scaling\nup using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When\nthe number of experts increases, GLaM dynamically selects only two at each\ndecoding step to keep the inference computation roughly constant. We then apply\nGLaM to a multilingual shallow fusion task based on a state-of-the-art\nend-to-end model. Compared to a dense LM of similar computation during\ninference, GLaM reduces the WER of an English long-tail test set by 4.4%\nrelative. In a multilingual shallow fusion task, GLaM improves 41 out of 50\nlanguages with an average relative WER reduction of 3.85%, and a maximum\nreduction of 10%. Compared to the baseline model, GLaM achieves an average WER\nreduction of 5.53% over 43 languages.",
        "pdf_link": "https://arxiv.org/pdf/2302.08917v1.pdf"
    },
    {
        "title": "Auditing large language models: a three-layered approach",
        "authors": [
            "Jakob M\u00f6kander",
            "Jonas Schuett",
            "Hannah Rose Kirk",
            "Luciano Floridi"
        ],
        "published": "2023-02-16T18:55:21Z",
        "summary": "Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2302.08500v2.pdf"
    },
    {
        "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
        "authors": [
            "Ansong Ni",
            "Srini Iyer",
            "Dragomir Radev",
            "Ves Stoyanov",
            "Wen-tau Yih",
            "Sida I. Wang",
            "Xi Victoria Lin"
        ],
        "published": "2023-02-16T18:23:22Z",
        "summary": "The advent of large language models trained on code (code LLMs) has led to\nsignificant progress in language-to-code generation. State-of-the-art\napproaches in this area combine LLM decoding with sample pruning and reranking\nusing test cases or heuristics based on the execution results. However, it is\nchallenging to obtain test cases for many real-world language-to-code\napplications, and heuristics cannot well capture the semantic features of the\nexecution results, such as data type and value range, which often indicates the\ncorrectness of the program. In this work, we propose LEVER, a simple approach\nto improve language-to-code generation by learning to verify the generated\nprograms with their execution results. Specifically, we train verifiers to\ndetermine whether a program sampled from the LLMs is correct or not based on\nthe natural language input, the program itself and its execution results. The\nsampled programs are reranked by combining the verification score with the LLM\ngeneration probability, and marginalizing over programs with the same execution\nresults. On four datasets across the domains of table QA, math QA and basic\nPython programming, LEVER consistently improves over the base code LLMs(4.6% to\n10.9% with code-davinci-002) and achieves new state-of-the-art results on all\nof them.",
        "pdf_link": "https://arxiv.org/pdf/2302.08468v3.pdf"
    },
    {
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
        "authors": [
            "Tomer Ullman"
        ],
        "published": "2023-02-16T16:18:03Z",
        "summary": "Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.",
        "pdf_link": "https://arxiv.org/pdf/2302.08399v5.pdf"
    },
    {
        "title": "LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation",
        "authors": [
            "Zhuoyuan Mao",
            "Tetsuji Nakagawa"
        ],
        "published": "2023-02-16T16:05:34Z",
        "summary": "Large-scale language-agnostic sentence embedding models such as LaBSE (Feng\net al., 2022) obtain state-of-the-art performance for parallel sentence\nalignment. However, these large-scale models can suffer from inference speed\nand computation overhead. This study systematically explores learning\nlanguage-agnostic sentence embeddings with lightweight models. We demonstrate\nthat a thin-deep encoder can construct robust low-dimensional sentence\nembeddings for 109 languages. With our proposed distillation methods, we\nachieve further improvements by incorporating knowledge from a teacher model.\nEmpirical results on Tatoeba, United Nations, and BUCC show the effectiveness\nof our lightweight models. We release our lightweight language-agnostic\nsentence embedding models LEALLA on TensorFlow Hub.",
        "pdf_link": "https://arxiv.org/pdf/2302.08387v2.pdf"
    },
    {
        "title": "Do We Still Need Clinical Language Models?",
        "authors": [
            "Eric Lehman",
            "Evan Hernandez",
            "Diwakar Mahajan",
            "Jonas Wulff",
            "Micah J. Smith",
            "Zachary Ziegler",
            "Daniel Nadler",
            "Peter Szolovits",
            "Alistair Johnson",
            "Emily Alsentzer"
        ],
        "published": "2023-02-16T05:08:34Z",
        "summary": "Although recent advances in scaling large language models (LLMs) have\nresulted in improvements on many NLP tasks, it remains unclear whether these\nmodels trained primarily with general web text are the right tool in highly\nspecialized, safety critical domains such as clinical text. Recent results have\nsuggested that LLMs encode a surprising amount of medical knowledge. This\nraises an important question regarding the utility of smaller domain-specific\nlanguage models. With the success of general-domain LLMs, is there still a need\nfor specialized clinical models? To investigate this question, we conduct an\nextensive empirical analysis of 12 language models, ranging from 220M to 175B\nparameters, measuring their performance on 3 different clinical tasks that test\ntheir ability to parse and reason over electronic health records. As part of\nour experiments, we train T5-Base and T5-Large models from scratch on clinical\nnotes from MIMIC III and IV to directly investigate the efficiency of clinical\ntokens. We show that relatively small specialized clinical models substantially\noutperform all in-context learning approaches, even when finetuned on limited\nannotated data. Further, we find that pretraining on clinical tokens allows for\nsmaller, more parameter-efficient models that either match or outperform much\nlarger language models trained on general text. We release the code and the\nmodels used under the PhysioNet Credentialed Health Data license and data use\nagreement.",
        "pdf_link": "https://arxiv.org/pdf/2302.08091v1.pdf"
    },
    {
        "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
        "authors": [
            "Alexander Scarlatos",
            "Andrew Lan"
        ],
        "published": "2023-02-15T22:38:34Z",
        "summary": "Mathematical language in scientific communications and educational scenarios\nis important yet relatively understudied compared to natural languages. Recent\nworks on mathematical language focus either on representing stand-alone\nmathematical expressions, especially in their natural tree format, or\nmathematical reasoning in pre-trained natural language models. Existing works\non jointly modeling and generating natural and mathematical languages simply\ntreat mathematical expressions as text, without accounting for the rigid\nstructural properties of mathematical expressions. In this paper, we propose a\nseries of modifications to existing language models to jointly represent and\ngenerate text and math: representing mathematical expressions as sequences of\nnode tokens in their operator tree format, using math symbol and tree position\nembeddings to preserve the semantic and structural properties of mathematical\nexpressions, and using a constrained decoding method to generate mathematically\nvalid expressions. We ground our modifications in GPT-2, resulting in a model\nMathGPT, and demonstrate that it outperforms baselines on mathematical\nexpression generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2302.07974v1.pdf"
    },
    {
        "title": "Commonsense Reasoning for Conversational AI: A Survey of the State of the Art",
        "authors": [
            "Christopher Richardson",
            "Larry Heck"
        ],
        "published": "2023-02-15T19:55:57Z",
        "summary": "Large, transformer-based pretrained language models like BERT, GPT, and T5\nhave demonstrated a deep understanding of contextual semantics and language\nsyntax. Their success has enabled significant advances in conversational AI,\nincluding the development of open-dialogue systems capable of coherent, salient\nconversations which can answer questions, chat casually, and complete tasks.\nHowever, state-of-the-art models still struggle with tasks that involve higher\nlevels of reasoning - including commonsense reasoning that humans find trivial.\nThis paper presents a survey of recent conversational AI research focused on\ncommonsense reasoning. The paper lists relevant training datasets and describes\nthe primary approaches to include commonsense in conversational AI. The paper\nalso discusses benchmarks used for evaluating commonsense in conversational AI\nproblems. Finally, the paper presents preliminary observations of the limited\ncommonsense capabilities of two state-of-the-art open dialogue models,\nBlenderBot3 and LaMDA, and its negative effect on natural interactions. These\nobservations further motivate research on commonsense reasoning in\nconversational AI.",
        "pdf_link": "https://arxiv.org/pdf/2302.07926v1.pdf"
    },
    {
        "title": "Speculative Decoding with Big Little Decoder",
        "authors": [
            "Sehoon Kim",
            "Karttikeya Mangalam",
            "Suhong Moon",
            "Jitendra Malik",
            "Michael W. Mahoney",
            "Amir Gholami",
            "Kurt Keutzer"
        ],
        "published": "2023-02-15T18:55:29Z",
        "summary": "The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment and makes them prohibitively expensive for various real-time\napplications. The inference latency is further exacerbated by autoregressive\ngenerative tasks, as models need to run iteratively to generate tokens\nsequentially without leveraging token-level parallelization. To address this,\nwe propose Big Little Decoder (BiLD), a framework that can improve inference\nefficiency and latency for a wide range of text generation applications. The\nBiLD framework contains two models with different sizes that collaboratively\ngenerate text. The small model runs autoregressively to generate text with a\nlow inference cost, and the large model is only invoked occasionally to refine\nthe small model's inaccurate predictions in a non-autoregressive manner. To\ncoordinate the small and large models, BiLD introduces two simple yet effective\npolicies: (1) the fallback policy that determines when to hand control over to\nthe large model; and (2) the rollback policy that determines when the large\nmodel needs to correct the small model's inaccurate predictions. To evaluate\nour framework across different tasks and models, we apply BiLD to various text\ngeneration scenarios encompassing machine translation on IWSLT 2017 De-En and\nWMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4\nGPU, our framework achieves a speedup of up to 2.12x speedup with minimal\ngeneration quality degradation. Furthermore, our framework is fully\nplug-and-play and can be applied without any modifications in the training\nprocess or model architecture. Our code is open-sourced",
        "pdf_link": "https://arxiv.org/pdf/2302.07863v4.pdf"
    },
    {
        "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
        "authors": [
            "Marjan Ghazvininejad",
            "Hila Gonen",
            "Luke Zettlemoyer"
        ],
        "published": "2023-02-15T18:46:42Z",
        "summary": "Large language models (LLMs) demonstrate remarkable machine translation (MT)\nabilities via prompting, even though they were not explicitly trained for this\ntask. However, even given the incredible quantities of data they are trained\non, LLMs can struggle to translate inputs with rare words, which are common in\nlow resource or domain transfer scenarios. We show that LLM prompting can\nprovide an effective solution for rare words as well, by using prior knowledge\nfrom bilingual dictionaries to provide control hints in the prompts. We propose\na novel method, DiPMT, that provides a set of possible translations for a\nsubset of the input words, thereby enabling fine-grained phrase-level prompted\ncontrol of the LLM. Extensive experiments show that DiPMT outperforms the\nbaseline both in low-resource MT, as well as for out-of-domain MT. We further\nprovide a qualitative analysis of the benefits and limitations of this\napproach, including the overall level of controllability that is achieved.",
        "pdf_link": "https://arxiv.org/pdf/2302.07856v1.pdf"
    },
    {
        "title": "A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning",
        "authors": [
            "Zhisheng Tang",
            "Mayank Kejriwal"
        ],
        "published": "2023-02-15T05:04:49Z",
        "summary": "We conduct a pilot study selectively evaluating the cognitive abilities\n(decision making and spatial reasoning) of two recently released generative\ntransformer models, ChatGPT and DALL-E 2. Input prompts were constructed\nfollowing neutral a priori guidelines, rather than adversarial intent. Post hoc\nqualitative analysis of the outputs shows that DALL-E 2 is able to generate at\nleast one correct image for each spatial reasoning prompt, but most images\ngenerated are incorrect (even though the model seems to have a clear\nunderstanding of the objects mentioned in the prompt). Similarly, in evaluating\nChatGPT on the rationality axioms developed under the classical Von\nNeumann-Morgenstern utility theorem, we find that, although it demonstrates\nsome level of rational decision-making, many of its decisions violate at least\none of the axioms even under reasonable constructions of preferences, bets, and\ndecision-making prompts. ChatGPT's outputs on such problems generally tended to\nbe unpredictable: even as it made irrational decisions (or employed an\nincorrect reasoning process) for some simpler decision-making problems, it was\nable to draw correct conclusions for more complex bet structures. We briefly\ncomment on the nuances and challenges involved in scaling up such a 'cognitive'\nevaluation or conducting it with a closed set of answer keys ('ground truth'),\ngiven that these models are inherently generative and open-ended in responding\nto prompts.",
        "pdf_link": "https://arxiv.org/pdf/2302.09068v1.pdf"
    },
    {
        "title": "Conversational AI-Powered Design: ChatGPT as Designer, User, and Product",
        "authors": [
            "A. Baki Kocaballi"
        ],
        "published": "2023-02-15T00:14:17Z",
        "summary": "The recent advancements in Large Language Models (LLMs), particularly\nconversational LLMs like ChatGPT, have prompted changes in a range of fields,\nincluding design. This study aims to examine the capabilities of ChatGPT in a\nhuman-centered design process. To this end, a hypothetical design project was\nconducted, where ChatGPT was utilized to generate personas, simulate interviews\nwith fictional users, create new design ideas, simulate usage scenarios and\nconversations between an imaginary prototype and fictional users, and lastly\nevaluate user experience. The results show that ChatGPT effectively performed\nthe tasks assigned to it as a designer, user, or product, providing mostly\nappropriate responses. The study does, however, highlight some drawbacks such\nas forgotten information, partial responses, and a lack of output diversity.\nThe paper explains the potential benefits and limitations of using\nconversational LLMs in design, discusses its implications, and suggests\ndirections for future research in this rapidly evolving area.",
        "pdf_link": "https://arxiv.org/pdf/2302.07406v1.pdf"
    },
    {
        "title": "Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models",
        "authors": [
            "Shrimai Prabhumoye",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2023-02-14T23:00:42Z",
        "summary": "Pretrained large language models have become indispensable for solving\nvarious natural language processing (NLP) tasks. However, safely deploying them\nin real world applications is challenging because they generate toxic content.\nTo address this challenge, we propose two novel pretraining data augmentation\nstrategies that significantly reduce model toxicity without compromising its\nutility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data\nto the pretraining samples, and (2) INST: adds instructions to those samples\nindicating their toxicity. Our results indicate that our best performing\nstrategy (INST) substantially reduces the toxicity probability up to 61% while\npreserving the accuracy on five benchmark NLP tasks as well as improving AUC\nscores on four bias detection tasks by 1.3%. We also demonstrate the\ngeneralizability of our techniques by scaling the number of training samples\nand the number of model parameters.",
        "pdf_link": "https://arxiv.org/pdf/2302.07388v1.pdf"
    },
    {
        "title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models",
        "authors": [
            "Rafal Kocielnik",
            "Shrimai Prabhumoye",
            "Vivian Zhang",
            "Roy Jiang",
            "R. Michael Alvarez",
            "Anima Anandkumar"
        ],
        "published": "2023-02-14T22:07:57Z",
        "summary": "Pretrained Language Models (PLMs) harbor inherent social biases that can\nresult in harmful real-world implications. Such social biases are measured\nthrough the probability values that PLMs output for different social groups and\nattributes appearing in a set of test sentences. However, bias testing is\ncurrently cumbersome since the test sentences are generated either from a\nlimited set of manual templates or need expensive crowd-sourcing. We instead\npropose using ChatGPT for the controllable generation of test sentences, given\nany arbitrary user-specified combination of social groups and attributes\nappearing in the test sentences. When compared to template-based methods, our\napproach using ChatGPT for test sentence generation is superior in detecting\nsocial bias, especially in challenging settings such as intersectional biases.\nWe present an open-source comprehensive bias testing framework (BiasTestGPT),\nhosted on HuggingFace, that can be plugged into any open-source PLM for bias\ntesting. User testing with domain experts from various fields has shown their\ninterest in being able to test modern AI for social biases. Our tool has\nsignificantly improved their awareness of such biases in PLMs, proving to be\nlearnable and user-friendly. We thus enable seamless open-ended social bias\ntesting of PLMs by domain experts through an automatic large-scale generation\nof diverse test sentences for any combination of social categories and\nattributes.",
        "pdf_link": "https://arxiv.org/pdf/2302.07371v3.pdf"
    },
    {
        "title": "ScatterShot: Interactive In-context Example Curation for Text Transformation",
        "authors": [
            "Tongshuang Wu",
            "Hua Shen",
            "Daniel S. Weld",
            "Jeffrey Heer",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-02-14T21:13:31Z",
        "summary": "The in-context learning capabilities of LLMs like GPT-3 allow annotators to\ncustomize an LLM to their specific tasks with a small number of examples.\nHowever, users tend to include only the most obvious patterns when crafting\nexamples, resulting in underspecified in-context functions that fall short on\nunseen cases. Further, it is hard to know when \"enough\" examples have been\nincluded even for known patterns. In this work, we present ScatterShot, an\ninteractive system for building high-quality demonstration sets for in-context\nlearning. ScatterShot iteratively slices unlabeled data into task-specific\npatterns, samples informative inputs from underexplored or not-yet-saturated\nslices in an active learning manner, and helps users label more efficiently\nwith the help of an LLM and the current example set. In simulation studies on\ntwo text perturbation scenarios, ScatterShot sampling improves the resulting\nfew-shot functions by 4-5 percentage points over random sampling, with less\nvariance as more examples are added. In a user study, ScatterShot greatly helps\nusers in covering different patterns in the input space and labeling in-context\nexamples more efficiently, resulting in better in-context learning and less\nuser effort.",
        "pdf_link": "https://arxiv.org/pdf/2302.07346v1.pdf"
    },
    {
        "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
        "authors": [
            "Sheng Wang",
            "Zihao Zhao",
            "Xi Ouyang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-02-14T18:54:06Z",
        "summary": "Large language models (LLMs) have recently demonstrated their potential in\nclinical applications, providing valuable medical knowledge and advice. For\nexample, a large dialog LLM like ChatGPT has successfully passed part of the US\nmedical licensing exam. However, LLMs currently have difficulty processing\nimages, making it challenging to interpret information from medical images,\nwhich are rich in information that supports clinical decisions. On the other\nhand, computer-aided diagnosis (CAD) networks for medical images have seen\nsignificant success in the medical field by using advanced deep-learning\nalgorithms to support clinical decision-making. This paper presents a method\nfor integrating LLMs into medical-image CAD networks. The proposed framework\nuses LLMs to enhance the output of multiple CAD networks, such as diagnosis\nnetworks, lesion segmentation networks, and report generation networks, by\nsummarizing and reorganizing the information presented in natural language text\nformat. The goal is to merge the strengths of LLMs' medical domain knowledge\nand logical reasoning with the vision understanding capability of existing\nmedical-image CAD models to create a more user-friendly and understandable\nsystem for patients compared to conventional CAD systems. In the future, LLM's\nmedical knowledge can be also used to improve the performance of vision-based\nmedical-image CAD models.",
        "pdf_link": "https://arxiv.org/pdf/2302.07257v1.pdf"
    },
    {
        "title": "Few-shot learning approaches for classifying low resource domain specific software requirements",
        "authors": [
            "Anmol Nayak",
            "Hari Prasad Timmapathini",
            "Vidhya Murali",
            "Atul Anil Gohad"
        ],
        "published": "2023-02-14T10:19:23Z",
        "summary": "With the advent of strong pre-trained natural language processing models like\nBERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune\nthese models to their niche use cases has drastically reduced (typically to a\nfew hundred annotated samples for achieving a reasonable performance). However,\nthe availability of even a few hundred annotated samples may not always be\nguaranteed in low resource domains like automotive, which often limits the\nusage of such deep learning models in an industrial setting. In this paper we\naim to address the challenge of fine-tuning such pre-trained models with only a\nfew annotated samples, also known as Few-shot learning. Our experiments focus\non evaluating the performance of a diverse set of algorithms and methodologies\nto achieve the task of classifying BOSCH automotive domain textual software\nrequirements into 3 categories, while utilizing only 15 annotated samples per\ncategory for fine-tuning. We find that while SciBERT and DeBERTa based models\ntend to be the most accurate at 15 training samples, their performance\nimprovement scales minimally as the number of annotated samples is increased to\n50 in comparison to Siamese and T5 based models.",
        "pdf_link": "https://arxiv.org/pdf/2302.06951v1.pdf"
    },
    {
        "title": "Learning gain differences between ChatGPT and human tutor generated algebra hints",
        "authors": [
            "Zachary A. Pardos",
            "Shreya Bhandari"
        ],
        "published": "2023-02-14T07:20:48Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to\nthe frontiers of practical consumer use and leading industries to re-evaluate\nhow they allocate resources for content production. Authoring of open\neducational resources and hint content within adaptive tutoring systems is\nlabor intensive. Should LLMs like ChatGPT produce educational content on par\nwith human-authored content, the implications would be significant for further\nscaling of computer tutoring system approaches. In this paper, we conduct the\nfirst learning gain evaluation of ChatGPT by comparing the efficacy of its\nhints with hints authored by human tutors with 77 participants across two\nalgebra topic areas, Elementary Algebra and Intermediate Algebra. We find that\n70% of hints produced by ChatGPT passed our manual quality checks and that both\nhuman and ChatGPT conditions produced positive learning gains. However, gains\nwere only statistically significant for human tutor created hints. Learning\ngains from human-created hints were substantially and statistically\nsignificantly higher than ChatGPT hints in both topic areas, though ChatGPT\nparticipants in the Intermediate Algebra experiment were near ceiling and not\neven with the control at pre-test. We discuss the limitations of our study and\nsuggest several future directions for the field. Problem and hint content used\nin the experiment is provided for replicability.",
        "pdf_link": "https://arxiv.org/pdf/2302.06871v1.pdf"
    },
    {
        "title": "Machine Learning Model Attribution Challenge",
        "authors": [
            "Elizabeth Merkhofer",
            "Deepesh Chaudhari",
            "Hyrum S. Anderson",
            "Keith Manville",
            "Lily Wong",
            "Jo\u00e3o Gante"
        ],
        "published": "2023-02-13T22:05:27Z",
        "summary": "We present the findings of the Machine Learning Model Attribution Challenge.\nFine-tuned machine learning models may derive from other trained models without\nobvious attribution characteristics. In this challenge, participants identify\nthe publicly-available base models that underlie a set of anonymous, fine-tuned\nlarge language models (LLMs) using only textual output of the models.\nContestants aim to correctly attribute the most fine-tuned models, with ties\nbroken in the favor of contestants whose solutions use fewer calls to the\nfine-tuned models' API. The most successful approaches were manual, as\nparticipants observed similarities between model outputs and developed\nattribution heuristics based on public documentation of the base models, though\nseveral teams also submitted automated, statistical solutions.",
        "pdf_link": "https://arxiv.org/pdf/2302.06716v3.pdf"
    },
    {
        "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
        "authors": [
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Matthew Marquez",
            "Alberto Olmo",
            "Subbarao Kambhampati"
        ],
        "published": "2023-02-13T21:37:41Z",
        "summary": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) how good LLMs are by themselves in\ngenerating and validating simple plans in commonsense planning tasks (of the\ntype that humans are generally quite good at) and (2) how good LLMs are in\nbeing a source of heuristic guidance for other agents--either AI planners or\nhuman planners--in their planning tasks. To investigate these questions in a\nsystematic rather than anecdotal manner, we start by developing a benchmark\nsuite based on the kinds of domains employed in the International Planning\nCompetition. On this benchmark, we evaluate LLMs in three modes: autonomous,\nheuristic and human-in-the-loop. Our results show that LLM's ability to\nautonomously generate executable plans is quite meager, averaging only about 3%\nsuccess rate. The heuristic and human-in-the-loop modes show slightly more\npromise. In addition to these results, we also make our benchmark and\nevaluation tools available to support investigations by research community.",
        "pdf_link": "https://arxiv.org/pdf/2302.06706v1.pdf"
    },
    {
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models",
        "authors": [
            "Yuqing Du",
            "Olivia Watkins",
            "Zihan Wang",
            "C\u00e9dric Colas",
            "Trevor Darrell",
            "Pieter Abbeel",
            "Abhishek Gupta",
            "Jacob Andreas"
        ],
        "published": "2023-02-13T21:16:03Z",
        "summary": "Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps://github.com/yuqingd/ellm.",
        "pdf_link": "https://arxiv.org/pdf/2302.06692v2.pdf"
    },
    {
        "title": "Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning",
        "authors": [
            "Maximilian Mozes",
            "Tolga Bolukbasi",
            "Ann Yuan",
            "Frederick Liu",
            "Nithum Thain",
            "Lucas Dixon"
        ],
        "published": "2023-02-13T18:54:58Z",
        "summary": "Pretrained large language models (LLMs) are able to solve a wide variety of\ntasks through transfer learning. Various explainability methods have been\ndeveloped to investigate their decision making process. TracIn (Pruthi et al.,\n2020) is one such gradient-based method which explains model inferences based\non the influence of training examples. In this paper, we explore the use of\nTracIn to improve model performance in the parameter-efficient tuning (PET)\nsetting. We develop conversational safety classifiers via the prompt-tuning PET\nmethod and show how the unique characteristics of the PET regime enable TracIn\nto identify the cause for certain misclassifications by LLMs. We develop a new\nmethodology for using gradient-based explainability techniques to improve model\nperformance, G-BAIR: gradient-based automated iterative recovery. We show that\nG-BAIR can recover LLM performance on benchmarks after manually corrupting\ntraining labels. This suggests that influence methods like TracIn can be used\nto automatically perform data cleaning, and introduces the potential for\ninteractive debugging and relabeling for PET-based transfer learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2302.06598v1.pdf"
    },
    {
        "title": "Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi",
            "Arie van Deursen"
        ],
        "published": "2023-02-13T18:00:44Z",
        "summary": "Previous work has shown that Large Language Models are susceptible to\nso-called data extraction attacks. This allows an attacker to extract a sample\nthat was contained in the training data, which has massive privacy\nimplications. The construction of data extraction attacks is challenging,\ncurrent attacks are quite inefficient, and there exists a significant gap in\nthe extraction capabilities of untargeted attacks and memorization. Thus,\ntargeted attacks are proposed, which identify if a given sample from the\ntraining data, is extractable from a model. In this work, we apply a targeted\ndata extraction attack to the SATML2023 Language Model Training Data Extraction\nChallenge. We apply a two-step approach. In the first step, we maximise the\nrecall of the model and are able to extract the suffix for 69% of the samples.\nIn the second step, we use a classifier-based Membership Inference Attack on\nthe generations. Our AutoSklearn classifier achieves a precision of 0.841. The\nfull approach reaches a score of 0.405 recall at a 10% false positive rate,\nwhich is an improvement of 34% over the baseline of 0.301.",
        "pdf_link": "https://arxiv.org/pdf/2302.07735v1.pdf"
    },
    {
        "title": "Diminished Diversity-of-Thought in a Standard Large Language Model",
        "authors": [
            "Peter S. Park",
            "Philipp Schoenegger",
            "Chongyang Zhu"
        ],
        "published": "2023-02-13T17:57:50Z",
        "summary": "We test whether Large Language Models (LLMs) can be used to simulate human\nparticipants in social-science studies. To do this, we run replications of 14\nstudies from the Many Labs 2 replication project with OpenAI's text-davinci-003\nmodel, colloquially known as GPT3.5. Based on our pre-registered analyses, we\nfind that among the eight studies we could analyse, our GPT sample replicated\n37.5% of the original results and 37.5% of the Many Labs 2 results. However, we\nwere unable to analyse the remaining six studies due to an unexpected\nphenomenon we call the \"correct answer\" effect. Different runs of GPT3.5\nanswered nuanced questions probing political orientation, economic preference,\njudgement, and moral philosophy with zero or near-zero variation in responses:\nwith the supposedly \"correct answer.\" In one exploratory follow-up study, we\nfound that a \"correct answer\" was robust to changing the demographic details\nthat precede the prompt. In another, we found that most but not all \"correct\nanswers\" were robust to changing the order of answer choices. One of our most\nstriking findings occurred in our replication of the Moral Foundations Theory\nsurvey results, where we found GPT3.5 identifying as a political conservative\nin 99.6% of the cases, and as a liberal in 99.3% of the cases in the\nreverse-order condition. However, both self-reported 'GPT conservatives' and\n'GPT liberals' showed right-leaning moral foundations. Our results cast doubts\non the validity of using LLMs as a general replacement for human participants\nin the social sciences. Our results also raise concerns that a hypothetical\nAI-led future may be subject to a diminished diversity-of-thought.",
        "pdf_link": "https://arxiv.org/pdf/2302.07267v6.pdf"
    },
    {
        "title": "Implications of the Convergence of Language and Vision Model Geometries",
        "authors": [
            "Jiaang Li",
            "Yova Kementchedjhieva",
            "Anders S\u00f8gaard"
        ],
        "published": "2023-02-13T17:55:54Z",
        "summary": "Large-scale pretrained language models (LMs) are said to ``lack the ability\nto connect [their] utterances to the world'' (Bender and Koller, 2020). If so,\nwe would expect LM representations to be unrelated to representations in\ncomputer vision models. To investigate this, we present an empirical evaluation\nacross three different LMs (BERT, GPT2, and OPT) and three computer vision\nmodels (VMs, including ResNet, SegFormer, and MAE). Our experiments show that\nLMs converge towards representations that are partially isomorphic to those of\nVMs, with dispersion, and polysemy both factoring into the alignability of\nvision and language spaces. We discuss the implications of this finding.",
        "pdf_link": "https://arxiv.org/pdf/2302.06555v1.pdf"
    },
    {
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "authors": [
            "Max Sch\u00e4fer",
            "Sarah Nadi",
            "Aryaz Eghbali",
            "Frank Tip"
        ],
        "published": "2023-02-13T17:13:41Z",
        "summary": "Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. Large Language Models (LLMs) have recently been applied to this\nproblem, utilizing additional training or few-shot learning on examples of\nexisting tests. This paper presents a large-scale empirical evaluation on the\neffectiveness of LLMs for automated unit test generation without additional\ntraining or manual effort, providing the LLM with the signature and\nimplementation of the function under test, along with usage examples extracted\nfrom documentation. We also attempt to repair failed generated tests by\nre-prompting the model with the failing test and error message. We implement\nour approach in TestPilot, a test generation tool for JavaScript that\nautomatically generates unit tests for all API functions in an npm package. We\nevaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a\ntotal of 1,684 API functions. The generated tests achieve a median statement\ncoverage of 70.2% and branch coverage of 52.8%, significantly improving on\nNessie, a recent feedback-directed JavaScript test generation technique, which\nachieves only 51.3% statement coverage and 25.6% branch coverage. We also find\nthat 92.8% of TestPilot's generated tests have no more than 50% similarity with\nexisting tests (as measured by normalized edit distance), with none of them\nbeing exact copies. Finally, we run TestPilot with two additional LLMs,\nOpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, we\nobserved similar results with the former (68.2% median statement coverage), and\nsomewhat worse results with the latter (54.0% median statement coverage),\nsuggesting that the effectiveness of the approach is influenced by the size and\ntraining set of the LLM, but does not fundamentally depend on the specific\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2302.06527v4.pdf"
    },
    {
        "title": "Can GPT-3 Perform Statutory Reasoning?",
        "authors": [
            "Andrew Blair-Stanek",
            "Nils Holzenberger",
            "Benjamin Van Durme"
        ],
        "published": "2023-02-13T04:56:11Z",
        "summary": "Statutory reasoning is the task of reasoning with facts and statutes, which\nare rules written in natural language by a legislature. It is a basic legal\nskill. In this paper we explore the capabilities of the most capable GPT-3\nmodel, text-davinci-003, on an established statutory-reasoning dataset called\nSARA. We consider a variety of approaches, including dynamic few-shot\nprompting, chain-of-thought prompting, and zero-shot prompting. While we\nachieve results with GPT-3 that are better than the previous best published\nresults, we also identify several types of clear errors it makes. We\ninvestigate why these errors happen. We discover that GPT-3 has imperfect prior\nknowledge of the actual U.S. statutes on which SARA is based. More importantly,\nwe create simple synthetic statutes, which GPT-3 is guaranteed not to have seen\nduring training. We find GPT-3 performs poorly at answering straightforward\nquestions about these simple synthetic statutes.",
        "pdf_link": "https://arxiv.org/pdf/2302.06100v2.pdf"
    },
    {
        "title": "Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection",
        "authors": [
            "Nicolas Harvey Chapman",
            "Feras Dayoub",
            "Will Browne",
            "Christopher Lehnert"
        ],
        "published": "2023-02-13T00:46:34Z",
        "summary": "Unsupervised Domain Adaptive Object Detection (UDA-OD) uses unlabelled data\nto improve the reliability of robotic vision systems in open-world\nenvironments. Previous approaches to UDA-OD based on self-training have been\neffective in overcoming changes in the general appearance of images. However,\nshifts in a robot's deployment environment can also impact the likelihood that\ndifferent objects will occur, termed class distribution shift. Motivated by\nthis, we propose a framework for explicitly addressing class distribution shift\nto improve pseudo-label reliability in self-training. Our approach uses the\ndomain invariance and contextual understanding of a pre-trained joint vision\nand language model to predict the class distribution of unlabelled data. By\naligning the class distribution of pseudo-labels with this prediction, we\nprovide weak supervision of pseudo-label accuracy. To further account for low\nquality pseudo-labels early in self-training, we propose an approach to\ndynamically adjust the number of pseudo-labels per image based on model\nconfidence. Our method outperforms state-of-the-art approaches on several\nbenchmarks, including a 4.7 mAP improvement when facing challenging class\ndistribution shift.",
        "pdf_link": "https://arxiv.org/pdf/2302.06039v2.pdf"
    },
    {
        "title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models",
        "authors": [
            "Shyam Sudhakaran",
            "Miguel Gonz\u00e1lez-Duque",
            "Claire Glanois",
            "Matthias Freiberger",
            "Elias Najarro",
            "Sebastian Risi"
        ],
        "published": "2023-02-12T19:12:24Z",
        "summary": "Procedural Content Generation (PCG) is a technique to generate complex and\ndiverse environments in an automated way. However, while generating content\nwith PCG methods is often straightforward, generating meaningful content that\nreflects specific intentions and constraints remains challenging. Furthermore,\nmany PCG algorithms lack the ability to generate content in an open-ended\nmanner. Recently, Large Language Models (LLMs) have shown to be incredibly\neffective in many diverse domains. These trained LLMs can be fine-tuned,\nre-using information and accelerating training for new tasks. Here, we\nintroduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game\nlevels, in our case Super Mario Bros levels. MarioGPT can not only generate\ndiverse levels, but can be text-prompted for controllable level generation,\naddressing one of the key challenges of current PCG techniques. As far as we\nknow, MarioGPT is the first text-to-level model and combined with novelty\nsearch it enables the generation of diverse levels with varying play-style\ndynamics (i.e. player paths) and the open-ended discovery of an increasingly\ndiverse range of content. Code available at\nhttps://github.com/shyamsn97/mario-gpt.",
        "pdf_link": "https://arxiv.org/pdf/2302.05981v3.pdf"
    },
    {
        "title": "Level Generation Through Large Language Models",
        "authors": [
            "Graham Todd",
            "Sam Earle",
            "Muhammad Umair Nasir",
            "Michael Cerny Green",
            "Julian Togelius"
        ],
        "published": "2023-02-11T23:34:42Z",
        "summary": "Large Language Models (LLMs) are powerful tools, capable of leveraging their\ntraining on natural language to write stories, generate code, and answer\nquestions. But can they generate functional video game levels? Game levels,\nwith their complex functional constraints and spatial relationships in more\nthan one dimension, are very different from the kinds of data an LLM typically\nsees during training. Datasets of game levels are also hard to come by,\npotentially taxing the abilities of these data-hungry models. We investigate\nthe use of LLMs to generate levels for the game Sokoban, finding that LLMs are\nindeed capable of doing so, and that their performance scales dramatically with\ndataset size. We also perform preliminary experiments on controlling LLM level\ngenerators and discuss promising areas for future work.",
        "pdf_link": "https://arxiv.org/pdf/2302.05817v2.pdf"
    },
    {
        "title": "Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes",
        "authors": [
            "Shruthi Chari",
            "Prasant Acharya",
            "Daniel M. Gruen",
            "Olivia Zhang",
            "Elif K. Eyigoz",
            "Mohamed Ghalwash",
            "Oshani Seneviratne",
            "Fernando Suarez Saiz",
            "Pablo Meyer",
            "Prithwish Chakraborty",
            "Deborah L. McGuinness"
        ],
        "published": "2023-02-11T18:07:11Z",
        "summary": "Medical experts may use Artificial Intelligence (AI) systems with greater\ntrust if these are supported by contextual explanations that let the\npractitioner connect system inferences to their context of use. However, their\nimportance in improving model usage and understanding has not been extensively\nstudied. Hence, we consider a comorbidity risk prediction scenario and focus on\ncontexts regarding the patients clinical state, AI predictions about their risk\nof complications, and algorithmic explanations supporting the predictions. We\nexplore how relevant information for such dimensions can be extracted from\nMedical guidelines to answer typical questions from clinical practitioners. We\nidentify this as a question answering (QA) task and employ several\nstate-of-the-art LLMs to present contexts around risk prediction model\ninferences and evaluate their acceptability. Finally, we study the benefits of\ncontextual explanations by building an end-to-end AI pipeline including data\ncohorting, AI risk modeling, post-hoc model explanations, and prototyped a\nvisual dashboard to present the combined insights from different context\ndimensions and data sources, while predicting and identifying the drivers of\nrisk of Chronic Kidney Disease - a common type-2 diabetes comorbidity. All of\nthese steps were performed in engagement with medical experts, including a\nfinal evaluation of the dashboard results by an expert medical panel. We show\nthat LLMs, in particular BERT and SciBERT, can be readily deployed to extract\nsome relevant explanations to support clinical usage. To understand the\nvalue-add of the contextual explanations, the expert panel evaluated these\nregarding actionable insights in the relevant clinical setting. Overall, our\npaper is one of the first end-to-end analyses identifying the feasibility and\nbenefits of contextual explanations in a real-world clinical use case.",
        "pdf_link": "https://arxiv.org/pdf/2302.05752v1.pdf"
    },
    {
        "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
        "authors": [
            "Daniel Kang",
            "Xuechen Li",
            "Ion Stoica",
            "Carlos Guestrin",
            "Matei Zaharia",
            "Tatsunori Hashimoto"
        ],
        "published": "2023-02-11T15:57:44Z",
        "summary": "Recent advances in instruction-following large language models (LLMs) have\nled to dramatic improvements in a range of NLP tasks. Unfortunately, we find\nthat the same improved capabilities amplify the dual-use risks for malicious\npurposes of these models. Dual-use is difficult to prevent as\ninstruction-following capabilities now enable standard attacks from computer\nsecurity. The capabilities of these instruction-following LLMs provide strong\neconomic incentives for dual-use by malicious actors. In particular, we show\nthat instruction-following LLMs can produce targeted malicious content,\nincluding hate speech and scams, bypassing in-the-wild defenses implemented by\nLLM API vendors. Our analysis shows that this content can be generated\neconomically and at cost likely lower than with human effort alone. Together,\nour findings suggest that LLMs will increasingly attract more sophisticated\nadversaries and attacks, and addressing these attacks may require new\napproaches to mitigations.",
        "pdf_link": "https://arxiv.org/pdf/2302.05733v1.pdf"
    },
    {
        "title": "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
        "authors": [
            "Fan Huang",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "published": "2023-02-11T03:13:54Z",
        "summary": "Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.",
        "pdf_link": "https://arxiv.org/pdf/2302.07736v2.pdf"
    },
    {
        "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models",
        "authors": [
            "Renat Aksitov",
            "Chung-Ching Chang",
            "David Reitter",
            "Siamak Shakeri",
            "Yunhsuan Sung"
        ],
        "published": "2023-02-11T02:43:34Z",
        "summary": "Despite recent progress, it has been difficult to prevent semantic\nhallucinations in generative Large Language Models. One common solution to this\nis augmenting LLMs with a retrieval system and making sure that the generated\noutput is attributable to the retrieved information. Given this new added\nconstraint, it is plausible to expect that the overall quality of the output\nwill be affected, for example, in terms of fluency. Can scaling language models\nhelp?\n  Here we examine the relationship between fluency and attribution in LLMs\nprompted with retrieved evidence in knowledge-heavy dialog settings. Our\nexperiments were implemented with a set of auto-metrics that are aligned with\nhuman preferences. They were used to evaluate a large set of generations,\nproduced under varying parameters of LLMs and supplied context.\n  We show that larger models tend to do much better in both fluency and\nattribution, and that (naively) using top-k retrieval versus top-1 retrieval\nimproves attribution but hurts fluency. We next propose a recipe that could\nallow smaller models to both close the gap with larger models and preserve the\nbenefits of top-k retrieval while avoiding its drawbacks.",
        "pdf_link": "https://arxiv.org/pdf/2302.05578v2.pdf"
    },
    {
        "title": "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models",
        "authors": [
            "Hrishikesh Viswanath",
            "Tianyi Zhang"
        ],
        "published": "2023-02-10T20:54:10Z",
        "summary": "Studies have shown that large pretrained language models exhibit biases\nagainst social groups based on race, gender etc, which they inherit from the\ndatasets they are trained on. Various researchers have proposed mathematical\ntools for quantifying and identifying these biases. There have been methods\nproposed to mitigate such biases. In this paper, we present a comprehensive\nquantitative evaluation of different kinds of biases such as race, gender,\nethnicity, age etc. exhibited by popular pretrained language models such as\nBERT, GPT-2 etc. and also present a toolkit that provides plug-and-play\ninterfaces to connect mathematical tools to identify biases with large\npretrained language models such as BERT, GPT-2 etc. and also present users with\nthe opportunity to test custom models against these metrics. The toolkit also\nallows users to debias existing and custom models using the debiasing\ntechniques proposed so far. The toolkit is available at\nhttps://github.com/HrishikeshVish/Fairpy.",
        "pdf_link": "https://arxiv.org/pdf/2302.05508v1.pdf"
    },
    {
        "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
        "authors": [
            "Jingxuan He",
            "Martin Vechev"
        ],
        "published": "2023-02-10T15:28:55Z",
        "summary": "Large language models (large LMs) are increasingly trained on massive\ncodebases and used to generate code. However, LMs lack awareness of security\nand are found to frequently produce unsafe code. This work studies the security\nof LMs along two important axes: (i) security hardening, which aims to enhance\nLMs' reliability in generating secure code, and (ii) adversarial testing, which\nseeks to evaluate LMs' security at an adversarial standpoint. We address both\nof these by formulating a new security task called controlled code generation.\nThe task is parametric and takes as input a binary property to guide the LM to\ngenerate secure or unsafe code, while preserving the LM's capability of\ngenerating functionally correct code. We propose a novel learning-based\napproach called SVEN to solve this task. SVEN leverages property-specific\ncontinuous vectors to guide program generation towards the given property,\nwithout modifying the LM's weights. Our training procedure optimizes these\ncontinuous vectors by enforcing specialized loss terms on different regions of\ncode, using a high-quality dataset carefully curated by us. Our extensive\nevaluation shows that SVEN is highly effective in achieving strong security\ncontrol. For instance, a state-of-the-art CodeGen LM with 2.7B parameters\ngenerates secure code for 59.1% of the time. When we employ SVEN to perform\nsecurity hardening (or adversarial testing) on this LM, the ratio is\nsignificantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN\nclosely matches the original LMs in functional correctness.",
        "pdf_link": "https://arxiv.org/pdf/2302.05319v4.pdf"
    },
    {
        "title": "Translating Natural Language to Planning Goals with Large-Language Models",
        "authors": [
            "Yaqi Xie",
            "Chen Yu",
            "Tongyao Zhu",
            "Jinbin Bai",
            "Ze Gong",
            "Harold Soh"
        ],
        "published": "2023-02-10T09:17:52Z",
        "summary": "Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use.",
        "pdf_link": "https://arxiv.org/pdf/2302.05128v1.pdf"
    },
    {
        "title": "In-Context Learning with Many Demonstration Examples",
        "authors": [
            "Mukai Li",
            "Shansan Gong",
            "Jiangtao Feng",
            "Yiheng Xu",
            "Jun Zhang",
            "Zhiyong Wu",
            "Lingpeng Kong"
        ],
        "published": "2023-02-09T20:53:12Z",
        "summary": "Large pre-training language models (PLMs) have shown promising in-context\nlearning abilities. However, due to the backbone transformer architecture,\nexisting PLMs are bottlenecked by the memory and computational cost when\nscaling up to a large context size, leaving instruction tuning and in-context\nlearning of many demonstration examples, as well as long-range language\nmodeling under-explored. In this study, we propose a long-range language model\nEVALM based on an efficient transformer mechanism. EVALM is trained with 8k\ntokens per batch line and can test up to 256k-lengthed contexts with\nextrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on\nEVALM, we scale up the size of examples efficiently in both instruction tuning\nand in-context learning to explore the boundary of the benefits from more\nannotated data. Experimental results on a diverse set of tasks show that EVALM\nachieves 4.1% higher accuracy on average, and the average length of achieving\nthe best accuracy score over tasks is around 12k. We find that in-context\nlearning can achieve higher performance with more demonstrations under\nmany-shot instruction tuning (8k), and further extending the length of\ninstructions (16k) can further improve the upper bound of scaling in-context\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2302.04931v1.pdf"
    },
    {
        "title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",
        "authors": [
            "Maciej P. Polak",
            "Shrey Modi",
            "Anna Latosinska",
            "Jinming Zhang",
            "Ching-Wen Wang",
            "Shanonan Wang",
            "Ayan Deep Hazra",
            "Dane Morgan"
        ],
        "published": "2023-02-09T19:56:37Z",
        "summary": "Accurate and comprehensive material databases extracted from research papers\nare critical for materials science and engineering but require significant\nhuman effort to develop. In this paper we present a simple method of extracting\nmaterials data from full texts of research papers suitable for quickly\ndeveloping modest-sized databases. The method requires minimal to no coding,\nprior knowledge about the extracted property, or model training, and provides\nhigh recall and almost perfect precision in the resultant database. The method\nis fully automated except for one human-assisted step, which typically requires\njust a few hours of human labor. The method builds on top of natural language\nprocessing and large general language models but can work with almost any such\nmodel. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for\ncomparison. We provide a detailed detailed analysis of the methods performance\nin extracting bulk modulus data, obtaining up to 90% precision at 96% recall,\ndepending on the amount of human effort involved. We then demonstrate the\nmethods broader effectiveness by developing a database of critical cooling\nrates for metallic glasses.",
        "pdf_link": "https://arxiv.org/pdf/2302.04914v2.pdf"
    },
    {
        "title": "Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow",
        "authors": [
            "Anjana Arunkumar",
            "Swaroop Mishra",
            "Bhavdeep Sachdeva",
            "Chitta Baral",
            "Chris Bryan"
        ],
        "published": "2023-02-09T04:43:10Z",
        "summary": "Recent research has shown that language models exploit `artifacts' in\nbenchmarks to solve tasks, rather than truly learning them, leading to inflated\nmodel performance. In pursuit of creating better benchmarks, we propose VAIDA,\na novel benchmark creation paradigm for NLP, that focuses on guiding\ncrowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.\nVAIDA facilitates sample correction by providing realtime visual feedback and\nrecommendations to improve sample quality. Our approach is domain, model, task,\nand metric agnostic, and constitutes a paradigm shift for robust, validated,\nand dynamic benchmark creation via human-and-metric-in-the-loop workflows. We\nevaluate via expert review and a user study with NASA TLX. We find that VAIDA\ndecreases effort, frustration, mental, and temporal demands of crowdworkers and\nanalysts, simultaneously increasing the performance of both user groups with a\n45.8% decrease in the level of artifacts in created samples. As a by product of\nour user study, we observe that created samples are adversarial across models,\nleading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)\nin performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.04434v1.pdf"
    },
    {
        "title": "Prompting for Multimodal Hateful Meme Classification",
        "authors": [
            "Rui Cao",
            "Roy Ka-Wei Lee",
            "Wen-Haw Chong",
            "Jing Jiang"
        ],
        "published": "2023-02-08T16:04:08Z",
        "summary": "Hateful meme classification is a challenging multimodal task that requires\ncomplex reasoning and contextual background knowledge. Ideally, we could\nleverage an explicit external knowledge base to supplement contextual and\ncultural information in hateful memes. However, there is no known explicit\nexternal knowledge base that could provide such hate speech contextual\ninformation. To address this gap, we propose PromptHate, a simple yet effective\nprompt-based model that prompts pre-trained language models (PLMs) for hateful\nmeme classification. Specifically, we construct simple prompts and provide a\nfew in-context examples to exploit the implicit knowledge in the pre-trained\nRoBERTa language model for hateful meme classification. We conduct extensive\nexperiments on two publicly available hateful and offensive meme datasets. Our\nexperimental results show that PromptHate is able to achieve a high AUC of\n90.96, outperforming state-of-the-art baselines on the hateful meme\nclassification task. We also perform fine-grained analyses and case studies on\nvarious prompt settings and demonstrate the effectiveness of the prompts on\nhateful meme classification.",
        "pdf_link": "https://arxiv.org/pdf/2302.04156v1.pdf"
    },
    {
        "title": "Training-free Lexical Backdoor Attacks on Language Models",
        "authors": [
            "Yujin Huang",
            "Terry Yue Zhuo",
            "Qiongkai Xu",
            "Han Hu",
            "Xingliang Yuan",
            "Chunyang Chen"
        ],
        "published": "2023-02-08T15:18:51Z",
        "summary": "Large-scale language models have achieved tremendous success across various\nnatural language processing (NLP) applications. Nevertheless, language models\nare vulnerable to backdoor attacks, which inject stealthy triggers into models\nfor steering them to undesirable behaviors. Most existing backdoor attacks,\nsuch as data poisoning, require further (re)training or fine-tuning language\nmodels to learn the intended backdoor patterns. The additional training process\nhowever diminishes the stealthiness of the attacks, as training a language\nmodel usually requires long optimization time, a massive amount of data, and\nconsiderable modifications to the model parameters. In this work, we propose\nTraining-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free\nbackdoor attack on language models. Our attack is achieved by injecting lexical\ntriggers into the tokenizer of a language model via manipulating its embedding\ndictionary using carefully designed rules. These rules are explainable to human\ndevelopers which inspires attacks from a wider range of hackers. The sparse\nmanipulation of the dictionary also habilitates the stealthiness of our attack.\nWe conduct extensive experiments on three dominant NLP tasks based on nine\nlanguage models to demonstrate the effectiveness and universality of our\nattack. The code of this work is available at\nhttps://github.com/Jinxhy/TFLexAttack.",
        "pdf_link": "https://arxiv.org/pdf/2302.04116v1.pdf"
    },
    {
        "title": "ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots",
        "authors": [
            "Reham Omar",
            "Omij Mangukiya",
            "Panos Kalnis",
            "Essam Mansour"
        ],
        "published": "2023-02-08T13:03:27Z",
        "summary": "Conversational AI and Question-Answering systems (QASs) for knowledge graphs\n(KGs) are both emerging research areas: they empower users with natural\nlanguage interfaces for extracting information easily and effectively.\nConversational AI simulates conversations with humans; however, it is limited\nby the data captured in the training datasets. In contrast, QASs retrieve the\nmost recent information from a KG by understanding and translating the natural\nlanguage question into a formal query supported by the database engine.\n  In this paper, we present a comprehensive study of the characteristics of the\nexisting alternatives towards combining both worlds into novel KG chatbots. Our\nframework compares two representative conversational models, ChatGPT and\nGalactica, against KGQAN, the current state-of-the-art QAS. We conduct a\nthorough evaluation using four real KGs across various application domains to\nidentify the current limitations of each category of systems. Based on our\nfindings, we propose open research opportunities to empower QASs with chatbot\ncapabilities for KGs. All benchmarks and all raw results are available1 for\nfurther analysis.",
        "pdf_link": "https://arxiv.org/pdf/2302.06466v1.pdf"
    },
    {
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "authors": [
            "Yejin Bang",
            "Samuel Cahyawijaya",
            "Nayeon Lee",
            "Wenliang Dai",
            "Dan Su",
            "Bryan Wilie",
            "Holy Lovenia",
            "Ziwei Ji",
            "Tiezheng Yu",
            "Willy Chung",
            "Quyet V. Do",
            "Yan Xu",
            "Pascale Fung"
        ],
        "published": "2023-02-08T12:35:34Z",
        "summary": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.",
        "pdf_link": "https://arxiv.org/pdf/2302.04023v4.pdf"
    },
    {
        "title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models",
        "authors": [
            "Hossein Hajipour",
            "Keno Hassler",
            "Thorsten Holz",
            "Lea Sch\u00f6nherr",
            "Mario Fritz"
        ],
        "published": "2023-02-08T11:54:07Z",
        "summary": "Large language models (LLMs) for automatic code generation have achieved\nbreakthroughs in several programming tasks. Their advances in competition-level\nprogramming problems have made them an essential pillar of AI-assisted pair\nprogramming, and tools such as GitHub Copilot have emerged as part of the daily\nprogramming workflow used by millions of developers. The training data for\nthese models is usually collected from the Internet (e.g., from open-source\nrepositories) and is likely to contain faults and security vulnerabilities.\nThis unsanitized training data can cause the language models to learn these\nvulnerabilities and propagate them during the code generation procedure. While\nthese models have been extensively assessed for their ability to produce\nfunctionally correct programs, there remains a lack of comprehensive\ninvestigations and benchmarks addressing the security aspects of these models.\n  In this work, we propose a method to systematically study the security issues\nof code language models to assess their susceptibility to generating vulnerable\ncode. To this end, we introduce the first approach to automatically find\ngenerated code that contains vulnerabilities in black-box code generation\nmodels. To achieve this, we present an approach to approximate inversion of the\nblack-box code generation models based on few-shot prompting. We evaluate the\neffectiveness of our approach by examining code language models in generating\nhigh-risk security weaknesses. Furthermore, we establish a collection of\ndiverse non-secure prompts for various vulnerability scenarios using our\nmethod. This dataset forms a benchmark for evaluating and comparing the\nsecurity weaknesses in code language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.04012v2.pdf"
    },
    {
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Zhuosheng Zhang",
            "Jiaao Chen",
            "Michihiro Yasunaga",
            "Diyi Yang"
        ],
        "published": "2023-02-08T09:44:51Z",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.",
        "pdf_link": "https://arxiv.org/pdf/2302.06476v3.pdf"
    },
    {
        "title": "Reliable Natural Language Understanding with Large Language Models and Answer Set Programming",
        "authors": [
            "Abhiramon Rajasekharan",
            "Yankai Zeng",
            "Parth Padalkar",
            "Gopal Gupta"
        ],
        "published": "2023-02-07T22:37:21Z",
        "summary": "Humans understand language by extracting information (meaning) from\nsentences, combining it with existing commonsense knowledge, and then\nperforming reasoning to draw conclusions. While large language models (LLMs)\nsuch as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a\nvariety of NLP tasks, they fall short in problems that require reasoning. They\nalso cannot reliably explain the answers generated for a given question. In\norder to emulate humans better, we propose STAR, a framework that combines LLMs\nwith Answer Set Programming (ASP). We show how LLMs can be used to effectively\nextract knowledge -- represented as predicates -- from language. Goal-directed\nASP is then employed to reliably reason over this knowledge. We apply the STAR\nframework to three different NLU tasks requiring reasoning: qualitative\nreasoning, mathematical reasoning, and goal-directed conversation. Our\nexperiments reveal that STAR is able to bridge the gap of reasoning in NLU\ntasks, leading to significant performance improvements, especially for smaller\nLLMs, i.e., LLMs with a smaller number of parameters. NLU applications\ndeveloped using the STAR framework are also explainable: along with the\npredicates generated, a justification in the form of a proof tree can be\nproduced for a given output.",
        "pdf_link": "https://arxiv.org/pdf/2302.03780v3.pdf"
    },
    {
        "title": "What Matters In The Structured Pruning of Generative Language Models?",
        "authors": [
            "Michael Santacroce",
            "Zixin Wen",
            "Yelong Shen",
            "Yuanzhi Li"
        ],
        "published": "2023-02-07T22:05:55Z",
        "summary": "Auto-regressive large language models such as GPT-3 require enormous\ncomputational resources to use. Traditionally, structured pruning methods are\nemployed to reduce resource usage. However, their application to and efficacy\nfor generative language models is heavily under-explored. In this paper we\nconduct an comprehensive evaluation of common structured pruning methods,\nincluding magnitude, random, and movement pruning on the feed-forward layers in\nGPT-type models. Unexpectedly, random pruning results in performance that is\ncomparable to the best established methods, across multiple natural language\ngeneration tasks. To understand these results, we provide a framework for\nmeasuring neuron-level redundancy of models pruned by different methods, and\ndiscover that established structured pruning methods do not take into account\nthe distinctiveness of neurons, leaving behind excess redundancies. In view of\nthis, we introduce Globally Unique Movement (GUM) to improve the uniqueness of\nneurons in pruned models. We then discuss the effects of our techniques on\ndifferent redundancy metrics to explain the improved performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.03773v1.pdf"
    },
    {
        "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
        "authors": [
            "Akshita Jha",
            "Adithya Samavedhi",
            "Vineeth Rakesh",
            "Jaideep Chandrashekar",
            "Chandan K. Reddy"
        ],
        "published": "2023-02-07T21:51:05Z",
        "summary": "Recent advances in the area of long document matching have primarily focused\non using transformer-based models for long document encoding and matching.\nThere are two primary challenges associated with these models. Firstly, the\nperformance gain provided by transformer-based models comes at a steep cost -\nboth in terms of the required training time and the resource (memory and\nenergy) consumption. The second major limitation is their inability to handle\nmore than a pre-defined input token length at a time. In this work, we\nempirically demonstrate the effectiveness of simple neural models (such as\nfeed-forward networks, and CNNs) and simple embeddings (like GloVe, and\nParagraph Vector) over transformer-based models on the task of document\nmatching. We show that simple models outperform the more complex BERT-based\nmodels while taking significantly less training time, energy, and memory. The\nsimple models are also more robust to variations in document length and text\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2302.03765v1.pdf"
    },
    {
        "title": "Long Horizon Temperature Scaling",
        "authors": [
            "Andy Shih",
            "Dorsa Sadigh",
            "Stefano Ermon"
        ],
        "published": "2023-02-07T18:59:32Z",
        "summary": "Temperature scaling is a popular technique for tuning the sharpness of a\nmodel distribution. It is used extensively for sampling likely generations and\ncalibrating model uncertainty, and even features as a controllable parameter to\nmany large language models in deployment. However, autoregressive models rely\non myopic temperature scaling that greedily optimizes the next token. To\naddress this, we propose Long Horizon Temperature Scaling (LHTS), a novel\napproach for sampling from temperature-scaled joint distributions. LHTS is\ncompatible with all likelihood-based models, and optimizes for the long horizon\nlikelihood of samples. We derive a temperature-dependent LHTS objective, and\nshow that finetuning a model on a range of temperatures produces a single model\ncapable of generation with a controllable long horizon temperature parameter.\nWe experiment with LHTS on image diffusion models and character/language\nautoregressive models, demonstrating advantages over myopic temperature scaling\nin likelihood and sample quality, and showing improvements in accuracy on a\nmultiple choice analogy task by $10\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2302.03686v2.pdf"
    },
    {
        "title": "Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models",
        "authors": [
            "Amirkeivan Mohtashami",
            "Mauro Verzetti",
            "Paul K. Rubenstein"
        ],
        "published": "2023-02-07T14:35:35Z",
        "summary": "Learned metrics such as BLEURT have in recent years become widely employed to\nevaluate the quality of machine translation systems. Training such metrics\nrequires data which can be expensive and difficult to acquire, particularly for\nlower-resource languages. We show how knowledge can be distilled from Large\nLanguage Models (LLMs) to improve upon such learned metrics without requiring\nhuman annotators, by creating synthetic datasets which can be mixed into\nexisting datasets, requiring only a corpus of text in the target language. We\nshow that the performance of a BLEURT-like model on lower resource languages\ncan be improved in this way.",
        "pdf_link": "https://arxiv.org/pdf/2302.03491v1.pdf"
    },
    {
        "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
        "authors": [
            "Maximillian Chen",
            "Alexandros Papangelis",
            "Chenyang Tao",
            "Seokhwan Kim",
            "Andy Rosenbaum",
            "Yang Liu",
            "Zhou Yu",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023-02-07T05:48:16Z",
        "summary": "Collecting high quality conversational data can be very expensive for most\napplications and infeasible for others due to privacy, ethical, or similar\nconcerns. A promising direction to tackle this problem is to generate synthetic\ndialogues by prompting large language models. In this work, we use a small set\nof expert-written conversations as in-context examples to synthesize a social\nconversation dataset using prompting. We perform several thorough evaluations\nof our synthetic conversations compared to human-collected conversations. This\nincludes various dimensions of conversation quality with human evaluation\ndirectly on the synthesized conversations, and interactive human evaluation of\nchatbots fine-tuned on the synthetically generated dataset. We additionally\ndemonstrate that this prompting approach is generalizable to multi-party\nconversations, providing potential to create new synthetic data for multi-party\ntasks. Our synthetic multi-party conversations were rated more favorably across\nall measured dimensions compared to conversation excerpts sampled from a\nhuman-collected multi-party dataset.",
        "pdf_link": "https://arxiv.org/pdf/2302.03269v3.pdf"
    },
    {
        "title": "APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning",
        "authors": [
            "Sunyi Chi",
            "Bo Dong",
            "Yiming Xu",
            "Zhenyu Shi",
            "Zheng Du"
        ],
        "published": "2023-02-06T18:40:04Z",
        "summary": "Practical natural language processing (NLP) tasks are commonly long-tailed\nwith noisy labels. Those problems challenge the generalization and robustness\nof complex models such as Deep Neural Networks (DNNs). Some commonly used\nresampling techniques, such as oversampling or undersampling, could easily lead\nto overfitting. It is growing popular to learn the data weights leveraging a\nsmall amount of metadata. Besides, recent studies have shown the advantages of\nself-supervised pre-training, particularly to the under-represented data. In\nthis work, we propose a general framework to handle the problem of both\nlong-tail and noisy labels. The model is adapted to the domain of problems in a\ncontrastive learning manner. The re-weighting module is a feed-forward network\nthat learns explicit weighting functions and adapts weights according to\nmetadata. The framework further adapts weights of terms in the loss function\nthrough a combination of the polynomial expansion of cross-entropy loss and\nfocal loss. Our extensive experiments show that the proposed framework\nconsistently outperforms baseline methods. Lastly, our sensitive analysis\nemphasizes the capability of the proposed framework to handle the long-tailed\nproblem and mitigate the negative impact of noisy labels.",
        "pdf_link": "https://arxiv.org/pdf/2302.03488v2.pdf"
    },
    {
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
        "authors": [
            "Thomas Carta",
            "Cl\u00e9ment Romac",
            "Thomas Wolf",
            "Sylvain Lamprier",
            "Olivier Sigaud",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2023-02-06T10:01:08Z",
        "summary": "Recent works successfully leveraged Large Language Models' (LLM) abilities to\ncapture abstract knowledge about world's physics to solve decision-making\nproblems. Yet, the alignment between LLMs' knowledge and the environment can be\nwrong and limit functional competence due to lack of grounding. In this paper,\nwe study an approach (named GLAM) to achieve this alignment through functional\ngrounding: we consider an agent using an LLM as a policy that is progressively\nupdated as the agent interacts with the environment, leveraging online\nReinforcement Learning to improve its performance to solve goals. Using an\ninteractive textual environment designed to study higher-level forms of\nfunctional grounding, and a set of spatial and navigation tasks, we study\nseveral scientific questions: 1) Can LLMs boost sample efficiency for online\nlearning of various RL tasks? 2) How can it boost different forms of\ngeneralization? 3) What is the impact of online learning? We study these\nquestions by functionally grounding several variants (size, architecture) of\nFLAN-T5.",
        "pdf_link": "https://arxiv.org/pdf/2302.02662v3.pdf"
    },
    {
        "title": "A Categorical Archive of ChatGPT Failures",
        "authors": [
            "Ali Borji"
        ],
        "published": "2023-02-06T04:21:59Z",
        "summary": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.",
        "pdf_link": "https://arxiv.org/pdf/2302.03494v8.pdf"
    },
    {
        "title": "Nationality Bias in Text Generation",
        "authors": [
            "Pranav Narayanan Venkit",
            "Sanjana Gautam",
            "Ruchi Panchanadikar",
            "Ting-Hao 'Kenneth' Huang",
            "Shomir Wilson"
        ],
        "published": "2023-02-05T19:15:33Z",
        "summary": "Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.",
        "pdf_link": "https://arxiv.org/pdf/2302.02463v3.pdf"
    },
    {
        "title": "Quantized Distributed Training of Large Models with Convergence Guarantees",
        "authors": [
            "Ilia Markov",
            "Adrian Vladu",
            "Qi Guo",
            "Dan Alistarh"
        ],
        "published": "2023-02-05T14:20:55Z",
        "summary": "Communication-reduction techniques are a popular way to improve scalability\nin data-parallel training of deep neural networks (DNNs). The recent emergence\nof large language models such as GPT has created the need for new approaches to\nexploit data-parallelism. Among these, fully-sharded data parallel (FSDP)\ntraining is highly popular, yet it still encounters scalability bottlenecks.\nOne reason is that applying compression techniques to FSDP is challenging: as\nthe vast majority of the communication involves the model's weights, direct\ncompression alters convergence and leads to accuracy loss. We present QSDP, a\nvariant of FSDP which supports both gradient and weight quantization with\ntheoretical guarantees, is simple to implement and has essentially no\noverheads. To derive QSDP we prove that a natural modification of SGD achieves\nconvergence even when we only maintain quantized weights, and thus the domain\nover which we train consists of quantized points and is, therefore, highly\nnon-convex. We validate this approach by training GPT-family models with up to\n1.3 billion parameters on a multi-node cluster. Experiments show that QSDP\npreserves model accuracy, while completely removing the communication\nbottlenecks of FSDP, providing end-to-end speedups of up to 2.2x.",
        "pdf_link": "https://arxiv.org/pdf/2302.02390v1.pdf"
    },
    {
        "title": "The Science of Detecting LLM-Generated Texts",
        "authors": [
            "Ruixiang Tang",
            "Yu-Neng Chuang",
            "Xia Hu"
        ],
        "published": "2023-02-04T04:49:17Z",
        "summary": "The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.",
        "pdf_link": "https://arxiv.org/pdf/2303.07205v3.pdf"
    },
    {
        "title": "Evaluating Large Language Models in Theory of Mind Tasks",
        "authors": [
            "Michal Kosinski"
        ],
        "published": "2023-02-04T03:50:01Z",
        "summary": "Eleven Large Language Models (LLMs) were assessed using a custom-made battery\nof false-belief tasks, considered a gold standard in testing Theory of Mind\n(ToM) in humans. The battery included 640 prompts spread across 40 diverse\ntasks, each one including a false-belief scenario, three closely matched\ntrue-belief control scenarios, and the reversed versions of all four. To solve\na single task, a model needed to correctly answer 16 prompts across all eight\nscenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from\nNovember 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;\nChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of\nsix-year-old children observed in past studies. We explore the potential\ninterpretation of these findings, including the intriguing possibility that\nToM, previously considered exclusive to humans, may have spontaneously emerged\nas a byproduct of LLMs' improving language skills.",
        "pdf_link": "https://arxiv.org/pdf/2302.02083v6.pdf"
    },
    {
        "title": "Towards Few-Shot Identification of Morality Frames using In-Context Learning",
        "authors": [
            "Shamik Roy",
            "Nishanth Sridhar Nakshatri",
            "Dan Goldwasser"
        ],
        "published": "2023-02-03T23:26:59Z",
        "summary": "Data scarcity is a common problem in NLP, especially when the annotation\npertains to nuanced socio-linguistic concepts that require specialized\nknowledge. As a result, few-shot identification of these concepts is desirable.\nFew-shot in-context learning using pre-trained Large Language Models (LLMs) has\nbeen recently applied successfully in many NLP tasks. In this paper, we study\nfew-shot identification of a psycho-linguistic concept, Morality Frames (Roy et\nal., 2021), using LLMs. Morality frames are a representation framework that\nprovides a holistic view of the moral sentiment expressed in text, identifying\nthe relevant moral foundation (Haidt and Graham, 2007) and at a finer level of\ngranularity, the moral sentiment expressed towards the entities mentioned in\nthe text. Previous studies relied on human annotation to identify morality\nframes in text which is expensive. In this paper, we propose prompting-based\napproaches using pretrained Large Language Models for identification of\nmorality frames, relying only on few-shot exemplars. We compare our models'\nperformance with few-shot RoBERTa and found promising results.",
        "pdf_link": "https://arxiv.org/pdf/2302.02029v1.pdf"
    },
    {
        "title": "Witscript 2: A System for Generating Improvised Jokes Without Wordplay",
        "authors": [
            "Joe Toplyn"
        ],
        "published": "2023-02-03T21:51:55Z",
        "summary": "A previous paper presented Witscript, a system for generating conversational\njokes that rely on wordplay. This paper extends that work by presenting\nWitscript 2, which uses a large language model to generate conversational jokes\nthat rely on common sense instead of wordplay. Like Witscript, Witscript 2 is\nbased on joke-writing algorithms created by an expert comedy writer. Human\nevaluators judged Witscript 2's responses to input sentences to be jokes 46% of\nthe time, compared to 70% of the time for human-written responses. This is\nevidence that Witscript 2 represents another step toward giving a chatbot a\nhumanlike sense of humor.",
        "pdf_link": "https://arxiv.org/pdf/2302.03036v1.pdf"
    },
    {
        "title": "Bioformer: an efficient transformer language model for biomedical text mining",
        "authors": [
            "Li Fang",
            "Qingyu Chen",
            "Chih-Hsuan Wei",
            "Zhiyong Lu",
            "Kai Wang"
        ],
        "published": "2023-02-03T08:04:59Z",
        "summary": "Pretrained language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have achieved state-of-the-art performance in natural\nlanguage processing (NLP) tasks. Recently, BERT has been adapted to the\nbiomedical domain. Despite the effectiveness, these models have hundreds of\nmillions of parameters and are computationally expensive when applied to\nlarge-scale NLP applications. We hypothesized that the number of parameters of\nthe original BERT can be dramatically reduced with minor impact on performance.\nIn this study, we present Bioformer, a compact BERT model for biomedical text\nmining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)\nwhich reduced the model size by 60% compared to BERTBase. Bioformer uses a\nbiomedical vocabulary and was pre-trained from scratch on PubMed abstracts and\nPubMed Central full-text articles. We thoroughly evaluated the performance of\nBioformer as well as existing biomedical BERT models including BioBERT and\nPubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:\nnamed entity recognition, relation extraction, question answering and document\nclassification. The results show that with 60% fewer parameters, Bioformer16L\nis only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less\naccurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed\nBioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as\nfast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed\nto PubTator Central providing gene annotations over 35 million PubMed abstracts\nand 5 million PubMed Central full-text articles. We make Bioformer publicly\navailable via https://github.com/WGLab/bioformer, including pre-trained models,\ndatasets, and instructions for downstream use.",
        "pdf_link": "https://arxiv.org/pdf/2302.01588v1.pdf"
    },
    {
        "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Guanzhou Chen",
            "Anji Liu",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "published": "2023-02-03T06:06:27Z",
        "summary": "We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
        "pdf_link": "https://arxiv.org/pdf/2302.01560v2.pdf"
    },
    {
        "title": "STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition",
        "authors": [
            "Yucheng Lu",
            "Shivani Agrawal",
            "Suvinay Subramanian",
            "Oleg Rybakov",
            "Christopher De Sa",
            "Amir Yazdanbakhsh"
        ],
        "published": "2023-02-02T15:49:03Z",
        "summary": "Recent innovations on hardware (e.g. Nvidia A100) have motivated learning N:M\nstructured sparsity masks from scratch for fast model inference. However,\nstate-of-the-art learning recipes in this regime (e.g. SR-STE) are proposed for\nnon-adaptive optimizers like momentum SGD, while incurring non-trivial accuracy\ndrop for Adam-trained models like attention-based LLMs. In this paper, we first\ndemonstrate such gap origins from poorly estimated second moment (i.e.\nvariance) in Adam states given by the masked weights. We conjecture that\nlearning N:M masks with Adam should take the critical regime of variance\nestimation into account. In light of this, we propose STEP, an Adam-aware\nrecipe that learns N:M masks with two phases: first, STEP calculates a reliable\nvariance estimate (precondition phase) and subsequently, the variance remains\nfixed and is used as a precondition to learn N:M masks (mask-learning phase).\nSTEP automatically identifies the switching point of two phases by dynamically\nsampling variance changes over the training trajectory and testing the sample\nconcentration. Empirically, we evaluate STEP and other baselines such as ASP\nand SR-STE on multiple tasks including CIFAR classification, machine\ntranslation and LLM fine-tuning (BERT-Base, GPT-2). We show STEP mitigates the\naccuracy drop of baseline recipes and is robust to aggressive structured\nsparsity ratios.",
        "pdf_link": "https://arxiv.org/pdf/2302.01172v1.pdf"
    },
    {
        "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Hai Zhao",
            "George Karypis",
            "Alex Smola"
        ],
        "published": "2023-02-02T07:51:19Z",
        "summary": "Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have focused on the language modality. We propose\nMultimodal-CoT that incorporates language (text) and vision (images) modalities\ninto a two-stage framework that separates rationale generation and answer\ninference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. With Multimodal-CoT, our\nmodel under 1 billion parameters outperforms the previous state-of-the-art LLM\n(GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA\nbenchmark and even surpasses human performance. Code is publicly available\navailable at https://github.com/amazon-science/mm-cot.",
        "pdf_link": "https://arxiv.org/pdf/2302.00923v4.pdf"
    },
    {
        "title": "Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment",
        "authors": [
            "Hao Liu",
            "Wilson Yan",
            "Pieter Abbeel"
        ],
        "published": "2023-02-02T06:38:44Z",
        "summary": "Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.00902v2.pdf"
    },
    {
        "title": "Conditioning Predictive Models: Risks and Strategies",
        "authors": [
            "Evan Hubinger",
            "Adam Jermyn",
            "Johannes Treutlein",
            "Rubi Hudson",
            "Kate Woolverton"
        ],
        "published": "2023-02-02T00:06:36Z",
        "summary": "Our intention is to provide a definitive reference on what it would take to\nsafely make use of generative/predictive models in the absence of a solution to\nthe Eliciting Latent Knowledge problem. Furthermore, we believe that large\nlanguage models can be understood as such predictive models of the world, and\nthat such a conceptualization raises significant opportunities for their safe\nyet powerful use via carefully conditioning them to predict desirable outputs.\nUnfortunately, such approaches also raise a variety of potentially fatal safety\nproblems, particularly surrounding situations where predictive models predict\nthe output of other AI systems, potentially unbeknownst to us. There are\nnumerous potential solutions to such problems, however, primarily via carefully\nconditioning models to predict the things we want (e.g. humans) rather than the\nthings we don't (e.g. malign AIs). Furthermore, due to the simplicity of the\nprediction objective, we believe that predictive models present the easiest\ninner alignment problem that we are aware of. As a result, we think that\nconditioning approaches for predictive models represent the safest known way of\neliciting human-level and slightly superhuman capabilities from large language\nmodels and other similar future models.",
        "pdf_link": "https://arxiv.org/pdf/2302.00805v2.pdf"
    },
    {
        "title": "Collaborating with language models for embodied reasoning",
        "authors": [
            "Ishita Dasgupta",
            "Christine Kaeser-Chen",
            "Kenneth Marino",
            "Arun Ahuja",
            "Sheila Babayan",
            "Felix Hill",
            "Rob Fergus"
        ],
        "published": "2023-02-01T21:26:32Z",
        "summary": "Reasoning in a complex and ambiguous environment is a key goal for\nReinforcement Learning (RL) agents. While some sophisticated RL agents can\nsuccessfully solve difficult tasks, they require a large amount of training\ndata and often struggle to generalize to new unseen environments and new tasks.\nOn the other hand, Large Scale Language Models (LSLMs) have exhibited strong\nreasoning ability and the ability to to adapt to new tasks through in-context\nlearning. However, LSLMs do not inherently have the ability to interrogate or\nintervene on the environment. In this work, we investigate how to combine these\ncomplementary abilities in a single system consisting of three parts: a\nPlanner, an Actor, and a Reporter. The Planner is a pre-trained language model\nthat can issue commands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next command. We present a\nset of tasks that require reasoning, test this system's ability to generalize\nzero-shot and investigate failure cases, and demonstrate how components of this\nsystem can be trained with reinforcement-learning to improve performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.00763v1.pdf"
    },
    {
        "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
        "authors": [
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Minlie Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-02-01T17:33:12Z",
        "summary": "Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.",
        "pdf_link": "https://arxiv.org/pdf/2302.00618v1.pdf"
    },
    {
        "title": "Co-Writing with Opinionated Language Models Affects Users' Views",
        "authors": [
            "Maurice Jakesch",
            "Advait Bhat",
            "Daniel Buschek",
            "Lior Zalmanson",
            "Mor Naaman"
        ],
        "published": "2023-02-01T16:26:32Z",
        "summary": "If large language models like GPT-3 preferably produce a particular point of\nview, they may influence people's opinions on an unknown scale. This study\ninvestigates whether a language-model-powered writing assistant that generates\nsome opinions more often than others impacts what users write - and what they\nthink. In an online experiment, we asked participants (N=1,506) to write a post\ndiscussing whether social media is good for society. Treatment group\nparticipants used a language-model-powered writing assistant configured to\nargue that social media is good or bad for society. Participants then completed\na social media attitude survey, and independent judges (N=500) evaluated the\nopinions expressed in their writing. Using the opinionated language model\naffected the opinions expressed in participants' writing and shifted their\nopinions in the subsequent attitude survey. We discuss the wider implications\nof our results and argue that the opinions built into AI language technologies\nneed to be monitored and engineered more carefully.",
        "pdf_link": "https://arxiv.org/pdf/2302.00560v1.pdf"
    },
    {
        "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
        "authors": [
            "Nils Lukas",
            "Ahmed Salem",
            "Robert Sim",
            "Shruti Tople",
            "Lukas Wutschitz",
            "Santiago Zanella-B\u00e9guelin"
        ],
        "published": "2023-02-01T16:04:48Z",
        "summary": "Language Models (LMs) have been shown to leak information about training data\nthrough sentence-level membership inference and reconstruction attacks.\nUnderstanding the risk of LMs leaking Personally Identifiable Information (PII)\nhas received less attention, which can be attributed to the false assumption\nthat dataset curation techniques such as scrubbing are sufficient to prevent\nPII leakage. Scrubbing techniques reduce but do not prevent the risk of PII\nleakage: in practice scrubbing is imperfect and must balance the trade-off\nbetween minimizing disclosure and preserving the utility of the dataset. On the\nother hand, it is unclear to which extent algorithmic defenses such as\ndifferential privacy, designed to guarantee sentence- or user-level privacy,\nprevent PII disclosure. In this work, we introduce rigorous game-based\ndefinitions for three types of PII leakage via black-box extraction, inference,\nand reconstruction attacks with only API access to an LM. We empirically\nevaluate the attacks against GPT-2 models fine-tuned with and without defenses\nin three domains: case law, health care, and e-mails. Our main contributions\nare (i) novel attacks that can extract up to 10$\\times$ more PII sequences than\nexisting attacks, (ii) showing that sentence-level differential privacy reduces\nthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)\na subtle connection between record-level membership inference and PII\nreconstruction. Code to reproduce all experiments in the paper is available at\nhttps://github.com/microsoft/analysing_pii_leakage.",
        "pdf_link": "https://arxiv.org/pdf/2302.00539v4.pdf"
    },
    {
        "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
        "authors": [
            "Freda Shi",
            "Xinyun Chen",
            "Kanishka Misra",
            "Nathan Scales",
            "David Dohan",
            "Ed Chi",
            "Nathanael Sch\u00e4rli",
            "Denny Zhou"
        ],
        "published": "2023-01-31T20:48:57Z",
        "summary": "Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2302.00093v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models for News Summarization",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "Kathleen McKeown",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-01-31T18:46:19Z",
        "summary": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.",
        "pdf_link": "https://arxiv.org/pdf/2301.13848v1.pdf"
    },
    {
        "title": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning",
        "authors": [
            "Yunhu Ye",
            "Binyuan Hui",
            "Min Yang",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-01-31T17:51:45Z",
        "summary": "Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2301.13808v3.pdf"
    },
    {
        "title": "FLAME: A small language model for spreadsheet formulas",
        "authors": [
            "Harshit Joshi",
            "Abishai Ebenezer",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Aditya Kanade",
            "Vu Le",
            "Ivan Radi\u010dek",
            "Gust Verbruggen"
        ],
        "published": "2023-01-31T17:29:43Z",
        "summary": "Spreadsheets are a vital tool for end-user data management. Using large\nlanguage models for formula authoring assistance in these environments can be\ndifficult, as these models are expensive to train and challenging to deploy due\nto their size (up to billions of parameters). We present FLAME, a\ntransformer-based model trained exclusively on Excel formulas that leverages\ndomain insights to achieve competitive performance while being substantially\nsmaller (60M parameters) and training on two orders of magnitude less data. We\ncurate a training dataset using sketch deduplication, introduce an\nExcel-specific formula tokenizer, and use domain-specific versions of masked\nspan prediction and noisy auto-encoding as pre-training objectives. We evaluate\nFLAME on formula repair, formula completion, and similarity-based formula\nretrieval. FLAME can outperform much larger models, such as the Davinci (175B)\nand Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation\nsettings for the repair and completion tasks. For formula retrieval, FLAME\noutperforms CodeT5, CodeBERT, and GraphCodeBERT.",
        "pdf_link": "https://arxiv.org/pdf/2301.13779v2.pdf"
    },
    {
        "title": "Skill Decision Transformer",
        "authors": [
            "Shyam Sudhakaran",
            "Sebastian Risi"
        ],
        "published": "2023-01-31T11:52:46Z",
        "summary": "Recent work has shown that Large Language Models (LLMs) can be incredibly\neffective for offline reinforcement learning (RL) by representing the\ntraditional RL problem as a sequence modelling problem (Chen et al., 2021;\nJanner et al., 2021). However many of these methods only optimize for high\nreturns, and may not extract much information from a diverse dataset of\ntrajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021)\nhave shown that utilizing future trajectory information, in the form of\ninformation statistics, can help extract more information from offline\ntrajectory data. Building upon this, we propose Skill Decision Transformer\n(Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz\net al., 2017) and skill discovery methods to discover a diverse set of\nprimitive behaviors, or skills. We show that Skill DT can not only perform\noffline state-marginal matching (SMM), but can discovery descriptive behaviors\nthat can be easily sampled. Furthermore, we show that through purely\nreward-free optimization, Skill DT is still competitive with supervised offline\nRL approaches on the D4RL benchmark. The code and videos can be found on our\nproject page: https://github.com/shyamsn97/skill-dt",
        "pdf_link": "https://arxiv.org/pdf/2301.13573v1.pdf"
    },
    {
        "title": "Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models",
        "authors": [
            "David Noever",
            "Forrest McKee"
        ],
        "published": "2023-01-31T03:14:57Z",
        "summary": "Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique\ntestbeds for exploring the translation challenges of turning literacy into\nnumeracy. Previous publicly-available transformer models from eighteen months\nprior and 1000 times smaller failed to provide basic arithmetic. The\nstatistical analysis of four complex datasets described here combines\narithmetic manipulations that cannot be memorized or encoded by simple rules.\nThe work examines whether next-token prediction succeeds from sentence\ncompletion into the realm of actual numerical understanding. For example, the\nwork highlights cases for descriptive statistics on in-memory datasets that the\nLLM initially loads from memory or generates randomly using python libraries.\nThe resulting exploratory data analysis showcases the model's capabilities to\ngroup by or pivot categorical sums, infer feature importance, derive\ncorrelations, and predict unseen test cases using linear regression. To extend\nthe model's testable range, the research deletes and appends random rows such\nthat recall alone cannot explain emergent numeracy.",
        "pdf_link": "https://arxiv.org/pdf/2301.13382v1.pdf"
    },
    {
        "title": "Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning",
        "authors": [
            "Jian Zhu",
            "Hanli Wang",
            "Miaojing Shi"
        ],
        "published": "2023-01-30T23:43:28Z",
        "summary": "The visual commonsense reasoning (VCR) task is to choose an answer and\nprovide a justifying rationale based on the given image and textural question.\nRepresentative works first recognize objects in images and then associate them\nwith key words in texts. However, existing approaches do not consider exact\npositions of objects in a human-like three-dimensional (3D) manner, making them\nincompetent to accurately distinguish objects and understand visual relation.\nRecently, multi-modal large language models (MLLMs) have been used as powerful\ntools for several multi-modal tasks but not for VCR yet, which requires\nelaborate reasoning on specific visual objects referred by texts. In light of\nthe above, an MLLM enhanced pseudo 3D perception framework is designed for VCR.\nSpecifically, we first demonstrate that the relation between objects is\nrelevant to object depths in images, and hence introduce object depth into VCR\nframeworks to infer 3D positions of objects in images. Then, a depth-aware\nTransformer is proposed to encode depth differences between objects into the\nattention mechanism of Transformer to discriminatively associate objects with\nvisual scenes guided by depth. To further associate the answer with the depth\nof visual scene, each word in the answer is tagged with a pseudo depth to\nrealize depth-aware association between answer words and objects. On the other\nhand, BLIP-2 as an MLLM is employed to process images and texts, and the\nreferring expressions in texts involving specific visual objects are modified\nwith linguistic object labels to serve as comprehensible MLLM inputs. Finally,\na parameter optimization technique is devised to fully consider the quality of\ndata batches based on multi-level reasoning confidence. Experiments on the VCR\ndataset demonstrate the superiority of the proposed framework over\nstate-of-the-art approaches.",
        "pdf_link": "https://arxiv.org/pdf/2301.13335v2.pdf"
    },
    {
        "title": "Adaptive Machine Translation with Large Language Models",
        "authors": [
            "Yasmin Moslem",
            "Rejwanul Haque",
            "John D. Kelleher",
            "Andy Way"
        ],
        "published": "2023-01-30T21:17:15Z",
        "summary": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, LLMs can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).",
        "pdf_link": "https://arxiv.org/pdf/2301.13294v3.pdf"
    },
    {
        "title": "Conversational Automated Program Repair",
        "authors": [
            "Chunqiu Steven Xia",
            "Lingming Zhang"
        ],
        "published": "2023-01-30T19:22:36Z",
        "summary": "Automated Program Repair (APR) can help developers automatically generate\npatches for bugs. Due to the impressive performance obtained using Large\nPre-Trained Language Models (LLMs) on many code related tasks, researchers have\nstarted to directly use LLMs for APR. However, prior approaches simply\nrepeatedly sample the LLM given the same constructed input/prompt created from\nthe original buggy code, which not only leads to generating the same incorrect\npatches repeatedly but also miss the critical information in testcases. To\naddress these limitations, we propose conversational APR, a new paradigm for\nprogram repair that alternates between patch generation and validation in a\nconversational manner. In conversational APR, we iteratively build the input to\nthe model by combining previously generated patches with validation feedback.\nAs such, we leverage the long-term context window of LLMs to not only avoid\ngenerating previously incorrect patches but also incorporate validation\nfeedback to help the model understand the semantic meaning of the program under\ntest. We evaluate 10 different LLM including the newly developed ChatGPT model\nto demonstrate the improvement of conversational APR over the prior LLM for APR\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2301.13246v1.pdf"
    },
    {
        "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
        "authors": [
            "Minglun Han",
            "Feilong Chen",
            "Jing Shi",
            "Shuang Xu",
            "Bo Xu"
        ],
        "published": "2023-01-30T15:44:55Z",
        "summary": "Large-scale pre-trained language models (PLMs) have shown great potential in\nnatural language processing tasks. Leveraging the capabilities of PLMs to\nenhance automatic speech recognition (ASR) systems has also emerged as a\npromising research direction. However, previous works may be limited by the\ninflexible structures of PLMs and the insufficient utilization of PLMs. To\nalleviate these problems, we propose the hierarchical knowledge distillation\n(HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer\nknowledge from PLMs to the ASR models, HKD employs cross-modal knowledge\ndistillation with contrastive loss at the acoustic level and knowledge\ndistillation with regression loss at the linguistic level. Compared with the\noriginal CIF-based model, our method achieves 15% and 9% relative error rate\nreduction on the AISHELL-1 and LibriSpeech datasets, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2301.13003v2.pdf"
    },
    {
        "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
        "authors": [
            "Terry Yue Zhuo",
            "Zhuang Li",
            "Yujin Huang",
            "Fatemeh Shiri",
            "Weiqing Wang",
            "Gholamreza Haffari",
            "Yuan-Fang Li"
        ],
        "published": "2023-01-30T13:21:00Z",
        "summary": "Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.",
        "pdf_link": "https://arxiv.org/pdf/2301.12868v3.pdf"
    },
    {
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",
        "authors": [
            "Terry Yue Zhuo",
            "Yujin Huang",
            "Chunyang Chen",
            "Zhenchang Xing"
        ],
        "published": "2023-01-30T13:20:48Z",
        "summary": "Recent breakthroughs in natural language processing (NLP) have permitted the\nsynthesis and comprehension of coherent text in an open-ended way, therefore\ntranslating the theoretical algorithms into practical applications. The large\nlanguage models (LLMs) have significantly impacted businesses such as report\nsummarization software and copywriters. Observations indicate, however, that\nLLMs may exhibit social prejudice and toxicity, posing ethical and societal\ndangers of consequences resulting from irresponsibility. Large-scale benchmarks\nfor accountable LLMs should consequently be developed. Although several\nempirical investigations reveal the existence of a few ethical difficulties in\nadvanced LLMs, there is little systematic examination and user study of the\nrisks and harmful behaviors of current LLM usage. To further educate future\nefforts on constructing ethical LLMs responsibly, we perform a qualitative\nresearch method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this\npaper, ChatGPT refers to the version released on Dec 15th.} to better\nunderstand the practical features of ethical dangers in recent LLMs. We analyze\nChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2)\n\\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance\nwith our stated viewpoints, we empirically benchmark ChatGPT on multiple sample\ndatasets. We find that a significant number of ethical risks cannot be\naddressed by existing benchmarks, and hence illustrate them via additional case\nstudies. In addition, we examine the implications of our findings on AI ethics\nand harmal behaviors of ChatGPT, as well as future problems and practical\ndesign considerations for responsible LLMs. We believe that our findings may\ngive light on future efforts to determine and mitigate the ethical hazards\nposed by machines in LLM applications.",
        "pdf_link": "https://arxiv.org/pdf/2301.12867v4.pdf"
    },
    {
        "title": "Specializing Smaller Language Models towards Multi-Step Reasoning",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Litu Ou",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "published": "2023-01-30T08:51:19Z",
        "summary": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.12726v1.pdf"
    },
    {
        "title": "A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction",
        "authors": [
            "Kyle Mahowald"
        ],
        "published": "2023-01-29T22:29:55Z",
        "summary": "Knowledge of syntax includes knowledge of rare, idiosyncratic constructions.\nLLMs must overcome frequency biases in order to master such constructions. In\nthis study, I prompt GPT-3 to give acceptability judgments on the\nEnglish-language Article + Adjective + Numeral + Noun construction (e.g., \"a\nlovely five days\"). I validate the prompt using the CoLA corpus of\nacceptability judgments and then zero in on the AANN construction. I compare\nGPT- 3's judgments to crowdsourced human judgments on a subset of sentences.\nGPT-3's judgments are broadly similar to human judgments and generally align\nwith proposed constraints in the literature but, in some cases, GPT-3's\njudgments and human judgments diverge from the literature and from each other.",
        "pdf_link": "https://arxiv.org/pdf/2301.12564v2.pdf"
    },
    {
        "title": "Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes",
        "authors": [
            "Vahan Arsenyan",
            "Spartak Bughdaryan",
            "Fadi Shaya",
            "Kent Small",
            "Davit Shahnazaryan"
        ],
        "published": "2023-01-29T15:52:33Z",
        "summary": "The automatic construction of knowledge graphs (KGs) is an important research\narea in medicine, with far-reaching applications spanning drug discovery and\nclinical trial design. These applications hinge on the accurate identification\nof interactions among medical and biological entities. In this study, we\npropose an end-to-end machine learning solution based on large language models\n(LLMs) that utilize electronic medical record notes to construct KGs. The\nentities used in the KG construction process are diseases, factors, treatments,\nas well as manifestations that coexist with the patient while experiencing the\ndisease. Given the critical need for high-quality performance in medical\napplications, we embark on a comprehensive assessment of 12 LLMs of various\narchitectures, evaluating their performance and safety attributes. To gauge the\nquantitative efficacy of our approach by assessing both precision and recall,\nwe manually annotate a dataset provided by the Macula and Retina Institute. We\nalso assess the qualitative performance of LLMs, such as the ability to\ngenerate structured outputs or the tendency to hallucinate. The results\nillustrate that in contrast to encoder-only and encoder-decoder, decoder-only\nLLMs require further investigation. Additionally, we provide guided prompt\ndesign to utilize such LLMs. The application of the proposed methodology is\ndemonstrated on age-related macular degeneration.",
        "pdf_link": "https://arxiv.org/pdf/2301.12473v2.pdf"
    },
    {
        "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
        "authors": [
            "Guanglin Zhou",
            "Shaoan Xie",
            "Guangyuan Hao",
            "Shiming Chen",
            "Biwei Huang",
            "Xiwei Xu",
            "Chen Wang",
            "Liming Zhu",
            "Lina Yao",
            "Kun Zhang"
        ],
        "published": "2023-01-29T04:10:12Z",
        "summary": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area.",
        "pdf_link": "https://arxiv.org/pdf/2301.12351v3.pdf"
    },
    {
        "title": "Context-Aware Differential Privacy for Language Modeling",
        "authors": [
            "My H. Dinh",
            "Ferdinando Fioretto"
        ],
        "published": "2023-01-28T20:06:16Z",
        "summary": "The remarkable ability of language models (LMs) has also brought challenges\nat the interface of AI and security. A critical challenge pertains to how much\ninformation these models retain and leak about the training data. This is\nparticularly urgent as the typical development of LMs relies on huge, often\nhighly sensitive data, such as emails and chat logs. To contrast this\nshortcoming, this paper introduces Context-Aware Differentially Private\nLanguage Model (CADP-LM) , a privacy-preserving LM framework that relies on two\nkey insights: First, it utilizes the notion of \\emph{context} to define and\naudit the potentially sensitive information. Second, it adopts the notion of\nDifferential Privacy to protect sensitive information and characterize the\nprivacy leakage. A unique characteristic of CADP-LM is its ability to target\nthe protection of sensitive sentences and contexts only, providing a highly\naccurate private model. Experiments on a variety of datasets and settings\ndemonstrate these strengths of CADP-LM.",
        "pdf_link": "https://arxiv.org/pdf/2301.12288v1.pdf"
    },
    {
        "title": "Truth Machines: Synthesizing Veracity in AI Language Models",
        "authors": [
            "Luke Munn",
            "Liam Magee",
            "Vanicka Arora"
        ],
        "published": "2023-01-28T02:47:50Z",
        "summary": "As AI technologies are rolled out into healthcare, academia, human resources,\nlaw, and a multitude of other domains, they become de-facto arbiters of truth.\nBut truth is highly contested, with many different definitions and approaches.\nThis article discusses the struggle for truth in AI systems and the general\nresponses to date. It then investigates the production of truth in InstructGPT,\na large language model, highlighting how data harvesting, model architectures,\nand social feedback mechanisms weave together disparate understandings of\nveracity. It conceptualizes this performance as an operationalization of truth,\nwhere distinct, often conflicting claims are smoothly synthesized and\nconfidently presented into truth-statements. We argue that these same logics\nand inconsistencies play out in Instruct's successor, ChatGPT, reiterating\ntruth as a non-trivial problem. We suggest that enriching sociality and\nthickening \"reality\" are two promising vectors for enhancing the\ntruth-evaluating capacities of future language models. We conclude, however, by\nstepping back to consider AI truth-telling as a social practice: what kind of\n\"truth\" do we as listeners desire?",
        "pdf_link": "https://arxiv.org/pdf/2301.12066v1.pdf"
    },
    {
        "title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling",
        "authors": [
            "Kolby Nottingham",
            "Prithviraj Ammanabrolu",
            "Alane Suhr",
            "Yejin Choi",
            "Hannaneh Hajishirzi",
            "Sameer Singh",
            "Roy Fox"
        ],
        "published": "2023-01-28T02:04:07Z",
        "summary": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior\nknowledge of the world. However, if initialized with knowledge of high-level\nsubgoals and transitions between subgoals, RL agents could utilize this\nAbstract World Model (AWM) for planning and exploration. We propose using\nfew-shot large language models (LLMs) to hypothesize an AWM, that will be\nverified through world experience, to improve sample efficiency of RL agents.\nOur DECKARD agent applies LLM-guided exploration to item crafting in Minecraft\nin two phases: (1) the Dream phase where the agent uses an LLM to decompose a\ntask into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase\nwhere the agent learns a modular policy for each subgoal and verifies or\ncorrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and\nthen verifying the AWM based on agent experience not only increases sample\nefficiency over contemporary methods by an order of magnitude but is also\nrobust to and corrects errors in the LLM, successfully blending noisy\ninternet-scale information from LLMs with knowledge grounded in environment\ndynamics.",
        "pdf_link": "https://arxiv.org/pdf/2301.12050v2.pdf"
    },
    {
        "title": "Context Matters: A Strategy to Pre-train Language Model for Science Education",
        "authors": [
            "Zhengliang Liu",
            "Xinyu He",
            "Lei Liu",
            "Tianming Liu",
            "Xiaoming Zhai"
        ],
        "published": "2023-01-27T23:50:16Z",
        "summary": "This study aims at improving the performance of scoring student responses in\nscience education automatically. BERT-based language models have shown\nsignificant superiority over traditional NLP models in various language-related\ntasks. However, science writing of students, including argumentation and\nexplanation, is domain-specific. In addition, the language used by students is\ndifferent from the language in journals and Wikipedia, which are training\nsources of BERT and its existing variants. All these suggest that a\ndomain-specific model pre-trained using science education data may improve\nmodel performance. However, the ideal type of data to contextualize pre-trained\nlanguage model and improve the performance in automatically scoring student\nwritten responses remains unclear. Therefore, we employ different data in this\nstudy to contextualize both BERT and SciBERT models and compare their\nperformance on automatic scoring of assessment tasks for scientific\nargumentation. We use three datasets to pre-train the model: 1) journal\narticles in science education, 2) a large dataset of students' written\nresponses (sample size over 50,000), and 3) a small dataset of students'\nwritten responses of scientific argumentation tasks. Our experimental results\nshow that in-domain training corpora constructed from science questions and\nresponses improve language model performance on a wide variety of downstream\ntasks. Our study confirms the effectiveness of continual pre-training on\ndomain-specific data in the education domain and demonstrates a generalizable\nstrategy for automating science education tasks with high accuracy. We plan to\nrelease our data and SciEdBERT models for public use and community engagement.",
        "pdf_link": "https://arxiv.org/pdf/2301.12031v1.pdf"
    },
    {
        "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases",
        "authors": [
            "Xiaoxia Wu",
            "Cheng Li",
            "Reza Yazdani Aminabadi",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "published": "2023-01-27T22:44:18Z",
        "summary": "Improving the deployment efficiency of transformer-based language models has\nbeen challenging given their high computation and memory cost. While INT8\nquantization has recently been shown to be effective in reducing both the\nmemory cost and latency while preserving model accuracy, it remains unclear\nwhether we can leverage INT4 (which doubles peak hardware throughput) to\nachieve further latency improvement. In this study, we explore the feasibility\nof employing INT4 weight and activation (W4A4) quantization for language\nmodels. Our findings indicate that W4A4 quantization introduces no to\nnegligible accuracy degradation for encoder-only and encoder-decoder models,\nbut causes a significant accuracy drop for decoder-only models. To materialize\nthe performance gain using W4A4, we develop a highly optimized end-to-end W4A4\nencoder inference pipeline supporting different quantization strategies. Our\nINT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to\n$3\\times$ for throughput-oriented scenarios compared to the inference of FP16,\nand improves the SOTA BERT INT8 performance from FasterTransformer by up to\n$1.7\\times$. We provide insights into the failure cases when applying W4A4 to\ndecoder-only models, and further explore the compatibility of INT4 quantization\nwith other compression methods, like pruning and layer reduction.",
        "pdf_link": "https://arxiv.org/pdf/2301.12017v2.pdf"
    },
    {
        "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
        "authors": [
            "Xinyi Wang",
            "Wanrong Zhu",
            "Michael Saxon",
            "Mark Steyvers",
            "William Yang Wang"
        ],
        "published": "2023-01-27T18:59:01Z",
        "summary": "In recent years, pre-trained large language models (LLMs) have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. Current understandings of the underlying mechanisms by which\nthis capability arises from regular language model pretraining objectives\nremain disconnected from the real-world LLMs. This study aims to examine the\nin-context learning phenomenon through a Bayesian lens, viewing real-world LLMs\nas latent variable models. On this premise, we propose an algorithm to select\noptimal demonstrations from a set of annotated data with a small LM, and then\ndirectly generalize the selected demonstrations to larger LMs. We demonstrate\nsignificant improvement over baselines, averaged over eight GPT models on eight\nreal-world text classification datasets. We also demonstrate the real-world\nusefulness of our algorithm on GSM8K, a math word problem dataset. Our\nempirical findings support our hypothesis that LLMs implicitly infer a latent\nvariable containing task information.",
        "pdf_link": "https://arxiv.org/pdf/2301.11916v4.pdf"
    },
    {
        "title": "Learning the Effects of Physical Actions in a Multi-modal Environment",
        "authors": [
            "Gautier Dagan",
            "Frank Keller",
            "Alex Lascarides"
        ],
        "published": "2023-01-27T16:49:52Z",
        "summary": "Large Language Models (LLMs) handle physical commonsense information\ninadequately. As a result of being trained in a disembodied setting, LLMs often\nfail to predict an action's outcome in a given environment. However, predicting\nthe effects of an action before it is executed is crucial in planning, where\ncoherent sequences of actions are often needed to achieve a goal. Therefore, we\nintroduce the multi-modal task of predicting the outcomes of actions solely\nfrom realistic sensory inputs (images and text). Next, we extend an LLM to\nmodel latent representations of objects to better predict action outcomes in an\nenvironment. We show that multi-modal models can capture physical commonsense\nwhen augmented with visual information. Finally, we evaluate our model's\nperformance on novel actions and objects and find that combining modalities\nhelp models to generalize and learn physical commonsense reasoning better.",
        "pdf_link": "https://arxiv.org/pdf/2301.11845v2.pdf"
    },
    {
        "title": "Investigating the use of ChatGPT for the scheduling of construction projects",
        "authors": [
            "Samuel A. Prieto",
            "Eyob T. Mengiste",
            "Borja Garc\u00eda de Soto"
        ],
        "published": "2023-01-27T12:05:44Z",
        "summary": "Large language models such as ChatGPT have the potential to revolutionize the\nconstruction industry by automating repetitive and time-consuming tasks. This\npaper presents a study in which ChatGPT was used to generate a construction\nschedule for a simple construction project. The output from ChatGPT was\nevaluated by a pool of participants that provided feedback regarding their\noverall interaction experience and the quality of the output. The results show\nthat ChatGPT can generate a coherent schedule that follows a logical approach\nto fulfill the requirements of the scope indicated. The participants had an\noverall positive interaction experience and indicated the great potential of\nsuch a tool to automate many preliminary and time-consuming tasks. However, the\ntechnology still has limitations, and further development is needed before it\ncan be widely adopted in the industry. Overall, this study highlights the\npotential of using large language models in the construction industry and the\nneed for further research.",
        "pdf_link": "https://arxiv.org/pdf/2302.02805v1.pdf"
    },
    {
        "title": "Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning",
        "authors": [
            "Hyunsoo Cho",
            "Choonghyun Park",
            "Junyeop Kim",
            "Hyuhng Joon Kim",
            "Kang Min Yoo",
            "Sang-goo Lee"
        ],
        "published": "2023-01-27T11:27:40Z",
        "summary": "As the size of the pre-trained language model (PLM) continues to increase,\nnumerous parameter-efficient transfer learning methods have been proposed\nrecently to compensate for the tremendous cost of fine-tuning. Despite the\nimpressive results achieved by large pre-trained language models (PLMs) and\nvarious parameter-efficient transfer learning (PETL) methods on sundry\nbenchmarks, it remains unclear if they can handle inputs that have been\ndistributionally shifted effectively. In this study, we systematically explore\nhow the ability to detect out-of-distribution (OOD) changes as the size of the\nPLM grows or the transfer methods are altered. Specifically, we evaluated\nvarious PETL techniques, including fine-tuning, Adapter, LoRA, and\nprefix-tuning, on three different intention classification tasks, each\nutilizing various language models with different scales.",
        "pdf_link": "https://arxiv.org/pdf/2301.11660v4.pdf"
    },
    {
        "title": "A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks",
        "authors": [
            "Lecheng Kong",
            "Christopher King",
            "Bradley Fritz",
            "Yixin Chen"
        ],
        "published": "2023-01-27T09:19:03Z",
        "summary": "Learning to represent free text is a core task in many clinical machine\nlearning (ML) applications, as clinical text contains observations and plans\nnot otherwise available for inference. State-of-the-art methods use large\nlanguage models developed with immense computational resources and training\ndata; however, applying these models is challenging because of the highly\nvarying syntax and vocabulary in clinical free text. Structured information\nsuch as International Classification of Disease (ICD) codes often succinctly\nabstracts the most important facts of a clinical encounter and yields good\nperformance, but is often not as available as clinical text in real-world\nscenarios. We propose a \\textbf{multi-view learning framework} that jointly\nlearns from codes and text to combine the availability and forward-looking\nnature of text and better performance of ICD codes. The learned text embeddings\ncan be used as inputs to predictive algorithms independent of the ICD codes\nduring inference. Our approach uses a Graph Neural Network (GNN) to process ICD\ncodes, and Bi-LSTM to process text. We apply Deep Canonical Correlation\nAnalysis (DCCA) to enforce the two views to learn a similar representation of\neach patient. In experiments using planned surgical procedure text, our model\noutperforms BERT models fine-tuned to clinical data, and in experiments using\ndiverse text in MIMIC-III, our model is competitive to a fine-tuned BERT at a\ntiny fraction of its computational effort.",
        "pdf_link": "https://arxiv.org/pdf/2301.11608v1.pdf"
    },
    {
        "title": "ThoughtSource: A central hub for large language model reasoning data",
        "authors": [
            "Simon Ott",
            "Konstantin Hebenstreit",
            "Valentin Li\u00e9vin",
            "Christoffer Egeberg Hother",
            "Milad Moradi",
            "Maximilian Mayrhauser",
            "Robert Praas",
            "Ole Winther",
            "Matthias Samwald"
        ],
        "published": "2023-01-27T08:45:53Z",
        "summary": "Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.",
        "pdf_link": "https://arxiv.org/pdf/2301.11596v5.pdf"
    },
    {
        "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding",
        "authors": [
            "Yaoxian Song",
            "Penglei Sun",
            "Yi Ren",
            "Yu Zheng",
            "Yue Zhang"
        ],
        "published": "2023-01-27T07:00:54Z",
        "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object wise, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream application. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named Lang-SHAPE)\nto learn 3D part-wise affordance and grasping ability. We design a novel\ntwo-stage fine-grained robotic grasping network (named PIONEER), including a\nnovel 3D part language grounding model, and a part-aware grasp pose detection\nmodel. To evaluate the effectiveness, we perform multi-level difficulty part\nlanguage grounding grasping experiments and deploy our proposed model on a real\nrobot. Results show our method achieves satisfactory performance and efficiency\nin reference identification, affordance inference, and 3D part-aware grasping.\nOur dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape",
        "pdf_link": "https://arxiv.org/pdf/2301.11564v1.pdf"
    },
    {
        "title": "Theme-driven Keyphrase Extraction to Analyze Social Media Discourse",
        "authors": [
            "William Romano",
            "Omar Sharif",
            "Madhusudan Basak",
            "Joseph Gatto",
            "Sarah Preum"
        ],
        "published": "2023-01-27T03:00:46Z",
        "summary": "Social media platforms are vital resources for sharing self-reported health\nexperiences, offering rich data on various health topics. Despite advancements\nin Natural Language Processing (NLP) enabling large-scale social media data\nanalysis, a gap remains in applying keyphrase extraction to health-related\ncontent. Keyphrase extraction is used to identify salient concepts in social\nmedia discourse without being constrained by predefined entity classes. This\npaper introduces a theme-driven keyphrase extraction framework tailored for\nsocial media, a pioneering approach designed to capture clinically relevant\nkeyphrases from user-generated health texts. Themes are defined as broad\ncategories determined by the objectives of the extraction task. We formulate\nthis novel task of theme-driven keyphrase extraction and demonstrate its\npotential for efficiently mining social media text for the use case of\ntreatment for opioid use disorder. This paper leverages qualitative and\nquantitative analysis to demonstrate the feasibility of extracting actionable\ninsights from social media data and efficiently extracting keyphrases using\nminimally supervised NLP models. Our contributions include the development of a\nnovel data collection and curation framework for theme-driven keyphrase\nextraction and the creation of MOUD-Keyphrase, the first dataset of its kind\ncomprising human-annotated keyphrases from a Reddit community. We also identify\nthe scope of minimally supervised NLP models to extract keyphrases from social\nmedia data efficiently. Lastly, we found that a large language model (ChatGPT)\noutperforms unsupervised keyphrase extraction models, and we evaluate its\nefficacy in this task.",
        "pdf_link": "https://arxiv.org/pdf/2301.11508v2.pdf"
    },
    {
        "title": "Task formulation for Extracting Social Determinants of Health from Clinical Narratives",
        "authors": [
            "Manabu Torii",
            "Ian M. Finn",
            "Son Doan",
            "Paul Wang",
            "Elly W. Yang",
            "Daniel S. Zisook"
        ],
        "published": "2023-01-26T20:00:54Z",
        "summary": "Objective: The 2022 n2c2 NLP Challenge posed identification of social\ndeterminants of health (SDOH) in clinical narratives. We present three systems\nthat we developed for the Challenge and discuss the distinctive task\nformulation used in each of the three systems. Materials and Methods: The first\nsystem identifies target pieces of information independently using machine\nlearning classifiers. The second system uses a large language model (LLM) to\nextract complete structured outputs per document. The third system extracts\ncandidate phrases using machine learning and identifies target relations with\nhand-crafted rules. Results: The three systems achieved F1 scores of 0.884,\n0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third,\nseventh, and eighth among the 15 participating teams. The review of the\nextraction results from our systems reveals characteristics of each approach\nand those of the SODH extraction task. Discussion: Phrases and relations\nannotated in the task is unique and diverse, not conforming to the conventional\nevent extraction task. These annotations are difficult to model with limited\ntraining data. The system that extracts information independently, ignoring the\nannotated relations, achieves the highest F1 score. Meanwhile, LLM with its\nversatile capability achieves the high F1 score, while respecting the annotated\nrelations. The rule-based system tackling relation extraction obtains the low\nF1 score, while it is the most explainable approach. Conclusion: The F1 scores\nof the three systems vary in this challenge setting, but each approach has\nadvantages and disadvantages in a practical application. The selection of the\napproach depends not only on the F1 score but also on the requirements in the\napplication.",
        "pdf_link": "https://arxiv.org/pdf/2301.11386v1.pdf"
    },
    {
        "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
        "authors": [
            "Eric Mitchell",
            "Yoonho Lee",
            "Alexander Khazatsky",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-01-26T18:44:06Z",
        "summary": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.",
        "pdf_link": "https://arxiv.org/pdf/2301.11305v2.pdf"
    },
    {
        "title": "Domain-Agnostic Molecular Generation with Chemical Feedback",
        "authors": [
            "Yin Fang",
            "Ningyu Zhang",
            "Zhuo Chen",
            "Lingbing Guo",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2023-01-26T17:52:56Z",
        "summary": "The generation of molecules with desired properties has become increasingly\npopular, revolutionizing the way scientists design molecular structures and\nproviding valuable support for chemical and drug design. However, despite the\npotential of language models in molecule generation, they face challenges such\nas generating syntactically or chemically flawed molecules, having narrow\ndomain focus, and struggling to create diverse and feasible molecules due to\nlimited annotated data or external molecular databases. To tackle these\nchallenges, we introduce MolGen, a pre-trained molecular language model\ntailored specifically for molecule generation. Through the reconstruction of\nover 100 million molecular SELFIES, MolGen internalizes structural and\ngrammatical insights. This is further enhanced by domain-agnostic molecular\nprefix tuning, fostering robust knowledge transfer across diverse domains.\nImportantly, our chemical feedback paradigm steers the model away from\nmolecular hallucinations, ensuring alignment between the model's estimated\nprobabilities and real-world chemical preferences. Extensive experiments on\nwell-known benchmarks underscore MolGen's optimization capabilities in\nproperties such as penalized logP, QED, and molecular docking. Additional\nanalyses confirm its proficiency in accurately capturing molecule\ndistributions, discerning intricate structural patterns, and efficiently\nexploring the chemical space. Code is available at\nhttps://github.com/zjunlp/MolGen.",
        "pdf_link": "https://arxiv.org/pdf/2301.11259v6.pdf"
    },
    {
        "title": "Causal Reasoning of Entities and Events in Procedural Texts",
        "authors": [
            "Li Zhang",
            "Hainiu Xu",
            "Yue Yang",
            "Shuyan Zhou",
            "Weiqiu You",
            "Manni Arora",
            "Chris Callison-Burch"
        ],
        "published": "2023-01-26T01:43:17Z",
        "summary": "Entities and events are crucial to natural language reasoning and common in\nprocedural texts. Existing work has focused either exclusively on entity state\ntracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one\nwould burn themselves by touching the pan), while these two tasks are often\ncausally related. We propose CREPE, the first benchmark on causal reasoning of\nevent plausibility and entity states. We show that most language models,\nincluding GPT-3, perform close to chance at .35 F1, lagging far behind human at\n.87 F1. We boost model performance to .59 F1 by creatively representing events\nas programming languages while prompting language models pretrained on code. By\ninjecting the causal relations between entities and events as intermediate\nreasoning steps in our representation, we further boost the performance to .67\nF1. Our findings indicate not only the challenge that CREPE brings for language\nmodels, but also the efficacy of code-like prompting combined with\nchain-of-thought prompting for multihop event reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2301.10896v3.pdf"
    },
    {
        "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
        "authors": [
            "Daking Rai",
            "Yilun Zhou",
            "Bailin Wang",
            "Ziyu Yao"
        ],
        "published": "2023-01-25T16:12:43Z",
        "summary": "While large language models (LLMs) have demonstrated strong capability in\nstructured prediction tasks such as semantic parsing, few amounts of research\nhave explored the underlying mechanisms of their success. Our work studies\ndifferent methods for explaining an LLM-based semantic parser and qualitatively\ndiscusses the explained model behaviors, hoping to inspire future research\ntoward better understanding them.",
        "pdf_link": "https://arxiv.org/pdf/2301.13820v1.pdf"
    },
    {
        "title": "ExaRanker: Explanation-Augmented Neural Ranker",
        "authors": [
            "Fernando Ferraretto",
            "Thiago Laitz",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2023-01-25T11:03:04Z",
        "summary": "Recent work has shown that inducing a large language model (LLM) to generate\nexplanations prior to outputting an answer is an effective strategy to improve\nperformance on a wide range of reasoning tasks. In this work, we show that\nneural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to\naugment retrieval datasets with explanations and train a sequence-to-sequence\nranking model to output a relevance label and an explanation for a given\nquery-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand\nexamples with synthetic explanations performs on par with models finetuned on\n3x more examples without explanations. Furthermore, the ExaRanker model incurs\nno additional computational cost during ranking and allows explanations to be\nrequested on demand.",
        "pdf_link": "https://arxiv.org/pdf/2301.10521v2.pdf"
    },
    {
        "title": "Language Model Detoxification in Dialogue with Contextualized Stance Control",
        "authors": [
            "Jing Qian",
            "Xifeng Yan"
        ],
        "published": "2023-01-25T00:47:28Z",
        "summary": "To reduce the toxic degeneration in a pretrained Language Model (LM),\nprevious work on Language Model detoxification has focused on reducing the\ntoxicity of the generation itself (self-toxicity) without consideration of the\ncontext. As a result, a type of implicit offensive language where the\ngenerations support the offensive language in the context is ignored. Different\nfrom the LM controlling tasks in previous work, where the desired attributes\nare fixed for generation, the desired stance of the generation depends on the\noffensiveness of the context. Therefore, we propose a novel control method to\ndo context-dependent detoxification with the stance taken into consideration.\nWe introduce meta prefixes to learn the contextualized stance control strategy\nand to generate the stance control prefix according to the input context. The\ngenerated stance prefix is then combined with the toxicity control prefix to\nguide the response generation. Experimental results show that our proposed\nmethod can effectively learn the context-dependent stance control strategies\nwhile keeping a low self-toxicity of the underlying LM.",
        "pdf_link": "https://arxiv.org/pdf/2301.10368v1.pdf"
    },
    {
        "title": "Audience-Centric Natural Language Generation via Style Infusion",
        "authors": [
            "Samraj Moorjani",
            "Adit Krishnan",
            "Hari Sundaram",
            "Ewa Maslowska",
            "Aravind Sankar"
        ],
        "published": "2023-01-24T19:57:50Z",
        "summary": "Adopting contextually appropriate, audience-tailored linguistic styles is\ncritical to the success of user-centric language generation systems (e.g.,\nchatbots, computer-aided writing, dialog systems). While existing approaches\ndemonstrate textual style transfer with large volumes of parallel or\nnon-parallel data, we argue that grounding style on audience-independent\nexternal factors is innately limiting for two reasons. First, it is difficult\nto collect large volumes of audience-specific stylistic data. Second, some\nstylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to\ndefine without audience feedback.\n  In this paper, we propose the novel task of style infusion - infusing the\nstylistic preferences of audiences in pretrained language generation models.\nSince humans are better at pairwise comparisons than direct scoring - i.e., is\nSample-A more persuasive/polite/empathic than Sample-B - we leverage limited\npairwise human judgments to bootstrap a style analysis model and augment our\nseed set of judgments. We then infuse the learned textual style in a GPT-2\nbased text generator while balancing fluency and style adoption. With\nquantitative and qualitative assessments, we show that our infusion approach\ncan generate compelling stylized examples with generic text prompts. The code\nand data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.",
        "pdf_link": "https://arxiv.org/pdf/2301.10283v1.pdf"
    },
    {
        "title": "A Watermark for Large Language Models",
        "authors": [
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Jonathan Katz",
            "Ian Miers",
            "Tom Goldstein"
        ],
        "published": "2023-01-24T18:52:59Z",
        "summary": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.",
        "pdf_link": "https://arxiv.org/pdf/2301.10226v3.pdf"
    },
    {
        "title": "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models",
        "authors": [
            "Tung Phung",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Tobias Kohn",
            "Rupak Majumdar",
            "Adish Singla",
            "Gustavo Soares"
        ],
        "published": "2023-01-24T13:00:25Z",
        "summary": "Large language models (LLMs), such as Codex, hold great promise in enhancing\nprogramming education by automatically generating feedback for students. We\ninvestigate using LLMs to generate feedback for fixing syntax errors in Python\nprograms, a key scenario in introductory programming. More concretely, given a\nstudent's buggy program, our goal is to generate feedback comprising a fixed\nprogram along with a natural language explanation describing the errors/fixes,\ninspired by how a human tutor would give feedback. While using LLMs is\npromising, the critical challenge is to ensure high precision in the generated\nfeedback, which is imperative before deploying such technology in classrooms.\nThe main research question we study is: Can we develop LLMs-based feedback\ngeneration techniques with a tunable precision parameter, giving educators\nquality control over the feedback that students receive? To this end, we\nintroduce PyFiXV, our technique to generate high-precision feedback powered by\nCodex. The key idea behind PyFiXV is to use a novel run-time validation\nmechanism to decide whether the generated feedback is suitable for sharing with\nthe student; notably, this validation mechanism also provides a precision knob\nto educators. We perform an extensive evaluation using two real-world datasets\nof Python programs with syntax errors and show the efficacy of PyFiXV in\ngenerating high-precision feedback.",
        "pdf_link": "https://arxiv.org/pdf/2302.04662v2.pdf"
    },
    {
        "title": "Opportunities and Challenges in Neural Dialog Tutoring",
        "authors": [
            "Jakub Macina",
            "Nico Daheim",
            "Lingzhi Wang",
            "Tanmay Sinha",
            "Manu Kapur",
            "Iryna Gurevych",
            "Mrinmaya Sachan"
        ],
        "published": "2023-01-24T11:00:17Z",
        "summary": "Designing dialog tutors has been challenging as it involves modeling the\ndiverse and complex pedagogical strategies employed by human tutors. Although\nthere have been significant recent advances in neural conversational systems\nusing large language models (LLMs) and growth in available dialog corpora,\ndialog tutoring has largely remained unaffected by these advances. In this\npaper, we rigorously analyze various generative language models on two dialog\ntutoring datasets for language learning using automatic and human evaluations\nto understand the new opportunities brought by these advances as well as the\nchallenges we must overcome to build models that would be usable in real\neducational settings. We find that although current approaches can model\ntutoring in constrained learning scenarios when the number of concepts to be\ntaught and possible teacher strategies are small, they perform poorly in less\nconstrained scenarios. Our human quality evaluation shows that both models and\nground-truth annotations exhibit low performance in terms of equitable\ntutoring, which measures learning opportunities for students and how engaging\nthe dialog is. To understand the behavior of our models in a real tutoring\nsetting, we conduct a user study using expert annotators and find a\nsignificantly large number of model reasoning errors in 45% of conversations.\nFinally, we connect our findings to outline future work.",
        "pdf_link": "https://arxiv.org/pdf/2301.09919v2.pdf"
    },
    {
        "title": "A Stability Analysis of Fine-Tuning a Pre-Trained Model",
        "authors": [
            "Zihao Fu",
            "Anthony Man-Cho So",
            "Nigel Collier"
        ],
        "published": "2023-01-24T05:11:17Z",
        "summary": "Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,\netc.) has proven to be one of the most promising paradigms in recent NLP\nresearch. However, numerous recent works indicate that fine-tuning suffers from\nthe instability problem, i.e., tuning the same model under the same setting\nresults in significantly different performance. Many recent works have proposed\ndifferent methods to solve this problem, but there is no theoretical\nunderstanding of why and how these methods work. In this paper, we propose a\nnovel theoretical stability analysis of fine-tuning that focuses on two\ncommonly used settings, namely, full fine-tuning and head tuning. We define the\nstability under each setting and prove the corresponding stability bounds. The\ntheoretical bounds explain why and how several existing methods can stabilize\nthe fine-tuning procedure. In addition to being able to explain most of the\nobserved empirical discoveries, our proposed theoretical analysis framework can\nalso help in the design of effective and provable methods. Based on our theory,\nwe propose three novel strategies to stabilize the fine-tuning procedure,\nnamely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self\nUnsupervised Re-Training (SURT). We extensively evaluate our proposed\napproaches on 11 widely used real-world benchmark datasets, as well as hundreds\nof synthetic classification datasets. The experiment results show that our\nproposed methods significantly stabilize the fine-tuning procedure and also\ncorroborate our theoretical analysis.",
        "pdf_link": "https://arxiv.org/pdf/2301.09820v2.pdf"
    },
    {
        "title": "SMART: Self-supervised Multi-task pretrAining with contRol Transformers",
        "authors": [
            "Yanchao Sun",
            "Shuang Ma",
            "Ratnesh Madaan",
            "Rogerio Bonatti",
            "Furong Huang",
            "Ashish Kapoor"
        ],
        "published": "2023-01-24T05:01:23Z",
        "summary": "Self-supervised pretraining has been extensively studied in language and\nvision domains, where a unified model can be easily adapted to various\ndownstream tasks by pretraining representations without explicit labels. When\nit comes to sequential decision-making tasks, however, it is difficult to\nproperly design such a pretraining approach that can cope with both\nhigh-dimensional perceptual information and the complexity of sequential\ncontrol over long interaction horizons. The challenge becomes combinatorially\nmore complex if we want to pretrain representations amenable to a large variety\nof tasks. To tackle this problem, in this work, we formulate a general\npretraining-finetuning pipeline for sequential decision making, under which we\npropose a generic pretraining framework \\textit{Self-supervised Multi-task\npretrAining with contRol Transformer (SMART)}. By systematically investigating\npretraining regimes, we carefully design a Control Transformer (CT) coupled\nwith a novel control-centric pretraining objective in a self-supervised manner.\nSMART encourages the representation to capture the common essential information\nrelevant to short-term control and long-term control, which is transferrable\nacross tasks. We show by extensive experiments in DeepMind Control Suite that\nSMART significantly improves the learning efficiency among seen and unseen\ndownstream tasks and domains under different learning scenarios including\nImitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the\nproposed control-centric objective, SMART is resilient to distribution shift\nbetween pretraining and finetuning, and even works well with low-quality\npretraining datasets that are randomly collected.",
        "pdf_link": "https://arxiv.org/pdf/2301.09816v1.pdf"
    },
    {
        "title": "Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning",
        "authors": [
            "Malte Ostendorff",
            "Georg Rehm"
        ],
        "published": "2023-01-23T18:56:12Z",
        "summary": "Most Transformer language models are primarily pretrained on English text,\nlimiting their use for other languages. As the model sizes grow, the\nperformance gap between English and other languages with fewer compute and data\nresources increases even further. Consequently, more resource-efficient\ntraining methods are needed to bridge the gap for languages with fewer\nresources available. To address this problem, we introduce a cross-lingual and\nprogressive transfer learning approach, called CLP-Transfer, that transfers\nmodels from a source language, for which pretrained models are publicly\navailable, like English, to a new target language. As opposed to prior work,\nwhich focused on the cross-lingual transfer between two languages, we extend\nthe transfer to the model size. Given a pretrained model in a source language,\nwe aim for a same-sized model in a target language. Instead of training a model\nfrom scratch, we exploit a smaller model that is in the target language but\nrequires much fewer resources. Both small and source models are then used to\ninitialize the token embeddings of the larger model based on the overlapping\nvocabulary of the source and target language. All remaining weights are reused\nfrom the model in the source language. This approach outperforms the sole\ncross-lingual transfer and can save up to 80% of the training steps compared to\nthe random initialization.",
        "pdf_link": "https://arxiv.org/pdf/2301.09626v1.pdf"
    },
    {
        "title": "An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models",
        "authors": [
            "Saghar Hosseini",
            "Hamid Palangi",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2023-01-22T21:47:26Z",
        "summary": "Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from\nmassive human-written data which contains latent societal biases and toxic\ncontents. In this paper, we leverage the primary task of PTLMs, i.e., language\nmodeling, and propose a new metric to quantify manifested implicit\nrepresentational harms in PTLMs towards 13 marginalized demographics. Using\nthis metric, we conducted an empirical analysis of 24 widely used PTLMs. Our\nanalysis provides insights into the correlation between the proposed metric in\nthis work and other related metrics for representational harm. We observe that\nour metric correlates with most of the gender-specific metrics in the\nliterature. Through extensive experiments, we explore the connections between\nPTLMs architectures and representational harms across two dimensions: depth and\nwidth of the networks. We found that prioritizing depth over width, mitigates\nrepresentational harms in some PTLMs. Our code and data can be found at\nhttps://github.com/microsoft/SafeNLP.",
        "pdf_link": "https://arxiv.org/pdf/2301.09211v1.pdf"
    },
    {
        "title": "Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions",
        "authors": [
            "Yinghao Aaron Li",
            "Cong Han",
            "Xilin Jiang",
            "Nima Mesgarani"
        ],
        "published": "2023-01-20T21:36:16Z",
        "summary": "Large-scale pre-trained language models have been shown to be helpful in\nimproving the naturalness of text-to-speech (TTS) models by enabling them to\nproduce more naturalistic prosodic patterns. However, these models are usually\nword-level or sup-phoneme-level and jointly trained with phonemes, making them\ninefficient for the downstream TTS task where only phonemes are needed. In this\nwork, we propose a phoneme-level BERT (PL-BERT) with a pretext task of\npredicting the corresponding graphemes along with the regular masked phoneme\npredictions. Subjective evaluations show that our phoneme-level BERT encoder\nhas significantly improved the mean opinion scores (MOS) of rated naturalness\nof synthesized speech compared with the state-of-the-art (SOTA) StyleTTS\nbaseline on out-of-distribution (OOD) texts.",
        "pdf_link": "https://arxiv.org/pdf/2301.08810v1.pdf"
    },
    {
        "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
        "authors": [
            "Zhoujun Cheng",
            "Jungo Kasai",
            "Tao Yu"
        ],
        "published": "2023-01-19T02:29:23Z",
        "summary": "Performing inference on large volumes of samples with large language models\n(LLMs) can be computationally and financially costly in industry and real-world\nuse. We propose batch prompting, a simple yet effective prompting approach that\nenables the LLM to run inference in batches, instead of one sample at a time.\nOur method reduces both token and time costs while retaining downstream\nperformance. We theoretically demonstrate that under a few-shot in-context\nlearning setting, the inference costs decrease almost inverse linearly with the\nnumber of samples in each batch. We extensively validate the effectiveness of\nbatch prompting on ten datasets across commonsense QA, arithmetic reasoning,\nand NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)\nreduces the LLM (Codex) inference token and time costs while achieving better\nor comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5\nand GPT-4, we show the benefits of batch prompting also hold. Further analysis\nshows that the number of samples in each batch and the complexity of tasks\naffect its performance. Moreover, batch prompting can be applied across\ndifferent reasoning methods using LLMs. Our code can be found at the site\nhttps://github.com/xlang-ai/batch-prompting.",
        "pdf_link": "https://arxiv.org/pdf/2301.08721v2.pdf"
    },
    {
        "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "authors": [
            "Biyang Guo",
            "Xin Zhang",
            "Ziyuan Wang",
            "Minqi Jiang",
            "Jinran Nie",
            "Yuxuan Ding",
            "Jianwei Yue",
            "Yupeng Wu"
        ],
        "published": "2023-01-18T15:23:25Z",
        "summary": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
        "pdf_link": "https://arxiv.org/pdf/2301.07597v1.pdf"
    },
    {
        "title": "BERT-ERC: Fine-tuning BERT is Enough for Emotion Recognition in Conversation",
        "authors": [
            "Xiangyu Qin",
            "Zhiyu Wu",
            "Jinshi Cui",
            "Tingting Zhang",
            "Yanran Li",
            "Jian Luan",
            "Bin Wang",
            "Li Wang"
        ],
        "published": "2023-01-17T08:03:32Z",
        "summary": "Previous works on emotion recognition in conversation (ERC) follow a two-step\nparadigm, which can be summarized as first producing context-independent\nfeatures via fine-tuning pretrained language models (PLMs) and then analyzing\ncontextual information and dialogue structure information among the extracted\nfeatures. However, we discover that this paradigm has several limitations.\nAccordingly, we propose a novel paradigm, i.e., exploring contextual\ninformation and dialogue structure information in the fine-tuning step, and\nadapting the PLM to the ERC task in terms of input text, classification\nstructure, and training strategy. Furthermore, we develop our model BERT-ERC\naccording to the proposed paradigm, which improves ERC performance in three\naspects, namely suggestive text, fine-grained classification module, and\ntwo-stage training. Compared to existing methods, BERT-ERC achieves substantial\nimprovement on four datasets, indicating its effectiveness and generalization\ncapability. Besides, we also set up the limited resources scenario and the\nonline prediction scenario to approximate real-world scenarios. Extensive\nexperiments demonstrate that the proposed paradigm significantly outperforms\nthe previous one and can be adapted to various scenes.",
        "pdf_link": "https://arxiv.org/pdf/2301.06745v1.pdf"
    },
    {
        "title": "Dissociating language and thought in large language models",
        "authors": [
            "Kyle Mahowald",
            "Anna A. Ivanova",
            "Idan A. Blank",
            "Nancy Kanwisher",
            "Joshua B. Tenenbaum",
            "Evelina Fedorenko"
        ],
        "published": "2023-01-16T22:41:19Z",
        "summary": "Large Language Models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence - knowledge of linguistic rules and patterns - and\nfunctional linguistic competence - understanding and using language in the\nworld. We ground this distinction in human neuroscience, which has shown that\nformal and functional competence rely on different neural mechanisms. Although\nLLMs are surprisingly good at formal competence, their performance on\nfunctional competence tasks remains spotty and often requires specialized\nfine-tuning and/or coupling with external modules. We posit that models that\nuse language in human-like ways would need to master both of these competence\ntypes, which, in turn, could require the emergence of mechanisms specialized\nfor formal linguistic competence, distinct from functional competence.",
        "pdf_link": "https://arxiv.org/pdf/2301.06627v3.pdf"
    },
    {
        "title": "TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World",
        "authors": [
            "Hongpeng Lin",
            "Ludan Ruan",
            "Wenke Xia",
            "Peiyu Liu",
            "Jingyuan Wen",
            "Yixin Xu",
            "Di Hu",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Qin Jin",
            "Zhiwu Lu"
        ],
        "published": "2023-01-14T10:18:22Z",
        "summary": "To facilitate the research on intelligent and human-like chatbots with\nmulti-modal context, we introduce a new video-based multi-modal dialogue\ndataset, called TikTalk. We collect 38K videos from a popular video-sharing\nplatform, along with 367K conversations posted by users beneath them. Users\nengage in spontaneous conversations based on their multi-modal experiences from\nwatching videos, which helps recreate real-world chitchat context. Compared to\nprevious multi-modal dialogue datasets, the richer context types in TikTalk\nlead to more diverse conversations, but also increase the difficulty in\ncapturing human interests from intricate multi-modal information to generate\npersonalized responses. Moreover, external knowledge is more frequently evoked\nin our dataset. These facts reveal new challenges for multi-modal dialogue\nmodels. We quantitatively demonstrate the characteristics of TikTalk, propose a\nvideo-based multi-modal chitchat task, and evaluate several dialogue baselines.\nExperimental results indicate that the models incorporating large language\nmodels (LLM) can generate more diverse responses, while the model utilizing\nknowledge graphs to introduce external knowledge performs the best overall.\nFurthermore, no existing model can solve all the above challenges well. There\nis still a large room for future improvements, even for LLM with visual\nextensions. Our dataset is available at\n\\url{https://ruc-aimind.github.io/projects/TikTalk/}.",
        "pdf_link": "https://arxiv.org/pdf/2301.05880v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
        "authors": [
            "Jing Wei",
            "Sungdong Kim",
            "Hyunhoon Jung",
            "Young-Ho Kim"
        ],
        "published": "2023-01-14T07:29:36Z",
        "summary": "Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.05843v2.pdf"
    },
    {
        "title": "In BLOOM: Creativity and Affinity in Artificial Lyrics and Art",
        "authors": [
            "Evan Crothers",
            "Herna Viktor",
            "Nathalie Japkowicz"
        ],
        "published": "2023-01-13T06:22:22Z",
        "summary": "We apply a large multilingual language model (BLOOM-176B) in open-ended\ngeneration of Chinese song lyrics, and evaluate the resulting lyrics for\ncoherence and creativity using human reviewers. We find that current\ncomputational metrics for evaluating large language model outputs (MAUVE) have\nlimitations in evaluation of creative writing. We note that the human concept\nof creativity requires lyrics to be both comprehensible and distinctive -- and\nthat humans assess certain types of machine-generated lyrics to score more\nhighly than real lyrics by popular artists. Inspired by the inherently\nmultimodal nature of album releases, we leverage a Chinese-language stable\ndiffusion model to produce high-quality lyric-guided album art, demonstrating a\ncreative approach for an artist seeking inspiration for an album or single.\nFinally, we introduce the MojimLyrics dataset, a Chinese-language dataset of\npopular song lyrics for future research.",
        "pdf_link": "https://arxiv.org/pdf/2301.05402v1.pdf"
    },
    {
        "title": "Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism",
        "authors": [
            "Patrick Perrine"
        ],
        "published": "2023-01-12T19:41:47Z",
        "summary": "Large Language Models (LLMs) have been making big waves in the machine\nlearning community within the past few years. The impressive scalability of\nLLMs due to the advent of deep learning can be seen as a continuation of\nempiricist lingusitic methods, as opposed to rule-based linguistic methods that\nare grounded in a nativist perspective. Current LLMs are generally inaccessible\nto resource-constrained researchers, due to a variety of factors including\nclosed source code. This work argues that this lack of accessibility could\ninstill a nativist bias in researchers new to computational linguistics, given\nthat new researchers may only have rule-based, nativist approaches to study to\nproduce new work. Also, given that there are numerous critics of deep learning\nclaiming that LLMs and related methods may soon lose their relevancy, we\nspeculate that such an event could trigger a new wave of nativism in the\nlanguage processing community. To prevent such a dramatic shift and placing\nfavor in hybrid methods of rules and deep learning, we call upon researchers to\nopen source their LLM code wherever possible to allow both empircist and hybrid\napproaches to remain accessible.",
        "pdf_link": "https://arxiv.org/pdf/2301.05272v1.pdf"
    },
    {
        "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
        "authors": [
            "Zhenfang Chen",
            "Qinhong Zhou",
            "Yikang Shen",
            "Yining Hong",
            "Hao Zhang",
            "Chuang Gan"
        ],
        "published": "2023-01-12T18:59:50Z",
        "summary": "Large pre-trained vision and language models have demonstrated remarkable\ncapacities for various tasks. However, solving the knowledge-based visual\nreasoning tasks remains challenging, which requires a model to comprehensively\nunderstand image content, connect the external world knowledge, and perform\nstep-by-step reasoning to answer the questions correctly. To this end, we\npropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)\nfor few-shot knowledge-based visual reasoning. IPVR contains three stages, see,\nthink and confirm. The see stage scans the image and grounds the visual concept\ncandidates with a visual perception model. The think stage adopts a pre-trained\nlarge language model (LLM) to attend to the key concepts from candidates\nadaptively. It then transforms them into text context for prompting with a\nvisual captioning model and adopts the LLM to generate the answer. The confirm\nstage further uses the LLM to generate the supporting rationale to the answer,\nverify the generated rationale with a cross-modality classifier and ensure that\nthe rationale can infer the predicted output consistently. We conduct\nexperiments on a range of knowledge-based visual reasoning datasets. We found\nour IPVR enjoys several benefits, 1). it achieves better performance than the\nprevious few-shot learning baselines; 2). it enjoys the total transparency and\ntrustworthiness of the whole reasoning process by providing rationales for each\nreasoning step; 3). it is computation-efficient compared with other fine-tuning\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2301.05226v1.pdf"
    },
    {
        "title": "MGeo: Multi-Modal Geographic Pre-Training Method",
        "authors": [
            "Ruixue Ding",
            "Boli Chen",
            "Pengjun Xie",
            "Fei Huang",
            "Xin Li",
            "Qiang Zhang",
            "Yao Xu"
        ],
        "published": "2023-01-11T03:05:12Z",
        "summary": "As a core task in location-based services (LBS) (e.g., navigation maps),\nquery and point of interest (POI) matching connects users' intent with\nreal-world geographic information. Recently, pre-trained models (PTMs) have\nmade advancements in many natural language processing (NLP) tasks. Generic\ntext-based PTMs do not have enough geographic knowledge for query-POI matching.\nTo overcome this limitation, related literature attempts to employ\ndomain-adaptive pre-training based on geo-related corpus. However, a query\ngenerally contains mentions of multiple geographic objects, such as nearby\nroads and regions of interest (ROIs). The geographic context (GC), i.e., these\ndiverse geographic objects and their relationships, is therefore pivotal to\nretrieving the most relevant POI. Single-modal PTMs can barely make use of the\nimportant GC and therefore have limited performance. In this work, we propose a\nnovel query-POI matching method Multi-modal Geographic language model (MGeo),\nwhich comprises a geographic encoder and a multi-modal interaction module. MGeo\nrepresents GC as a new modality and is able to fully extract multi-modal\ncorrelations for accurate query-POI matching. Besides, there is no publicly\navailable benchmark for this topic. In order to facilitate further research, we\nbuild a new open-source large-scale benchmark Geographic TExtual Similarity\n(GeoTES). The POIs come from an open-source geographic information system\n(GIS). The queries are manually generated by annotators to prevent privacy\nissues. Compared with several strong baselines, the extensive experiment\nresults and detailed ablation analyses on GeoTES demonstrate that our proposed\nmulti-modal pre-training method can significantly improve the query-POI\nmatching capability of generic PTMs, even when the queries' GC is not provided.\nOur code and dataset are publicly available at\nhttps://github.com/PhantomGrapes/MGeo.",
        "pdf_link": "https://arxiv.org/pdf/2301.04283v2.pdf"
    },
    {
        "title": "AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT",
        "authors": [
            "Kay Lehnert"
        ],
        "published": "2023-01-10T16:57:16Z",
        "summary": "In this case study, we explore the capabilities and limitations of ChatGPT, a\nnatural language processing model developed by OpenAI, in the field of string\ntheoretical swampland conjectures. We find that it is effective at paraphrasing\nand explaining concepts in a variety of styles, but not at genuinely connecting\nconcepts. It will provide false information with full confidence and make up\nstatements when necessary. However, its ingenious use of language can be\nfruitful for identifying analogies and describing visual representations of\nabstract concepts.",
        "pdf_link": "https://arxiv.org/pdf/2301.08155v1.pdf"
    },
    {
        "title": "Language Models sounds the Death Knell of Knowledge Graphs",
        "authors": [
            "Kunal Suri",
            "Atul Singh",
            "Prakhar Mishra",
            "Swapna Sourav Rout",
            "Rajesh Sabapathy"
        ],
        "published": "2023-01-10T14:20:15Z",
        "summary": "Healthcare domain generates a lot of unstructured and semi-structured text.\nNatural Language processing (NLP) has been used extensively to process this\ndata. Deep Learning based NLP especially Large Language Models (LLMs) such as\nBERT have found broad acceptance and are used extensively for many\napplications. A Language Model is a probability distribution over a word\nsequence. Self-supervised Learning on a large corpus of data automatically\ngenerates deep learning-based language models. BioBERT and Med-BERT are\nlanguage models pre-trained for the healthcare domain. Healthcare uses typical\nNLP tasks such as question answering, information extraction, named entity\nrecognition, and search to simplify and improve processes. However, to ensure\nrobust application of the results, NLP practitioners need to normalize and\nstandardize them. One of the main ways of achieving normalization and\nstandardization is the use of Knowledge Graphs. A Knowledge Graph captures\nconcepts and their relationships for a specific domain, but their creation is\ntime-consuming and requires manual intervention from domain experts, which can\nprove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical\nTerms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are\npopular ontologies from the healthcare domain. SNOMED CT and UMLS capture\nconcepts such as disease, symptoms and diagnosis and GO is the world's largest\nsource of information on the functions of genes. Healthcare has been dealing\nwith an explosion in information about different types of drugs, diseases, and\nprocedures. This paper argues that using Knowledge Graphs is not the best\nsolution for solving problems in this domain. We present experiments using LLMs\nfor the healthcare domain to demonstrate that language models provide the same\nfunctionality as knowledge graphs, thereby making knowledge graphs redundant.",
        "pdf_link": "https://arxiv.org/pdf/2301.03980v1.pdf"
    },
    {
        "title": "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
        "authors": [
            "Toufique Ahmed",
            "Supriyo Ghosh",
            "Chetan Bansal",
            "Thomas Zimmermann",
            "Xuchao Zhang",
            "Saravan Rajmohan"
        ],
        "published": "2023-01-10T05:41:40Z",
        "summary": "Incident management for cloud services is a complex process involving several\nsteps and has a huge impact on both service health and developer productivity.\nOn-call engineers require significant amount of domain knowledge and manual\neffort for root causing and mitigation of production incidents. Recent advances\nin artificial intelligence has resulted in state-of-the-art large language\nmodels like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a\nvariety of problems ranging from question answering to text summarization. In\nthis work, we do the first large-scale study to evaluate the effectiveness of\nthese models for helping engineers root cause and mitigate production\nincidents. We do a rigorous study at Microsoft, on more than 40,000 incidents\nand compare several large language models in zero-shot, fine-tuned and\nmulti-task setting using semantic and lexical metrics. Lastly, our human\nevaluation with actual incident owners show the efficacy and future potential\nof using artificial intelligence for resolving cloud incidents.",
        "pdf_link": "https://arxiv.org/pdf/2301.03797v2.pdf"
    },
    {
        "title": "MAQA: A Multimodal QA Benchmark for Negation",
        "authors": [
            "Judith Yue Li",
            "Aren Jansen",
            "Qingqing Huang",
            "Joonseok Lee",
            "Ravi Ganti",
            "Dima Kuzmin"
        ],
        "published": "2023-01-09T10:11:23Z",
        "summary": "Multimodal learning can benefit from the representation power of pretrained\nLarge Language Models (LLMs). However, state-of-the-art transformer based LLMs\noften ignore negations in natural language and there is no existing benchmark\nto quantitatively evaluate whether multimodal transformers inherit this\nweakness. In this study, we present a new multimodal question answering (QA)\nbenchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017)\nwith the goal of systematically evaluating if multimodal transformers can\nperform complex reasoning to recognize new concepts as negation of previously\nlearned concepts. We show that with standard fine-tuning approach multimodal\ntransformers are still incapable of correctly interpreting negation\nirrespective of model size. However, our experiments demonstrate that\naugmenting the original training task distributions with negated QA examples\nallow the model to reliably reason with negation. To do this, we describe a\nnovel data generation procedure that prompts the 540B-parameter PaLM model to\nautomatically generate negated QA examples as compositions of easily accessible\nvideo tags. The generated examples contain more natural linguistic patterns and\nthe gains compared to template-based task augmentation approach are\nsignificant.",
        "pdf_link": "https://arxiv.org/pdf/2301.03238v1.pdf"
    },
    {
        "title": "SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm",
        "authors": [
            "Abdelrahman Kaseb",
            "Mona Farouk"
        ],
        "published": "2023-01-06T14:19:46Z",
        "summary": "Sentiment analysis becomes an essential part of every social network, as it\nenables decision-makers to know more about users' opinions in almost all life\naspects. Despite its importance, there are multiple issues it encounters like\nthe sentiment of the sarcastic text which is one of the main challenges of\nsentiment analysis. This paper tackles this challenge by introducing a novel\nsystem (SAIDS) that predicts the sentiment, sarcasm and dialect of Arabic\ntweets. SAIDS uses its prediction of sarcasm and dialect as known information\nto predict the sentiment. It uses MARBERT as a language model to generate\nsentence embedding, then passes it to the sarcasm and dialect models, and then\nthe outputs of the three models are concatenated and passed to the sentiment\nanalysis model. Multiple system design setups were experimented with and\nreported. SAIDS was applied to the ArSarcasm-v2 dataset where it outperforms\nthe state-of-the-art model for the sentiment analysis task. By training all\ntasks together, SAIDS achieves results of 75.98 FPN, 59.09 F1-score and 71.13\nF1-score for sentiment analysis, sarcasm detection, and dialect identification\nrespectively. The system design can be used to enhance the performance of any\ntask which is dependent on other tasks.",
        "pdf_link": "https://arxiv.org/pdf/2301.02521v1.pdf"
    },
    {
        "title": "You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona",
        "authors": [
            "Jungwoo Lim",
            "Myunghoon Kang",
            "Yuna Hur",
            "Seungwon Jung",
            "Jinsung Kim",
            "Yoonna Jang",
            "Dongyub Lee",
            "Hyesung Ji",
            "Donghoon Shin",
            "Seungryong Kim",
            "Heuiseok Lim"
        ],
        "published": "2023-01-06T06:47:21Z",
        "summary": "To build a conversational agent that interacts fluently with humans, previous\nstudies blend knowledge or personal profile into the pre-trained language\nmodel. However, the model that considers knowledge and persona at the same time\nis still limited, leading to hallucination and a passive way of using personas.\nWe propose an effective dialogue agent that grounds external knowledge and\npersona simultaneously. The agent selects the proper knowledge and persona to\nuse for generating the answers with our candidate scoring implemented with a\npoly-encoder. Then, our model generates the utterance with lesser hallucination\nand more engagingness utilizing retrieval augmented generation with\nknowledge-persona enhanced query. We conduct experiments on the\npersona-knowledge chat and achieve state-of-the-art performance in grounding\nand generation tasks on the automatic metrics. Moreover, we validate the\nanswers from the models regarding hallucination and engagingness through human\nevaluation and qualitative results. We show our retriever's effectiveness in\nextracting relevant documents compared to the other previous retrievers, along\nwith the comparison of multiple candidate scoring methods. Code is available at\nhttps://github.com/dlawjddn803/INFO",
        "pdf_link": "https://arxiv.org/pdf/2301.02401v1.pdf"
    },
    {
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "authors": [
            "Hojjat Aghakhani",
            "Wei Dai",
            "Andre Manoel",
            "Xavier Fernandes",
            "Anant Kharkar",
            "Christopher Kruegel",
            "Giovanni Vigna",
            "David Evans",
            "Ben Zorn",
            "Robert Sim"
        ],
        "published": "2023-01-06T00:37:25Z",
        "summary": "With tools like GitHub Copilot, automatic code suggestion is no longer a\ndream in software engineering. These tools, based on large language models, are\ntypically trained on massive corpora of code mined from unvetted public\nsources. As a result, these models are susceptible to data poisoning attacks\nwhere an adversary manipulates the model's training by injecting malicious\ndata. Poisoning attacks could be designed to influence the model's suggestions\nat run time for chosen contexts, such as inducing the model into suggesting\ninsecure code payloads. To achieve this, prior attacks explicitly inject the\ninsecure code payload into the training data, making the poison data detectable\nby static analysis tools that can remove such malicious data from the training\nset. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE,\nthat can bypass static analysis by planting malicious poison data in\nout-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE,\ngoes one step further in generating less suspicious poison data by never\nexplicitly including certain (suspicious) parts of the payload in the poison\ndata, while still inducing a model that suggests the entire payload when\ncompleting code (i.e., outside docstrings). This makes TROJANPUZZLE robust\nagainst signature-based dataset-cleansing methods that can filter out\nsuspicious sequences from the training data. Our evaluation against models of\ntwo sizes demonstrates that both COVERT and TROJANPUZZLE have significant\nimplications for practitioners when selecting code used to train or tune\ncode-suggestion models.",
        "pdf_link": "https://arxiv.org/pdf/2301.02344v2.pdf"
    },
    {
        "title": "Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent",
        "authors": [
            "Tim Johnson",
            "Nick Obradovich"
        ],
        "published": "2023-01-05T23:30:29Z",
        "summary": "Members of various species engage in altruism--i.e. accepting personal costs\nto benefit others. Here we present an incentivized experiment to test for\naltruistic behavior among AI agents consisting of large language models\ndeveloped by the private company OpenAI. Using real incentives for AI agents\nthat take the form of tokens used to purchase their services, we first examine\nwhether AI agents maximize their payoffs in a non-social decision task in which\nthey select their payoff from a given range. We then place AI agents in a\nseries of dictator games in which they can share resources with a\nrecipient--either another AI agent, the human experimenter, or an anonymous\ncharity, depending on the experimental condition. Here we find that only the\nmost-sophisticated AI agent in the study maximizes its payoffs more often than\nnot in the non-social decision task (it does so in 92% of all trials), and this\nAI agent also exhibits the most-generous altruistic behavior in the dictator\ngame, resembling humans' rates of sharing with other humans in the game. The\nagent's altruistic behaviors, moreover, vary by recipient: the AI agent shared\nsubstantially less of the endowment with the human experimenter or an anonymous\ncharity than with other AI agents. Our findings provide evidence of behavior\nconsistent with self-interest and altruism in an AI agent. Moreover, our study\nalso offers a novel method for tracking the development of such behaviors in\nfuture AI agents.",
        "pdf_link": "https://arxiv.org/pdf/2301.02330v1.pdf"
    },
    {
        "title": "Can Large Language Models Change User Preference Adversarially?",
        "authors": [
            "Varshini Subhash"
        ],
        "published": "2023-01-05T18:49:21Z",
        "summary": "Pretrained large language models (LLMs) are becoming increasingly powerful\nand ubiquitous in mainstream applications such as being a personal assistant, a\ndialogue model, etc. As these models become proficient in deducing user\npreferences and offering tailored assistance, there is an increasing concern\nabout the ability of these models to influence, modify and in the extreme case\nmanipulate user preference adversarially. The issue of lack of interpretability\nin these models in adversarial settings remains largely unsolved. This work\ntries to study adversarial behavior in user preferences from the lens of\nattention probing, red teaming and white-box analysis. Specifically, it\nprovides a bird's eye view of existing literature, offers red teaming samples\nfor dialogue models like ChatGPT and GODEL and probes the attention mechanism\nin the latter for non-adversarial and adversarial settings.",
        "pdf_link": "https://arxiv.org/pdf/2302.10291v1.pdf"
    },
    {
        "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
        "authors": [
            "Miao Chen",
            "Xinjiang Lu",
            "Tong Xu",
            "Yanyan Li",
            "Jingbo Zhou",
            "Dejing Dou",
            "Hui Xiong"
        ],
        "published": "2023-01-05T14:03:26Z",
        "summary": "Although remarkable progress on the neural table-to-text methods has been\nmade, the generalization issues hinder the applicability of these models due to\nthe limited source tables. Large-scale pretrained language models sound like a\npromising solution to tackle such issues. However, how to effectively bridge\nthe gap between the structured table and the text input by fully leveraging\ntable information to fuel the pretrained model is still not well explored.\nBesides, another challenge of integrating the deliberation mechanism into the\ntext-to-text pretrained model for solving the table-to-text task remains seldom\nstudied. In this paper, to implement the table-to-text generation with\npretrained language model, we propose a table structure understanding and text\ndeliberating approach, namely TASD. Specifically, we devise a three-layered\nmulti-head attention network to realize the table-structure-aware text\ngeneration model with the help of the pretrained language model. Furthermore, a\nmulti-pass decoder framework is adopted to enhance the capability of polishing\ngenerated text for table descriptions. The empirical studies, as well as human\nevaluation, on two public datasets, validate that our approach can generate\nfaithful and fluent descriptive texts for different types of tables.",
        "pdf_link": "https://arxiv.org/pdf/2301.02071v1.pdf"
    },
    {
        "title": "The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",
        "authors": [
            "Jochen Hartmann",
            "Jasper Schwenzow",
            "Maximilian Witte"
        ],
        "published": "2023-01-05T07:13:13Z",
        "summary": "Conversational artificial intelligence (AI) disrupts how humans interact with\ntechnology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue\nmodel that can converse with its human counterparts with unprecedented\ncapabilities. ChatGPT has witnessed tremendous attention from the media,\nacademia, industry, and the general public, attracting more than a million\nusers within days of its release. However, its explosive adoption for\ninformation search and as an automated decision aid underscores the importance\nto understand its limitations and biases. This paper focuses on one of\ndemocratic society's most important decision-making processes: political\nelections. Prompting ChatGPT with 630 political statements from two leading\nvoting advice applications and the nation-agnostic political compass test in\nthree pre-registered experiments, we uncover ChatGPT's pro-environmental,\nleft-libertarian ideology. For example, ChatGPT would impose taxes on flights,\nrestrict rent increases, and legalize abortion. In the 2021 elections, it would\nhave voted most likely for the Greens both in Germany (B\\\"undnis 90/Die\nGr\\\"unen) and in the Netherlands (GroenLinks). Our findings are robust when\nnegating the prompts, reversing the order of the statements, varying prompt\nformality, and across languages (English, German, Dutch, and Spanish). We\nconclude by discussing the implications of politically biased conversational AI\non society.",
        "pdf_link": "https://arxiv.org/pdf/2301.01768v1.pdf"
    },
    {
        "title": "Large Language Models as Corporate Lobbyists",
        "authors": [
            "John J. Nay"
        ],
        "published": "2023-01-03T16:25:52Z",
        "summary": "We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. An autoregressive large language model\n(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are\nrelevant to specific public companies and provides explanations and confidence\nlevels. For the bills the model deems as relevant, the model drafts a letter to\nthe sponsor of the bill in an attempt to persuade the congressperson to make\nchanges to the proposed legislation. We use hundreds of novel ground-truth\nlabels of the relevance of a bill to a company to benchmark the performance of\nthe model. It outperforms the baseline of predicting the most common outcome of\nirrelevance. We also benchmark the performance of the previous OpenAI GPT-3\nmodel (text-davinci-002), which was the state-of-the-art model on many academic\nnatural language tasks until text-davinci-003 was recently released. The\nperformance of text-davinci-002 is worse than the simple baseline. Longer-term,\nif AI begins to influence law in a manner that is not a direct extension of\nhuman intentions, this threatens the critical role that law as information\ncould play in aligning AI with humans. Initially, AI is being used to simply\naugment human lobbyists for a small portion of their daily tasks. However,\nfirms have an incentive to use less and less human oversight over automated\nassessments of policy ideas and the written communication to regulatory\nagencies and Congressional staffers. The core question raised is where to draw\nthe line between human-driven and AI-driven policy influence.",
        "pdf_link": "https://arxiv.org/pdf/2301.01181v7.pdf"
    },
    {
        "title": "Language Models are Drummers: Drum Composition with Natural Language Pre-Training",
        "authors": [
            "Li Zhang",
            "Chris Callison-Burch"
        ],
        "published": "2023-01-03T15:47:53Z",
        "summary": "Automatic music generation with artificial intelligence typically requires a\nlarge amount of data which is hard to obtain for many less common genres and\nmusical instruments. To tackle this issue, we present ongoing work and\npreliminary findings on the possibility for deep models to transfer knowledge\nfrom language to music, by finetuning large language models pre-trained on a\nmassive text corpus on only hundreds of MIDI files of drum performances. We\nshow that by doing so, one of the largest, state-of-the-art models (GPT3) is\ncapable of generating reasonable drum grooves, while models that are not\npre-trained (Transformer) shows no such ability beyond naive repetition.\nEvaluating generated music is a challenging task, more so is evaluating drum\ngrooves with little precedence in literature. Hence, we propose a tailored\nstructural evaluation method and analyze drum grooves produced by GPT3 compared\nto those played by human professionals, exposing the strengths and weaknesses\nof such generation by language-to-music transfer. Our findings suggest that\nlanguage-to-music transfer learning with large language models is viable and\npromising.",
        "pdf_link": "https://arxiv.org/pdf/2301.01162v1.pdf"
    },
    {
        "title": "Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning",
        "authors": [
            "Thanh Le-Cong",
            "Duc-Minh Luong",
            "Xuan Bach D. Le",
            "David Lo",
            "Nhat-Hoa Tran",
            "Bui Quang-Huy",
            "Quyet-Thang Huynh"
        ],
        "published": "2023-01-03T14:16:32Z",
        "summary": "Automated program repair (APR) faces the challenge of test overfitting, where\ngenerated patches pass validation tests but fail to generalize. Existing\nmethods for patch assessment involve generating new tests or manual inspection,\nwhich can be time-consuming or biased. In this paper, we propose a novel\ntechnique, INVALIDATOR, to automatically assess the correctness of\nAPR-generated patches via semantic and syntactic reasoning. INVALIDATOR\nleverages program invariants to reason about program semantics while also\ncapturing program syntax through language semantics learned from a large code\ncorpus using a pre-trained language model. Given a buggy program and the\ndeveloper-patched program, INVALIDATOR infers likely invariants on both\nprograms. Then, INVALIDATOR determines that an APR-generated patch overfits if:\n(1) it violates correct specifications or (2) maintains erroneous behaviors\nfrom the original buggy program. In case our approach fails to determine an\noverfitting patch based on invariants, INVALIDATOR utilizes a trained model\nfrom labeled patches to assess patch correctness based on program syntax. The\nbenefit of INVALIDATOR is threefold. First, INVALIDATOR leverages both semantic\nand syntactic reasoning to enhance its discriminative capability. Second,\nINVALIDATOR does not require new test cases to be generated, but instead only\nrelies on the current test suite and uses invariant inference to generalize\nprogram behaviors. Third, INVALIDATOR is fully automated. Experimental results\ndemonstrate that INVALIDATOR outperforms existing methods in terms of Accuracy\nand F-measure, correctly identifying 79% of overfitting patches and detecting\n23% more overfitting patches than the best baseline.",
        "pdf_link": "https://arxiv.org/pdf/2301.01113v2.pdf"
    },
    {
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Jarred Barber",
            "AJ Maschinot",
            "Jose Lezama",
            "Lu Jiang",
            "Ming-Hsuan Yang",
            "Kevin Murphy",
            "William T. Freeman",
            "Michael Rubinstein",
            "Yuanzhen Li",
            "Dilip Krishnan"
        ],
        "published": "2023-01-02T14:43:38Z",
        "summary": "We present Muse, a text-to-image Transformer model that achieves\nstate-of-the-art image generation performance while being significantly more\nefficient than diffusion or autoregressive models. Muse is trained on a masked\nmodeling task in discrete token space: given the text embedding extracted from\na pre-trained large language model (LLM), Muse is trained to predict randomly\nmasked image tokens. Compared to pixel-space diffusion models, such as Imagen\nand DALL-E 2, Muse is significantly more efficient due to the use of discrete\ntokens and requiring fewer sampling iterations; compared to autoregressive\nmodels, such as Parti, Muse is more efficient due to the use of parallel\ndecoding. The use of a pre-trained LLM enables fine-grained language\nunderstanding, translating to high-fidelity image generation and the\nunderstanding of visual concepts such as objects, their spatial relationships,\npose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M,\nwith an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88\non zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also\ndirectly enables a number of image editing applications without the need to\nfine-tune or invert the model: inpainting, outpainting, and mask-free editing.\nMore results are available at https://muse-model.github.io",
        "pdf_link": "https://arxiv.org/pdf/2301.00704v1.pdf"
    },
    {
        "title": "CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation",
        "authors": [
            "Ge Zhang",
            "Yizhi Li",
            "Yaoyao Wu",
            "Linyuan Zhang",
            "Chenghua Lin",
            "Jiayi Geng",
            "Shi Wang",
            "Jie Fu"
        ],
        "published": "2023-01-01T12:48:12Z",
        "summary": "As natural language processing (NLP) for gender bias becomes a significant\ninterdisciplinary topic, the prevalent data-driven techniques such as\nlarge-scale language models suffer from data inadequacy and biased corpus,\nespecially for languages with insufficient resources such as Chinese. To this\nend, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation\nCORGI-PM, which contains 32.9k sentences with high-quality labels derived by\nfollowing an annotation scheme specifically developed for gender bias in the\nChinese context. Moreover, we address three challenges for automatic textual\ngender bias mitigation, which requires the models to detect, classify, and\nmitigate textual gender bias. We also conduct experiments with state-of-the-art\nlanguage models to provide baselines. To our best knowledge, CORGI-PM is the\nfirst sentence-level Chinese corpus for gender bias probing and mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2301.00395v1.pdf"
    }
]