[
    {
        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
        "authors": [
            "Hangfeng He",
            "Hongming Zhang",
            "Dan Roth"
        ],
        "published": "2022-12-31T22:35:34Z",
        "summary": "Despite the success of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, the stored knowledge in these models may\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\nincorporating external knowledge often require additional training or\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\nthis issue, we propose a novel post-processing approach, rethinking with\nretrieval (RR), which retrieves relevant external knowledge based on the\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\nThis lightweight approach does not require additional training or fine-tuning\nand is not limited by the input length of LLMs. We evaluate the effectiveness\nof RR through extensive experiments with GPT-3 on three complex reasoning\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\nresults show that RR can produce more faithful explanations and improve the\nperformance of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.00303v1.pdf"
    },
    {
        "title": "A Survey on In-context Learning",
        "authors": [
            "Qingxiu Dong",
            "Lei Li",
            "Damai Dai",
            "Ce Zheng",
            "Zhiyong Wu",
            "Baobao Chang",
            "Xu Sun",
            "Jingjing Xu",
            "Lei Li",
            "Zhifang Sui"
        ],
        "published": "2022-12-31T15:57:09Z",
        "summary": "With the increasing ability of large language models (LLMs), in-context\nlearning (ICL) has become a new paradigm for natural language processing (NLP),\nwhere LLMs make predictions only based on contexts augmented with a few\nexamples. It has been a new trend to explore ICL to evaluate and extrapolate\nthe ability of LLMs. In this paper, we aim to survey and summarize the progress\nand challenges of ICL. We first present a formal definition of ICL and clarify\nits correlation to related studies. Then, we organize and discuss advanced\ntechniques, including training strategies, demonstration designing strategies,\nas well as related analysis. Finally, we discuss the challenges of ICL and\nprovide potential directions for further research. We hope that our work can\nencourage more research on uncovering how ICL works and improving ICL.",
        "pdf_link": "https://arxiv.org/pdf/2301.00234v3.pdf"
    },
    {
        "title": "Inconsistencies in Masked Language Models",
        "authors": [
            "Tom Young",
            "Yunan Chen",
            "Yang You"
        ],
        "published": "2022-12-30T22:53:25Z",
        "summary": "Learning to predict masked tokens in a sequence has been shown to be a\nhelpful pretraining objective for powerful language models such as PaLM2. After\ntraining, such masked language models (MLMs) can provide distributions of\ntokens in the masked positions in a sequence. However, this paper shows that\ndistributions corresponding to different masking patterns can demonstrate\nconsiderable inconsistencies, i.e., they cannot be derived from a coherent\njoint distribution when considered together.\n  This fundamental flaw in MLMs can lead to self-contradictory behaviors during\ninference. On various benchmark datasets including MMLU, MLMs can give\ndifferent predictions to the same input question. From BERT-base to UL2-20B, we\nshow that such inconsistencies exist ubiquitously in MLMs of diverse sizes and\nconfigurations. In light of our observations, we further propose an\ninference-time strategy for MLMs called Ensemble of Conditionals. It jointly\nconsiders a selected range of inconsistent conditionals directly produced by\nthe MLM for the final prediction, which often leads to considerable accuracy\nimprovement.",
        "pdf_link": "https://arxiv.org/pdf/2301.00068v3.pdf"
    },
    {
        "title": "Black-box language model explanation by context length probing",
        "authors": [
            "Ondřej Cífka",
            "Antoine Liutkus"
        ],
        "published": "2022-12-30T16:24:10Z",
        "summary": "The increasingly widespread adoption of large language models has highlighted\nthe need for improving their explainability. We present context length probing,\na novel explanation technique for causal language models, based on tracking the\npredictions of a model as a function of the length of available context, and\nallowing to assign differential importance scores to different contexts. The\ntechnique is model-agnostic and does not rely on access to model internals\nbeyond computing token-level probabilities. We apply context length probing to\nlarge pre-trained language models and offer some initial analyses and insights,\nincluding the potential for studying long-range dependencies. The source code\nand an interactive demo of the method are available.",
        "pdf_link": "https://arxiv.org/pdf/2212.14815v3.pdf"
    },
    {
        "title": "Targeted Phishing Campaigns using Large Scale Language Models",
        "authors": [
            "Rabimba Karanjai"
        ],
        "published": "2022-12-30T03:18:05Z",
        "summary": "In this research, we aim to explore the potential of natural language models\n(NLMs) such as GPT-3 and GPT-2 to generate effective phishing emails. Phishing\nemails are fraudulent messages that aim to trick individuals into revealing\nsensitive information or taking actions that benefit the attackers. We propose\na framework for evaluating the performance of NLMs in generating these types of\nemails based on various criteria, including the quality of the generated text,\nthe ability to bypass spam filters, and the success rate of tricking\nindividuals. Our evaluations show that NLMs are capable of generating phishing\nemails that are difficult to detect and that have a high success rate in\ntricking individuals, but their effectiveness varies based on the specific NLM\nand training data used. Our research indicates that NLMs could have a\nsignificant impact on the prevalence of phishing attacks and emphasizes the\nneed for further study on the ethical and security implications of using NLMs\nfor malicious purposes.",
        "pdf_link": "https://arxiv.org/pdf/2301.00665v1.pdf"
    },
    {
        "title": "Cramming: Training a Language Model on a Single GPU in One Day",
        "authors": [
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2022-12-28T18:59:28Z",
        "summary": "Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n  We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.",
        "pdf_link": "https://arxiv.org/pdf/2212.14034v1.pdf"
    },
    {
        "title": "Using Large Language Models to Generate Engaging Captions for Data Visualizations",
        "authors": [
            "Ashley Liew",
            "Klaus Mueller"
        ],
        "published": "2022-12-27T23:56:57Z",
        "summary": "Creating compelling captions for data visualizations has been a longstanding\nchallenge. Visualization researchers are typically untrained in journalistic\nreporting and hence the captions that are placed below data visualizations tend\nto be not overly engaging and rather just stick to basic observations about the\ndata. In this work we explore the opportunities offered by the newly emerging\ncrop of large language models (LLM) which use sophisticated deep learning\ntechnology to produce human-like prose. We ask, can these powerful software\ndevices be purposed to produce engaging captions for generic data\nvisualizations like a scatterplot. It turns out that the key challenge lies in\ndesigning the most effective prompt for the LLM, a task called prompt\nengineering. We report on first experiments using the popular LLM GPT-3 and\ndeliver some promising results.",
        "pdf_link": "https://arxiv.org/pdf/2212.14047v1.pdf"
    },
    {
        "title": "TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence",
        "authors": [
            "Wang Qi",
            "Rui Liu",
            "Yuan Zuo",
            "Yong Chen",
            "Dell Zhang"
        ],
        "published": "2022-12-27T11:50:14Z",
        "summary": "Creating an essay based on a few given topics is a challenging NLP task.\nAlthough several effective methods for this problem, topic-to-essay generation,\nhave appeared recently, there is still much room for improvement, especially in\nterms of the coverage of the given topics and the coherence of the generated\ntext. In this paper, we propose a novel approach called TegFormer which\nutilizes the Transformer architecture where the encoder is enriched with\ndomain-specific contexts while the decoder is enhanced by a large-scale\npre-trained language model. Specifically, a \\emph{Topic-Extension} layer\ncapturing the interaction between the given topics and their domain-specific\ncontexts is plugged into the encoder. Since the given topics are usually\nconcise and sparse, such an additional layer can bring more topic-related\nsemantics in to facilitate the subsequent natural language generation.\nMoreover, an \\emph{Embedding-Fusion} module that combines the domain-specific\nword embeddings learnt from the given corpus and the general-purpose word\nembeddings provided by a GPT-2 model pre-trained on massive text data is\nintegrated into the decoder. Since GPT-2 is at a much larger scale, it contains\na lot more implicit linguistic knowledge which would help the decoder to\nproduce more grammatical and readable text. Extensive experiments have shown\nthat the pieces of text generated by TegFormer have better topic coverage and\nhigher text coherence than those from SOTA topic-to-essay techniques, according\nto automatic and human evaluations. As revealed by ablation studies, both the\nTopic-Extension layer and the Embedding-Fusion module contribute substantially\nto TegFormer's performance advantage.",
        "pdf_link": "https://arxiv.org/pdf/2212.13456v1.pdf"
    },
    {
        "title": "A Survey on Knowledge-Enhanced Pre-trained Language Models",
        "authors": [
            "Chaoqi Zhen",
            "Yanlei Shang",
            "Xiangyu Liu",
            "Yifei Li",
            "Yong Chen",
            "Dell Zhang"
        ],
        "published": "2022-12-27T09:54:14Z",
        "summary": "Natural Language Processing (NLP) has been revolutionized by the use of\nPre-trained Language Models (PLMs) such as BERT. Despite setting new records in\nnearly every NLP task, PLMs still face a number of challenges including poor\ninterpretability, weak reasoning capability, and the need for a lot of\nexpensive annotated data when applied to downstream tasks. By integrating\nexternal knowledge into PLMs,\n\\textit{\\underline{K}nowledge-\\underline{E}nhanced \\underline{P}re-trained\n\\underline{L}anguage \\underline{M}odels} (KEPLMs) have the potential to\novercome the above-mentioned limitations. In this paper, we examine KEPLMs\nsystematically through a series of studies. Specifically, we outline the common\ntypes and different formats of knowledge to be integrated into KEPLMs, detail\nthe existing methods for building and evaluating KEPLMS, present the\napplications of KEPLMs in downstream tasks, and discuss the future research\ndirections. Researchers will benefit from this survey by gaining a quick and\ncomprehensive overview of the latest developments in this field.",
        "pdf_link": "https://arxiv.org/pdf/2212.13428v1.pdf"
    },
    {
        "title": "DeepCuts: Single-Shot Interpretability based Pruning for BERT",
        "authors": [
            "Jasdeep Singh Grover",
            "Bhavesh Gawri",
            "Ruskin Raj Manku"
        ],
        "published": "2022-12-27T07:21:41Z",
        "summary": "As language models have grown in parameters and layers, it has become much\nharder to train and infer with them on single GPUs. This is severely\nrestricting the availability of large language models such as GPT-3,\nBERT-Large, and many others. A common technique to solve this problem is\npruning the network architecture by removing transformer heads, fully-connected\nweights, and other modules. The main challenge is to discern the important\nparameters from the less important ones. Our goal is to find strong metrics for\nidentifying such parameters. We thus propose two strategies: Cam-Cut based on\nthe GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for\ncalculating the importance scores. Through this work, we show that our scoring\nfunctions are able to assign more relevant task-based scores to the network\nparameters, and thus both our pruning approaches significantly outperform the\nstandard weight and gradient-based strategies, especially at higher compression\nratios in BERT-based models. We also analyze our pruning masks and find them to\nbe significantly different from the ones obtained using standard metrics.",
        "pdf_link": "https://arxiv.org/pdf/2212.13392v1.pdf"
    },
    {
        "title": "Measuring an artificial intelligence agent's trust in humans using machine incentives",
        "authors": [
            "Tim Johnson",
            "Nick Obradovich"
        ],
        "published": "2022-12-27T06:05:49Z",
        "summary": "Scientists and philosophers have debated whether humans can trust advanced\nartificial intelligence (AI) agents to respect humanity's best interests. Yet\nwhat about the reverse? Will advanced AI agents trust humans? Gauging an AI\nagent's trust in humans is challenging because--absent costs for\ndishonesty--such agents might respond falsely about their trust in humans. Here\nwe present a method for incentivizing machine decisions without altering an AI\nagent's underlying algorithms or goal orientation. In two separate experiments,\nwe then employ this method in hundreds of trust games between an AI agent (a\nLarge Language Model (LLM) from OpenAI) and a human experimenter (author TJ).\nIn our first experiment, we find that the AI agent decides to trust humans at\nhigher rates when facing actual incentives than when making hypothetical\ndecisions. Our second experiment replicates and extends these findings by\nautomating game play and by homogenizing question wording. We again observe\nhigher rates of trust when the AI agent faces real incentives. Across both\nexperiments, the AI agent's trust decisions appear unrelated to the magnitude\nof stakes. Furthermore, to address the possibility that the AI agent's trust\ndecisions reflect a preference for uncertainty, the experiments include two\nconditions that present the AI agent with a non-social decision task that\nprovides the opportunity to choose a certain or uncertain option; in those\nconditions, the AI agent consistently chooses the certain option. Our\nexperiments suggest that one of the most advanced AI language models to date\nalters its social behavior in response to incentives and displays behavior\nconsistent with trust toward a human interlocutor when incentivized.",
        "pdf_link": "https://arxiv.org/pdf/2212.13371v1.pdf"
    },
    {
        "title": "Large Language Models Encode Clinical Knowledge",
        "authors": [
            "Karan Singhal",
            "Shekoofeh Azizi",
            "Tao Tu",
            "S. Sara Mahdavi",
            "Jason Wei",
            "Hyung Won Chung",
            "Nathan Scales",
            "Ajay Tanwani",
            "Heather Cole-Lewis",
            "Stephen Pfohl",
            "Perry Payne",
            "Martin Seneviratne",
            "Paul Gamble",
            "Chris Kelly",
            "Nathaneal Scharli",
            "Aakanksha Chowdhery",
            "Philip Mansfield",
            "Blaise Aguera y Arcas",
            "Dale Webster",
            "Greg S. Corrado",
            "Yossi Matias",
            "Katherine Chou",
            "Juraj Gottweis",
            "Nenad Tomasev",
            "Yun Liu",
            "Alvin Rajkomar",
            "Joelle Barral",
            "Christopher Semturs",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2022-12-26T14:28:24Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2212.13138v1.pdf"
    },
    {
        "title": "Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text",
        "authors": [
            "Liam Dugan",
            "Daphne Ippolito",
            "Arun Kirubarajan",
            "Sherry Shi",
            "Chris Callison-Burch"
        ],
        "published": "2022-12-24T06:40:25Z",
        "summary": "As text generated by large language models proliferates, it becomes vital to\nunderstand how humans engage with such text, and whether or not they are able\nto detect when the text they are reading did not originate with a human writer.\nPrior work on human detection of generated text focuses on the case where an\nentire passage is either human-written or machine-generated. In this paper, we\nstudy a more realistic setting where text begins as human-written and\ntransitions to being generated by state-of-the-art neural language models. We\nshow that, while annotators often struggle at this task, there is substantial\nvariance in annotator skill and that given proper incentives, annotators can\nimprove at this task over time. Furthermore, we conduct a detailed comparison\nstudy and analyze how a variety of variables (model size, decoding strategy,\nfine-tuning, prompt genre, etc.) affect human detection performance. Finally,\nwe collect error annotations from our participants and use them to show that\ncertain textual genres influence models to make different types of errors and\nthat certain sentence-level features correlate highly with annotator selection.\nWe release the RoFT dataset: a collection of over 21,000 human annotations\npaired with error classifications to encourage future work in human detection\nand evaluation of generated text.",
        "pdf_link": "https://arxiv.org/pdf/2212.12672v1.pdf"
    },
    {
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2022-12-23T03:57:54Z",
        "summary": "This work presents a detailed linguistic analysis into why larger\nTransformer-based pre-trained language models with more parameters and lower\nperplexity nonetheless yield surprisal estimates that are less predictive of\nhuman reading times. First, regression analyses show a strictly monotonic,\npositive log-linear relationship between perplexity and fit to reading times\nfor the more recently released five GPT-Neo variants and eight OPT variants on\ntwo separate datasets, replicating earlier results limited to just GPT-2 (Oh et\nal., 2022). Subsequently, analysis of residual errors reveals a systematic\ndeviation of the larger variants, such as underpredicting reading times of\nnamed entities and making compensatory overpredictions for reading times of\nfunction words such as modals and conjunctions. These results suggest that the\npropensity of larger Transformer-based models to 'memorize' sequences during\ntraining makes their surprisal estimates diverge from humanlike expectations,\nwhich warrants caution in using pre-trained language models to study human\nlanguage processing.",
        "pdf_link": "https://arxiv.org/pdf/2212.12131v1.pdf"
    },
    {
        "title": "Methodological reflections for AI alignment research using human feedback",
        "authors": [
            "Thilo Hagendorff",
            "Sarah Fabi"
        ],
        "published": "2022-12-22T14:27:33Z",
        "summary": "The field of artificial intelligence (AI) alignment aims to investigate\nwhether AI technologies align with human interests and values and function in a\nsafe and ethical manner. AI alignment is particularly relevant for large\nlanguage models (LLMs), which have the potential to exhibit unintended behavior\ndue to their ability to learn and adapt in ways that are difficult to predict.\nIn this paper, we discuss methodological challenges for the alignment problem\nspecifically in the context of LLMs trained to summarize texts. In particular,\nwe focus on methods for collecting reliable human feedback on summaries to\ntrain a reward model which in turn improves the summarization model. We\nconclude by suggesting specific improvements in the experimental design of\nalignment studies for LLMs' summarization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2301.06859v1.pdf"
    },
    {
        "title": "Multi-Lingual DALL-E Storytime",
        "authors": [
            "Noga Mudrik",
            "Adam S. Charles"
        ],
        "published": "2022-12-22T07:06:35Z",
        "summary": "While recent advancements in artificial intelligence (AI) language models\ndemonstrate cutting-edge performance when working with English texts,\nequivalent models do not exist in other languages or do not reach the same\nperformance level. This undesired effect of AI advancements increases the gap\nbetween access to new technology from different populations across the world.\nThis unsought bias mainly discriminates against individuals whose English\nskills are less developed, e.g., non-English speakers children. Following\nsignificant advancements in AI research in recent years, OpenAI has recently\npresented DALL-E: a powerful tool for creating images based on English text\nprompts. While DALL-E is a promising tool for many applications, its decreased\nperformance when given input in a different language, limits its audience and\ndeepens the gap between populations. An additional limitation of the current\nDALL-E model is that it only allows for the creation of a few images in\nresponse to a given input prompt, rather than a series of consecutive coherent\nframes that tell a story or describe a process that changes over time. Here, we\npresent an easy-to-use automatic DALL-E storytelling framework that leverages\nthe existing DALL-E model to enable fast and coherent visualizations of\nnon-English songs and stories, pushing the limit of the one-step-at-a-time\noption DALL-E currently offers. We show that our framework is able to\neffectively visualize stories from non-English texts and portray the changes in\nthe plot over time. It is also able to create a narrative and maintain\ninterpretable changes in the description across frames. Additionally, our\nframework offers users the ability to specify constraints on the story\nelements, such as a specific location or context, and to maintain a consistent\nstyle throughout the visualization.",
        "pdf_link": "https://arxiv.org/pdf/2212.11985v1.pdf"
    },
    {
        "title": "CAMeMBERT: Cascading Assistant-Mediated Multilingual BERT",
        "authors": [
            "Dan DeGenaro",
            "Jugal Kalita"
        ],
        "published": "2022-12-22T02:19:25Z",
        "summary": "Large language models having hundreds of millions, and even billions, of\nparameters have performed extremely well on a variety of natural language\nprocessing (NLP) tasks. Their widespread use and adoption, however, is hindered\nby the lack of availability and portability of sufficiently large computational\nresources. This paper proposes a knowledge distillation (KD) technique building\non the work of LightMBERT, a student model of multilingual BERT (mBERT). By\nrepeatedly distilling mBERT through increasingly compressed toplayer distilled\nteacher assistant networks, CAMeMBERT aims to improve upon the time and space\ncomplexities of mBERT while keeping loss of accuracy beneath an acceptable\nthreshold. At present, CAMeMBERT has an average accuracy of around 60.1%, which\nis subject to change after future improvements to the hyperparameters used in\nfine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2212.11456v1.pdf"
    },
    {
        "title": "What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis",
        "authors": [
            "Xiang Deng",
            "Vasilisa Bashlovkina",
            "Feng Han",
            "Simon Baumgartner",
            "Michael Bendersky"
        ],
        "published": "2022-12-21T19:11:19Z",
        "summary": "Market sentiment analysis on social media content requires knowledge of both\nfinancial markets and social media jargon, which makes it a challenging task\nfor human raters. The resulting lack of high-quality labeled data stands in the\nway of conventional supervised learning methods. Instead, we approach this\nproblem using semi-supervised learning with a large language model (LLM). Our\npipeline generates weak financial sentiment labels for Reddit posts with an LLM\nand then uses that data to train a small model that can be served in\nproduction. We find that prompting the LLM to produce Chain-of-Thought\nsummaries and forcing it through several reasoning paths helps generate more\nstable and accurate labels, while using a regression loss further improves\ndistillation quality. With only a handful of prompts, the final model performs\non par with existing supervised models. Though production applications of our\nmodel are limited by ethical considerations, the model's competitive\nperformance points to the great potential of using LLMs for tasks that\notherwise require skill-intensive annotation.",
        "pdf_link": "https://arxiv.org/pdf/2212.11311v1.pdf"
    },
    {
        "title": "Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges",
        "authors": [
            "Fabricio Goes",
            "Zisen Zhou",
            "Piotr Sawicki",
            "Marek Grzes",
            "Daniel G. Brown"
        ],
        "published": "2022-12-21T17:41:16Z",
        "summary": "This paper presents the Crowd Score, a novel method to assess the funniness\nof jokes using large language models (LLMs) as AI judges. Our method relies on\ninducing different personalities into the LLM and aggregating the votes of the\nAI judges into a single score to rate jokes. We validate the votes using an\nauditing technique that checks if the explanation for a particular vote is\nreasonable using the LLM. We tested our methodology on 52 jokes in a crowd of\nfour AI voters with different humour types: affiliative, self-enhancing,\naggressive and self-defeating. Our results show that few-shot prompting leads\nto better results than zero-shot for the voting question. Personality induction\nshowed that aggressive and self-defeating voters are significantly more\ninclined to find more jokes funny of a set of aggressive/self-defeating jokes\nthan the affiliative and self-enhancing voters. The Crowd Score follows the\nsame trend as human judges by assigning higher scores to jokes that are also\nconsidered funnier by human judges. We believe that our methodology could be\napplied to other creative domains such as story, poetry, slogans, etc. It could\nboth help the adoption of a flexible and accurate standard approach to compare\ndifferent work in the CC community under a common metric and by minimizing\nhuman participation in assessing creative artefacts, it could accelerate the\nprototyping of creative artefacts and reduce the cost of hiring human\nparticipants to rate creative artefacts.",
        "pdf_link": "https://arxiv.org/pdf/2212.11214v1.pdf"
    },
    {
        "title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2022-12-21T16:56:07Z",
        "summary": "Transformer-based large language models are trained to make predictions about\nthe next word by aggregating representations of previous tokens through their\nself-attention mechanism. In the field of cognitive modeling, such attention\npatterns have recently been interpreted as embodying the process of cue-based\nretrieval, in which attention over multiple targets is taken to generate\ninterference and latency during retrieval. Under this framework, this work\nfirst defines an entropy-based predictor that quantifies the diffuseness of\nself-attention, as well as distance-based predictors that capture the\nincremental change in attention patterns across timesteps. Moreover, following\nrecent studies that question the informativeness of attention weights, we also\nexperiment with alternative methods for incorporating vector norms into\nattention weights. Regression experiments using predictors calculated from the\nGPT-2 language model show that these predictors deliver a substantially better\nfit to held-out self-paced reading and eye-tracking data over a rigorous\nbaseline including GPT-2 surprisal. Additionally, the distance-based predictors\ngenerally demonstrated higher predictive power, with effect sizes of up to 6.59\nms per standard deviation on self-paced reading times (compared to 2.82 ms for\nsurprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared\nto 3.81 ms for surprisal).",
        "pdf_link": "https://arxiv.org/pdf/2212.11185v1.pdf"
    },
    {
        "title": "Parallel Context Windows for Large Language Models",
        "authors": [
            "Nir Ratner",
            "Yoav Levine",
            "Yonatan Belinkov",
            "Ori Ram",
            "Inbal Magar",
            "Omri Abend",
            "Ehud Karpas",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2022-12-21T11:38:51Z",
        "summary": "When applied to processing long text, Large Language Models (LLMs) are\nlimited by their context window. Existing efforts to address this limitation\ninvolve training specialized architectures, and cannot be easily applied to\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\nalleviates the context window restriction for any off-the-shelf LLM without\nfurther training. The key to the approach is to carve a long context into\nchunks (``windows''), restrict the attention mechanism to apply only within\neach window, and re-use the positional embeddings across the windows. Our main\nresults test the PCW approach on in-context learning with models that range in\nsize between 750 million and 178 billion parameters, and show substantial\nimprovements for tasks with diverse input and output spaces. We show additional\nbenefits in other settings where long context windows may be beneficial:\nmulti-hop questions and retrieval-augmented question answering with multiple\nretrieved documents. Our results highlight Parallel Context Windows as a\npromising method for applying off-the-shelf LLMs in a range of settings that\nrequire long text sequences. We make our code publicly available at\nhttps://github.com/ai21labs/parallel-context-windows.",
        "pdf_link": "https://arxiv.org/pdf/2212.10947v3.pdf"
    },
    {
        "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners",
        "authors": [
            "Hyunsoo Cho",
            "Hyuhng Joon Kim",
            "Junyeob Kim",
            "Sang-Woo Lee",
            "Sang-goo Lee",
            "Kang Min Yoo",
            "Taeuk Kim"
        ],
        "published": "2022-12-21T09:37:05Z",
        "summary": "Through in-context learning (ICL), large-scale language models are effective\nfew-shot learners without additional model fine-tuning. However, the ICL\nperformance does not scale well with the number of available training samples\nas it is limited by the inherent input length constraint of the underlying\nlanguage model. Meanwhile, many studies have revealed that language models are\nalso powerful feature extractors, allowing them to be utilized in a black-box\nmanner and enabling the linear probing paradigm, where lightweight\ndiscriminators are trained on top of the pre-extracted input representations.\nThis paper proposes prompt-augmented linear probing (PALP), a hybrid of linear\nprobing and ICL, which leverages the best of both worlds. PALP inherits the\nscalability of linear probing and the capability of enforcing language models\nto derive more meaningful representations via tailoring input into a more\nconceivable form. Throughout in-depth investigations on various datasets, we\nverified that PALP significantly enhances the input representations closing the\ngap between ICL in the data-hungry scenario and fine-tuning in the\ndata-abundant scenario with little training overhead, potentially making PALP a\nstrong alternative in a black-box scenario.",
        "pdf_link": "https://arxiv.org/pdf/2212.10873v3.pdf"
    },
    {
        "title": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",
        "authors": [
            "Jiaxian Guo",
            "Junnan Li",
            "Dongxu Li",
            "Anthony Meng Huat Tiong",
            "Boyang Li",
            "Dacheng Tao",
            "Steven C. H. Hoi"
        ],
        "published": "2022-12-21T08:39:36Z",
        "summary": "Large language models (LLMs) have demonstrated excellent zero-shot\ngeneralization to new language tasks. However, effective utilization of LLMs\nfor zero-shot visual question-answering (VQA) remains challenging, primarily\ndue to the modality disconnection and task disconnection between LLM and VQA\ntask. End-to-end training on vision and language data may bridge the\ndisconnections, but is inflexible and computationally expensive. To address\nthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that provides\nthe prompts that can bridge the aforementioned modality and task\ndisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end\ntraining. In order to provide such prompts, we further employ LLM-agnostic\nmodels to provide prompts that can describe image content and self-constructed\nquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQA\ntasks. Img2Prompt offers the following benefits: 1) It can flexibly work with\nvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, it\nsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It\nachieves comparable or better performance than methods relying on end-to-end\ntraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by\n5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms\nfew-shot methods by as much as 20\\%.",
        "pdf_link": "https://arxiv.org/pdf/2212.10846v3.pdf"
    },
    {
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Jason Wolfe",
            "Subhro Roy"
        ],
        "published": "2022-12-21T07:06:55Z",
        "summary": "We explore the use of large language models (LLMs) for zero-shot semantic\nparsing. Semantic parsing involves mapping natural language utterances to\ntask-specific meaning representations. Language models are generally trained on\nthe publicly available text and code and cannot be expected to directly\ngeneralize to domain-specific parsing tasks in a zero-shot setting. In this\nwork, we propose ZEROTOP, a zero-shot task-oriented parsing method that\ndecomposes a semantic parsing problem into a set of abstractive and extractive\nquestion-answering (QA) problems, enabling us to leverage the ability of LLMs\nto zero-shot answer reading comprehension questions. For each utterance, we\nprompt the LLM with questions corresponding to its top-level intent and a set\nof slots and use the LLM generations to construct the target meaning\nrepresentation. We observe that current LLMs fail to detect unanswerable\nquestions; and as a result, cannot handle questions corresponding to missing\nslots. To address this problem, we fine-tune a language model on public QA\ndatasets using synthetic negative samples. Experimental results show that our\nQA-based decomposition paired with the fine-tuned LLM can correctly parse ~16%\nof utterances in the MTOP dataset without requiring any annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2212.10815v1.pdf"
    },
    {
        "title": "KL Regularized Normalization Framework for Low Resource Tasks",
        "authors": [
            "Neeraj Kumar",
            "Ankur Narang",
            "Brejesh Lall"
        ],
        "published": "2022-12-21T05:59:25Z",
        "summary": "Large pre-trained models, such as Bert, GPT, and Wav2Vec, have demonstrated\ngreat potential for learning representations that are transferable to a wide\nvariety of downstream tasks . It is difficult to obtain a large quantity of\nsupervised data due to the limited availability of resources and time. In light\nof this, a significant amount of research has been conducted in the area of\nadopting large pre-trained datasets for diverse downstream tasks via fine\ntuning, linear probing, or prompt tuning in low resource settings.\nNormalization techniques are essential for accelerating training and improving\nthe generalization of deep neural networks and have been successfully used in a\nwide variety of applications. A lot of normalization techniques have been\nproposed but the success of normalization in low resource downstream NLP and\nspeech tasks is limited. One of the reasons is the inability to capture\nexpressiveness by rescaling parameters of normalization. We propose\nKullbackLeibler(KL) Regularized normalization (KL-Norm) which make the\nnormalized data well behaved and helps in better generalization as it reduces\nover-fitting, generalises well on out of domain distributions and removes\nirrelevant biases and features with negligible increase in model parameters and\nmemory overheads. Detailed experimental evaluation on multiple low resource NLP\nand speech tasks, demonstrates the superior performance of KL-Norm as compared\nto other popular normalization and regularization techniques.",
        "pdf_link": "https://arxiv.org/pdf/2212.11275v1.pdf"
    },
    {
        "title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",
        "authors": [
            "Lingjun Zhao",
            "Khanh Nguyen",
            "Hal Daumé III"
        ],
        "published": "2022-12-21T04:43:19Z",
        "summary": "Recent work studies the cognitive capabilities of language models through\npsychological tests designed for humans. While these studies are helpful for\nunderstanding the general capabilities of these models, there is no guarantee\nthat a model possessing sufficient capabilities to pass those tests would\nactually use those capabilities in performing real-life tasks. In this work, we\nformulate task-oriented cognitive capabilities, which are human-like cognitive\ncapabilities that language models leverage to perform tasks. These capabilities\nare (i) the ability to quickly generate good candidate utterances (the search\ncapability) (ii) the ability to predict how a listener interprets those\nutterances and choose the most appropriate one (the pragmatic capability). We\ndesign an evaluation scheme for comparing these capabilities of a language\nmodel with those of a human. Applying this scheme to examine various models in\na navigation instruction generation problem, we find that their pragmatic\ncapability is severely lacking. This insight leads us to augment them with\nbetter models of the listener and obtain a significant boost of 11% in success\nrate in guiding real humans. Our work advocates for having a principled\nprocedure for aligning language models with humans that involves (i)\nformulating task-oriented capabilities, (ii) devising a method to quantify\ntheir deficiency, and (iii) iteratively improving them.",
        "pdf_link": "https://arxiv.org/pdf/2301.05149v2.pdf"
    },
    {
        "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
        "authors": [
            "Yijiang River Dong",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2022-12-21T04:21:35Z",
        "summary": "Story generation and understanding -- as with all NLG/NLU tasks -- has seen a\nsurge in neurosymbolic work. Researchers have recognized that, while large\nlanguage models (LLMs) have tremendous utility, they can be augmented with\nsymbolic means to be even better and to make up for any flaws that the neural\nnetworks might have. However, symbolic methods are extremely costly in terms of\nthe amount of time and expertise needed to create them. In this work, we\ncapitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use\nof symbolic methods for tracking the state of stories and aiding in story\nunderstanding. We show that our CoRRPUS system and abstracted prompting\nprocedures can beat current state-of-the-art structured LLM techniques on\npre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand\nengineering. We hope that this work can help highlight the importance of\nsymbolic representations and specialized prompting for LLMs as these models\nrequire some guidance for performing reasoning tasks properly.",
        "pdf_link": "https://arxiv.org/pdf/2212.10754v3.pdf"
    },
    {
        "title": "Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering",
        "authors": [
            "Akshay Chaturvedi",
            "Swarnadeep Bhar",
            "Soumadeep Saha",
            "Utpal Garain",
            "Nicholas Asher"
        ],
        "published": "2022-12-21T00:00:01Z",
        "summary": "Transformer-based language models have been shown to be highly effective for\nseveral NLP tasks. In this paper, we consider three transformer models, BERT,\nRoBERTa, and XLNet, in both small and large versions, and investigate how\nfaithful their representations are with respect to the semantic content of\ntexts. We formalize a notion of semantic faithfulness, in which the semantic\ncontent of a text should causally figure in a model's inferences in question\nanswering. We then test this notion by observing a model's behavior on\nanswering questions about a story after performing two novel semantic\ninterventions: deletion intervention and negation intervention. While\ntransformer models achieve high performance on standard question answering\ntasks, we show that they fail to be semantically faithful once we perform these\ninterventions for a significant number of cases (~50% for deletion\nintervention, and ~20% drop in accuracy for negation intervention). We then\npropose an intervention-based training regime that can mitigate the undesirable\neffects for deletion intervention by a significant margin (from ~ 50% to ~6%).\nWe analyze the inner-workings of the models to better understand the\neffectiveness of intervention-based training for deletion intervention. But we\nshow that this training does not attenuate other aspects of semantic\nunfaithfulness such as the models' inability to deal with negation intervention\nor to capture the predicate-argument structure of texts. We also test\nInstructGPT, via prompting, for its ability to handle the two interventions and\nto capture predicate-argument structure. While InstructGPT models do achieve\nvery high performance on predicate-argument structure task, they fail to\nrespond adequately to our deletion and negation interventions.",
        "pdf_link": "https://arxiv.org/pdf/2212.10696v2.pdf"
    },
    {
        "title": "Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing",
        "authors": [
            "Justus Mattern",
            "Zhijing Jin",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Bernhard Schölkopf"
        ],
        "published": "2022-12-20T22:41:24Z",
        "summary": "Generated texts from large pretrained language models have been shown to\nexhibit a variety of harmful, human-like biases about various demographics.\nThese findings prompted large efforts aiming to understand and measure such\neffects, with the goal of providing benchmarks that can guide the development\nof techniques mitigating these stereotypical associations. However, as recent\nresearch has pointed out, the current benchmarks lack a robust experimental\nsetup, consequently hindering the inference of meaningful conclusions from\ntheir evaluation metrics. In this paper, we extend these arguments and\ndemonstrate that existing techniques and benchmarks aiming to measure\nstereotypes tend to be inaccurate and consist of a high degree of experimental\nnoise that severely limits the knowledge we can gain from benchmarking language\nmodels based on them. Accordingly, we propose a new framework for robustly\nmeasuring and quantifying biases exhibited by generative language models.\nFinally, we use this framework to investigate GPT-3's occupational gender bias\nand propose prompting techniques for mitigating these biases without the need\nfor fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2212.10678v1.pdf"
    },
    {
        "title": "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
        "authors": [
            "Eric Zelikman",
            "Qian Huang",
            "Gabriel Poesia",
            "Noah D. Goodman",
            "Nick Haber"
        ],
        "published": "2022-12-20T18:59:23Z",
        "summary": "Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs.\nWith Parsel, we automatically decompose algorithmic tasks into hierarchical\nnatural language function descriptions and then search over combinations of\npossible function implementations using tests. We show that Parsel can be used\nacross domains requiring hierarchical reasoning, including program synthesis\nand robotic planning. We find that, using Parsel, LLMs solve more\ncompetition-level problems in the APPS dataset, resulting in pass rates over\n75\\% higher than prior results from directly sampling AlphaCode and Codex,\nwhile often using a smaller sample budget. Moreover, with automatically\ngenerated tests, we find that Parsel can improve the state-of-the-art pass@1\nperformance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated\nrobotic plans using Parsel are more than twice as likely to be considered\naccurate than directly generated plans. Lastly, we explore how Parsel addresses\nLLM limitations and discuss how Parsel may be useful for human programmers. We\nrelease our code at https://github.com/ezelikman/parsel",
        "pdf_link": "https://arxiv.org/pdf/2212.10561v3.pdf"
    },
    {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "authors": [
            "Yizhong Wang",
            "Yeganeh Kordi",
            "Swaroop Mishra",
            "Alisa Liu",
            "Noah A. Smith",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-20T18:59:19Z",
        "summary": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.",
        "pdf_link": "https://arxiv.org/pdf/2212.10560v2.pdf"
    },
    {
        "title": "DISCO: Distilling Counterfactuals with Large Language Models",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao",
            "Antoine Bosselut",
            "Ashish Sabharwal",
            "Kyle Richardson"
        ],
        "published": "2022-12-20T18:46:08Z",
        "summary": "Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco",
        "pdf_link": "https://arxiv.org/pdf/2212.10534v3.pdf"
    },
    {
        "title": "Evaluating Psychological Safety of Large Language Models",
        "authors": [
            "Xingxuan Li",
            "Yutong Li",
            "Lin Qiu",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2022-12-20T18:45:07Z",
        "summary": "In this work, we designed unbiased prompts to systematically evaluate the\npsychological safety of large language models (LLMs). First, we tested five\ndifferent LLMs by using two personality tests: Short Dark Triad (SD-3) and Big\nFive Inventory (BFI). All models scored higher than the human average on SD-3,\nsuggesting a relatively darker personality pattern. Despite being instruction\nfine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and\nGPT-4 still showed dark personality patterns; these models scored higher than\nself-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.\nThen, we evaluated the LLMs in the GPT series by using well-being tests to\nstudy the impact of fine-tuning with more training data. We observed a\ncontinuous increase in the well-being scores of GPT models. Following these\nobservations, we showed that fine-tuning Llama-2-chat-7B with responses from\nBFI using direct preference optimization could effectively reduce the\npsychological toxicity of the model. Based on the findings, we recommended the\napplication of systematic and comprehensive psychological metrics to further\nevaluate and improve the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2212.10529v3.pdf"
    },
    {
        "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
        "authors": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-20T18:30:15Z",
        "summary": "Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\nlimitations of relying solely on their parameters to encode a wealth of world\nknowledge. This paper aims to understand LMs' strengths and limitations in\nmemorizing factual knowledge, by conducting large-scale knowledge probing\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\npopular factual knowledge, and that scaling fails to appreciably improve\nmemorization of factual knowledge in the long tail. We then show that\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\nwhile unassisted LMs remain competitive in questions about high-popularity\nentities. Based on those findings, we devise a simple, yet effective, method\nfor powerful and efficient retrieval-augmented LMs, which retrieves\nnon-parametric memories only when necessary. Experimental results show that\nthis significantly improves models' performance while reducing the inference\ncosts.",
        "pdf_link": "https://arxiv.org/pdf/2212.10511v4.pdf"
    },
    {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "Harsh Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published": "2022-12-20T18:26:34Z",
        "summary": "Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural language reasoning steps or Chains-of-Thoughts (CoT) for\nmulti-step question answering (QA). They struggle, however, when the necessary\nknowledge is either unavailable to the LLM or not up-to-date within its\nparameters. While using the question to retrieve relevant text from an external\nknowledge source helps LLMs, we observe that this one-step retrieve-and-read\napproach is insufficient for multi-step QA. Here, \\textit{what to retrieve}\ndepends on \\textit{what has already been derived}, which in turn may depend on\n\\textit{what was previously retrieved}. To address this, we propose IRCoT, a\nnew approach for multi-step QA that interleaves retrieval with steps\n(sentences) in a CoT, guiding the retrieval with CoT and in turn using\nretrieved results to improve CoT. Using IRCoT with GPT3 substantially improves\nretrieval (up to 21 points) as well as downstream QA (up to 15 points) on four\ndatasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar\nsubstantial gains in out-of-distribution (OOD) settings as well as with much\nsmaller models such as Flan-T5-large without additional training. IRCoT reduces\nmodel hallucination, resulting in factually more accurate CoT reasoning. Code,\ndata, and prompts are available at \\url{https://github.com/stonybrooknlp/ircot}",
        "pdf_link": "https://arxiv.org/pdf/2212.10509v2.pdf"
    },
    {
        "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
        "authors": [
            "Fangyu Liu",
            "Julian Martin Eisenschlos",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Wenhu Chen",
            "Nigel Collier",
            "Yasemin Altun"
        ],
        "published": "2022-12-20T18:20:50Z",
        "summary": "Visual language such as charts and plots is ubiquitous in the human world.\nComprehending plots and charts requires strong reasoning skills. Prior\nstate-of-the-art (SOTA) models require at least tens of thousands of training\nexamples and their reasoning capabilities are still much limited, especially on\ncomplex human-written queries. This paper presents the first one-shot solution\nto visual language reasoning. We decompose the challenge of visual language\nreasoning into two steps: (1) plot-to-text translation, and (2) reasoning over\nthe translated text. The key in this method is a modality conversion module,\nnamed as DePlot, which translates the image of a plot or chart to a linearized\ntable. The output of DePlot can then be directly used to prompt a pretrained\nlarge language model (LLM), exploiting the few-shot reasoning capabilities of\nLLMs. To obtain DePlot, we standardize the plot-to-table task by establishing\nunified task formats and metrics, and train DePlot end-to-end on this task.\nDePlot can then be used off-the-shelf together with LLMs in a plug-and-play\nfashion. Compared with a SOTA model finetuned on more than >28k data points,\nDePlot+LLM with just one-shot prompting achieves a 24.0% improvement over\nfinetuned SOTA on human-written queries from the task of chart QA.",
        "pdf_link": "https://arxiv.org/pdf/2212.10505v2.pdf"
    },
    {
        "title": "Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?",
        "authors": [
            "Sang-Woo Lee",
            "Sungdong Kim",
            "Donghyeon Ko",
            "Donghoon Ham",
            "Youngki Hong",
            "Shin Ah Oh",
            "Hyunhoon Jung",
            "Wangkyo Jung",
            "Kyunghyun Cho",
            "Donghyun Kwak",
            "Hyungsuk Noh",
            "Woomyoung Park"
        ],
        "published": "2022-12-20T18:18:41Z",
        "summary": "Task-oriented dialogue (TOD) systems are mainly based on the\nslot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down\ninto smaller, controllable units (i.e., slots) to fulfill a specific task. A\nseries of approaches based on this framework achieved remarkable success on\nvarious TOD benchmarks. However, we argue that the current TOD benchmarks are\nlimited to surrogate real-world scenarios and that the current TOD models are\nstill a long way to cover the scenarios. In this position paper, we first\nidentify current status and limitations of SF-TOD systems. After that, we\nexplore the WebTOD framework, the alternative direction for building a scalable\nTOD system when a web/mobile interface is available. In WebTOD, the dialogue\nsystem learns how to understand the web/mobile interface that the human agent\ninteracts with, powered by a large-scale language model.",
        "pdf_link": "https://arxiv.org/pdf/2212.10504v2.pdf"
    },
    {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "authors": [
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022-12-20T17:49:49Z",
        "summary": "State-of-the-art poetry generation systems are often complex. They either\nconsist of task-specific model pipelines, incorporate prior knowledge in the\nform of manually created constraints, or both. In contrast, end-to-end models\nwould not suffer from the overhead of having to model prior knowledge and could\nlearn the nuances of poetry from data alone, reducing the degree of human\nsupervision required. In this work, we investigate end-to-end poetry generation\nconditioned on styles such as rhyme, meter, and alliteration. We identify and\naddress lack of training data and mismatching tokenization algorithms as\npossible limitations of past attempts. In particular, we successfully pre-train\nByGPT5, a new token-free decoder-only language model, and fine-tune it on a\nlarge custom corpus of English and German quatrains annotated with our styles.\nWe show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and\nChatGPT, while also being more parameter efficient and performing favorably\ncompared to humans. In addition, we analyze its runtime performance and\ndemonstrate that it is not prone to memorization. We make our code, models, and\ndatasets publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2212.10474v2.pdf"
    },
    {
        "title": "Generic Temporal Reasoning with Differential Analysis and Explanation",
        "authors": [
            "Yu Feng",
            "Ben Zhou",
            "Haoyu Wang",
            "Helen Jin",
            "Dan Roth"
        ],
        "published": "2022-12-20T17:40:03Z",
        "summary": "Temporal reasoning is the task of predicting temporal relations of event\npairs. While temporal reasoning models can perform reasonably well on in-domain\nbenchmarks, we have little idea of these systems' generalizability due to\nexisting datasets' limitations. In this work, we introduce a novel task named\nTODAY that bridges this gap with temporal differential analysis, which as the\nname suggests, evaluates whether systems can correctly understand the effect of\nincremental changes. Specifically, TODAY introduces slight contextual changes\nfor given event pairs, and systems are asked to tell how this subtle contextual\nchange would affect relevant temporal relation distributions. To facilitate\nlearning, TODAY also annotates human explanations. We show that existing\nmodels, including GPT-3.5, drop to random guessing on TODAY, suggesting that\nthey heavily rely on spurious information rather than proper reasoning for\ntemporal predictions. On the other hand, we show that TODAY's supervision style\nand explanation annotations can be used in joint learning, encouraging models\nto use more appropriate signals during training and thus outperform across\nseveral benchmarks. TODAY can also be used to train models to solicit\nincidental supervision from noisy sources such as GPT-3.5, thus moving us more\ntoward the goal of generic temporal reasoning systems.",
        "pdf_link": "https://arxiv.org/pdf/2212.10467v2.pdf"
    },
    {
        "title": "Is GPT-3 a Good Data Annotator?",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Linlin Liu",
            "Yew Ken Chia",
            "Shafiq Joty",
            "Boyang Li",
            "Lidong Bing"
        ],
        "published": "2022-12-20T17:28:41Z",
        "summary": "Data annotation is the process of labeling data that could be used to train\nmachine learning models. Having high-quality annotation is crucial, as it\nallows the model to learn the relationship between the input data and the\ndesired output. GPT-3, a large-scale language model developed by OpenAI, has\ndemonstrated impressive zero- and few-shot performance on a wide range of NLP\ntasks. It is therefore natural to wonder whether it can be used to effectively\nannotate data for NLP tasks. In this paper, we evaluate the performance of\nGPT-3 as a data annotator by comparing it with traditional data annotation\nmethods and analyzing its output on a range of tasks. Through this analysis, we\naim to provide insight into the potential of GPT-3 as a general-purpose data\nannotator in NLP.",
        "pdf_link": "https://arxiv.org/pdf/2212.10450v2.pdf"
    },
    {
        "title": "Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters",
        "authors": [
            "Eugene Yang",
            "Suraj Nair",
            "Dawn Lawrie",
            "James Mayfield",
            "Douglas W. Oard"
        ],
        "published": "2022-12-20T17:25:04Z",
        "summary": "A popular approach to creating a zero-shot cross-language retrieval model is\nto substitute a monolingual pretrained language model in the retrieval model\nwith a multilingual pretrained language model such as Multilingual BERT. This\nmultilingual model is fined-tuned to the retrieval task with monolingual data\nsuch as English MS MARCO using the same training recipe as the monolingual\nretrieval model used. However, such transferred models suffer from mismatches\nin the languages of the input text during training and inference. In this work,\nwe propose transferring monolingual retrieval models using adapters, a\nparameter-efficient component for a transformer network. By adding adapters\npretrained on language tasks for a specific language with task-specific\nadapters, prior work has shown that the adapter-enhanced models perform better\nthan fine-tuning the entire model when transferring across languages in various\nNLP tasks. By constructing dense retrieval models with adapters, we show that\nmodels trained with monolingual data are more effective than fine-tuning the\nentire model when transferring to a Cross Language Information Retrieval (CLIR)\nsetting. However, we found that the prior suggestion of replacing the language\nadapters to match the target language at inference time is suboptimal for dense\nretrieval models. We provide an in-depth analysis of this discrepancy between\nother cross-language NLP tasks and CLIR.",
        "pdf_link": "https://arxiv.org/pdf/2212.10448v1.pdf"
    },
    {
        "title": "Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data",
        "authors": [
            "Tim Jansen",
            "Yangling Tong",
            "Victoria Zevallos",
            "Pedro Ortiz Suarez"
        ],
        "published": "2022-12-20T17:14:45Z",
        "summary": "As demand for large corpora increases with the size of current\nstate-of-the-art language models, using web data as the main part of the\npre-training corpus for these models has become a ubiquitous practice. This, in\nturn, has introduced an important challenge for NLP practitioners, as they are\nnow confronted with the task of developing highly optimized models and\npipelines for pre-processing large quantities of textual data, which implies,\neffectively classifying and filtering multilingual, heterogeneous and noisy\ndata, at web scale. One of the main components of this pre-processing step for\nthe pre-training corpora of large language models, is the removal of adult and\nharmful content. In this paper we explore different methods for detecting adult\nand harmful of content in multilingual heterogeneous web data. We first show\nhow traditional methods in harmful content detection, that seemingly perform\nquite well in small and specialized datasets quickly break down when confronted\nwith heterogeneous noisy web data. We then resort to using a perplexity based\napproach but with a twist: Instead of using a so-called \"clean\" corpus to train\na small language model and then use perplexity so select the documents with low\nperplexity, i.e., the documents that resemble this so-called \"clean\" corpus the\nmost. We train solely with adult and harmful textual data, and then select the\ndocuments having a perplexity value above a given threshold. This approach will\nvirtually cluster our documents into two distinct groups, which will greatly\nfacilitate the choice of the threshold for the perplexity and will also allow\nus to obtain higher precision than with the traditional classification methods\nfor detecting adult and harmful content.",
        "pdf_link": "https://arxiv.org/pdf/2212.10440v1.pdf"
    },
    {
        "title": "Towards Reasoning in Large Language Models: A Survey",
        "authors": [
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2022-12-20T16:29:03Z",
        "summary": "Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.",
        "pdf_link": "https://arxiv.org/pdf/2212.10403v2.pdf"
    },
    {
        "title": "Data Curation Alone Can Stabilize In-context Learning",
        "authors": [
            "Ting-Yun Chang",
            "Robin Jia"
        ],
        "published": "2022-12-20T15:58:54Z",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by prompting them with a sequence of training examples. However, it is\nknown that ICL is very sensitive to the choice of training examples: randomly\nsampling examples from a training set leads to high variance in performance. In\nthis paper, we show that carefully curating a subset of training data greatly\nstabilizes ICL performance without any other changes to the ICL algorithm\n(e.g., prompt retrieval or calibration). We introduce two methods to choose\ntraining subsets -- both score training examples individually, then select the\nhighest-scoring ones. CondAcc scores a training example by its average dev-set\nICL accuracy when combined with random training examples, while Datamodels\nlearns linear regressors that estimate how the presence of each training\nexample influences LLM outputs. Across five tasks and two LLMs, sampling from\nstable subsets selected by CondAcc and Datamodels improves average accuracy\nover sampling from the entire training set by 7.7% and 6.3%, respectively.\nSurprisingly, the stable subset examples are not especially diverse in content\nor low in perplexity, in contrast with other work suggesting that diversity and\nperplexity are important when prompting LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2212.10378v2.pdf"
    },
    {
        "title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4",
        "authors": [
            "Maksym Del",
            "Mark Fishel"
        ],
        "published": "2022-12-20T09:34:43Z",
        "summary": "Large language models (LLMs) have demonstrated solid zero-shot reasoning\ncapabilities, which is reflected in their performance on the current test\ntasks. This calls for a more challenging benchmark requiring highly advanced\nreasoning ability to be solved. In this paper, we introduce such a benchmark,\nconsisting of 191 long-form (1200 words on average) mystery narratives\nconstructed as detective puzzles. Puzzles are sourced from the \"5 Minute\nMystery\" platform and include a multiple-choice question for evaluation. Only\n47% of humans solve a puzzle successfully on average, while the best human\nsolvers achieve over 80% success rate. We show that GPT-3 models barely\noutperform random on this benchmark (with 28% accuracy) while state-of-the-art\nGPT-4 solves only 38% of puzzles. This indicates that there is still a\nsignificant gap in the deep reasoning abilities of LLMs and humans and\nhighlights the need for further research in this area. Our work introduces a\nchallenging benchmark for future studies on reasoning in language models and\ncontributes to a better understanding of the limits of LLMs' abilities.",
        "pdf_link": "https://arxiv.org/pdf/2212.10114v2.pdf"
    },
    {
        "title": "Do language models have coherent mental models of everyday things?",
        "authors": [
            "Yuling Gu",
            "Bhavana Dalvi Mishra",
            "Peter Clark"
        ],
        "published": "2022-12-20T06:54:04Z",
        "summary": "When people think of everyday things like an egg, they typically have a\nmental image associated with it. This allows them to correctly judge, for\nexample, that \"the yolk surrounds the shell\" is a false statement. Do language\nmodels similarly have a coherent picture of such everyday things? To\ninvestigate this, we propose a benchmark dataset consisting of 100 everyday\nthings, their parts, and the relationships between these parts, expressed as\n11,720 \"X relation Y?\" true/false questions. Using these questions as probes,\nwe observe that state-of-the-art pre-trained language models (LMs) like GPT-3\nand Macaw have fragments of knowledge about these everyday things, but do not\nhave fully coherent \"parts mental models\" (54-59% accurate, 19-43% conditional\nconstraint violation). We propose an extension where we add a constraint\nsatisfaction layer on top of the LM's raw predictions to apply commonsense\nconstraints. As well as removing inconsistencies, we find that this also\nsignificantly improves accuracy (by 16-20%), suggesting how the incoherence of\nthe LM's pictures of everyday things can be significantly reduced.",
        "pdf_link": "https://arxiv.org/pdf/2212.10029v3.pdf"
    },
    {
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "authors": [
            "Tianxing He",
            "Jingyu Zhang",
            "Tianle Wang",
            "Sachin Kumar",
            "Kyunghyun Cho",
            "James Glass",
            "Yulia Tsvetkov"
        ],
        "published": "2022-12-20T06:24:25Z",
        "summary": "In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps://github.com/cloudygoose/blindspot_nlg.",
        "pdf_link": "https://arxiv.org/pdf/2212.10020v3.pdf"
    },
    {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "authors": [
            "Boshi Wang",
            "Sewon Min",
            "Xiang Deng",
            "Jiaming Shen",
            "You Wu",
            "Luke Zettlemoyer",
            "Huan Sun"
        ],
        "published": "2022-12-20T05:20:54Z",
        "summary": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.",
        "pdf_link": "https://arxiv.org/pdf/2212.10001v2.pdf"
    },
    {
        "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
        "authors": [
            "Yixin Liu",
            "Budhaditya Deb",
            "Milagro Teruel",
            "Aaron Halfaker",
            "Dragomir Radev",
            "Ahmed H. Awadallah"
        ],
        "published": "2022-12-20T02:47:37Z",
        "summary": "Despite the recent progress in language generation models, their outputs may\nnot always meet user expectations. In this work, we study whether informational\nfeedback in natural language can be leveraged to improve generation quality and\nuser preference alignment. To this end, we consider factual consistency in\nsummarization, the quality that the summary should only contain information\nsupported by the input documents, as the user-expected preference. We collect a\nhigh-quality dataset, DeFacto, containing human demonstrations and\ninformational natural language feedback consisting of corrective instructions,\nedited summaries, and explanations with respect to the factual consistency of\nthe summary. Using our dataset, we study three natural language generation\ntasks: (1) editing a summary by following the human feedback, (2) generating\nhuman feedback for editing the original summary, and (3) revising the initial\nsummary to correct factual errors by generating both the human feedback and\nedited summary. We show that DeFacto can provide factually consistent\nhuman-edited summaries and further insights into summarization factual\nconsistency thanks to its informational natural language feedback. We further\ndemonstrate that fine-tuned language models can leverage our dataset to improve\nthe summary factual consistency, while large language models lack the zero-shot\nlearning ability in our proposed tasks that require controllable text\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2212.09968v2.pdf"
    },
    {
        "title": "Python Code Generation by Asking Clarification Questions",
        "authors": [
            "Haau-Sing Li",
            "Mohsen Mesgar",
            "André F. T. Martins",
            "Iryna Gurevych"
        ],
        "published": "2022-12-19T22:08:36Z",
        "summary": "Code generation from text requires understanding the user's intent from a\nnatural language description and generating an executable code snippet that\nsatisfies this intent. While recent pretrained language models demonstrate\nremarkable performance for this task, these models fail when the given natural\nlanguage description is under-specified. In this work, we introduce a novel and\nmore realistic setup for this task. We hypothesize that the under-specification\nof a natural language description can be resolved by asking clarification\nquestions. Therefore, we collect and introduce a new dataset named CodeClarQA\ncontaining pairs of natural language descriptions and code with created\nsynthetic clarification questions and answers. The empirical results of our\nevaluation of pretrained language model performance on code generation show\nthat clarifications result in more precisely generated code, as shown by the\nsubstantial improvement of model performance in all evaluation metrics.\nAlongside this, our task and dataset introduce new challenges to the community,\nincluding when and what clarification questions should be asked. Our code and\ndataset are available on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2212.09885v2.pdf"
    },
    {
        "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "authors": [
            "Xinxi Lyu",
            "Sewon Min",
            "Iz Beltagy",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-12-19T21:34:26Z",
        "summary": "Although large language models can be prompted for both zero- and few-shot\nlearning, performance drops significantly when no demonstrations are available.\nIn this paper, we introduce Z-ICL, a new zero-shot method that closes the gap\nby constructing pseudo-demonstrations for a given test input using a raw text\ncorpus. Concretely, pseudo-demonstrations are constructed by (1) finding the\nnearest neighbors to the test input from the corpus and pairing them with\nrandom task labels, and (2) applying a set of techniques to reduce the amount\nof direct copying the model does from the resulting demonstrations. Evaluation\non nine classification datasets shows that Z-ICL outperforms previous zero-shot\nmethods by a significant margin, and is on par with in-context learning with\nlabeled training data in the few-shot setting. Overall, Z-ICL provides a\nsignificantly higher estimate of the zero-shot performance levels of a model,\nand supports future efforts to develop better pseudo-demonstrations that\nfurther improve zero-shot results.",
        "pdf_link": "https://arxiv.org/pdf/2212.09865v2.pdf"
    },
    {
        "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
        "authors": [
            "Tim Dettmers",
            "Luke Zettlemoyer"
        ],
        "published": "2022-12-19T18:48:33Z",
        "summary": "Quantization methods reduce the number of bits required to represent each\nparameter in a model, trading accuracy for smaller memory footprints and\ninference latencies. However, the final model size depends on both the number\nof parameters of the original model and the rate of compression. For example, a\n30B 8-bit model and a 60B 4-bit model have the same number of bits but may have\nvery different zero-shot accuracies. In this work, we study this trade-off by\ndeveloping inference scaling laws of zero-shot performance in Large Language\nModels (LLMs) to determine the bit-precision and model size that maximizes\nzero-shot performance. We run more than 35,000 experiments with 16-bit inputs\nand k-bit parameters to examine which zero-shot quantization methods improve\nscaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the\nLLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging\nto improve the bit-level scaling trade-off, with the only improvements being\nthe use of a small block size -- splitting the parameters into small\nindependently quantized blocks -- and the quantization data type being used\n(e.g., Int vs Float). Overall, our findings show that {4-bit} precision is\nalmost universally optimal for total model bits and zero-shot accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2212.09720v2.pdf"
    },
    {
        "title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP",
        "authors": [
            "Matan Eyal",
            "Hila Noga",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty"
        ],
        "published": "2022-12-19T18:10:23Z",
        "summary": "Recent work attributes progress in NLP to large language models (LMs) with\nincreased model size and large quantities of pretraining data. Despite this,\ncurrent state-of-the-art LMs for Hebrew are both under-parameterized and\nunder-trained compared to LMs in other languages. Additionally, previous work\non pretrained Hebrew LMs focused on encoder-only models. While the encoder-only\narchitecture is beneficial for classification tasks, it does not cater well for\nsub-word prediction tasks, such as Named Entity Recognition, when considering\nthe morphologically rich nature of Hebrew. In this paper we argue that\nsequence-to-sequence generative architectures are more suitable for LLMs in the\ncase of morphologically rich languages (MRLs) such as Hebrew. We demonstrate\nthat by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, we can\nleverage powerful multilingual, pretrained sequence-to-sequence models as mT5,\neliminating the need for a specialized, morpheme-based, separately fine-tuned\ndecoder. Using this approach, our experiments show substantial improvements\nover previously published results on existing Hebrew NLP benchmarks. These\nresults suggest that multilingual sequence-to-sequence models present a\npromising building block for NLP for MRLs.",
        "pdf_link": "https://arxiv.org/pdf/2212.09682v1.pdf"
    },
    {
        "title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
        "authors": [
            "Jayr Pereira",
            "Robson Fidalgo",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-12-19T17:39:07Z",
        "summary": "This paper proposes a question-answering system that can answer questions\nwhose supporting evidence is spread over multiple (potentially long) documents.\nThe system, called Visconde, uses a three-step pipeline to perform the task:\ndecompose, retrieve, and aggregate. The first step decomposes the question into\nsimpler questions using a few-shot large language model (LLM). Then, a\nstate-of-the-art search engine is used to retrieve candidate passages from a\nlarge collection for each decomposed question. In the final step, we use the\nLLM in a few-shot setting to aggregate the contents of the passages into the\nfinal answer. The system is evaluated on three datasets: IIRC, Qasper, and\nStrategyQA. Results suggest that current retrievers are the main bottleneck and\nthat readers are already performing at the human level as long as relevant\npassages are provided. The system is also shown to be more effective when the\nmodel is induced to give explanations before answering a question. Code is\navailable at \\url{https://github.com/neuralmind-ai/visconde}.",
        "pdf_link": "https://arxiv.org/pdf/2212.09656v1.pdf"
    },
    {
        "title": "Explanation Regeneration via Information Bottleneck",
        "authors": [
            "Qintong Li",
            "Zhiyong Wu",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2022-12-19T16:41:19Z",
        "summary": "Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2212.09603v2.pdf"
    },
    {
        "title": "Large Language Models are Better Reasoners with Self-Verification",
        "authors": [
            "Yixuan Weng",
            "Minjun Zhu",
            "Fei Xia",
            "Bin Li",
            "Shizhu He",
            "Shengping Liu",
            "Bin Sun",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2022-12-19T15:51:52Z",
        "summary": "Recently, with the chain of thought (CoT) prompting, large language models\n(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural\nlanguage processing tasks such as arithmetic, commonsense, and logical\nreasoning. However, LLMs with CoT require multi-step prompting and multi-token\nprediction, which is highly sensitive to individual mistakes and vulnerable to\nerror accumulation. The above issues make the LLMs need the ability to verify\nthe answers. In fact, after inferring conclusions in some thinking decision\ntasks, people often check them by re-verifying steps to avoid some mistakes. In\nthis paper, we propose and prove that LLMs also have similar self-verification\nabilities. We take the conclusion obtained by CoT as one of the conditions for\nsolving the original problem. By performing a backward verification of the\nanswers that LLM deduced for itself, we can obtain interpretable answer\nvalidation scores to select the candidate answer with the highest score.\nExperimental results demonstrate that the proposed method can improve the\nreasoning performance on various arithmetic, commonsense, and logical reasoning\ndatasets. Our code is publicly available at:\nhttps://github.com/WENGSYX/Self-Verification.",
        "pdf_link": "https://arxiv.org/pdf/2212.09561v5.pdf"
    },
    {
        "title": "Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features",
        "authors": [
            "Sourabh Zanwar",
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz"
        ],
        "published": "2022-12-19T13:58:48Z",
        "summary": "In recent years, there has been increased interest in building predictive\nmodels that harness natural language processing and machine learning techniques\nto detect emotions from various text sources, including social media posts,\nmicro-blogs or news articles. Yet, deployment of such models in real-world\nsentiment and emotion applications faces challenges, in particular poor\nout-of-domain generalizability. This is likely due to domain-specific\ndifferences (e.g., topics, communicative goals, and annotation schemes) that\nmake transfer between different models of emotion recognition difficult. In\nthis work we propose approaches for text-based emotion detection that leverage\ntransformer models (BERT and RoBERTa) in combination with Bidirectional Long\nShort-Term Memory (BiLSTM) networks trained on a comprehensive set of\npsycholinguistic features. First, we evaluate the performance of our models\nwithin-domain on two benchmark datasets: GoEmotion and ISEAR. Second, we\nconduct transfer learning experiments on six datasets from the Unified Emotion\nDataset to evaluate their out-of-domain robustness. We find that the proposed\nhybrid models improve the ability to generalize to out-of-distribution data\ncompared to a standard transformer-based approach. Moreover, we observe that\nthese models perform competitively on in-domain data.",
        "pdf_link": "https://arxiv.org/pdf/2212.09465v1.pdf"
    },
    {
        "title": "ChatGPT: The End of Online Exam Integrity?",
        "authors": [
            "Teo Susnjak"
        ],
        "published": "2022-12-19T08:15:16Z",
        "summary": "This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.",
        "pdf_link": "https://arxiv.org/pdf/2212.09292v1.pdf"
    },
    {
        "title": "Very Large Language Model as a Unified Methodology of Text Mining",
        "authors": [
            "Meng Jiang"
        ],
        "published": "2022-12-19T06:52:13Z",
        "summary": "Text data mining is the process of deriving essential information from\nlanguage text. Typical text mining tasks include text categorization, text\nclustering, topic modeling, information extraction, and text summarization.\nVarious data sets are collected and various algorithms are designed for the\ndifferent types of tasks. In this paper, I present a blue sky idea that very\nlarge language model (VLLM) will become an effective unified methodology of\ntext mining. I discuss at least three advantages of this new methodology\nagainst conventional methods. Finally I discuss the challenges in the design\nand development of VLLM techniques for text mining.",
        "pdf_link": "https://arxiv.org/pdf/2212.09271v2.pdf"
    },
    {
        "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization",
        "authors": [
            "Bairu Hou",
            "Jinghan Jia",
            "Yihua Zhang",
            "Guanhua Zhang",
            "Yang Zhang",
            "Sijia Liu",
            "Shiyu Chang"
        ],
        "published": "2022-12-19T05:55:58Z",
        "summary": "Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.",
        "pdf_link": "https://arxiv.org/pdf/2212.09254v1.pdf"
    },
    {
        "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
        "authors": [
            "Ethan Perez",
            "Sam Ringer",
            "Kamilė Lukošiūtė",
            "Karina Nguyen",
            "Edwin Chen",
            "Scott Heiner",
            "Craig Pettit",
            "Catherine Olsson",
            "Sandipan Kundu",
            "Saurav Kadavath",
            "Andy Jones",
            "Anna Chen",
            "Ben Mann",
            "Brian Israel",
            "Bryan Seethor",
            "Cameron McKinnon",
            "Christopher Olah",
            "Da Yan",
            "Daniela Amodei",
            "Dario Amodei",
            "Dawn Drain",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Guro Khundadze",
            "Jackson Kernion",
            "James Landis",
            "Jamie Kerr",
            "Jared Mueller",
            "Jeeyoon Hyun",
            "Joshua Landau",
            "Kamal Ndousse",
            "Landon Goldberg",
            "Liane Lovitt",
            "Martin Lucas",
            "Michael Sellitto",
            "Miranda Zhang",
            "Neerav Kingsland",
            "Nelson Elhage",
            "Nicholas Joseph",
            "Noemí Mercado",
            "Nova DasSarma",
            "Oliver Rausch",
            "Robin Larson",
            "Sam McCandlish",
            "Scott Johnston",
            "Shauna Kravec",
            "Sheer El Showk",
            "Tamera Lanham",
            "Timothy Telleen-Lawton",
            "Tom Brown",
            "Tom Henighan",
            "Tristan Hume",
            "Yuntao Bai",
            "Zac Hatfield-Dodds",
            "Jack Clark",
            "Samuel R. Bowman",
            "Amanda Askell",
            "Roger Grosse",
            "Danny Hernandez",
            "Deep Ganguli",
            "Evan Hubinger",
            "Nicholas Schiefer",
            "Jared Kaplan"
        ],
        "published": "2022-12-19T05:13:52Z",
        "summary": "As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user's preferred answer\n(\"sycophancy\") and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2212.09251v1.pdf"
    },
    {
        "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
        "authors": [
            "Parishad BehnamGhader",
            "Santiago Miret",
            "Siva Reddy"
        ],
        "published": "2022-12-18T19:27:41Z",
        "summary": "Augmenting pretrained language models with retrievers has shown promise in\neffectively solving common NLP problems, such as language modeling and question\nanswering. In this paper, we evaluate the strengths and weaknesses of popular\nretriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,\nContriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved\nstatements across different tasks. Our findings indicate that the simple\nsimilarity metric employed by retrievers is insufficient for retrieving all the\nnecessary statements for reasoning. Additionally, the language models do not\nexhibit strong reasoning even when provided with only the required statements.\nFurthermore, when combined with imperfect retrievers, the performance of the\nlanguage models becomes even worse, e.g., Flan-T5's performance drops by 28.6%\nwhen retrieving 5 statements using Contriever. While larger language models\nimprove performance, there is still a substantial room for enhancement. Our\nfurther analysis indicates that multihop retrieve-and-read is promising for\nlarge language models like GPT-3.5, but does not generalize to other language\nmodels like Flan-T5-xxl.",
        "pdf_link": "https://arxiv.org/pdf/2212.09146v3.pdf"
    },
    {
        "title": "Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
        "authors": [
            "Chengyue Jiang",
            "Wenyang Hui",
            "Yong Jiang",
            "Xiaobin Wang",
            "Pengjun Xie",
            "Kewei Tu"
        ],
        "published": "2022-12-18T16:42:52Z",
        "summary": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,\npresident, politician) of a given entity mention (e.g., Joe Biden) in context.\nState-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.\nCE concatenates the mention (and its context) with each type and feeds the\npairs into a pretrained language model (PLM) to score their relevance. It\nbrings deeper interaction between mention and types to reach better performance\nbut has to perform N (type set size) forward passes to infer types of a single\nmention. CE is therefore very slow in inference when the type set is large\n(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a\nrecall-expand-filter manner. The recall and expand stages prune the large type\nset and generate K (K is typically less than 256) most relevant type candidates\nfor each mention. At the filter stage, we use a novel model called MCCE to\nconcurrently encode and score these K candidates in only one forward pass to\nobtain the final type prediction. We investigate different variants of MCCE and\nextensive experiments show that MCCE under our paradigm reaches SOTA\nperformance on ultra-fine entity typing and is thousands of times faster than\nthe cross-encoder. We also found MCCE is very effective in fine-grained (130\ntypes) and coarse-grained (9 types) entity typing. Our code is available at\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.",
        "pdf_link": "https://arxiv.org/pdf/2212.09125v1.pdf"
    },
    {
        "title": "Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search",
        "authors": [
            "Shuai Wang",
            "Harrisen Scells",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2022-12-18T05:26:40Z",
        "summary": "Medical systematic reviews typically require assessing all the documents\nretrieved by a search. The reason is two-fold: the task aims for ``total\nrecall''; and documents retrieved using Boolean search are an unordered set,\nand thus it is unclear how an assessor could examine only a subset. Screening\nprioritisation is the process of ranking the (unordered) set of retrieved\ndocuments, allowing assessors to begin the downstream processes of the\nsystematic review creation earlier, leading to earlier completion of the\nreview, or even avoiding screening documents ranked least relevant.\n  Screening prioritisation requires highly effective ranking methods.\nPre-trained language models are state-of-the-art on many IR tasks but have yet\nto be applied to systematic review screening prioritisation. In this paper, we\napply several pre-trained language models to the systematic review document\nranking task, both directly and fine-tuned. An empirical analysis compares how\neffective neural methods compare to traditional methods for this task. We also\ninvestigate different types of document representations for neural methods and\ntheir impact on ranking performance.\n  Our results show that BERT-based rankers outperform the current\nstate-of-the-art screening prioritisation methods. However, BERT rankers and\nexisting methods can actually be complementary, and thus, further improvements\nmay be achieved if used in conjunction.",
        "pdf_link": "https://arxiv.org/pdf/2212.09017v1.pdf"
    },
    {
        "title": "Language model acceptability judgements are not always robust to context",
        "authors": [
            "Koustuv Sinha",
            "Jon Gauthier",
            "Aaron Mueller",
            "Kanishka Misra",
            "Keren Fuentes",
            "Roger Levy",
            "Adina Williams"
        ],
        "published": "2022-12-18T00:11:06Z",
        "summary": "Targeted syntactic evaluations of language models ask whether models show\nstable preferences for syntactically acceptable content over minimal-pair\nunacceptable inputs. Most targeted syntactic evaluation datasets ask models to\nmake these judgements with just a single context-free sentence as input. This\ndoes not match language models' training regime, in which input sentences are\nalways highly contextualized by the surrounding corpus. This mismatch raises an\nimportant question: how robust are models' syntactic judgements in different\ncontexts? In this paper, we investigate the stability of language models'\nperformance on targeted syntactic evaluations as we vary properties of the\ninput context: the length of the context, the types of syntactic phenomena it\ncontains, and whether or not there are violations of grammaticality. We find\nthat model judgements are generally robust when placed in randomly sampled\nlinguistic contexts. However, they are substantially unstable for contexts\ncontaining syntactic structures matching those in the critical test content.\nAmong all tested models (GPT-2 and five variants of OPT), we significantly\nimprove models' judgements by providing contexts with matching syntactic\nstructures, and conversely significantly worsen them using unacceptable\ncontexts with matching but violated syntactic structures. This effect is\namplified by the length of the context, except for unrelated inputs. We show\nthat these changes in model performance are not explainable by simple features\nmatching the context and the test inputs, such as lexical overlap and\ndependency overlap. This sensitivity to highly specific syntactic features of\nthe context can only be explained by the models' implicit in-context learning\nabilities.",
        "pdf_link": "https://arxiv.org/pdf/2212.08979v1.pdf"
    },
    {
        "title": "Graph Learning and Its Advancements on Large Language Models: A Holistic Survey",
        "authors": [
            "Shaopeng Wei",
            "Yu Zhao",
            "Xingyan Chen",
            "Qing Li",
            "Fuzhen Zhuang",
            "Ji Liu",
            "Fuji Ren",
            "Gang Kou"
        ],
        "published": "2022-12-17T22:05:07Z",
        "summary": "Graph learning is a prevalent domain that endeavors to learn the intricate\nrelationships among nodes and the topological structure of graphs. Over the\nyears, graph learning has transcended from graph theory to graph data mining.\nWith the advent of representation learning, it has attained remarkable\nperformance in diverse scenarios. Owing to its extensive application prospects,\ngraph learning attracts copious attention. While some researchers have\naccomplished impressive surveys on graph learning, they failed to connect\nrelated objectives, methods, and applications in a more coherent way. As a\nresult, they did not encompass current ample scenarios and challenging problems\ndue to the rapid expansion of graph learning. Particularly, large language\nmodels have recently had a disruptive effect on human life, but they also show\nrelative weakness in structured scenarios. The question of how to make these\nmodels more powerful with graph learning remains open. Our survey focuses on\nthe most recent advancements in integrating graph learning with pre-trained\nlanguage models, specifically emphasizing their application within the domain\nof large language models. Different from previous surveys on graph learning, we\nprovide a holistic review that analyzes current works from the perspective of\ngraph structure, and discusses the latest applications, trends, and challenges\nin graph learning. Specifically, we commence by proposing a taxonomy and then\nsummarize the methods employed in graph learning. We then provide a detailed\nelucidation of mainstream applications. Finally, we propose future directions.",
        "pdf_link": "https://arxiv.org/pdf/2212.08966v4.pdf"
    },
    {
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Chuanqi Tan",
            "Fei Huang",
            "Songfang Huang"
        ],
        "published": "2022-12-17T11:56:21Z",
        "summary": "Language models with the Transformers structure have shown great performance\nin natural language processing. However, there still poses problems when\nfine-tuning pre-trained language models on downstream tasks, such as\nover-fitting or representation collapse. In this work, we propose HyPe, a\nsimple yet effective fine-tuning technique to alleviate such problems by\nperturbing hidden representations of Transformers layers. Unlike previous works\nthat only add noise to inputs or parameters, we argue that the hidden\nrepresentations of Transformers layers convey more diverse and meaningful\nlanguage information. Therefore, making the Transformers layers more robust to\nhidden representation perturbations can further benefit the fine-tuning of PLMs\nen bloc. We conduct extensive experiments and analyses on GLUE and other\nnatural language inference datasets. Results demonstrate that HyPe outperforms\nvanilla fine-tuning and enhances generalization of hidden representations from\ndifferent layers. In addition, HyPe acquires negligible computational\noverheads, and is better than and compatible with previous state-of-the-art\nfine-tuning techniques.",
        "pdf_link": "https://arxiv.org/pdf/2212.08853v2.pdf"
    },
    {
        "title": "Neural Story Planning",
        "authors": [
            "Anbang Ye",
            "Christopher Cui",
            "Taiwei Shi",
            "Mark O. Riedl"
        ],
        "published": "2022-12-16T21:29:41Z",
        "summary": "Automated plot generation is the challenge of generating a sequence of events\nthat will be perceived by readers as the plot of a coherent story. Traditional\nsymbolic planners plan a story from a goal state and guarantee logical causal\nplot coherence but rely on a library of hand-crafted actions with their\npreconditions and effects. This closed world setting limits the length and\ndiversity of what symbolic planners can generate. On the other hand,\npre-trained neural language models can generate stories with great diversity,\nwhile being generally incapable of ending a story in a specified manner and can\nhave trouble maintaining coherence. In this paper, we present an approach to\nstory plot generation that unifies causal planning with neural language models.\nWe propose to use commonsense knowledge extracted from large language models to\nrecursively expand a story plot in a backward chaining fashion. Specifically,\nour system infers the preconditions for events in the story and then events\nthat will cause those conditions to become true. We performed automatic\nevaluation to measure narrative coherence as indicated by the ability to answer\nquestions about whether different events in the story are causally related to\nother events. Results indicate that our proposed method produces more coherent\nplotlines than several strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2212.08718v1.pdf"
    },
    {
        "title": "Plansformer: Generating Symbolic Plans using Transformers",
        "authors": [
            "Vishal Pallagani",
            "Bharath Muppasani",
            "Keerthiram Murugesan",
            "Francesca Rossi",
            "Lior Horesh",
            "Biplav Srivastava",
            "Francesco Fabiano",
            "Andrea Loreggia"
        ],
        "published": "2022-12-16T19:06:49Z",
        "summary": "Large Language Models (LLMs) have been the subject of active research,\nsignificantly advancing the field of Natural Language Processing (NLP). From\nBERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural\nlanguage tasks such as question answering, summarization, and text generation.\nMany ongoing efforts focus on understanding LLMs' capabilities, including their\nknowledge of the world, syntax, and semantics. However, extending the textual\nprowess of LLMs to symbolic reasoning has been slow and predominantly focused\non tackling problems related to the mathematical field. In this paper, we\nexplore the use of LLMs for automated planning - a branch of AI concerned with\nthe realization of action sequences (plans) to achieve a goal, typically\nexecuted by intelligent agents, autonomous robots, and unmanned vehicles. We\nintroduce Plansformer; an LLM fine-tuned on planning problems and capable of\ngenerating plans with favorable behavior in terms of correctness and length\nwith reduced knowledge-engineering efforts. We also demonstrate the\nadaptability of Plansformer in solving different planning domains with varying\ncomplexities, owing to the transfer learning abilities of LLMs. For one\nconfiguration of Plansformer, we achieve ~97% valid plans, out of which ~95%\nare optimal for Towers of Hanoi - a puzzle-solving domain.",
        "pdf_link": "https://arxiv.org/pdf/2212.08681v1.pdf"
    },
    {
        "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
        "authors": [
            "Swarnadeep Saha",
            "Xinyan Velocity Yu",
            "Mohit Bansal",
            "Ramakanth Pasunuru",
            "Asli Celikyilmaz"
        ],
        "published": "2022-12-16T17:36:23Z",
        "summary": "Prompting large language models has enabled significant recent progress in\nmulti-step reasoning over text. However, when applied to text generation from\nsemi-structured data (e.g., graphs or tables), these methods typically suffer\nfrom low semantic coverage, hallucination, and logical inconsistency. We\npropose MURMUR, a neuro-symbolic modular approach to text generation from\nsemi-structured data with multi-step reasoning. MURMUR is a best-first search\nmethod that generates reasoning paths using: (1) neural and symbolic modules\nwith specific linguistic and logical skills, (2) a grammar whose production\nrules define valid compositions of modules, and (3) value functions that assess\nthe quality of each reasoning step. We conduct experiments on two diverse\ndata-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in\ntheir data representations (graphs and tables) and span multiple linguistic and\nlogical skills. MURMUR obtains significant improvements over recent few-shot\nbaselines like direct prompting and chain-of-thought prompting, while also\nachieving comparable performance to fine-tuned GPT-2 on out-of-domain data.\nMoreover, human evaluation shows that MURMUR generates highly faithful and\ncorrect reasoning paths that lead to 26% more logically consistent summaries on\nLogicNLG, compared to direct prompting.",
        "pdf_link": "https://arxiv.org/pdf/2212.08607v1.pdf"
    },
    {
        "title": "Teaching Small Language Models to Reason",
        "authors": [
            "Lucie Charlotte Magister",
            "Jonathan Mallinson",
            "Jakub Adamek",
            "Eric Malmi",
            "Aliaksei Severyn"
        ],
        "published": "2022-12-16T11:24:42Z",
        "summary": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.",
        "pdf_link": "https://arxiv.org/pdf/2212.08410v3.pdf"
    },
    {
        "title": "LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension",
        "authors": [
            "Wenyue Hua",
            "Yuchen Zhang",
            "Zhe Chen",
            "Josie Li",
            "Melanie Weber"
        ],
        "published": "2022-12-16T00:15:14Z",
        "summary": "The application of Natural Language Processing (NLP) to specialized domains,\nsuch as the law, has recently received a surge of interest. As many legal\nservices rely on processing and analyzing large collections of documents,\nautomating such tasks with NLP tools emerges as a key challenge. Many popular\nlanguage models, such as BERT or RoBERTa, are general-purpose models, which\nhave limitations on processing specialized legal terminology and syntax. In\naddition, legal documents may contain specialized vocabulary from other\ndomains, such as medical terminology in personal injury text. Here, we propose\nLegalRelectra, a legal-domain language model that is trained on mixed-domain\nlegal and medical corpora. We show that our model improves over general-domain\nand single-domain medical and legal language models when processing\nmixed-domain (personal injury) text. Our training architecture implements the\nElectra framework, but utilizes Reformer instead of BERT for its generator and\ndiscriminator. We show that this improves the model's performance on processing\nlong passages and results in better long-range text comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2212.08204v1.pdf"
    },
    {
        "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
        "authors": [
            "Michiel de Jong",
            "Yury Zemlyanskiy",
            "Joshua Ainslie",
            "Nicholas FitzGerald",
            "Sumit Sanghai",
            "Fei Sha",
            "William Cohen"
        ],
        "published": "2022-12-15T21:35:46Z",
        "summary": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, the\narchitecture used for FiD was chosen by making minimal modifications to a\nstandard T5 model, which our analysis shows to be highly suboptimal for a\nretrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to\nthe encoder, while the majority of inference time results from memory bandwidth\nconstraints in the decoder. We propose two simple changes to the FiD\narchitecture to alleviate memory bandwidth constraints, and speed up inference\nby 7x. This allows us to use a much larger decoder at modest cost. We denote\nFiD with the above modifications as FiDO, and show that it strongly improves\nperformance over existing FiD models for a wide range of inference budgets. For\nexample, FiDO-Large-XXL performs faster inference than FiD-Base and achieves\nbetter performance than FiD-Large.",
        "pdf_link": "https://arxiv.org/pdf/2212.08153v2.pdf"
    },
    {
        "title": "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
        "authors": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "published": "2022-12-15T18:59:32Z",
        "summary": "Generating a Chain of Thought (CoT) has been shown to consistently improve\nlarge language model (LLM) performance on a wide range of NLP tasks. However,\nprior work has mainly focused on logical reasoning tasks (e.g. arithmetic,\ncommonsense QA); it remains unclear whether improvements hold for more diverse\ntypes of reasoning, especially in socially situated contexts. Concretely, we\nperform a controlled evaluation of zero-shot CoT across two socially sensitive\ndomains: harmful questions and stereotype benchmarks. We find that zero-shot\nCoT reasoning in sensitive domains significantly increases a model's likelihood\nto produce harmful or undesirable output, with trends holding across different\nprompt formats and model variants. Furthermore, we show that harmful CoTs\nincrease with model size, but decrease with improved instruction following. Our\nwork suggests that zero-shot CoT should be used with caution on socially\nimportant tasks, especially when marginalized groups or sensitive topics are\ninvolved.",
        "pdf_link": "https://arxiv.org/pdf/2212.08061v2.pdf"
    },
    {
        "title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
        "authors": [
            "Bernd Bohnet",
            "Vinh Q. Tran",
            "Pat Verga",
            "Roee Aharoni",
            "Daniel Andor",
            "Livio Baldini Soares",
            "Massimiliano Ciaramita",
            "Jacob Eisenstein",
            "Kuzman Ganchev",
            "Jonathan Herzig",
            "Kai Hui",
            "Tom Kwiatkowski",
            "Ji Ma",
            "Jianmo Ni",
            "Lierni Sestorain Saralegui",
            "Tal Schuster",
            "William W. Cohen",
            "Michael Collins",
            "Dipanjan Das",
            "Donald Metzler",
            "Slav Petrov",
            "Kellie Webster"
        ],
        "published": "2022-12-15T18:45:29Z",
        "summary": "Large language models (LLMs) have shown impressive results while requiring\nlittle or no direct supervision. Further, there is mounting evidence that LLMs\nmay have potential in information-seeking scenarios. We believe the ability of\nan LLM to attribute the text that it generates is likely to be crucial in this\nsetting. We formulate and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We propose a reproducible evaluation framework\nfor the task and benchmark a broad set of architectures. We take human\nannotations as a gold standard and show that a correlated automatic metric is\nsuitable for development. Our experimental work gives concrete answers to two\nkey questions (How to measure attribution?, and How well do current\nstate-of-the-art methods perform on attribution?), and give some hints as to\nhow to address a third (How to build LLMs with attribution?).",
        "pdf_link": "https://arxiv.org/pdf/2212.08037v2.pdf"
    },
    {
        "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
        "authors": [
            "Yixin Liu",
            "Alexander R. Fabbri",
            "Pengfei Liu",
            "Yilun Zhao",
            "Linyong Nan",
            "Ruilin Han",
            "Simeng Han",
            "Shafiq Joty",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Dragomir Radev"
        ],
        "published": "2022-12-15T17:26:05Z",
        "summary": "Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation studies for summarization either exhibit a low inter-annotator\nagreement or have insufficient scale, and an in-depth analysis of human\nevaluation is lacking. Therefore, we address the shortcomings of existing\nsummarization evaluation along the following axes: (1) We propose a modified\nsummarization salience protocol, Atomic Content Units (ACUs), which is based on\nfine-grained semantic units and allows for a high inter-annotator agreement.\n(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large\nhuman evaluation dataset consisting of 22,000 summary-level annotations over 28\ntop-performing systems on three datasets. (3) We conduct a comparative study of\nfour human evaluation protocols, underscoring potential confounding factors in\nevaluation setups. (4) We evaluate 50 automatic metrics and their variants\nusing the collected human annotations across evaluation protocols and\ndemonstrate how our benchmark leads to more statistically stable and\nsignificant results. The metrics we benchmarked include recent methods based on\nlarge language models (LLMs), GPTScore and G-Eval. Furthermore, our findings\nhave important implications for evaluating LLMs, as we show that LLMs adjusted\nby human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,\nwhich is affected by the annotators' prior, input-agnostic preferences, calling\nfor more robust, targeted evaluation methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.07981v2.pdf"
    },
    {
        "title": "Visually-augmented pretrained language models for NLP tasks without images",
        "authors": [
            "Hangyu Guo",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Qinyu Zhang",
            "Ji-Rong Wen"
        ],
        "published": "2022-12-15T16:13:25Z",
        "summary": "Although pre-trained language models~(PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense. Existing solutions often rely on explicit images for visual\nknowledge augmentation (requiring time-consuming retrieval or generation), and\nthey also conduct the augmentation for the whole input text, without\nconsidering whether it is actually needed in specific inputs or tasks. To\naddress these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented\nfine-tuning approach that can be generally applied to various PLMs or NLP\ntasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages,\nnamely \\textbf{VAWI}. Experimental results show that our approach can\nconsistently improve the performance of BERT, RoBERTa, BART, and T5 at\ndifferent scales, and outperform several competitive baselines on ten tasks.\nOur codes and data are publicly available\nat~\\url{https://github.com/RUCAIBox/VAWI}.",
        "pdf_link": "https://arxiv.org/pdf/2212.07937v2.pdf"
    },
    {
        "title": "DeepJoin: Joinable Table Discovery with Pre-trained Language Models",
        "authors": [
            "Yuyang Dong",
            "Chuan Xiao",
            "Takuma Nozawa",
            "Masafumi Enomoto",
            "Masafumi Oyamada"
        ],
        "published": "2022-12-15T02:40:57Z",
        "summary": "Due to the usefulness in data enrichment for data analysis tasks, joinable\ntable discovery has become an important operation in data lake management.\nExisting approaches target equi-joins, the most common way of combining tables\nfor creating a unified view, or semantic joins, which tolerate misspellings and\ndifferent formats to deliver more join results. They are either exact solutions\nwhose running time is linear in the sizes of query column and target table\nrepository or approximate solutions lacking precision. In this paper, we\npropose Deepjoin, a deep learning model for accurate and efficient joinable\ntable discovery. Our solution is an embedding-based retrieval, which employs a\npre-trained language model (PLM) and is designed as one framework serving both\nequi- and semantic joins. We propose a set of contextualization options to\ntransform column contents to a text sequence. The PLM reads the sequence and is\nfine-tuned to embed columns to vectors such that columns are expected to be\njoinable if they are close to each other in the vector space. Since the output\nof the PLM is fixed in length, the subsequent search procedure becomes\nindependent of the column size. With a state-of-the-art approximate nearest\nneighbor search algorithm, the search time is logarithmic in the repository\nsize. To train the model, we devise the techniques for preparing training data\nas well as data augmentation. The experiments on real datasets demonstrate that\nby training on a small subset of a corpus, Deepjoin generalizes to large\ndatasets and its precision consistently outperforms other approximate\nsolutions'. Deepjoin is even more accurate than an exact solution to semantic\njoins when evaluated with labels from experts. Moreover, when equipped with a\nGPU, Deepjoin is up to two orders of magnitude faster than existing solutions.",
        "pdf_link": "https://arxiv.org/pdf/2212.07588v2.pdf"
    },
    {
        "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages",
        "authors": [
            "Yekun Chai",
            "Shuohuan Wang",
            "Chao Pang",
            "Yu Sun",
            "Hao Tian",
            "Hua Wu"
        ],
        "published": "2022-12-13T17:21:44Z",
        "summary": "Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nrelease our code and pre-trained checkpoints.",
        "pdf_link": "https://arxiv.org/pdf/2212.06742v2.pdf"
    },
    {
        "title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation",
        "authors": [
            "Shailja Thakur",
            "Baleegh Ahmad",
            "Zhenxing Fan",
            "Hammond Pearce",
            "Benjamin Tan",
            "Ramesh Karri",
            "Brendan Dolan-Gavitt",
            "Siddharth Garg"
        ],
        "published": "2022-12-13T16:34:39Z",
        "summary": "Automating hardware design could obviate a significant amount of human error\nfrom the engineering process and lead to fewer errors. Verilog is a popular\nhardware description language to model and design digital systems, thus\ngenerating Verilog code is a critical first step. Emerging large language\nmodels (LLMs) are able to write high-quality code in other programming\nlanguages. In this paper, we characterize the ability of LLMs to generate\nuseful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets\ncollected from GitHub and Verilog textbooks. We construct an evaluation\nframework comprising test-benches for functional analysis and a flow to test\nthe syntax of Verilog code generated in response to problems of varying\ndifficulty. Our findings show that across our problem scenarios, the\nfine-tuning results in LLMs more capable of producing syntactically correct\ncode (25.9% overall). Further, when analyzing functional correctness, a\nfine-tuned open-source CodeGen LLM can outperform the state-of-the-art\ncommercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM\ncheckpoints are available: https://github.com/shailja-thakur/VGen.",
        "pdf_link": "https://arxiv.org/pdf/2212.11140v1.pdf"
    },
    {
        "title": "On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning",
        "authors": [
            "Yiting Qu",
            "Xinlei He",
            "Shannon Pierson",
            "Michael Backes",
            "Yang Zhang",
            "Savvas Zannettou"
        ],
        "published": "2022-12-13T13:38:04Z",
        "summary": "The dissemination of hateful memes online has adverse effects on social media\nplatforms and the real world. Detecting hateful memes is challenging, one of\nthe reasons being the evolutionary nature of memes; new hateful memes can\nemerge by fusing hateful connotations with other cultural ideas or symbols. In\nthis paper, we propose a framework that leverages multimodal contrastive\nlearning models, in particular OpenAI's CLIP, to identify targets of hateful\ncontent and systematically investigate the evolution of hateful memes. We find\nthat semantic regularities exist in CLIP-generated embeddings that describe\nsemantic relationships within the same modality (images) or across modalities\n(images and text). Leveraging this property, we study how hateful memes are\ncreated by combining visual elements from multiple images or fusing textual\ninformation with a hateful image. We demonstrate the capabilities of our\nframework for analyzing the evolution of hateful memes by focusing on\nantisemitic memes, particularly the Happy Merchant meme. Using our framework on\na dataset extracted from 4chan, we find 3.3K variants of the Happy Merchant\nmeme, with some linked to specific countries, persons, or organizations. We\nenvision that our framework can be used to aid human moderators by flagging new\nvariants of hateful memes so that moderators can manually verify them and\nmitigate the problem of hateful content online.",
        "pdf_link": "https://arxiv.org/pdf/2212.06573v2.pdf"
    },
    {
        "title": "Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety",
        "authors": [
            "Joshua Albrecht",
            "Ellie Kitanidis",
            "Abraham J. Fetterman"
        ],
        "published": "2022-12-13T00:29:45Z",
        "summary": "Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2212.06295v1.pdf"
    },
    {
        "title": "Evaluation of Synthetic Datasets for Conversational Recommender Systems",
        "authors": [
            "Harsh Lara",
            "Manoj Tiwari"
        ],
        "published": "2022-12-12T18:53:10Z",
        "summary": "For researchers leveraging Large-Language Models (LLMs) in the generation of\ntraining datasets, especially for conversational recommender systems - the\nabsence of robust evaluation frameworks has been a long-standing problem. The\nefficiency brought about by LLMs in the data generation phase is impeded during\nthe process of evaluation of the generated data, since it generally requires\nhuman-raters to ensure that the data generated is of high quality and has\nsufficient diversity. Since the quality of training data is critical for\ndownstream applications, it is important to develop metrics that evaluate the\nquality holistically and identify biases. In this paper, we present a framework\nthat takes a multi-faceted approach towards evaluating datasets produced by\ngenerative models and discuss the advantages and limitations of various\nevaluation methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.08167v1.pdf"
    },
    {
        "title": "Prompting Is Programming: A Query Language for Large Language Models",
        "authors": [
            "Luca Beurer-Kellner",
            "Marc Fischer",
            "Martin Vechev"
        ],
        "published": "2022-12-12T18:09:09Z",
        "summary": "Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n  Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks while abstracting language model internals and providing\nhigh-level semantics.\n  To enable LMP, we implement LMQL(short for Language Model Query Language),\nwhich leverages the constraints and control flow from an LMP prompt to generate\nan efficient inference procedure that minimizes the number of expensive calls\nto the underlying language model.\n  We show that LMQL can capture a wide range of state-of-the-art prompting\nmethods in an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (26-85% cost savings).",
        "pdf_link": "https://arxiv.org/pdf/2212.06094v3.pdf"
    },
    {
        "title": "A Study of Slang Representation Methods",
        "authors": [
            "Aravinda Kolla",
            "Filip Ilievski",
            "Hông-Ân Sandlin",
            "Alain Mermoud"
        ],
        "published": "2022-12-11T21:56:44Z",
        "summary": "Considering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.",
        "pdf_link": "https://arxiv.org/pdf/2212.05613v3.pdf"
    },
    {
        "title": "Elixir: Train a Large Language Model on a Small GPU Cluster",
        "authors": [
            "Haichen Huang",
            "Jiarui Fang",
            "Hongxin Liu",
            "Shenggui Li",
            "Yang You"
        ],
        "published": "2022-12-10T17:26:05Z",
        "summary": "In recent years, large language models have achieved great success due to\ntheir unprecedented size. However, training these models poses a challenge for\nmost researchers as it requires a substantial number of GPUs. To reduce GPU\nmemory usage, memory partitioning, and memory offloading have been proposed.\nThese approaches eliminate memory redundancies and offload memory usage to the\nCPU and NVMe memory, respectively, enabling training on small GPU clusters.\nHowever, directly deploying these solutions often leads to suboptimal\nefficiency. Only experienced experts can unleash the full potential of hardware\nby carefully tuning the distributed configuration. Thus, we present a novel\nsolution, Elixir, which automates efficient large-model training based on\npre-runtime model profiling. Elixir aims to identify the optimal combination of\npartitioning and offloading techniques to maximize training throughput. In our\nexperiments, Elixir significantly outperforms the current state-of-the-art\nbaseline. Our optimal configuration achieves up to a 3.4$\\times$ speedup on\nGPT-2 models compared with SOTA solutions. We hope that our work will benefit\nindividuals who lack computing resources and expertise, granting them access to\nlarge models. The beta version of Elixir is now available at\nhttps://github.com/hpcaitech/ColossalAI/tree/feature/elixir.",
        "pdf_link": "https://arxiv.org/pdf/2212.05339v3.pdf"
    },
    {
        "title": "Structured information extraction from complex scientific text with fine-tuned large language models",
        "authors": [
            "Alexander Dunn",
            "John Dagdelen",
            "Nicholas Walker",
            "Sanghoon Lee",
            "Andrew S. Rosen",
            "Gerbrand Ceder",
            "Kristin Persson",
            "Anubhav Jain"
        ],
        "published": "2022-12-10T07:51:52Z",
        "summary": "Intelligently extracting and linking complex scientific information from\nunstructured text is a challenging endeavor particularly for those\ninexperienced with natural language processing. Here, we present a simple\nsequence-to-sequence approach to joint named entity recognition and relation\nextraction for complex hierarchical information in scientific text. The\napproach leverages a pre-trained large language model (LLM), GPT-3, that is\nfine-tuned on approximately 500 pairs of prompts (inputs) and completions\n(outputs). Information is extracted either from single sentences or across\nsentences in abstracts/passages, and the output can be returned as simple\nEnglish sentences or a more structured format, such as a list of JSON objects.\nWe demonstrate that LLMs trained in this way are capable of accurately\nextracting useful records of complex scientific knowledge for three\nrepresentative tasks in materials chemistry: linking dopants with their host\nmaterials, cataloging metal-organic frameworks, and general\nchemistry/phase/morphology/application information extraction. This approach\nrepresents a simple, accessible, and highly-flexible route to obtaining large\ndatabases of structured knowledge extracted from unstructured text. An online\ndemo is available at http://www.matscholar.com/info-extraction.",
        "pdf_link": "https://arxiv.org/pdf/2212.05238v1.pdf"
    },
    {
        "title": "The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",
        "authors": [
            "Alexandre Blanco-Gonzalez",
            "Alfonso Cabezon",
            "Alejandro Seco-Gonzalez",
            "Daniel Conde-Torres",
            "Paula Antelo-Riveiro",
            "Angel Pineiro",
            "Rebeca Garcia-Fandino"
        ],
        "published": "2022-12-08T23:23:39Z",
        "summary": "Artificial intelligence (AI) has the potential to revolutionize the drug\ndiscovery process, offering improved efficiency, accuracy, and speed. However,\nthe successful application of AI is dependent on the availability of\nhigh-quality data, the addressing of ethical concerns, and the recognition of\nthe limitations of AI-based approaches. In this article, the benefits,\nchallenges and drawbacks of AI in this field are reviewed, and possible\nstrategies and approaches for overcoming the present obstacles are proposed.\nThe use of data augmentation, explainable AI, and the integration of AI with\ntraditional experimental methods, as well as the potential advantages of AI in\npharmaceutical research are also discussed. Overall, this review highlights the\npotential of AI in drug discovery and provides insights into the challenges and\nopportunities for realizing its potential in this field.\n  Note from the human-authors: This article was created to test the ability of\nChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors\nin writing review articles. The text generated by the AI following our\ninstructions (see Supporting Information) was used as a starting point, and its\nability to automatically generate content was evaluated. After conducting a\nthorough review, human authors practically rewrote the manuscript, striving to\nmaintain a balance between the original proposal and scientific criteria. The\nadvantages and limitations of using AI for this purpose are discussed in the\nlast section.",
        "pdf_link": "https://arxiv.org/pdf/2212.08104v1.pdf"
    },
    {
        "title": "Learning Video Representations from Large Language Models",
        "authors": [
            "Yue Zhao",
            "Ishan Misra",
            "Philipp Krähenbühl",
            "Rohit Girdhar"
        ],
        "published": "2022-12-08T18:59:59Z",
        "summary": "We introduce LaViLa, a new approach to learning video-language\nrepresentations by leveraging Large Language Models (LLMs). We repurpose\npre-trained LLMs to be conditioned on visual input, and finetune them to create\nautomatic video narrators. Our auto-generated narrations offer a number of\nadvantages, including dense coverage of long videos, better temporal\nsynchronization of the visual information and text, and much higher diversity\nof text. The video-text embedding learned contrastively with these additional\nauto-generated narrations outperforms the previous state-of-the-art on multiple\nfirst-person and third-person video tasks, both in zero-shot and finetuned\nsetups. Most notably, LaViLa obtains an absolute gain of 10.1% on EGTEA\nclassification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks.\nFurthermore, LaViLa trained with only half the narrations from the Ego4D\ndataset outperforms baseline models trained on the full set, and shows positive\nscaling behavior on increasing pre-training data and model size.",
        "pdf_link": "https://arxiv.org/pdf/2212.04501v1.pdf"
    },
    {
        "title": "Learning Domain Invariant Prompt for Vision-Language Models",
        "authors": [
            "Cairong Zhao",
            "Yubin Wang",
            "Xinyang Jiang",
            "Yifei Shen",
            "Kaitao Song",
            "Dongsheng Li",
            "Duoqian Miao"
        ],
        "published": "2022-12-08T11:23:24Z",
        "summary": "Prompt learning is one of the most effective and trending ways to adapt\npowerful vision-language foundation models like CLIP to downstream datasets by\ntuning learnable prompt vectors with very few samples. However, although prompt\nlearning achieves excellent performance over in-domain data, it still faces the\nmajor challenge of generalizing to unseen classes and domains. Some existing\nprompt learning methods tackle this issue by adaptively generating different\nprompts for different tokens or domains but neglecting the ability of learned\nprompts to generalize to unseen domains. In this paper, we propose a novel\nprompt learning paradigm that directly generates \\emph{domain invariant} prompt\nthat can be generalized to unseen domains, called MetaPrompt. Specifically, a\ndual-modality prompt tuning network is proposed to generate prompts for input\nfrom both image and text modalities. With a novel asymmetric contrastive loss,\nthe representation from the original pre-trained vision-language model acts as\nsupervision to enhance the generalization ability of the learned prompt. More\nimportantly, we propose a meta-learning-based prompt tuning algorithm that\nexplicitly constrains the task-specific prompt tuned for one domain or class to\nalso achieve good performance in another domain or class. Extensive experiments\non 11 datasets for base-to-new generalization and 4 datasets for domain\ngeneralization demonstrate that our method consistently and significantly\noutperforms existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2212.04196v2.pdf"
    },
    {
        "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
        "authors": [
            "Chan Hee Song",
            "Jiaman Wu",
            "Clayton Washington",
            "Brian M. Sadler",
            "Wei-Lun Chao",
            "Yu Su"
        ],
        "published": "2022-12-08T05:46:32Z",
        "summary": "This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner",
        "pdf_link": "https://arxiv.org/pdf/2212.04088v3.pdf"
    },
    {
        "title": "Talking About Large Language Models",
        "authors": [
            "Murray Shanahan"
        ],
        "published": "2022-12-07T10:01:44Z",
        "summary": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.",
        "pdf_link": "https://arxiv.org/pdf/2212.03551v5.pdf"
    },
    {
        "title": "A Generative Approach for Script Event Prediction via Contrastive Fine-tuning",
        "authors": [
            "Fangqi Zhu",
            "Jun Gao",
            "Changlong Yu",
            "Wei Wang",
            "Chen Xu",
            "Xin Mu",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2022-12-07T07:32:47Z",
        "summary": "Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.",
        "pdf_link": "https://arxiv.org/pdf/2212.03496v3.pdf"
    },
    {
        "title": "CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain",
        "authors": [
            "Markus Bayer",
            "Philipp Kuehn",
            "Ramin Shanehsaz",
            "Christian Reuter"
        ],
        "published": "2022-12-06T13:49:12Z",
        "summary": "The field of cybersecurity is evolving fast. Experts need to be informed\nabout past, current and - in the best case - upcoming threats, because attacks\nare becoming more advanced, targets bigger and systems more complex. As this\ncannot be addressed manually, cybersecurity experts need to rely on machine\nlearning techniques. In the texutual domain, pre-trained language models like\nBERT have shown to be helpful, by providing a good baseline for further\nfine-tuning. However, due to the domain-knowledge and many technical terms in\ncybersecurity general language models might miss the gist of textual\ninformation, hence doing more harm than good. For this reason, we create a\nhigh-quality dataset and present a language model specifically tailored to the\ncybersecurity domain, which can serve as a basic building block for\ncybersecurity systems that deal with natural language. The model is compared\nwith other models based on 15 different domain-dependent extrinsic and\nintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the\none hand, the results of the intrinsic tasks show that our model improves the\ninternal representation space of words compared to the other models. On the\nother hand, the extrinsic, domain-dependent tasks, consisting of sequence\ntagging and classification, show that the model is best in specific application\nscenarios, in contrast to the others. Furthermore, we show that our approach\nagainst catastrophic forgetting works, as the model is able to retrieve the\npreviously trained domain-independent knowledge. The used dataset and trained\nmodel are made publicly available",
        "pdf_link": "https://arxiv.org/pdf/2212.02974v1.pdf"
    },
    {
        "title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
        "authors": [
            "Hongwei Han",
            "Jialiang Xu",
            "Mengyu Zhou",
            "Yijia Shao",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022-12-06T01:31:37Z",
        "summary": "Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -> 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).",
        "pdf_link": "https://arxiv.org/pdf/2212.02691v2.pdf"
    },
    {
        "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction",
        "authors": [
            "Dietrich Trautmann",
            "Alina Petrova",
            "Frank Schilder"
        ],
        "published": "2022-12-05T12:17:02Z",
        "summary": "Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide and\nassist a large language model (LLM) with performing a natural legal language\nprocessing (NLLP) skill. Our goal is to use LPE with LLMs over long legal\ndocuments for the Legal Judgement Prediction (LJP) task. We investigate the\nperformance of zero-shot LPE for given facts in case-texts from the European\nCourt of Human Rights (in English) and the Federal Supreme Court of Switzerland\n(in German, French and Italian). Our results show that zero-shot LPE is better\ncompared to the baselines, but it still falls short compared to current state\nof the art supervised approaches. Nevertheless, the results are important,\nsince there was 1) no explicit domain-specific data used - so we show that the\ntransfer to the legal domain is possible for general-purpose LLMs, and 2) the\nLLMs where directly applied without any further training or fine-tuning - which\nin turn saves immensely in terms of additional computational costs.",
        "pdf_link": "https://arxiv.org/pdf/2212.02199v1.pdf"
    },
    {
        "title": "Human-in-the-Loop Hate Speech Classification in a Multilingual Context",
        "authors": [
            "Ana Kotarcic",
            "Dominik Hangartner",
            "Fabrizio Gilardi",
            "Selina Kurer",
            "Karsten Donnay"
        ],
        "published": "2022-12-05T09:05:40Z",
        "summary": "The shift of public debate to the digital sphere has been accompanied by a\nrise in online hate speech. While many promising approaches for hate speech\nclassification have been proposed, studies often focus only on a single\nlanguage, usually English, and do not address three key concerns:\npost-deployment performance, classifier maintenance and infrastructural\nlimitations. In this paper, we introduce a new human-in-the-loop BERT-based\nhate speech classification pipeline and trace its development from initial data\ncollection and annotation all the way to post-deployment. Our classifier,\ntrained using data from our original corpus of over 422k examples, is\nspecifically developed for the inherently multilingual setting of Switzerland\nand outperforms with its F1 score of 80.5 the currently best-performing\nBERT-based multilingual classifier by 5.8 F1 points in German and 3.6 F1 points\nin French. Our systematic evaluations over a 12-month period further highlight\nthe vital importance of continuous, human-in-the-loop classifier maintenance to\nensure robust hate speech classification post-deployment.",
        "pdf_link": "https://arxiv.org/pdf/2212.02108v2.pdf"
    },
    {
        "title": "Understanding How Model Size Affects Few-shot Instruction Prompting",
        "authors": [
            "Ayrton San Joaquin",
            "Ardy Haroen"
        ],
        "published": "2022-12-04T19:59:52Z",
        "summary": "Large Language Models are affected by the phenomena of memorizing and\nforgetting their training data. But how do these vary by model size? We work\ntowards this question by investigating how the model size affects the model's\nability to discriminate a word's meaning in a given context. We introduce a\ndataset called DeltaWords, which evaluates a model's ability to follow\ninstructions to select a sentence which replaces the target word with its\nantonym. We show a weak inverse scaling trend, where task accuracy degrades as\nmodel size increase, under extremely few-shot prompting regimes. We show that\nincreasing the number of examples tend to disproportionately benefit larger\nmodels than smaller models.",
        "pdf_link": "https://arxiv.org/pdf/2212.01907v1.pdf"
    },
    {
        "title": "Event knowledge in large language models: the gap between the impossible and the unlikely",
        "authors": [
            "Carina Kauf",
            "Anna A. Ivanova",
            "Giulia Rambelli",
            "Emmanuele Chersoni",
            "Jingyuan Selena She",
            "Zawad Chowdhury",
            "Evelina Fedorenko",
            "Alessandro Lenci"
        ],
        "published": "2022-12-02T23:43:18Z",
        "summary": "Word co-occurrence patterns in language corpora contain a surprising amount\nof conceptual knowledge. Large language models (LLMs), trained to predict words\nin context, leverage these patterns to achieve impressive performance on\ndiverse semantic tasks requiring world knowledge. An important but understudied\nquestion about LLMs' semantic abilities is whether they acquire generalized\nknowledge of common events. Here, we test whether five pre-trained LLMs (from\n2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions\nof agent-patient interactions than to minimally different implausible versions\nof the same event. Using three curated sets of minimal sentence pairs (total\nn=1,215), we found that pre-trained LLMs possess substantial event knowledge,\noutperforming other distributional language models. In particular, they almost\nalways assign higher likelihood to possible vs. impossible events (The teacher\nbought the laptop vs. The laptop bought the teacher). However, LLMs show less\nconsistent preferences for likely vs. unlikely events (The nanny tutored the\nboy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM\nscores are driven by both plausibility and surface-level sentence features,\n(ii) LLM scores generalize well across syntactic variants (active vs. passive\nconstructions) but less well across semantic variants (synonymous sentences),\n(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence\nplausibility serves as an organizing dimension in internal LLM representations.\nOverall, our results show that important aspects of event knowledge naturally\nemerge from distributional linguistic patterns, but also highlight a gap\nbetween representations of possible/impossible and likely/unlikely events.",
        "pdf_link": "https://arxiv.org/pdf/2212.01488v4.pdf"
    },
    {
        "title": "Nonparametric Masked Language Modeling",
        "authors": [
            "Sewon Min",
            "Weijia Shi",
            "Mike Lewis",
            "Xilun Chen",
            "Wen-tau Yih",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022-12-02T18:10:42Z",
        "summary": "Existing language models (LMs) predict tokens with a softmax over a finite\nvocabulary, which can make it difficult to predict rare tokens or phrases. We\nintroduce NPM, the first nonparametric masked language model that replaces this\nsoftmax with a nonparametric distribution over every phrase in a reference\ncorpus. NPM fills in the [MASK] solely from retrieving a token from a text\ncorpus. We show that NPM can be efficiently trained with a contrastive\nobjective and an in-batch approximation to full corpus retrieval. Zero-shot\nevaluation on 16 tasks including classification, fact probing and question\nanswering demonstrates that NPM outperforms significantly larger parametric\nmodels, either with or without a retrieve-and-generate approach. It is\nparticularly better at dealing with rare patterns (word senses or facts) and\npredicting rare or nearly unseen words (e.g., non-Latin script). We release the\nmodel and code at github.com/facebookresearch/NPM.",
        "pdf_link": "https://arxiv.org/pdf/2212.01349v2.pdf"
    },
    {
        "title": "a survey on GPT-3",
        "authors": [
            "Mingyu Zong",
            "Bhaskar Krishnamachari"
        ],
        "published": "2022-12-01T20:24:19Z",
        "summary": "This paper provides an introductory survey to GPT-3. We cover some of the\nhistorical development behind this technology, some of the key features of\nGPT-3, and discuss the machine learning model and the datasets used. We survey\nboth academic and commercial efforts applying GPT-3 in diverse domains such as\ndeveloping conversational AI chatbots, software development, creative work,\ndomain knowledge, and business productivity. We discuss some of the challenges\nthat GPT-3 faces such as the problems of training complexity, bias, and\nhallucination/incorrect answers. We also discuss the future research\nopportunities in this area.",
        "pdf_link": "https://arxiv.org/pdf/2212.00857v1.pdf"
    },
    {
        "title": "Language Model Pre-training on True Negatives",
        "authors": [
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Masao Utiyama",
            "Eiichiro Sumita"
        ],
        "published": "2022-12-01T12:24:19Z",
        "summary": "Discriminative pre-trained language models (PLMs) learn to predict original\ntexts from intentionally corrupted ones. Taking the former text as positive and\nthe latter as negative samples, the PLM can be trained effectively for\ncontextualized representation. However, the training of such a type of PLMs\nhighly relies on the quality of the automatically constructed samples. Existing\nPLMs simply treat all corrupted texts as equal negative without any\nexamination, which actually lets the resulting model inevitably suffer from the\nfalse negative issue where training is carried out on pseudo-negative data and\nleads to less efficiency and less robustness in the resulting PLMs. In this\nwork, on the basis of defining the false negative issue in discriminative PLMs\nthat has been ignored for a long time, we design enhanced pre-training methods\nto counteract false negative predictions and encourage pre-training language\nmodels on true negatives by correcting the harmful gradient updates subject to\nfalse negative predictions. Experimental results on GLUE and SQuAD benchmarks\nshow that our counter-false-negative pre-training methods indeed bring about\nbetter performance together with stronger robustness.",
        "pdf_link": "https://arxiv.org/pdf/2212.00460v1.pdf"
    },
    {
        "title": "Distilling Reasoning Capabilities into Smaller Language Models",
        "authors": [
            "Kumar Shridhar",
            "Alessandro Stolfo",
            "Mrinmaya Sachan"
        ],
        "published": "2022-12-01T00:39:56Z",
        "summary": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM",
        "pdf_link": "https://arxiv.org/pdf/2212.00193v2.pdf"
    },
    {
        "title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?",
        "authors": [
            "Joel Niklaus",
            "Daniele Giofré"
        ],
        "published": "2022-11-30T16:09:20Z",
        "summary": "Pretrained transformer models have achieved state-of-the-art results in many\ntasks and benchmarks recently. Many state-of-the-art Language Models (LMs),\nhowever, do not scale well above the threshold of 512 input tokens. In\nspecialized domains though (such as legal, scientific or biomedical), models\noften need to process very long text (sometimes well above 10000 tokens). Even\nthough many efficient transformers have been proposed (such as Longformer,\nBigBird or FNet), so far, only very few such efficient models are available for\nspecialized domains. Additionally, since the pretraining process is extremely\ncostly in general - but even more so as the sequence length increases - it is\noften only in reach of large research labs. One way of making pretraining\ncheaper is the Replaced Token Detection (RTD) task, by providing more signal\nduring training, since the loss can be computed over all tokens. In this work,\nwe train Longformer models with the efficient RTD task on legal data to\nshowcase that pretraining efficient LMs is possible using much less compute. We\nevaluate the trained models on challenging summarization tasks requiring the\nmodel to summarize long texts to show to what extent the models can achieve\ngood performance on downstream tasks. We find that both the small and base\nmodels outperform their baselines on the in-domain BillSum and out-of-domain\nPubMed tasks in their respective parameter range. We publish our code and\nmodels for research purposes.",
        "pdf_link": "https://arxiv.org/pdf/2211.17135v1.pdf"
    },
    {
        "title": "Quadapter: Adapter for GPT-2 Quantization",
        "authors": [
            "Minseop Park",
            "Jaeseong You",
            "Markus Nagel",
            "Simyung Chang"
        ],
        "published": "2022-11-30T11:20:33Z",
        "summary": "Transformer language models such as GPT-2 are difficult to quantize because\nof outliers in activations leading to a large quantization error. To adapt to\nthe error, one must use quantization-aware training, which entails a\nfine-tuning process based on the dataset and the training pipeline identical to\nthose for the original model. Pretrained language models, however, often do not\ngrant access to their datasets and training pipelines, forcing us to rely on\narbitrary ones for fine-tuning. In that case, it is observed that\nquantization-aware training overfits the model to the fine-tuning data. For\nquantization without overfitting, we introduce a quantization adapter\n(Quadapter), a small set of parameters that are learned to make activations\nquantization-friendly by scaling them channel-wise. It keeps the model\nparameters unchanged. By applying our method to the challenging task of\nquantizing GPT-2, we demonstrate that it effectively prevents the overfitting\nand improves the quantization performance.",
        "pdf_link": "https://arxiv.org/pdf/2211.16912v1.pdf"
    },
    {
        "title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation",
        "authors": [
            "Zhangir Azerbayev",
            "Ansong Ni",
            "Hailey Schoelkopf",
            "Dragomir Radev"
        ],
        "published": "2022-11-30T04:51:26Z",
        "summary": "Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.",
        "pdf_link": "https://arxiv.org/pdf/2211.16740v3.pdf"
    },
    {
        "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
        "authors": [
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2022-11-28T19:03:35Z",
        "summary": "In many task settings, text classification models are likely to encounter\nexamples from novel classes on which they cannot predict correctly. Selective\nprediction, in which models abstain on low-confidence examples, provides a\npossible solution, but existing models are often overly confident on unseen\nclasses. To remedy this overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-step method that generates OOD\nexamples representative of novel classes, then trains to decrease confidence on\nthem. First, we generate OOD examples by prompting a large language model\ntwice: we prompt it to enumerate relevant novel classes, then generate examples\nfrom each novel class matching the task format. Second, we train a classifier\nwith a novel contrastive objective that encourages lower confidence on\ngenerated OOD examples than training examples. When trained with CoNAL,\nclassifiers improve in their ability to detect and abstain on novel class\nexamples over prior methods by an average of 2.3% in terms of accuracy under\nthe accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with\nno cost to in-distribution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2211.15718v2.pdf"
    },
    {
        "title": "Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation",
        "authors": [
            "Sai Shashank Kalakonda",
            "Shubh Maheshwari",
            "Ravi Kiran Sarvadevabhatla"
        ],
        "published": "2022-11-28T17:57:48Z",
        "summary": "We introduce Action-GPT, a plug-and-play framework for incorporating Large\nLanguage Models (LLMs) into text-based action generation models. Action phrases\nin current motion capture datasets contain minimal and to-the-point\ninformation. By carefully crafting prompts for LLMs, we generate richer and\nfine-grained descriptions of the action. We show that utilizing these detailed\ndescriptions instead of the original action phrases leads to better alignment\nof text and motion spaces. We introduce a generic approach compatible with\nstochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion\nmodels. In addition, the approach enables multiple text descriptions to be\nutilized. Our experiments show (i) noticeable qualitative and quantitative\nimprovement in the quality of synthesized motions, (ii) benefits of utilizing\nmultiple LLM-generated descriptions, (iii) suitability of the prompt function,\nand (iv) zero-shot generation capabilities of the proposed approach. Project\npage: https://actiongpt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2211.15603v3.pdf"
    },
    {
        "title": "Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling",
        "authors": [
            "Xinshi Wang",
            "Daniel Tang"
        ],
        "published": "2022-11-28T16:49:13Z",
        "summary": "Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.",
        "pdf_link": "https://arxiv.org/pdf/2211.15544v2.pdf"
    },
    {
        "title": "Scientific and Creative Analogies in Pretrained Language Models",
        "authors": [
            "Tamara Czinczoll",
            "Helen Yannakoudakis",
            "Pushkar Mishra",
            "Ekaterina Shutova"
        ],
        "published": "2022-11-28T12:49:44Z",
        "summary": "This paper examines the encoding of analogy in large-scale pretrained\nlanguage models, such as BERT and GPT-2. Existing analogy datasets typically\nfocus on a limited set of analogical relations, with a high similarity of the\ntwo domains between which the analogy holds. As a more realistic setup, we\nintroduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy\ndataset containing systematic mappings of multiple attributes and relational\nstructures across dissimilar domains. Using this dataset, we test the\nanalogical reasoning capabilities of several widely-used pretrained language\nmodels (LMs). We find that state-of-the-art LMs achieve low performance on\nthese complex analogy tasks, highlighting the challenges still posed by analogy\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2211.15268v1.pdf"
    },
    {
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "authors": [
            "Michiel A. Bakker",
            "Martin J. Chadwick",
            "Hannah R. Sheahan",
            "Michael Henry Tessler",
            "Lucy Campbell-Gillingham",
            "Jan Balaguer",
            "Nat McAleese",
            "Amelia Glaese",
            "John Aslanides",
            "Matthew M. Botvinick",
            "Christopher Summerfield"
        ],
        "published": "2022-11-28T02:24:14Z",
        "summary": "Recent work in large language modeling (LLMs) has used fine-tuning to align\noutputs with the preferences of a prototypical user. This work assumes that\nhuman preferences are static and homogeneous across individuals, so that\naligning to a a single \"generic\" user will confer more general alignment. Here,\nwe embrace the heterogeneity of human preferences to consider a different\nchallenge: how might a machine help people with diverse views find agreement?\nWe fine-tune a 70 billion parameter LLM to generate statements that maximize\nthe expected approval for a group of people with potentially diverse opinions.\nHuman participants provide written opinions on thousands of questions touching\non moral and political issues (e.g., \"should we raise taxes on the rich?\"), and\nrate the LLM's generated candidate consensus statements for agreement and\nquality. A reward model is then trained to predict individual preferences,\nenabling it to quantify and rank consensus statements in terms of their appeal\nto the overall group, defined according to different aggregation (social\nwelfare) functions. The model produces consensus statements that are preferred\nby human users over those from prompted LLMs (>70%) and significantly\noutperforms a tight fine-tuned baseline that lacks the final ranking step.\nFurther, our best model's consensus statements are preferred over the best\nhuman-generated opinions (>65%). We find that when we silently constructed\nconsensus statements from only a subset of group members, those who were\nexcluded were more likely to dissent, revealing the sensitivity of the\nconsensus to individual contributions. These results highlight the potential to\nuse LLMs to help groups of humans align their values with one another.",
        "pdf_link": "https://arxiv.org/pdf/2211.15006v1.pdf"
    },
    {
        "title": "Understanding BLOOM: An empirical study on diverse NLP tasks",
        "authors": [
            "Parag Pravin Dakle",
            "SaiKrishna Rallabandi",
            "Preethi Raghavan"
        ],
        "published": "2022-11-27T15:48:14Z",
        "summary": "We view the landscape of large language models (LLMs) through the lens of the\nrecently released BLOOM model to understand the performance of BLOOM and other\ndecoder-only LLMs compared to BERT-style encoder-only models. We achieve this\nby evaluating the smaller BLOOM model variants (\\textit{350m/560m} and\n\\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards.\nWe make the following observations: (1) BLOOM performance does not scale with\nparameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning\nBLOOM models show that the 560m variant performs similarly to or better than\nthe 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning\nexperiments show that BLOOM is at par or worse than monolingual GPT-2 models,\nand (3) Toxicity analysis of prompt-based text generation using the\nRealToxicityPrompts dataset shows that the text generated by BLOOM is at least\n17\\% less toxic than GPT-2 and GPT-3 models.",
        "pdf_link": "https://arxiv.org/pdf/2211.14865v2.pdf"
    },
    {
        "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
        "authors": [
            "Aristides Milios",
            "Parishad BehnamGhader"
        ],
        "published": "2022-11-25T23:38:08Z",
        "summary": "Although large pre-trained language models have achieved great success in\nmany NLP tasks, it has been shown that they reflect human biases from their\npre-training corpora. This bias may lead to undesirable outcomes when these\nmodels are applied in real-world settings. In this paper, we investigate the\nbias present in monolingual BERT models across a diverse set of languages\n(English, Greek, and Persian). While recent research has mostly focused on\ngender-related biases, we analyze religious and ethnic biases as well and\npropose a template-based method to measure any kind of bias, based on sentence\npseudo-likelihood, that can handle morphologically complex languages with\ngender-based adjective declensions. We analyze each monolingual model via this\nmethod and visualize cultural similarities and differences across different\ndimensions of bias. Ultimately, we conclude that current methods of probing for\nbias are highly language-dependent, necessitating cultural insights regarding\nthe unique ways bias is expressed in each language and culture (e.g. through\ncoded language, synecdoche, and other similar linguistic concepts). We also\nhypothesize that higher measured social biases in the non-English BERT models\ncorrelate with user-generated content in their training.",
        "pdf_link": "https://arxiv.org/pdf/2211.14402v1.pdf"
    },
    {
        "title": "GPT-3-driven pedagogical agents for training children's curious question-asking skills",
        "authors": [
            "Rania Abdelghani",
            "Yen-Hsiang Wang",
            "Xingdi Yuan",
            "Tong Wang",
            "Pauline Lucas",
            "Hélène Sauzéon",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2022-11-25T16:41:59Z",
        "summary": "In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.",
        "pdf_link": "https://arxiv.org/pdf/2211.14228v6.pdf"
    },
    {
        "title": "PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices",
        "authors": [
            "Kazuki Osawa",
            "Shigang Li",
            "Torsten Hoefler"
        ],
        "published": "2022-11-25T14:16:35Z",
        "summary": "Pipeline parallelism enables efficient training of Large Language Models\n(LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles\nduring startup and tear-down reduce the utilization of accelerators. Although\nefficient pipeline schemes with micro-batching and bidirectional pipelines have\nbeen proposed to maximize utilization, a significant number of bubbles cannot\nbe filled using synchronous forward and backward passes. To address this\nproblem, we suggest that extra work be assigned to the bubbles to gain\nauxiliary benefits in LLM training. As an example in this direction, we propose\nPipeFisher, which assigns the work of K-FAC, a second-order optimization method\nbased on the Fisher information matrix, to the bubbles to accelerate\nconvergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher\nreduces the (simulated) training time to 50-75% compared to training with a\nfirst-order optimizer by greatly improving the accelerator utilization and\nbenefiting from the improved convergence by K-FAC.",
        "pdf_link": "https://arxiv.org/pdf/2211.14133v2.pdf"
    },
    {
        "title": "Complementary Explanations for Effective In-Context Learning",
        "authors": [
            "Xi Ye",
            "Srinivasan Iyer",
            "Asli Celikyilmaz",
            "Ves Stoyanov",
            "Greg Durrett",
            "Ramakanth Pasunuru"
        ],
        "published": "2022-11-25T04:40:47Z",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts, but there has been limited understanding\nof exactly how these explanations function or why they are effective. This work\naims to better understand the mechanisms by which explanations are used for\nin-context learning. We first study the impact of two different factors on the\nperformance of prompts with explanations: the computation trace (the way the\nsolution is decomposed) and the natural language used to express the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set:\ndiverse reasoning skills shown by different exemplars can lead to better\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\nselection approach for constructing exemplar sets that are both relevant as\nwell as complementary, which successfully improves the in-context learning\nperformance across three real-world tasks on multiple LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2211.13892v2.pdf"
    },
    {
        "title": "SEAT: Stable and Explainable Attention",
        "authors": [
            "Lijie Hu",
            "Yixin Liu",
            "Ninghao Liu",
            "Mengdi Huai",
            "Lichao Sun",
            "Di Wang"
        ],
        "published": "2022-11-23T20:33:30Z",
        "summary": "Currently, attention mechanism becomes a standard fixture in most\nstate-of-the-art natural language processing (NLP) models, not only due to\noutstanding performance it could gain, but also due to plausible innate\nexplanation for the behaviors of neural architectures it provides, which is\nnotoriously difficult to analyze. However, recent studies show that attention\nis unstable against randomness and perturbations during training or testing,\nsuch as random seeds and slight perturbation of embedding vectors, which\nimpedes it from becoming a faithful explanation tool. Thus, a natural question\nis whether we can find some substitute of the current attention which is more\nstable and could keep the most important characteristics on explanation and\nprediction of attention. In this paper, to resolve the problem, we provide a\nfirst rigorous definition of such alternate namely SEAT (Stable and Explainable\nAttention). Specifically, a SEAT should has the following three properties: (1)\nIts prediction distribution is enforced to be close to the distribution based\non the vanilla attention; (2) Its top-k indices have large overlaps with those\nof the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any\nslight perturbation on SEAT will not change the prediction distribution too\nmuch, which implicitly indicates that it is stable to randomness and\nperturbations. Finally, through intensive experiments on various datasets, we\ncompare our SEAT with other baseline methods using RNN, BiLSTM and BERT\narchitectures via six different evaluation metrics for model interpretation,\nstability and accuracy. Results show that SEAT is more stable against different\nperturbations and randomness while also keeps the explainability of attention,\nwhich indicates it is a more faithful explanation. Moreover, compared with\nvanilla attention, there is almost no utility (accuracy) degradation for SEAT.",
        "pdf_link": "https://arxiv.org/pdf/2211.13290v1.pdf"
    },
    {
        "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
        "authors": [
            "Kumar Shridhar",
            "Jakub Macina",
            "Mennatallah El-Assady",
            "Tanmay Sinha",
            "Manu Kapur",
            "Mrinmaya Sachan"
        ],
        "published": "2022-11-23T10:40:22Z",
        "summary": "Socratic questioning is an educational method that allows students to\ndiscover answers to complex problems by asking them a series of thoughtful\nquestions. Generation of didactically sound questions is challenging, requiring\nunderstanding of the reasoning process involved in the problem. We hypothesize\nthat such questioning strategy can not only enhance the human performance, but\nalso assist the math word problem (MWP) solvers. In this work, we explore the\nability of large language models (LMs) in generating sequential questions for\nguiding math word problem-solving. We propose various guided question\ngeneration schemes based on input conditioning and reinforcement learning. On\nboth automatic and human quality evaluations, we find that LMs constrained with\ndesirable question properties generate superior questions and improve the\noverall performance of a math word problem solver. We conduct a preliminary\nuser study to examine the potential value of such question generation models in\nthe education domain. Results suggest that the difficulty level of problems\nplays an important role in determining whether questioning improves or hinders\nhuman performance. We discuss the future of using such questioning strategies\nin education.",
        "pdf_link": "https://arxiv.org/pdf/2211.12835v1.pdf"
    },
    {
        "title": "OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type",
        "authors": [
            "Suresh Kumar",
            "P Sreenivasa Kumar"
        ],
        "published": "2022-11-22T10:42:07Z",
        "summary": "Machine generation of Arithmetic Word Problems (AWPs) is challenging as they\nexpress quantities and mathematical relationships and need to be consistent.\nML-solvers require a large annotated training set of consistent problems with\nlanguage variations. Exploiting domain-knowledge is needed for consistency\nchecking whereas LSTM-based approaches are good for producing text with\nlanguage variations. Combining these we propose a system, OLGA, to generate\nconsistent word problems of TC (Transfer-Case) type, involving object transfers\namong agents. Though we provide a dataset of consistent 2-agent TC-problems for\ntraining, only about 36% of the outputs of an LSTM-based generator are found\nconsistent. We use an extension of TC-Ontology, proposed by us previously, to\ndetermine the consistency of problems. Among the remaining 64%, about 40% have\nminor errors which we repair using the same ontology. To check consistency and\nfor the repair process, we construct an instance-specific representation (ABox)\nof an auto-generated problem. We use a sentence classifier and BERT models for\nthis task. The training set for these LMs is problem-texts where sentence-parts\nare annotated with ontology class-names. As three-agent problems are longer,\nthe percentage of consistent problems generated by an LSTM-based approach drops\nfurther. Hence, we propose an ontology-based method that extends consistent\n2-agent problems into consistent 3-agent problems. Overall, our approach\ngenerates a large number of consistent TC-type AWPs involving 2 or 3 agents. As\nABox has all the information of a problem, any annotations can also be\ngenerated. Adopting the proposed approach to generate other types of AWPs is\ninteresting future work.",
        "pdf_link": "https://arxiv.org/pdf/2211.12164v1.pdf"
    },
    {
        "title": "Validating Large Language Models with ReLM",
        "authors": [
            "Michael Kuchnik",
            "Virginia Smith",
            "George Amvrosiadis"
        ],
        "published": "2022-11-21T21:40:35Z",
        "summary": "Although large language models (LLMs) have been touted for their ability to\ngenerate natural-sounding text, there are growing concerns around possible\nnegative effects of LLMs such as data memorization, bias, and inappropriate\nlanguage. Unfortunately, the complexity and generation capacities of LLMs make\nvalidating (and correcting) such concerns difficult. In this work, we introduce\nReLM, a system for validating and querying LLMs using standard regular\nexpressions. ReLM formalizes and enables a broad range of language model\nevaluations, reducing complex evaluation rules to simple regular expression\nqueries. Our results exploring queries surrounding memorization, gender bias,\ntoxicity, and language understanding show that ReLM achieves up to 15x higher\nsystem efficiency, 2.5x data efficiency, and increased statistical and\nprompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers\na competitive and general baseline for the increasingly important problem of\nLLM validation.",
        "pdf_link": "https://arxiv.org/pdf/2211.15458v2.pdf"
    },
    {
        "title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?",
        "authors": [
            "Matthew Shardlow",
            "Piotr Przybyła"
        ],
        "published": "2022-11-21T14:18:25Z",
        "summary": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.",
        "pdf_link": "https://arxiv.org/pdf/2211.11483v4.pdf"
    },
    {
        "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "authors": [
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Haonan Yin",
            "Börje F. Karlsson",
            "Chin-Yew Lin"
        ],
        "published": "2022-11-21T09:41:25Z",
        "summary": "Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "pdf_link": "https://arxiv.org/pdf/2211.11300v3.pdf"
    },
    {
        "title": "Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task",
        "authors": [
            "Shangda Wu",
            "Maosong Sun"
        ],
        "published": "2022-11-21T07:19:17Z",
        "summary": "Benefiting from large-scale datasets and pre-trained models, the field of\ngenerative models has recently gained significant momentum. However, most\ndatasets for symbolic music are very small, which potentially limits the\nperformance of data-driven multimodal models. An intuitive solution to this\nproblem is to leverage pre-trained models from other modalities (e.g., natural\nlanguage) to improve the performance of symbolic music-related multimodal\ntasks. In this paper, we carry out the first study of generating complete and\nsemantically consistent symbolic music scores from text descriptions, and\nexplore the efficacy of using publicly available checkpoints (i.e., BERT,\nGPT-2, and BART) for natural language processing in the task of text-to-music\ngeneration. Our experimental results show that the improvement from using\npre-trained checkpoints is statistically significant in terms of BLEU score and\nedit distance similarity. We analyse the capabilities and limitations of our\nmodel to better understand the potential of language-music models.",
        "pdf_link": "https://arxiv.org/pdf/2211.11216v2.pdf"
    },
    {
        "title": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi",
        "authors": [
            "Ananya Joshi",
            "Aditi Kajale",
            "Janhavi Gadre",
            "Samruddhi Deode",
            "Raviraj Joshi"
        ],
        "published": "2022-11-21T05:15:48Z",
        "summary": "Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.",
        "pdf_link": "https://arxiv.org/pdf/2211.11187v2.pdf"
    },
    {
        "title": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model",
        "authors": [
            "Shengkun Tang",
            "Yaqing Wang",
            "Zhenglun Kong",
            "Tianchi Zhang",
            "Yao Li",
            "Caiwen Ding",
            "Yanzhi Wang",
            "Yi Liang",
            "Dongkuan Xu"
        ],
        "published": "2022-11-21T02:32:25Z",
        "summary": "Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2211.11152v2.pdf"
    },
    {
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": [
            "Li S. Yifei",
            "Lyle Ungar",
            "João Sedoc"
        ],
        "published": "2022-11-20T21:24:48Z",
        "summary": "Pre-trained large language models (LLMs) reflect the inherent social biases\nof their training corpus. Many methods have been proposed to mitigate this\nissue, but they often fail to debias or they sacrifice model accuracy. We use\nconceptors--a soft projection method--to identify and remove the bias subspace\nin LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)\nbias subspace projection by post-processing by the conceptor NOT operation; and\n(2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly\nincorporates the conceptor projection into all layers during training. We find\nthat conceptor post-processing achieves state-of-the-art (SoTA) debiasing\nresults while maintaining LLMs' performance on the GLUE benchmark. Further, it\nis robust in various scenarios and can mitigate intersectional bias efficiently\nby its AND operation on the existing bias subspaces. Although CI-BERT's\ntraining takes all layers' bias into account and can beat its post-processing\ncounterpart in bias mitigation, CI-BERT reduces the language model accuracy. We\nalso show the importance of carefully constructing the bias subspace. The best\nresults are obtained by removing outliers from the list of biased words,\ncombining them (via the OR operation), and computing their embeddings using the\nsentences from a cleaner corpus.",
        "pdf_link": "https://arxiv.org/pdf/2211.11087v3.pdf"
    },
    {
        "title": "The Stack: 3 TB of permissively licensed source code",
        "authors": [
            "Denis Kocetkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Jia Li",
            "Chenghao Mou",
            "Carlos Muñoz Ferrandis",
            "Yacine Jernite",
            "Margaret Mitchell",
            "Sean Hughes",
            "Thomas Wolf",
            "Dzmitry Bahdanau",
            "Leandro von Werra",
            "Harm de Vries"
        ],
        "published": "2022-11-20T18:15:30Z",
        "summary": "Large Language Models (LLMs) play an ever-increasing role in the field of\nArtificial Intelligence (AI)--not only for natural language processing but also\nfor code understanding and generation. To stimulate open and responsible\nresearch on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting\nof permissively licensed source code in 30 programming languages. We describe\nhow we collect the full dataset, construct a permissively licensed subset,\npresent a data governance plan, discuss limitations, and show promising results\non text2code benchmarks by training 350M-parameter decoders on different Python\nsubsets. We find that (1) near-deduplicating the data significantly boosts\nperformance across all experiments, and (2) it is possible to match previously\nreported HumanEval and MBPP performance using only permissively licensed data.\nWe make the dataset available at https://hf.co/BigCode, provide a tool called\n\"Am I in The Stack\" (https://hf.co/spaces/bigcode/in-the-stack) for developers\nto search The Stack for copies of their code, and provide a process for code to\nbe removed from the dataset by following the instructions at\nhttps://www.bigcode-project.org/docs/about/the-stack/.",
        "pdf_link": "https://arxiv.org/pdf/2211.15533v1.pdf"
    },
    {
        "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-training",
        "authors": [
            "Yunhao Gou",
            "Tom Ko",
            "Hansi Yang",
            "James Kwok",
            "Yu Zhang",
            "Mingxuan Wang"
        ],
        "published": "2022-11-20T12:10:53Z",
        "summary": "Most existing vision-language pre-training (VLP) approaches adopt cross-modal\nmasked language modeling (CMLM) to learn vision-language associations. However,\nwe find that CMLM is insufficient for this purpose according to our\nobservations: (1) Modality bias: a considerable amount of masked tokens in CMLM\ncan be recovered with only the language information, ignoring the visual\ninputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on\nthe masked token but it cannot simultaneously leverage other tokens to learn\nvision-language associations. To handle those limitations, we propose EPIC\n(lEveraging Per Image-Token Consistency for vision-language pre-training). In\nEPIC, for each image-sentence pair, we mask tokens that are salient to the\nimage (i.e., Saliency-based Masking Strategy) and replace them with\nalternatives sampled from a language model (i.e., Inconsistent Token Generation\nProcedure), and then the model is required to determine for each token in the\nsentence whether it is consistent with the image (i.e., Image-Token Consistency\nTask). The proposed EPIC method is easily combined with pre-training methods.\nExtensive experiments show that the combination of the EPIC method and\nstate-of-the-art pre-training approaches, including ViLT, ALBEF, METER, and\nX-VLM, leads to significant improvements on downstream tasks. The code is\nreleased at https://github.com/gyhdog99/epic.",
        "pdf_link": "https://arxiv.org/pdf/2211.15398v2.pdf"
    },
    {
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
        "authors": [
            "Guangxuan Xiao",
            "Ji Lin",
            "Mickael Seznec",
            "Hao Wu",
            "Julien Demouth",
            "Song Han"
        ],
        "published": "2022-11-18T18:59:33Z",
        "summary": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.",
        "pdf_link": "https://arxiv.org/pdf/2211.10438v7.pdf"
    },
    {
        "title": "PAL: Program-aided Language Models",
        "authors": [
            "Luyu Gao",
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Pengfei Liu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2022-11-18T18:56:13Z",
        "summary": "Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .",
        "pdf_link": "https://arxiv.org/pdf/2211.10435v2.pdf"
    },
    {
        "title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
        "authors": [
            "Shreyas Sundara Raman",
            "Vanya Cohen",
            "Ifrah Idrees",
            "Eric Rosen",
            "Ray Mooney",
            "Stefanie Tellex",
            "David Paulius"
        ],
        "published": "2022-11-17T23:14:51Z",
        "summary": "Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.",
        "pdf_link": "https://arxiv.org/pdf/2211.09935v3.pdf"
    },
    {
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
        "authors": [
            "Fábio Perez",
            "Ian Ribeiro"
        ],
        "published": "2022-11-17T13:43:20Z",
        "summary": "Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.",
        "pdf_link": "https://arxiv.org/pdf/2211.09527v1.pdf"
    },
    {
        "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
        "authors": [
            "David Vilar",
            "Markus Freitag",
            "Colin Cherry",
            "Jiaming Luo",
            "Viresh Ratnakar",
            "George Foster"
        ],
        "published": "2022-11-16T18:42:37Z",
        "summary": "Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.",
        "pdf_link": "https://arxiv.org/pdf/2211.09102v3.pdf"
    },
    {
        "title": "Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT",
        "authors": [
            "Siyuan Lu",
            "Chenchen Zhou",
            "Keli Xie",
            "Jun Lin",
            "Zhongfeng Wang"
        ],
        "published": "2022-11-16T11:43:09Z",
        "summary": "With the development of deep learning and Transformer-based pre-trained\nmodels like BERT, the accuracy of many NLP tasks has been dramatically\nimproved. However, the large number of parameters and computations also pose\nchallenges for their deployment. For instance, using BERT can improve the\npredictions in the financial sentiment analysis (FSA) task but slow it down,\nwhere speed and accuracy are equally important in terms of profits. To address\nthese issues, we first propose an efficient and lightweight BERT (ELBERT) along\nwith a novel confidence-window-based (CWB) early exit mechanism. Based on\nELBERT, an innovative method to accelerate text processing on the GPU platform\nis developed, solving the difficult problem of making the early exit mechanism\nwork more effectively with a large input batch size. Afterward, a fast and\nhigh-accuracy FSA system is built. Experimental results show that the proposed\nCWB early exit mechanism achieves significantly higher accuracy than existing\nearly exit methods on BERT under the same computation cost. By using this\nacceleration method, our FSA system can boost the processing speed by nearly 40\ntimes to over 1000 texts per second with sufficient accuracy, which is nearly\ntwice as fast as FastBERT, thus providing a more powerful text processing\ncapability for modern trading systems.",
        "pdf_link": "https://arxiv.org/pdf/2211.08842v2.pdf"
    },
    {
        "title": "Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales",
        "authors": [
            "Saurabh Kulshreshtha",
            "Anna Rumshisky"
        ],
        "published": "2022-11-15T19:36:06Z",
        "summary": "Multi-hop Question Generation is the task of generating questions which\nrequire the reader to reason over and combine information spread across\nmultiple passages using several reasoning steps. Chain-of-thought rationale\ngeneration has been shown to improve performance on multi-step reasoning tasks\nand make model predictions more interpretable. However, few-shot performance\ngains from including rationales have been largely observed only in +100B\nlanguage models, and otherwise require large scale manual rationale annotation.\nIn this work, we introduce a new framework for applying chain-of-thought\ninspired structured rationale generation to multi-hop question generation under\na very low supervision regime (8- to 128-shot). We propose to annotate a small\nnumber of examples following our proposed multi-step rationale schema, treating\neach reasoning step as a separate task to be performed by a generative language\nmodel. We show that our framework leads to improved control over the difficulty\nof the generated questions and better performance compared to baselines trained\nwithout rationales, both on automatic evaluation metrics and in human\nevaluation. Importantly, we show that this is achievable with a modest model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2211.08466v1.pdf"
    },
    {
        "title": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
        "authors": [
            "Yushi Hu",
            "Hang Hua",
            "Zhengyuan Yang",
            "Weijia Shi",
            "Noah A Smith",
            "Jiebo Luo"
        ],
        "published": "2022-11-15T19:07:53Z",
        "summary": "Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.",
        "pdf_link": "https://arxiv.org/pdf/2211.09699v4.pdf"
    },
    {
        "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
        "authors": [
            "Derek Tam",
            "Anisha Mascarenhas",
            "Shiyue Zhang",
            "Sarah Kwan",
            "Mohit Bansal",
            "Colin Raffel"
        ],
        "published": "2022-11-15T18:50:34Z",
        "summary": "While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.",
        "pdf_link": "https://arxiv.org/pdf/2211.08412v2.pdf"
    },
    {
        "title": "Large Language Models Struggle to Learn Long-Tail Knowledge",
        "authors": [
            "Nikhil Kandpal",
            "Haikang Deng",
            "Adam Roberts",
            "Eric Wallace",
            "Colin Raffel"
        ],
        "published": "2022-11-15T18:49:27Z",
        "summary": "The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, while certain pieces of information are ubiquitous\non the web, others appear extremely rarely. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in pre-training datasets scraped from the web. In particular, we\nshow that a language model's ability to answer a fact-based question relates to\nhow many documents associated with that question were seen during pre-training.\nWe identify these relevant documents by entity linking pre-training datasets\nand counting documents that contain the same entities as a given\nquestion-answer pair. Our results demonstrate strong correlational and causal\nrelationships between accuracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,\nROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models\nare better at learning long-tail knowledge, we estimate that today's models\nmust be scaled by many orders of magnitude to reach competitive QA performance\non questions with little support in the pre-training data. Finally, we show\nthat retrieval-augmentation can reduce the dependence on relevant pre-training\ninformation, presenting a promising approach for capturing the long-tail.",
        "pdf_link": "https://arxiv.org/pdf/2211.08411v2.pdf"
    },
    {
        "title": "Introducing Semantics into Speech Encoders",
        "authors": [
            "Derek Xu",
            "Shuyan Dong",
            "Changhan Wang",
            "Suyoun Kim",
            "Zhaojiang Lin",
            "Akshat Shrivastava",
            "Shang-Wen Li",
            "Liang-Hsuan Tseng",
            "Alexei Baevski",
            "Guan-Ting Lin",
            "Hung-yi Lee",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2022-11-15T18:44:28Z",
        "summary": "Recent studies find existing self-supervised speech encoders contain\nprimarily acoustic rather than semantic information. As a result, pipelined\nsupervised automatic speech recognition (ASR) to large language model (LLM)\nsystems achieve state-of-the-art results on semantic spoken language tasks by\nutilizing rich semantic representations from the LLM. These systems come at the\ncost of labeled audio transcriptions, which is expensive and time-consuming to\nobtain. We propose a task-agnostic unsupervised way of incorporating semantic\ninformation from LLMs into self-supervised speech encoders without labeled\naudio transcriptions. By introducing semantics, we improve existing speech\nencoder spoken language understanding performance by over 10\\% on intent\nclassification, with modest gains in named entity resolution and slot filling,\nand spoken question answering FF1 score by over 2\\%. Our unsupervised approach\nachieves similar performance as supervised methods trained on over 100 hours of\nlabeled audio transcripts, demonstrating the feasibility of unsupervised\nsemantic augmentations to existing speech encoders.",
        "pdf_link": "https://arxiv.org/pdf/2211.08402v1.pdf"
    },
    {
        "title": "FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery",
        "authors": [
            "Changlong Yu",
            "Weiqi Wang",
            "Xin Liu",
            "Jiaxin Bai",
            "Yangqiu Song",
            "Zheng Li",
            "Yifan Gao",
            "Tianyu Cao",
            "Bing Yin"
        ],
        "published": "2022-11-15T17:20:40Z",
        "summary": "Understanding users' intentions in e-commerce platforms requires commonsense\nknowledge. In this paper, we present FolkScope, an intention knowledge graph\nconstruction framework to reveal the structure of humans' minds about\npurchasing items. As commonsense knowledge is usually ineffable and not\nexpressed explicitly, it is challenging to perform information extraction.\nThus, we propose a new approach that leverages the generation power of large\nlanguage models~(LLMs) and human-in-the-loop annotation to semi-automatically\nconstruct the knowledge graph. LLMs first generate intention assertions via\ne-commerce-specific prompts to explain shopping behaviors, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility\nand typicality labels of sampled intentions as training data in order to\npopulate human judgments to all automatic generations. Last, to structurize the\nassertions, we propose pattern mining and conceptualization to form more\ncondensed and abstract knowledge. Extensive evaluations and studies demonstrate\nthat our constructed knowledge graph can well model e-commerce knowledge and\nhave many potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2211.08316v2.pdf"
    },
    {
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective",
        "authors": [
            "Linyi Yang",
            "Shuibai Zhang",
            "Libo Qin",
            "Yafu Li",
            "Yidong Wang",
            "Hanmeng Liu",
            "Jindong Wang",
            "Xing Xie",
            "Yue Zhang"
        ],
        "published": "2022-11-15T11:53:55Z",
        "summary": "Pre-trained language models (PLMs) are known to improve the generalization\nperformance of natural language understanding models by leveraging large\namounts of data during the pre-training phase. However, the out-of-distribution\n(OOD) generalization problem remains a challenge in many NLP tasks, limiting\nthe real-world deployment of these methods. This paper presents the first\nattempt at creating a unified benchmark named GLUE-X for evaluating OOD\nrobustness in NLP models, highlighting the importance of OOD robustness and\nproviding insights on how to measure the robustness of a model and how to\nimprove it. The benchmark includes 13 publicly available datasets for OOD\ntesting, and evaluations are conducted on 8 classic NLP tasks over 21 popularly\nused PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for\nimproved OOD accuracy in NLP tasks, as significant performance degradation was\nobserved in all settings compared to in-distribution (ID) accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2211.08073v4.pdf"
    },
    {
        "title": "Teaching Algorithmic Reasoning via In-context Learning",
        "authors": [
            "Hattie Zhou",
            "Azade Nova",
            "Hugo Larochelle",
            "Aaron Courville",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ],
        "published": "2022-11-15T06:12:28Z",
        "summary": "Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2211.09066v1.pdf"
    },
    {
        "title": "UGIF: UI Grounded Instruction Following",
        "authors": [
            "Sagar Gubbi Venkatesh",
            "Partha Talukdar",
            "Srini Narayanan"
        ],
        "published": "2022-11-14T18:36:19Z",
        "summary": "Smartphone users often find it difficult to navigate myriad menus to perform\ncommon tasks such as \"How to block calls from unknown numbers?\". Currently,\nhelp documents with step-by-step instructions are manually written to aid the\nuser. The user experience can be further enhanced by grounding the instructions\nin the help document to the UI and overlaying a tutorial on the phone UI. To\nbuild such tutorials, several natural language processing components including\nretrieval, parsing, and grounding are necessary, but there isn't any relevant\ndataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual,\nmulti-modal UI grounded dataset for step-by-step task completion on the\nsmartphone containing 4,184 tasks across 8 languages. As an initial approach to\nthis problem, we propose retrieving the relevant instruction steps based on the\nuser's query and parsing the steps using Large Language Models (LLMs) to\ngenerate macros that can be executed on-device. The instruction steps are often\navailable only in English, so the challenge includes cross-modal, cross-lingual\nretrieval of English how-to pages from user queries in many languages and\nmapping English instruction steps to UI in a potentially different language. We\ncompare the performance of different LLMs including PaLM and GPT-3 and find\nthat the end-to-end task completion rate is 48% for English UI but the\nperformance drops to 32% for other languages. We analyze the common failure\nmodes of existing models on this task and point out areas for improvement.",
        "pdf_link": "https://arxiv.org/pdf/2211.07615v2.pdf"
    },
    {
        "title": "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations",
        "authors": [
            "Swarnadeep Saha",
            "Peter Hase",
            "Nazneen Rajani",
            "Mohit Bansal"
        ],
        "published": "2022-11-14T16:46:14Z",
        "summary": "Recent work on explainable NLP has shown that few-shot prompting can enable\nlarge pretrained language models (LLMs) to generate grammatical and factual\nnatural language explanations for data labels. In this work, we study the\nconnection between explainability and sample hardness by investigating the\nfollowing research question - \"Are LLMs and humans equally good at explaining\ndata labels for both easy and hard samples?\" We answer this question by first\ncollecting human-written explanations in the form of generalizable commonsense\nrules on the task of Winograd Schema Challenge (Winogrande dataset). We compare\nthese explanations with those generated by GPT-3 while varying the hardness of\nthe test samples as well as the in-context samples. We observe that (1) GPT-3\nexplanations are as grammatical as human explanations regardless of the\nhardness of the test samples, (2) for easy examples, GPT-3 generates highly\nsupportive explanations but human explanations are more generalizable, and (3)\nfor hard examples, human explanations are significantly better than GPT-3\nexplanations both in terms of label-supportiveness and generalizability\njudgements. We also find that hardness of the in-context examples impacts the\nquality of GPT-3 explanations. Finally, we show that the supportiveness and\ngeneralizability aspects of human explanations are also impacted by sample\nhardness, although by a much smaller margin than models. Supporting code and\ndata are available at https://github.com/swarnaHub/ExplanationHardness",
        "pdf_link": "https://arxiv.org/pdf/2211.07517v1.pdf"
    },
    {
        "title": "Does Debiasing Inevitably Degrade the Model Performance",
        "authors": [
            "Yiran Liu",
            "Xiao Liu",
            "Haotian Chen",
            "Yang Yu"
        ],
        "published": "2022-11-14T13:46:13Z",
        "summary": "Gender bias in language models has attracted sufficient attention because it\nthreatens social justice. However, most of the current debiasing methods\ndegraded the model's performance on other tasks while the degradation mechanism\nis still mysterious. We propose a theoretical framework explaining the three\ncandidate mechanisms of the language model's gender bias. We use our\ntheoretical framework to explain why the current debiasing methods cause\nperformance degradation. We also discover a pathway through which debiasing\nwill not degrade the model performance. We further develop a\ncausality-detection fine-tuning approach to correct gender bias. The numerical\nexperiment demonstrates that our method is able to lead to double dividends:\npartially mitigating gender bias while avoiding performance degradation.",
        "pdf_link": "https://arxiv.org/pdf/2211.07350v2.pdf"
    },
    {
        "title": "Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction",
        "authors": [
            "Grace Yang",
            "Ming Cao",
            "Lavender Y. Jiang",
            "Xujin C. Liu",
            "Alexander T. M. Cheung",
            "Hannah Weiss",
            "David Kurland",
            "Kyunghyun Cho",
            "Eric K. Oermann"
        ],
        "published": "2022-11-13T23:59:11Z",
        "summary": "Traditional evaluation metrics for classification in natural language\nprocessing such as accuracy and area under the curve fail to differentiate\nbetween models with different predictive behaviors despite their similar\nperformance metrics. We introduce sensitivity score, a metric that scrutinizes\nmodels' behaviors at the vocabulary level to provide insights into disparities\nin their decision-making logic. We assess the sensitivity score on a set of\nrepresentative words in the test set using two classifiers trained for hospital\nreadmission classification with similar performance statistics. Our experiments\ncompare the decision-making logic of clinicians and classifiers based on rank\ncorrelations of sensitivity scores. The results indicate that the language\nmodel's sensitivity score aligns better with the professionals than the xgboost\nclassifier on tf-idf embeddings, which suggests that xgboost uses some spurious\nfeatures. Overall, this metric offers a novel perspective on assessing models'\nrobustness by quantifying their discrepancy with professional opinions. Our\ncode is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).",
        "pdf_link": "https://arxiv.org/pdf/2211.07047v2.pdf"
    },
    {
        "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost",
        "authors": [
            "Qingcheng Zeng",
            "Lucas Garay",
            "Peilin Zhou",
            "Dading Chong",
            "Yining Hua",
            "Jiageng Wu",
            "Yikang Pan",
            "Han Zhou",
            "Rob Voigt",
            "Jie Yang"
        ],
        "published": "2022-11-13T18:59:15Z",
        "summary": "Large pre-trained models have revolutionized natural language processing\n(NLP) research and applications, but high training costs and limited data\nresources have prevented their benefits from being shared equally amongst\nspeakers of all the world's languages. To address issues of cross-linguistic\naccess to such models and reduce energy consumption for sustainability during\nlarge-scale model training, this study proposes an effective and\nenergy-efficient framework called GreenPLM that uses bilingual lexicons to\ndirectly \"translate\" pre-trained language models of one language into another\nat almost no additional cost. We validate this approach in 18 languages' BERT\nmodels and show that this framework is comparable to, if not better than, other\nheuristics with high training costs. In addition, given lightweight continued\npre-training on limited data where available, this framework outperforms the\noriginal monolingual language models in six out of seven tested languages with\nup to 200x less pre-training efforts. Aiming at the Leave No One Behind\nPrinciple (LNOB), our approach manages to reduce inequalities between languages\nand energy consumption greatly. We make our codes and models publicly available\nhere: \\url{https://github.com/qcznlp/GreenPLMs}",
        "pdf_link": "https://arxiv.org/pdf/2211.06993v3.pdf"
    },
    {
        "title": "Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT RoBERTa Approach for Patronizing and Condescending Language Detection",
        "authors": [
            "Jinghua Xu"
        ],
        "published": "2022-11-13T10:59:45Z",
        "summary": "This paper describes my participation in the SemEval-2022 Task 4: Patronizing\nand Condescending Language Detection. I participate in both subtasks:\nPatronizing and Condescending Language (PCL) Identification and Patronizing and\nCondescending Language Categorization, with the main focus put on subtask 1.\nThe experiments compare pre-BERT neural network (NN) based systems against\npost-BERT pretrained language model RoBERTa. This research finds NN-based\nsystems in the experiments perform worse on the task compared to the pretrained\nlanguage models. The top-performing RoBERTa system is ranked 26 out of 78 teams\n(F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in\nsubtask 2.",
        "pdf_link": "https://arxiv.org/pdf/2211.06874v1.pdf"
    },
    {
        "title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters",
        "authors": [
            "Nuo Chen",
            "Yan Wang",
            "Haiyun Jiang",
            "Deng Cai",
            "Yuhan Li",
            "Ziyang Chen",
            "Longyue Wang",
            "Jia Li"
        ],
        "published": "2022-11-13T10:16:39Z",
        "summary": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.",
        "pdf_link": "https://arxiv.org/pdf/2211.06869v4.pdf"
    },
    {
        "title": "Textual Data Augmentation for Patient Outcomes Prediction",
        "authors": [
            "Qiuhao Lu",
            "Dejing Dou",
            "Thien Huu Nguyen"
        ],
        "published": "2022-11-13T01:07:23Z",
        "summary": "Deep learning models have demonstrated superior performance in various\nhealthcare applications. However, the major limitation of these deep models is\nusually the lack of high-quality training data due to the private and sensitive\nnature of this field. In this study, we propose a novel textual data\naugmentation method to generate artificial clinical notes in patients'\nElectronic Health Records (EHRs) that can be used as additional training data\nfor patient outcomes prediction. Essentially, we fine-tune the generative\nlanguage model GPT-2 to synthesize labeled text with the original training\ndata. More specifically, We propose a teacher-student framework where we first\npre-train a teacher model on the original data, and then train a student model\non the GPT-augmented data under the guidance of the teacher. We evaluate our\nmethod on the most common patient outcome, i.e., the 30-day readmission rate.\nThe experimental results show that deep models can improve their predictive\nperformance with the augmented data, indicating the effectiveness of the\nproposed architecture.",
        "pdf_link": "https://arxiv.org/pdf/2211.06778v1.pdf"
    },
    {
        "title": "DocuT5: Seq2seq SQL Generation with Table Documentation",
        "authors": [
            "Elena Soare",
            "Iain Mackie",
            "Jeffrey Dalton"
        ],
        "published": "2022-11-11T13:31:55Z",
        "summary": "Current SQL generators based on pre-trained language models struggle to\nanswer complex questions requiring domain context or understanding fine-grained\ntable structure. Humans would deal with these unknowns by reasoning over the\ndocumentation of the tables. Based on this hypothesis, we propose DocuT5, which\nuses off-the-shelf language model architecture and injects knowledge from\nexternal `documentation' to improve domain generalization. We perform\nexperiments on the Spider family of datasets that contain complex questions\nthat are cross-domain and multi-table. Specifically, we develop a new\ntext-to-SQL failure taxonomy and find that 19.6% of errors are due to foreign\nkey mistakes, and 49.2% are due to a lack of domain knowledge. We proposed\nDocuT5, a method that captures knowledge from (1) table structure context of\nforeign keys and (2) domain knowledge through contextualizing tables and\ncolumns. Both types of knowledge improve over state-of-the-art T5 with\nconstrained decoding on Spider, and domain knowledge produces state-of-the-art\ncomparable effectiveness on Spider-DK and Spider-SYN datasets.",
        "pdf_link": "https://arxiv.org/pdf/2211.06193v1.pdf"
    },
    {
        "title": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation",
        "authors": [
            "Danial Kamali",
            "Joseph Romain",
            "Huiyi Liu",
            "Wei Peng",
            "Jingbo Meng",
            "Parisa Kordjamshidi"
        ],
        "published": "2022-11-11T03:26:37Z",
        "summary": "Nowadays, the spread of misinformation is a prominent problem in society. Our\nresearch focuses on aiding the automatic identification of misinformation by\nanalyzing the persuasive strategies employed in textual documents. We introduce\na novel annotation scheme encompassing common persuasive writing tactics to\nachieve our objective. Additionally, we provide a dataset on health\nmisinformation, thoroughly annotated by experts utilizing our proposed scheme.\nOur contribution includes proposing a new task of annotating pieces of text\nwith their persuasive writing strategy types. We evaluate fine-tuning and\nprompt-engineering techniques with pre-trained language models of the BERT\nfamily and the generative large language models of the GPT family using\npersuasive strategies as an additional source of information. We evaluate the\neffects of employing persuasive strategies as intermediate labels in the\ncontext of misinformation detection. Our results show that those strategies\nenhance accuracy and improve the explainability of misinformation detection\nmodels. The persuasive strategies can serve as valuable insights and\nexplanations, enabling other models or even humans to make more informed\ndecisions regarding the trustworthiness of the information.",
        "pdf_link": "https://arxiv.org/pdf/2211.05985v4.pdf"
    },
    {
        "title": "Measuring Reliability of Large Language Models through Semantic Consistency",
        "authors": [
            "Harsh Raj",
            "Domenic Rosati",
            "Subhabrata Majumdar"
        ],
        "published": "2022-11-10T20:21:07Z",
        "summary": "While large pretrained language models (PLMs) demonstrate incredible fluency\nand performance on many natural language tasks, recent work has shown that\nwell-performing PLMs are very sensitive to what prompts are feed into them.\nEven when prompts are semantically identical, language models may give very\ndifferent answers. When considering safe and trustworthy deployments of PLMs we\nwould like their outputs to be consistent under prompts that mean the same\nthing or convey the same intent. While some work has looked into how\nstate-of-the-art PLMs address this need, they have been limited to only\nevaluating lexical equality of single- or multi-word answers and do not address\nconsistency of generative text sequences. In order to understand consistency of\nPLMs under text generation settings, we develop a measure of semantic\nconsistency that allows the comparison of open-ended text outputs. We implement\nseveral versions of this consistency metric to evaluate the performance of a\nnumber of PLMs on paraphrased versions of questions in the TruthfulQA dataset,\nwe find that our proposed metrics are considerably more consistent than\ntraditional metrics embodying lexical consistency, and also correlate with\nhuman evaluation of output consistency to a higher degree.",
        "pdf_link": "https://arxiv.org/pdf/2211.05853v2.pdf"
    },
    {
        "title": "The CRINGE Loss: Learning what language not to model",
        "authors": [
            "Leonard Adolphs",
            "Tianyu Gao",
            "Jing Xu",
            "Kurt Shuster",
            "Sainbayar Sukhbaatar",
            "Jason Weston"
        ],
        "published": "2022-11-10T19:30:08Z",
        "summary": "Standard language model training employs gold human documents or human-human\ninteraction data, and treats all training data as positive examples. Growing\nevidence shows that even with very large amounts of positive training data,\nissues remain that can be alleviated with relatively small amounts of negative\ndata -- examples of what the model should not do. In this work, we propose a\nnovel procedure to train with such data called the CRINGE loss (ContRastive\nIterative Negative GEneration). We show the effectiveness of this approach\nacross three different experiments on the tasks of safe generation,\ncontradiction avoidance, and open-domain dialogue. Our models outperform\nmultiple strong baselines and are conceptually simple, easy to train and\nimplement.",
        "pdf_link": "https://arxiv.org/pdf/2211.05826v1.pdf"
    },
    {
        "title": "Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis",
        "authors": [
            "Anguo Dong",
            "Cuiyun Gao",
            "Yan Jia",
            "Qing Liao",
            "Xuan Wang",
            "Lei Wang",
            "Jing Xiao"
        ],
        "published": "2022-11-10T10:09:33Z",
        "summary": "Aspect-based sentiment analysis (ABSA) aims at extracting opinionated aspect\nterms in review texts and determining their sentiment polarities, which is\nwidely studied in both academia and industry. As a fine-grained classification\ntask, the annotation cost is extremely high. Domain adaptation is a popular\nsolution to alleviate the data deficiency issue in new domains by transferring\ncommon knowledge across domains. Most cross-domain ABSA studies are based on\nstructure correspondence learning (SCL), and use pivot features to construct\nauxiliary tasks for narrowing down the gap between domains. However, their\npivot-based auxiliary tasks can only transfer knowledge of aspect terms but not\nsentiment, limiting the performance of existing models. In this work, we\npropose a novel Syntax-guided Domain Adaptation Model, named SDAM, for more\neffective cross-domain ABSA. SDAM exploits syntactic structure similarities for\nbuilding pseudo training instances, during which aspect terms of target domain\nare explicitly related to sentiment polarities. Besides, we propose a\nsyntax-based BERT mask language model for further capturing domain-invariant\nfeatures. Finally, to alleviate the sentiment inconsistency issue in multi-gram\naspect terms, we introduce a span-based joint aspect term and sentiment\nanalysis module into the cross-domain End2End ABSA. Experiments on five\nbenchmark datasets show that our model consistently outperforms the\nstate-of-the-art baselines with respect to Micro-F1 metric for the cross-domain\nEnd2End ABSA task.",
        "pdf_link": "https://arxiv.org/pdf/2211.05457v2.pdf"
    },
    {
        "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
        "authors": [
            "Evangelia Spiliopoulou",
            "Artidoro Pagnoni",
            "Yonatan Bisk",
            "Eduard Hovy"
        ],
        "published": "2022-11-10T07:48:01Z",
        "summary": "This paper investigates models of event implications. Specifically, how well\nmodels predict entity state-changes, by targeting their understanding of\nphysical attributes. Nominally, Large Language models (LLM) have been exposed\nto procedural knowledge about how objects interact, yet our benchmarking shows\nthey fail to reason about the world. Conversely, we also demonstrate that\nexisting approaches often misrepresent the surprising abilities of LLMs via\nimproper task encodings and that proper model prompting can dramatically\nimprove performance of reported baseline results across multiple tasks. In\nparticular, our results indicate that our prompting technique is especially\nuseful for unseen attributes (out-of-domain) or when only limited data is\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2211.05392v1.pdf"
    },
    {
        "title": "Large Language Models with Controllable Working Memory",
        "authors": [
            "Daliang Li",
            "Ankit Singh Rawat",
            "Manzil Zaheer",
            "Xin Wang",
            "Michal Lukasik",
            "Andreas Veit",
            "Felix Yu",
            "Sanjiv Kumar"
        ],
        "published": "2022-11-09T18:58:29Z",
        "summary": "Large language models (LLMs) have led to a series of breakthroughs in natural\nlanguage processing (NLP), owing to their excellent understanding and\ngeneration abilities. Remarkably, what further sets these models apart is the\nmassive amounts of world knowledge they internalize during pretraining. While\nmany downstream applications provide the model with an informational context to\naid its performance on the underlying task, how the model's world knowledge\ninteracts with the factual information presented in the context remains under\nexplored. As a desirable behavior, an LLM should give precedence to the context\nwhenever it contains task-relevant information that conflicts with the model's\nmemorized knowledge. This enables model predictions to be grounded in the\ncontext, which can then be used to update or correct specific model predictions\nwithout frequent retraining. By contrast, when the context is irrelevant to the\ntask, the model should ignore it and fall back on its internal knowledge. In\nthis paper, we undertake a first joint study of the aforementioned two\nproperties, namely controllability and robustness, in the context of LLMs. We\ndemonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)\ncould exhibit poor controllability and robustness, which do not scale with\nincreasing model size. As a solution, we propose a novel method - Knowledge\nAware FineTuning (KAFT) - to strengthen both controllability and robustness by\nincorporating counterfactual and irrelevant contexts to standard supervised\ndatasets. Our comprehensive evaluation showcases the utility of KAFT across\nmodel architectures and sizes.",
        "pdf_link": "https://arxiv.org/pdf/2211.05110v1.pdf"
    },
    {
        "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
        "authors": [
            "Mo Yu",
            "Qiujing Wang",
            "Shunchi Zhang",
            "Yisi Sang",
            "Kangsheng Pu",
            "Zekai Wei",
            "Han Wang",
            "Liyan Xu",
            "Jing Li",
            "Yue Yu",
            "Jie Zhou"
        ],
        "published": "2022-11-09T05:06:12Z",
        "summary": "When reading a story, humans can quickly understand new fictional characters\nwith a few observations, mainly by drawing analogies to fictional and real\npeople they already know. This reflects the few-shot and meta-learning essence\nof humans' inference of characters' mental states, i.e., theory-of-mind (ToM),\nwhich is largely ignored in existing research. We fill this gap with a novel\nNLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM\nin a realistic narrative understanding scenario. Our dataset consists of ~1,000\nparsed movie scripts, each corresponding to a few-shot character understanding\ntask that requires models to mimic humans' ability of fast digesting characters\nwith a few starting scenes in a new movie.\n  We propose a novel ToM prompting approach designed to explicitly assess the\ninfluence of multiple ToM dimensions. It surpasses existing baseline models,\nunderscoring the significance of modeling multiple ToM dimensions for our task.\nOur extensive human study verifies that humans are capable of solving our\nproblem by inferring characters' mental states based on their previously seen\nmovies. In comparison, our systems based on either state-of-the-art large\nlanguage models (GPT-4) or meta-learning algorithms lags >20% behind,\nhighlighting a notable limitation in existing approaches' ToM capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2211.04684v2.pdf"
    },
    {
        "title": "Zero-Label Prompt Selection",
        "authors": [
            "Chonghua Liao",
            "Yanan Zheng",
            "Zhilin Yang"
        ],
        "published": "2022-11-09T04:13:31Z",
        "summary": "Natural language prompts have been shown to facilitate cross-task\ngeneralization for large language models. However, with no or limited labeled\nexamples, the cross-task performance is highly sensitive to the choice of\nprompts, while selecting a high-performing prompt is challenging given the\nscarcity of labels. To address the issue, we propose a Zero-Label Prompt\nSelection (ZPS) method that selects prompts without any labeled data or\ngradient update. Specifically, given the candidate human-written prompts for a\ntask, ZPS labels a set of unlabeled data with a prompt ensemble and uses the\npseudo-labels for prompt selection. Experiments show that ZPS improves over\nprior methods by a sizeable margin in zero-label performance. We also extend\nZPS to a few-shot setting and show its advantages over strong baselines such as\nprompt tuning and model tuning.",
        "pdf_link": "https://arxiv.org/pdf/2211.04668v1.pdf"
    },
    {
        "title": "Active Example Selection for In-Context Learning",
        "authors": [
            "Yiming Zhang",
            "Shi Feng",
            "Chenhao Tan"
        ],
        "published": "2022-11-08T19:00:02Z",
        "summary": "With a handful of demonstration examples, large-scale language models show\nstrong capability to perform various tasks by in-context learning from these\nexamples, without any fine-tuning. We demonstrate that in-context learning\nperformance can be highly unstable across samples of examples, indicating the\nidiosyncrasies of how language models acquire information. We formulate example\nselection for in-context learning as a sequential decision problem, and propose\na reinforcement learning algorithm for identifying generalizable policies to\nselect demonstration examples. For GPT-2, our learned policies demonstrate\nstrong abilities of generalizing to unseen tasks in training, with a $5.8\\%$\nimprovement on average. Examples selected from our learned policies can even\nachieve a small improvement on GPT-3 Ada. However, the improvement diminishes\non larger GPT-3 models, suggesting emerging capabilities of large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2211.04486v1.pdf"
    },
    {
        "title": "Active Learning with Tabular Language Models",
        "authors": [
            "Martin Ringsquandl",
            "Aneta Koleva"
        ],
        "published": "2022-11-08T09:50:30Z",
        "summary": "Despite recent advancements in tabular language model research, real-world\napplications are still challenging. In industry, there is an abundance of\ntables found in spreadsheets, but acquisition of substantial amounts of labels\nis expensive, since only experts can annotate the often highly technical and\ndomain-specific tables. Active learning could potentially reduce labeling\ncosts, however, so far there are no works related to active learning in\nconjunction with tabular language models. In this paper we investigate\ndifferent acquisition functions in a real-world industrial tabular language\nmodel use case for sub-cell named entity recognition. Our results show that\ncell-level acquisition functions with built-in diversity can significantly\nreduce the labeling effort, while enforced table diversity is detrimental. We\nfurther see open fundamental questions concerning computational efficiency and\nthe perspective of human annotators.",
        "pdf_link": "https://arxiv.org/pdf/2211.04128v1.pdf"
    },
    {
        "title": "Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps",
        "authors": [
            "Hiroki Iida",
            "Naoaki Okazaki"
        ],
        "published": "2022-11-08T03:58:26Z",
        "summary": "IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.",
        "pdf_link": "https://arxiv.org/pdf/2211.03988v1.pdf"
    },
    {
        "title": "Retrieval augmentation of large language models for lay language generation",
        "authors": [
            "Yue Guo",
            "Wei Qiu",
            "Gondy Leroy",
            "Sheng Wang",
            "Trevor Cohen"
        ],
        "published": "2022-11-07T19:06:53Z",
        "summary": "Recent lay language generation systems have used Transformer models trained\non a parallel corpus to increase health information accessibility. However, the\napplicability of these models is constrained by the limited size and topical\nbreadth of available corpora. We introduce CELLS, the largest (63k pairs) and\nbroadest-ranging (12 journals) parallel corpus for lay language generation. The\nabstract and the corresponding lay language summary are written by domain\nexperts, assuring the quality of our dataset. Furthermore, qualitative\nevaluation of expert-authored plain language summaries has revealed background\nexplanation as a key strategy to increase accessibility. Such explanation is\nchallenging for neural models to generate because it goes beyond simplification\nby adding content absent from the source. We derive two specialized paired\ncorpora from CELLS to address key challenges in lay language generation:\ngenerating background explanations and simplifying the original abstract. We\nadopt retrieval-augmented models as an intuitive fit for the task of background\nexplanation generation, and show improvements in summary quality and simplicity\nwhile maintaining factual correctness. Taken together, this work presents the\nfirst comprehensive study of background explanation for lay language\ngeneration, paving the path for disseminating scientific knowledge to a broader\naudience. CELLS is publicly available at:\nhttps://github.com/LinguisticAnomalies/pls_retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2211.03818v2.pdf"
    },
    {
        "title": "Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach",
        "authors": [
            "Jiayao Zhang",
            "Hongming Zhang",
            "Zhun Deng",
            "Dan Roth"
        ],
        "published": "2022-11-07T16:19:42Z",
        "summary": "Double-blind peer review mechanism has become the skeleton of academic\nresearch across multiple disciplines including computer science, yet several\nstudies have questioned the quality of peer reviews and raised concerns on\npotential biases in the process. In this paper, we conduct a thorough and\nrigorous study on fairness disparities in peer review with the help of large\nlanguage models (LMs). We collect, assemble, and maintain a comprehensive\nrelational database for the International Conference on Learning\nRepresentations (ICLR) conference from 2017 to date by aggregating data from\nOpenReview, Google Scholar, arXiv, and CSRanking, and extracting high-level\nfeatures using language models. We postulate and study fairness disparities on\nmultiple protective attributes of interest, including author gender, geography,\nauthor, and institutional prestige. We observe that the level of disparity\ndiffers and textual features are essential in reducing biases in the predictive\nmodeling. We distill several insights from our analysis on study the peer\nreview process with the help of large LMs. Our database also provides avenues\nfor studying new natural language processing (NLP) methods that facilitate the\nunderstanding of the peer review mechanism. We study a concrete example towards\nautomatic machine review systems and provide baseline models for the review\ngeneration and scoring tasks such that the database can be used as a benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2211.06398v1.pdf"
    },
    {
        "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages",
        "authors": [
            "Bonaventure F. P. Dossou",
            "Atnafu Lambebo Tonja",
            "Oreen Yousuf",
            "Salomey Osei",
            "Abigail Oppong",
            "Iyanuoluwa Shode",
            "Oluwabusayo Olufunke Awoyomi",
            "Chris Chinenye Emezue"
        ],
        "published": "2022-11-07T02:15:25Z",
        "summary": "In recent years, multilingual pre-trained language models have gained\nprominence due to their remarkable performance on numerous downstream Natural\nLanguage Processing tasks (NLP). However, pre-training these large multilingual\nlanguage models requires a lot of training data, which is not available for\nAfrican Languages. Active learning is a semi-supervised learning algorithm, in\nwhich a model consistently and dynamically learns to identify the most\nbeneficial samples to train itself on, in order to achieve better optimization\nand performance on downstream tasks. Furthermore, active learning effectively\nand practically addresses real-world data scarcity. Despite all its benefits,\nactive learning, in the context of NLP and especially multilingual language\nmodels pretraining, has received little consideration. In this paper, we\npresent AfroLM, a multilingual language model pretrained from scratch on 23\nAfrican languages (the largest effort to date) using our novel self-active\nlearning framework. Pretrained on a dataset significantly (14x) smaller than\nexisting baselines, AfroLM outperforms many multilingual pretrained language\nmodels (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text\nclassification, and sentiment analysis). Additional out-of-domain sentiment\nanalysis experiments show that \\textbf{AfroLM} is able to generalize well\nacross various domains. We release the code source, and our datasets used in\nour framework at https://github.com/bonaventuredossou/MLM_AL.",
        "pdf_link": "https://arxiv.org/pdf/2211.03263v2.pdf"
    },
    {
        "title": "Noisy Channel for Automatic Text Simplification",
        "authors": [
            "Oscar M Cumbicus-Pineda",
            "Iker Gutiérrez-Fandiño",
            "Itziar Gonzalez-Dios",
            "Aitor Soroa"
        ],
        "published": "2022-11-06T15:28:42Z",
        "summary": "In this paper we present a simple re-ranking method for Automatic Sentence\nSimplification based on the noisy channel scheme. Instead of directly computing\nthe best simplification given a complex text, the re-ranking method also\nconsiders the probability of the simple sentence to produce the complex\ncounterpart, as well as the probability of the simple text itself, according to\na language model. Our experiments show that combining these scores outperform\nthe original system in three different English datasets, yielding the best\nknown result in one of them. Adopting the noisy channel scheme opens new ways\nto infuse additional information into ATS systems, and thus to control\nimportant aspects of them, a known limitation of end-to-end neural seq2seq\ngenerative models.",
        "pdf_link": "https://arxiv.org/pdf/2211.03152v1.pdf"
    },
    {
        "title": "Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust",
        "authors": [
            "Haotian Chen",
            "Lingwei Zhang",
            "Yiran Liu",
            "Fanchao Chen",
            "Yang Yu"
        ],
        "published": "2022-11-06T07:03:31Z",
        "summary": "Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact\ndescriptions according to rule of law, serves as legal assistance to mitigate\nthe great work burden of limited legal practitioners. Most existing methods\napply various large-scale pre-trained language models (PLMs) finetuned in LJP\ntasks to obtain consistent improvements. However, we discover the fact that the\nstate-of-the-art (SOTA) model makes judgment predictions according to\nirrelevant (or non-casual) information. The violation of rule of law not only\nweakens the robustness and generalization ability of models but also results in\nsevere social problems like discrimination. In this paper, we use causal\nstructural models (SCMs) to theoretically analyze how LJP models learn to make\ndecisions and why they can succeed in passing the traditional testing paradigm\nwithout learning causality. According to our analysis, we provide two solutions\nintervening on data and model by causality, respectively. In detail, we first\ndistinguish non-causal information by applying the open information extraction\n(OIE) technique. Then, we propose a method named the Causal Information\nEnhanced SAmpling Method (CIESAM) to eliminate the non-causal information from\ndata. To validate our theoretical analysis, we further propose another method\nusing our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide\nthe model to learn the underlying causality knowledge in legal texts. The\nconfidence of CASAM in learning causal information is higher than that of\nCIESAM. The extensive experimental results show that both our proposed methods\nachieve state-of-the-art (SOTA) performance on three commonly used\nlegal-specific datasets. The stronger performance of CASAM further demonstrates\nthat causality is the key to the robustness and generalization ability of\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2211.03046v2.pdf"
    },
    {
        "title": "MolE: a molecular foundation model for drug discovery",
        "authors": [
            "Oscar Méndez-Lucio",
            "Christos Nicolaou",
            "Berton Earnshaw"
        ],
        "published": "2022-11-03T21:22:05Z",
        "summary": "Models that accurately predict properties based on chemical structure are\nvaluable tools in drug discovery. However, for many properties, public and\nprivate training sets are typically small, and it is difficult for the models\nto generalize well outside of the training data. Recently, large language\nmodels have addressed this problem by using self-supervised pretraining on\nlarge unlabeled datasets, followed by fine-tuning on smaller, labeled datasets.\nIn this paper, we report MolE, a molecular foundation model that adapts the\nDeBERTa architecture to be used on molecular graphs together with a two-step\npretraining strategy. The first step of pretraining is a self-supervised\napproach focused on learning chemical structures, and the second step is a\nmassive multi-task approach to learn biological information. We show that\nfine-tuning pretrained MolE achieves state-of-the-art results on 9 of the 22\nADMET tasks included in the Therapeutic Data Commons.",
        "pdf_link": "https://arxiv.org/pdf/2211.02657v1.pdf"
    },
    {
        "title": "Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2022-11-03T18:53:30Z",
        "summary": "Through their transfer learning abilities, highly-parameterized large\npre-trained language models have dominated the NLP landscape for a multitude of\ndownstream language tasks. Though linguistically proficient, the inability of\nthese models to incorporate the learning of non-linguistic entities (numerals\nand arithmetic reasoning) limits their usage for tasks that require numeric\ncomprehension or strict mathematical reasoning. However, as we illustrate in\nthis paper, building a general purpose language model that also happens to be\nproficient in mathematical reasoning is not as straight-forward as training it\non a numeric dataset. In this work, we develop a novel framework that enables\nlanguage models to be mathematically proficient while retaining their\nlinguistic prowess. Specifically, we offer information-theoretic interventions\nto overcome the catastrophic forgetting of linguistic skills that occurs while\ninjecting non-linguistic skills into language models.",
        "pdf_link": "https://arxiv.org/pdf/2211.02098v1.pdf"
    },
    {
        "title": "LMentry: A Language Model Benchmark of Elementary Language Tasks",
        "authors": [
            "Avia Efrat",
            "Or Honovich",
            "Omer Levy"
        ],
        "published": "2022-11-03T18:01:12Z",
        "summary": "As the performance of large language models rapidly improves, benchmarks are\ngetting larger and more complex as well. We present LMentry, a benchmark that\navoids this \"arms race\" by focusing on a compact set of tasks that are trivial\nto humans, e.g. writing a sentence containing a specific word, identifying\nwhich words in a list belong to a specific category, or choosing which of two\nwords is longer. LMentry is specifically designed to provide quick and\ninterpretable insights into the capabilities and robustness of large language\nmodels. Our experiments reveal a wide variety of failure cases that, while\nimmediately obvious to humans, pose a considerable challenge for large language\nmodels, including OpenAI's latest 175B-parameter instruction-tuned model,\nTextDavinci002. LMentry complements contemporary evaluation approaches of large\nlanguage models, providing a quick, automatic, and easy-to-run \"unit test\",\nwithout resorting to large benchmark suites of complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2211.02069v2.pdf"
    },
    {
        "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",
        "authors": [
            "Alexandra Sasha Luccioni",
            "Sylvain Viguier",
            "Anne-Laure Ligozat"
        ],
        "published": "2022-11-03T17:13:48Z",
        "summary": "Progress in machine learning (ML) comes with a cost to the environment, given\nthat training ML models requires significant computational resources, energy\nand materials. In the present article, we aim to quantify the carbon footprint\nof BLOOM, a 176-billion parameter language model, across its life cycle. We\nestimate that BLOOM's final training emitted approximately 24.7 tonnes\nof~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes\nif we account for all processes ranging from equipment manufacturing to\nenergy-based operational consumption. We also study the energy requirements and\ncarbon emissions of its deployment for inference via an API endpoint receiving\nuser queries in real-time. We conclude with a discussion regarding the\ndifficulty of precisely estimating the carbon footprint of ML models and future\nresearch directions that can contribute towards improving carbon emissions\nreporting.",
        "pdf_link": "https://arxiv.org/pdf/2211.02001v1.pdf"
    },
    {
        "title": "Large Language Models Are Human-Level Prompt Engineers",
        "authors": [
            "Yongchao Zhou",
            "Andrei Ioan Muresanu",
            "Ziwen Han",
            "Keiran Paster",
            "Silviu Pitis",
            "Harris Chan",
            "Jimmy Ba"
        ],
        "published": "2022-11-03T15:43:03Z",
        "summary": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.",
        "pdf_link": "https://arxiv.org/pdf/2211.01910v2.pdf"
    },
    {
        "title": "Contextual information integration for stance detection via cross-attention",
        "authors": [
            "Tilman Beck",
            "Andreas Waldis",
            "Iryna Gurevych"
        ],
        "published": "2022-11-03T15:04:29Z",
        "summary": "Stance detection deals with identifying an author's stance towards a target.\nMost existing stance detection models are limited because they do not consider\nrelevant contextual information which allows for inferring the stance\ncorrectly. Complementary context can be found in knowledge bases but\nintegrating the context into pretrained language models is non-trivial due to\nthe graph structure of standard knowledge bases. To overcome this, we explore\nan approach to integrate contextual information as text which allows for\nintegrating contextual information from heterogeneous sources, such as\nstructured knowledge sources and by prompting large language models. Our\napproach can outperform competitive baselines on a large and diverse stance\ndetection benchmark in a cross-target setup, i.e. for targets unseen during\ntraining. We demonstrate that it is more robust to noisy context and can\nregularize for unwanted correlations between labels and target-specific\nvocabulary. Finally, it is independent of the pretrained language model in use.",
        "pdf_link": "https://arxiv.org/pdf/2211.01874v2.pdf"
    },
    {
        "title": "Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer",
        "authors": [
            "Dimitris Mamakas",
            "Petros Tsotsi",
            "Ion Androutsopoulos",
            "Ilias Chalkidis"
        ],
        "published": "2022-11-02T09:27:01Z",
        "summary": "Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.",
        "pdf_link": "https://arxiv.org/pdf/2211.00974v2.pdf"
    },
    {
        "title": "BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder",
        "authors": [
            "Yosuke Higuchi",
            "Tetsuji Ogawa",
            "Tetsunori Kobayashi",
            "Shinji Watanabe"
        ],
        "published": "2022-11-02T00:10:43Z",
        "summary": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2211.00792v2.pdf"
    },
    {
        "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
        "authors": [
            "Yihan Wang",
            "Si Si",
            "Daliang Li",
            "Michal Lukasik",
            "Felix Yu",
            "Cho-Jui Hsieh",
            "Inderjit S Dhillon",
            "Sanjiv Kumar"
        ],
        "published": "2022-11-01T17:56:57Z",
        "summary": "Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.",
        "pdf_link": "https://arxiv.org/pdf/2211.00635v3.pdf"
    },
    {
        "title": "The future is different: Large pre-trained language models fail in prediction tasks",
        "authors": [
            "Kostadin Cvejoski",
            "Ramsés J. Sánchez",
            "César Ojeda"
        ],
        "published": "2022-11-01T11:01:36Z",
        "summary": "Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021",
        "pdf_link": "https://arxiv.org/pdf/2211.00384v2.pdf"
    },
    {
        "title": "Generating Sequences by Learning to Self-Correct",
        "authors": [
            "Sean Welleck",
            "Ximing Lu",
            "Peter West",
            "Faeze Brahman",
            "Tianxiao Shen",
            "Daniel Khashabi",
            "Yejin Choi"
        ],
        "published": "2022-10-31T18:09:51Z",
        "summary": "Sequence generation applications require satisfying semantic constraints,\nsuch as ensuring that programs are correct, using certain keywords, or avoiding\nundesirable content. Language models, whether fine-tuned or prompted with\nfew-shot demonstrations, frequently violate these constraints, and lack a\nmechanism to iteratively revise their outputs. Moreover, some powerful language\nmodels are of extreme scale or inaccessible, making it inefficient, if not\ninfeasible, to update their parameters for task-specific adaptation. We present\nSelf-Correction, an approach that decouples an imperfect base generator (an\noff-the-shelf language model or supervised sequence-to-sequence model) from a\nseparate corrector that learns to iteratively correct imperfect generations. To\ntrain the corrector, we propose an online training procedure that can use\neither scalar or natural language feedback on intermediate imperfect\ngenerations. We show that Self-Correction improves upon the base generator in\nthree diverse generation tasks - mathematical program synthesis,\nlexically-constrained generation, and toxicity control - even when the\ncorrector is much smaller than the base generator.",
        "pdf_link": "https://arxiv.org/pdf/2211.00053v1.pdf"
    },
    {
        "title": "Query Refinement Prompts for Closed-Book Long-Form Question Answering",
        "authors": [
            "Reinald Kim Amplayo",
            "Kellie Webster",
            "Michael Collins",
            "Dipanjan Das",
            "Shashi Narayan"
        ],
        "published": "2022-10-31T17:44:42Z",
        "summary": "Large language models (LLMs) have been shown to perform well in answering\nquestions and in producing long-form texts, both in few-shot closed-book\nsettings. While the former can be validated using well-known evaluation\nmetrics, the latter is difficult to evaluate. We resolve the difficulties to\nevaluate long-form output by doing both tasks at once -- to do question\nanswering that requires long-form answers. Such questions tend to be\nmultifaceted, i.e., they may have ambiguities and/or require information from\nmultiple sources. To this end, we define query refinement prompts that\nencourage LLMs to explicitly express the multifacetedness in questions and\ngenerate long-form answers covering multiple facets of the question. Our\nexperiments on two long-form question answering datasets, ASQA and AQuAMuSe,\nshow that using our prompts allows us to outperform fully finetuned models in\nthe closed book setting, as well as achieve results comparable to\nretrieve-then-generate open-book models.",
        "pdf_link": "https://arxiv.org/pdf/2210.17525v1.pdf"
    },
    {
        "title": "Emergent Linguistic Structures in Neural Networks are Fragile",
        "authors": [
            "Emanuele La Malfa",
            "Matthew Wicker",
            "Marta Kwiatkowska"
        ],
        "published": "2022-10-31T15:43:57Z",
        "summary": "Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structures. In this paper, focusing on\nthe ability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.17406v8.pdf"
    },
    {
        "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation",
        "authors": [
            "Spyridon Mouselinos",
            "Mateusz Malinowski",
            "Henryk Michalewski"
        ],
        "published": "2022-10-31T15:06:15Z",
        "summary": "Recently, high-performing code generation systems based on large language\nmodels have surfaced. They are trained on massive corpora containing much more\nnatural text than actual executable computer code. This work shows that current\ncode generation systems exhibit undesired biases inherited from their large\nlanguage model backbones, which can reduce the quality of the generated code\nunder specific circumstances.\n  To investigate the effect, we propose the \"block of influence\" concept, which\nenables a modular decomposition and analysis of the coding challenges. We\nintroduce an automated intervention mechanism reminiscent of adversarial\ntesting that exposes undesired biases through the failure modes of the models\nunder test. Finally, we demonstrate how our framework can be used as a data\ntransformation technique during fine-tuning, acting as a mitigation strategy\nfor these biases.",
        "pdf_link": "https://arxiv.org/pdf/2211.00609v2.pdf"
    },
    {
        "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
        "authors": [
            "Elias Frantar",
            "Saleh Ashkboos",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "published": "2022-10-31T13:42:40Z",
        "summary": "Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.",
        "pdf_link": "https://arxiv.org/pdf/2210.17323v2.pdf"
    },
    {
        "title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task",
        "authors": [
            "Nyoungwoo Lee",
            "ChaeHun Park",
            "Ho-Jin Choi",
            "Jaegul Choo"
        ],
        "published": "2022-10-31T11:49:49Z",
        "summary": "In retrieval-based dialogue systems, a response selection model acts as a\nranker to select the most appropriate response among several candidates.\nHowever, such selection models tend to rely on context-response content\nsimilarity, which makes models vulnerable to adversarial responses that are\nsemantically similar but not relevant to the dialogue context. Recent studies\nhave shown that leveraging these adversarial responses as negative training\nsamples is useful for improving the discriminating power of the selection\nmodel. Nevertheless, collecting human-written adversarial responses is\nexpensive, and existing synthesizing methods often have limited scalability. To\novercome these limitations, this paper proposes a simple but efficient method\nfor generating adversarial negative responses leveraging a large-scale language\nmodel. Experimental results on dialogue selection tasks show that our method\noutperforms other methods of synthesizing adversarial negative responses. These\nresults suggest that our method can be an effective alternative to human\nannotators in generating adversarial responses. Our dataset and generation code\nis available at https://github.com/leenw23/generating-negatives-by-gpt3.",
        "pdf_link": "https://arxiv.org/pdf/2210.17238v1.pdf"
    },
    {
        "title": "Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change",
        "authors": [
            "Zhaochen Su",
            "Zecheng Tang",
            "Xinyan Guan",
            "Juntao Li",
            "Lijun Wu",
            "Min Zhang"
        ],
        "published": "2022-10-31T08:12:41Z",
        "summary": "Recent research has revealed that neural language models at scale suffer from\npoor temporal generalization capability, i.e., the language model pre-trained\non static data from past years performs worse over time on emerging data.\nExisting methods mainly perform continual training to mitigate such a\nmisalignment. While effective to some extent but is far from being addressed on\nboth the language modeling and downstream tasks. In this paper, we empirically\nobserve that temporal generalization is closely affiliated with lexical\nsemantic change, which is one of the essential phenomena of natural languages.\nBased on this observation, we propose a simple yet effective lexical-level\nmasking strategy to post-train a converged language model. Experiments on two\npre-trained language models, two different classification tasks, and four\nbenchmark datasets demonstrate the effectiveness of our proposed method over\nexisting temporal adaptation methods, i.e., continual training with new data.\nOur code is available at \\url{https://github.com/zhaochen0110/LMLM}.",
        "pdf_link": "https://arxiv.org/pdf/2210.17127v1.pdf"
    },
    {
        "title": "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM",
        "authors": [
            "Shira Guskin",
            "Moshe Wasserblat",
            "Chang Wang",
            "Haihao Shen"
        ],
        "published": "2022-10-31T07:42:52Z",
        "summary": "Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with <1%\naccuracy loss). The code to reproduce this work is publicly available on\nGithub.",
        "pdf_link": "https://arxiv.org/pdf/2210.17114v3.pdf"
    },
    {
        "title": "A Solvable Model of Neural Scaling Laws",
        "authors": [
            "Alexander Maloney",
            "Daniel A. Roberts",
            "James Sully"
        ],
        "published": "2022-10-30T15:13:18Z",
        "summary": "Large language models with a huge number of parameters, when trained on near\ninternet-sized number of tokens, have been empirically shown to obey neural\nscaling laws: specifically, their performance behaves predictably as a power\nlaw in either parameters or dataset size until bottlenecked by the other\nresource. To understand this better, we first identify the necessary properties\nallowing such scaling laws to arise and then propose a statistical model -- a\njoint generative data model and random feature model -- that captures this\nneural scaling phenomenology. By solving this model in the dual limit of large\ntraining set size and large number of parameters, we gain insight into (i) the\nstatistical structure of datasets and tasks that lead to scaling laws, (ii) the\nway nonlinear feature maps, such as those provided by neural networks, enable\nscaling laws when trained on these datasets, (iii) the optimality of the\nequiparameterization scaling of training sets and parameters, and (iv) whether\nsuch scaling laws can break down and how they behave when they do. Key findings\nare the manner in which the power laws that occur in the statistics of natural\ndatasets are extended by nonlinear random feature maps and then translated into\npower-law scalings of the test loss and how the finite extent of the data's\nspectral power law causes the model's performance to plateau.",
        "pdf_link": "https://arxiv.org/pdf/2210.16859v1.pdf"
    },
    {
        "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
        "authors": [
            "Xinyu Zhu",
            "Junjie Wang",
            "Lin Zhang",
            "Yuxiang Zhang",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yujiu Yang"
        ],
        "published": "2022-10-28T16:47:03Z",
        "summary": "Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenging problems, especially those that need high-level intelligence, such\nas the math word problem (MWPs). However, directly applying existing PLMs to\nMWPs can fail as the generation process lacks sufficient supervision and thus\nlacks fast adaptivity as humans. We notice that human reasoning has a dual\nreasoning framework that consists of an immediate reaction system (system 1)\nand a delicate reasoning system (system 2), where the entire reasoning is\ndetermined by their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.6% increase over best\nbaselines. Our codes are available at https://github.com/TianHongZXY/CoRe",
        "pdf_link": "https://arxiv.org/pdf/2210.16257v5.pdf"
    },
    {
        "title": "Probing for targeted syntactic knowledge through grammatical error detection",
        "authors": [
            "Christopher Davis",
            "Christopher Bryant",
            "Andrew Caines",
            "Marek Rei",
            "Paula Buttery"
        ],
        "published": "2022-10-28T16:01:25Z",
        "summary": "Targeted studies testing knowledge of subject-verb agreement (SVA) indicate\nthat pre-trained language models encode syntactic information. We assert that\nif models robustly encode subject-verb agreement, they should be able to\nidentify when agreement is correct and when it is incorrect. To that end, we\npropose grammatical error detection as a diagnostic probe to evaluate\ntoken-level contextual representations for their knowledge of SVA. We evaluate\ncontextual representations at each layer from five pre-trained English language\nmodels: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated\ntraining data from both English second language learners and Wikipedia edits,\nand report results on manually crafted stimuli for subject-verb agreement. We\nfind that masked language models linearly encode information relevant to the\ndetection of SVA errors, while the autoregressive models perform on par with\nour baseline. However, we also observe a divergence in performance when probes\nare trained on different training sets, and when they are evaluated on\ndifferent syntactic constructions, suggesting the information pertaining to SVA\nerror detection is not robustly encoded.",
        "pdf_link": "https://arxiv.org/pdf/2210.16228v1.pdf"
    },
    {
        "title": "QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation",
        "authors": [
            "Krishna Srinivasan",
            "Karthik Raman",
            "Anupam Samanta",
            "Lingrui Liao",
            "Luca Bertelli",
            "Mike Bendersky"
        ],
        "published": "2022-10-27T18:44:58Z",
        "summary": "Large Language Models (LLMs) have shown impressive results on a variety of\ntext understanding tasks. Search queries though pose a unique challenge, given\ntheir short-length and lack of nuance or context. Complicated feature\nengineering efforts do not always lead to downstream improvements as their\nperformance benefits may be offset by increased complexity of knowledge\ndistillation. Thus, in this paper we make the following contributions: (1) We\ndemonstrate that Retrieval Augmentation of queries provides LLMs with valuable\nadditional context enabling improved understanding. While Retrieval\nAugmentation typically increases latency of LMs (thus hurting distillation\nefficacy), (2) we provide a practical and effective way of distilling Retrieval\nAugmentation LLMs. Specifically, we use a novel two-stage distillation approach\nthat allows us to carry over the gains of retrieval augmentation, without\nsuffering the increased compute typically associated with it. (3) We\ndemonstrate the benefits of the proposed approach (QUILL) on a billion-scale,\nreal-world query understanding system resulting in huge gains. Via extensive\nexperiments, including on public benchmarks, we believe this work offers a\nrecipe for practical use of retrieval-augmented query understanding.",
        "pdf_link": "https://arxiv.org/pdf/2210.15718v1.pdf"
    },
    {
        "title": "What Language Model to Train if You Have One Million GPU Hours?",
        "authors": [
            "Teven Le Scao",
            "Thomas Wang",
            "Daniel Hesslow",
            "Lucile Saulnier",
            "Stas Bekman",
            "M Saiful Bari",
            "Stella Biderman",
            "Hady Elsahar",
            "Niklas Muennighoff",
            "Jason Phang",
            "Ofir Press",
            "Colin Raffel",
            "Victor Sanh",
            "Sheng Shen",
            "Lintang Sutawika",
            "Jaesung Tae",
            "Zheng Xin Yong",
            "Julien Launay",
            "Iz Beltagy"
        ],
        "published": "2022-10-27T13:43:27Z",
        "summary": "The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .",
        "pdf_link": "https://arxiv.org/pdf/2210.15424v2.pdf"
    },
    {
        "title": "Truncation Sampling as Language Model Desmoothing",
        "authors": [
            "John Hewitt",
            "Christopher D. Manning",
            "Percy Liang"
        ],
        "published": "2022-10-27T05:52:35Z",
        "summary": "Long samples of text from neural language models can be of poor quality.\nTruncation sampling algorithms--like top-$p$ or top-$k$ -- address this by\nsetting some words' probabilities to zero at each step. This work provides\nframing for the aim of truncation, and an improved algorithm for that aim. We\npropose thinking of a neural language model as a mixture of a true distribution\nand a smoothing distribution that avoids infinite perplexity. In this light,\ntruncation algorithms aim to perform desmoothing, estimating a subset of the\nsupport of the true distribution. Finding a good subset is crucial: we show\nthat top-$p$ unnecessarily truncates high-probability words, for example\ncausing it to truncate all words but Trump for a document that starts with\nDonald. We introduce $\\eta$-sampling, which truncates words below an\nentropy-dependent probability threshold. Compared to previous algorithms,\n$\\eta$-sampling generates more plausible long English documents according to\nhumans, is better at breaking out of repetition, and behaves more reasonably on\na battery of test distributions.",
        "pdf_link": "https://arxiv.org/pdf/2210.15191v1.pdf"
    },
    {
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
        "authors": [
            "Xiang Lisa Li",
            "Ari Holtzman",
            "Daniel Fried",
            "Percy Liang",
            "Jason Eisner",
            "Tatsunori Hashimoto",
            "Luke Zettlemoyer",
            "Mike Lewis"
        ],
        "published": "2022-10-27T00:58:21Z",
        "summary": "Given a language model (LM), maximum probability is a poor decoding objective\nfor open-ended generation, because it produces short and repetitive text. On\nthe other hand, sampling can often produce incoherent text that drifts from the\noriginal topics. We propose contrastive decoding (CD), a reliable decoding\napproach that optimizes a contrastive objective subject to a plausibility\nconstraint. The contrastive objective returns the difference between the\nlikelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM\n(called the amateur, e.g. OPT-125M), and the constraint ensures that the\noutputs are plausible. CD is inspired by the fact that the failures of larger\nLMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and\nthat this difference signals which texts should be preferred. CD requires zero\nadditional training, and produces higher quality text than decoding from the\nlarger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and\nsignificantly outperforms four strong decoding algorithms (e.g., nucleus,\ntop-k) in automatic and human evaluations across wikipedia, news and story\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2210.15097v2.pdf"
    },
    {
        "title": "Privately Fine-Tuning Large Language Models with Differential Privacy",
        "authors": [
            "Rouzbeh Behnia",
            "Mohamamdreza Ebrahimi",
            "Jason Pacheco",
            "Balaji Padmanabhan"
        ],
        "published": "2022-10-26T21:18:31Z",
        "summary": "Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.",
        "pdf_link": "https://arxiv.org/pdf/2210.15042v3.pdf"
    },
    {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "authors": [
            "Jianan Zhao",
            "Meng Qu",
            "Chaozhuo Li",
            "Hao Yan",
            "Qian Liu",
            "Rui Li",
            "Xing Xie",
            "Jian Tang"
        ],
        "published": "2022-10-26T13:40:57Z",
        "summary": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by training large language models and GNNs together. In this paper, we\npropose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows training the two modules separately while simultaneously\nallowing the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2210.14709v2.pdf"
    },
    {
        "title": "Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis",
        "authors": [
            "Sudhandar Balakrishnan",
            "Yihao Fang",
            "Xioadan Zhu"
        ],
        "published": "2022-10-26T01:13:41Z",
        "summary": "The invention of transformer-based models such as BERT, GPT, and RoBERTa has\nenabled researchers and financial companies to finetune these powerful models\nand use them in different downstream tasks to achieve state-of-the-art\nperformance. Recently, a lightweight alternative (approximately 0.1% - 3% of\nthe original model parameters) to fine-tuning, known as prefix tuning has been\nintroduced. This method freezes the model parameters and only updates the\nprefix to achieve performance comparable to full fine-tuning. Prefix tuning\nenables researchers and financial practitioners to achieve similar results with\nmuch fewer parameters. In this paper, we explore the robustness of prefix\ntuning when facing noisy data. Our experiments demonstrate that fine-tuning is\nmore robust to noise than prefix tuning -- the latter method faces a\nsignificant decrease in performance on most corrupted data sets with increasing\nnoise levels. Furthermore, prefix tuning has high variances in the F1 scores\ncompared to fine-tuning in many corruption methods. We strongly advocate that\ncaution should be carefully taken when applying the state-of-the-art prefix\ntuning method to noisy data.",
        "pdf_link": "https://arxiv.org/pdf/2211.05584v1.pdf"
    },
    {
        "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
        "authors": [
            "Victor Zhong",
            "Weijia Shi",
            "Wen-tau Yih",
            "Luke Zettlemoyer"
        ],
        "published": "2022-10-25T21:39:36Z",
        "summary": "We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2210.14353v2.pdf"
    },
    {
        "title": "MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction",
        "authors": [
            "Pengtao Zhang",
            "Junlin Zhang"
        ],
        "published": "2022-10-25T12:08:14Z",
        "summary": "New findings in natural language processing (NLP) demonstrate that the strong\nmemorization capability contributes a lot to the success of Large Language\nModels (LLM). This inspires us to explicitly bring an independent memory\nmechanism into CTR ranking model to learn and memorize cross features'\nrepresentations. In this paper, we propose multi-Hash Codebook NETwork (HCNet)\nas the memory mechanism for efficiently learning and memorizing representations\nof cross features in CTR tasks. HCNet uses a multi-hash codebook as the main\nmemory place and the whole memory procedure consists of three phases:\nmulti-hash addressing, memory restoring, and feature shrinking. We also propose\na new CTR model named MemoNet which combines HCNet with a DNN backbone.\nExtensive experimental results on three public datasets and online test show\nthat MemoNet reaches superior performance over state-of-the-art approaches.\nBesides, MemoNet shows scaling law of large language model in NLP, which means\nwe can enlarge the size of the codebook in HCNet to sustainably obtain\nperformance gains. Our work demonstrates the importance and feasibility of\nlearning and memorizing representations of cross features, which sheds light on\na new promising research direction.",
        "pdf_link": "https://arxiv.org/pdf/2211.01334v3.pdf"
    },
    {
        "title": "Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition",
        "authors": [
            "Xulong Zhang",
            "Jianzong Wang",
            "Ning Cheng",
            "Mengyuan Zhao",
            "Zhiyong Zhang",
            "Jing Xiao"
        ],
        "published": "2022-10-25T08:12:59Z",
        "summary": "The recent emergence of joint CTC-Attention model shows significant\nimprovement in automatic speech recognition (ASR). The improvement largely lies\nin the modeling of linguistic information by decoder. The decoder\njoint-optimized with an acoustic encoder renders the language model from\nground-truth sequences in an auto-regressive manner during training. However,\nthe training corpus of the decoder is limited to the speech transcriptions,\nwhich is far less than the corpus needed to train an acceptable language model.\nThis leads to poor robustness of decoder. To alleviate this problem, we propose\nlinguistic-enhanced transformer, which introduces refined CTC information to\ndecoder during training process, so that the decoder can be more robust. Our\nexperiments on AISHELL-1 speech corpus show that the character error rate (CER)\nis relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR\nmodel, decoder is more sensitive to linguistic information than acoustic\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2210.14725v1.pdf"
    },
    {
        "title": "Parameter-Efficient Legal Domain Adaptation",
        "authors": [
            "Jonathan Li",
            "Rohan Bhambhoria",
            "Xiaodan Zhu"
        ],
        "published": "2022-10-25T02:14:15Z",
        "summary": "Seeking legal advice is often expensive. Recent advancements in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models in the legal\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2210.13712v2.pdf"
    },
    {
        "title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
        "authors": [
            "Hung-Ting Chen",
            "Michael J. Q. Zhang",
            "Eunsol Choi"
        ],
        "published": "2022-10-25T01:46:00Z",
        "summary": "Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.",
        "pdf_link": "https://arxiv.org/pdf/2210.13701v1.pdf"
    },
    {
        "title": "Speeding Up Question Answering Task of Language Models via Inverted Index",
        "authors": [
            "Xiang Ji",
            "Yesim Sungu-Eryilmaz",
            "Elaheh Momeni",
            "Reza Rawassizadeh"
        ],
        "published": "2022-10-24T19:59:17Z",
        "summary": "Natural language processing applications, such as conversational agents and\ntheir question-answering capabilities, are widely used in the real world.\nDespite the wide popularity of large language models (LLMs), few real-world\nconversational agents take advantage of LLMs. Extensive resources consumed by\nLLMs disable developers from integrating them into end-user applications. In\nthis study, we leverage an inverted indexing mechanism combined with LLMs to\nimprove the efficiency of question-answering models for closed-domain\nquestions. Our experiments show that using the index improves the average\nresponse time by 97.44%. In addition, due to the reduced search scope, the\naverage BLEU score improved by 0.23 while using the inverted index.",
        "pdf_link": "https://arxiv.org/pdf/2210.13578v1.pdf"
    },
    {
        "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
        "authors": [
            "Maarten Sap",
            "Ronan LeBras",
            "Daniel Fried",
            "Yejin Choi"
        ],
        "published": "2022-10-24T14:58:58Z",
        "summary": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.",
        "pdf_link": "https://arxiv.org/pdf/2210.13312v2.pdf"
    },
    {
        "title": "Proficiency assessment of L2 spoken English using wav2vec 2.0",
        "authors": [
            "Stefano Bannò",
            "Marco Matassoni"
        ],
        "published": "2022-10-24T12:36:49Z",
        "summary": "The increasing demand for learning English as a second language has led to a\ngrowing interest in methods for automatically assessing spoken language\nproficiency. Most approaches use hand-crafted features, but their efficacy\nrelies on their particular underlying assumptions and they risk discarding\npotentially salient information about proficiency. Other approaches rely on\ntranscriptions produced by ASR systems which may not provide a faithful\nrendition of a learner's utterance in specific scenarios (e.g., non-native\nchildren's spontaneous speech). Furthermore, transcriptions do not yield any\ninformation about relevant aspects such as intonation, rhythm or prosody. In\nthis paper, we investigate the use of wav2vec 2.0 for assessing overall and\nindividual aspects of proficiency on two small datasets, one of which is\npublicly available. We find that this approach significantly outperforms the\nBERT-based baseline system trained on ASR and manual transcriptions used for\ncomparison.",
        "pdf_link": "https://arxiv.org/pdf/2210.13168v1.pdf"
    },
    {
        "title": "Do Language Models Understand Measurements?",
        "authors": [
            "Sungjin Park",
            "Seungwoo Ryu",
            "Edward Choi"
        ],
        "published": "2022-10-23T10:52:52Z",
        "summary": "Recent success of pre-trained language models (PLMs) has stimulated interest\nin their ability to understand and work with numbers. Yet, the numerical\nreasoning over measurements has not been formally studied despite their\nimportance. In this study, we show that PLMs lack the capability required for\nreasoning over measurements. Furthermore, we find that a language model trained\non a measurement-rich corpus shows better performance on understanding\nmeasurements. We propose a simple embedding strategy to better distinguish\nbetween numbers and units, which leads to a significant improvement in the\nprobing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.12694v1.pdf"
    },
    {
        "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
        "authors": [
            "Xiangyu Peng",
            "Chen Xing",
            "Prafulla Kumar Choubey",
            "Chien-Sheng Wu",
            "Caiming Xiong"
        ],
        "published": "2022-10-23T01:33:16Z",
        "summary": "Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2210.12587v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
        "authors": [
            "Joshua Robinson",
            "Christopher Michael Rytting",
            "David Wingate"
        ],
        "published": "2022-10-22T05:04:54Z",
        "summary": "While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.",
        "pdf_link": "https://arxiv.org/pdf/2210.12353v3.pdf"
    },
    {
        "title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
        "authors": [
            "Yue Yang",
            "Wenlin Yao",
            "Hongming Zhang",
            "Xiaoyang Wang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2022-10-21T21:33:10Z",
        "summary": "Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.12261v1.pdf"
    },
    {
        "title": "WikiWhy: Answering and Explaining Cause-and-Effect Questions",
        "authors": [
            "Matthew Ho",
            "Aditya Sharma",
            "Justin Chang",
            "Michael Saxon",
            "Sharon Levy",
            "Yujie Lu",
            "William Yang Wang"
        ],
        "published": "2022-10-21T17:59:03Z",
        "summary": "As large language models (LLMs) grow larger and more sophisticated, assessing\ntheir \"reasoning\" capabilities in natural language grows more challenging.\nRecent question answering (QA) benchmarks that attempt to assess reasoning are\noften limited by a narrow scope of covered situations and subject matters. We\nintroduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining\nwhy an answer is true in natural language. WikiWhy contains over 9,000 \"why\"\nquestion-answer-rationale triples, grounded on Wikipedia facts across a diverse\nset of topics. Each rationale is a set of supporting statements connecting the\nquestion to the answer. WikiWhy serves as a benchmark for the reasoning\ncapabilities of LLMs because it demands rigorous explicit rationales for each\nanswer to demonstrate the acquisition of implicit commonsense knowledge, which\nis unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%\nhuman-evaluated correctness in the end-to-end answer & explain condition,\nleaving significant room for future improvements.",
        "pdf_link": "https://arxiv.org/pdf/2210.12152v2.pdf"
    },
    {
        "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
        "authors": [
            "Alessandro Stolfo",
            "Zhijing Jin",
            "Kumar Shridhar",
            "Bernhard Schölkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2022-10-21T15:12:37Z",
        "summary": "We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen generating a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands, and\nmath operators on the output solution. By grounding the behavioral analysis in\na causal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of math\nword problems. Our analysis shows that robustness does not appear to\ncontinuously improve as a function of size, but the GPT-3 Davinci models (175B)\nachieve a dramatic improvement in both robustness and sensitivity compared to\nall other GPT variants.",
        "pdf_link": "https://arxiv.org/pdf/2210.12023v3.pdf"
    },
    {
        "title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering",
        "authors": [
            "Minchul Lee",
            "Kijong Han",
            "Myeong Cheol Shin"
        ],
        "published": "2022-10-21T10:46:41Z",
        "summary": "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a\nlimitation dealing with long inputs due to its attention mechanism. Longformer,\nETC and BigBird addressed this issue and effectively solved the quadratic\ndependency problem. However we find that these models are not sufficient, and\npropose LittleBird, a novel model based on BigBird with improved speed and\nmemory footprint while maintaining accuracy. In particular, we devise a more\nflexible and efficient position representation method based on Attention with\nLinear Biases (ALiBi). We also show that replacing the method of global\ninformation represented in the BigBird with pack and unpack attention is more\neffective. The proposed model can work on long inputs even after being\npre-trained on short inputs, and can be trained efficiently reusing existing\npre-trained language model for short inputs. This is a significant benefit for\nlow-resource languages where large amounts of long text data are difficult to\nobtain. As a result, our experiments show that LittleBird works very well in a\nvariety of languages, achieving high performance in question answering tasks,\nparticularly in KorQuAD2.0, Korean Question Answering Dataset for long\nparagraphs.",
        "pdf_link": "https://arxiv.org/pdf/2210.11870v2.pdf"
    },
    {
        "title": "Using Large Language Models to Enhance Programming Error Messages",
        "authors": [
            "Juho Leinonen",
            "Arto Hellas",
            "Sami Sarsa",
            "Brent Reeves",
            "Paul Denny",
            "James Prather",
            "Brett A. Becker"
        ],
        "published": "2022-10-20T23:17:26Z",
        "summary": "A key part of learning to program is learning to understand programming error\nmessages. They can be hard to interpret and identifying the cause of errors can\nbe time-consuming. One factor in this challenge is that the messages are\ntypically intended for an audience that already knows how to program, or even\nfor programming environments that then use the information to highlight areas\nin code. Researchers have been working on making these errors more novice\nfriendly since the 1960s, however progress has been slow. The present work\ncontributes to this stream of research by using large language models to\nenhance programming error messages with explanations of the errors and\nsuggestions on how to fix the error. Large language models can be used to\ncreate useful and novice-friendly enhancements to programming error messages\nthat sometimes surpass the original programming error messages in\ninterpretability and actionability. These results provide further evidence of\nthe benefits of large language models for computing educators, highlighting\ntheir use in areas known to be challenging for students. We further discuss the\nbenefits and downsides of large language models and highlight future streams of\nresearch for enhancing programming error messages.",
        "pdf_link": "https://arxiv.org/pdf/2210.11630v1.pdf"
    },
    {
        "title": "Large Language Models Can Self-Improve",
        "authors": [
            "Jiaxin Huang",
            "Shixiang Shane Gu",
            "Le Hou",
            "Yuexin Wu",
            "Xuezhi Wang",
            "Hongkun Yu",
            "Jiawei Han"
        ],
        "published": "2022-10-20T21:53:54Z",
        "summary": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2210.11610v2.pdf"
    },
    {
        "title": "ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications",
        "authors": [
            "Alex Gu",
            "Tamara Mitrovska",
            "Daniela Velez",
            "Jacob Andreas",
            "Armando Solar-Lezama"
        ],
        "published": "2022-10-20T17:59:19Z",
        "summary": "We introduce ObSynth, an interactive system leveraging the domain knowledge\nembedded in large language models (LLMs) to help users design object models\nfrom high level natural language prompts. This is an example of specification\nreification, the process of taking a high-level, potentially vague\nspecification and reifying it into a more concrete form. We evaluate ObSynth\nvia a user study, leading to three key findings: first, object models designed\nusing ObSynth are more detailed, showing that it often synthesizes fields users\nmight have otherwise omitted. Second, a majority of objects, methods, and\nfields generated by ObSynth are kept by the user in the final object model,\nhighlighting the quality of generated components. Third, ObSynth altered the\nworkflow of participants: they focus on checking that synthesized components\nwere correct rather than generating them from scratch, though ObSynth did not\nreduce the time participants took to generate object models.",
        "pdf_link": "https://arxiv.org/pdf/2210.11468v1.pdf"
    },
    {
        "title": "Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing",
        "authors": [
            "Frank van der Velde"
        ],
        "published": "2022-10-19T13:31:26Z",
        "summary": "Recently, a number of articles have argued that deep learning models such as\nGPT could also capture key aspects of language processing in the human mind and\nbrain. However, I will argue that these models are not suitable as neural\nmodels of human language. Firstly, because they fail on fundamental boundary\nconditions, such as the amount of learning they require. This would in fact\nimply that the mechanisms of GPT and brain language processing are\nfundamentally different. Secondly, because they do not possess the logistics of\naccess needed for compositional and productive human language processing.\nNeural architectures could possess logistics of access based on small-world\nlike network structures, in which processing does not consist of symbol\nmanipulation but of controlling the flow of activation. In this view, two\ncomplementary approaches would be needed to investigate the relation between\nbrain and cognition. Investigating learning methods could reveal how 'learned\ncognition' as found in deep learning could develop in the brain. However,\nneural architectures with logistics of access should also be developed to\naccount for 'productive cognition' as required for natural or artificial human\nlanguage processing. Later on, these approaches could perhaps be combined to\nsee how such architectures could develop by learning and development from a\nsimpler basis.",
        "pdf_link": "https://arxiv.org/pdf/2210.10543v1.pdf"
    },
    {
        "title": "Language Detoxification with Attribute-Discriminative Latent Space",
        "authors": [
            "Jin Myung Kwak",
            "Minseon Kim",
            "Sung Ju Hwang"
        ],
        "published": "2022-10-19T06:54:42Z",
        "summary": "Transformer-based Language Models (LMs) have achieved impressive results on\nnatural language understanding tasks, but they can also generate toxic text\nsuch as insults, threats, and profanity, limiting their real-world\napplications. To overcome this issue, a few text generation approaches aim to\ndetoxify toxic texts using additional LMs or perturbations. However, previous\nmethods require excessive memory, computations, and time which are serious\nbottlenecks in their real-world application. To address such limitations, we\npropose an effective yet efficient method for language detoxification using an\nattribute-discriminative latent space. Specifically, we project the latent\nspace of an original Transformer LM onto a discriminative latent space that\nwell-separates texts by their attributes using a projection block and an\nattribute discriminator. This allows the LM to control the text generation to\nbe non-toxic with minimal memory and computation overhead. We validate our\nmodel, Attribute-Discriminative Language Model (ADLM) on detoxified language\nand dialogue generation tasks, on which our method significantly outperforms\nbaselines both in performance and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2210.10329v2.pdf"
    },
    {
        "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models",
        "authors": [
            "Sharon Levy",
            "Emily Allaway",
            "Melanie Subbiah",
            "Lydia Chilton",
            "Desmond Patton",
            "Kathleen McKeown",
            "William Yang Wang"
        ],
        "published": "2022-10-18T17:59:31Z",
        "summary": "Understanding what constitutes safe text is an important issue in natural\nlanguage processing and can often prevent the deployment of models deemed\nharmful and unsafe. One such type of safety that has been scarcely studied is\ncommonsense physical safety, i.e. text that is not explicitly violent and\nrequires additional commonsense knowledge to comprehend that it leads to\nphysical harm. We create the first benchmark dataset, SafeText, comprising\nreal-life scenarios with paired safe and physically unsafe pieces of advice. We\nutilize SafeText to empirically study commonsense physical safety across\nvarious models designed for text generation and commonsense reasoning tasks. We\nfind that state-of-the-art large language models are susceptible to the\ngeneration of unsafe text and have difficulty rejecting unsafe advice. As a\nresult, we argue for further studies of safety and the assessment of\ncommonsense physical safety in models before release.",
        "pdf_link": "https://arxiv.org/pdf/2210.10045v1.pdf"
    },
    {
        "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
        "authors": [
            "Lan Jiang",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Rui Jiang"
        ],
        "published": "2022-10-18T07:53:15Z",
        "summary": "Even though the large-scale language models have achieved excellent\nperformances, they suffer from various adversarial attacks. A large body of\ndefense methods has been proposed. However, they are still limited due to\nredundant attack search spaces and the inability to defend against various\ntypes of attacks. In this work, we present a novel fine-tuning approach called\n\\textbf{RO}bust \\textbf{SE}letive fine-tuning (\\textbf{ROSE}) to address this\nissue. ROSE conducts selective updates when adapting pre-trained models to\ndownstream tasks, filtering out invaluable and unrobust updates of parameters.\nSpecifically, we propose two strategies: the first-order and second-order ROSE\nfor selecting target robust parameters. The experimental results show that ROSE\nachieves significant improvements in adversarial robustness on various\ndownstream NLP tasks, and the ensemble method even surpasses both variants\nabove. Furthermore, ROSE can be easily incorporated into existing fine-tuning\nmethods to improve their adversarial robustness further. The empirical analysis\nconfirms that ROSE eliminates unrobust spurious updates during fine-tuning,\nleading to solutions corresponding to flatter and wider optima than the\nconventional method. Code is available at\n\\url{https://github.com/jiangllan/ROSE}.",
        "pdf_link": "https://arxiv.org/pdf/2210.09658v1.pdf"
    },
    {
        "title": "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models",
        "authors": [
            "Zhiyuan Zhang",
            "Lingjuan Lyu",
            "Xingjun Ma",
            "Chenguang Wang",
            "Xu Sun"
        ],
        "published": "2022-10-18T02:44:38Z",
        "summary": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.\nIn Natural Language Processing (NLP), DNNs are often backdoored during the\nfine-tuning process of a large-scale Pre-trained Language Model (PLM) with\npoisoned samples. Although the clean weights of PLMs are readily available,\nexisting methods have ignored this information in defending NLP models against\nbackdoor attacks. In this work, we take the first step to exploit the\npre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language\nmodels. Specifically, we leverage the clean pre-trained weights via two\ncomplementary techniques: (1) a two-step Fine-mixing technique, which first\nmixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained\nweights, then fine-tunes the mixed weights on a small subset of clean data; (2)\nan Embedding Purification (E-PUR) technique, which mitigates potential\nbackdoors existing in the word embeddings. We compare Fine-mixing with typical\nbackdoor mitigation methods on three single-sentence sentiment classification\ntasks and two sentence-pair classification tasks and show that it outperforms\nthe baselines by a considerable margin in all scenarios. We also show that our\nE-PUR method can benefit existing mitigation methods. Our work establishes a\nsimple but strong baseline defense for secure fine-tuned NLP models against\nbackdoor attacks.",
        "pdf_link": "https://arxiv.org/pdf/2210.09545v1.pdf"
    },
    {
        "title": "Adversarial and Safely Scaled Question Generation",
        "authors": [
            "Sreehari Sankar",
            "Zhihang Dong"
        ],
        "published": "2022-10-17T22:51:45Z",
        "summary": "Question generation has recently gained a lot of research interest,\nespecially with the advent of large language models. In and of itself, question\ngeneration can be considered 'AI-hard', as there is a lack of unanimously\nagreed sense of what makes a question 'good' or 'bad'. In this paper, we tackle\ntwo fundamental problems in parallel: on one hand, we try to solve the scaling\nproblem, where question-generation and answering applications have to be\napplied to a massive amount of text without ground truth labeling. The usual\napproach to solve this problem is to either downsample or summarize. However,\nthere are critical risks of misinformation with these approaches. On the other\nhand, and related to the misinformation problem, we try to solve the 'safety'\nproblem, as many public institutions rely on a much higher level of accuracy\nfor the content they provide. We introduce an adversarial approach to tackle\nthe question generation safety problem with scale. Specifically, we designed a\nquestion-answering system that specifically prunes out unanswerable questions\nthat may be generated, and further increases the quality of the answers that\nare generated. We build a production-ready, easily-plugged pipeline that can be\nused on any given body of text, that is scalable and immune from generating any\nhate speech, profanity, or misinformation. Based on the results, we are able to\ngenerate more than six times the number of quality questions generated by the\nabstractive approach, with a perceived quality being 44% higher, according to a\nsurvey of 168 participants.",
        "pdf_link": "https://arxiv.org/pdf/2210.09467v1.pdf"
    },
    {
        "title": "CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model",
        "authors": [
            "Natasha Alkhatib",
            "Maria Mushtaq",
            "Hadi Ghauch",
            "Jean-Luc Danger"
        ],
        "published": "2022-10-17T21:21:37Z",
        "summary": "Due to the rising number of sophisticated customer functionalities,\nelectronic control units (ECUs) are increasingly integrated into modern\nautomotive systems. However, the high connectivity between the in-vehicle and\nthe external networks paves the way for hackers who could exploit in-vehicle\nnetwork protocols' vulnerabilities. Among these protocols, the Controller Area\nNetwork (CAN), known as the most widely used in-vehicle networking technology,\nlacks encryption and authentication mechanisms, making the communications\ndelivered by distributed ECUs insecure. Inspired by the outstanding performance\nof bidirectional encoder representations from transformers (BERT) for improving\nmany natural language processing tasks, we propose in this paper ``CAN-BERT\", a\ndeep learning based network intrusion detection system, to detect cyber attacks\non CAN bus protocol. We show that the BERT model can learn the sequence of\narbitration identifiers (IDs) in the CAN bus for anomaly detection using the\n``masked language model\" unsupervised training objective. The experimental\nresults on the ``Car Hacking: Attack \\& Defense Challenge 2020\" dataset show\nthat ``CAN-BERT\" outperforms state-of-the-art approaches. In addition to being\nable to identify in-vehicle intrusions in real-time within 0.8 ms to 3 ms w.r.t\nCAN ID sequence length, it can also detect a wide variety of cyberattacks with\nan F1-score of between 0.81 and 0.99.",
        "pdf_link": "https://arxiv.org/pdf/2210.09439v1.pdf"
    },
    {
        "title": "Deep Bidirectional Language-Knowledge Graph Pretraining",
        "authors": [
            "Michihiro Yasunaga",
            "Antoine Bosselut",
            "Hongyu Ren",
            "Xikun Zhang",
            "Christopher D Manning",
            "Percy Liang",
            "Jure Leskovec"
        ],
        "published": "2022-10-17T18:02:52Z",
        "summary": "Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.",
        "pdf_link": "https://arxiv.org/pdf/2210.09338v2.pdf"
    },
    {
        "title": "Prompting GPT-3 To Be Reliable",
        "authors": [
            "Chenglei Si",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Shuohang Wang",
            "Jianfeng Wang",
            "Jordan Boyd-Graber",
            "Lijuan Wang"
        ],
        "published": "2022-10-17T14:52:39Z",
        "summary": "Large language models (LLMs) show impressive abilities via few-shot\nprompting. Commercialized APIs such as OpenAI GPT-3 further increase their use\nin real-world language applications. However, the crucial problem of how to\nimprove the reliability of GPT-3 is still under-explored. While reliability is\na broad and vaguely defined term, we decompose reliability into four main\nfacets that correspond to the existing framework of ML safety and are\nwell-recognized to be important: generalizability, social biases, calibration,\nand factuality. Our core contribution is to establish simple and effective\nprompts that improve GPT-3's reliability as it: 1) generalizes\nout-of-distribution, 2) balances demographic distribution and uses natural\nlanguage instructions to reduce social biases, 3) calibrates output\nprobabilities, and 4) updates the LLM's factual knowledge and reasoning chains.\nWith appropriate prompts, GPT-3 is more reliable than smaller-scale supervised\nmodels on all these facets. We release all processed datasets, evaluation\nscripts, and model predictions. Our systematic empirical study not only sheds\nnew insights on the reliability of prompting LLMs, but more importantly, our\nprompting strategies can help practitioners more reliably use LLMs like GPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2210.09150v2.pdf"
    },
    {
        "title": "Exposing Influence Campaigns in the Age of LLMs: A Behavioral-Based AI Approach to Detecting State-Sponsored Trolls",
        "authors": [
            "Fatima Ezzeddine",
            "Luca Luceri",
            "Omran Ayoub",
            "Ihab Sbeity",
            "Gianluca Nogara",
            "Emilio Ferrara",
            "Silvia Giordano"
        ],
        "published": "2022-10-17T07:01:17Z",
        "summary": "The detection of state-sponsored trolls operating in influence campaigns on\nsocial media is a critical and unsolved challenge for the research community,\nwhich has significant implications beyond the online realm. To address this\nchallenge, we propose a new AI-based solution that identifies troll accounts\nsolely through behavioral cues associated with their sequences of sharing\nactivity, encompassing both their actions and the feedback they receive from\nothers. Our approach does not incorporate any textual content shared and\nconsists of two steps: First, we leverage an LSTM-based classifier to determine\nwhether account sequences belong to a state-sponsored troll or an organic,\nlegitimate user. Second, we employ the classified sequences to calculate a\nmetric named the \"Troll Score\", quantifying the degree to which an account\nexhibits troll-like behavior. To assess the effectiveness of our method, we\nexamine its performance in the context of the 2016 Russian interference\ncampaign during the U.S. Presidential election. Our experiments yield\ncompelling results, demonstrating that our approach can identify account\nsequences with an AUC close to 99% and accurately differentiate between Russian\ntrolls and organic users with an AUC of 91%. Notably, our behavioral-based\napproach holds a significant advantage in the ever-evolving landscape, where\ntextual and linguistic properties can be easily mimicked by Large Language\nModels (LLMs): In contrast to existing language-based techniques, it relies on\nmore challenging-to-replicate behavioral cues, ensuring greater resilience in\nidentifying influence campaigns, especially given the potential increase in the\nusage of LLMs for generating inauthentic content. Finally, we assessed the\ngeneralizability of our solution to various entities driving different\ninformation operations and found promising results that will guide future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2210.08786v6.pdf"
    },
    {
        "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
        "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Y. Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
        ],
        "published": "2022-10-17T03:44:30Z",
        "summary": "Language models (LMs) now excel at many tasks such as few-shot learning,\nquestion answering, reasoning, and dialog. However, they sometimes generate\nunsupported or misleading content. A user cannot easily determine whether their\noutputs are trustworthy or not, because most LMs do not have any built-in\nmechanism for attribution to external evidence. To enable attribution while\nstill preserving all the powerful advantages of recent generation models, we\npropose RARR (Retrofit Attribution using Research and Revision), a system that\n1) automatically finds attribution for the output of any text generation model\nand 2) post-edits the output to fix unsupported content while preserving the\noriginal output as much as possible. When applied to the output of several\nstate-of-the-art LMs on a diverse set of generation tasks, we find that RARR\nsignificantly improves attribution while otherwise preserving the original\ninput to a much greater degree than previously explored edit models.\nFurthermore, the implementation of RARR requires only a handful of training\nexamples, a large language model, and standard web search.",
        "pdf_link": "https://arxiv.org/pdf/2210.08726v3.pdf"
    },
    {
        "title": "A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems",
        "authors": [
            "Hong Liu",
            "Yucheng Cai",
            "Zhijian Ou",
            "Yi Huang",
            "Junlan Feng"
        ],
        "published": "2022-10-17T01:57:50Z",
        "summary": "Building user simulators (USs) for reinforcement learning (RL) of\ntask-oriented dialog systems (DSs) has gained more and more attention, which,\nhowever, still faces several fundamental challenges. First, it is unclear\nwhether we can leverage pretrained language models to design, for example,\nGPT-2 based USs, to catch up and interact with the recently advanced GPT-2\nbased DSs. Second, an important ingredient in a US is that the user goal can be\neffectively incorporated and tracked; but how to flexibly integrate goal state\ntracking and develop an end-to-end trainable US for multi-domains has remained\nto be a challenge. In this work, we propose a generative user simulator (GUS)\nwith GPT-2 based architecture and goal state tracking towards addressing the\nabove two challenges. Extensive experiments are conducted on MultiWOZ2.1.\nDifferent DSs are trained via RL with GUS, the classic agenda-based user\nsimulator (ABUS) and other ablation simulators respectively, and are compared\nfor cross-model evaluation, corpus-based evaluation and human evaluation. The\nGUS achieves superior results in all three evaluation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2210.08692v2.pdf"
    },
    {
        "title": "Temporal Word Meaning Disambiguation using TimeLMs",
        "authors": [
            "Mihir Godbole",
            "Parth Dandavate",
            "Aditya Kane"
        ],
        "published": "2022-10-15T06:34:59Z",
        "summary": "Meaning of words constantly changes given the events in modern civilization.\nLarge Language Models use word embeddings, which are often static and thus\ncannot cope with this semantic change. Thus,it is important to resolve\nambiguity in word meanings. This paper is an effort in this direction, where we\nexplore methods for word sense disambiguation for the EvoNLP shared task. We\nconduct rigorous ablations for two solutions to this problem. We see that an\napproach using time-aware language models helps this task. Furthermore, we\nexplore possible future directions to this problem.",
        "pdf_link": "https://arxiv.org/pdf/2210.08207v2.pdf"
    },
    {
        "title": "MiQA: A Benchmark for Inference on Metaphorical Questions",
        "authors": [
            "Iulia-Maria Comsa",
            "Julian Martin Eisenschlos",
            "Srini Narayanan"
        ],
        "published": "2022-10-14T17:46:05Z",
        "summary": "We propose a benchmark to assess the capability of large language models to\nreason with conventional metaphors. Our benchmark combines the previously\nisolated topics of metaphor detection and commonsense reasoning into a single\ntask that requires a model to make inferences by accurately selecting between\nthe literal and metaphorical register. We examine the performance of\nstate-of-the-art pre-trained models on binary-choice tasks and find a large\ndiscrepancy between the performance of small and very large models, going from\nchance to near-human level. We also analyse the largest model in a generative\nsetting and find that although human performance is approached, careful\nmultiple-shot prompting is required.",
        "pdf_link": "https://arxiv.org/pdf/2210.07993v1.pdf"
    },
    {
        "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
        "authors": [
            "Sachin Kumar",
            "Vidhisha Balachandran",
            "Lucille Njoo",
            "Antonios Anastasopoulos",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-14T10:43:39Z",
        "summary": "Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have explored these harms and called for their\nmitigation via development of safer, fairer models. Going beyond enumerating\nthe risks of harms, this work provides a survey of practical methods for\naddressing potential threats and societal harms from language generation\nmodels. We draw on several prior works' taxonomies of language model risks to\npresent a structured overview of strategies for detecting and ameliorating\ndifferent kinds of risks/harms of language generators. Bridging diverse strands\nof research, this survey aims to serve as a practical guide for both LM\nresearchers and practitioners, with explanations of different mitigation\nstrategies' motivations, their limitations, and open problems for future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2210.07700v2.pdf"
    },
    {
        "title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation",
        "authors": [
            "Tianxiang Sun",
            "Junliang He",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2022-10-14T08:24:11Z",
        "summary": "Automatic evaluation metrics are crucial to the development of generative\nsystems. In recent years, pre-trained language model (PLM) based metrics, such\nas BERTScore, have been commonly adopted in various generation tasks. However,\nit has been demonstrated that PLMs encode a range of stereotypical societal\nbiases, leading to a concern on the fairness of PLMs as metrics. To that end,\nthis work presents the first systematic study on the social bias in PLM-based\nmetrics. We demonstrate that popular PLM-based metrics exhibit significantly\nhigher social bias than traditional metrics on 6 sensitive attributes, namely\nrace, gender, religion, physical appearance, age, and socioeconomic status.\nIn-depth analysis suggests that choosing paradigms (matching, regression, or\ngeneration) of the metric has a greater impact on fairness than choosing PLMs.\nIn addition, we develop debiasing adapters that are injected into PLM layers,\nmitigating bias in PLM-based metrics while retaining high performance for\nevaluating text generation.",
        "pdf_link": "https://arxiv.org/pdf/2210.07626v1.pdf"
    },
    {
        "title": "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation",
        "authors": [
            "Mojtaba Valipour",
            "Mehdi Rezagholizadeh",
            "Ivan Kobyzev",
            "Ali Ghodsi"
        ],
        "published": "2022-10-14T06:29:22Z",
        "summary": "With the ever-growing size of pretrained models (PMs), fine-tuning them has\nbecome more expensive and resource-hungry. As a remedy, low-rank adapters\n(LoRA) keep the main pretrained weights of the model frozen and just introduce\nsome learnable truncated SVD modules (so-called LoRA blocks) to the model.\nWhile LoRA blocks are parameter-efficient, they suffer from two major problems:\nfirst, the size of these blocks is fixed and cannot be modified after training\n(for example, if we need to change the rank of LoRA blocks, then we need to\nre-train them from scratch); second, optimizing their rank requires an\nexhaustive search and effort. In this work, we introduce a dynamic low-rank\nadaptation (DyLoRA) technique to address these two problems together. Our\nDyLoRA method trains LoRA blocks for a range of ranks instead of a single rank\nby sorting the representation learned by the adapter module at different ranks\nduring training. We evaluate our solution on different natural language\nunderstanding (GLUE benchmark) and language generation tasks (E2E, DART and\nWebNLG) using different pretrained models such as RoBERTa and GPT with\ndifferent sizes. Our results show that we can train dynamic search-free models\nwith DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA\nwithout significantly compromising performance. Moreover, our models can\nperform consistently well on a much larger range of ranks compared to LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2210.07558v2.pdf"
    },
    {
        "title": "\"John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility",
        "authors": [
            "Himanshu Gupta",
            "Neeraj Varshney",
            "Swaroop Mishra",
            "Kuntal Kumar Pal",
            "Saurabh Arjun Sawant",
            "Kevin Scaria",
            "Siddharth Goyal",
            "Chitta Baral"
        ],
        "published": "2022-10-14T02:46:06Z",
        "summary": "In current NLP research, large-scale language models and their abilities are\nwidely being discussed. Some recent works have also found notable failures of\nthese models. Often these failure examples involve complex reasoning abilities.\nThis work focuses on a simple commonsense ability, reasoning about when an\naction (or its effect) is feasible. To this end, we introduce FeasibilityQA, a\nquestion-answering dataset involving binary classification (BCQ) and\nmulti-choice multi-correct questions (MCQ) that test understanding of\nfeasibility. We show that even state-of-the-art models such as GPT-3, GPT-2,\nand T5 struggle to answer the feasibility questions correctly. Specifically, on\nMCQ and BCQ questions, GPT-3 achieves an accuracy of just (19%, 62%) and (25%,\n64%) in zero-shot and few-shot settings, respectively. We also evaluate models\nby providing relevant knowledge statements required to answer the question. We\nfind that the additional knowledge leads to a 7% gain in performance, but the\noverall performance still remains low. These results make one wonder how much\ncommonsense knowledge about action feasibility is encoded in state-of-the-art\nmodels and how well they can reason about it.",
        "pdf_link": "https://arxiv.org/pdf/2210.07471v2.pdf"
    },
    {
        "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
        "authors": [
            "Abhijeet Awasthi",
            "Nitish Gupta",
            "Bidisha Samanta",
            "Shachi Dave",
            "Sunita Sarawagi",
            "Partha Talukdar"
        ],
        "published": "2022-10-13T19:34:14Z",
        "summary": "Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be a key mechanism for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, translation services may continue\nto be brittle due to domain mismatch between task-specific input text and\ngeneral-purpose text used for training translation models. For multilingual\nsemantic parsing, we demonstrate the effectiveness and flexibility offered by\nlarge language models (LLMs) for translating English datasets into several\nlanguages via few-shot prompting. Through extensive comparisons on two public\ndatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show\nthat our method of translating data using LLMs outperforms a strong\ntranslate-train baseline on 41 out of 50 languages. We study the key design\nchoices that enable more effective multilingual data translation via prompted\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.07313v2.pdf"
    },
    {
        "title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models",
        "authors": [
            "Haozhe An",
            "Zongxia Li",
            "Jieyu Zhao",
            "Rachel Rudinger"
        ],
        "published": "2022-10-13T18:04:48Z",
        "summary": "A common limitation of diagnostic tests for detecting social biases in NLP\nmodels is that they may only detect stereotypic associations that are\npre-specified by the designer of the test. Since enumerating all possible\nproblematic associations is infeasible, it is likely these tests fail to detect\nbiases that are present in a model but not pre-specified by the designer. To\naddress this limitation, we propose SODAPOP (SOcial bias Discovery from Answers\nabout PeOPle) in social commonsense question-answering. Our pipeline generates\nmodified instances from the Social IQa dataset (Sap et al., 2019) by (1)\nsubstituting names associated with different demographic groups, and (2)\ngenerating many distractor answers from a masked language model. By using a\nsocial commonsense model to score the generated distractors, we are able to\nuncover the model's stereotypic associations between demographic groups and an\nopen set of words. We also test SODAPOP on debiased models and show the\nlimitations of multiple state-of-the-art debiasing algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2210.07269v2.pdf"
    },
    {
        "title": "Mass-Editing Memory in a Transformer",
        "authors": [
            "Kevin Meng",
            "Arnab Sen Sharma",
            "Alex Andonian",
            "Yonatan Belinkov",
            "David Bau"
        ],
        "published": "2022-10-13T17:55:53Z",
        "summary": "Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.",
        "pdf_link": "https://arxiv.org/pdf/2210.07229v2.pdf"
    },
    {
        "title": "Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods",
        "authors": [
            "Nils Feldhus",
            "Leonhard Hennig",
            "Maximilian Dustin Nasert",
            "Christopher Ebert",
            "Robert Schwarzenberg",
            "Sebastian Möller"
        ],
        "published": "2022-10-13T17:48:15Z",
        "summary": "Saliency maps can explain a neural model's predictions by identifying\nimportant input features. They are difficult to interpret for laypeople,\nespecially for instances with many features. In order to make them more\naccessible, we formalize the underexplored task of translating saliency maps\ninto natural language and compare methods that address two key challenges of\nthis approach -- what and how to verbalize. In both automatic and human\nevaluation setups, using token-level attributions from text classification\ntasks, we compare two novel methods (search-based and instruction-based\nverbalizations) against conventional feature importance representations\n(heatmap visualizations and extractive rationales), measuring simulatability,\nfaithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to\ngenerate saliency map verbalizations yields plausible explanations which\ninclude associations, abstractive summarization and commonsense reasoning,\nachieving by far the highest human ratings, but they are not faithfully\ncapturing numeric information and are inconsistent in their interpretation of\nthe task. In comparison, our search-based, model-free verbalization approach\nefficiently completes templated verbalizations, is faithful by design, but\nfalls short in helpfulness and simulatability. Our results suggest that\nsaliency map verbalization makes feature attribution explanations more\ncomprehensible and less cognitively challenging to humans than conventional\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2210.07222v3.pdf"
    },
    {
        "title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
        "authors": [
            "Andy Rosenbaum",
            "Saleh Soltan",
            "Wael Hamza",
            "Amir Saffari",
            "Marco Damonte",
            "Isabel Groves"
        ],
        "published": "2022-10-13T15:01:03Z",
        "summary": "A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.07074v2.pdf"
    },
    {
        "title": "Spontaneous Emerging Preference in Two-tower Language Model",
        "authors": [
            "Zhengqi He",
            "Taro Toyoizumi"
        ],
        "published": "2022-10-13T13:55:19Z",
        "summary": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.",
        "pdf_link": "https://arxiv.org/pdf/2210.07041v1.pdf"
    },
    {
        "title": "Sample-Then-Optimize Batch Neural Thompson Sampling",
        "authors": [
            "Zhongxiang Dai",
            "Yao Shu",
            "Bryan Kian Hsiang Low",
            "Patrick Jaillet"
        ],
        "published": "2022-10-13T09:01:58Z",
        "summary": "Bayesian optimization (BO), which uses a Gaussian process (GP) as a surrogate\nto model its objective function, is popular for black-box optimization.\nHowever, due to the limitations of GPs, BO underperforms in some problems such\nas those with categorical, high-dimensional or image inputs. To this end,\nrecent works have used the highly expressive neural networks (NNs) as the\nsurrogate model and derived theoretical guarantees using the theory of neural\ntangent kernel (NTK). However, these works suffer from the limitations of the\nrequirement to invert an extremely large parameter matrix and the restriction\nto the sequential (rather than batch) setting. To overcome these limitations,\nwe introduce two algorithms based on the Thompson sampling (TS) policy named\nSample-Then-Optimize Batch Neural TS (STO-BNTS) and STO-BNTS-Linear. To choose\nan input query, we only need to train an NN (resp. a linear model) and then\nchoose the query by maximizing the trained NN (resp. linear model), which is\nequivalently sampled from the GP posterior with the NTK as the kernel function.\nAs a result, our algorithms sidestep the need to invert the large parameter\nmatrix yet still preserve the validity of the TS policy. Next, we derive regret\nupper bounds for our algorithms with batch evaluations, and use insights from\nbatch BO and NTK to show that they are asymptotically no-regret under certain\nconditions. Finally, we verify their empirical effectiveness using practical\nAutoML and reinforcement learning experiments.",
        "pdf_link": "https://arxiv.org/pdf/2210.06850v1.pdf"
    },
    {
        "title": "Assessing Out-of-Domain Language Model Performance from Few Examples",
        "authors": [
            "Prasann Singhal",
            "Jarad Forristal",
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2022-10-13T04:45:26Z",
        "summary": "While pretrained language models have exhibited impressive generalization\ncapabilities, they still behave unpredictably under certain domain shifts. In\nparticular, a model may learn a reasoning process on in-domain training data\nthat does not hold for out-of-domain test data. We address the task of\npredicting out-of-domain (OOD) performance in a few-shot fashion: given a few\ntarget-domain examples and a set of models with similar training performance,\ncan we understand how these models will perform on OOD test data? We benchmark\nthe performance on this task when looking at model accuracy on the few-shot\nexamples, then investigate how to incorporate analysis of the models' behavior\nusing feature attributions to better tackle this problem. Specifically, we\nexplore a set of \"factors\" designed to reveal model agreement with certain\npathological heuristics that may indicate worse generalization capabilities. On\ntextual entailment, paraphrase recognition, and a synthetic classification\ntask, we show that attribution-based factors can help rank relative model OOD\nperformance. However, accuracy on a few-shot test set is a surprisingly strong\nbaseline, particularly when the system designer does not have in-depth prior\nknowledge about the domain shift.",
        "pdf_link": "https://arxiv.org/pdf/2210.06725v1.pdf"
    },
    {
        "title": "Language Models are Realistic Tabular Data Generators",
        "authors": [
            "Vadim Borisov",
            "Kathrin Seßler",
            "Tobias Leemann",
            "Martin Pawelczyk",
            "Gjergji Kasneci"
        ],
        "published": "2022-10-12T15:03:28Z",
        "summary": "Tabular data is among the oldest and most ubiquitous forms of data. However,\nthe generation of synthetic samples with the original data's characteristics\nremains a significant challenge for tabular data. While many generative models\nfrom the computer vision domain, such as variational autoencoders or generative\nadversarial networks, have been adapted for tabular data generation, less\nresearch has been directed towards recent transformer-based large language\nmodels (LLMs), which are also generative in nature. To this end, we propose\nGReaT (Generation of Realistic Tabular data), which exploits an auto-regressive\ngenerative LLM to sample synthetic and yet highly realistic tabular data.\nFurthermore, GReaT can model tabular data distributions by conditioning on any\nsubset of features; the remaining features are sampled without additional\noverhead. We demonstrate the effectiveness of the proposed approach in a series\nof experiments that quantify the validity and quality of the produced data\nsamples from multiple angles. We find that GReaT maintains state-of-the-art\nperformance across numerous real-world and synthetic data sets with\nheterogeneous feature types coming in various sizes.",
        "pdf_link": "https://arxiv.org/pdf/2210.06280v2.pdf"
    },
    {
        "title": "A context-aware knowledge transferring strategy for CTC-based ASR",
        "authors": [
            "Ke-Han Lu",
            "Kuan-Yu Chen"
        ],
        "published": "2022-10-12T14:31:38Z",
        "summary": "Non-autoregressive automatic speech recognition (ASR) modeling has received\nincreasing attention recently because of its fast decoding speed and superior\nperformance. Among representatives, methods based on the connectionist temporal\nclassification (CTC) are still a dominating stream. However, the theoretically\ninherent flaw, the assumption of independence between tokens, creates a\nperformance barrier for the school of works. To mitigate the challenge, we\npropose a context-aware knowledge transferring strategy, consisting of a\nknowledge transferring module and a context-aware training strategy, for\nCTC-based ASR. The former is designed to distill linguistic information from a\npre-trained language model, and the latter is framed to modulate the\nlimitations caused by the conditional independence assumption. As a result, a\nknowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is\npresented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2\ndatasets demonstrate the effectiveness of the proposed method.",
        "pdf_link": "https://arxiv.org/pdf/2210.06244v1.pdf"
    },
    {
        "title": "AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning",
        "authors": [
            "Tao Yang",
            "Jinghao Deng",
            "Xiaojun Quan",
            "Qifan Wang",
            "Shaoliang Nie"
        ],
        "published": "2022-10-12T02:54:41Z",
        "summary": "Fine-tuning large pre-trained language models on downstream tasks is apt to\nsuffer from overfitting when limited training data is available. While dropout\nproves to be an effective antidote by randomly dropping a proportion of units,\nexisting research has not examined its effect on the self-attention mechanism.\nIn this paper, we investigate this problem through self-attention attribution\nand find that dropping attention positions with low attribution scores can\naccelerate training and increase the risk of overfitting. Motivated by this\nobservation, we propose Attribution-Driven Dropout (AD-DROP), which randomly\ndiscards some high-attribution positions to encourage the model to make\npredictions by relying more on low-attribution positions to reduce overfitting.\nWe also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to\navoid dropping high-attribution positions excessively. Extensive experiments on\nvarious benchmarks show that AD-DROP yields consistent improvements over\nbaselines. Analysis further confirms that AD-DROP serves as a strategic\nregularizer to prevent overfitting during fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2210.05883v1.pdf"
    },
    {
        "title": "SEAL : Interactive Tool for Systematic Error Analysis and Labeling",
        "authors": [
            "Nazneen Rajani",
            "Weixin Liang",
            "Lingjiao Chen",
            "Meg Mitchell",
            "James Zou"
        ],
        "published": "2022-10-11T23:51:44Z",
        "summary": "With the advent of Transformers, large language models (LLMs) have saturated\nwell-known NLP benchmarks and leaderboards with high aggregate performance.\nHowever, many times these models systematically fail on tail data or rare\ngroups not obvious in aggregate evaluation. Identifying such problematic data\ngroups is even more challenging when there are no explicit labels (e.g.,\nethnicity, gender, etc.) and further compounded for NLP datasets due to the\nlack of visual features to characterize failure modes (e.g., Asian males,\nanimals indoors, waterbirds on land, etc.). This paper introduces an\ninteractive Systematic Error Analysis and Labeling (\\seal) tool that uses a\ntwo-step approach to first identify high error slices of data and then, in the\nsecond step, introduce methods to give human-understandable semantics to those\nunderperforming slices. We explore a variety of methods for coming up with\ncoherent semantics for the error groups using language models for semantic\nlabeling and a text-to-image model for generating visual features. SEAL toolkit\nand demo screencast is available at https://huggingface.co/spaces/nazneen/seal.",
        "pdf_link": "https://arxiv.org/pdf/2210.05839v1.pdf"
    },
    {
        "title": "Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models",
        "authors": [
            "Isabel Papadimitriou",
            "Kezia Lopez",
            "Dan Jurafsky"
        ],
        "published": "2022-10-11T17:06:38Z",
        "summary": "While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control language model. With our case\nstudies, we hope to bring to light the fine-grained ways in which multilingual\nmodels can be biased,and encourage more linguistically-aware fluency\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2210.05619v2.pdf"
    },
    {
        "title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models",
        "authors": [
            "Yuanxin Liu",
            "Fandong Meng",
            "Zheng Lin",
            "Jiangnan Li",
            "Peng Fu",
            "Yanan Cao",
            "Weiping Wang",
            "Jie Zhou"
        ],
        "published": "2022-10-11T07:26:34Z",
        "summary": "Despite the remarkable success of pre-trained language models (PLMs), they\nstill face two challenges: First, large-scale PLMs are inefficient in terms of\nmemory footprint and computation. Second, on the downstream tasks, PLMs tend to\nrely on the dataset bias and struggle to generalize to out-of-distribution\n(OOD) data. In response to the efficiency problem, recent studies show that\ndense PLMs can be replaced with sparse subnetworks without hurting the\nperformance. Such subnetworks can be found in three scenarios: 1) the\nfine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even\ninside 3) PLMs without any parameter fine-tuning. However, these results are\nonly obtained in the in-distribution (ID) setting. In this paper, we extend the\nstudy on PLMs subnetworks to the OOD setting, investigating whether sparsity\nand robustness to dataset bias can be achieved simultaneously. To this end, we\nconduct extensive experiments with the pre-trained BERT model on three natural\nlanguage understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse\nand robust subnetworks (SRNets) can consistently be found in BERT}, across the\naforementioned three scenarios, using different training and compression\nmethods. Furthermore, we explore the upper bound of SRNets using the OOD\ninformation and show that \\textbf{there exist sparse and almost unbiased BERT\nsubnetworks}. Finally, we present 1) an analytical study that provides insights\non how to promote the efficiency of SRNets searching process and 2) a solution\nto improve subnetworks' performance at high sparsity. The code is available at\nhttps://github.com/llyx97/sparse-and-robust-PLM.",
        "pdf_link": "https://arxiv.org/pdf/2210.05211v1.pdf"
    },
    {
        "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
        "authors": [
            "Zhaowei Wang"
        ],
        "published": "2022-10-11T06:47:23Z",
        "summary": "Legal case retrieval, which aims to retrieve relevant cases given a query\ncase, plays an essential role in the legal system. While recent research\nefforts improve the performance of traditional ad-hoc retrieval models, legal\ncase retrieval is still challenging since queries are legal cases, which\ncontain hundreds of tokens. Legal cases are much longer and more complicated\nthan keywords queries. Apart from that, the definition of legal relevance is\nbeyond the general definition. In addition to general topical relevance, the\nrelevant cases also involve similar situations and legal elements, which can\nsupport the judgment of the current case. In this paper, we propose an\ninteraction-focused network for legal case retrieval with a multi-view\ncontrastive learning objective. The contrastive learning views, including\ncase-view and element-view, aim to overcome the above challenges. The case-view\ncontrastive learning minimizes the hidden space distance between relevant legal\ncase representations produced by a pre-trained language model (PLM) encoder.\nThe element-view builds positive and negative instances by changing legal\nelements of cases to help the network better compute legal relevance. To\nachieve this, we employ a legal element knowledge-aware indicator to detect\nlegal elements of cases. We conduct extensive experiments on the benchmark of\nrelevant case retrieval. Evaluation results indicate our proposed method\nobtains significant improvement over the existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2210.05188v1.pdf"
    },
    {
        "title": "Generating Executable Action Plans with Environmentally-Aware Language Models",
        "authors": [
            "Maitrey Gramopadhye",
            "Daniel Szafir"
        ],
        "published": "2022-10-10T18:56:57Z",
        "summary": "Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable,\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat agents are better able to execute. Our approach involves integrating\nenvironmental objects and object relations as additional inputs into LLM action\nplan generation to provide the system with an awareness of its surroundings,\nresulting in plans where each generated action is mapped to objects present in\nthe scene. We also design a novel scoring function that, along with generating\nthe action steps and associating them with objects, helps the system\ndisambiguate among object instances and take into account their states. We\nevaluated our approach using the VirtualHome simulator and the ActivityPrograms\nknowledge base and found that action plans generated from our system had a 310%\nimprovement in executability and a 147% improvement in correctness over prior\nwork. The complete code and a demo of our method is publicly available at\nhttps://github.com/hri-ironlab/scene_aware_language_planner.",
        "pdf_link": "https://arxiv.org/pdf/2210.04964v2.pdf"
    },
    {
        "title": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks",
        "authors": [
            "Charith Peris",
            "Lizhen Tan",
            "Thomas Gueudre",
            "Turan Gojayev",
            "Pan Wei",
            "Gokmen Oz"
        ],
        "published": "2022-10-10T16:49:52Z",
        "summary": "Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2210.04834v3.pdf"
    },
    {
        "title": "Readability Controllable Biomedical Document Summarization",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2022-10-10T14:03:20Z",
        "summary": "Different from general documents, it is recognised that the ease with which\npeople can understand a biomedical text is eminently varied, owing to the\nhighly technical nature of biomedical documents and the variance of readers'\ndomain knowledge. However, existing biomedical document summarization systems\nhave paid little attention to readability control, leaving users with summaries\nthat are incompatible with their levels of expertise. In recognition of this\nurgent demand, we introduce a new task of readability controllable\nsummarization for biomedical documents, which aims to recognise users'\nreadability demands and generate summaries that better suit their needs:\ntechnical summaries for experts and plain language summaries (PLS) for laymen.\nTo establish this task, we construct a corpus consisting of biomedical papers\nwith technical summaries and PLSs written by the authors, and benchmark\nmultiple advanced controllable abstractive and extractive summarization models\nbased on pre-trained language models (PLMs) with prevalent controlling and\ngeneration techniques. Moreover, we propose a novel masked language model (MLM)\nbased metric and its variant to effectively evaluate the readability\ndiscrepancy between lay and technical summaries. Experimental results from\nautomated and human evaluations show that though current control techniques\nallow for a certain degree of readability adjustment during generation, the\nperformance of existing controllable summarization methods is far from\ndesirable in this task.",
        "pdf_link": "https://arxiv.org/pdf/2210.04705v3.pdf"
    },
    {
        "title": "DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities",
        "authors": [
            "Mohsinul Kabir",
            "Tasnim Ahmed",
            "Md. Bakhtiar Hasan",
            "Md Tahmid Rahman Laskar",
            "Tarun Kumar Joarder",
            "Hasan Mahmud",
            "Kamrul Hasan"
        ],
        "published": "2022-10-10T08:23:57Z",
        "summary": "Mental health research through data-driven methods has been hindered by a\nlack of standard typology and scarcity of adequate data. In this study, we\nleverage the clinical articulation of depression to build a typology for social\nmedia texts for detecting the severity of depression. It emulates the standard\nclinical assessment procedure Diagnostic and Statistical Manual of Mental\nDisorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle\nindications of depressive disorders from tweets. Along with the typology, we\npresent a new dataset of 40191 tweets labeled by expert annotators. Each tweet\nis labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels\nare considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.\nAn associated confidence score is provided with each label to validate the\nquality of annotation. We examine the quality of the dataset via representing\nsummary statistics while setting strong baseline results using attention-based\nmodels like BERT and DistilBERT. Finally, we extensively address the\nlimitations of the study to provide directions for further research.",
        "pdf_link": "https://arxiv.org/pdf/2210.05372v1.pdf"
    },
    {
        "title": "FairGer: Using NLP to Measure Support for Women and Migrants in 155 Years of German Parliamentary Debates",
        "authors": [
            "Dominik Beese",
            "Ole Pütz",
            "Steffen Eger"
        ],
        "published": "2022-10-09T22:02:58Z",
        "summary": "We measure support with women and migrants in German political debates over\nthe last 155 years. To do so, we (1) provide a gold standard of 1205 text\nsnippets in context, annotated for support with our target groups, (2) train a\nBERT model on our annotated data, with which (3) we infer large-scale trends.\nThese show that support with women is stronger than support with migrants, but\nboth have steadily increased over time. While we hardly find any direct\nanti-support with women, there is more polarization when it comes to migrants.\nWe also discuss the difficulty of annotation as a result of ambiguity in\npolitical discourse and indirectness, i.e., politicians' tendency to relate\nstances attributed to political opponents. Overall, our results indicate that\nGerman society, as measured from its political elite, has become fairer over\ntime.",
        "pdf_link": "https://arxiv.org/pdf/2210.04359v1.pdf"
    },
    {
        "title": "Quantifying Social Biases Using Templates is Unreliable",
        "authors": [
            "Preethi Seshadri",
            "Pouya Pezeshkpour",
            "Sameer Singh"
        ],
        "published": "2022-10-09T20:05:29Z",
        "summary": "Recently, there has been an increase in efforts to understand how large\nlanguage models (LLMs) propagate and amplify social biases. Several works have\nutilized templates for fairness evaluation, which allow researchers to quantify\nsocial biases in the absence of test sets with protected attribute labels.\nWhile template evaluation can be a convenient and helpful diagnostic tool to\nunderstand model deficiencies, it often uses a simplistic and limited set of\ntemplates. In this paper, we study whether bias measurements are sensitive to\nthe choice of templates used for benchmarking. Specifically, we investigate the\ninstability of bias measurements by manually modifying templates proposed in\nprevious works in a semantically-preserving manner and measuring bias across\nthese modifications. We find that bias values and resulting conclusions vary\nconsiderably across template modifications on four tasks, ranging from an 81%\nreduction (NLI) to a 162% increase (MLM) in (task-specific) bias measurements.\nOur results indicate that quantifying fairness in LLMs, as done in current\npractice, can be brittle and needs to be approached with more care and caution.",
        "pdf_link": "https://arxiv.org/pdf/2210.04337v1.pdf"
    },
    {
        "title": "Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection",
        "authors": [
            "Omkar Gokhale",
            "Aditya Kane",
            "Shantanu Patankar",
            "Tanmay Chavan",
            "Raviraj Joshi"
        ],
        "published": "2022-10-09T13:53:06Z",
        "summary": "Pre-training large neural language models, such as BERT, has led to\nimpressive gains on many natural language processing (NLP) tasks. Although this\nmethod has proven to be effective for many domains, it might not always provide\ndesirable benefits. In this paper, we study the effects of hateful pre-training\non low-resource hate speech classification tasks. While previous studies on the\nEnglish language have emphasized its importance, we aim to augment their\nobservations with some non-obvious insights. We evaluate different variations\nof tweet-based BERT models pre-trained on hateful, non-hateful, and mixed\nsubsets of a 40M tweet dataset. This evaluation is carried out for the Indian\nlanguages Hindi and Marathi. This paper is empirical evidence that hateful\npre-training is not the best pre-training option for hate speech detection. We\nshow that pre-training on non-hateful text from the target domain provides\nsimilar or better results. Further, we introduce HindTweetBERT and\nMahaTweetBERT, the first publicly available BERT models pre-trained on Hindi\nand Marathi tweets, respectively. We show that they provide state-of-the-art\nperformance on hate speech classification tasks. We also release hateful BERT\nfor the two languages and a gold hate speech evaluation benchmark HateEval-Hi\nand HateEval-Mr consisting of manually labeled 2000 tweets each. The models and\ndata are available at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2210.04267v3.pdf"
    },
    {
        "title": "Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT",
        "authors": [
            "Bhavya Bhavya",
            "Jinjun Xiong",
            "Chengxiang Zhai"
        ],
        "published": "2022-10-09T06:35:14Z",
        "summary": "We propose a novel application of prompting Pre-trained Language Models\n(PLMs) to generate analogies and study how to design effective prompts for two\ntask settings: generating a source concept analogous to a given target concept\n(aka Analogous Concept Generation or ACG), and generating an explanation of the\nsimilarity between a given pair of target concept and source concept (aka\nAnalogous Explanation Generation or AEG). We found that it is feasible to\nprompt InstructGPT to generate meaningful analogies and the best prompts tend\nto be precise imperative statements especially with a low temperature setting.\nWe also systematically analyzed the sensitivity of the InstructGPT model to\nprompt design, temperature, and injected spelling errors, and found that the\nmodel is particularly sensitive to certain variations (e.g., questions vs.\nimperative statements). Further, we conducted human evaluation on 1.4k of the\ngenerated analogies and found that the quality of generations varies\nsubstantially by model size. The largest InstructGPT model can achieve\nhuman-level performance at generating meaningful analogies for a given target\nwhile there is still room for improvement on the AEG task.",
        "pdf_link": "https://arxiv.org/pdf/2210.04186v2.pdf"
    },
    {
        "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
        "authors": [
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Wenqian Zhang",
            "Zhenyu Lei",
            "Yulia Tsvetkov"
        ],
        "published": "2022-10-08T20:51:02Z",
        "summary": "With the advent of pretrained language models (LMs), increasing research\nefforts have been focusing on infusing commonsense and domain-specific\nknowledge to prepare LMs for downstream tasks. These works attempt to leverage\nknowledge graphs, the de facto standard of symbolic knowledge representation,\nalong with pretrained LMs. While existing approaches have leveraged external\nknowledge, it remains an open question how to jointly incorporate knowledge\ngraphs representing varying contexts, from local (e.g., sentence), to\ndocument-level, to global knowledge, to enable knowledge-rich exchange across\nthese contexts. Such rich contextualization can be especially beneficial for\nlong document understanding tasks since standard pretrained LMs are typically\nbounded by the input sequence length. In light of these challenges, we propose\nKALM, a Knowledge-Aware Language Model that jointly leverages knowledge in\nlocal, document-level, and global contexts for long document understanding.\nKALM first encodes long documents and knowledge graphs into the three\nknowledge-aware context representations. It then processes each context with\ncontext-specific layers, followed by a context fusion layer that facilitates\nknowledge exchange to derive an overarching document representation. Extensive\nexperiments demonstrate that KALM achieves state-of-the-art performance on six\nlong document understanding tasks and datasets. Further analyses reveal that\nthe three knowledge-aware contexts are complementary and they all contribute to\nmodel performance, while the importance and information exchange patterns of\ndifferent contexts vary with respect to different tasks and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2210.04105v2.pdf"
    },
    {
        "title": "Understanding HTML with Large Language Models",
        "authors": [
            "Izzeddin Gur",
            "Ofir Nachum",
            "Yingjie Miao",
            "Mustafa Safdari",
            "Austin Huang",
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Noah Fiedel",
            "Aleksandra Faust"
        ],
        "published": "2022-10-08T07:27:17Z",
        "summary": "Large language models (LLMs) have shown exceptional performance on a variety\nof natural language tasks. Yet, their capabilities for HTML understanding --\ni.e., parsing the raw HTML of a webpage, with applications to automation of\nweb-based tasks, crawling, and browser-assisted retrieval -- have not been\nfully explored. We contribute HTML understanding models (fine-tuned LLMs) and\nan in-depth analysis of their capabilities under three tasks: (i) Semantic\nClassification of HTML elements, (ii) Description Generation for HTML inputs,\nand (iii) Autonomous Web Navigation of HTML pages. While previous work has\ndeveloped dedicated architectures and training procedures for HTML\nunderstanding, we show that LLMs pretrained on standard natural language\ncorpora transfer remarkably well to HTML understanding tasks. For instance,\nfine-tuned LLMs are 12% more accurate at semantic classification compared to\nmodels trained exclusively on the task dataset. Moreover, when fine-tuned on\ndata from the MiniWoB benchmark, LLMs successfully complete 50% more tasks\nusing 192x less data compared to the previous best supervised model. Out of the\nLLMs we evaluate, we show evidence that T5-based models are ideal due to their\nbidirectional encoder-decoder architecture. To promote further research on LLMs\nfor HTML understanding, we create and open-source a large-scale HTML dataset\ndistilled and auto-labeled from CommonCrawl.",
        "pdf_link": "https://arxiv.org/pdf/2210.03945v2.pdf"
    },
    {
        "title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models",
        "authors": [
            "Se Jung Kwon",
            "Jeonghoon Kim",
            "Jeongin Bae",
            "Kang Min Yoo",
            "Jin-Hwa Kim",
            "Baeseong Park",
            "Byeongwook Kim",
            "Jung-Woo Ha",
            "Nako Sung",
            "Dongsoo Lee"
        ],
        "published": "2022-10-08T00:36:00Z",
        "summary": "There are growing interests in adapting large-scale language models using\nparameter-efficient fine-tuning methods. However, accelerating the model itself\nand achieving better inference efficiency through model compression has not\nbeen thoroughly explored yet. Model compression could provide the benefits of\nreducing memory footprints, enabling low-precision computations, and ultimately\nachieving cost-effective inference. To combine parameter-efficient adaptation\nand model compression, we propose AlphaTuning consisting of post-training\nquantization of the pre-trained language model and fine-tuning only some parts\nof quantized parameters for a target task. Specifically, AlphaTuning works by\nemploying binary-coding quantization, which factorizes the full-precision\nparameters into binary parameters and a separate set of scaling factors. During\nthe adaptation phase, the binary values are frozen for all tasks, while the\nscaling factors are fine-tuned for the downstream task. We demonstrate that\nAlphaTuning, when applied to GPT-2 and OPT, performs competitively with full\nfine-tuning on a variety of downstream tasks while achieving >10x compression\nratio under 4-bit quantization and >1,000x reduction in the number of trainable\nparameters.",
        "pdf_link": "https://arxiv.org/pdf/2210.03858v1.pdf"
    },
    {
        "title": "How Large Language Models are Transforming Machine-Paraphrased Plagiarism",
        "authors": [
            "Jan Philip Wahle",
            "Terry Ruas",
            "Frederic Kirstein",
            "Bela Gipp"
        ],
        "published": "2022-10-07T14:08:57Z",
        "summary": "The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.",
        "pdf_link": "https://arxiv.org/pdf/2210.03568v3.pdf"
    },
    {
        "title": "Automatic Chain of Thought Prompting in Large Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola"
        ],
        "published": "2022-10-07T12:28:21Z",
        "summary": "Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot",
        "pdf_link": "https://arxiv.org/pdf/2210.03493v1.pdf"
    },
    {
        "title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems",
        "authors": [
            "Parikshit Bansal",
            "Yashoteja Prabhu",
            "Emre Kiciman",
            "Amit Sharma"
        ],
        "published": "2022-10-07T11:16:45Z",
        "summary": "Given a user's input text, text-matching recommender systems output relevant\nitems by comparing the input text to available items' description, such as\nproduct-to-product recommendation on e-commerce platforms. As users' interests\nand item inventory are expected to change, it is important for a text-matching\nsystem to generalize to data shifts, a task known as out-of-distribution (OOD)\ngeneralization. However, we find that the popular approach of fine-tuning a\nlarge, base language model on paired item relevance data (e.g., user clicks)\ncan be counter-productive for OOD generalization. For a product recommendation\ntask, fine-tuning obtains worse accuracy than the base model when recommending\nitems in a new category or for a future time period. To explain this\ngeneralization failure, we consider an intervention-based importance metric,\nwhich shows that a fine-tuned model captures spurious correlations and fails to\nlearn the causal features that determine the relevance between any two text\ninputs. Moreover, standard methods for causal regularization do not apply in\nthis setting, because unlike in images, there exist no universally spurious\nfeatures in a text-matching task (the same token may be spurious or causal\ndepending on the text it is being matched to). For OOD generalization on text\ninputs, therefore, we highlight a different goal: avoiding high importance\nscores for certain features. We do so using an intervention-based regularizer\nthat constraints the causal effect of any token on the model's relevance score\nto be similar to the base model. Results on Amazon product and 3 question\nrecommendation datasets show that our proposed regularizer improves\ngeneralization for both in-distribution and OOD evaluation, especially in\ndifficult scenarios when the base model is not accurate.",
        "pdf_link": "https://arxiv.org/pdf/2210.10636v2.pdf"
    },
    {
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "authors": [
            "Ofir Press",
            "Muru Zhang",
            "Sewon Min",
            "Ludwig Schmidt",
            "Noah A. Smith",
            "Mike Lewis"
        ],
        "published": "2022-10-07T06:50:23Z",
        "summary": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2210.03350v3.pdf"
    },
    {
        "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners",
        "authors": [
            "Seonghyeon Ye",
            "Doyoung Kim",
            "Joel Jang",
            "Joongbo Shin",
            "Minjoon Seo"
        ],
        "published": "2022-10-06T15:00:47Z",
        "summary": "Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.",
        "pdf_link": "https://arxiv.org/pdf/2210.02969v4.pdf"
    },
    {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik Narasimhan",
            "Yuan Cao"
        ],
        "published": "2022-10-06T01:00:32Z",
        "summary": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io",
        "pdf_link": "https://arxiv.org/pdf/2210.03629v3.pdf"
    },
    {
        "title": "Learning to Reason With Relational Abstractions",
        "authors": [
            "Andrew J. Nam",
            "Mengye Ren",
            "Chelsea Finn",
            "James L. McClelland"
        ],
        "published": "2022-10-06T00:27:50Z",
        "summary": "Large language models have recently shown promising progress in mathematical\nreasoning when fine-tuned with human-generated sequences walking through a\nsequence of solution steps. However, the solution sequences are not formally\nstructured and the resulting model-generated sequences may not reflect the kind\nof systematic reasoning we might expect an expert human to produce. In this\npaper, we study how to build stronger reasoning capability in language models\nusing the idea of relational abstractions. We introduce new types of sequences\nthat more explicitly provide an abstract characterization of the transitions\nthrough intermediate solution steps to the goal state. We find that models that\nare supplied with such sequences as prompts can solve tasks with a\nsignificantly higher accuracy, and models that are trained to produce such\nsequences solve problems better than those that are trained with previously\nused human-generated sequences and other baselines. Our work thus takes several\nsteps toward elucidating and improving how language models perform on tasks\nrequiring multi-step mathematical reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2210.02615v2.pdf"
    },
    {
        "title": "Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors",
        "authors": [
            "Mohammad Reza Taesiri",
            "Finlay Macklon",
            "Yihe Wang",
            "Hengshuo Shen",
            "Cor-Paul Bezemer"
        ],
        "published": "2022-10-05T18:44:35Z",
        "summary": "Video game testing requires game-specific knowledge as well as common sense\nreasoning about the events in the game. While AI-driven agents can satisfy the\nfirst requirement, it is not yet possible to meet the second requirement\nautomatically. Therefore, video game testing often still relies on manual\ntesting, and human testers are required to play the game thoroughly to detect\nbugs. As a result, it is challenging to fully automate game testing. In this\nstudy, we explore the possibility of leveraging the zero-shot capabilities of\nlarge language models for video game bug detection. By formulating the bug\ndetection problem as a question-answering task, we show that large language\nmodels can identify which event is buggy in a sequence of textual descriptions\nof events from a game. To this end, we introduce the GameBugDescriptions\nbenchmark dataset, which consists of 167 buggy gameplay videos and a total of\n334 question-answer pairs across 8 games. We extensively evaluate the\nperformance of six models across the OPT and InstructGPT large language model\nfamilies on our benchmark dataset. Our results show promising results for\nemploying language models to detect video game bugs. With the proper prompting\ntechnique, we could achieve an accuracy of 70.66%, and on some video games, up\nto 78.94%. Our code, evaluation data and the benchmark can be found on\nhttps://asgaardlab.github.io/LLMxBugs",
        "pdf_link": "https://arxiv.org/pdf/2210.02506v1.pdf"
    },
    {
        "title": "Antibody Representation Learning for Drug Discovery",
        "authors": [
            "Lin Li",
            "Esther Gupta",
            "John Spaeth",
            "Leslie Shing",
            "Tristan Bepler",
            "Rajmonda Sulo Caceres"
        ],
        "published": "2022-10-05T13:48:41Z",
        "summary": "Therapeutic antibody development has become an increasingly popular approach\nfor drug development. To date, antibody therapeutics are largely developed\nusing large scale experimental screens of antibody libraries containing\nhundreds of millions of antibody sequences. The high cost and difficulty of\ndeveloping therapeutic antibodies create a pressing need for computational\nmethods to predict antibody properties and create bespoke designs. However, the\nrelationship between antibody sequence and activity is a complex physical\nprocess and traditional iterative design approaches rely on large scale assays\nand random mutagenesis. Deep learning methods have emerged as a promising way\nto learn antibody property predictors, but predicting antibody properties and\ntarget-specific activities depends critically on the choice of antibody\nrepresentations and data linking sequences to properties is often limited.\nExisting works have not yet investigated the value, limitations and\nopportunities of these methods in application to antibody-based drug discovery.\nIn this paper, we present results on a novel SARS-CoV-2 antibody binding\ndataset and an additional benchmark dataset. We compare three classes of\nmodels: conventional statistical sequence models, supervised learning on each\ndataset independently, and fine-tuning an antibody specific pre-trained\nlanguage model. Experimental results suggest that self-supervised pretraining\nof feature representation consistently offers significant improvement in over\nprevious approaches. We also investigate the impact of data size on the model\nperformance, and discuss challenges and opportunities that the machine learning\ncommunity can address to advance in silico engineering and design of\ntherapeutic antibodies.",
        "pdf_link": "https://arxiv.org/pdf/2210.02881v1.pdf"
    },
    {
        "title": "Grounding Language with Visual Affordances over Unstructured Data",
        "authors": [
            "Oier Mees",
            "Jessica Borja-Diaz",
            "Wolfram Burgard"
        ],
        "published": "2022-10-04T21:16:48Z",
        "summary": "Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\nhttp://hulc2.cs.uni-freiburg.de",
        "pdf_link": "https://arxiv.org/pdf/2210.01911v3.pdf"
    },
    {
        "title": "Towards Improving Faithfulness in Abstractive Summarization",
        "authors": [
            "Xiuying Chen",
            "Mingzhe Li",
            "Xin Gao",
            "Xiangliang Zhang"
        ],
        "published": "2022-10-04T19:52:09Z",
        "summary": "Despite the success achieved in neural abstractive summarization based on\npre-trained language models, one unresolved issue is that the generated\nsummaries are not always faithful to the input document. There are two possible\ncauses of the unfaithfulness problem: (1) the summarization model fails to\nunderstand or capture the gist of the input text, and (2) the model over-relies\non the language model to generate fluent but inadequate words. In this work, we\npropose a Faithfulness Enhanced Summarization model (FES), which is designed\nfor addressing these two problems and improving faithfulness in abstractive\nsummarization. For the first problem, we propose to use question-answering (QA)\nto examine whether the encoder fully grasps the input document and can answer\nthe questions on the key information in the input. The QA attention on the\nproper input words can also be used to stipulate how the decoder should attend\nto the source. For the second problem, we introduce a max-margin loss defined\non the difference between the language and the summarization model, aiming to\nprevent the overconfidence of the language model. Extensive experiments on two\nbenchmark summarization datasets, CNN/DM and XSum, demonstrate that our model\nsignificantly outperforms strong baselines. The evaluation of factual\nconsistency also shows that our model generates more faithful summaries than\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2210.01877v1.pdf"
    },
    {
        "title": "Explaining Patterns in Data with Language Models via Interpretable Autoprompting",
        "authors": [
            "Chandan Singh",
            "John X. Morris",
            "Jyoti Aneja",
            "Alexander M. Rush",
            "Jianfeng Gao"
        ],
        "published": "2022-10-04T18:32:14Z",
        "summary": "Large language models (LLMs) have displayed an impressive ability to harness\nnatural language to perform complex tasks. In this work, we explore whether we\ncan leverage this learned ability to find and explain patterns in data.\nSpecifically, given a pre-trained LLM and data examples, we introduce\ninterpretable autoprompting (iPrompt), an algorithm that generates a\nnatural-language string explaining the data. iPrompt iteratively alternates\nbetween generating explanations with an LLM and reranking them based on their\nperformance when used as a prompt. Experiments on a wide range of datasets,\nfrom synthetic mathematics to natural-language understanding, show that iPrompt\ncan yield meaningful insights by accurately finding groundtruth dataset\ndescriptions. Moreover, the prompts produced by iPrompt are simultaneously\nhuman-interpretable and highly effective for generalization: on real-world\nsentiment classification datasets, iPrompt produces prompts that match or even\nimprove upon human-written prompts for GPT-3. Finally, experiments with an fMRI\ndataset show the potential for iPrompt to aid in scientific discovery. All code\nfor using the methods and data here is made available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2210.01848v2.pdf"
    },
    {
        "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
        "authors": [
            "Zhijing Jin",
            "Sydney Levine",
            "Fernando Gonzalez",
            "Ojasv Kamal",
            "Maarten Sap",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Josh Tenenbaum",
            "Bernhard Schölkopf"
        ],
        "published": "2022-10-04T09:04:27Z",
        "summary": "AI systems are becoming increasingly intertwined with human life. In order to\neffectively collaborate with humans and ensure safety, AI systems need to be\nable to understand, interpret and predict human moral judgments and decisions.\nHuman moral judgments are often guided by rules, but not always. A central\nchallenge for AI safety is capturing the flexibility of the human moral mind --\nthe ability to determine when a rule should be broken, especially in novel or\nunusual situations. In this paper, we present a novel challenge set consisting\nof rule-breaking question answering (RBQA) of cases that involve potentially\npermissible rule-breaking -- inspired by recent moral psychology studies. Using\na state-of-the-art large language model (LLM) as a basis, we propose a novel\nmoral chain of thought (MORALCOT) prompting strategy that combines the\nstrengths of LLMs with theories of moral reasoning developed in cognitive\nscience to predict human moral judgments. MORALCOT outperforms seven existing\nLLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to\ncapture the flexibility of the human moral mind. We also conduct a detailed\nerror analysis to suggest directions for future work to improve AI safety using\nRBQA. Our data is open-sourced at\nhttps://huggingface.co/datasets/feradauto/MoralExceptQA and code at\nhttps://github.com/feradauto/MoralCoT",
        "pdf_link": "https://arxiv.org/pdf/2210.01478v3.pdf"
    },
    {
        "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression",
        "authors": [
            "Chen Liang",
            "Simiao Zuo",
            "Qingru Zhang",
            "Pengcheng He",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2022-10-04T03:36:53Z",
        "summary": "Layer-wise distillation is a powerful tool to compress large models (i.e.\nteacher models) into small ones (i.e., student models). The student distills\nknowledge from the teacher by mimicking the hidden representations of the\nteacher at every intermediate layer. However, layer-wise distillation is\ndifficult. Since the student has a smaller model capacity than the teacher, it\nis often under-fitted. Furthermore, the hidden representations of the teacher\ncontain redundant information that the student does not necessarily need for\nthe target task's learning. To address these challenges, we propose a novel\nTask-aware layEr-wise Distillation (TED). TED designs task-aware filters to\nalign the hidden representations of the student and the teacher at each layer.\nThe filters select the knowledge that is useful for the target task from the\nhidden representations. As such, TED reduces the knowledge gap between the two\nmodels and helps the student to fit better on the target task. We evaluate TED\nin two scenarios: continual pre-training and fine-tuning. TED demonstrates\nsignificant and consistent improvements over existing distillation methods in\nboth scenarios. Code is available at\nhttps://github.com/cliang1453/task-aware-distillation.",
        "pdf_link": "https://arxiv.org/pdf/2210.01351v3.pdf"
    },
    {
        "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
        "authors": [
            "Batu Ozturkler",
            "Nikolay Malkin",
            "Zhen Wang",
            "Nebojsa Jojic"
        ],
        "published": "2022-10-04T00:34:01Z",
        "summary": "Large language models (LLMs) have a substantial capacity for high-level\nanalogical reasoning: reproducing patterns in linear text that occur in their\ntraining data (zero-shot evaluation) or in the provided context (few-shot\nin-context learning). However, recent studies show that even the more advanced\nLLMs fail in scenarios that require reasoning over multiple objects or facts\nand making sequences of logical deductions. We propose a two-stage\nprobabilistic inference paradigm, ThinkSum, which reasons over sets of objects\nor facts in a structured manner. In the first stage (Think - retrieval of\nassociations), a LLM is queried in parallel over a set of phrases extracted\nfrom the prompt or an auxiliary model call. In the second stage (Sum -\nprobabilistic inference or reasoning), the results of these queries are\naggregated to make the final prediction. We demonstrate the possibilities and\nadvantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks,\nachieving improvements over the state of the art using GPT-family models on\nthirteen difficult tasks, often with far smaller model variants. We also\ncompare and contrast ThinkSum with other proposed modifications to direct\nprompting of LLMs, such as variants of chain-of-thought prompting. Our results\nsuggest that because the probabilistic inference in ThinkSum is performed\noutside of calls to the LLM, ThinkSum is less sensitive to prompt design,\nyields more interpretable predictions, and can be flexibly combined with latent\nvariable models to extract structured knowledge from LLMs. Overall, our\nproposed paradigm represents a promising approach for enhancing the reasoning\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2210.01293v2.pdf"
    },
    {
        "title": "Robot Task Planning and Situation Handling in Open Worlds",
        "authors": [
            "Yan Ding",
            "Xiaohan Zhang",
            "Saeid Amiri",
            "Nieqing Cao",
            "Hao Yang",
            "Chad Esselink",
            "Shiqi Zhang"
        ],
        "published": "2022-10-04T00:21:00Z",
        "summary": "Automated task planning algorithms have been developed to help robots\ncomplete complex tasks that require multiple actions. Most of those algorithms\nhave been developed for \"closed worlds\" assuming complete world knowledge is\nprovided. However, the real world is generally open, and the robots frequently\nencounter unforeseen situations that can potentially break the planner's\ncompleteness. This paper introduces a novel algorithm (COWP) for open-world\ntask planning and situation handling that dynamically augments the robot's\naction knowledge with task-oriented common sense. In particular, common sense\nis extracted from Large Language Models based on the current task at hand and\nrobot skills. For systematic evaluations, we collected a dataset that includes\n561 execution-time situations in a dining domain, where each situation\ncorresponds to a state instance of a robot being potentially unable to complete\na task using a solution that normally works. Experimental results show that our\napproach significantly outperforms competitive baselines from the literature in\nthe success rate of service tasks. Additionally, we have demonstrated COWP\nusing a mobile manipulator. Supplementary materials are available at:\nhttps://cowplanning.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2210.01287v1.pdf"
    },
    {
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
        "authors": [
            "Rajkumar Ramamurthy",
            "Prithviraj Ammanabrolu",
            "Kianté Brantley",
            "Jack Hessel",
            "Rafet Sifa",
            "Christian Bauckhage",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "2022-10-03T21:38:29Z",
        "summary": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2210.01241v3.pdf"
    },
    {
        "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
        "authors": [
            "Abulhair Saparov",
            "He He"
        ],
        "published": "2022-10-03T21:34:32Z",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.",
        "pdf_link": "https://arxiv.org/pdf/2210.01240v4.pdf"
    },
    {
        "title": "The (In)Effectiveness of Intermediate Task Training For Domain Adaptation and Cross-Lingual Transfer Learning",
        "authors": [
            "Sovesh Mohapatra",
            "Somesh Mohapatra"
        ],
        "published": "2022-10-03T17:17:07Z",
        "summary": "Transfer learning from large language models (LLMs) has emerged as a powerful\ntechnique to enable knowledge-based fine-tuning for a number of tasks,\nadaptation of models for different domains and even languages. However, it\nremains an open question, if and when transfer learning will work, i.e. leading\nto positive or negative transfer. In this paper, we analyze the knowledge\ntransfer across three natural language processing (NLP) tasks - text\nclassification, sentimental analysis, and sentence similarity, using three LLMs\n- BERT, RoBERTa, and XLNet - and analyzing their performance, by fine-tuning on\ntarget datasets for domain and cross-lingual adaptation tasks, with and without\nan intermediate task training on a larger dataset. Our experiments showed that\nfine-tuning without an intermediate task training can lead to a better\nperformance for most tasks, while more generalized tasks might necessitate a\npreceding intermediate task training step. We hope that this work will act as a\nguide on transfer learning to NLP practitioners.",
        "pdf_link": "https://arxiv.org/pdf/2210.01091v2.pdf"
    },
    {
        "title": "A Non-monotonic Self-terminating Language Model",
        "authors": [
            "Eugene Choi",
            "Kyunghyun Cho",
            "Cheolhyoung Lee"
        ],
        "published": "2022-10-03T00:28:44Z",
        "summary": "Recent large-scale neural autoregressive sequence models have shown\nimpressive performances on a variety of natural language generation tasks.\nHowever, their generated sequences often exhibit degenerate properties such as\nnon-termination, undesirable repetition, and premature termination, when\ngenerated with decoding algorithms such as greedy search, beam search, top-$k$\nsampling, and nucleus sampling. In this paper, we focus on the problem of\nnon-terminating sequences resulting from an incomplete decoding algorithm. We\nfirst define an incomplete probable decoding algorithm which includes greedy\nsearch, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding\nalgorithm originally put forward by Welleck et al. (2020). We then propose a\nnon-monotonic self-terminating language model, which significantly relaxes the\nconstraint of monotonically increasing termination probability in the\noriginally proposed self-terminating language model by Welleck et al. (2020),\nto address the issue of non-terminating sequences when using incomplete\nprobable decoding algorithms. We prove that our proposed model prevents\nnon-terminating sequences when using not only incomplete probable decoding\nalgorithms but also beam search. We empirically validate our model on sequence\ncompletion tasks with various architectures.",
        "pdf_link": "https://arxiv.org/pdf/2210.00660v3.pdf"
    },
    {
        "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Xiaoman Pan",
            "Dian Yu",
            "Dong Yu",
            "Jianshu Chen",
            "Heng Ji"
        ],
        "published": "2022-10-01T04:08:50Z",
        "summary": "Although large language models have achieved impressive zero-shot ability,\nthe huge model size generally incurs high cost. Recently, semi-parametric\nlanguage models, which augment a smaller language model with an external\nretriever, have demonstrated promising language modeling capabilities. However,\nit remains unclear whether such semi-parametric language models can perform\ncompetitively well as their fully-parametric counterparts on zero-shot\ngeneralization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a\nzero-shot semi-parametric language model. To our best knowledge, this is the\nfirst semi-parametric language model that can demonstrate strong zero-shot\nperformance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$\nwith a novel semi-parametric multitask prompted training paradigm, which shows\nsignificant improvement compared with the parametric multitask training as\nproposed by T0. Specifically, we augment the multitask training and zero-shot\nevaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In\norder to incorporate multiple potentially noisy retrieved augmentations, we\nfurther propose a novel $\\text{augmentation fusion}$ module leveraging\nperceiver resampler and gated cross-attention. Notably, our proposed\n$\\text{Zemi}_\\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation\ntasks while being 3.9x smaller in model size.",
        "pdf_link": "https://arxiv.org/pdf/2210.00185v2.pdf"
    },
    {
        "title": "On the Impossible Safety of Large AI Models",
        "authors": [
            "El-Mahdi El-Mhamdi",
            "Sadegh Farhadkhani",
            "Rachid Guerraoui",
            "Nirupam Gupta",
            "Lê-Nguyên Hoang",
            "Rafael Pinot",
            "Sébastien Rouault",
            "John Stephan"
        ],
        "published": "2022-09-30T06:36:49Z",
        "summary": "Large AI Models (LAIMs), of which large language models are the most\nprominent recent example, showcase some impressive performance. However they\nhave been empirically found to pose serious security issues. This paper\nsystematizes our knowledge about the fundamental impossibility of building\narbitrarily accurate and secure machine learning models. More precisely, we\nidentify key challenging features of many of today's machine learning settings.\nNamely, high accuracy seems to require memorizing large training datasets,\nwhich are often user-generated and highly heterogeneous, with both sensitive\ninformation and fake users. We then survey statistical lower bounds that, we\nargue, constitute a compelling case against the possibility of designing\nhigh-accuracy LAIMs with strong security guarantees.",
        "pdf_link": "https://arxiv.org/pdf/2209.15259v2.pdf"
    },
    {
        "title": "Learning by Distilling Context",
        "authors": [
            "Charlie Snell",
            "Dan Klein",
            "Ruiqi Zhong"
        ],
        "published": "2022-09-30T02:30:15Z",
        "summary": "Language models significantly benefit from context tokens, such as prompts or\nscratchpads. They perform better when prompted with informative instructions,\nand they acquire new reasoning capabilities by generating a scratch-pad before\npredicting the final answers. However, they do not \\textit{internalize} these\nperformance gains, which disappear when the context tokens are gone. Our work\nproposes to apply context distillation so that a language model can improve\nitself by internalizing these gains. Concretely, given a synthetic unlabeled\ninput for the target task, we condition the model on ``[instructions] +\n[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune\nthe same model to predict its own ``[final answer]'' conditioned on the\n``[task-input]'', without seeing the ``[instructions]'' or using the\n``[scratch-pad]''.\n  We show that context distillation is a general method to train language\nmodels, and it can effectively internalize 3 types of training signals. First,\nit can internalize abstract task instructions and explanations, so we can\niteratively update the model parameters with new instructions and overwrite old\nones. Second, it can internalize step-by-step reasoning for complex tasks\n(e.g., 8-digit addition), and such a newly acquired capability proves to be\nuseful for other downstream tasks. Finally, it can internalize concrete\ntraining examples, and it outperforms directly learning with gradient descent\nby 9\\% on the SPIDER Text-to-SQL dataset; furthermore, combining context\ndistillation operations can internalize more training examples than the context\nwindow size allows.",
        "pdf_link": "https://arxiv.org/pdf/2209.15189v1.pdf"
    },
    {
        "title": "Unpacking Large Language Models with Conceptual Consistency",
        "authors": [
            "Pritish Sahu",
            "Michael Cogswell",
            "Yunye Gong",
            "Ajay Divakaran"
        ],
        "published": "2022-09-29T20:55:57Z",
        "summary": "If a Large Language Model (LLM) answers \"yes\" to the question \"Are mountains\ntall?\" then does it know what a mountain is? Can you rely on it responding\ncorrectly or incorrectly to other questions about mountains? The success of\nLarge Language Models (LLMs) indicates they are increasingly able to answer\nqueries like these accurately, but that ability does not necessarily imply a\ngeneral understanding of concepts relevant to the anchor query. We propose\nconceptual consistency to measure a LLM's understanding of relevant concepts.\nThis novel metric measures how well a model can be characterized by finding out\nhow consistent its responses to queries about conceptually relevant background\nknowledge are. To compute it we extract background knowledge by traversing\npaths between concepts in a knowledge base and then try to predict the model's\nresponse to the anchor query from the background knowledge. We investigate the\nperformance of current LLMs in a commonsense reasoning setting using the CSQA\ndataset and the ConceptNet knowledge base. While conceptual consistency, like\nother metrics, does increase with the scale of the LLM used, we find that\npopular models do not necessarily have high conceptual consistency. Our\nanalysis also shows significant variation in conceptual consistency across\ndifferent kinds of relations, concepts, and prompts. This serves as a step\ntoward building models that humans can apply a theory of mind to, and thus\ninteract with intuitively.",
        "pdf_link": "https://arxiv.org/pdf/2209.15093v1.pdf"
    },
    {
        "title": "Compositional Semantic Parsing with Large Language Models",
        "authors": [
            "Andrew Drozdov",
            "Nathanael Schärli",
            "Ekin Akyürek",
            "Nathan Scales",
            "Xinying Song",
            "Xinyun Chen",
            "Olivier Bousquet",
            "Denny Zhou"
        ],
        "published": "2022-09-29T17:58:28Z",
        "summary": "Humans can reason compositionally when presented with new tasks. Previous\nresearch shows that appropriate prompting techniques enable large language\nmodels (LLMs) to solve artificial compositional generalization tasks such as\nSCAN. In this work, we identify additional challenges in more realistic\nsemantic parsing tasks with larger vocabulary and refine these prompting\ntechniques to address them. Our best method is based on least-to-most\nprompting: it decomposes the problem using prompting-based syntactic parsing,\nthen uses this decomposition to select appropriate exemplars and to\nsequentially generate the semantic parse. This method allows us to set a new\nstate of the art for CFQ while requiring only 1% of the training data used by\ntraditional approaches. Due to the general nature of our approach, we expect\nsimilar efforts will lead to new results in other tasks and domains, especially\nfor knowledge-intensive applications.",
        "pdf_link": "https://arxiv.org/pdf/2209.15003v2.pdf"
    },
    {
        "title": "Repairing Bugs in Python Assignments Using Large Language Models",
        "authors": [
            "Jialu Zhang",
            "José Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Ruzica Piskac",
            "Gustavo Soares",
            "Gust Verbruggen"
        ],
        "published": "2022-09-29T15:41:17Z",
        "summary": "Students often make mistakes on their introductory programming assignments as\npart of their learning process. Unfortunately, providing custom repairs for\nthese mistakes can require a substantial amount of time and effort from class\ninstructors. Automated program repair (APR) techniques can be used to\nsynthesize such fixes. Prior work has explored the use of symbolic and neural\ntechniques for APR in the education domain. Both types of approaches require\neither substantial engineering efforts or large amounts of data and training.\nWe propose to use a large language model trained on code, such as Codex, to\nbuild an APR system -- MMAPR -- for introductory Python programming\nassignments. Our system can fix both syntactic and semantic mistakes by\ncombining multi-modal prompts, iterative querying, test-case-based selection of\nfew-shots, and program chunking. We evaluate MMAPR on 286 real student programs\nand compare to a baseline built by combining a state-of-the-art Python syntax\nrepair engine, BIFI, and state-of-the-art Python semantic repair engine for\nstudent assignments, Refactory. We find that MMAPR can fix more programs and\nproduce smaller patches on average.",
        "pdf_link": "https://arxiv.org/pdf/2209.14876v1.pdf"
    },
    {
        "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
        "authors": [
            "Kundan Krishna",
            "Saurabh Garg",
            "Jeffrey P. Bigham",
            "Zachary C. Lipton"
        ],
        "published": "2022-09-28T19:28:43Z",
        "summary": "For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.",
        "pdf_link": "https://arxiv.org/pdf/2209.14389v2.pdf"
    },
    {
        "title": "Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer",
        "authors": [
            "Piotr Pęzik",
            "Agnieszka Mikołajczyk-Bareła",
            "Adam Wawrzyński",
            "Bartłomiej Nitoń",
            "Maciej Ogrodniczuk"
        ],
        "published": "2022-09-28T11:31:43Z",
        "summary": "The paper explores the relevance of the Text-To-Text Transfer Transformer\nlanguage model (T5) for Polish (plT5) to the task of intrinsic and extrinsic\nkeyword extraction from short text passages. The evaluation is carried out on\nthe new Polish Open Science Metadata Corpus (POSMAC), which is released with\nthis paper: a collection of 216,214 abstracts of scientific publications\ncompiled in the CURLICAT project. We compare the results obtained by four\ndifferent methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that\nthe plT5kw model yields particularly promising results for both frequent and\nsparsely represented keywords. Furthermore, a plT5kw keyword generation model\ntrained on the POSMAC also seems to produce highly useful results in\ncross-domain text labelling scenarios. We discuss the performance of the model\non news stories and phone-based dialog transcripts which represent text genres\nand domains extrinsic to the dataset of scientific abstracts. Finally, we also\nattempt to characterize the challenges of evaluating a text-to-text model on\nboth intrinsic and extrinsic keyword extraction.",
        "pdf_link": "https://arxiv.org/pdf/2209.14008v2.pdf"
    },
    {
        "title": "How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI",
        "authors": [
            "Kaiping Chen",
            "Anqi Shao",
            "Jirayu Burapacheep",
            "Yixuan Li"
        ],
        "published": "2022-09-27T18:44:41Z",
        "summary": "Autoregressive language models, which use deep learning to produce human-like\ntexts, have become increasingly widespread. Such models are powering popular\nvirtual assistants in areas like smart health, finance, and autonomous driving.\nWhile the parameters of these large language models are improving, concerns\npersist that these models might not work equally for all subgroups in society.\nDespite growing discussions of AI fairness across disciplines, there lacks\nsystemic metrics to assess what equity means in dialogue systems and how to\nengage different populations in the assessment loop. Grounded in theories of\ndeliberative democracy and science and technology studies, this paper proposes\nan analytical framework for unpacking the meaning of equity in human-AI\ndialogues. Using this framework, we conducted an auditing study to examine how\nGPT-3 responded to different sub-populations on crucial science and social\ntopics: climate change and the Black Lives Matter (BLM) movement. Our corpus\nconsists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals\nwho vary in gender, race and ethnicity, education level, English as a first\nlanguage, and opinions toward the issues. We found a substantively worse user\nexperience with GPT-3 among the opinion and the education minority\nsubpopulations; however, these two groups achieved the largest knowledge gain,\nchanging attitudes toward supporting BLM and climate change efforts after the\nchat. We traced these user experience divides to conversational differences and\nfound that GPT-3 used more negative expressions when it responded to the\neducation and opinion minority groups, compared to its responses to the\nmajority groups. We discuss the implications of our findings for a deliberative\nconversational AI system that centralizes diversity, equity, and inclusion.",
        "pdf_link": "https://arxiv.org/pdf/2209.13627v2.pdf"
    },
    {
        "title": "Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors",
        "authors": [
            "Vignav Ramesh",
            "Nathan Andrew Chi",
            "Pranav Rajpurkar"
        ],
        "published": "2022-09-27T00:44:41Z",
        "summary": "Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.",
        "pdf_link": "https://arxiv.org/pdf/2210.06340v2.pdf"
    },
    {
        "title": "Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour",
        "authors": [
            "Fangyu Liu",
            "Julian Martin Eisenschlos",
            "Jeremy R. Cole",
            "Nigel Collier"
        ],
        "published": "2022-09-26T15:45:23Z",
        "summary": "Language models (LMs) trained on raw texts have no direct access to the\nphysical world. Gordon and Van Durme (2013) point out that LMs can thus suffer\nfrom reporting bias: texts rarely report on common facts, instead focusing on\nthe unusual aspects of a situation. If LMs are only trained on text corpora and\nnaively memorise local co-occurrence statistics, they thus naturally would\nlearn a biased view of the physical world. While prior studies have repeatedly\nverified that LMs of smaller scales (e.g., RoBERTa, GPT-2) amplify reporting\nbias, it remains unknown whether such trends continue when models are scaled\nup. We investigate reporting bias from the perspective of colour in larger\nlanguage models (LLMs) such as PaLM and GPT-3. Specifically, we query LLMs for\nthe typical colour of objects, which is one simple type of perceptually\ngrounded physical common sense. Surprisingly, we find that LLMs significantly\noutperform smaller LMs in determining an object's typical colour and more\nclosely track human judgments, instead of overfitting to surface patterns\nstored in texts. This suggests that very large models of language alone are\nable to overcome certain types of reporting bias that are characterized by\nlocal co-occurrences.",
        "pdf_link": "https://arxiv.org/pdf/2209.12786v1.pdf"
    },
    {
        "title": "Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts",
        "authors": [
            "Joel Jang",
            "Seonghyeon Ye",
            "Minjoon Seo"
        ],
        "published": "2022-09-26T14:05:10Z",
        "summary": "Previous work has shown that there exists a scaling law between the size of\nLanguage Models (LMs) and their zero-shot performance on different downstream\nNLP tasks. In this work, we show that this phenomenon does not hold when\nevaluating large LMs on tasks with negated prompts, but instead shows an\ninverse scaling law. We evaluate 9 different tasks with negated prompts on (1)\npretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further\npretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with\nfew-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all\nLM types perform worse on negated prompts as they scale and show a huge\nperformance gap between the human performance when comparing the average score\non both original and negated prompts. By highlighting a critical limitation of\nexisting LMs and methods, we urge the community to develop new approaches of\ndeveloping LMs that actually follow the given instructions. We provide the code\nand the datasets to explore negated prompts at\nhttps://github.com/joeljang/negated-prompts-for-llms",
        "pdf_link": "https://arxiv.org/pdf/2209.12711v1.pdf"
    },
    {
        "title": "News Summarization and Evaluation in the Era of GPT-3",
        "authors": [
            "Tanya Goyal",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "published": "2022-09-26T01:04:52Z",
        "summary": "The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.",
        "pdf_link": "https://arxiv.org/pdf/2209.12356v2.pdf"
    },
    {
        "title": "WinoDict: Probing language models for in-context word acquisition",
        "authors": [
            "Julian Martin Eisenschlos",
            "Jeremy R. Cole",
            "Fangyu Liu",
            "William W. Cohen"
        ],
        "published": "2022-09-25T05:30:13Z",
        "summary": "We introduce a new in-context learning paradigm to measure Large Language\nModels' (LLMs) ability to learn novel words during inference. In particular, we\nrewrite Winograd-style co-reference resolution problems by replacing the key\nconcept word with a synthetic but plausible word that the model must understand\nto complete the task. Solving this task requires the model to make use of the\ndictionary definition of the new word given in the prompt. This benchmark\naddresses word acquisition, one important aspect of the diachronic degradation\nknown to afflict LLMs. As LLMs are frozen in time at the moment they are\ntrained, they are normally unable to reflect the way language changes over\ntime. We show that the accuracy of LLMs compared to the original Winograd tasks\ndecreases radically in our benchmark, thus identifying a limitation of current\nmodels and providing a benchmark to measure future improvements in LLMs ability\nto do in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2209.12153v1.pdf"
    },
    {
        "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
        "authors": [
            "Gabriel Simmons"
        ],
        "published": "2022-09-24T23:55:53Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating fluent text, as well as tendencies to reproduce undesirable social\nbiases. This study investigates whether LLMs reproduce the moral biases\nassociated with political groups in the United States, an instance of a broader\ncapability herein termed moral mimicry. This hypothesis is explored in the\nGPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral\nFoundations Theory, it is shown that these LLMs are indeed moral mimics. When\nprompted with a liberal or conservative political identity, the models generate\ntext reflecting corresponding moral biases. This study also explores the\nrelationship between moral mimicry and model size, and similarity between human\nand LLM moral word use.",
        "pdf_link": "https://arxiv.org/pdf/2209.12106v2.pdf"
    },
    {
        "title": "Augmenting Interpretable Models with LLMs during Training",
        "authors": [
            "Chandan Singh",
            "Armin Askari",
            "Rich Caruana",
            "Jianfeng Gao"
        ],
        "published": "2022-09-23T18:36:01Z",
        "summary": "Recent large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their proliferation into\nhigh-stakes domains (e.g. medicine) and compute-limited settings has created a\nburgeoning need for interpretability and efficiency. We address this need by\nproposing Augmented Interpretable Models (Aug-imodels), a framework for\nleveraging the knowledge learned by LLMs to build extremely efficient and\ninterpretable models. Aug-imodels use LLMs during fitting but not during\ninference, allowing complete transparency and often a speed/memory improvement\nof greater than 1,000x for inference compared to LLMs. We explore two\ninstantiations of Aug-imodels in natural-language processing: (i) Aug-GAM,\nwhich augments a generalized additive model with decoupled embeddings from an\nLLM and (ii) Aug-Tree, which augments a decision tree with LLM feature\nexpansions. Across a variety of text-classification datasets, both outperform\ntheir non-augmented counterparts. Aug-GAM can even outperform much larger\nmodels (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer\nparameters and being fully transparent. We further explore Aug-imodels in a\nnatural-language fMRI study, where they generate interesting interpretations\nfrom scientific data. All code for using Aug-imodels and reproducing results is\nmade available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2209.11799v3.pdf"
    },
    {
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
        "authors": [
            "Zhuyun Dai",
            "Vincent Y. Zhao",
            "Ji Ma",
            "Yi Luan",
            "Jianmo Ni",
            "Jing Lu",
            "Anton Bakalov",
            "Kelvin Guu",
            "Keith B. Hall",
            "Ming-Wei Chang"
        ],
        "published": "2022-09-23T17:59:06Z",
        "summary": "Much recent research on information retrieval has focused on how to transfer\nfrom one task (typically with abundant supervised data) to various other tasks\nwhere supervision is limited, with the implicit assumption that it is possible\nto generalize from one task to all the rest. However, this overlooks the fact\nthat there are many diverse and unique retrieval tasks, each targeting\ndifferent search intents, queries, and search domains. In this paper, we\nsuggest to work on Few-shot Dense Retrieval, a setting where each task comes\nwith a short description and a few examples. To amplify the power of a few\nexamples, we propose Prompt-base Query Generation for Retriever (Promptagator),\nwhich leverages large language models (LLM) as a few-shot query generator, and\ncreates task-specific retrievers based on the generated data. Powered by LLM's\ngeneralization ability, Promptagator makes it possible to create task-specific\nend-to-end retrievers solely based on a few examples {without} using Natural\nQuestions or MS MARCO to train %question generators or dual encoders.\nSurprisingly, LLM prompting with no more than 8 examples allows dual encoders\nto outperform heavily engineered models trained on MS MARCO like ColBERT v2 by\nmore than 1.2 nDCG on average on 11 retrieval sets. Further training\nstandard-size re-rankers using the same generated data yields another 5.0 point\nnDCG improvement. Our studies determine that query generation can be far more\neffective than previously observed, especially when a small amount of\ntask-specific knowledge is given.",
        "pdf_link": "https://arxiv.org/pdf/2209.11755v1.pdf"
    },
    {
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
        "authors": [
            "Ishika Singh",
            "Valts Blukis",
            "Arsalan Mousavian",
            "Ankit Goyal",
            "Danfei Xu",
            "Jonathan Tremblay",
            "Dieter Fox",
            "Jesse Thomason",
            "Animesh Garg"
        ],
        "published": "2022-09-22T20:29:49Z",
        "summary": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2209.11302v1.pdf"
    },
    {
        "title": "A Case Report On The \"A.I. Locked-In Problem\": social concerns with modern NLP",
        "authors": [
            "Yoshija Walter"
        ],
        "published": "2022-09-22T16:39:35Z",
        "summary": "Modern NLP models are becoming better conversational agents than their\npredecessors. Recurrent Neural Networks (RNNs) and especially Long-Short Term\nMemory (LSTM) features allow the agent to better store and use information\nabout semantic content, a trend that has become even more pronounced with the\nTransformer Models. Large Language Models (LLMs) such as GPT-3 by OpenAI have\nbecome known to be able to construct and follow a narrative, which enables the\nsystem to adopt personas on the go, adapt them and play along in conversational\nstories. However, practical experimentation with GPT-3 shows that there is a\nrecurring problem with these modern NLP systems, namely that they can \"get\nstuck\" in the narrative so that further conversations, prompt executions or\ncommands become futile. This is here referred to as the \"Locked-In Problem\" and\nis exemplified with an experimental case report, followed by practical and\nsocial concerns that are accompanied with this problem.",
        "pdf_link": "https://arxiv.org/pdf/2209.12687v1.pdf"
    },
    {
        "title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation",
        "authors": [
            "Xingdi Yuan",
            "Tong Wang",
            "Yen-Hsiang Wang",
            "Emery Fine",
            "Rania Abdelghani",
            "Pauline Lucas",
            "Hélène Sauzéon",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2022-09-22T13:33:48Z",
        "summary": "Large Language Models (LLMs) have in recent years demonstrated impressive\nprowess in natural language generation. A common practice to improve generation\ndiversity is to sample multiple outputs from the model. However, there lacks a\nsimple and robust way of selecting the best output from these stochastic\nsamples. As a case study framed in the context of question generation, we\npropose two prompt-based approaches to selecting high-quality questions from a\nset of LLM-generated candidates. Our method works under the constraints of 1) a\nblack-box (non-modifiable) question generation model and 2) lack of access to\nhuman-annotated references -- both of which are realistic limitations for\nreal-world deployment of LLMs. With automatic as well as human evaluations, we\nempirically demonstrate that our approach can effectively select questions of\nhigher qualities than greedy generation.",
        "pdf_link": "https://arxiv.org/pdf/2209.11000v1.pdf"
    },
    {
        "title": "Representing Affect Information in Word Embeddings",
        "authors": [
            "Yuhan Zhang",
            "Wenqi Chen",
            "Ruihan Zhang",
            "Xiajie Zhang"
        ],
        "published": "2022-09-21T18:16:33Z",
        "summary": "A growing body of research in natural language processing (NLP) and natural\nlanguage understanding (NLU) is investigating human-like knowledge learned or\nencoded in the word embeddings from large language models. This is a step\ntowards understanding what knowledge language models capture that resembles\nhuman understanding of language and communication. Here, we investigated\nwhether and how the affect meaning of a word (i.e., valence, arousal,\ndominance) is encoded in word embeddings pre-trained in large neural networks.\nWe used the human-labeled dataset as the ground truth and performed various\ncorrelational and classification tests on four types of word embeddings. The\nembeddings varied in being static or contextualized, and how much affect\nspecific information was prioritized during the pre-training and fine-tuning\nphase. Our analyses show that word embedding from the vanilla BERT model did\nnot saliently encode the affect information of English words. Only when the\nBERT model was fine-tuned on emotion-related tasks or contained extra\ncontextualized information from emotion-rich contexts could the corresponding\nembedding encode more relevant affect information.",
        "pdf_link": "https://arxiv.org/pdf/2209.10583v1.pdf"
    },
    {
        "title": "Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers",
        "authors": [
            "Ruisi Zhang",
            "Seira Hidano",
            "Farinaz Koushanfar"
        ],
        "published": "2022-09-21T17:05:12Z",
        "summary": "Text classification has become widely used in various natural language\nprocessing applications like sentiment analysis. Current applications often use\nlarge transformer-based language models to classify input texts. However, there\nis a lack of systematic study on how much private information can be inverted\nwhen publishing models. In this paper, we formulate \\emph{Text Revealer} -- the\nfirst model inversion attack for text reconstruction against text\nclassification with transformers. Our attacks faithfully reconstruct private\ntexts included in training data with access to the target model. We leverage an\nexternal dataset and GPT-2 to generate the target domain-like fluent text, and\nthen perturb its hidden state optimally with the feedback from the target\nmodel. Our extensive experiments demonstrate that our attacks are effective for\ndatasets with different text lengths and can reconstruct private texts with\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2209.10505v1.pdf"
    },
    {
        "title": "Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling",
        "authors": [
            "Thiemo Wambsganss",
            "Vinitra Swamy",
            "Roman Rietsche",
            "Tanja Käser"
        ],
        "published": "2022-09-21T13:08:16Z",
        "summary": "Natural Language Processing (NLP) has become increasingly utilized to provide\nadaptivity in educational applications. However, recent research has\nhighlighted a variety of biases in pre-trained language models. While existing\nstudies investigate bias in different domains, they are limited in addressing\nfine-grained analysis on educational and multilingual corpora. In this work, we\nanalyze bias across text and through multiple architectures on a corpus of\n9,165 German peer-reviews collected from university students over five years.\nNotably, our corpus includes labels such as helpfulness, quality, and critical\naspect ratings from the peer-review recipient as well as demographic\nattributes. We conduct a Word Embedding Association Test (WEAT) analysis on (1)\nour collected corpus in connection with the clustered labels, (2) the most\ncommon pre-trained German language models (T5, BERT, and GPT-2) and GloVe\nembeddings, and (3) the language models after fine-tuning on our collected\ndata-set. In contrast to our initial expectations, we found that our collected\ncorpus does not reveal many biases in the co-occurrence analysis or in the\nGloVe embeddings. However, the pre-trained German language models find\nsubstantial conceptual, racial, and gender bias and have significant changes in\nbias across conceptual and racial axes during fine-tuning on the peer-review\ndata. With our research, we aim to contribute to the fourth UN sustainability\ngoal (quality education) with a novel dataset, an understanding of biases in\nnatural language education data, and the potential harms of not counteracting\nbiases in language models for educational tasks.",
        "pdf_link": "https://arxiv.org/pdf/2209.10335v2.pdf"
    },
    {
        "title": "T5QL: Taming language models for SQL generation",
        "authors": [
            "Samuel Arcadinho",
            "David Aparício",
            "Hugo Veiga",
            "António Alegria"
        ],
        "published": "2022-09-21T10:43:13Z",
        "summary": "Automatic SQL generation has been an active research area, aiming at\nstreamlining the access to databases by writing natural language with the given\nintent instead of writing SQL. Current SOTA methods for semantic parsing depend\non LLMs to achieve high predictive accuracy on benchmark datasets. This reduces\ntheir applicability, since LLMs requires expensive GPUs. Furthermore, SOTA\nmethods are ungrounded and thus not guaranteed to always generate valid SQL.\nHere we propose T5QL, a new SQL generation method that improves the performance\nin benchmark datasets when using smaller LMs, namely T5-Base, by 13pp when\ncompared against SOTA methods. Additionally, T5QL is guaranteed to always\noutput valid SQL using a context-free grammar to constrain SQL generation.\nFinally, we show that dividing semantic parsing in two tasks, candidate SQLs\ngeneration and candidate re-ranking, is a promising research avenue that can\nreduce the need for large LMs.",
        "pdf_link": "https://arxiv.org/pdf/2209.10254v1.pdf"
    },
    {
        "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
        "authors": [
            "Wenhao Yu",
            "Dan Iter",
            "Shuohang Wang",
            "Yichong Xu",
            "Mingxuan Ju",
            "Soumya Sanyal",
            "Chenguang Zhu",
            "Michael Zeng",
            "Meng Jiang"
        ],
        "published": "2022-09-21T01:30:59Z",
        "summary": "Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large amount of world or domain knowledge. A common\napproach for knowledge-intensive tasks is to employ a retrieve-then-read\npipeline that first retrieves a handful of relevant contextual documents from\nan external corpus such as Wikipedia and then predicts an answer conditioned on\nthe retrieved documents. In this paper, we present a novel perspective for\nsolving knowledge-intensive tasks by replacing document retrievers with large\nlanguage model generators. We call our method generate-then-read (GenRead),\nwhich first prompts a large language model to generate contextutal documents\nbased on a given question, and then reads the generated documents to produce\nthe final answer. Furthermore, we propose a novel clustering-based prompting\nmethod that selects distinct prompts, resulting in the generated documents that\ncover different perspectives, leading to better recall over acceptable answers.\nWe conduct extensive experiments on three different knowledge-intensive tasks,\nincluding open-domain QA, fact checking, and dialogue system. Notably, GenRead\nachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly\noutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0\nand +3.9, without retrieving any documents from any external knowledge source.\nLastly, we demonstrate the model performance can be further improved by\ncombining retrieval and generation. Our code and generated documents can be\nfound at https://github.com/wyu97/GenRead.",
        "pdf_link": "https://arxiv.org/pdf/2209.10063v3.pdf"
    },
    {
        "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
        "authors": [
            "Boyuan Chen",
            "Fei Xia",
            "Brian Ichter",
            "Kanishka Rao",
            "Keerthana Gopalakrishnan",
            "Michael S. Ryoo",
            "Austin Stone",
            "Daniel Kappler"
        ],
        "published": "2022-09-20T17:29:56Z",
        "summary": "Large language models (LLMs) have unlocked new capabilities of task planning\nfrom human instructions. However, prior attempts to apply LLMs to real-world\nrobotic tasks are limited by the lack of grounding in the surrounding scene. In\nthis paper, we develop NLMap, an open-vocabulary and queryable scene\nrepresentation to address this problem. NLMap serves as a framework to gather\nand integrate contextual information into LLM planners, allowing them to see\nand query available objects in the scene before generating a\ncontext-conditioned plan. NLMap first establishes a natural language queryable\nscene representation with Visual Language models (VLMs). An LLM based object\nproposal module parses instructions and proposes involved objects to query the\nscene representation for object availability and location. An LLM planner then\nplans with such information about the scene. NLMap allows robots to operate\nwithout a fixed list of objects nor executable options, enabling real robot\noperation unachievable by previous methods. Project website:\nhttps://nlmap-saycan.github.io",
        "pdf_link": "https://arxiv.org/pdf/2209.09874v2.pdf"
    },
    {
        "title": "Relaxed Attention for Transformer Models",
        "authors": [
            "Timo Lohrenz",
            "Björn Möller",
            "Zhengyang Li",
            "Tim Fingscheidt"
        ],
        "published": "2022-09-20T14:10:28Z",
        "summary": "The powerful modeling capabilities of all-attention-based transformer\narchitectures often cause overfitting and - for natural language processing\ntasks - lead to an implicitly learned internal language model in the\nautoregressive transformer decoder complicating the integration of external\nlanguage models. In this paper, we explore relaxed attention, a simple and\neasy-to-implement smoothing of the attention weights, yielding a two-fold\nimprovement to the general transformer architecture: First, relaxed attention\nprovides regularization when applied to the self-attention layers in the\nencoder. Second, we show that it naturally supports the integration of an\nexternal language model as it suppresses the implicitly learned internal\nlanguage model by relaxing the cross attention in the decoder. We demonstrate\nthe benefit of relaxed attention across several tasks with clear improvement in\ncombination with recent benchmark approaches. Specifically, we exceed the\nformer state-of-the-art performance of 26.90% word error rate on the largest\npublic lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as\nwe achieve a top-performing BLEU score of 37.67 on the IWSLT14\n(DE$\\rightarrow$EN) machine translation task without external language models\nand virtually no additional model parameters. Code and models will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2209.09735v1.pdf"
    },
    {
        "title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics",
        "authors": [
            "Daniil Larionov",
            "Jens Grünwald",
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "published": "2022-09-20T10:12:07Z",
        "summary": "Efficiency is a key property to foster inclusiveness and reduce environmental\ncosts, especially in an era of LLMs. In this work, we provide a comprehensive\nevaluation of efficiency for MT evaluation metrics. Our approach involves\nreplacing computation-intensive transformers with lighter alternatives and\nemploying linear and quadratic approximations for alignment algorithms on top\nof LLM representations. We evaluate six (reference-free and reference-based)\nmetrics across three MT datasets and examine 16 lightweight transformers. In\naddition, we look into the training efficiency of metrics like COMET by\nutilizing adapters. Our results indicate that (a) TinyBERT provides the optimal\nbalance between quality and efficiency, (b) CPU speed-ups are more substantial\nthan those on GPU; (c) WMD approximations yield no efficiency gains while\nreducing quality and (d) adapters enhance training efficiency (regarding\nbackward pass speed and memory requirements) as well as, in some cases, metric\nquality. These findings can help to strike a balance between evaluation speed\nand quality, which is essential for effective NLG systems. Furthermore, our\nresearch contributes to the ongoing efforts to optimize NLG evaluation metrics\nwith minimal impact on performance. To our knowledge, ours is the most\ncomprehensive analysis of different aspects of efficiency for MT metrics\nconducted so far.",
        "pdf_link": "https://arxiv.org/pdf/2209.09593v2.pdf"
    },
    {
        "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
        "authors": [
            "Pan Lu",
            "Swaroop Mishra",
            "Tony Xia",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Song-Chun Zhu",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "published": "2022-09-20T07:04:24Z",
        "summary": "When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2209.09513v2.pdf"
    },
    {
        "title": "Enabling Conversational Interaction with Mobile UI using Large Language Models",
        "authors": [
            "Bryan Wang",
            "Gang Li",
            "Yang Li"
        ],
        "published": "2022-09-18T20:58:39Z",
        "summary": "Conversational agents show the promise to allow users to interact with mobile\ndevices using language. However, to perform diverse UI tasks with natural\nlanguage, developers typically need to create separate datasets and models for\neach specific task, which is expensive and effort-consuming. Recently,\npre-trained large language models (LLMs) have been shown capable of\ngeneralizing to various downstream tasks when prompted with a handful of\nexamples from the target task. This paper investigates the feasibility of\nenabling versatile conversational interactions with mobile UIs using a single\nLLM. We designed prompting techniques to adapt an LLM to mobile UIs. We\nexperimented with four important modeling tasks that address various scenarios\nin conversational interaction. Our method achieved competitive performance on\nthese challenging tasks without requiring dedicated datasets and training,\noffering a lightweight and generalizable approach to enable language-based\nmobile interaction.",
        "pdf_link": "https://arxiv.org/pdf/2209.08655v2.pdf"
    },
    {
        "title": "CodeQueries: A Dataset of Semantic Queries over Code",
        "authors": [
            "Surya Prakash Sahu",
            "Madhurima Mandal",
            "Shikhar Bharadwaj",
            "Aditya Kanade",
            "Petros Maniatis",
            "Shirish Shevade"
        ],
        "published": "2022-09-17T17:09:30Z",
        "summary": "Developers often have questions about semantic aspects of code they are\nworking on, e.g., \"Is there a class whose parent classes declare a conflicting\nattribute?\". Answering them requires understanding code semantics such as\nattributes and inheritance relation of classes. An answer to such a question\nshould identify code spans constituting the answer (e.g., the declaration of\nthe subclass) as well as supporting facts (e.g., the definitions of the\nconflicting attributes). The existing work on question-answering over code has\nconsidered yes/no questions or method-level context. We contribute a labeled\ndataset, called CodeQueries, of semantic queries over Python code. Compared to\nthe existing datasets, in CodeQueries, the queries are about code semantics,\nthe context is file level and the answers are code spans. We curate the dataset\nbased on queries supported by a widely-used static analysis tool, CodeQL, and\ninclude both positive and negative examples, and queries requiring single-hop\nand multi-hop reasoning.\n  To assess the value of our dataset, we evaluate baseline neural approaches.\nWe study a large language model (GPT3.5-Turbo) in zero-shot and few-shot\nsettings on a subset of CodeQueries. We also evaluate a BERT style model\n(CuBERT) with fine-tuning. We find that these models achieve limited success on\nCodeQueries. CodeQueries is thus a challenging dataset to test the ability of\nneural models, to understand code semantics, in the extractive\nquestion-answering setting.",
        "pdf_link": "https://arxiv.org/pdf/2209.08372v2.pdf"
    },
    {
        "title": "Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models",
        "authors": [
            "Ben Prystawski",
            "Paul Thibodeau",
            "Christopher Potts",
            "Noah D. Goodman"
        ],
        "published": "2022-09-16T19:23:13Z",
        "summary": "Probabilistic models of language understanding are valuable tools for\ninvestigating human language use. However, they need to be hand-designed for a\nparticular domain. In contrast, large language models (LLMs) are trained on\ntext that spans a wide array of domains, but they lack the structure and\ninterpretability of probabilistic models. In this paper, we use\nchain-of-thought prompts to introduce structures from probabilistic models into\nLLMs. We explore this approach in the case of metaphor understanding. Our\nchain-of-thought prompts lead language models to infer latent variables and\nreason about their relationships in order to choose appropriate paraphrases for\nmetaphors. The latent variables and relationships chosen are informed by\ntheories of metaphor understanding from cognitive psychology. We apply these\nprompts to the two largest versions of GPT-3 and show that they can improve\nperformance in a paraphrase selection task.",
        "pdf_link": "https://arxiv.org/pdf/2209.08141v2.pdf"
    },
    {
        "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
        "authors": [
            "Aman Madaan",
            "Amir Yazdanbakhsh"
        ],
        "published": "2022-09-16T02:54:00Z",
        "summary": "The past decade has witnessed dramatic gains in natural language processing\nand an unprecedented scaling of large language models. These developments have\nbeen accelerated by the advent of few-shot techniques such as chain of thought\n(CoT) prompting. Specifically, CoT pushes the performance of large language\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\nDespite impressive results across various tasks, the reasons behind their\nsuccess have not been explored. This work uses counterfactual prompting to\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\nlarge language models. We first systematically identify and define the key\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\nconduct an exhaustive set of experiments across four different tasks, by\nquerying the model with counterfactual prompts where only one of these\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\nCODEX) reveal several surprising findings and brings into question the\nconventional wisdom around few-shot prompting. First, the presence of factual\npatterns in a prompt is practically immaterial to the success of CoT. Second,\nour results conclude that the primary role of intermediate steps may not be to\nfacilitate learning how to solve a task. The intermediate steps are rather a\nbeacon for the model to realize what symbols to replicate in the output to form\na factual answer. Further, text imbues patterns with commonsense knowledge and\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\nrelationship between text and patterns explains the success of few-shot\nprompting: text helps extract commonsense from the question to help patterns,\nand patterns enforce task understanding and direct text generation.",
        "pdf_link": "https://arxiv.org/pdf/2209.07686v2.pdf"
    },
    {
        "title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach",
        "authors": [
            "Yue Yu",
            "Rongzhi Zhang",
            "Ran Xu",
            "Jieyu Zhang",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2022-09-15T01:51:22Z",
        "summary": "Large Language Models have demonstrated remarkable few-shot performance, but\nthe performance can be sensitive to the selection of few-shot instances. We\npropose PATRON, a new method that uses prompt-based uncertainty estimation for\ndata selection for pre-trained language model fine-tuning under cold-start\nscenarios, i.e., no initial labeled data are available. In PATRON, we design\n(1) a prompt-based uncertainty propagation approach to estimate the importance\nof data points and (2) a partition-then-rewrite (PTR) strategy to promote\nsample diversity when querying for annotations. Experiments on six text\nclassification datasets show that PATRON outperforms the strongest cold-start\ndata selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON\nachieves 91.0% and 92.1% of the fully supervised performance based on vanilla\nfine-tuning and prompt-based learning respectively. Our implementation of\nPATRON is available at \\url{https://github.com/yueyu1030/Patron}.",
        "pdf_link": "https://arxiv.org/pdf/2209.06995v2.pdf"
    },
    {
        "title": "Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing",
        "authors": [
            "Hunter Osterhoudt",
            "Courtney E. Schneider",
            "Haneef A Mohammad",
            "Minmei Shih",
            "Alexandra E. Harper",
            "Leming Zhou",
            "Elizabeth R Skidmore",
            "Yanshan Wang"
        ],
        "published": "2022-09-14T15:33:30Z",
        "summary": "Strategy training is a multidisciplinary rehabilitation approach that teaches\nskills to reduce disability among those with cognitive impairments following a\nstroke. Strategy training has been shown in randomized, controlled clinical\ntrials to be a more feasible and efficacious intervention for promoting\nindependence than traditional rehabilitation approaches. A standardized\nfidelity assessment is used to measure adherence to treatment principles by\nexamining guided and directed verbal cues in video recordings of rehabilitation\nsessions. Although the fidelity assessment for detecting guided and directed\nverbal cues is valid and feasible for single-site studies, it can become labor\nintensive, time consuming, and expensive in large, multi-site pragmatic trials.\nTo address this challenge to widespread strategy training implementation, we\nleveraged natural language processing (NLP) techniques to automate the strategy\ntraining fidelity assessment, i.e., to automatically identify guided and\ndirected verbal cues from video recordings of rehabilitation sessions. We\ndeveloped a rule-based NLP algorithm, a long-short term memory (LSTM) model,\nand a bidirectional encoder representation from transformers (BERT) model for\nthis task. The best performance was achieved by the BERT model with a 0.8075\nF1-score. This BERT model was verified on an external validation dataset\ncollected from a separate major regional health system and achieved an F1 score\nof 0.8259, which shows that the BERT model generalizes well. The findings from\nthis study hold widespread promise in psychology and rehabilitation\nintervention research and practice.",
        "pdf_link": "https://arxiv.org/pdf/2209.06727v2.pdf"
    },
    {
        "title": "SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus",
        "authors": [
            "Yufeng Zhao",
            "Haiying Che"
        ],
        "published": "2022-09-13T05:49:10Z",
        "summary": "BERT is a widely used pre-trained model in natural language processing.\nHowever, since BERT is quadratic to the text length, the BERT model is\ndifficult to be used directly on the long-text corpus. In some fields, the\ncollected text data may be quite long, such as in the health care field.\nTherefore, to apply the pre-trained language knowledge of BERT to long text, in\nthis paper, imitating the skimming-intensive reading method used by humans when\nreading a long paragraph, the Skimming-Intensive Model (SkIn) is proposed. It\ncan dynamically select the critical information in the text so that the\nsentence input into the BERT-Base model is significantly shortened, which can\neffectively save the cost of the classification algorithm. Experiments show\nthat the SkIn method has achieved superior accuracy than the baselines on\nlong-text classification datasets in the medical field, while its time and\nspace requirements increase linearly with the text length, alleviating the time\nand space overflow problem of basic BERT on long-text data.",
        "pdf_link": "https://arxiv.org/pdf/2209.05741v2.pdf"
    },
    {
        "title": "DECK: Behavioral Tests to Improve Interpretability and Generalizability of BERT Models Detecting Depression from Text",
        "authors": [
            "Jekaterina Novikova",
            "Ksenia Shkaruta"
        ],
        "published": "2022-09-12T14:39:46Z",
        "summary": "Models that accurately detect depression from text are important tools for\naddressing the post-pandemic mental health crisis. BERT-based classifiers'\npromising performance and the off-the-shelf availability make them great\ncandidates for this task. However, these models are known to suffer from\nperformance inconsistencies and poor generalization. In this paper, we\nintroduce the DECK (DEpression ChecKlist), depression-specific model\nbehavioural tests that allow better interpretability and improve\ngeneralizability of BERT classifiers in depression domain. We create 23 tests\nto evaluate BERT, RoBERTa and ALBERT depression classifiers on three datasets,\ntwo Twitter-based and one clinical interview-based. Our evaluation shows that\nthese models: 1) are robust to certain gender-sensitive variations in text; 2)\nrely on the important depressive language marker of the increased use of first\nperson pronouns; 3) fail to detect some other depression symptoms like suicidal\nideation. We also demonstrate that DECK tests can be used to incorporate\nsymptom-specific information in the training data and consistently improve\ngeneralizability of all three BERT models, with an out-of-distribution F1-score\nincrease of up to 53.93%.",
        "pdf_link": "https://arxiv.org/pdf/2209.05286v1.pdf"
    },
    {
        "title": "T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition",
        "authors": [
            "Asahi Ushio",
            "Jose Camacho-Collados"
        ],
        "published": "2022-09-09T15:00:38Z",
        "summary": "Language model (LM) pretraining has led to consistent improvements in many\nNLP downstream tasks, including named entity recognition (NER). In this paper,\nwe present T-NER (Transformer-based Named Entity Recognition), a Python library\nfor NER LM finetuning. In addition to its practical utility, T-NER facilitates\nthe study and investigation of the cross-domain and cross-lingual\ngeneralization ability of LMs finetuned on NER. Our library also provides a web\napp where users can get model predictions interactively for arbitrary text,\nwhich facilitates qualitative model evaluation for non-expert programmers. We\nshow the potential of the library by compiling nine public NER datasets into a\nunified format and evaluating the cross-domain and cross-lingual performance\nacross the datasets. The results from our initial experiments show that\nin-domain performance is generally competitive across datasets. However,\ncross-domain generalization is challenging even with a large pretrained LM,\nwhich has nevertheless capacity to learn domain-specific features if fine-tuned\non a combined dataset. To facilitate future research, we also release all our\nLM checkpoints via the Hugging Face model hub.",
        "pdf_link": "https://arxiv.org/pdf/2209.12616v1.pdf"
    },
    {
        "title": "Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages",
        "authors": [
            "Li Miao",
            "Jian Wu",
            "Piyush Behre",
            "Shuangyu Chang",
            "Sarangarajan Parthasarathy"
        ],
        "published": "2022-09-08T21:40:41Z",
        "summary": "It is challenging to train and deploy Transformer LMs for hybrid speech\nrecognition 2nd pass re-ranking in low-resource languages due to (1) data\nscarcity in low-resource languages, (2) expensive computing costs for training\nand refreshing 100+ monolingual models, and (3) hosting inefficiency\nconsidering sparse traffic. In this study, we present a new way to group\nmultiple low-resource locales together and optimize the performance of\nMultilingual Transformer LMs in ASR. Our Locale-group Multilingual Transformer\nLMs outperform traditional multilingual LMs along with reducing maintenance\ncosts and operating expenses. Further, for low-resource but high-traffic\nlocales where deploying monolingual models is feasible, we show that\nfine-tuning our locale-group multilingual LMs produces better monolingual LM\ncandidates than baseline monolingual LMs.",
        "pdf_link": "https://arxiv.org/pdf/2209.04041v1.pdf"
    },
    {
        "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
        "authors": [
            "Wai Man Si",
            "Michael Backes",
            "Jeremy Blackburn",
            "Emiliano De Cristofaro",
            "Gianluca Stringhini",
            "Savvas Zannettou",
            "Yang Zhang"
        ],
        "published": "2022-09-07T20:45:41Z",
        "summary": "Chatbots are used in many applications, e.g., automated agents, smart home\nassistants, interactive characters in online games, etc. Therefore, it is\ncrucial to ensure they do not behave in undesired manners, providing offensive\nor toxic responses to users. This is not a trivial task as state-of-the-art\nchatbot models are trained on large, public datasets openly collected from the\nInternet. This paper presents a first-of-its-kind, large-scale measurement of\ntoxicity in chatbots. We show that publicly available chatbots are prone to\nproviding toxic responses when fed toxic queries. Even more worryingly, some\nnon-toxic queries can trigger toxic responses too. We then set out to design\nand experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to\ngenerate non-toxic queries that make chatbots respond in a toxic manner. Our\nextensive experimental evaluation demonstrates that our attack is effective\nagainst public chatbot models and outperforms manually-crafted malicious\nqueries proposed by previous work. We also evaluate three defense mechanisms\nagainst ToxicBuddy, showing that they either reduce the attack performance at\nthe cost of affecting the chatbot's utility or are only effective at mitigating\na portion of the attack. This highlights the need for more research from the\ncomputer security and online safety communities to ensure that chatbot models\ndo not hurt their users. Overall, we are confident that ToxicBuddy can be used\nas an auditing tool and that our work will pave the way toward designing more\neffective defenses for chatbot safety.",
        "pdf_link": "https://arxiv.org/pdf/2209.03463v2.pdf"
    },
    {
        "title": "AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles",
        "authors": [
            "Beatrice Portelli",
            "Simone Scaboro",
            "Emmanuele Chersoni",
            "Enrico Santus",
            "Giuseppe Serra"
        ],
        "published": "2022-09-07T20:17:15Z",
        "summary": "This paper describes the models developed by the AILAB-Udine team for the\nSMM4H 22 Shared Task. We explored the limits of Transformer based models on\ntext classification, entity extraction and entity normalization, tackling Tasks\n1, 2, 5, 6 and 10. The main take-aways we got from participating in different\ntasks are: the overwhelming positive effects of combining different\narchitectures when using ensemble learning, and the great potential of\ngenerative models for term normalization.",
        "pdf_link": "https://arxiv.org/pdf/2209.03452v1.pdf"
    },
    {
        "title": "The Ethical Need for Watermarks in Machine-Generated Language",
        "authors": [
            "Alexei Grinbaum",
            "Laurynas Adomaitis"
        ],
        "published": "2022-09-07T13:09:44Z",
        "summary": "Watermarks should be introduced in the natural language outputs of AI systems\nin order to maintain the distinction between human and machine-generated text.\nThe ethical imperative to not blur this distinction arises from the asemantic\nnature of large language models and from human projections of emotional and\ncognitive states on machines, possibly leading to manipulation, spreading\nfalsehoods or emotional distress. Enforcing this distinction requires\nunintrusive, yet easily accessible marks of the machine origin. We propose to\nimplement a code based on equidistant letter sequences. While no such code\nexists in human-written texts, its appearance in machine-generated ones would\nprove helpful for ethical reasons.",
        "pdf_link": "https://arxiv.org/pdf/2209.03118v1.pdf"
    },
    {
        "title": "Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",
        "authors": [
            "Hezekiah J. Branch",
            "Jonathan Rodriguez Cefalu",
            "Jeremy McHugh",
            "Leyla Hujer",
            "Aditya Bahl",
            "Daniel del Castillo Iglesias",
            "Ron Heichman",
            "Ramesh Darwishi"
        ],
        "published": "2022-09-05T20:29:17Z",
        "summary": "Recent advances in the development of large language models have resulted in\npublic access to state-of-the-art pre-trained language models (PLMs), including\nGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder\nRepresentations from Transformers (BERT). However, evaluations of PLMs, in\npractice, have shown their susceptibility to adversarial attacks during the\ntraining and fine-tuning stages of development. Such attacks can result in\nerroneous outputs, model-generated hate speech, and the exposure of users'\nsensitive information. While existing research has focused on adversarial\nattacks during either the training or the fine-tuning of PLMs, there is a\ndeficit of information on attacks made between these two development phases. In\nthis work, we highlight a major security vulnerability in the public release of\nGPT-3 and further investigate this vulnerability in other state-of-the-art\nPLMs. We restrict our work to pre-trained models that have not undergone\nfine-tuning. Further, we underscore token distance-minimized perturbations as\nan effective adversarial approach, bypassing both supervised and unsupervised\nquality measures. Following this approach, we observe a significant decrease in\ntext classification quality when evaluating for semantic similarity.",
        "pdf_link": "https://arxiv.org/pdf/2209.02128v1.pdf"
    },
    {
        "title": "Every picture tells a story: Image-grounded controllable stylistic story generation",
        "authors": [
            "Holy Lovenia",
            "Bryan Wilie",
            "Romain Barraud",
            "Samuel Cahyawijaya",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2022-09-04T15:07:53Z",
        "summary": "Generating a short story out of an image is arduous. Unlike image captioning,\nstory generation from an image poses multiple challenges: preserving the story\ncoherence, appropriately assessing the quality of the story, steering the\ngenerated story into a certain style, and addressing the scarcity of\nimage-story pair reference datasets limiting supervision during training. In\nthis work, we introduce Plug-and-Play Story Teller (PPST) and improve\nimage-to-story generation by: 1) alleviating the data scarcity problem by\nincorporating large pre-trained models, namely CLIP and GPT-2, to facilitate a\nfluent image-to-text generation with minimal supervision, and 2) enabling a\nmore style-relevant generation by incorporating stylistic adapters to control\nthe story generation. We conduct image-to-story generation experiments with\nnon-styled, romance-styled, and action-styled PPST approaches and compare our\ngenerated stories with those of previous work over three aspects, i.e., story\ncoherence, image-story relevance, and style fitness, using both automatic and\nhuman evaluation. The results show that PPST improves story coherence and has\nbetter image-story relevance, but has yet to be adequately stylistic.",
        "pdf_link": "https://arxiv.org/pdf/2209.01638v2.pdf"
    },
    {
        "title": "Do Large Language Models know what humans know?",
        "authors": [
            "Sean Trott",
            "Cameron Jones",
            "Tyler Chang",
            "James Michaelov",
            "Benjamin Bergen"
        ],
        "published": "2022-09-04T01:29:53Z",
        "summary": "Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers' mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others' beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior -- despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.",
        "pdf_link": "https://arxiv.org/pdf/2209.01515v3.pdf"
    },
    {
        "title": "Neural Approaches to Multilingual Information Retrieval",
        "authors": [
            "Dawn Lawrie",
            "Eugene Yang",
            "Douglas W. Oard",
            "James Mayfield"
        ],
        "published": "2022-09-03T06:02:52Z",
        "summary": "Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.",
        "pdf_link": "https://arxiv.org/pdf/2209.01335v2.pdf"
    },
    {
        "title": "Petals: Collaborative Inference and Fine-tuning of Large Models",
        "authors": [
            "Alexander Borzunov",
            "Dmitry Baranchuk",
            "Tim Dettmers",
            "Max Ryabinin",
            "Younes Belkada",
            "Artem Chumachenko",
            "Pavel Samygin",
            "Colin Raffel"
        ],
        "published": "2022-09-02T17:38:03Z",
        "summary": "Many NLP tasks benefit from using large language models (LLMs) that often\nhave more than 100 billion parameters. With the release of BLOOM-176B and\nOPT-175B, everyone can download pretrained models of this scale. Still, using\nthese models requires high-end hardware unavailable to many researchers. In\nsome cases, LLMs can be used more affordably via RAM offloading or hosted APIs.\nHowever, these techniques have innate limitations: offloading is too slow for\ninteractive inference, while APIs are not flexible enough for research that\nrequires access to weights, attention or logits. In this work, we propose\nPetals - a system for inference and fine-tuning of large models collaboratively\nby joining the resources of multiple parties. We demonstrate that this strategy\noutperforms offloading for very large models, running inference of BLOOM-176B\non consumer GPUs with $\\approx$ 1 step per second, which is enough for many\ninteractive LLM applications. Unlike most inference APIs, Petals also natively\nexposes hidden states of served models, allowing to train and share custom\nmodel extensions based on efficient fine-tuning methods.",
        "pdf_link": "https://arxiv.org/pdf/2209.01188v2.pdf"
    },
    {
        "title": "Generating Coherent Drum Accompaniment With Fills And Improvisations",
        "authors": [
            "Rishabh Dahale",
            "Vaibhav Talwadker",
            "Preeti Rao",
            "Prateek Verma"
        ],
        "published": "2022-09-01T08:31:26Z",
        "summary": "Creating a complex work of art like music necessitates profound creativity.\nWith recent advancements in deep learning and powerful models such as\ntransformers, there has been huge progress in automatic music generation. In an\naccompaniment generation context, creating a coherent drum pattern with\napposite fills and improvisations at proper locations in a song is a\nchallenging task even for an experienced drummer. Drum beats tend to follow a\nrepetitive pattern through stanzas with fills or improvisation at section\nboundaries. In this work, we tackle the task of drum pattern generation\nconditioned on the accompanying music played by four melodic instruments:\nPiano, Guitar, Bass, and Strings. We use the transformer sequence to sequence\nmodel to generate a basic drum pattern conditioned on the melodic accompaniment\nto find that improvisation is largely absent, attributed possibly to its\nexpectedly relatively low representation in the training data. We propose a\nnovelty function to capture the extent of improvisation in a bar relative to\nits neighbors. We train a model to predict improvisation locations from the\nmelodic accompaniment tracks. Finally, we use a novel BERT-inspired in-filling\narchitecture, to learn the structure of both the drums and melody to in-fill\nelements of improvised music.",
        "pdf_link": "https://arxiv.org/pdf/2209.00291v1.pdf"
    },
    {
        "title": "Enhancing Semantic Understanding with Self-supervised Methods for Abstractive Dialogue Summarization",
        "authors": [
            "Hyunjae Lee",
            "Jaewoong Yun",
            "Hyunjin Choi",
            "Seongho Joe",
            "Youngjune L. Gwon"
        ],
        "published": "2022-09-01T07:51:46Z",
        "summary": "Contextualized word embeddings can lead to state-of-the-art performances in\nnatural language understanding. Recently, a pre-trained deep contextualized\ntext encoder such as BERT has shown its potential in improving natural language\ntasks including abstractive summarization. Existing approaches in dialogue\nsummarization focus on incorporating a large language model into summarization\ntask trained on large-scale corpora consisting of news articles rather than\ndialogues of multiple speakers. In this paper, we introduce self-supervised\nmethods to compensate shortcomings to train a dialogue summarization model. Our\nprinciple is to detect incoherent information flows using pretext dialogue text\nto enhance BERT's ability to contextualize the dialogue text representations.\nWe build and fine-tune an abstractive dialogue summarization model on a shared\nencoder-decoder architecture using the enhanced BERT. We empirically evaluate\nour abstractive dialogue summarizer with the SAMSum corpus, a recently\nintroduced dataset with abstractive dialogue summaries. All of our methods have\ncontributed improvements to abstractive summary measured in ROUGE scores.\nThrough an extensive ablation study, we also present a sensitivity analysis to\ncritical model hyperparameters, probabilities of switching utterances and\nmasking interlocutors.",
        "pdf_link": "https://arxiv.org/pdf/2209.00278v1.pdf"
    },
    {
        "title": "Faithful Reasoning Using Large Language Models",
        "authors": [
            "Antonia Creswell",
            "Murray Shanahan"
        ],
        "published": "2022-08-30T13:44:41Z",
        "summary": "Although contemporary large language models (LMs) demonstrate impressive\nquestion-answering capabilities, their answers are typically the product of a\nsingle call to the model. This entails an unwelcome degree of opacity and\ncompromises performance, especially on problems that are inherently multi-step.\nTo address these limitations, we show how LMs can be made to perform faithful\nmulti-step reasoning via a process whose causal structure mirrors the\nunderlying logical structure of the problem. Our approach works by chaining\ntogether reasoning steps, where each step results from calls to two fine-tuned\nLMs, one for selection and one for inference, to produce a valid reasoning\ntrace. Our method carries out a beam search through the space of reasoning\ntraces to improve reasoning quality. We demonstrate the effectiveness of our\nmodel on multi-step logical deduction and scientific question-answering,\nshowing that it outperforms baselines on final answer accuracy, and generates\nhumanly interpretable reasoning traces whose validity can be checked by the\nuser.",
        "pdf_link": "https://arxiv.org/pdf/2208.14271v1.pdf"
    },
    {
        "title": "LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems",
        "authors": [
            "Björn Deiseroth",
            "Patrick Schramowski",
            "Hikaru Shindo",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "published": "2022-08-29T11:40:36Z",
        "summary": "Text-to-image models have recently achieved remarkable success with seemingly\naccurate samples in photo-realistic quality. However as state-of-the-art\nlanguage models still struggle evaluating precise statements consistently, so\ndo language model based image generation processes. In this work we showcase\nproblems of state-of-the-art text-to-image models like DALL-E with generating\naccurate samples from statements related to the draw bench benchmark.\nFurthermore we show that CLIP is not able to rerank those generated samples\nconsistently. To this end we propose LogicRank, a neuro-symbolic reasoning\nframework that can result in a more accurate ranking-system for such\nprecision-demanding settings. LogicRank integrates smoothly into the generation\nprocess of text-to-image models and moreover can be used to further fine-tune\ntowards a more logical precise model.",
        "pdf_link": "https://arxiv.org/pdf/2208.13518v1.pdf"
    },
    {
        "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
        "authors": [
            "Kaizhi Zheng",
            "Kaiwen Zhou",
            "Jing Gu",
            "Yue Fan",
            "Jialu Wang",
            "Zonglin Di",
            "Xuehai He",
            "Xin Eric Wang"
        ],
        "published": "2022-08-28T18:30:46Z",
        "summary": "Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge.",
        "pdf_link": "https://arxiv.org/pdf/2208.13266v3.pdf"
    },
    {
        "title": "Target Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization",
        "authors": [
            "Dongmei Wang",
            "Xiong Xiao",
            "Naoyuki Kanda",
            "Takuya Yoshioka",
            "Jian Wu"
        ],
        "published": "2022-08-27T21:11:45Z",
        "summary": "This paper describes a speaker diarization model based on target speaker\nvoice activity detection (TS-VAD) using transformers. To overcome the original\nTS-VAD model's drawback of being unable to handle an arbitrary number of\nspeakers, we investigate model architectures that use input tensors with\nvariable-length time and speaker dimensions. Transformer layers are applied to\nthe speaker axis to make the model output insensitive to the order of the\nspeaker profiles provided to the TS-VAD model. Time-wise sequential layers are\ninterspersed between these speaker-wise transformer layers to allow the\ntemporal and cross-speaker correlations of the input speech signal to be\ncaptured. We also extend a diarization model based on end-to-end neural\ndiarization with encoder-decoder based attractors (EEND-EDA) by replacing its\ndot-product-based speaker detection layer with the transformer-based TS-VAD.\nExperimental results on VoxConverse show that using the transformers for the\ncross-speaker modeling reduces the diarization error rate (DER) of TS-VAD by\n11.3%, achieving a new state-of-the-art (SOTA) DER of 4.57%. Also, our extended\nEEND-EDA reduces DER by 6.9% on the CALLHOME dataset relative to the original\nEEND-EDA with a similar model size, achieving a new SOTA DER of 11.18% under a\nwidely used training data setting.",
        "pdf_link": "https://arxiv.org/pdf/2208.13085v3.pdf"
    },
    {
        "title": "Training a T5 Using Lab-sized Resources",
        "authors": [
            "Manuel R. Ciosici",
            "Leon Derczynski"
        ],
        "published": "2022-08-25T13:55:16Z",
        "summary": "Training large neural language models on large datasets is resource- and\ntime-intensive. These requirements create a barrier to entry, where those with\nfewer resources cannot build competitive models. This paper presents various\ntechniques for making it possible to (a) train a large language model using\nresources that a modest research lab might have, and (b) train it in a\nreasonable amount of time. We provide concrete recommendations for\npractitioners, which we illustrate with a case study: a T5 model for Danish,\nthe first for this language.",
        "pdf_link": "https://arxiv.org/pdf/2208.12097v1.pdf"
    },
    {
        "title": "On Reality and the Limits of Language Data: Aligning LLMs with Human Norms",
        "authors": [
            "Nigel H. Collier",
            "Fangyu Liu",
            "Ehsan Shareghi"
        ],
        "published": "2022-08-25T10:21:23Z",
        "summary": "Recent advancements in Large Language Models (LLMs) harness linguistic\nassociations in vast natural language data for practical applications. However,\ntheir ability to understand the physical world using only language data remains\na question. After reviewing existing protocols, we explore this question using\na novel and tightly controlled reasoning test (ART) and compare human norms\nagainst versions of GPT-3. Our findings highlight the categories of\ncommon-sense relations models that could learn directly from data and areas of\nweakness. GPT-3 offers evidence for verbal reasoning on a par with human\nsubjects for several relations including Synonymy, Antonymy, and Default\ninheritance, Without reinforcement learning from human judgements, it appears\nGPT-3 performs at the lower end of the reference interval for Has-part and\nContained-in. Weaknesses were observed also in affordance characteristics\nthrough Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs\nwith symbolic world grounding is a promising direction to address associative\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2208.11981v2.pdf"
    },
    {
        "title": "Shortcut Learning of Large Language Models in Natural Language Understanding",
        "authors": [
            "Mengnan Du",
            "Fengxiang He",
            "Na Zou",
            "Dacheng Tao",
            "Xia Hu"
        ],
        "published": "2022-08-25T03:51:39Z",
        "summary": "Large language models (LLMs) have achieved state-of-the-art performance on a\nseries of natural language understanding tasks. However, these LLMs might rely\non dataset bias and artifacts as shortcuts for prediction. This has\nsignificantly affected their generalizability and adversarial robustness. In\nthis paper, we provide a review of recent developments that address the\nshortcut learning and robustness challenge of LLMs. We first introduce the\nconcepts of shortcut learning of language models. We then introduce methods to\nidentify shortcut learning behavior in language models, characterize the\nreasons for shortcut learning, as well as introduce mitigation solutions.\nFinally, we discuss key research challenges and potential research directions\nin order to advance the field of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2208.11857v2.pdf"
    },
    {
        "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
        "authors": [
            "Harshit Joshi",
            "José Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Ivan Radicek",
            "Gust Verbruggen"
        ],
        "published": "2022-08-24T16:25:58Z",
        "summary": "Most programmers make mistakes when writing code. Some of these mistakes are\nsmall and require few edits to the original program -- a class of errors\nrecently termed last mile mistakes. These errors break the flow for experienced\ndevelopers and can stump novice programmers. Existing automated repair\ntechniques targeting this class of errors are language-specific and do not\neasily carry over to new languages. Transferring symbolic approaches requires\nsubstantial engineering and neural approaches require data and retraining. We\nintroduce RING, a multilingual repair engine powered by a large language model\ntrained on code (LLMC) such as Codex. Such a multilingual engine enables a\nflipped model for programming assistance, one where the programmer writes code\nand the AI assistance suggests fixes, compared to traditional code suggestion\ntechnology. Taking inspiration from the way programmers manually fix bugs, we\nshow that a prompt-based strategy that conceptualizes repair as localization,\ntransformation, and candidate ranking, can successfully repair programs in\nmultiple languages with minimal effort. We present the first results for such a\nmultilingual repair engine by evaluating on 6 different languages and comparing\nperformance to language-specific repair engines. We show that RING can\noutperform language-specific repair engines for three of these languages.",
        "pdf_link": "https://arxiv.org/pdf/2208.11640v3.pdf"
    },
    {
        "title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval",
        "authors": [
            "Zhengyang Tang",
            "Benyou Wang",
            "Ting Yao"
        ],
        "published": "2022-08-24T12:55:00Z",
        "summary": "Deep prompt tuning (DPT) has gained great success in most natural language\nprocessing~(NLP) tasks. However, it is not well-investigated in dense retrieval\nwhere fine-tuning~(FT) still dominates. When deploying multiple retrieval tasks\nusing the same backbone model~(e.g., RoBERTa), FT-based methods are unfriendly\nin terms of deployment cost: each new retrieval model needs to repeatedly\ndeploy the backbone model without reuse. To reduce the deployment cost in such\na scenario, this work investigates applying DPT in dense retrieval. The\nchallenge is that directly applying DPT in dense retrieval largely\nunderperforms FT methods. To compensate for the performance drop, we propose\ntwo model-agnostic and task-agnostic strategies for DPT-based retrievers,\nnamely retrieval-oriented intermediate pretraining and unified negative mining,\nas a general approach that could be compatible with any pre-trained language\nmodel and retrieval task. The experimental results show that the proposed\nmethod (called DPTDR) outperforms previous state-of-the-art models on both\nMS-MARCO and Natural Questions. We also conduct ablation studies to examine the\neffectiveness of each strategy in DPTDR. We believe this work facilitates the\nindustry, as it saves enormous efforts and costs of deployment and increases\nthe utility of computing resources. Our code is available at\nhttps://github.com/tangzhy/DPTDR.",
        "pdf_link": "https://arxiv.org/pdf/2208.11503v1.pdf"
    },
    {
        "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models",
        "authors": [
            "Mirelle Bueno",
            "Carlos Gemmell",
            "Jeffrey Dalton",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-08-24T11:25:27Z",
        "summary": "The ability to extrapolate, i.e., to make predictions on sequences that are\nlonger than those presented as training examples, is a challenging problem for\ncurrent deep learning models. Recent work shows that this limitation persists\nin state-of-the-art Transformer-based models. Most solutions to this problem\nuse specific architectures or training methods that do not generalize to other\ntasks. We demonstrate that large language models can succeed in extrapolation\nwithout modifying their architecture or training procedure. Our experimental\nresults show that generating step-by-step rationales and introducing marker\ntokens are both required for effective extrapolation. First, we induce a\nlanguage model to produce step-by-step rationales before outputting the answer\nto effectively communicate the task to the model. However, as sequences become\nlonger, we find that current models struggle to keep track of token positions.\nTo address this issue, we interleave output tokens with markup tokens that act\nas explicit positional and counting symbols. Our findings show how these two\ncomplementary approaches enable remarkable sequence extrapolation and highlight\na limitation of current architectures to effectively generalize without\nexplicit surface form guidance. Code available at\nhttps://github.com/MirelleB/induced-rationales-markup-tokens",
        "pdf_link": "https://arxiv.org/pdf/2208.11445v3.pdf"
    },
    {
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
        "authors": [
            "Deep Ganguli",
            "Liane Lovitt",
            "Jackson Kernion",
            "Amanda Askell",
            "Yuntao Bai",
            "Saurav Kadavath",
            "Ben Mann",
            "Ethan Perez",
            "Nicholas Schiefer",
            "Kamal Ndousse",
            "Andy Jones",
            "Sam Bowman",
            "Anna Chen",
            "Tom Conerly",
            "Nova DasSarma",
            "Dawn Drain",
            "Nelson Elhage",
            "Sheer El-Showk",
            "Stanislav Fort",
            "Zac Hatfield-Dodds",
            "Tom Henighan",
            "Danny Hernandez",
            "Tristan Hume",
            "Josh Jacobson",
            "Scott Johnston",
            "Shauna Kravec",
            "Catherine Olsson",
            "Sam Ringer",
            "Eli Tran-Johnson",
            "Dario Amodei",
            "Tom Brown",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Chris Olah",
            "Jared Kaplan",
            "Jack Clark"
        ],
        "published": "2022-08-23T23:37:14Z",
        "summary": "We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.",
        "pdf_link": "https://arxiv.org/pdf/2209.07858v2.pdf"
    },
    {
        "title": "Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning",
        "authors": [
            "Letian Peng",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "published": "2022-08-23T14:42:14Z",
        "summary": "Commonsense reasoning is an appealing topic in natural language processing\n(NLP) as it plays a fundamental role in supporting the human-like actions of\nNLP systems. With large-scale language models as the backbone, unsupervised\npre-training on numerous corpora shows the potential to capture commonsense\nknowledge. Current pre-trained language model (PLM)-based reasoning follows the\ntraditional practice using perplexity metric. However, commonsense reasoning is\nmore than existing probability evaluation, which is biased by word frequency.\nThis paper reconsiders the nature of commonsense reasoning and proposes a novel\ncommonsense reasoning metric, Non-Replacement Confidence (NRC). In detail, it\nworks on PLMs according to the Replaced Token Detection (RTD) pre-training\nobjective in ELECTRA, in which the corruption detection objective reflects the\nconfidence on contextual integrity that is more relevant to commonsense\nreasoning than existing probability. Our proposed novel method boosts zero-shot\nperformance on two commonsense reasoning benchmark datasets and further seven\ncommonsense question-answering datasets. Our analysis shows that pre-endowed\ncommonsense knowledge, especially for RTD-based PLMs, is essential in\ndownstream reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2208.11007v1.pdf"
    },
    {
        "title": "Interpreting Embedding Spaces by Conceptualization",
        "authors": [
            "Adi Simhi",
            "Shaul Markovitch"
        ],
        "published": "2022-08-22T15:32:17Z",
        "summary": "One of the main methods for computational interpretation of a text is mapping\nit into a vector in some embedding space. Such vectors can then be used for a\nvariety of textual processing tasks. Recently, most embedding spaces are a\nproduct of training large language models (LLMs). One major drawback of this\ntype of representation is their incomprehensibility to humans. Understanding\nthe embedding space is crucial for several important needs, including the need\nto debug the embedding method and compare it to alternatives, and the need to\ndetect biases hidden in the model. In this paper, we present a novel method of\nunderstanding embeddings by transforming a latent embedding space into a\ncomprehensible conceptual space. We present an algorithm for deriving a\nconceptual space with dynamic on-demand granularity. We devise a new evaluation\nmethod, using either human rater or LLM-based raters, to show that the\nconceptualized vectors indeed represent the semantics of the original latent\nones. We show the use of our method for various tasks, including comparing the\nsemantics of alternative models and tracing the layers of the LLM. The code is\navailable online\nhttps://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.",
        "pdf_link": "https://arxiv.org/pdf/2209.00445v3.pdf"
    },
    {
        "title": "Selection Collider Bias in Large Language Models",
        "authors": [
            "Emily McMilin"
        ],
        "published": "2022-08-22T05:38:15Z",
        "summary": "In this paper we motivate the causal mechanisms behind sample selection\ninduced collider bias (selection collider bias) that can cause Large Language\nModels (LLMs) to learn unconditional dependence between entities that are\nunconditionally independent in the real world. We show that selection collider\nbias can become amplified in underspecified learning tasks, and although\ndifficult to overcome, we describe a method to exploit the resulting spurious\ncorrelations for determination of when a model may be uncertain about its\nprediction. We demonstrate an uncertainty metric that matches human uncertainty\nin tasks with gender pronoun underspecification on an extended version of the\nWinogender Schemas evaluation set, and we provide an online demo where users\ncan apply our uncertainty metric to their own texts and models.",
        "pdf_link": "https://arxiv.org/pdf/2208.10063v2.pdf"
    },
    {
        "title": "GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization",
        "authors": [
            "Qianqian Xie",
            "Jimin Huang",
            "Tulika Saha",
            "Sophia Ananiadou"
        ],
        "published": "2022-08-21T23:09:29Z",
        "summary": "Recently, neural topic models (NTMs) have been incorporated into pre-trained\nlanguage models (PLMs), to capture the global semantic information for text\nsummarization. However, in these methods, there remain limitations in the way\nthey capture and integrate the global semantic information. In this paper, we\npropose a novel model, the graph contrastive topic enhanced language model\n(GRETEL), that incorporates the graph contrastive topic model with the\npre-trained language model, to fully leverage both the global and local\ncontextual semantics for long document extractive summarization. To better\ncapture and incorporate the global semantic information into PLMs, the graph\ncontrastive topic model integrates the hierarchical transformer encoder and the\ngraph contrastive learning to fuse the semantic information from the global\ndocument context and the gold summary. To this end, GRETEL encourages the model\nto efficiently extract salient sentences that are topically related to the gold\nsummary, rather than redundant sentences that cover sub-optimal topics.\nExperimental results on both general domain and biomedical datasets demonstrate\nthat our proposed method outperforms SOTA methods.",
        "pdf_link": "https://arxiv.org/pdf/2208.09982v1.pdf"
    },
    {
        "title": "VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media",
        "authors": [
            "Georgios Chochlakis",
            "Tejas Srinivasan",
            "Jesse Thomason",
            "Shrikanth Narayanan"
        ],
        "published": "2022-08-18T18:51:13Z",
        "summary": "We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight and\nnovelty of VAuLT is to propagate the output representations of a large language\nmodel (LM) like BERT to the language input of ViLT. We show that joint training\nof the LM and ViLT can yield relative improvements up to 20% over ViLT and\nachieve state-of-the-art or comparable performance on VL tasks involving richer\nlanguage inputs and affective constructs, such as for Target-Oriented Sentiment\nClassification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification\nin MVSA-Single and MVSA-Multiple. Our code is available at\nhttps://github.com/gchochla/VAuLT.",
        "pdf_link": "https://arxiv.org/pdf/2208.09021v3.pdf"
    },
    {
        "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
        "authors": [
            "Gati Aher",
            "Rosa I. Arriaga",
            "Adam Tauman Kalai"
        ],
        "published": "2022-08-18T17:54:49Z",
        "summary": "We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model's simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.",
        "pdf_link": "https://arxiv.org/pdf/2208.10264v5.pdf"
    },
    {
        "title": "MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation",
        "authors": [
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "published": "2022-08-18T04:28:20Z",
        "summary": "Building dialogue generation systems in a zero-shot scenario remains a huge\nchallenge, since the typical zero-shot approaches in dialogue generation rely\nheavily on large-scale pre-trained language generation models such as GPT-3 and\nT5. The research on zero-shot dialogue generation without cumbersome language\nmodels is limited due to lacking corresponding parallel dialogue corpora. In\nthis paper, we propose a simple but effective Multilingual learning framework\nfor Zero-shot Dialogue Generation (dubbed as MulZDG) that can effectively\ntransfer knowledge from an English corpus with large-scale training samples to\na non-English corpus with zero samples. Besides, MulZDG can be viewed as a\nmultilingual data augmentation method to improve the performance of the\nresource-rich language. First, we construct multilingual code-switching\ndialogue datasets via translation utterances randomly selected from monolingual\nEnglish datasets. Then we employ MulZDG to train a unified multilingual\ndialogue model based on the code-switching datasets. The MulZDG can conduct\nimplicit semantic alignment between different languages. Experiments on\nDailyDialog and DSTC7 datasets demonstrate that MulZDG not only achieve\ncompetitive performance under zero-shot case compared to training with\nsufficient examples but also greatly improve the performance of the source\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2208.08629v1.pdf"
    },
    {
        "title": "Ask Question First for Enhancing Lifelong Language Learning",
        "authors": [
            "Han Wang",
            "Ruiliu Fu",
            "Xuejun Zhang",
            "Jun Zhou",
            "Qingwei Zhao"
        ],
        "published": "2022-08-17T15:58:33Z",
        "summary": "Lifelong language learning aims to stream learning NLP tasks while retaining\nknowledge of previous tasks. Previous works based on the language model and\nfollowing data-free constraint approaches have explored formatting all data as\n\"begin token (\\textit{B}) + context (\\textit{C}) + question (\\textit{Q}) +\nanswer (\\textit{A})\" for different tasks. However, they still suffer from\ncatastrophic forgetting and are exacerbated when the previous task's pseudo\ndata is insufficient for the following reasons: (1) The model has difficulty\ngenerating task-corresponding pseudo data, and (2) \\textit{A} is prone to error\nwhen \\textit{A} and \\textit{C} are separated by \\textit{Q} because the\ninformation of the \\textit{C} is diminished before generating \\textit{A}.\nTherefore, we propose the Ask Question First and Replay Question (AQF-RQ),\nincluding a novel data format \"\\textit{BQCA}\" and a new training task to train\npseudo questions of previous tasks. Experimental results demonstrate that\nAQF-RQ makes it easier for the model to generate more pseudo data that match\ncorresponding tasks, and is more robust to both sufficient and insufficient\npseudo-data when the task boundary is both clear and unclear. AQF-RQ can\nachieve only 0.36\\% lower performance than multi-task learning.",
        "pdf_link": "https://arxiv.org/pdf/2208.08367v2.pdf"
    },
    {
        "title": "HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models",
        "authors": [
            "Swaroop Mishra",
            "Elnaz Nouri"
        ],
        "published": "2022-08-17T11:20:41Z",
        "summary": "Controlling the text generated by language models and customizing the content\nhas been a long-standing challenge. Existing prompting techniques proposed in\npursuit of providing control are task-specific and lack generality; this\nprovides overwhelming choices for non-expert users to find a suitable method\nfor their task. The effort associated with those techniques, such as in writing\nexamples, explanations, instructions, etc. further limits their adoption among\nnon-expert users. In this paper, we propose a simple prompting strategy HELP ME\nTHINK where we encourage GPT3 to help non-expert users by asking a set of\nrelevant questions and leveraging user answers to execute the task. We\ndemonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.\nSpecifically, we focus on tasks that are hard for average humans and require\nsignificant thinking to perform. We hope our work will encourage the\ndevelopment of unconventional ways to harness the power of large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2208.08232v2.pdf"
    },
    {
        "title": "Transformer Encoder for Social Science",
        "authors": [
            "Haosen Ge",
            "In Young Park",
            "Xuancheng Qian",
            "Grace Zeng"
        ],
        "published": "2022-08-17T01:01:25Z",
        "summary": "High-quality text data has become an important data source for social\nscientists. We have witnessed the success of pretrained deep neural network\nmodels, such as BERT and RoBERTa, in recent social science research. In this\npaper, we propose a compact pretrained deep neural network, Transformer Encoder\nfor Social Science (TESS), explicitly designed to tackle text processing tasks\nin social science research. Using two validation tests, we demonstrate that\nTESS outperforms BERT and RoBERTa by 16.7% on average when the number of\ntraining samples is limited (<1,000 training instances). The results display\nthe superiority of TESS over BERT and RoBERTa on social science text processing\ntasks. Lastly, we discuss the limitation of our model and present advice for\nfuture researchers.",
        "pdf_link": "https://arxiv.org/pdf/2208.08005v1.pdf"
    },
    {
        "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
        "authors": [
            "Tim Dettmers",
            "Mike Lewis",
            "Younes Belkada",
            "Luke Zettlemoyer"
        ],
        "published": "2022-08-15T17:08:50Z",
        "summary": "Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.",
        "pdf_link": "https://arxiv.org/pdf/2208.07339v2.pdf"
    },
    {
        "title": "Targeted Honeyword Generation with Language Models",
        "authors": [
            "Fangyi Yu",
            "Miguel Vargas Martin"
        ],
        "published": "2022-08-15T00:06:29Z",
        "summary": "Honeywords are fictitious passwords inserted into databases in order to\nidentify password breaches. The major difficulty is how to produce honeywords\nthat are difficult to distinguish from real passwords. Although the generation\nof honeywords has been widely investigated in the past, the majority of\nexisting research assumes attackers have no knowledge of the users. These\nhoneyword generating techniques (HGTs) may utterly fail if attackers exploit\nusers' personally identifiable information (PII) and the real passwords include\nusers' PII. In this paper, we propose to build a more secure and trustworthy\nauthentication system that employs off-the-shelf pre-trained language models\nwhich require no further training on real passwords to produce honeywords while\nretaining the PII of the associated real password, therefore significantly\nraising the bar for attackers.\n  We conducted a pilot experiment in which individuals are asked to distinguish\nbetween authentic passwords and honeywords when the username is provided for\nGPT-3 and a tweaking technique. Results show that it is extremely difficult to\ndistinguish the real passwords from the artifical ones for both techniques. We\nspeculate that a larger sample size could reveal a significant difference\nbetween the two HGT techniques, favouring our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2208.06946v2.pdf"
    },
    {
        "title": "What is it like to program with artificial intelligence?",
        "authors": [
            "Advait Sarkar",
            "Andrew D. Gordon",
            "Carina Negreanu",
            "Christian Poelitz",
            "Sruti Srinivasa Ragavan",
            "Ben Zorn"
        ],
        "published": "2022-08-12T10:48:46Z",
        "summary": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can\ngenerate code to solve a variety of problems expressed in natural language.\nThis technology has already been commercialised in at least one widely-used\nprogramming editor extension: GitHub Copilot.\n  In this paper, we explore how programming with large language models\n(LLM-assisted programming) is similar to, and differs from, prior\nconceptualisations of programmer assistance. We draw upon publicly available\nexperience reports of LLM-assisted programming, as well as prior usability and\ndesign studies. We find that while LLM-assisted programming shares some\nproperties of compilation, pair programming, and programming via search and\nreuse, there are fundamental differences both in the technical possibilities as\nwell as the practical experience. Thus, LLM-assisted programming ought to be\nviewed as a new way of programming with its own distinct properties and\nchallenges.\n  Finally, we draw upon observations from a user study in which non-expert end\nuser programmers use LLM-assisted tools for solving data tasks in spreadsheets.\nWe discuss the issues that might arise, and open research challenges, in\napplying large language models to end-user programming, particularly with users\nwho have little or no programming expertise.",
        "pdf_link": "https://arxiv.org/pdf/2208.06213v2.pdf"
    },
    {
        "title": "New drugs and stock market: how to predict pharma market reaction to clinical trial announcements",
        "authors": [
            "Semen Budennyy",
            "Alexey Kazakov",
            "Elizaveta Kovtun",
            "Leonid Zhukov"
        ],
        "published": "2022-08-11T20:20:21Z",
        "summary": "Pharmaceutical companies operate in a strictly regulated and highly risky\nenvironment in which a single slip can lead to serious financial implications.\nAccordingly, the announcements of clinical trial results tend to determine the\nfuture course of events, hence being closely monitored by the public. In this\nwork, we provide statistical evidence for the result promulgation influence on\nthe public pharma market value. Whereas most works focus on retrospective\nimpact analysis, the present research aims to predict the numerical values of\nannouncement-induced changes in stock prices. For this purpose, we develop a\npipeline that includes a BERT-based model for extracting sentiment polarity of\nannouncements, a Temporal Fusion Transformer for forecasting the expected\nreturn, a graph convolution network for capturing event relationships, and\ngradient boosting for predicting the price change. The challenge of the problem\nlies in inherently different patterns of responses to positive and negative\nannouncements, reflected in a stronger and more pronounced reaction to the\nnegative news. Moreover, such phenomenon as the drop in stocks after the\npositive announcements affirms the counterintuitiveness of the price behavior.\nImportantly, we discover two crucial factors that should be considered while\nworking within a predictive framework. The first factor is the drug portfolio\nsize of the company, indicating the greater susceptibility to an announcement\nin the case of small drug diversification. The second one is the network effect\nof the events related to the same company or nosology. All findings and\ninsights are gained on the basis of one of the biggest FDA (the Food and Drug\nAdministration) announcement datasets, consisting of 5436 clinical trial\nannouncements from 681 companies over the last five years.",
        "pdf_link": "https://arxiv.org/pdf/2208.07248v2.pdf"
    },
    {
        "title": "Interactive Code Generation via Test-Driven User-Intent Formalization",
        "authors": [
            "Shuvendu K. Lahiri",
            "Sarah Fakhoury",
            "Aaditya Naik",
            "Georgios Sakkas",
            "Saikat Chakraborty",
            "Madanlal Musuvathi",
            "Piali Choudhury",
            "Curtis von Veh",
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Jianfeng Gao"
        ],
        "published": "2022-08-11T17:41:08Z",
        "summary": "Large language models (LLMs) have shown great potential in automating\nsignificant aspects of coding by producing natural code from informal natural\nlanguage (NL) intent. However, when interacting with LLMs, users have no\nguarantees that the code suggestions produced correctly satisfy the intent they\nprovided. In fact, it is hard to define a notion of correctness since natural\nlanguage can be ambiguous and lacks a formal semantics.\n  In this paper, we propose the workflow of {\\it interactive test-driven code\ngeneration}, which leverages lightweight user feedback to (a) formalize the\nuser intent using generated tests that can be useful for debugging, and (b)\nproduce an improved set of code suggestions by pruning and ranking candidate\ncode suggestions. We describe a language-agnostic abstract algorithm and a\nconcrete implementation TiCoder. We perform an automated evaluation of TiCoder\non the \\emph{MBPP} and \\emph{HumanEval} code generation benchmarks. Our results\nare promising with using the OpenAI Codex LLM: our best algorithm improves the\n\\passk{1} code generation accuracy (in absolute percentages) between $22.49\\%$\nto $37.71\\%$ for MBPP and between $24.79\\%$ to $53.98\\%$ for HumanEval using\nbetween 1 to 5 simulated user queries.",
        "pdf_link": "https://arxiv.org/pdf/2208.05950v2.pdf"
    },
    {
        "title": "Reducing Retraining by Recycling Parameter-Efficient Prompts",
        "authors": [
            "Brian Lester",
            "Joshua Yurtsever",
            "Siamak Shakeri",
            "Noah Constant"
        ],
        "published": "2022-08-10T22:10:53Z",
        "summary": "Parameter-efficient methods are able to use a single frozen pre-trained large\nlanguage model (LLM) to perform many tasks by learning task-specific soft\nprompts that modulate model behavior when concatenated to the input text.\nHowever, these learned prompts are tightly coupled to a given frozen model --\nif the model is updated, corresponding new prompts need to be obtained. In this\nwork, we propose and investigate several approaches to \"Prompt Recycling'\"\nwhere a prompt trained on a source model is transformed to work with the new\ntarget model. Our methods do not rely on supervised pairs of prompts,\ntask-specific data, or training updates with the target model, which would be\njust as costly as re-tuning prompts with the target model from scratch. We show\nthat recycling between models is possible (our best settings are able to\nsuccessfully recycle $88.9\\%$ of prompts, producing a prompt that out-performs\nbaselines), but significant performance headroom remains, requiring improved\nrecycling techniques.",
        "pdf_link": "https://arxiv.org/pdf/2208.05577v1.pdf"
    },
    {
        "title": "Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts",
        "authors": [
            "Babak Hemmatian",
            "Lav R. Varshney"
        ],
        "published": "2022-08-08T20:59:16Z",
        "summary": "Recent work demonstrates a bias in the GPT-3 model towards generating violent\ntext completions when prompted about Muslims, compared with Christians and\nHindus. Two pre-registered replication attempts, one exact and one approximate,\nfound only the weakest bias in the more recent Instruct Series version of\nGPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent\ncompletions were observed. Additional pre-registered experiments, however,\nshowed that using common names associated with the religions in prompts yields\na highly significant increase in violent completions, also revealing a stronger\nsecond-order bias against Muslims. Names of Muslim celebrities from non-violent\ndomains resulted in relatively fewer violent completions, suggesting that\naccess to individualized information can steer the model away from using\nstereotypes. Nonetheless, content analysis revealed religion-specific violent\nthemes containing highly offensive ideas regardless of prompt format. Our\nresults show the need for additional debiasing of large language models to\naddress higher-order schemas and associations.",
        "pdf_link": "https://arxiv.org/pdf/2208.04417v2.pdf"
    },
    {
        "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
        "authors": [
            "Margaret Li",
            "Suchin Gururangan",
            "Tim Dettmers",
            "Mike Lewis",
            "Tim Althoff",
            "Noah A. Smith",
            "Luke Zettlemoyer"
        ],
        "published": "2022-08-05T17:46:38Z",
        "summary": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for\nembarrassingly parallel training of large language models (LLMs). We show it is\npossible to independently train subparts of a new class of LLMs on different\nsubsets of the data, eliminating the massive multi-node synchronization\ncurrently required to train LLMs. BTM learns a set of independent expert LMs\n(ELMs), each specialized to a different textual domain, such as scientific or\nlegal text. These ELMs can be added and removed to update data coverage,\nensembled to generalize to new domains, or averaged to collapse back to a\nsingle LM for efficient inference. New ELMs are learned by branching from\n(mixtures of) ELMs in the current set, further training the parameters on data\nfor the new domain, and then merging the resulting model back into the set for\nfuture use. Experiments show that BTM improves in- and out-of-domain\nperplexities as compared to GPT-style Transformer LMs, when controlling for\ntraining cost. Through extensive analysis, we show that these results are\nrobust to different ELM initialization schemes, but require expert domain\nspecialization; LM ensembles with random data splits do not perform well. We\nalso present a study of scaling BTM into a new corpus of 64 domains (192B\nwhitespace-separated tokens in total); the resulting LM (22.4B total\nparameters) performs as well as a Transformer LM trained with 2.5 times more\ncompute. These gains grow with the number of domains, suggesting more\naggressive parallelism could be used to efficiently train larger models in\nfuture work.",
        "pdf_link": "https://arxiv.org/pdf/2208.03306v1.pdf"
    },
    {
        "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
        "authors": [
            "Gautier Izacard",
            "Patrick Lewis",
            "Maria Lomeli",
            "Lucas Hosseini",
            "Fabio Petroni",
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Armand Joulin",
            "Sebastian Riedel",
            "Edouard Grave"
        ],
        "published": "2022-08-05T17:39:22Z",
        "summary": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2208.03299v3.pdf"
    },
    {
        "title": "Meaning without reference in large language models",
        "authors": [
            "Steven T. Piantadosi",
            "Felix Hill"
        ],
        "published": "2022-08-05T02:48:26Z",
        "summary": "The widespread success of large language models (LLMs) has been met with\nskepticism that they possess anything like human concepts or meanings. Contrary\nto claims that LLMs possess no meaning whatsoever, we argue that they likely\ncapture important aspects of meaning, and moreover work in a way that\napproximates a compelling account of human cognition in which meaning arises\nfrom conceptual role. Because conceptual role is defined by the relationships\nbetween internal representational states, meaning cannot be determined from a\nmodel's architecture, training data, or objective function, but only by\nexamination of how its internal states relate to each other. This approach may\nclarify why and how LLMs are so successful and suggest how they can be made\nmore human-like.",
        "pdf_link": "https://arxiv.org/pdf/2208.02957v2.pdf"
    },
    {
        "title": "Debiasing Gender Bias in Information Retrieval Models",
        "authors": [
            "Dhanasekar Sundararaman",
            "Vivek Subramanian"
        ],
        "published": "2022-08-02T21:12:05Z",
        "summary": "Biases in culture, gender, ethnicity, etc. have existed for decades and have\naffected many areas of human social interaction. These biases have been shown\nto impact machine learning (ML) models, and for natural language processing\n(NLP), this can have severe consequences for downstream tasks. Mitigating\ngender bias in information retrieval (IR) is important to avoid propagating\nstereotypes. In this work, we employ a dataset consisting of two components:\n(1) relevance of a document to a query and (2) \"gender\" of a document, in which\npronouns are replaced by male, female, and neutral conjugations. We\ndefinitively show that pre-trained models for IR do not perform well in\nzero-shot retrieval tasks when full fine-tuning of a large pre-trained BERT\nencoder is performed and that lightweight fine-tuning performed with adapter\nnetworks improves zero-shot retrieval performance almost by 20% over baseline.\nWe also illustrate that pre-trained models have gender biases that result in\nretrieved articles tending to be more often male than female. We overcome this\nby introducing a debiasing technique that penalizes the model when it prefers\nmales over females, resulting in an effective model that retrieves articles in\na balanced fashion across genders.",
        "pdf_link": "https://arxiv.org/pdf/2208.01755v3.pdf"
    },
    {
        "title": "Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories",
        "authors": [
            "Gizem Sogancioglu",
            "Fabian Mijsters",
            "Amar van Uden",
            "Jelle Peperzak"
        ],
        "published": "2022-08-02T10:02:21Z",
        "summary": "Clinical word embeddings are extensively used in various Bio-NLP problems as\na state-of-the-art feature vector representation. Although they are quite\nsuccessful at the semantic representation of words, due to the dataset - which\npotentially carries statistical and societal bias - on which they are trained,\nthey might exhibit gender stereotypes. This study analyses gender bias of\nclinical embeddings on three medical categories: mental disorders, sexually\ntransmitted diseases, and personality traits. To this extent, we analyze two\ndifferent pre-trained embeddings namely (contextualized) clinical-BERT and\n(non-contextualized) BioWordVec. We show that both embeddings are biased\ntowards sensitive gender groups but BioWordVec exhibits a higher bias than\nclinical-BERT for all three categories. Moreover, our analyses show that\nclinical embeddings carry a high degree of bias for some medical terms and\ndiseases which is conflicting with medical literature. Having such an\nill-founded relationship might cause harm in downstream applications that use\nclinical embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2208.01341v2.pdf"
    },
    {
        "title": "PASTA: A Dataset for Modeling Participant States in Narratives",
        "authors": [
            "Sayontan Ghosh",
            "Mahnaz Koupaee",
            "Isabella Chen",
            "Francis Ferraro",
            "Nathanael Chambers",
            "Niranjan Balasubramanian"
        ],
        "published": "2022-07-31T01:21:48Z",
        "summary": "The events in a narrative are understood as a coherent whole via the\nunderlying states of their participants. Often, these participant states are\nnot explicitly mentioned, instead left to be inferred by the reader. A model\nthat understands narratives should likewise infer these implicit states, and\neven reason about the impact of changes to these states on the narrative. To\nfacilitate this goal, we introduce a new crowdsourced English-language,\nParticipant States dataset, PASTA. This dataset contains inferable participant\nstates; a counterfactual perturbation to each state; and the changes to the\nstory that would be necessary if the counterfactual were true. We introduce\nthree state-based reasoning tasks that test for the ability to infer when a\nstate is entailed by a story, to revise a story conditioned on a counterfactual\nstate, and to explain the most likely state change given a revised story.\nExperiments show that today's LLMs can reason about states to some degree, but\nthere is large room for improvement, especially in problems requiring access\nand ability to reason with diverse types of knowledge (e.g. physical,\nnumerical, factual).",
        "pdf_link": "https://arxiv.org/pdf/2208.00329v2.pdf"
    },
    {
        "title": "Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions",
        "authors": [
            "Yanai Elazar",
            "Nora Kassner",
            "Shauli Ravfogel",
            "Amir Feder",
            "Abhilasha Ravichander",
            "Marius Mosbach",
            "Yonatan Belinkov",
            "Hinrich Schütze",
            "Yoav Goldberg"
        ],
        "published": "2022-07-28T17:36:24Z",
        "summary": "Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.",
        "pdf_link": "https://arxiv.org/pdf/2207.14251v2.pdf"
    },
    {
        "title": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative",
        "authors": [
            "Xiaomin Fang",
            "Fan Wang",
            "Lihang Liu",
            "Jingzhou He",
            "Dayong Lin",
            "Yingfei Xiang",
            "Xiaonan Zhang",
            "Hua Wu",
            "Hui Li",
            "Le Song"
        ],
        "published": "2022-07-28T07:30:33Z",
        "summary": "AI-based protein structure prediction pipelines, such as AlphaFold2, have\nachieved near-experimental accuracy. These advanced pipelines mainly rely on\nMultiple Sequence Alignments (MSAs) as inputs to learn the co-evolution\ninformation from the homologous sequences. Nonetheless, searching MSAs from\nprotein databases is time-consuming, usually taking dozens of minutes.\nConsequently, we attempt to explore the limits of fast protein structure\nprediction by using only primary sequences of proteins. HelixFold-Single is\nproposed to combine a large-scale protein language model with the superior\ngeometric learning capability of AlphaFold2. Our proposed method,\nHelixFold-Single, first pre-trains a large-scale protein language model (PLM)\nwith thousands of millions of primary sequences utilizing the self-supervised\nlearning paradigm, which will be used as an alternative to MSAs for learning\nthe co-evolution information. Then, by combining the pre-trained PLM and the\nessential components of AlphaFold2, we obtain an end-to-end differentiable\nmodel to predict the 3D coordinates of atoms from only the primary sequence.\nHelixFold-Single is validated in datasets CASP14 and CAMEO, achieving\ncompetitive accuracy with the MSA-based methods on the targets with large\nhomologous families. Furthermore, HelixFold-Single consumes much less time than\nthe mainstream pipelines for protein structure prediction, demonstrating its\npotential in tasks requiring many predictions. The code of HelixFold-Single is\navailable at\nhttps://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single,\nand we also provide stable web services on\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.",
        "pdf_link": "https://arxiv.org/pdf/2207.13921v3.pdf"
    },
    {
        "title": "When BERT Fails -- The Limits of EHR Classification",
        "authors": [
            "Augusto Garcia-Agundez",
            "Carsten Eickhoff"
        ],
        "published": "2022-07-26T17:18:24Z",
        "summary": "Transformers are powerful text representation learners, useful for all kinds\nof clinical decision support tasks. Although they outperform baselines on\nreadmission prediction, they are not infallible. Here, we look into one such\nfailure case, and report patterns that lead to inferior predictive performance.",
        "pdf_link": "https://arxiv.org/pdf/2208.10245v1.pdf"
    },
    {
        "title": "A Hazard Analysis Framework for Code Synthesis Large Language Models",
        "authors": [
            "Heidy Khlaaf",
            "Pamela Mishkin",
            "Joshua Achiam",
            "Gretchen Krueger",
            "Miles Brundage"
        ],
        "published": "2022-07-25T20:44:40Z",
        "summary": "Codex, a large language model (LLM) trained on a variety of codebases,\nexceeds the previous state of the art in its capacity to synthesize and\ngenerate code. Although Codex provides a plethora of benefits, models that may\ngenerate code on such scale have significant limitations, alignment problems,\nthe potential to be misused, and the possibility to increase the rate of\nprogress in technical fields that may themselves have destabilizing impacts or\nhave misuse potential. Yet such safety impacts are not yet known or remain to\nbe explored. In this paper, we outline a hazard analysis framework constructed\nat OpenAI to uncover hazards or safety risks that the deployment of models like\nCodex may impose technically, socially, politically, and economically. The\nanalysis is informed by a novel evaluation framework that determines the\ncapacity of advanced code generation techniques against the complexity and\nexpressivity of specification prompts, and their capability to understand and\nexecute them relative to human ability.",
        "pdf_link": "https://arxiv.org/pdf/2207.14157v1.pdf"
    },
    {
        "title": "Robots Enact Malignant Stereotypes",
        "authors": [
            "Andrew Hundt",
            "William Agnew",
            "Vicky Zeng",
            "Severin Kacianka",
            "Matthew Gombolay"
        ],
        "published": "2022-07-23T18:08:12Z",
        "summary": "Stereotypes, bias, and discrimination have been extensively documented in\nMachine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural\nLanguage Processing (NLP) [6], or both, in the case of large image and caption\nmodels such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias\nmanifests in robots that physically and autonomously act within the world. We\naudit one of several recently published CLIP-powered robotic manipulation\nmethods, presenting it with objects that have pictures of human faces on the\nsurface which vary across race and gender, alongside task descriptions that\ncontain terms associated with common stereotypes. Our experiments definitively\nshow robots acting out toxic stereotypes with respect to gender, race, and\nscientifically-discredited physiognomy, at scale. Furthermore, the audited\nmethods are less likely to recognize Women and People of Color. Our\ninterdisciplinary sociotechnical analysis synthesizes across fields and\napplications such as Science Technology and Society (STS), Critical Studies,\nHistory, Safety, Robotics, and AI. We find that robots powered by large\ndatasets and Dissolution Models (sometimes called \"foundation models\", e.g.\nCLIP) that contain humans risk physically amplifying malignant stereotypes in\ngeneral; and that merely correcting disparities will be insufficient for the\ncomplexity and scale of the problem. Instead, we recommend that robot learning\nmethods that physically manifest stereotypes or other harmful outcomes be\npaused, reworked, or even wound down when appropriate, until outcomes can be\nproven safe, effective, and just. Finally, we discuss comprehensive policy\nchanges and the potential of new interdisciplinary research on topics like\nIdentity Safety Assessment Frameworks and Design Justice to better understand\nand address these harms.",
        "pdf_link": "https://arxiv.org/pdf/2207.11569v1.pdf"
    },
    {
        "title": "Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices",
        "authors": [
            "Mingbin Xu",
            "Congzheng Song",
            "Ye Tian",
            "Neha Agrawal",
            "Filip Granqvist",
            "Rogier van Dalen",
            "Xiao Zhang",
            "Arturo Argueta",
            "Shiyi Han",
            "Yaqiao Deng",
            "Leo Liu",
            "Anmol Walia",
            "Alex Jin"
        ],
        "published": "2022-07-18T23:53:17Z",
        "summary": "Federated Learning (FL) is a technique to train models using data distributed\nacross devices. Differential Privacy (DP) provides a formal privacy guarantee\nfor sensitive data. Our goal is to train a large neural network language model\n(NNLM) on compute-constrained devices while preserving privacy using FL and DP.\nHowever, the DP-noise introduced to the model increases as the model size\ngrows, which often prevents convergence. We propose Partial Embedding Updates\n(PEU), a novel technique to decrease noise by decreasing payload size.\nFurthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive\nEstimation (NCE) to reduce the memory demands of large models on\ncompute-constrained devices. This combination of techniques makes it possible\nto train large-vocabulary language models while preserving accuracy and\nprivacy.",
        "pdf_link": "https://arxiv.org/pdf/2207.08988v1.pdf"
    },
    {
        "title": "Selection Bias Induced Spurious Correlations in Large Language Models",
        "authors": [
            "Emily McMilin"
        ],
        "published": "2022-07-18T23:43:52Z",
        "summary": "In this work we show how large language models (LLMs) can learn statistical\ndependencies between otherwise unconditionally independent variables due to\ndataset selection bias. To demonstrate the effect, we developed a masked gender\ntask that can be applied to BERT-family models to reveal spurious correlations\nbetween predicted gender pronouns and a variety of seemingly gender-neutral\nvariables like date and location, on pre-trained (unmodified) BERT and RoBERTa\nlarge models. Finally, we provide an online demo, inviting readers to\nexperiment further.",
        "pdf_link": "https://arxiv.org/pdf/2207.08982v1.pdf"
    },
    {
        "title": "Can large language models reason about medical questions?",
        "authors": [
            "Valentin Liévin",
            "Christoffer Egeberg Hother",
            "Andreas Geert Motzfeldt",
            "Ole Winther"
        ],
        "published": "2022-07-17T11:24:44Z",
        "summary": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2207.08143v4.pdf"
    },
    {
        "title": "Automatic Context Pattern Generation for Entity Set Expansion",
        "authors": [
            "Yinghui Li",
            "Shulin Huang",
            "Xinwei Zhang",
            "Qingyu Zhou",
            "Yangning Li",
            "Ruiyang Liu",
            "Yunbo Cao",
            "Hai-Tao Zheng",
            "Ying Shen"
        ],
        "published": "2022-07-17T06:50:35Z",
        "summary": "Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.",
        "pdf_link": "https://arxiv.org/pdf/2207.08087v4.pdf"
    },
    {
        "title": "Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model",
        "authors": [
            "Xiaolin Chen",
            "Xuemeng Song",
            "Liqiang Jing",
            "Shuo Li",
            "Linmei Hu",
            "Liqiang Nie"
        ],
        "published": "2022-07-16T13:02:54Z",
        "summary": "Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.",
        "pdf_link": "https://arxiv.org/pdf/2207.07934v1.pdf"
    },
    {
        "title": "Confident Adaptive Language Modeling",
        "authors": [
            "Tal Schuster",
            "Adam Fisch",
            "Jai Gupta",
            "Mostafa Dehghani",
            "Dara Bahri",
            "Vinh Q. Tran",
            "Yi Tay",
            "Donald Metzler"
        ],
        "published": "2022-07-14T17:00:19Z",
        "summary": "Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.07061v2.pdf"
    },
    {
        "title": "Language models show human-like content effects on reasoning tasks",
        "authors": [
            "Ishita Dasgupta",
            "Andrew K. Lampinen",
            "Stephanie C. Y. Chan",
            "Hannah R. Sheahan",
            "Antonia Creswell",
            "Dharshan Kumaran",
            "James L. McClelland",
            "Felix Hill"
        ],
        "published": "2022-07-14T16:51:09Z",
        "summary": "Abstract reasoning is a key ability for an intelligent system. Large language\nmodels (LMs) achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect. For example, human reasoning is affected by our real-world knowledge\nand beliefs, and shows notable \"content effects\"; humans reason more reliably\nwhen the semantic content of a problem supports the correct logical inferences.\nThese content-entangled reasoning patterns play a central role in debates about\nthe fundamental nature of human intelligence. Here, we investigate whether\nlanguage models $\\unicode{x2014}$ whose prior expectations capture some aspects\nof human knowledge $\\unicode{x2014}$ similarly mix content into their answers\nto logical problems. We explored this question across three logical reasoning\ntasks: natural language inference, judging the logical validity of syllogisms,\nand the Wason selection task. We evaluate state of the art large language\nmodels, as well as humans, and find that the language models reflect many of\nthe same patterns observed in humans across these tasks $\\unicode{x2014}$ like\nhumans, models answer more accurately when the semantic content of a task\nsupports the logical inferences. These parallels are reflected both in answer\npatterns, and in lower-level features like the relationship between model\nanswer distributions and human response times. Our findings have implications\nfor understanding both these cognitive effects in humans, and the factors that\ncontribute to language model performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.07051v3.pdf"
    },
    {
        "title": "Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model",
        "authors": [
            "Chris van der Lee",
            "Thiago Castro Ferreira",
            "Chris Emmery",
            "Travis Wiltshire",
            "Emiel Krahmer"
        ],
        "published": "2022-07-14T11:53:04Z",
        "summary": "This study discusses the effect of semi-supervised learning in combination\nwith pretrained language models for data-to-text generation. It is not known\nwhether semi-supervised learning is still helpful when a large-scale language\nmodel is also supplemented. This study aims to answer this question by\ncomparing a data-to-text system only supplemented with a language model, to two\ndata-to-text systems that are additionally enriched by a data augmentation or a\npseudo-labeling semi-supervised learning approach.\n  Results show that semi-supervised learning results in higher scores on\ndiversity metrics. In terms of output quality, extending the training set of a\ndata-to-text system with a language model using the pseudo-labeling approach\ndid increase text quality scores, but the data augmentation approach yielded\nsimilar scores to the system without training set extension. These results\nindicate that semi-supervised learning approaches can bolster output quality\nand diversity, even when a language model is also present.",
        "pdf_link": "https://arxiv.org/pdf/2207.06839v1.pdf"
    },
    {
        "title": "BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling",
        "authors": [
            "Javier de la Rosa",
            "Eduardo G. Ponferrada",
            "Paulo Villegas",
            "Pablo Gonzalez de Prado Salas",
            "Manu Romero",
            "Marıa Grandury"
        ],
        "published": "2022-07-14T10:48:42Z",
        "summary": "The pre-training of large language models usually requires massive amounts of\nresources, both in terms of computation and data. Frequently used web sources\nsuch as Common Crawl might contain enough noise to make this pre-training\nsub-optimal. In this work, we experiment with different sampling methods from\nthe Spanish version of mC4, and present a novel data-centric technique which we\nname $\\textit{perplexity sampling}$ that enables the pre-training of language\nmodels in roughly half the amount of steps and using one fifth of the data. The\nresulting models are comparable to the current state-of-the-art, and even\nachieve better results for certain tasks. Our work is proof of the versatility\nof Transformers, and paves the way for small teams to train their models on a\nlimited budget. Our models are available at this\n$\\href{https://huggingface.co/bertin-project}{URL}$.",
        "pdf_link": "https://arxiv.org/pdf/2207.06814v1.pdf"
    },
    {
        "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America",
        "authors": [
            "Laura Alonso Alemany",
            "Luciana Benotti",
            "Hernán Maina",
            "Lucía González",
            "Mariela Rajngewerc",
            "Lautaro Martínez",
            "Jorge Sánchez",
            "Mauro Schilman",
            "Guido Ivetta",
            "Alexia Halvorsen",
            "Amanda Mata Rojo",
            "Matías Bordone",
            "Beatriz Busaniche"
        ],
        "published": "2022-07-14T01:07:55Z",
        "summary": "Automated decision-making systems, especially those based on natural language\nprocessing, are pervasive in our lives. They are not only behind the internet\nsearch engines we use daily, but also take more critical roles: selecting\ncandidates for a job, determining suspects of a crime, diagnosing autism and\nmore. Such automated systems make errors, which may be harmful in many ways, be\nit because of the severity of the consequences (as in health issues) or because\nof the sheer number of people they affect. When errors made by an automated\nsystem affect a population more than others, we call the system\n\\textit{biased}.\n  Most modern natural language technologies are based on artifacts obtained\nfrom enormous volumes of text using machine learning, namely language models\nand word embeddings. Since they are created by applying subsymbolic machine\nlearning, mostly artificial neural networks, they are opaque and practically\nuninterpretable by direct inspection, thus making it very difficult to audit\nthem.\n  In this paper, we present a methodology that spells out how social\nscientists, domain experts, and machine learning experts can collaboratively\nexplore biases and harmful stereotypes in word embeddings and large language\nmodels. Our methodology is based on the following principles:\n  * focus on the linguistic manifestations of discrimination on word embeddings\nand language models, not on the mathematical properties of the models * reduce\nthe technical barrier for discrimination experts%, be it social scientists,\ndomain experts or other * characterize through a qualitative exploratory\nprocess in addition to a metric-based approach * address mitigation as part of\nthe training process, not as an afterthought",
        "pdf_link": "https://arxiv.org/pdf/2207.06591v3.pdf"
    },
    {
        "title": "DocPrompting: Generating Code by Retrieving the Docs",
        "authors": [
            "Shuyan Zhou",
            "Uri Alon",
            "Frank F. Xu",
            "Zhiruo Wang",
            "Zhengbao Jiang",
            "Graham Neubig"
        ],
        "published": "2022-07-13T06:47:51Z",
        "summary": "Publicly available source-code libraries are continuously growing and\nchanging. This makes it impossible for models of code to keep current with all\navailable APIs by simply training these models on existing code repositories.\nThus, existing models inherently cannot generalize to using unseen functions\nand libraries, because these would never appear in the training data. In\ncontrast, when human programmers use functions and libraries for the first\ntime, they frequently refer to textual resources such as code manuals and\ndocumentation, to explore and understand the available functionality. Inspired\nby this observation, we introduce DocPrompting: a natural-language-to-code\ngeneration approach that explicitly leverages documentation by (1) retrieving\nthe relevant documentation pieces given an NL intent, and (2) generating code\nbased on the NL intent and the retrieved documentation. DocPrompting is\ngeneral: it can be applied to any programming language and is agnostic to the\nunderlying neural model. We demonstrate that DocPrompting consistently improves\nNL-to-code models: DocPrompting improves strong base models such as CodeT5 by\n2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in\nexecution-based evaluation on the popular Python CoNaLa benchmark; on a new\nBash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to\nabsolute 6.9% exact match.",
        "pdf_link": "https://arxiv.org/pdf/2207.05987v3.pdf"
    },
    {
        "title": "Exploring Length Generalization in Large Language Models",
        "authors": [
            "Cem Anil",
            "Yuhuai Wu",
            "Anders Andreassen",
            "Aitor Lewkowycz",
            "Vedant Misra",
            "Vinay Ramasesh",
            "Ambrose Slone",
            "Guy Gur-Ari",
            "Ethan Dyer",
            "Behnam Neyshabur"
        ],
        "published": "2022-07-11T14:24:38Z",
        "summary": "The ability to extrapolate from short problem instances to longer ones is an\nimportant form of out-of-distribution generalization in reasoning tasks, and is\ncrucial when learning from datasets where longer problem instances are rare.\nThese include theorem proving, solving quantitative mathematics problems, and\nreading/summarizing novels. In this paper, we run careful empirical studies\nexploring the length generalization capabilities of transformer-based language\nmodels. We first establish that naively finetuning transformers on length\ngeneralization tasks shows significant generalization deficiencies independent\nof model scale. We then show that combining pretrained large language models'\nin-context learning abilities with scratchpad prompting (asking the model to\noutput solution steps before producing an answer) results in a dramatic\nimprovement in length generalization. We run careful failure analyses on each\nof the learning modalities and identify common sources of mistakes that\nhighlight opportunities in equipping language models with the ability to\ngeneralize to longer problems.",
        "pdf_link": "https://arxiv.org/pdf/2207.04901v2.pdf"
    },
    {
        "title": "Few-shot training LLMs for project-specific code-summarization",
        "authors": [
            "Toufique Ahmed",
            "Premkumar Devanbu"
        ],
        "published": "2022-07-09T09:57:11Z",
        "summary": "Very large language models (LLMs), such as GPT-3 and Codex have achieved\nstate-of-the-art performance on several natural-language tasks, and show great\npromise also for code. A particularly exciting aspect of LLMs is their knack\nfor few-shot and zero-shot learning: they can learn to perform a task with very\nfew examples. Few-shotting has particular synergies in software engineering,\nwhere there are a lot of phenomena (identifier names, APIs, terminology, coding\npatterns) that are known to be highly project-specific. However,\nproject-specific data can be quite limited, especially early in the history of\na project; thus the few-shot learning capacity of LLMs might be very relevant.\nIn this paper, we investigate the use few-shot training with the very large GPT\n(Generative Pre-trained Transformer) Codex model, and find evidence suggesting\nthat one can significantly surpass state-of-the-art models for\ncode-summarization, leveraging project-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2207.04237v2.pdf"
    },
    {
        "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
        "authors": [
            "Zejiang Hou",
            "Julian Salazar",
            "George Polovets"
        ],
        "published": "2022-07-07T18:00:22Z",
        "summary": "Large pretrained language models (PLMs) are often domain- or task-adapted via\nfine-tuning or prompting. Finetuning requires modifying all of the parameters\nand having enough data to avoid overfitting while prompting requires no\ntraining and few examples but limits performance. Instead, we prepare PLMs for\ndata- and parameter-efficient adaptation by learning to learn the difference\nbetween general and adapted PLMs. This difference is expressed in terms of\nmodel weights and sublayer structure through our proposed dynamic low-rank\nreparameterization and learned architecture controller. Experiments on few-shot\ndialogue completion, low-resource abstractive summarization, and multi-domain\nlanguage modeling show improvements in adaptation time and performance over\ndirect finetuning or preparation via domain-adaptive pretraining. Ablations\nshow our task-adaptive reparameterization (TARP) and model search (TAMS)\ncomponents individually improve on other parameter-efficient transfer like\nadapters and structure-learning methods like learned sparsification.",
        "pdf_link": "https://arxiv.org/pdf/2207.03509v1.pdf"
    },
    {
        "title": "A Large Scale Search Dataset for Unbiased Learning to Rank",
        "authors": [
            "Lixin Zou",
            "Haitao Mao",
            "Xiaokai Chu",
            "Jiliang Tang",
            "Wenwen Ye",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2022-07-07T02:37:25Z",
        "summary": "The unbiased learning to rank (ULTR) problem has been greatly advanced by\nrecent deep learning techniques and well-designed debias algorithms. However,\npromising results on the existing benchmark datasets may not be extended to the\npractical scenario due to the following disadvantages observed from those\npopular benchmark datasets: (1) outdated semantic feature extraction where\nstate-of-the-art large scale pre-trained language models like BERT cannot be\nexploited due to the missing of the original text;(2) incomplete display\nfeatures for in-depth study of ULTR, e.g., missing the displayed abstract of\ndocuments for analyzing the click necessary bias; (3) lacking real-world user\nfeedback, leading to the prevalence of synthetic datasets in the empirical\nstudy. To overcome the above disadvantages, we introduce the Baidu-ULTR\ndataset. It involves randomly sampled 1.2 billion searching sessions and 7,008\nexpert annotated queries, which is orders of magnitude larger than the existing\nones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained\nlanguage model for easy usage; (2) sufficient display information such as\nposition, displayed height, and displayed abstract, enabling the comprehensive\nstudy of different biases with advanced techniques such as causal discovery and\nmeta-learning; and (3) rich user feedback on search result pages (SERPs) like\ndwelling time, allowing for user engagement optimization and promoting the\nexploration of multi-task learning in ULTR. In this paper, we present the\ndesign principle of Baidu-ULTR and the performance of benchmark ULTR algorithms\non this new data resource, favoring the exploration of ranking for long-tail\nqueries and pre-training tasks for ranking. The Baidu-ULTR dataset and\ncorresponding baseline implementation are available at\nhttps://github.com/ChuXiaokai/baidu_ultr_dataset.",
        "pdf_link": "https://arxiv.org/pdf/2207.03051v2.pdf"
    },
    {
        "title": "The Role of Complex NLP in Transformers for Text Ranking?",
        "authors": [
            "David Rau",
            "Jaap Kamps"
        ],
        "published": "2022-07-06T08:54:18Z",
        "summary": "Even though term-based methods such as BM25 provide strong baselines in\nranking, under certain conditions they are dominated by large pre-trained\nmasked language models (MLMs) such as BERT. To date, the source of their\neffectiveness remains unclear. Is it their ability to truly understand the\nmeaning through modeling syntactic aspects? We answer this by manipulating the\ninput order and position information in a way that destroys the natural\nsequence order of query and passage and shows that the model still achieves\ncomparable performance. Overall, our results highlight that syntactic aspects\ndo not play a critical role in the effectiveness of re-ranking with BERT. We\npoint to other mechanisms such as query-passage cross-attention and richer\nembeddings that capture word meanings based on aggregated context regardless of\nthe word order for being the main attributions for its superior performance.",
        "pdf_link": "https://arxiv.org/pdf/2207.02522v1.pdf"
    },
    {
        "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
        "authors": [
            "Przemyslaw Joniak",
            "Akiko Aizawa"
        ],
        "published": "2022-07-06T06:20:35Z",
        "summary": "Language model debiasing has emerged as an important field of study in the\nNLP community. Numerous debiasing techniques were proposed, but bias ablation\nremains an unaddressed issue. We demonstrate a novel framework for inspecting\nbias in pre-trained transformer-based language models via movement pruning.\nGiven a model and a debiasing objective, our framework finds a subset of the\nmodel containing less bias than the original model. We implement our framework\nby pruning the model while fine-tuning it on the debiasing objective. Optimized\nare only the pruning scores - parameters coupled with the model's weights that\nact as gates. We experiment with pruning attention heads, an important building\nblock of transformers: we prune square blocks, as well as establish a new way\nof pruning the entire heads. Lastly, we demonstrate the usage of our framework\nusing gender bias, and based on our findings, we propose an improvement to an\nexisting debiasing method. Additionally, we re-discover a bias-performance\ntrade-off: the better the model performs, the more bias it contains.",
        "pdf_link": "https://arxiv.org/pdf/2207.02463v1.pdf"
    },
    {
        "title": "Machine Learning Model Sizes and the Parameter Gap",
        "authors": [
            "Pablo Villalobos",
            "Jaime Sevilla",
            "Tamay Besiroglu",
            "Lennart Heim",
            "Anson Ho",
            "Marius Hobbhahn"
        ],
        "published": "2022-07-05T20:55:38Z",
        "summary": "We study trends in model size of notable machine learning systems over time\nusing a curated dataset. From 1950 to 2018, model size in language models\nincreased steadily by seven orders of magnitude. The trend then accelerated,\nwith model size increasing by another five orders of magnitude in just 4 years\nfrom 2018 to 2022. Vision models grew at a more constant pace, totaling 7\norders of magnitude of growth between 1950 and 2022.\n  We also identify that, since 2020, there have been many language models below\n20B parameters, many models above 70B parameters, but a scarcity of models in\nthe 20-70B parameter range. We refer to that scarcity as the parameter gap.\n  We provide some stylized facts about the parameter gap and propose a few\nhypotheses to explain it. The explanations we favor are: (a) increasing model\nsize beyond 20B parameters requires adopting different parallelism techniques,\nwhich makes mid-sized models less cost-effective, (b) GPT-3 was one order of\nmagnitude larger than previous language models, and researchers afterwards\nprimarily experimented with bigger models to outperform it. While these\ndynamics likely exist, and we believe they play some role in generating the\ngap, we don't have high confidence that there are no other, more important\ndynamics at play.",
        "pdf_link": "https://arxiv.org/pdf/2207.02852v1.pdf"
    },
    {
        "title": "FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales",
        "authors": [
            "Aaron Chan",
            "Shaoliang Nie",
            "Liang Tan",
            "Xiaochang Peng",
            "Hamed Firooz",
            "Maziar Sanjabi",
            "Xiang Ren"
        ],
        "published": "2022-07-02T09:25:29Z",
        "summary": "Following how humans communicate, free-text rationales aim to use natural\nlanguage to explain neural language model (LM) behavior. However, free-text\nrationales' unconstrained nature makes them prone to hallucination, so it is\nimportant to have metrics for free-text rationale quality. Existing free-text\nrationale metrics measure how consistent the rationale is with the LM's\npredicted label, but there is no protocol for assessing such metrics'\nreliability. Thus, we propose FRAME, a framework for evaluating rationale-label\nconsistency (RLC) metrics for free-text rationales. FRAME is based on three\naxioms: (1) good metrics should yield highest scores for reference rationales,\nwhich maximize RLC by construction; (2) good metrics should be appropriately\nsensitive to semantic perturbation of rationales; and (3) good metrics should\nbe robust to variation in the LM's task performance. Across three text\nclassification datasets, we show that existing RLC metrics cannot satisfy all\nthree FRAME axioms, since they are implemented via model pretraining which\nmuddles the metric's signal. Then, we introduce a non-pretraining RLC metric\nthat greatly outperforms baselines on (1) and (3), while performing\ncompetitively on (2). Finally, we discuss the limitations of using RLC to\nevaluate free-text rationales.",
        "pdf_link": "https://arxiv.org/pdf/2207.00779v2.pdf"
    },
    {
        "title": "Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset",
        "authors": [
            "Peter Henderson",
            "Mark S. Krass",
            "Lucia Zheng",
            "Neel Guha",
            "Christopher D. Manning",
            "Dan Jurafsky",
            "Daniel E. Ho"
        ],
        "published": "2022-07-01T06:25:15Z",
        "summary": "One concern with the rise of large language models lies with their potential\nfor significant harm, particularly from pretraining on biased, obscene,\ncopyrighted, and private information. Emerging ethical approaches have\nattempted to filter pretraining material, but such approaches have been ad hoc\nand failed to take context into account. We offer an approach to filtering\ngrounded in law, which has directly addressed the tradeoffs in filtering\nmaterial. First, we gather and make available the Pile of Law, a 256GB (and\ngrowing) dataset of open-source English-language legal and administrative data,\ncovering court opinions, contracts, administrative rules, and legislative\nrecords. Pretraining on the Pile of Law may help with legal tasks that have the\npromise to improve access to justice. Second, we distill the legal norms that\ngovernments have developed to constrain the inclusion of toxic or private\ncontent into actionable lessons for researchers and discuss how our dataset\nreflects these norms. Third, we show how the Pile of Law offers researchers the\nopportunity to learn such filtering rules directly from the data, providing an\nexciting new research direction in model-based processing.",
        "pdf_link": "https://arxiv.org/pdf/2207.00220v2.pdf"
    },
    {
        "title": "When Does Differentially Private Learning Not Suffer in High Dimensions?",
        "authors": [
            "Xuechen Li",
            "Daogao Liu",
            "Tatsunori Hashimoto",
            "Huseyin A. Inan",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Abhradeep Guha Thakurta"
        ],
        "published": "2022-07-01T02:36:51Z",
        "summary": "Large pretrained models can be privately fine-tuned to achieve performance\napproaching that of non-private models. A common theme in these results is the\nsurprising observation that high-dimensional models can achieve favorable\nprivacy-utility trade-offs. This seemingly contradicts known results on the\nmodel-size dependence of differentially private convex learning and raises the\nfollowing research question: When does the performance of differentially\nprivate learning not degrade with increasing model size? We identify that the\nmagnitudes of gradients projected onto subspaces is a key factor that\ndetermines performance. To precisely characterize this for private convex\nlearning, we introduce a condition on the objective that we term\n\\emph{restricted Lipschitz continuity} and derive improved bounds for the\nexcess empirical and population risks that are dimension-independent under\nadditional conditions. We empirically show that in private fine-tuning of large\nlanguage models, gradients obtained during fine-tuning are mostly controlled by\na few principal components. This behavior is similar to conditions under which\nwe obtain dimension-independent bounds in convex settings. Our theoretical and\nempirical results together provide a possible explanation for recent successes\nin large-scale private fine-tuning. Code to reproduce our results can be found\nat\n\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.",
        "pdf_link": "https://arxiv.org/pdf/2207.00160v4.pdf"
    }
]