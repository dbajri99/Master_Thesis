[
    {
        "title": "Fairness in Serving Large Language Models",
        "authors": [
            "Ying Sheng",
            "Shiyi Cao",
            "Dacheng Li",
            "Banghua Zhu",
            "Zhuohan Li",
            "Danyang Zhuo",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-12-31T21:15:54Z",
        "summary": "High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions.",
        "pdf_link": "https://arxiv.org/pdf/2401.00588v1.pdf"
    },
    {
        "title": "HSC-GPT: A Large Language Model for Human Settlements Construction",
        "authors": [
            "Chen Ran",
            "Yao Xueqi",
            "Jiang Xuhui",
            "Han Zhengqi",
            "Guo Jingze",
            "Zhang Xianyue",
            "Lin Chunyu",
            "Liu Chumin",
            "Zhao Jing",
            "Lian Zeke",
            "Zhang Jingjing",
            "Li Keke"
        ],
        "published": "2023-12-31T13:56:15Z",
        "summary": "The field of human settlement construction encompasses a range of spatial\ndesigns and management tasks, including urban planning and landscape\narchitecture design. These tasks involve a plethora of instructions and\ndescriptions presented in natural language, which are essential for\nunderstanding design requirements and producing effective design solutions.\nRecent research has sought to integrate natural language processing (NLP) and\ngenerative artificial intelligence (AI) into human settlement construction\ntasks. Due to the efficient processing and analysis capabilities of AI with\ndata, significant successes have been achieved in design within this domain.\nHowever, this task still faces several fundamental challenges. The semantic\ninformation involved includes complex spatial details, diverse data source\nformats, high sensitivity to regional culture, and demanding requirements for\ninnovation and rigor in work scenarios. These factors lead to limitations when\napplying general generative AI in this field, further exacerbated by a lack of\nhigh-quality data for model training. To address these challenges, this paper\nfirst proposes HSC-GPT, a large-scale language model framework specifically\ndesigned for tasks in human settlement construction, considering the unique\ncharacteristics of this domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.00504v1.pdf"
    },
    {
        "title": "BatchEval: Towards Human-like Text Evaluation",
        "authors": [
            "Peiwen Yuan",
            "Shaoxiong Feng",
            "Yiwei Li",
            "Xinglin Wang",
            "Boyuan Pan",
            "Heda Wang",
            "Kan Li"
        ],
        "published": "2023-12-31T09:34:51Z",
        "summary": "Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.",
        "pdf_link": "https://arxiv.org/pdf/2401.00437v1.pdf"
    },
    {
        "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
        "authors": [
            "Yuanhao Wu",
            "Juno Zhu",
            "Siliang Xu",
            "Kashun Shum",
            "Cheng Niu",
            "Randy Zhong",
            "Juntong Song",
            "Tong Zhang"
        ],
        "published": "2023-12-31T04:43:45Z",
        "summary": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2401.00396v1.pdf"
    },
    {
        "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness",
        "authors": [
            "Neeraj Varshney",
            "Pavel Dolin",
            "Agastya Seth",
            "Chitta Baral"
        ],
        "published": "2023-12-30T17:37:06Z",
        "summary": "As Large Language Models (LLMs) play an increasingly pivotal role in natural\nlanguage processing applications, their safety concerns become critical areas\nof NLP research. This paper presents Safety and Over-Defensiveness Evaluation\n(SODE) benchmark: a collection of diverse safe and unsafe prompts with\ncarefully designed evaluation methods that facilitate systematic evaluation,\ncomparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we\nstudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,\nwhich reveals several interesting and important findings, such as (a) the\nwidely popular 'self-checking' techniques indeed improve the safety against\nunsafe inputs, but this comes at the cost of extreme over-defensiveness on the\nsafe inputs, (b) providing a safety instruction along with in-context exemplars\n(of both safe and unsafe inputs) consistently improves safety and also\nmitigates undue over-defensiveness of the models, (c) providing contextual\nknowledge easily breaks the safety guardrails and makes the models more\nvulnerable to generating unsafe responses. Overall, our work reveals numerous\nsuch critical findings that we believe will pave the way and facilitate further\nresearch in improving the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.00287v1.pdf"
    },
    {
        "title": "Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation",
        "authors": [
            "Reza Fayyazi",
            "Rozhina Taghdimi",
            "Shanchieh Jay Yang"
        ],
        "published": "2023-12-30T16:56:24Z",
        "summary": "Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use\nto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK\nframework can be challenging for cybersecurity practitioners due to presumed\nexpertise, complex dependencies, and inherent ambiguity. Meanwhile,\nadvancements with Large Language Models (LLMs) have led to recent surge in\nstudies exploring its uses in cybersecurity operations. This leads us to\nquestion how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5)\nLLMs can comprehend and summarize TTPs to inform analysts of the intended\npurposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs\nhave shown to be prone to hallucination by providing inaccurate information,\nwhich is problematic in critical domains like cybersecurity. Therefore, we\npropose the use of Retrieval Augmented Generation (RAG) techniques to extract\nrelevant contexts for each cyberattack procedure for decoder-only LLMs (without\nfine-tuning). We further contrast such approach against supervised fine-tuning\n(SFT) of encoder-only LLMs. Our results reveal that both the direct-use of\ndecoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-only\nLLMs offer inaccurate interpretation of cyberattack procedures. Significant\nimprovements are shown when RAG is used for decoder-only LLMs, particularly\nwhen directly relevant context is found. This study further sheds insights on\nthe limitations and capabilities of using RAG for LLMs in interpreting TTPs.",
        "pdf_link": "https://arxiv.org/pdf/2401.00280v2.pdf"
    },
    {
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
        "authors": [
            "Yuanzhao Zhai",
            "Han Zhang",
            "Yu Lei",
            "Yue Yu",
            "Kele Xu",
            "Dawei Feng",
            "Bo Ding",
            "Huaimin Wang"
        ],
        "published": "2023-12-30T14:14:14Z",
        "summary": "Reinforcement learning from human feedback (RLHF) emerges as a promising\nparadigm for aligning large language models (LLMs). However, a notable\nchallenge in RLHF is overoptimization, where beyond a certain threshold, the\npursuit of higher rewards leads to a decline in human preferences. In this\npaper, we observe the weakness of KL regularization which is commonly employed\nin existing RLHF methods to address overoptimization. To mitigate this\nlimitation, we scrutinize the RLHF objective in the offline dataset and propose\nuncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty\nregularization during RL-finetuning. To enhance the uncertainty quantification\nabilities for reward models, we first propose a diverse low-rank adaptation\n(LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations.\nThen we optimize policy models utilizing penalized rewards, determined by both\nrewards and uncertainties provided by the diverse reward LoRA ensembles. Our\nexperimental results, based on two real human preference datasets, showcase the\neffectiveness of diverse reward LoRA ensembles in quantifying reward\nuncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be\npivotal in mitigating overoptimization, thereby contributing to the overall\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2401.00243v1.pdf"
    },
    {
        "title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
        "authors": [
            "Hengrui Cai",
            "Shengjie Liu",
            "Rui Song"
        ],
        "published": "2023-12-30T04:51:46Z",
        "summary": "This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes \"do-operators\" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs' causal reasoning ability depends on\nthe context and domain-specific knowledge provided, and supports the argument\nthat \"knowledge is, indeed, what LLMs principally require for sound causal\nreasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a\ndegree of causal reasoning using the available numerical data, albeit with\nlimitations in the calculations.",
        "pdf_link": "https://arxiv.org/pdf/2401.00139v1.pdf"
    },
    {
        "title": "Teach Large Language Models to Forget Privacy",
        "authors": [
            "Ran Yan",
            "Yujun Li",
            "Wenqian Li",
            "Peihua Mai",
            "Yan Pang",
            "Yinchuan Li"
        ],
        "published": "2023-12-30T01:26:42Z",
        "summary": "Large Language Models (LLMs) have proven powerful, but the risk of privacy\nleakage remains a significant concern. Traditional privacy-preserving methods,\nsuch as Differential Privacy and Homomorphic Encryption, are inadequate for\nblack-box API-only settings, demanding either model transparency or heavy\ncomputational resources. We propose Prompt2Forget (P2F), the first framework\ndesigned to tackle the LLM local privacy challenge by teaching LLM to forget.\nThe method involves decomposing full questions into smaller segments,\ngenerating fabricated answers, and obfuscating the model's memory of the\noriginal input. A benchmark dataset was crafted with questions containing\nprivacy-sensitive information from diverse fields. P2F achieves zero-shot\ngeneralization, allowing adaptability across a wide range of use cases without\nmanual adjustments. Experimental results indicate P2F's robust capability to\nobfuscate LLM's memory, attaining a forgetfulness score of around 90\\% without\nany utility loss. This represents an enhancement of up to 63\\% when contrasted\nwith the naive direct instruction technique, highlighting P2F's efficacy in\nmitigating memory retention of sensitive information within LLMs. Our findings\nestablish the first benchmark in the novel field of the LLM forgetting task,\nrepresenting a meaningful advancement in privacy preservation in the emerging\nLLM domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.00870v1.pdf"
    },
    {
        "title": "ChatEd: A Chatbot Leveraging ChatGPT for an Enhanced Learning Experience in Higher Education",
        "authors": [
            "Kevin Wang",
            "Jason Ramos",
            "Ramon Lawrence"
        ],
        "published": "2023-12-29T19:11:55Z",
        "summary": "With the rapid evolution of Natural Language Processing (NLP), Large Language\nModels (LLMs) like ChatGPT have emerged as powerful tools capable of\ntransforming various sectors. Their vast knowledge base and dynamic interaction\ncapabilities represent significant potential in improving education by\noperating as a personalized assistant. However, the possibility of generating\nincorrect, biased, or unhelpful answers are a key challenge to resolve when\ndeploying LLMs in an education context. This work introduces an innovative\narchitecture that combines the strengths of ChatGPT with a traditional\ninformation retrieval based chatbot framework to offer enhanced student support\nin higher education. Our empirical evaluations underscore the high promise of\nthis approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.00052v1.pdf"
    },
    {
        "title": "Principled Gradient-based Markov Chain Monte Carlo for Text Generation",
        "authors": [
            "Li Du",
            "Afra Amini",
            "Lucas Torroba Hennigen",
            "Xinyan Velocity Yu",
            "Jason Eisner",
            "Holden Lee",
            "Ryan Cotterell"
        ],
        "published": "2023-12-29T18:00:56Z",
        "summary": "Recent papers have demonstrated the possibility of energy-based text\ngeneration by adapting gradient-based sampling algorithms, a paradigm of MCMC\nalgorithms that promises fast convergence. However, as we show in this paper,\nprevious attempts on this approach to text generation all fail to sample\ncorrectly from the target language model distributions. To address this\nlimitation, we consider the problem of designing text samplers that are\nfaithful, meaning that they have the target text distribution as its limiting\ndistribution. We propose several faithful gradient-based sampling algorithms to\nsample from the target energy-based text distribution correctly, and study\ntheir theoretical properties. Through experiments on various forms of text\ngeneration, we demonstrate that faithful samplers are able to generate more\nfluent text while adhering to the control objectives better.",
        "pdf_link": "https://arxiv.org/pdf/2312.17710v1.pdf"
    },
    {
        "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
        "authors": [
            "Julien Piet",
            "Maha Alrashed",
            "Chawin Sitawarin",
            "Sizhe Chen",
            "Zeming Wei",
            "Elizabeth Sun",
            "Basel Alomair",
            "David Wagner"
        ],
        "published": "2023-12-29T16:37:53Z",
        "summary": "Large Language Models (LLMs) are attracting significant research attention\ndue to their instruction-following abilities, allowing users and developers to\nleverage LLMs for a variety of tasks. However, LLMs are vulnerable to\nprompt-injection attacks: a class of attacks that hijack the model's\ninstruction-following abilities, changing responses to prompts to undesired,\npossibly malicious ones. In this work, we introduce Jatmo, a method for\ngenerating task-specific models resilient to prompt-injection attacks. Jatmo\nleverages the fact that LLMs can only follow instructions once they have\nundergone instruction tuning. It harnesses a teacher instruction-tuned model to\ngenerate a task-specific dataset, which is then used to fine-tune a base model\n(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a\ndataset of inputs for the task: it uses the teacher model to generate outputs.\nFor situations with no pre-existing datasets, Jatmo can use a single example,\nor in some cases none at all, to produce a fully synthetic dataset. Our\nexperiments on seven tasks show that Jatmo models provide similar quality of\noutputs on their specific task as standard LLMs, while being resilient to\nprompt injections. The best attacks succeeded in less than 0.5% of cases\nagainst our models, versus 87% success rate against GPT-3.5-Turbo. We release\nJatmo at https://github.com/wagner-group/prompt-injection-defense.",
        "pdf_link": "https://arxiv.org/pdf/2312.17673v2.pdf"
    },
    {
        "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao"
        ],
        "published": "2023-12-29T15:57:49Z",
        "summary": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as\nOpenAI's GPT-4V(ision), has significantly impacted both academic and industrial\nrealms. These models enhance Large Language Models (LLMs) with advanced visual\nunderstanding capabilities, facilitating their application in a variety of\nmultimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM\ndesigned specifically for multimodal integration. Despite its advancements,\npreliminary benchmarks indicate that Gemini lags behind GPT models in\ncommonsense reasoning tasks. However, this assessment, based on a limited\ndataset (i.e., HellaSWAG), does not fully capture Gemini's authentic\ncommonsense reasoning potential. To address this gap, our study undertakes a\nthorough evaluation of Gemini's performance in complex reasoning tasks that\nnecessitate the integration of commonsense knowledge across modalities. We\ncarry out a comprehensive analysis of 12 commonsense reasoning datasets,\nranging from general to domain-specific tasks. This includes 11 datasets\nfocused solely on language, as well as one that incorporates multimodal\nelements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's\ncompetitive commonsense reasoning capabilities. Additionally, we identify\ncommon challenges faced by current LLMs and MLLMs in addressing commonsense\nproblems, underscoring the need for further advancements in enhancing the\ncommonsense reasoning abilities of these models.",
        "pdf_link": "https://arxiv.org/pdf/2312.17661v1.pdf"
    },
    {
        "title": "Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception",
        "authors": [
            "Yuncheng Huang",
            "Qianyu He",
            "Jiaqing Liang",
            "Sihang Jiang",
            "Yanghua Xiao",
            "Yunwen Chen"
        ],
        "published": "2023-12-29T09:29:37Z",
        "summary": "Quantities are distinct and critical components of texts that characterize\nthe magnitude properties of entities, providing a precise perspective for the\nunderstanding of natural language, especially for reasoning tasks. In recent\nyears, there has been a flurry of research on reasoning tasks based on large\nlanguage models (LLMs), most of which solely focus on numerical values,\nneglecting the dimensional concept of quantities with units despite its\nimportance. We argue that the concept of dimension is essential for precisely\nunderstanding quantities and of great significance for LLMs to perform\nquantitative reasoning. However, the lack of dimension knowledge and\nquantity-related benchmarks has resulted in low performance of LLMs. Hence, we\npresent a framework to enhance the quantitative reasoning ability of language\nmodels based on dimension perception. We first construct a dimensional unit\nknowledge base (DimUnitKB) to address the knowledge gap in this area. We\npropose a benchmark DimEval consisting of seven tasks of three categories to\nprobe and enhance the dimension perception skills of LLMs. To evaluate the\neffectiveness of our methods, we propose a quantitative reasoning task and\nconduct experiments. The experimental results show that our dimension\nperception method dramatically improves accuracy (43.55%->50.67%) on\nquantitative reasoning tasks compared to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.17532v1.pdf"
    },
    {
        "title": "Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning",
        "authors": [
            "Xiao-Yang Liu",
            "Rongyi Zhu",
            "Daochen Zha",
            "Jiechao Gao",
            "Shan Zhong",
            "Meikang Qiu"
        ],
        "published": "2023-12-29T06:50:38Z",
        "summary": "The surge in interest and application of large language models (LLMs) has\nsparked a drive to fine-tune these models to suit specific applications, such\nas finance and medical science. However, concerns regarding data privacy have\nemerged, especially when multiple stakeholders aim to collaboratively enhance\nLLMs using sensitive data. In this scenario, federated learning becomes a\nnatural choice, allowing decentralized fine-tuning without exposing raw data to\ncentral servers. Motivated by this, we investigate how data privacy can be\nensured in LLM fine-tuning through practical federated learning approaches,\nenabling secure contributions from multiple parties to enhance LLMs. Yet,\nchallenges arise: 1) despite avoiding raw data exposure, there is a risk of\ninferring sensitive information from model outputs, and 2) federated learning\nfor LLMs incurs notable communication overhead. To address these challenges,\nthis article introduces DP-LoRA, a novel federated learning algorithm tailored\nfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that\nadds noise in weight updates, maintaining individual data privacy while\nfacilitating collaborative model training. Moreover, DP-LoRA optimizes\ncommunication efficiency via low-rank adaptation, minimizing the transmission\nof updated weights during distributed training. The experimental results across\nmedical, financial, and general datasets using various LLMs demonstrate that\nDP-LoRA effectively ensures strict privacy constraints while minimizing\ncommunication overhead.",
        "pdf_link": "https://arxiv.org/pdf/2312.17493v1.pdf"
    },
    {
        "title": "Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters",
        "authors": [
            "Manikanta Loya",
            "Divya Anand Sinha",
            "Richard Futrell"
        ],
        "published": "2023-12-29T05:19:11Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to their widespread\nuse across a broad spectrum of tasks including decision making. Prior studies\nhave compared the decision making abilities of LLMs with those of humans from a\npsychological perspective. However, these studies have not always properly\naccounted for the sensitivity of LLMs' behavior to hyperparameters and\nvariations in the prompt. In this study, we examine LLMs' performance on the\nHorizon decision making task studied by Binz and Schulz (2023) analyzing how\nLLMs respond to variations in prompts and hyperparameters. By experimenting on\nthree OpenAI language models possessing different capabilities, we observe that\nthe decision making abilities fluctuate based on the input prompts and\ntemperature settings. Contrary to previous findings language models display a\nhuman-like exploration exploitation tradeoff after simple adjustments to the\nprompt.",
        "pdf_link": "https://arxiv.org/pdf/2312.17476v1.pdf"
    },
    {
        "title": "LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model",
        "authors": [
            "Senqiao Yang",
            "Tianyuan Qu",
            "Xin Lai",
            "Zhuotao Tian",
            "Bohao Peng",
            "Shu Liu",
            "Jiaya Jia"
        ],
        "published": "2023-12-28T18:58:33Z",
        "summary": "While LISA effectively bridges the gap between segmentation and large\nlanguage models to enable reasoning segmentation, it poses certain limitations:\nunable to distinguish different instances of the target region, and constrained\nby the pre-defined textual response formats. In this work, we introduce LISA++,\nan update to the existing LISA model, focusing on improving core\nfunctionalities while keeping the base architecture intact. The main\nenhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance\nsegmentation ability has been added, providing a more detailed scene analysis\nalong with the existing multi-region semantic segmentation. \\textbf{2) More\nNatural Conversation}: Improved capability for multi-turn dialogue, with the\nability to incorporate segmentation results directly into text responses, i.e.,\nSegmentation in Dialogue (SiD). These improvements are achieved by curating the\nexisting samples of generic segmentation datasets, aimed specifically at\nenhancing the segmentation and conversational skills without structural change\nand additional data sources. Comparative analysis with the original LISA model\nshows significant advancements in these areas, positioning LISA++ as a notable\nupgrade in visual understanding and interaction. LISA++'s adaptability and\nimproved features highlight the versatility of the mask-as-embedding paradigm\nproposed by LISA, and the potential as a foundational model for diverse\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2312.17240v3.pdf"
    },
    {
        "title": "Fast Inference of Mixture-of-Experts Language Models with Offloading",
        "authors": [
            "Artyom Eliseev",
            "Denis Mazur"
        ],
        "published": "2023-12-28T18:58:13Z",
        "summary": "With the widespread adoption of Large Language Models (LLMs), many deep\nlearning practitioners are looking for strategies of running these models more\nefficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a\ntype of model architectures where only a fraction of model layers are active\nfor any given input. This property allows MoE-based language models to generate\ntokens faster than their dense counterparts, but it also increases model size\ndue to having multiple experts. Unfortunately, this makes state-of-the-art MoE\nlanguage models difficult to run without high-end GPUs. In this work, we study\nthe problem of running large MoE language models on consumer hardware with\nlimited accelerator memory. We build upon parameter offloading algorithms and\npropose a novel strategy that accelerates offloading by taking advantage of\ninnate properties of MoE LLMs. Using this strategy, we build can run\nMixtral-8x7B with mixed quantization on desktop hardware and free-tier Google\nColab instances.",
        "pdf_link": "https://arxiv.org/pdf/2312.17238v1.pdf"
    },
    {
        "title": "Large Language Model for Causal Decision Making",
        "authors": [
            "Haitao Jiang",
            "Lin Ge",
            "Yuhe Gao",
            "Jianian Wang",
            "Rui Song"
        ],
        "published": "2023-12-28T16:59:06Z",
        "summary": "Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\ninference based on user-specified structured data and knowledge in corpus-rare\nconcepts like causal decision-making is still limited. In this work, we explore\nthe possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can\nidentify the causal task, execute a corresponding function, and interpret its\nnumerical results based on users' queries and the provided dataset. Meanwhile,\nwe propose a data generation process for more controllable GPT prompting and\npresent two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal\nproblem identification and input parameter extraction for causal function\ncalling and (2) Causal-Interpret-Bench for in-context causal interpretation.\nWith three case studies, we showed that LLM4Causal can deliver end-to-end\nsolutions for causal problems and provide easy-to-understand answers. Numerical\nstudies also reveal that it has a remarkable ability to identify the correct\ncausal task given a query.",
        "pdf_link": "https://arxiv.org/pdf/2312.17122v2.pdf"
    },
    {
        "title": "How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation",
        "authors": [
            "Yang Xiao",
            "Yi Cheng",
            "Jinlan Fu",
            "Jiashuo Wang",
            "Wenjie Li",
            "Pengfei Liu"
        ],
        "published": "2023-12-28T16:51:11Z",
        "summary": "Human behavior simulation of AI agents necessitates the agents to possess a\nquality of believability, which is crucial as it facilitates users in\nestablishing trust toward the agents and streamlines the fulfillment of the\nagents' goal. While recent advancements in Large Language Model (LLM) based\nagents have improved human behavior simulation, challenges inherent to LLMs\n(e.g., long context modeling) can undermine their believability. Consequently,\nevaluating AI agent believability becomes imperative. Unfortunately, prior\nresearch often neglects the negative impacts of LLM deficiencies. To address\nthese gaps, we introduce two metrics for assessing LLM-based agent\nbelievability: consistency, and robustness, together with a benchmark,\nSimulateBench, with which, we evaluate the consistency and robustness of agents\nimplemented with popular LLMs. We find that agents (i) struggle to accurately\ndepict character information when presented with lengthy profile inputs; (ii)\nexhibit vulnerability to profile perturbations; and (iii) are significantly\naffected by certain key factors that impact their overall believability. Code\nand SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.",
        "pdf_link": "https://arxiv.org/pdf/2312.17115v1.pdf"
    },
    {
        "title": "GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension",
        "authors": [
            "Bohan Lyu",
            "Xin Cong",
            "Heyang Yu",
            "Pan Yang",
            "Yujia Qin",
            "Yining Ye",
            "Yaxi Lu",
            "Zhong Zhang",
            "Yukun Yan",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-12-28T15:47:30Z",
        "summary": "While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nexceptional proficiency in natural language processing, their efficacy in\naddressing complex, multifaceted tasks remains limited. A growing area of\nresearch focuses on LLM-based agents equipped with external tools capable of\nperforming diverse tasks. However, existing LLM-based agents only support a\nlimited set of tools which is unable to cover a diverse range of user queries,\nespecially for those involving expertise domains. It remains a challenge for\nLLM-based agents to extend their tools autonomously when confronted with\nvarious user queries. As GitHub has hosted a multitude of repositories which\ncan be seen as a good resource for tools, a promising solution is that\nLLM-based agents can autonomously integrate the repositories in GitHub\naccording to the user queries to extend their tool set. In this paper, we\nintroduce GitAgent, an agent capable of achieving the autonomous tool extension\nfrom GitHub. GitAgent follows a four-phase procedure to incorporate\nrepositories and it can learn human experience by resorting to GitHub\nIssues/PRs to solve problems encountered during the procedure. Experimental\nevaluation involving 30 user queries demonstrates GitAgent's effectiveness,\nachieving a 69.4% success rate on average.",
        "pdf_link": "https://arxiv.org/pdf/2312.17294v1.pdf"
    },
    {
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "authors": [
            "Chen Qian",
            "Yufan Dang",
            "Jiahao Li",
            "Wei Liu",
            "Weize Chen",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-12-28T13:50:42Z",
        "summary": "Recent advancements in large language models (LLMs) have brought significant\nchanges to various domains, especially through LLM-driven autonomous agents.\nThese agents are now capable of collaborating seamlessly, splitting tasks and\nenhancing accuracy, thus minimizing the need for human involvement. However,\nthese agents often approach a diverse range of tasks in isolation, without\nbenefiting from past experiences. This isolation can lead to repeated mistakes\nand inefficient trials in task solving. To this end, this paper introduces\nExperiential Co-Learning, a novel framework in which instructor and assistant\nagents gather shortcut-oriented experiences from their historical trajectories\nand use these past experiences for mutual reasoning. This paradigm, enriched\nwith previous experiences, equips agents to more effectively address unseen\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.17025v2.pdf"
    },
    {
        "title": "AI Content Self-Detection for Transformer-based Large Language Models",
        "authors": [
            "Ant\u00f4nio Junior Alves Caiado",
            "Michael Hahsler"
        ],
        "published": "2023-12-28T10:08:57Z",
        "summary": "$ $The usage of generative artificial intelligence (AI) tools based on large\nlanguage models, including ChatGPT, Bard, and Claude, for text generation has\nmany exciting applications with the potential for phenomenal productivity\ngains. One issue is authorship attribution when using AI tools. This is\nespecially important in an academic setting where the inappropriate use of\ngenerative AI tools may hinder student learning or stifle research by creating\na large amount of automatically generated derivative work. Existing plagiarism\ndetection systems can trace the source of submitted text but are not yet\nequipped with methods to accurately detect AI-generated text. This paper\nintroduces the idea of direct origin detection and evaluates whether generative\nAI systems can recognize their output and distinguish it from human-written\ntexts. We argue why current transformer-based models may be able to self-detect\ntheir own generated text and perform a small empirical study using zero-shot\nlearning to investigate if that is the case. Results reveal varying\ncapabilities of AI systems to identify their generated text. Google's Bard\nmodel exhibits the largest capability of self-detection with an accuracy of\n94\\%, followed by OpenAI's ChatGPT with 83\\%. On the other hand, Anthropic's\nClaude model seems to be not able to self-detect.",
        "pdf_link": "https://arxiv.org/pdf/2312.17289v1.pdf"
    },
    {
        "title": "Spike No More: Stabilizing the Pre-training of Large Language Models",
        "authors": [
            "Sho Takase",
            "Shun Kiyono",
            "Sosuke Kobayashi",
            "Jun Suzuki"
        ],
        "published": "2023-12-28T08:53:27Z",
        "summary": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. To investigate the cause of loss spikes, we focus on\ngradients of internal layers. Through theoretical analyses, we reveal two\ncauses of the exploding gradients, and provide requirements to prevent the\nexplosion. In addition, we propose a method to satisfy the requirements by\ncombining the initialization method and a simple modification to embeddings. We\nconduct various experiments to verify our theoretical analyses empirically.\nExperimental results indicate that the combination is effective in preventing\nspikes during pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2312.16903v2.pdf"
    },
    {
        "title": "Large Language Models for Conducting Advanced Text Analytics Information Systems Research",
        "authors": [
            "Benjamin M. Ampel",
            "Chi-Heng Yang",
            "James Hu",
            "Hsinchun Chen"
        ],
        "published": "2023-12-27T19:49:00Z",
        "summary": "The exponential growth of digital content has generated massive textual\ndatasets, necessitating advanced analytical approaches. Large Language Models\n(LLMs) have emerged as tools capable of processing and extracting insights from\nmassive unstructured textual datasets. However, how to leverage LLMs for\ntext-based Information Systems (IS) research is currently unclear. To assist IS\nresearch in understanding how to operationalize LLMs, we propose a Text\nAnalytics for Information Systems Research (TAISR) framework. Our proposed\nframework provides detailed recommendations grounded in IS and LLM literature\non how to conduct meaningful text-based IS research. We conducted three case\nstudies in business intelligence using our TAISR framework to demonstrate its\napplication across several IS research contexts. We also outline potential\nchallenges and limitations in adopting LLMs for IS. By offering a systematic\napproach and evidence of its utility, our TAISR framework contributes to future\nIS research streams looking to incorporate powerful LLMs for text analytics.",
        "pdf_link": "https://arxiv.org/pdf/2312.17278v1.pdf"
    },
    {
        "title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model",
        "authors": [
            "Yongchang Cao",
            "Liang He",
            "Zhen Wu",
            "Xinyu Dai"
        ],
        "published": "2023-12-27T16:11:07Z",
        "summary": "BERT-based models have shown a remarkable ability in the Chinese Spelling\nCheck (CSC) task recently. However, traditional BERT-based methods still suffer\nfrom two limitations. First, although previous works have identified that\nexplicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the\nCSC task, they neglected the fact that spelling errors inherent in CSC data can\nlead to incorrect tags and therefore mislead models. Additionally, they ignored\nthe correlation between the implicit hierarchical information encoded by BERT's\nintermediate layers and different linguistic phenomena. This results in\nsub-optimal accuracy. To alleviate the above two issues, we design a\nheterogeneous knowledge-infused framework to strengthen BERT-based CSC models.\nTo incorporate explicit POS knowledge, we utilize an auxiliary task strategy\ndriven by Gaussian mixture model. Meanwhile, to incorporate implicit\nhierarchical linguistic knowledge within the encoder, we propose a novel form\nof n-gram-based layerwise self-attention to generate a multilayer\nrepresentation. Experimental results show that our proposed framework yields a\nstable performance boost over four strong baseline models and outperforms the\nprevious state-of-the-art methods on two datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.16623v1.pdf"
    },
    {
        "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
        "authors": [
            "Qingyao Li",
            "Lingyue Fu",
            "Weiming Zhang",
            "Xianyu Chen",
            "Jingwei Yu",
            "Wei Xia",
            "Weinan Zhang",
            "Ruiming Tang",
            "Yong Yu"
        ],
        "published": "2023-12-27T14:37:32Z",
        "summary": "Online education platforms, leveraging the internet to distribute education\nresources, seek to provide convenient education but often fall short in\nreal-time communication with students. They often struggle to offer\npersonalized education resources due to the challenge of addressing the diverse\nobstacles students encounter throughout their learning journey. Recently, the\nemergence of large language models (LLMs), such as ChatGPT, offers the\npossibility for resolving this issue by comprehending individual requests.\nAlthough LLMs have been successful in various fields, creating an LLM-based\neducation system is still challenging for the wide range of educational skills\nrequired. This paper reviews the recently emerged LLM researches related to\neducational capabilities, including mathematics, writing, programming,\nreasoning, and knowledge-based question answering, with the aim to explore\ntheir potential in constructing the next-generation intelligent education\nsystem. Based on the current development status, we further outline two\napproaches for an LLM-based education system: a unified approach and a\nmixture-of-expert (MoE) approach. Finally, we explore the challenges and future\ndirections, providing new research opportunities and perspectives on adapting\nLLMs for education.",
        "pdf_link": "https://arxiv.org/pdf/2401.08664v2.pdf"
    },
    {
        "title": "How Robust are LLMs to In-Context Majority Label Bias?",
        "authors": [
            "Karan Gupta",
            "Sumegh Roychowdhury",
            "Siva Rajesh Kasa",
            "Santhosh Kumar Kasa",
            "Anish Bhanushali",
            "Nikhil Pattisapu",
            "Prasanna Srinivasa Murthy"
        ],
        "published": "2023-12-27T12:20:12Z",
        "summary": "In the In-Context Learning (ICL) setup, various forms of label biases can\nmanifest. One such manifestation is majority label bias, which arises when the\ndistribution of labeled examples in the in-context samples is skewed towards\none or more specific classes making Large Language Models (LLMs) more prone to\npredict those labels. Such discrepancies can arise from various factors,\nincluding logistical constraints, inherent biases in data collection methods,\nlimited access to diverse data sources, etc. which are unavoidable in a\nreal-world industry setup. In this work, we study the robustness of in-context\nlearning in LLMs to shifts that occur due to majority label bias within the\npurview of text classification tasks. Prior works have shown that in-context\nlearning with LLMs is susceptible to such biases. In our study, we go one level\ndeeper and show that the robustness boundary varies widely for different models\nand tasks, with certain LLMs being highly robust (~90%) to majority label bias.\nAdditionally, our findings also highlight the impact of model size and the\nrichness of instructional prompts contributing towards model robustness. We\nrestrict our study to only publicly available open-source models to ensure\ntransparency and reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2312.16549v1.pdf"
    },
    {
        "title": "LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis",
        "authors": [
            "Jinwen He",
            "Yujia Gong",
            "Kai Chen",
            "Zijin Lin",
            "Chengan Wei",
            "Yue Zhao"
        ],
        "published": "2023-12-27T01:44:47Z",
        "summary": "Large Language Models (LLMs) have revolutionized various domains with\nextensive knowledge and creative capabilities. However, a critical issue with\nLLMs is their tendency to produce outputs that diverge from factual reality.\nThis phenomenon is particularly concerning in sensitive applications such as\nmedical consultation and legal advice, where accuracy is paramount. In this\npaper, we introduce the LLM factoscope, a novel Siamese network-based model\nthat leverages the inner states of LLMs for factual detection. Our\ninvestigation reveals distinguishable patterns in LLMs' inner states when\ngenerating factual versus non-factual content. We demonstrate the LLM\nfactoscope's effectiveness across various architectures, achieving over 96%\naccuracy in factual detection. Our work opens a new avenue for utilizing LLMs'\ninner states for factual detection and encourages further exploration into\nLLMs' inner workings for enhanced reliability and transparency.",
        "pdf_link": "https://arxiv.org/pdf/2312.16374v2.pdf"
    },
    {
        "title": "LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing",
        "authors": [
            "Luyi Ma",
            "Nikhil Thakurdesai",
            "Jiao Chen",
            "Jianpeng Xu",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "published": "2023-12-26T23:08:38Z",
        "summary": "Data processing is one of the fundamental steps in machine learning pipelines\nto ensure data quality. Majority of the applications consider the user-defined\nfunction (UDF) design pattern for data processing in databases. Although the\nUDF design pattern introduces flexibility, reusability and scalability, the\nincreasing demand on machine learning pipelines brings three new challenges to\nthis design pattern -- not low-code, not dependency-free and not\nknowledge-aware. To address these challenges, we propose a new design pattern\nthat large language models (LLMs) could work as a generic data operator\n(LLM-GDO) for reliable data cleansing, transformation and modeling with their\nhuman-compatible performance. In the LLM-GDO design pattern, user-defined\nprompts (UDPs) are used to represent the data processing logic rather than\nimplementations with a specific programming language. LLMs can be centrally\nmaintained so users don't have to manage the dependencies at the run-time.\nFine-tuning LLMs with domain-specific data could enhance the performance on the\ndomain-specific tasks which makes data processing knowledge-aware. We\nillustrate these advantages with examples in different data processing tasks.\nFurthermore, we summarize the challenges and opportunities introduced by LLMs\nto provide a complete view of this design pattern for more discussions.",
        "pdf_link": "https://arxiv.org/pdf/2312.16351v1.pdf"
    },
    {
        "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
        "authors": [
            "Changmao Li",
            "Jeffrey Flanigan"
        ],
        "published": "2023-12-26T21:17:46Z",
        "summary": "Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2312.16337v1.pdf"
    },
    {
        "title": "Can ChatGPT Read Who You Are?",
        "authors": [
            "Erik Derner",
            "Dalibor Ku\u010dera",
            "Nuria Oliver",
            "Jan Zah\u00e1lka"
        ],
        "published": "2023-12-26T14:43:04Z",
        "summary": "The interplay between artificial intelligence (AI) and psychology,\nparticularly in personality assessment, represents an important emerging area\nof research. Accurate personality trait estimation is crucial not only for\nenhancing personalization in human-computer interaction but also for a wide\nvariety of applications ranging from mental health to education. This paper\nanalyzes the capability of a generic chatbot, ChatGPT, to effectively infer\npersonality traits from short texts. We report the results of a comprehensive\nuser study featuring texts written in Czech by a representative population\nsample of 155 participants. Their self-assessments based on the Big Five\nInventory (BFI) questionnaire serve as the ground truth. We compare the\npersonality trait estimations made by ChatGPT against those by human raters and\nreport ChatGPT's competitive performance in inferring personality traits from\ntext. We also uncover a 'positivity bias' in ChatGPT's assessments across all\npersonality dimensions and explore the impact of prompt composition on\naccuracy. This work contributes to the understanding of AI capabilities in\npsychological assessment, highlighting both the potential and limitations of\nusing large language models for personality inference. Our research underscores\nthe importance of responsible AI development, considering ethical implications\nsuch as privacy, consent, autonomy, and bias in AI applications.",
        "pdf_link": "https://arxiv.org/pdf/2312.16070v1.pdf"
    },
    {
        "title": "A Prompt Learning Framework for Source Code Summarization",
        "authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yudu You",
            "Yuchen Chen",
            "Yi Liu",
            "Chong Wang",
            "Jian Zhang",
            "Quanjun Zhang",
            "Hanwei Qian",
            "Wei Zhao",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "published": "2023-12-26T14:37:55Z",
        "summary": "(Source) code summarization is the task of automatically generating natural\nlanguage summaries for given code snippets. Such summaries play a key role in\nhelping developers understand and maintain source code. Recently, with the\nsuccessful application of large language models (LLMs) in numerous fields,\nsoftware engineering researchers have also attempted to adapt LLMs to solve\ncode summarization tasks. The main adaptation schemes include instruction\nprompting and task-oriented fine-tuning. However, instruction prompting\ninvolves designing crafted prompts for zero-shot learning or selecting\nappropriate samples for few-shot learning and requires users to have\nprofessional domain knowledge, while task-oriented fine-tuning requires high\ntraining costs. In this paper, we propose a novel prompt learning framework for\ncode summarization called PromptCS. PromptCS trains a prompt agent that can\ngenerate continuous prompts to unleash the potential for LLMs in code\nsummarization. Compared to the human-written discrete prompt, the continuous\nprompts are produced under the guidance of LLMs and are therefore easier to\nunderstand by LLMs. PromptCS freezes the parameters of LLMs when training the\nprompt agent, which can greatly reduce the requirements for training resources.\nWe evaluate PromptCS on the CodeSearchNet dataset involving multiple\nprogramming languages. The results show that PromptCS significantly outperforms\ninstruction prompting schemes on all four widely used metrics. In some base\nLLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS even\noutperforms the task-oriented fine-tuning scheme. More importantly, the\ntraining efficiency of PromptCS is faster than the task-oriented fine-tuning\nscheme, with a more pronounced advantage on larger LLMs. The results of the\nhuman evaluation demonstrate that PromptCS can generate more good summaries\ncompared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2312.16066v1.pdf"
    },
    {
        "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks",
        "authors": [
            "Jingyao Li",
            "Pengguang Chen",
            "Jiaya Jia"
        ],
        "published": "2023-12-26T08:49:57Z",
        "summary": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.",
        "pdf_link": "https://arxiv.org/pdf/2312.15960v2.pdf"
    },
    {
        "title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph",
        "authors": [
            "Tiezheng Guo",
            "Qingwen Yang",
            "Chen Wang",
            "Yanyi Liu",
            "Pan Li",
            "Jiawei Tang",
            "Dapeng Li",
            "Yingyou Wen"
        ],
        "published": "2023-12-26T04:22:56Z",
        "summary": "Large language model (LLM) has achieved outstanding performance on various\ndownstream tasks with its powerful natural language understanding and zero-shot\ncapability, but LLM still suffers from knowledge limitation. Especially in\nscenarios that require long logical chains or complex reasoning, the\nhallucination and knowledge limitation of LLM limit its performance in question\nanswering (QA). In this paper, we propose a novel framework KnowledgeNavigator\nto address these challenges by efficiently and accurately retrieving external\nknowledge from knowledge graph and using it as a key factor to enhance LLM\nreasoning. Specifically, KnowledgeNavigator first mines and enhances the\npotential constraints of the given question to guide the reasoning. Then it\nretrieves and filters external knowledge that supports answering through\niterative reasoning on knowledge graph with the guidance of LLM and the\nquestion. Finally, KnowledgeNavigator constructs the structured knowledge into\neffective prompts that are friendly to LLM to help its reasoning. We evaluate\nKnowledgeNavigator on multiple public KGQA benchmarks, the experiments show the\nframework has great effectiveness and generalization, outperforming previous\nknowledge graph enhanced LLM methods and is comparable to the fully supervised\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2312.15880v2.pdf"
    },
    {
        "title": "SecQA: A Concise Question-Answering Dataset for Evaluating Large Language Models in Computer Security",
        "authors": [
            "Zefang Liu"
        ],
        "published": "2023-12-26T00:59:30Z",
        "summary": "In this paper, we introduce SecQA, a novel dataset tailored for evaluating\nthe performance of Large Language Models (LLMs) in the domain of computer\nsecurity. Utilizing multiple-choice questions generated by GPT-4 based on the\n\"Computer Systems Security: Planning for Success\" textbook, SecQA aims to\nassess LLMs' understanding and application of security principles. We detail\nthe structure and intent of SecQA, which includes two versions of increasing\ncomplexity, to provide a concise evaluation across various difficulty levels.\nAdditionally, we present an extensive evaluation of prominent LLMs, including\nGPT-3.5-Turbo, GPT-4, Llama-2, Vicuna, Mistral, and Zephyr models, using both\n0-shot and 5-shot learning settings. Our results, encapsulated in the SecQA v1\nand v2 datasets, highlight the varying capabilities and limitations of these\nmodels in the computer security context. This study not only offers insights\ninto the current state of LLMs in understanding security-related content but\nalso establishes SecQA as a benchmark for future advancements in this critical\nresearch area.",
        "pdf_link": "https://arxiv.org/pdf/2312.15838v1.pdf"
    },
    {
        "title": "Large Language Models are Not Stable Recommender Systems",
        "authors": [
            "Tianhui Ma",
            "Yuan Cheng",
            "Hengshu Zhu",
            "Hui Xiong"
        ],
        "published": "2023-12-25T14:54:33Z",
        "summary": "With the significant successes of large language models (LLMs) in many\nnatural language processing tasks, there is growing interest among researchers\nin exploring LLMs for novel recommender systems. However, we have observed that\ndirectly using LLMs as a recommender system is usually unstable due to its\ninherent position bias. To this end, we introduce exploratory research and find\nconsistent patterns of positional bias in LLMs that influence the performance\nof recommendation across a range of scenarios. Then, we propose a Bayesian\nprobabilistic framework, STELLA (Stable LLM for Recommendation), which involves\na two-stage pipeline. During the first probing stage, we identify patterns in a\ntransition matrix using a probing detection dataset. And in the second\nrecommendation stage, a Bayesian strategy is employed to adjust the biased\noutput of LLMs with an entropy indicator. Therefore, our framework can\ncapitalize on existing pattern information to calibrate instability of LLMs,\nand enhance recommendation performance. Finally, extensive experiments clearly\nvalidate the effectiveness of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.15746v1.pdf"
    },
    {
        "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023-12-25T12:32:49Z",
        "summary": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2312.15710v2.pdf"
    },
    {
        "title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data",
        "authors": [
            "Shirong Ma",
            "Shen Huang",
            "Shulin Huang",
            "Xiaobin Wang",
            "Yangning Li",
            "Hai-Tao Zheng",
            "Pengjun Xie",
            "Fei Huang",
            "Yong Jiang"
        ],
        "published": "2023-12-25T11:31:47Z",
        "summary": "Large Language Models (LLMs) pre-trained on massive corpora have exhibited\nremarkable performance on various NLP tasks. However, applying these models to\nspecific domains still poses significant challenges, such as lack of domain\nknowledge, limited capacity to leverage domain knowledge and inadequate\nadaptation to domain-specific data formats. Considering the exorbitant cost of\ntraining LLMs from scratch and the scarcity of annotated data within particular\ndomains, in this work, we focus on domain-specific continual pre-training of\nLLMs using E-commerce domain as an exemplar. Specifically, we explore the\nimpact of continual pre-training on LLMs employing unlabeled general and\nE-commercial corpora. Furthermore, we design a mixing strategy among different\ndata sources to better leverage E-commercial semi-structured data. We construct\nmultiple tasks to assess LLMs' few-shot In-context Learning ability and their\nzero-shot performance after instruction tuning in E-commerce domain.\nExperimental results demonstrate the effectiveness of continual pre-training of\nE-commerce LLMs and the efficacy of our devised data mixing strategy.",
        "pdf_link": "https://arxiv.org/pdf/2312.15696v1.pdf"
    },
    {
        "title": "Instruction Fusion: Advancing Prompt Evolution through Hybridization",
        "authors": [
            "Weidong Guo",
            "Jiuding Yang",
            "Kaitong Yang",
            "Xiangyang Li",
            "Zhuwei Rao",
            "Yu Xu",
            "Di Niu"
        ],
        "published": "2023-12-25T11:00:37Z",
        "summary": "The fine-tuning of Large Language Models (LLMs) specialized in code\ngeneration has seen notable advancements through the use of open-domain coding\nqueries. Despite the successes, existing methodologies like Evol-Instruct\nencounter performance limitations, impeding further enhancements in code\ngeneration tasks. This paper examines the constraints of existing prompt\nevolution techniques and introduces a novel approach, Instruction Fusion (IF).\nIF innovatively combines two distinct prompts through a hybridization process,\nthereby enhancing the evolution of training prompts for code LLMs. Our\nexperimental results reveal that the proposed novel method effectively\naddresses the shortcomings of prior methods, significantly improving the\nperformance of Code LLMs across five code generation benchmarks, namely\nHumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the\neffectiveness of Instruction Fusion in advancing the capabilities of LLMs in\ncode generation.",
        "pdf_link": "https://arxiv.org/pdf/2312.15692v3.pdf"
    },
    {
        "title": "ESGReveal: An LLM-based approach for extracting structured data from ESG reports",
        "authors": [
            "Yi Zou",
            "Mengying Shi",
            "Zhongjie Chen",
            "Zhu Deng",
            "ZongXiong Lei",
            "Zihan Zeng",
            "Shiming Yang",
            "HongXiang Tong",
            "Lei Xiao",
            "Wenwen Zhou"
        ],
        "published": "2023-12-25T06:44:32Z",
        "summary": "ESGReveal is an innovative method proposed for efficiently extracting and\nanalyzing Environmental, Social, and Governance (ESG) data from corporate\nreports, catering to the critical need for reliable ESG information retrieval.\nThis approach utilizes Large Language Models (LLM) enhanced with Retrieval\nAugmented Generation (RAG) techniques. The ESGReveal system includes an ESG\nmetadata module for targeted queries, a preprocessing module for assembling\ndatabases, and an LLM agent for data extraction. Its efficacy was appraised\nusing ESG reports from 166 companies across various sectors listed on the Hong\nKong Stock Exchange in 2022, ensuring comprehensive industry and market\ncapitalization representation. Utilizing ESGReveal unearthed significant\ninsights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in\ndata extraction and 83.7% in disclosure analysis, which is an improvement over\nbaseline models. This highlights the framework's capacity to refine ESG data\nanalysis precision. Moreover, it revealed a demand for reinforced ESG\ndisclosures, with environmental and social data disclosures standing at 69.5%\nand 57.2%, respectively, suggesting a pursuit for more corporate transparency.\nWhile current iterations of ESGReveal do not process pictorial information, a\nfunctionality intended for future enhancement, the study calls for continued\nresearch to further develop and compare the analytical capabilities of various\nLLMs. In summary, ESGReveal is a stride forward in ESG data processing,\noffering stakeholders a sophisticated tool to better evaluate and advance\ncorporate sustainability efforts. Its evolution is promising in promoting\ntransparency in corporate reporting and aligning with broader sustainable\ndevelopment aims.",
        "pdf_link": "https://arxiv.org/pdf/2312.17264v1.pdf"
    },
    {
        "title": "Privacy-Preserved Neural Graph Databases",
        "authors": [
            "Qi Hu",
            "Haoran Li",
            "Jiaxin Bai",
            "Zihao Wang",
            "Yangqiu Song"
        ],
        "published": "2023-12-25T02:32:05Z",
        "summary": "In the era of large language models (LLMs), efficient and accurate data\nretrieval has become increasingly crucial for the use of domain-specific or\nprivate data in the retrieval augmented generation (RAG). Neural graph\ndatabases (NGDBs) have emerged as a powerful paradigm that combines the\nstrengths of graph databases (GDBs) and neural networks to enable efficient\nstorage, retrieval, and analysis of graph-structured data which can be\nadaptively trained with LLMs. The usage of neural embedding storage and Complex\nneural logical Query Answering (CQA) provides NGDBs with generalization\nability. When the graph is incomplete, by extracting latent patterns and\nrepresentations, neural graph databases can fill gaps in the graph structure,\nrevealing hidden relationships and enabling accurate query answering.\nNevertheless, this capability comes with inherent trade-offs, as it introduces\nadditional privacy risks to the domain-specific or private databases. Malicious\nattackers can infer more sensitive information in the database using\nwell-designed queries such as from the answer sets of where Turing Award\nwinners born before 1950 and after 1940 lived, the living places of Turing\nAward winner Hinton are probably exposed, although the living places may have\nbeen deleted in the training stage due to the privacy concerns. In this work,\nwe propose a privacy-preserved neural graph database (P-NGDB) framework to\nalleviate the risks of privacy leakage in NGDBs. We introduce adversarial\ntraining techniques in the training stage to enforce the NGDBs to generate\nindistinguishable answers when queried with private information, enhancing the\ndifficulty of inferring sensitive information through combinations of multiple\ninnocuous queries.",
        "pdf_link": "https://arxiv.org/pdf/2312.15591v4.pdf"
    },
    {
        "title": "Reducing LLM Hallucinations using Epistemic Neural Networks",
        "authors": [
            "Shreyas Verma",
            "Kien Tran",
            "Yusuf Ali",
            "Guangyu Min"
        ],
        "published": "2023-12-25T01:17:01Z",
        "summary": "Reducing and detecting hallucinations in large language models is an open\nresearch problem. In this project, we attempt to leverage recent advances in\nthe field of uncertainty estimation to reduce hallucinations in frozen large\nlanguage models. Epistemic neural networks have recently been proposed to\nimprove output joint distributions for large pre-trained models. ENNs are small\nnetworks attached to large, frozen models to improve the model's joint\ndistributions and uncertainty estimates. In this work, we train an epistemic\nneural network on top of the Llama-2 7B model combined with a contrastive\ndecoding feature enhancement technique. We are the first to train an ENN for\nthe next token prediction task and explore the efficacy of this method in\nreducing hallucinations on the TruthfulQA dataset. In essence, we provide a\nmethod that leverages a pre-trained model's latent embeddings to reduce\nhallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.15576v1.pdf"
    },
    {
        "title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective",
        "authors": [
            "George Gui",
            "Olivier Toubia"
        ],
        "published": "2023-12-24T16:32:35Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive potential to\nsimulate human behavior. Using a causal inference framework, we empirically and\ntheoretically analyze the challenges of conducting LLM-simulated experiments,\nand explore potential solutions. In the context of demand estimation, we show\nthat variations in the treatment included in the prompt (e.g., price of focal\nproduct) can cause variations in unspecified confounding factors (e.g., price\nof competitors, historical prices, outside temperature), introducing\nendogeneity and yielding implausibly flat demand curves. We propose a\ntheoretical framework suggesting this endogeneity issue generalizes to other\ncontexts and won't be fully resolved by merely improving the training data.\nUnlike real experiments where researchers assign pre-existing units across\nconditions, LLMs simulate units based on the entire prompt, which includes the\ndescription of the treatment. Therefore, due to associations in the training\ndata, the characteristics of individuals and environments simulated by the LLM\ncan be affected by the treatment assignment. We explore two potential\nsolutions. The first specifies all contextual variables that affect both\ntreatment and outcome, which we demonstrate to be challenging for a\ngeneral-purpose LLM. The second explicitly specifies the source of treatment\nvariation in the prompt given to the LLM (e.g., by informing the LLM that the\nstore is running an experiment). While this approach only allows the estimation\nof a conditional average treatment effect that depends on the specific\nexperimental design, it provides valuable directional results for exploratory\nanalysis.",
        "pdf_link": "https://arxiv.org/pdf/2312.15524v1.pdf"
    },
    {
        "title": "A Group Fairness Lens for Large Language Models",
        "authors": [
            "Guanqun Bi",
            "Lei Shen",
            "Yuqiang Xie",
            "Yanan Cao",
            "Tiangang Zhu",
            "Xiaodong He"
        ],
        "published": "2023-12-24T13:25:15Z",
        "summary": "The rapid advancement of large language models has revolutionized various\napplications but also raised crucial concerns about their potential to\nperpetuate biases and unfairness when deployed in social media contexts.\nEvaluating LLMs' potential biases and fairness has become crucial, as existing\nmethods rely on limited prompts focusing on just a few groups, lacking a\ncomprehensive categorical perspective. In this paper, we propose evaluating LLM\nbiases from a group fairness lens using a novel hierarchical schema\ncharacterizing diverse social groups. Specifically, we construct a dataset,\nGFair, encapsulating target-attribute combinations across multiple dimensions.\nIn addition, we introduce statement organization, a new open-ended text\ngeneration task, to uncover complex biases in LLMs. Extensive evaluations of\npopular LLMs reveal inherent safety concerns. To mitigate the biases of LLM\nfrom a group fairness perspective, we pioneer a novel chain-of-thought method\nGF-Think to mitigate biases of LLMs from a group fairness perspective.\nExperimental results demonstrate its efficacy in mitigating bias in LLMs to\nachieve fairness.",
        "pdf_link": "https://arxiv.org/pdf/2312.15478v1.pdf"
    },
    {
        "title": "Towards Consistent Language Models Using Declarative Constraints",
        "authors": [
            "Jasmin Mousavi",
            "Arash Termehchy"
        ],
        "published": "2023-12-24T12:53:07Z",
        "summary": "Large language models have shown unprecedented abilities in generating\nlinguistically coherent and syntactically correct natural language output.\nHowever, they often return incorrect and inconsistent answers to input\nquestions. Due to the complexity and uninterpretability of the internally\nlearned representations, it is challenging to modify language models such that\nthey provide correct and consistent results. The data management community has\ndeveloped various methods and tools for providing consistent answers over\ninconsistent datasets. In these methods, users specify the desired properties\nof data in a domain in the form of high-level declarative constraints. This\napproach has provided usable and scalable methods to delivering consistent\ninformation from inconsistent datasets. We aim to build upon this success and\nleverage these methods to modify language models such that they deliver\nconsistent and accurate results. We investigate the challenges of using these\nideas to obtain consistent and relevant answers from language models and report\nsome preliminary empirical studies.",
        "pdf_link": "https://arxiv.org/pdf/2312.15472v1.pdf"
    },
    {
        "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
        "authors": [
            "Chen Zhang",
            "Luis Fernando D'Haro",
            "Yiming Chen",
            "Malu Zhang",
            "Haizhou Li"
        ],
        "published": "2023-12-24T04:50:57Z",
        "summary": "Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.",
        "pdf_link": "https://arxiv.org/pdf/2312.15407v2.pdf"
    },
    {
        "title": "Fairness-Aware Structured Pruning in Transformers",
        "authors": [
            "Abdelrahman Zayed",
            "Goncalo Mordido",
            "Samira Shabanian",
            "Ioana Baldini",
            "Sarath Chandar"
        ],
        "published": "2023-12-24T03:57:52Z",
        "summary": "The increasing size of large language models (LLMs) has introduced challenges\nin their training and inference. Removing model components is perceived as a\nsolution to tackle the large model sizes, however, existing pruning methods\nsolely focus on performance, without considering an essential aspect for the\nresponsible use of LLMs: model fairness. It is crucial to address the fairness\nof LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish\ncommunities, among others, as they are being deployed and available to a wide\naudience. In this work, first, we investigate how attention heads impact\nfairness and performance in pre-trained transformer-based language models. We\nthen propose a novel method to prune the attention heads that negatively impact\nfairness while retaining the heads critical for performance, i.e. language\nmodeling capabilities. Our approach is practical in terms of time and\nresources, as it does not require fine-tuning the final pruned, and fairer,\nmodel. Our findings demonstrate a reduction in gender bias by 19%, 19.5%,\n39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different\nsizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased\nmodel, with only a slight decrease in performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.15398v1.pdf"
    },
    {
        "title": "On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications",
        "authors": [
            "Chenjiao Tan",
            "Qian Cao",
            "Yiwei Li",
            "Jielu Zhang",
            "Xiao Yang",
            "Huaqin Zhao",
            "Zihao Wu",
            "Zhengliang Liu",
            "Hao Yang",
            "Nemin Wu",
            "Tao Tang",
            "Xinyue Ye",
            "Lilong Chai",
            "Ninghao Liu",
            "Changying Li",
            "Lan Mu",
            "Tianming Liu",
            "Gengchen Mai"
        ],
        "published": "2023-12-23T22:36:58Z",
        "summary": "The advent of large language models (LLMs) has heightened interest in their\npotential for multimodal applications that integrate language and vision. This\npaper explores the capabilities of GPT-4V in the realms of geography,\nenvironmental science, agriculture, and urban planning by evaluating its\nperformance across a variety of tasks. Data sources comprise satellite imagery,\naerial photos, ground-level images, field images, and public datasets. The\nmodel is evaluated on a series of tasks including geo-localization, textual\ndata extraction from maps, remote sensing image classification, visual question\nanswering, crop type identification, disease/pest/weed recognition, chicken\nbehavior analysis, agricultural object counting, urban planning knowledge\nquestion answering, and plan generation. The results indicate the potential of\nGPT-4V in geo-localization, land cover classification, visual question\nanswering, and basic image understanding. However, there are limitations in\nseveral tasks requiring fine-grained recognition and precise counting. While\nzero-shot learning shows promise, performance varies across problem domains and\nimage complexities. The work provides novel insights into GPT-4V's capabilities\nand limitations for real-world geospatial, environmental, agricultural, and\nurban planning challenges. Further research should focus on augmenting the\nmodel's knowledge and reasoning for specialized domains through expanded\ntraining. Overall, the analysis demonstrates foundational multimodal\nintelligence, highlighting the potential of multimodal foundation models (FMs)\nto advance interdisciplinary applications at the nexus of computer vision and\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2312.17016v1.pdf"
    },
    {
        "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Hongyi Jin",
            "Tianqi Chen",
            "Zhihao Jia"
        ],
        "published": "2023-12-23T11:57:53Z",
        "summary": "In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.",
        "pdf_link": "https://arxiv.org/pdf/2312.15234v1.pdf"
    },
    {
        "title": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs",
        "authors": [
            "Max Zimmer",
            "Megi Andoni",
            "Christoph Spiegel",
            "Sebastian Pokutta"
        ],
        "published": "2023-12-23T11:45:22Z",
        "summary": "Neural Networks can be efficiently compressed through pruning, significantly\nreducing storage and computational demands while maintaining predictive\nperformance. Simple yet effective methods like Iterative Magnitude Pruning\n(IMP, Han et al., 2015) remove less important parameters and require a costly\nretraining procedure to recover performance after pruning. However, with the\nrise of Large Language Models (LLMs), full retraining has become infeasible due\nto memory and compute constraints. In this study, we challenge the practice of\nretraining all parameters by demonstrating that updating only a small subset of\nhighly expressive parameters is often sufficient to recover or even improve\nperformance compared to full retraining. Surprisingly, retraining as little as\n0.27%-0.35% of the parameters of GPT-architectures achieves comparable\nperformance to One Shot IMP across various sparsity levels. Our approach,\nParameter-Efficient Retraining after Pruning (PERP), drastically reduces\ncompute and memory demands, enabling pruning and retraining of up to 30 billion\nparameter models on a single NVIDIA A100 GPU within minutes. Despite magnitude\npruning being considered as unsuited for pruning LLMs, our findings show that\nPERP positions it as a strong contender against state-of-the-art\nretraining-free approaches such as Wanda (Sun et al., 2023) and SparseGPT\n(Frantar & Alistarh, 2023), opening up a promising alternative to avoiding\nretraining.",
        "pdf_link": "https://arxiv.org/pdf/2312.15230v2.pdf"
    },
    {
        "title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering",
        "authors": [
            "Hengrui Gu",
            "Kaixiong Zhou",
            "Xiaotian Han",
            "Ninghao Liu",
            "Ruobing Wang",
            "Xin Wang"
        ],
        "published": "2023-12-23T08:32:13Z",
        "summary": "Multi-hop question answering (MQA) is one of the challenging tasks to\nevaluate machine's comprehension and reasoning abilities, where large language\nmodels (LLMs) have widely achieved the human-comparable performance. Due to the\ndynamics of knowledge facts in real world, knowledge editing has been explored\nto update model with the up-to-date facts while avoiding expensive re-training\nor fine-tuning. Starting from the edited fact, the updated model needs to\nprovide cascading changes in the chain of MQA. The previous art simply adopts a\nmix-up prompt to instruct LLMs conducting multiple reasoning tasks\nsequentially, including question decomposition, answer generation, and conflict\nchecking via comparing with edited facts. However, the coupling of these\nfunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending\nand answering questions while disturbing them with the unskilled task of\nconflict checking. We thus propose a framework, Programmable knowledge editing\nfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,\nwe prompt LLMs to decompose knowledge-augmented multi-hop question, while\ninteracting with a detached trainable scope detector to modulate LLMs behavior\ndepending on external conflict signal. The experiments on three LLM backbones\nand two benchmark datasets validate our superiority in knowledge editing of\nMQA, outperforming all competitors by a large margin in almost all settings and\nconsistently producing reliable reasoning process.",
        "pdf_link": "https://arxiv.org/pdf/2312.15194v2.pdf"
    },
    {
        "title": "ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-order Optimization",
        "authors": [
            "Shuoran Jiang",
            "Qingcai Chen",
            "Youchen Pan",
            "Yang Xiang",
            "Yukang Lin",
            "Xiangping Wu",
            "Chuanyi Liu",
            "Xiaobao Song"
        ],
        "published": "2023-12-23T07:46:31Z",
        "summary": "Lowering the memory requirement in full-parameter training on large models\nhas become a hot research area. MeZO fine-tunes the large language models\n(LLMs) by just forward passes in a zeroth-order SGD optimizer (ZO-SGD),\ndemonstrating excellent performance with the same GPU memory usage as\ninference. However, the simulated perturbation stochastic approximation for\ngradient estimate in MeZO leads to severe oscillations and incurs a substantial\ntime overhead. Moreover, without momentum regularization, MeZO shows severe\nover-fitting problems. Lastly, the perturbation-irrelevant momentum on ZO-SGD\ndoes not improve the convergence rate. This study proposes ZO-AdaMU to resolve\nthe above problems by adapting the simulated perturbation with momentum in its\nstochastic approximation. Unlike existing adaptive momentum methods, we\nrelocate momentum on simulated perturbation in stochastic gradient\napproximation. Our convergence analysis and experiments prove this is a better\nway to improve convergence stability and rate in ZO-SGD. Extensive experiments\ndemonstrate that ZO-AdaMU yields better generalization for LLMs fine-tuning\nacross various NLP tasks than MeZO and its momentum variants.",
        "pdf_link": "https://arxiv.org/pdf/2312.15184v1.pdf"
    },
    {
        "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
        "authors": [
            "Zhen Tan",
            "Tianlong Chen",
            "Zhenyu Zhang",
            "Huan Liu"
        ],
        "published": "2023-12-22T19:55:58Z",
        "summary": "Large Language Models (LLMs) have achieved unprecedented breakthroughs in\nvarious natural language processing domains. However, the enigmatic\n``black-box'' nature of LLMs remains a significant challenge for\ninterpretability, hampering transparent and accountable applications. While\npast approaches, such as attention visualization, pivotal subnetwork\nextraction, and concept-based analyses, offer some insight, they often focus on\neither local or global explanations within a single dimension, occasionally\nfalling short in providing comprehensive clarity. In response, we propose a\nnovel methodology anchored in sparsity-guided techniques, aiming to provide a\nholistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively\nintegrates sparsity to elucidate three intertwined layers of interpretation:\ninput, subnetwork, and concept levels. In addition, the newly introduced\ndimension of interpretable inference-time intervention facilitates dynamic\nadjustments to the model during deployment. Through rigorous empirical\nevaluations on real-world datasets, we demonstrate that SparseCBM delivers a\nprofound understanding of LLM behaviors, setting it apart in both interpreting\nand ameliorating model inaccuracies. Codes are provided in supplements.",
        "pdf_link": "https://arxiv.org/pdf/2312.15033v1.pdf"
    },
    {
        "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
        "authors": [
            "Lizhou Fan",
            "Wenyue Hua",
            "Lingyao Li",
            "Haoyang Ling",
            "Yongfeng Zhang"
        ],
        "published": "2023-12-22T18:07:44Z",
        "summary": "Complex reasoning ability is one of the most important features of current\nLLMs, which has also been leveraged to play an integral role in complex\ndecision-making tasks. Therefore, the investigation into the reasoning\ncapabilities of Large Language Models (LLMs) is critical: numerous benchmarks\nhave been established to assess the reasoning abilities of LLMs. However,\ncurrent benchmarks are inadequate in offering a rigorous evaluation of the full\nextent of reasoning abilities that LLMs are capable of achieving. They are also\nprone to the risk of overfitting, as these benchmarks, being publicly\naccessible and static, allow models to potentially tailor their responses to\nspecific benchmark metrics, thereby inflating their performance. Addressing\nthese limitations, our research introduces a new benchmark, named NPHardEval.\nThis benchmark is designed to evaluate the reasoning abilities of LLMs across a\nbroad spectrum of 900 algorithmic questions, extending up to the NP-Hard\ncomplexity class. These questions are meticulously chosen to represent a wide\nrange of complexity class below the NP-hard complexity class, offering a\nrigorous measure of the reasoning ability of LLMs. Through this study, we shed\nlight on the current state of reasoning in LLMs, providing an objective and\nrigorous perspective through the comparison of LLMs' performance across complex\nclasses. Moreover, this benchmark is designed with a dynamic update mechanism,\nwhere the datapoints are refreshed on a monthly basis. Such regular updates\nplay a crucial role in mitigating the risk of LLMs overfitting to the\nbenchmark, promoting a more accurate and reliable assessment of their reasoning\ncapabilities. The benchmark dataset and code of NPHardEval are available at\nhttps://github.com/casmlab/NPHardEval.",
        "pdf_link": "https://arxiv.org/pdf/2312.14890v4.pdf"
    },
    {
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
        "authors": [
            "Filippos Christianos",
            "Georgios Papoudakis",
            "Matthieu Zimmer",
            "Thomas Coste",
            "Zhihao Wu",
            "Jingxuan Chen",
            "Khyati Khandelwal",
            "James Doran",
            "Xidong Feng",
            "Jiacheng Liu",
            "Zheng Xiong",
            "Yicheng Luo",
            "Jianye Hao",
            "Kun Shao",
            "Haitham Bou-Ammar",
            "Jun Wang"
        ],
        "published": "2023-12-22T17:57:57Z",
        "summary": "A key method for creating Artificial Intelligence (AI) agents is\nReinforcement Learning (RL). However, constructing a standalone RL policy that\nmaps perception to action directly encounters severe problems, chief among them\nbeing its lack of generality across multiple tasks and the need for a large\namount of training data. The leading cause is that it cannot effectively\nintegrate prior information into the perception-action cycle when devising the\npolicy. Large language models (LLMs) emerged as a fundamental way to\nincorporate cross-domain knowledge into AI agents but lack crucial learning and\nadaptation toward specific decision problems. This paper presents a general\nframework model for integrating and learning structured reasoning into AI\nagents' policies. Our methodology is motivated by the modularity found in the\nhuman brain. The framework utilises the construction of intrinsic and extrinsic\nfunctions to add previous understandings of reasoning structures. It also\nprovides the adaptive ability to learn models inside every module or function,\nconsistent with the modular structure of cognitive processes. We describe the\nframework in-depth and compare it with other AI pipelines and existing\nframeworks. The paper explores practical applications, covering experiments\nthat show the effectiveness of our method. Our results indicate that AI agents\nperform and adapt far better when organised reasoning and prior knowledge are\nembedded. This opens the door to more resilient and general AI agent systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.14878v1.pdf"
    },
    {
        "title": "Robust Knowledge Extraction from Large Language Models using Social Choice Theory",
        "authors": [
            "Nico Potyka",
            "Yuqicheng Zhu",
            "Yunjie He",
            "Evgeny Kharlamov",
            "Steffen Staab"
        ],
        "published": "2023-12-22T17:57:29Z",
        "summary": "Large-language models (LLMs) can support a wide range of applications like\nconversational agents, creative writing or general query answering. However,\nthey are ill-suited for query answering in high-stake domains like medicine\nbecause they are typically not robust - even the same query can result in\ndifferent answers when prompted multiple times. In order to improve the\nrobustness of LLM queries, we propose using ranking queries repeatedly and to\naggregate the queries using methods from social choice theory. We study ranking\nqueries in diagnostic settings like medical and fault diagnosis and discuss how\nthe Partial Borda Choice function from the literature can be applied to merge\nmultiple query results. We discuss some additional interesting properties in\nour setting and evaluate the robustness of our approach empirically.",
        "pdf_link": "https://arxiv.org/pdf/2312.14877v2.pdf"
    },
    {
        "title": "Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code",
        "authors": [
            "Shahin Honarvar",
            "Mark van der Wilk",
            "Alastair Donaldson"
        ],
        "published": "2023-12-22T17:29:08Z",
        "summary": "We present a method for systematically evaluating the correctness and\nrobustness of instruction-tuned large language models (LLMs) for code\ngeneration via a new benchmark, Turbulence. Turbulence consists of a large set\nof natural language $\\textit{question templates}$, each of which is a\nprogramming problem, parameterised so that it can be asked in many different\nforms. Each question template has an associated $\\textit{test oracle}$ that\njudges whether a code solution returned by an LLM is correct. Thus, from a\nsingle question template, it is possible to ask an LLM a\n$\\textit{neighbourhood}$ of very similar programming questions, and assess the\ncorrectness of the result returned for each question. This allows gaps in an\nLLM's code generation abilities to be identified, including\n$\\textit{anomalies}$ where the LLM correctly solves $\\textit{almost all}$\nquestions in a neighbourhood but fails for particular parameter instantiations.\nWe present experiments against five LLMs from OpenAI, Cohere and Meta, each at\ntwo temperature configurations. Our findings show that, across the board,\nTurbulence is able to reveal gaps in LLM reasoning ability. This goes beyond\nmerely highlighting that LLMs sometimes produce wrong code (which is no\nsurprise): by systematically identifying cases where LLMs are able to solve\nsome problems in a neighbourhood but do not manage to generalise to solve the\nwhole neighbourhood, our method is effective at highlighting\n$\\textit{robustness}$ issues. We present data and examples that shed light on\nthe kinds of mistakes that LLMs make when they return incorrect code results.",
        "pdf_link": "https://arxiv.org/pdf/2312.14856v2.pdf"
    },
    {
        "title": "Plan, Posture and Go: Towards Open-World Text-to-Motion Generation",
        "authors": [
            "Jinpeng Liu",
            "Wenxun Dai",
            "Chunyu Wang",
            "Yiji Cheng",
            "Yansong Tang",
            "Xin Tong"
        ],
        "published": "2023-12-22T17:02:45Z",
        "summary": "Conventional text-to-motion generation methods are usually trained on limited\ntext-motion pairs, making them hard to generalize to open-world scenarios. Some\nworks use the CLIP model to align the motion space and the text space, aiming\nto enable motion generation from natural language motion descriptions. However,\nthey are still constrained to generate limited and unrealistic in-place\nmotions. To address these issues, we present a divide-and-conquer framework\nnamed PRO-Motion, which consists of three modules as motion planner,\nposture-diffuser and go-diffuser. The motion planner instructs Large Language\nModels (LLMs) to generate a sequence of scripts describing the key postures in\nthe target motion. Differing from natural languages, the scripts can describe\nall possible postures following very simple text templates. This significantly\nreduces the complexity of posture-diffuser, which transforms a script to a\nposture, paving the way for open-world generation. Finally, go-diffuser,\nimplemented as another diffusion model, estimates whole-body translations and\nrotations for all postures, resulting in realistic motions. Experimental\nresults have shown the superiority of our method with other counterparts, and\ndemonstrated its capability of generating diverse and realistic motions from\ncomplex open-world prompts such as \"Experiencing a profound sense of joy\". The\nproject page is available at https://moonsliu.github.io/Pro-Motion.",
        "pdf_link": "https://arxiv.org/pdf/2312.14828v1.pdf"
    },
    {
        "title": "Large Language Model (LLM) Bias Index -- LLMBI",
        "authors": [
            "Abiodun Finbarrs Oketunji",
            "Muhammad Anas",
            "Deepthi Saina"
        ],
        "published": "2023-12-22T15:38:13Z",
        "summary": "The Large Language Model Bias Index (LLMBI) is a pioneering approach designed\nto quantify and address biases inherent in large language models (LLMs), such\nas GPT-4. We recognise the increasing prevalence and impact of LLMs across\ndiverse sectors. This research introduces a novel metric, LLMBI, to\nsystematically measure and mitigate biases potentially skewing model responses.\nWe formulated LLMBI using a composite scoring system incorporating multiple\ndimensions of bias, including but not limited to age, gender, and racial\nbiases. To operationalise this metric, we engaged in a multi-step process\ninvolving collecting and annotating LLM responses, applying sophisticated\nNatural Language Processing (NLP) techniques for bias detection, and computing\nthe LLMBI score through a specially crafted mathematical formula. The formula\nintegrates weighted averages of various bias dimensions, a penalty for dataset\ndiversity deficiencies, and a correction for sentiment biases. Our empirical\nanalysis, conducted using responses from OpenAI's API, employs advanced\nsentiment analysis as a representative method for bias detection. The research\nreveals LLMs, whilst demonstrating impressive capabilities in text generation,\nexhibit varying degrees of bias across different dimensions. LLMBI provides a\nquantifiable measure to compare biases across models and over time, offering a\nvital tool for systems engineers, researchers and regulators in enhancing the\nfairness and reliability of LLMs. It highlights the potential of LLMs in\nmimicking unbiased human-like responses. Additionally, it underscores the\nnecessity of continuously monitoring and recalibrating such models to align\nwith evolving societal norms and ethical standards.",
        "pdf_link": "https://arxiv.org/pdf/2312.14769v3.pdf"
    },
    {
        "title": "Theory of Hallucinations based on Equivariance",
        "authors": [
            "Hisaichi Shibata"
        ],
        "published": "2023-12-22T08:08:45Z",
        "summary": "This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.14504v2.pdf"
    },
    {
        "title": "Empowering Working Memory for Large Language Model Agents",
        "authors": [
            "Jing Guo",
            "Nan Li",
            "Jianchuan Qi",
            "Hang Yang",
            "Ruiqiao Li",
            "Yuzhen Feng",
            "Si Zhang",
            "Ming Xu"
        ],
        "published": "2023-12-22T05:59:00Z",
        "summary": "Large language models (LLMs) have achieved impressive linguistic\ncapabilities. However, a key limitation persists in their lack of human-like\nmemory faculties. LLMs exhibit constrained memory retention across sequential\ninteractions, hindering complex reasoning. This paper explores the potential of\napplying cognitive psychology's working memory frameworks, to enhance LLM\narchitecture. The limitations of traditional LLM memory designs are analyzed,\nincluding their isolation of distinct dialog episodes and lack of persistent\nmemory links. To address this, an innovative model is proposed incorporating a\ncentralized Working Memory Hub and Episodic Buffer access to retain memories\nacross episodes. This architecture aims to provide greater continuity for\nnuanced contextual reasoning during intricate tasks and collaborative\nscenarios. While promising, further research is required into optimizing\nepisodic memory encoding, storage, prioritization, retrieval, and security.\nOverall, this paper provides a strategic blueprint for developing LLM agents\nwith more sophisticated, human-like memory capabilities, highlighting memory\nmechanisms as a vital frontier in artificial general intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2312.17259v1.pdf"
    },
    {
        "title": "Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models",
        "authors": [
            "Priyesh Vakharia",
            "Devavrat Joshi",
            "Meenal Chavan",
            "Dhananjay Sonawane",
            "Bhrigu Garg",
            "Parsa Mazaheri"
        ],
        "published": "2023-12-22T00:31:46Z",
        "summary": "Large Language Models (LLMs) are adept at text manipulation -- tasks such as\nmachine translation and text summarization. However, these models can also be\nprone to hallucination, which can be detrimental to the faithfulness of any\nanswers that the model provides. Recent works in combating hallucinations in\nLLMs deal with identifying hallucinated sentences and categorizing the\ndifferent ways in which models hallucinate. This paper takes a deep dive into\nLLM behavior with respect to hallucinations, defines a token-level approach to\nidentifying different kinds of hallucinations, and further utilizes this\ntoken-level tagging to improve the interpretability and faithfulness of LLMs in\ndialogue summarization tasks. Through this, the paper presents a new, enhanced\ndataset and a new training paradigm.",
        "pdf_link": "https://arxiv.org/pdf/2312.14346v2.pdf"
    },
    {
        "title": "Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs",
        "authors": [
            "Behnam Rahdari",
            "Hao Ding",
            "Ziwei Fan",
            "Yifei Ma",
            "Zhuotong Chen",
            "Anoop Deoras",
            "Branislav Kveton"
        ],
        "published": "2023-12-22T00:30:10Z",
        "summary": "The unique capabilities of Large Language Models (LLMs), such as the natural\nlanguage text generation ability, position them as strong candidates for\nproviding explanation for recommendations. However, despite the size of the\nLLM, most existing models struggle to produce zero-shot explanations reliably.\nTo address this issue, we propose a framework called Logic-Scaffolding, that\ncombines the ideas of aspect-based explanation and chain-of-thought prompting\nto generate explanations through intermediate reasoning steps. In this paper,\nwe share our experience in building the framework and present an interactive\ndemonstration for exploring our results.",
        "pdf_link": "https://arxiv.org/pdf/2312.14345v2.pdf"
    },
    {
        "title": "Context-aware Decoding Reduces Hallucination in Query-focused Summarization",
        "authors": [
            "Zhichao Xu"
        ],
        "published": "2023-12-21T23:42:13Z",
        "summary": "Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",
        "pdf_link": "https://arxiv.org/pdf/2312.14335v2.pdf"
    },
    {
        "title": "Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion",
        "authors": [
            "Katrin Tomanek",
            "Shanqing Cai",
            "Subhashini Venugopalan"
        ],
        "published": "2023-12-21T22:52:44Z",
        "summary": "Abbreviation expansion is a strategy used to speed up communication by\nlimiting the amount of typing and using a language model to suggest expansions.\nHere we look at personalizing a Large Language Model's (LLM) suggestions based\non prior conversations to enhance the relevance of predictions, particularly\nwhen the user data is small (~1000 samples). Specifically, we compare\nfine-tuning, prompt-tuning, and retrieval augmented generation of expanded text\nsuggestions for abbreviated inputs. Our case study with a deployed 8B parameter\nLLM on a real user living with ALS, and experiments on movie character\npersonalization indicates that (1) customization may be necessary in some\nscenarios and prompt-tuning generalizes well to those, (2) fine-tuning on\nin-domain data (with as few as 600 samples) still shows some gains, however (3)\nretrieval augmented few-shot selection also outperforms fine-tuning. (4)\nParameter efficient tuning allows for efficient and scalable personalization.\nFor prompt-tuning, we also find that initializing the learned \"soft-prompts\" to\nuser relevant concept tokens leads to higher accuracy than random\ninitialization.",
        "pdf_link": "https://arxiv.org/pdf/2312.14327v1.pdf"
    },
    {
        "title": "From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models",
        "authors": [
            "Wolfgang Messner",
            "Tatum Greene",
            "Josephine Matalone"
        ],
        "published": "2023-12-21T22:50:14Z",
        "summary": "Large language models (LLMs) are able to engage in natural-sounding\nconversations with humans, showcasing unprecedented capabilities for\ninformation retrieval and automated decision support. They have disrupted\nhuman-technology interaction and the way businesses operate. However,\ntechnologies based on generative artificial intelligence (GenAI) are known to\nhallucinate, misinform, and display biases introduced by the massive datasets\non which they are trained. Existing research indicates that humans may\nunconsciously internalize these biases, which can persist even after they stop\nusing the programs. This study explores the cultural self-perception of LLMs by\nprompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from\nthe GLOBE project. The findings reveal that their cultural self-perception is\nmost closely aligned with the values of English-speaking countries and\ncountries characterized by sustained economic competitiveness. Recognizing the\ncultural biases of LLMs and understanding how they work is crucial for all\nmembers of society because one does not want the black box of artificial\nintelligence to perpetuate bias in humans, who might, in turn, inadvertently\ncreate and train even more biased algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2312.17256v1.pdf"
    },
    {
        "title": "LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding",
        "authors": [
            "Senqiao Yang",
            "Jiaming Liu",
            "Ray Zhang",
            "Mingjie Pan",
            "Zoey Guo",
            "Xiaoqi Li",
            "Zehui Chen",
            "Peng Gao",
            "Yandong Guo",
            "Shanghang Zhang"
        ],
        "published": "2023-12-21T17:52:12Z",
        "summary": "Recently, Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs) have shown promise in instruction following and 2D image understanding.\nWhile these models are powerful, they have not yet been developed to comprehend\nthe more challenging 3D physical scenes, especially when it comes to the sparse\noutdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw\nLiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs\nto gain a comprehensive understanding of outdoor 3D scenes. The central insight\nof our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a\nlanguage modeling problem, encompassing tasks such as 3D captioning, 3D\ngrounding, 3D question answering, etc. Specifically, due to the scarcity of 3D\nLiDAR-text pairing data, we introduce a three-stage training strategy and\ngenerate relevant datasets, progressively aligning the 3D modality with the\nlanguage embedding space of LLM. Furthermore, we design a View-Aware\nTransformer (VAT) to connect the 3D encoder with the LLM, which effectively\nbridges the modality gap and enhances the LLM's spatial orientation\ncomprehension of visual features. Our experiments show that LiDAR-LLM possesses\nfavorable capabilities to comprehend various instructions regarding 3D scenes\nand engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the\n3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\%\nBEV mIoU on the 3D grounding task. Web page:\nhttps://sites.google.com/view/lidar-llm",
        "pdf_link": "https://arxiv.org/pdf/2312.14074v1.pdf"
    },
    {
        "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
        "authors": [
            "Zehui Chen",
            "Weihua Du",
            "Wenwei Zhang",
            "Kuikun Liu",
            "Jiangning Liu",
            "Miao Zheng",
            "Jingming Zhuo",
            "Songyang Zhang",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "published": "2023-12-21T17:02:06Z",
        "summary": "Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2312.14033v3.pdf"
    },
    {
        "title": "AsyncMLD: Asynchronous Multi-LLM Framework for Dialogue Recommendation System",
        "authors": [
            "Naoki Yoshimaru",
            "Motoharu Okuma",
            "Takamasa Iio",
            "Kenji Hatano"
        ],
        "published": "2023-12-21T15:12:59Z",
        "summary": "We have reached a practical and realistic phase in human-support dialogue\nagents by developing a large language model (LLM). However, when requiring\nexpert knowledge or anticipating the utterance content using the massive size\nof the dialogue database, we still need help with the utterance content's\neffectiveness and the efficiency of its output speed, even if using LLM.\nTherefore, we propose a framework that uses LLM asynchronously in the part of\nthe system that returns an appropriate response and in the part that\nunderstands the user's intention and searches the database. In particular,\nnoting that it takes time for the robot to speak, threading related to database\nsearches is performed while the robot is speaking.",
        "pdf_link": "https://arxiv.org/pdf/2312.13925v1.pdf"
    },
    {
        "title": "SimLM: Can Language Models Infer Parameters of Physical Systems?",
        "authors": [
            "Sean Memery",
            "Mirella Lapata",
            "Kartic Subr"
        ],
        "published": "2023-12-21T12:05:19Z",
        "summary": "Several machine learning methods aim to learn or reason about complex\nphysical systems. A common first-step towards reasoning is to infer system\nparameters from observations of its behavior. In this paper, we investigate the\nperformance of Large Language Models (LLMs) at performing parameter inference\nin the context of physical systems. Our experiments suggest that they are not\ninherently suited to this task, even for simple systems. We propose a promising\ndirection of exploration, which involves the use of physical simulators to\naugment the context of LLMs. We assess and compare the performance of different\nLLMs on a simple example with and without access to physical simulation.",
        "pdf_link": "https://arxiv.org/pdf/2312.14215v2.pdf"
    },
    {
        "title": "On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning",
        "authors": [
            "Chengzu Li",
            "Han Zhou",
            "Goran Glava\u0161",
            "Anna Korhonen",
            "Ivan Vuli\u0107"
        ],
        "published": "2023-12-21T11:55:10Z",
        "summary": "Following the standard supervised fine-tuning (SFT) paradigm, in-context\nlearning (ICL) has become an efficient approach propelled by the recent\nadvancements in large language models (LLMs), yielding promising performance\nacross various tasks in few-shot data setups. However, both paradigms are prone\nto suffer from the critical problem of overconfidence (i.e., miscalibration),\nespecially in such limited data setups. In this work, we deliver an in-depth\nanalysis of the behavior across different choices of learning methods from the\nperspective of both performance and calibration, as well as their interplay.\nThrough extensive controlled experiments, we find that simultaneous gains for\nboth task performance and calibration are difficult to achieve, and the problem\nof miscalibration exists across all learning methods in low-resource scenarios.\nTo address this challenging trade-off between performance and calibration, we\nthen investigate the potential of self-ensembling techniques applied at\ndifferent modeling stages (e.g., variations of in-context examples or\nvariations in prompts or different ensembling strategies). We justify the\nfeasibility of self-ensembling on SFT in addition to ICL, to make the\npredictions more calibrated and have comparable or even better performance. Our\nwork sheds light on which learning paradigm to choose and how to enhance both\ntask performance and calibration of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.13772v2.pdf"
    },
    {
        "title": "Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",
        "authors": [
            "Samuel J. Aronson",
            "Kalotina Machini",
            "Jiyeon Shin",
            "Pranav Sriraman",
            "Sean Hamill",
            "Emma R. Henricks",
            "Charlotte Mailly",
            "Angie J. Nottage",
            "Sami S. Amr",
            "Michael Oates",
            "Matthew S. Lebo"
        ],
        "published": "2023-12-21T01:56:00Z",
        "summary": "Background. Large Language Models (LLMs) hold promise for improving genetic\nvariant literature review in clinical testing. We assessed Generative\nPretrained Transformer 4's (GPT-4) performance, nondeterminism, and drift to\ninform its suitability for use in complex clinical processes. Methods. A\n2-prompt process for classification of functional evidence was optimized using\na development set of 45 articles. The prompts asked GPT-4 to supply all\nfunctional data present in an article related to a variant or indicate that no\nfunctional evidence is present. For articles indicated as containing functional\nevidence, a second prompt asked GPT-4 to classify the evidence into pathogenic,\nbenign, or intermediate/inconclusive categories. A final test set of 72\nmanually classified articles was used to test performance. Results. Over a\n2.5-month period (Dec 2023-Feb 2024), we observed substantial differences in\nintraday (nondeterminism) and across day (drift) results, which lessened after\n1/18/24. This variability is seen within and across models in the GPT-4 series,\naffecting different performance statistics to different degrees. Twenty runs\nafter 1/18/24 identified articles containing functional evidence with 92.2%\nsensitivity, 95.6% positive predictive value (PPV) and 86.3% negative\npredictive value (NPV). The second prompt's identified pathogenic functional\nevidence with 90.0% sensitivity, 74.0% PPV and 95.3% NVP and for benign\nevidence with 88.0% sensitivity, 76.6% PPV and 96.9% NVP. Conclusion.\nNondeterminism and drift within LLMs must be assessed and monitored when\nintroducing LLM based functionality into clinical workflows. Failing to do this\nassessment or accounting for these challenges could lead to incorrect or\nmissing information that is critical for patient care. The performance of our\nprompts appears adequate to assist in article prioritization but not in\nautomated decision making.",
        "pdf_link": "https://arxiv.org/pdf/2312.13521v2.pdf"
    },
    {
        "title": "L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs",
        "authors": [
            "Md. Kowsher",
            "Md. Shohanur Islam Sobuj",
            "Asif Mahmud",
            "Nusrat Jahan Prottasha",
            "Prakash Bhat"
        ],
        "published": "2023-12-21T01:47:49Z",
        "summary": "Efficiently fine-tuning Large Language Models (LLMs) for specific tasks\npresents a considerable challenge in natural language processing. Traditional\nmethods, like prompt or prefix tuning, typically rely on arbitrary tokens for\ntraining, leading to prolonged training times and generalized token use across\nvarious class labels. To address these issues, this paper introduces L-Tuning,\nan efficient fine-tuning approach designed for classification tasks within the\nNatural Language Inference (NLI) framework. Diverging from conventional\nmethods, L-Tuning focuses on the fine-tuning of label tokens processed through\na pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This\ntechnique not only improves the fine-tuning accuracy and efficiency but also\nfacilitates the generation of distinct label embeddings for each class,\nenhancing the model's training nuance. Our experimental results indicate a\nsignificant improvement in training efficiency and classification accuracy with\nL-Tuning compared to traditional approaches, marking a promising advancement in\nfine-tuning LLMs for complex language tasks. \\\\ Code is available at:\n\\textcolor{red}{\\href{https://github.com/Kowsher/L-Tuning}{\\texttt{https://github.com/Kowsher/L-Tuning}}}.",
        "pdf_link": "https://arxiv.org/pdf/2402.01643v1.pdf"
    },
    {
        "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
        "authors": [
            "Jingwei Yi",
            "Yueqi Xie",
            "Bin Zhu",
            "Emre Kiciman",
            "Guangzhong Sun",
            "Xing Xie",
            "Fangzhao Wu"
        ],
        "published": "2023-12-21T01:08:39Z",
        "summary": "The integration of large language models (LLMs) with external content has\nenabled more up-to-date and wide-ranging applications of LLMs, such as\nMicrosoft Copilot. However, this integration has also exposed LLMs to the risk\nof indirect prompt injection attacks, where an attacker can embed malicious\ninstructions within external content, compromising LLM output and causing\nresponses to deviate from user expectations. To investigate this important but\nunderexplored issue, we introduce the first benchmark for indirect prompt\ninjection attacks, named BIPIA, to evaluate the risk of such attacks. Based on\nthe evaluation, our work makes a key analysis of the underlying reason for the\nsuccess of the attack, namely the inability of LLMs to distinguish between\ninstructions and external content and the absence of LLMs' awareness to not\nexecute instructions within external content. Building upon this analysis, we\ndevelop two black-box methods based on prompt learning and a white-box defense\nmethod based on fine-tuning with adversarial training accordingly. Experimental\nresults demonstrate that black-box defenses are highly effective in mitigating\nthese attacks, while the white-box defense reduces the attack success rate to\nnear-zero levels. Overall, our work systematically investigates indirect prompt\ninjection attacks by introducing a benchmark, analyzing the underlying reason\nfor the success of the attack, and developing an initial set of defenses.",
        "pdf_link": "https://arxiv.org/pdf/2312.14197v3.pdf"
    },
    {
        "title": "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation",
        "authors": [
            "Difei Gao",
            "Lei Ji",
            "Zechen Bai",
            "Mingyu Ouyang",
            "Peiran Li",
            "Dongxing Mao",
            "Qinchen Wu",
            "Weichen Zhang",
            "Peiyi Wang",
            "Xiangwu Guo",
            "Hengxu Wang",
            "Luowei Zhou",
            "Mike Zheng Shou"
        ],
        "published": "2023-12-20T15:28:38Z",
        "summary": "Graphical User Interface (GUI) automation holds significant promise for\nassisting users with complex tasks, thereby boosting human productivity.\nExisting works leveraging Large Language Model (LLM) or LLM-based AI agents\nhave shown capabilities in automating tasks on Android and Web platforms.\nHowever, these tasks are primarily aimed at simple device usage and\nentertainment operations. This paper presents a novel benchmark, AssistGUI, to\nevaluate whether models are capable of manipulating the mouse and keyboard on\nthe Windows platform in response to user-requested tasks. We carefully\ncollected a set of 100 tasks from nine widely-used software applications, such\nas, After Effects and MS Word, each accompanied by the necessary project files\nfor better evaluation. Moreover, we propose an advanced Actor-Critic Embodied\nAgent framework, which incorporates a sophisticated GUI parser driven by an\nLLM-agent and an enhanced reasoning mechanism adept at handling lengthy\nprocedural tasks. Our experimental results reveal that our GUI Parser and\nReasoning mechanism outshine existing methods in performance. Nevertheless, the\npotential remains substantial, with the best model attaining only a 46% success\nrate on our benchmark. We conclude with a thorough analysis of the current\nmethods' limitations, setting the stage for future breakthroughs in this\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2312.13108v2.pdf"
    },
    {
        "title": "Retrieval-augmented Multilingual Knowledge Editing",
        "authors": [
            "Weixuan Wang",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "published": "2023-12-20T14:08:58Z",
        "summary": "Knowledge represented in Large Language Models (LLMs) is quite often\nincorrect and can also become obsolete over time. Updating knowledge via\nfine-tuning is computationally resource-hungry and not reliable, and so\nknowledge editing (KE) has developed as an effective and economical alternative\nto inject new knowledge or to fix factual errors in LLMs. Although there has\nbeen considerable interest in this area, current KE research exclusively\nfocuses on the monolingual setting, typically in English. However, what happens\nif the new knowledge is supplied in one language, but we would like to query\nthe LLM in a different language? To address the problem of multilingual\nknowledge editing, we propose Retrieval-augmented Multilingual Knowledge Editor\n(ReMaKE) to update new knowledge in LLMs. ReMaKE can perform model-agnostic\nknowledge editing in multilingual settings. ReMaKE concatenates the new\nknowledge retrieved from a multilingual knowledge base with prompts. Our\nexperimental results show that ReMaKE outperforms baseline knowledge editing\nmethods by a significant margin and is the first KE method to work in a\nmultilingual setting. We provide our multilingual knowledge editing dataset\n(MzsRE) in 12 languages, which along with code, and additional project\ninformation is available at https://github.com/Vicky-Wil/ReMaKE.",
        "pdf_link": "https://arxiv.org/pdf/2312.13040v1.pdf"
    },
    {
        "title": "Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors",
        "authors": [
            "Yi-Fan Zhang",
            "Zhang Zhang",
            "Liang Wang",
            "Tieniu Tan",
            "Rong Jin"
        ],
        "published": "2023-12-20T10:53:53Z",
        "summary": "To combat the potential misuse of Natural Language Generation (NLG)\ntechnology, a variety of algorithms have been developed for the detection of\nAI-generated texts. Traditionally, this task is treated as a binary\nclassification problem. Although supervised learning has demonstrated promising\nresults, acquiring labeled data for detection purposes poses real-world\nchallenges and the risk of overfitting. In an effort to address these issues,\nwe delve into the realm of zero-shot machine-generated text detection. Existing\nzero-shot detectors, typically designed for specific tasks or topics, often\nassume uniform testing scenarios, limiting their practicality. In our research,\nwe explore various advanced Large Language Models (LLMs) and their specialized\nvariants, contributing to this field in several ways. In empirical studies, we\nuncover a significant correlation between topics and detection performance.\nSecondly, we delve into the influence of topic shifts on zero-shot detectors.\nThese investigations shed light on the adaptability and robustness of these\ndetection methods across diverse topics. The code is available at\n\\url{https://github.com/yfzhang114/robustness-detection}.",
        "pdf_link": "https://arxiv.org/pdf/2312.12918v2.pdf"
    },
    {
        "title": "Testing the Segment Anything Model on radiology data",
        "authors": [
            "Jos\u00e9 Guilherme de Almeida",
            "Nuno M. Rodrigues",
            "Sara Silva",
            "Nickolas Papanikolaou"
        ],
        "published": "2023-12-20T09:45:21Z",
        "summary": "Deep learning models trained with large amounts of data have become a recent\nand effective approach to predictive problem solving -- these have become known\nas \"foundation models\" as they can be used as fundamental tools for other\napplications. While the paramount examples of image classification (earlier)\nand large language models (more recently) led the way, the Segment Anything\nModel (SAM) was recently proposed and stands as the first foundation model for\nimage segmentation, trained on over 10 million images and with recourse to over\n1 billion masks. However, the question remains -- what are the limits of this\nfoundation? Given that magnetic resonance imaging (MRI) stands as an important\nmethod of diagnosis, we sought to understand whether SAM could be used for a\nfew tasks of zero-shot segmentation using MRI data. Particularly, we wanted to\nknow if selecting masks from the pool of SAM predictions could lead to good\nsegmentations.\n  Here, we provide a critical assessment of the performance of SAM on magnetic\nresonance imaging data. We show that, while acceptable in a very limited set of\ncases, the overall trend implies that these models are insufficient for MRI\nsegmentation across the whole volume, but can provide good segmentations in a\nfew, specific slices. More importantly, we note that while foundation models\ntrained on natural images are set to become key aspects of predictive\nmodelling, they may prove ineffective when used on other imaging modalities.",
        "pdf_link": "https://arxiv.org/pdf/2312.12880v1.pdf"
    },
    {
        "title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models",
        "authors": [
            "Dan Shi",
            "Chaobin You",
            "Jiantao Huang",
            "Taihao Li",
            "Deyi Xiong"
        ],
        "published": "2023-12-20T09:06:18Z",
        "summary": "As an indispensable ingredient of intelligence, commonsense reasoning is\ncrucial for large language models (LLMs) in real-world scenarios. In this\npaper, we propose CORECODE, a dataset that contains abundant commonsense\nknowledge manually annotated on dyadic dialogues, to evaluate the commonsense\nreasoning and commonsense conflict detection capabilities of Chinese LLMs. We\ncategorize commonsense knowledge in everyday conversations into three\ndimensions: entity, event, and social interaction. For easy and consistent\nannotation, we standardize the form of commonsense knowledge annotation in\nopen-domain dialogues as \"domain: slot = value\". A total of 9 domains and 37\nslots are defined to capture diverse commonsense knowledge. With these\npre-defined domains and slots, we collect 76,787 commonsense knowledge\nannotations from 19,700 dialogues through crowdsourcing. To evaluate and\nenhance the commonsense reasoning capability for LLMs on the curated dataset,\nwe establish a series of dialogue-level reasoning and detection tasks,\nincluding commonsense knowledge filling, commonsense knowledge generation,\ncommonsense conflict phrase detection, domain identification, slot\nidentification, and event causal inference. A wide variety of existing\nopen-source Chinese LLMs are evaluated with these tasks on our dataset.\nExperimental results demonstrate that these models are not competent to predict\nCORECODE's plentiful reasoning content, and even ChatGPT could only achieve\n0.275 and 0.084 accuracy on the domain identification and slot identification\ntasks under the zero-shot setting. We release the data and codes of CORECODE at\nhttps://github.com/danshi777/CORECODE to promote commonsense reasoning\nevaluation and study of LLMs in the context of daily conversations.",
        "pdf_link": "https://arxiv.org/pdf/2312.12853v1.pdf"
    },
    {
        "title": "Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data",
        "authors": [
            "Yiwei Li",
            "Peiwen Yuan",
            "Shaoxiong Feng",
            "Boyuan Pan",
            "Bin Sun",
            "Xinglin Wang",
            "Heda Wang",
            "Kan Li"
        ],
        "published": "2023-12-20T08:28:36Z",
        "summary": "Large Language Models (LLMs) have performed well on various reasoning tasks,\nbut their inaccessibility and numerous parameters hinder wide application in\npractice. One promising way is distilling the reasoning ability from LLMs to\nsmall models by the generated chain-of-thought reasoning paths. In some cases,\nhowever, LLMs may produce incorrect reasoning chains, especially when facing\ncomplex mathematical problems. Previous studies only transfer knowledge from\npositive samples and drop the synthesized data with wrong answers. In this\nwork, we illustrate the merit of negative data and propose a model\nspecialization framework to distill LLMs with negative samples besides positive\nones. The framework consists of three progressive steps, covering from training\nto inference stages, to absorb knowledge from negative data. We conduct\nextensive experiments across arithmetic reasoning tasks to demonstrate the role\nof negative data in distillation from LLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.12832v1.pdf"
    },
    {
        "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
        "authors": [
            "Edmund Mills",
            "Shiye Su",
            "Stuart Russell",
            "Scott Emmons"
        ],
        "published": "2023-12-20T03:44:18Z",
        "summary": "How do we measure the efficacy of language model explainability methods?\nWhile many explainability methods have been developed, they are typically\nevaluated on bespoke tasks, preventing an apples-to-apples comparison. To help\nfill this gap, we present ALMANACS, a language model explainability benchmark.\nALMANACS scores explainability methods on simulatability, i.e., how well the\nexplanations improve behavior prediction on new inputs. The ALMANACS scenarios\nspan twelve safety-relevant topics such as ethical reasoning and advanced AI\nbehaviors; they have idiosyncratic premises to invoke model-specific behavior;\nand they have a train-test distributional shift to encourage faithful\nexplanations. By using another language model to predict behavior based on the\nexplanations, ALMANACS is a fully automated benchmark. We use ALMANACS to\nevaluate counterfactuals, rationalizations, attention, and Integrated Gradients\nexplanations. Our results are sobering: when averaged across all topics, no\nexplanation method outperforms the explanation-free control. We conclude that\ndespite modest successes in prior work, developing an explanation method that\naids simulatability in ALMANACS remains an open challenge.",
        "pdf_link": "https://arxiv.org/pdf/2312.12747v1.pdf"
    },
    {
        "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
        "authors": [
            "Jiachen Zhao",
            "Zhun Deng",
            "David Madras",
            "James Zou",
            "Mengye Ren"
        ],
        "published": "2023-12-20T03:18:50Z",
        "summary": "As the number of large language models (LLMs) released to the public grows,\nthere is a pressing need to understand the safety implications associated with\nthese models learning from third-party custom finetuning data. We explore the\nbehavior of LLMs finetuned on noisy custom data containing unsafe content,\nrepresented by datasets that contain biases, toxicity, and harmfulness, finding\nthat while aligned LLMs can readily learn this unsafe content, they also tend\nto forget it more significantly than other examples when subsequently finetuned\non safer content. Drawing inspiration from the discrepancies in forgetting, we\nintroduce the \"ForgetFilter\" algorithm, which filters unsafe data based on how\nstrong the model's forgetting signal is for that data. We demonstrate that the\nForgetFilter algorithm ensures safety in customized finetuning without\ncompromising downstream task performance, unlike sequential safety finetuning.\nForgetFilter outperforms alternative strategies like replay and moral\nself-correction in curbing LLMs' ability to assimilate unsafe content during\ncustom finetuning, e.g. 75% lower than not applying any safety measures and 62%\nlower than using self-correction in toxicity score.",
        "pdf_link": "https://arxiv.org/pdf/2312.12736v1.pdf"
    },
    {
        "title": "A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges",
        "authors": [
            "Roberto Francisco de Lima Junior",
            "Luiz Fernando Paes de Barros Presta",
            "Lucca Santos Borborema",
            "Vanderson Nogueira da Silva",
            "Marcio Leal de Melo Dahia",
            "Anderson Carlos Sousa e Santos"
        ],
        "published": "2023-12-19T20:59:02Z",
        "summary": "This paper presents a detailed case study examining the application of Large\nLanguage Models (LLMs) in the construction of test cases within the context of\nsoftware engineering. LLMs, characterized by their advanced natural language\nprocessing capabilities, are increasingly garnering attention as tools to\nautomate and enhance various aspects of the software development life cycle.\nLeveraging a case study methodology, we systematically explore the integration\nof LLMs in the test case construction process, aiming to shed light on their\npractical efficacy, challenges encountered, and implications for software\nquality assurance. The study encompasses the selection of a representative\nsoftware application, the formulation of test case construction methodologies\nemploying LLMs, and the subsequent evaluation of outcomes. Through a blend of\nqualitative and quantitative analyses, this study assesses the impact of LLMs\non test case comprehensiveness, accuracy, and efficiency. Additionally, delves\ninto challenges such as model interpretability and adaptation to diverse\nsoftware contexts. The findings from this case study contributes with nuanced\ninsights into the practical utility of LLMs in the domain of test case\nconstruction, elucidating their potential benefits and limitations. By\naddressing real-world scenarios and complexities, this research aims to inform\nsoftware practitioners and researchers alike about the tangible implications of\nincorporating LLMs into the software testing landscape, fostering a more\ncomprehensive understanding of their role in optimizing the software\ndevelopment process.",
        "pdf_link": "https://arxiv.org/pdf/2312.12598v2.pdf"
    },
    {
        "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
        "authors": [
            "Jason Vega",
            "Isha Chaudhary",
            "Changming Xu",
            "Gagandeep Singh"
        ],
        "published": "2023-12-19T16:47:12Z",
        "summary": "With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we show that SOTA open-source LLMs are\nvulnerable to simple, optimization-free attacks we refer to as $\\textit{priming\nattacks}$, which are easy to execute and effectively bypass alignment from\nsafety training. Our proposed attack improves the Attack Success Rate on\nHarmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to\nbaselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks .",
        "pdf_link": "https://arxiv.org/pdf/2312.12321v1.pdf"
    },
    {
        "title": "On Early Detection of Hallucinations in Factual Question Answering",
        "authors": [
            "Ben Snyder",
            "Marius Moisescu",
            "Muhammad Bilal Zafar"
        ],
        "published": "2023-12-19T14:35:04Z",
        "summary": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks like search and summarization, hallucinations\nremain a major impediment towards gaining user trust. The fluency and coherence\nof model generations even when hallucinating makes it difficult to detect\nwhether or not a model is hallucinating. In this work, we explore if the\nartifacts associated with the model generations can provide hints that the\ngeneration will contain hallucinations. Specifically, we probe LLMs at 1) the\ninputs via Integrated Gradients based token attribution, 2) the outputs via the\nSoftmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.\nWe further show that tokens preceding a hallucination can predict the\nsubsequent hallucination before it occurs.",
        "pdf_link": "https://arxiv.org/pdf/2312.14183v2.pdf"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment",
        "authors": [
            "Lingling Xu",
            "Haoran Xie",
            "Si-Zhao Joe Qin",
            "Xiaohui Tao",
            "Fu Lee Wang"
        ],
        "published": "2023-12-19T13:31:24Z",
        "summary": "With the continuous growth in the number of parameters of transformer-based\npretrained language models (PLMs), particularly the emergence of large language\nmodels (LLMs) with billions of parameters, many natural language processing\n(NLP) tasks have demonstrated remarkable success. However, the enormous size\nand computational demands of these models pose significant challenges for\nadapting them to specific downstream tasks, especially in environments with\nlimited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers\nan effective solution by reducing the number of fine-tuning parameters and\nmemory usage while achieving comparable performance to full fine-tuning. The\ndemands for fine-tuning PLMs, especially LLMs, have led to a surge in the\ndevelopment of PEFT methods, as depicted in Fig. 1. In this paper, we present a\ncomprehensive and systematic review of PEFT methods for PLMs. We summarize\nthese PEFT methods, discuss their applications, and outline future directions.\nFurthermore, we conduct experiments using several representative PEFT methods\nto better understand their effectiveness in parameter efficiency and memory\nefficiency. By offering insights into the latest advancements and practical\napplications, this survey serves as an invaluable resource for researchers and\npractitioners seeking to navigate the challenges and opportunities presented by\nPEFT in the context of PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.12148v1.pdf"
    },
    {
        "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes",
        "authors": [
            "Nabeel Seedat",
            "Nicolas Huynh",
            "Boris van Breugel",
            "Mihaela van der Schaar"
        ],
        "published": "2023-12-19T12:34:46Z",
        "summary": "Machine Learning (ML) in low-data settings remains an underappreciated yet\ncrucial problem. Hence, data augmentation methods to increase the sample size\nof datasets needed for ML are key to unlocking the transformative potential of\nML in data-deprived regions and domains. Unfortunately, the limited training\nset constrains traditional tabular synthetic data generators in their ability\nto generate a large and diverse augmented dataset needed for ML tasks. To\naddress this challenge, we introduce CLLM, which leverages the prior knowledge\nof Large Language Models (LLMs) for data augmentation in the low-data regime.\nHowever, not all the data generated by LLMs will improve downstream utility, as\nfor any generative model. Consequently, we introduce a principled curation\nmechanism, leveraging learning dynamics, coupled with confidence and\nuncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple\nreal-world datasets, we demonstrate the superior performance of CLLM in the\nlow-data regime compared to conventional generators. Additionally, we provide\ninsights into the LLM generation and curation mechanism, shedding light on the\nfeatures that enable them to output high-quality augmented datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.12112v2.pdf"
    },
    {
        "title": "A Performance Evaluation of a Quantized Large Language Model on Various Smartphones",
        "authors": [
            "Tolga \u00c7\u00f6pl\u00fc",
            "Marc Loedi",
            "Arto Bendiken",
            "Mykhailo Makohin",
            "Joshua J. Bouw",
            "Stephen Cobb"
        ],
        "published": "2023-12-19T10:19:39Z",
        "summary": "This paper explores the feasibility and performance of on-device large\nlanguage model (LLM) inference on various Apple iPhone models. Amidst the rapid\nevolution of generative AI, on-device LLMs offer solutions to privacy,\nsecurity, and connectivity challenges inherent in cloud-based models.\nLeveraging existing literature on running multi-billion parameter LLMs on\nresource-limited devices, our study examines the thermal effects and\ninteraction speeds of a high-performing LLM across different smartphone\ngenerations. We present real-world performance results, providing insights into\non-device inference capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2312.12472v1.pdf"
    },
    {
        "title": "Climate Change from Large Language Models",
        "authors": [
            "Hongyin Zhu",
            "Prayag Tiwari"
        ],
        "published": "2023-12-19T09:26:46Z",
        "summary": "Climate change presents significant challenges to the global community, and\nit is imperative to raise widespread awareness of the climate crisis and\neducate users about low-carbon living. Artificial intelligence, particularly\nlarge language models (LLMs), have emerged as powerful tools in mitigating the\nclimate crisis, leveraging their extensive knowledge, broad user base, and\nnatural language interaction capabilities. However, despite the growing body of\nresearch on climate change, there is a lack of comprehensive assessments of\nclimate crisis knowledge within LLMs. This paper aims to resolve this gap by\nproposing an automatic evaluation framework. We employ a hybrid approach to\ndata acquisition that combines data synthesis and manual collection to compile\na diverse set of questions related to the climate crisis. These questions cover\nvarious aspects of climate change, including its causes, impacts, mitigation\nstrategies, and adaptation measures. We then evaluate the model knowledge\nthrough prompt engineering based on the collected questions and generated\nanswers. We propose a set of comprehensive metrics to evaluate the climate\ncrisis knowledge, incorporating indicators from 10 different perspectives.\nExperimental results show that our method is effective in evaluating the\nknowledge of LLMs regarding the climate crisis. We evaluate several\nstate-of-the-art LLMs and find that their knowledge falls short in terms of\ntimeliness.",
        "pdf_link": "https://arxiv.org/pdf/2312.11985v2.pdf"
    },
    {
        "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
        "authors": [
            "Chen Gao",
            "Xiaochong Lan",
            "Nian Li",
            "Yuan Yuan",
            "Jingtao Ding",
            "Zhilun Zhou",
            "Fengli Xu",
            "Yong Li"
        ],
        "published": "2023-12-19T09:06:45Z",
        "summary": "Agent-based modeling and simulation has evolved as a powerful tool for\nmodeling complex systems, offering insights into emergent behaviors and\ninteractions among diverse agents. Integrating large language models into\nagent-based modeling and simulation presents a promising avenue for enhancing\nsimulation capabilities. This paper surveys the landscape of utilizing large\nlanguage models in agent-based modeling and simulation, examining their\nchallenges and promising future directions. In this survey, since this is an\ninterdisciplinary field, we first introduce the background of agent-based\nmodeling and simulation and large language model-empowered agents. We then\ndiscuss the motivation for applying large language models to agent-based\nsimulation and systematically analyze the challenges in environment perception,\nhuman alignment, action generation, and evaluation. Most importantly, we\nprovide a comprehensive overview of the recent works of large language\nmodel-empowered agent-based modeling and simulation in multiple scenarios,\nwhich can be divided into four domains: cyber, physical, social, and hybrid,\ncovering simulation of both real-world and virtual environments. Finally, since\nthis area is new and quickly evolving, we discuss the open problems and\npromising future directions.",
        "pdf_link": "https://arxiv.org/pdf/2312.11970v1.pdf"
    },
    {
        "title": "Text-Conditioned Resampler For Long Form Video Understanding",
        "authors": [
            "Bruno Korbar",
            "Yongqin Xian",
            "Alessio Tonioni",
            "Andrew Zisserman",
            "Federico Tombari"
        ],
        "published": "2023-12-19T06:42:47Z",
        "summary": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.",
        "pdf_link": "https://arxiv.org/pdf/2312.11897v2.pdf"
    },
    {
        "title": "Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models",
        "authors": [
            "Soaad Hossain",
            "Syed Ishtiaque Ahmed"
        ],
        "published": "2023-12-19T06:28:43Z",
        "summary": "Given the success of ChatGPT, LaMDA and other large language models (LLMs),\nthere has been an increase in development and usage of LLMs within the\ntechnology sector and other sectors. While the level in which LLMs has not\nreached a level where it has surpassed human intelligence, there will be a time\nwhen it will. Such LLMs can be referred to as advanced LLMs. Currently, there\nare limited usage of ethical artificial intelligence (AI) principles and\nguidelines addressing advanced LLMs due to the fact that we have not reached\nthat point yet. However, this is a problem as once we do reach that point, we\nwill not be adequately prepared to deal with the aftermath of it in an ethical\nand optimal way, which will lead to undesired and unexpected consequences. This\npaper addresses this issue by discussing what ethical AI principles and\nguidelines can be used to address highly advanced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.10745v1.pdf"
    },
    {
        "title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction",
        "authors": [
            "Unggi Lee",
            "Sungjun Yoon",
            "Joon Seo Yun",
            "Kyoungsoo Park",
            "YoungHoon Jung",
            "Damji Stratton",
            "Hyeoncheol Kim"
        ],
        "published": "2023-12-19T06:26:25Z",
        "summary": "This paper presents novel techniques for enhancing the performance of\nknowledge tracing (KT) models by focusing on the crucial factor of question and\nconcept difficulty level. Despite the acknowledged significance of difficulty,\nprevious KT research has yet to exploit its potential for model optimization\nand has struggled to predict difficulty from unseen data. To address these\nproblems, we propose a difficulty-centered contrastive learning method for KT\nmodels and a Large Language Model (LLM)-based framework for difficulty\nprediction. These innovative methods seek to improve the performance of KT\nmodels and provide accurate difficulty estimates for unseen data. Our ablation\nstudy demonstrates the efficacy of these techniques by demonstrating enhanced\nKT model performance. Nonetheless, the complex relationship between language\nand difficulty merits further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2312.11890v1.pdf"
    },
    {
        "title": "Efficient LLM inference solution on Intel GPU",
        "authors": [
            "Hui Wu",
            "Yi Gan",
            "Feng Yuan",
            "Jing Ma",
            "Wei Zhu",
            "Yutao Xu",
            "Hong Zhu",
            "Yuhua Zhu",
            "Xiaoli Liu",
            "Jinghui Gu"
        ],
        "published": "2023-12-19T05:40:43Z",
        "summary": "Transformer based Large Language Models (LLMs) have been widely used in many\nfields, and the efficiency of LLM inference becomes hot topic in real\napplications. However, LLMs are usually complicatedly designed in model\nstructure with massive operations and perform inference in the auto-regressive\nmode, making it a challenging task to design a system with high efficiency.\n  In this paper, we propose an efficient LLM inference solution with low\nlatency and high throughput. Firstly, we simplify the LLM decoder layer by\nfusing data movement and element-wise operations to reduce the memory access\nfrequency and lower system latency. We also propose a segment KV cache policy\nto keep key/value of the request and response tokens in separate physical\nmemory for effective device memory management, helping enlarge the runtime\nbatch size and improve system throughput. A customized\nScaled-Dot-Product-Attention kernel is designed to match our fusion policy\nbased on the segment KV cache solution. We implement our LLM inference solution\non Intel GPU and publish it publicly. Compared with the standard HuggingFace\nimplementation, the proposed solution achieves up to 7x lower token latency and\n27x higher throughput for some popular LLMs on Intel GPU.",
        "pdf_link": "https://arxiv.org/pdf/2401.05391v1.pdf"
    },
    {
        "title": "Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies",
        "authors": [
            "Anaelia Ovalle",
            "Ninareh Mehrabi",
            "Palash Goyal",
            "Jwala Dhamala",
            "Kai-Wei Chang",
            "Richard Zemel",
            "Aram Galstyan",
            "Yuval Pinter",
            "Rahul Gupta"
        ],
        "published": "2023-12-19T01:28:46Z",
        "summary": "Gender-inclusive NLP research has documented the harmful limitations of\ngender binary-centric large language models (LLM), such as the inability to\ncorrectly use gender-diverse English neopronouns (e.g., xe, zir, fae). While\ndata scarcity is a known culprit, the precise mechanisms through which scarcity\naffects this behavior remain underexplored. We discover LLM misgendering is\nsignificantly influenced by Byte-Pair Encoding (BPE) tokenization, the\ntokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments\nneopronouns, a direct consequence of data scarcity during tokenizer training.\nThis disparate tokenization mirrors tokenizer limitations observed in\nmultilingual and low-resource NLP, unlocking new misgendering mitigation\nstrategies. We propose two techniques: (1) pronoun tokenization parity, a\nmethod to enforce consistent tokenization across gendered pronouns, and (2)\nutilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency.\nOur proposed methods outperform finetuning with standard BPE, improving\nneopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM\nmisgendering to tokenization and deficient neopronoun grammar, indicating that\nLLMs unable to correctly treat neopronouns as pronouns are more prone to\nmisgender.",
        "pdf_link": "https://arxiv.org/pdf/2312.11779v3.pdf"
    },
    {
        "title": "Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled Spatial Ontologies",
        "authors": [
            "Jared Strader",
            "Nathan Hughes",
            "William Chen",
            "Alberto Speranzon",
            "Luca Carlone"
        ],
        "published": "2023-12-18T21:20:28Z",
        "summary": "This paper proposes an approach to build 3D scene graphs in arbitrary (indoor\nand outdoor) environments. Such extension is challenging; the hierarchy of\nconcepts that describe an outdoor environment is more complex than for indoors,\nand manually defining such hierarchy is time-consuming and does not scale.\nFurthermore, the lack of training data prevents the straightforward application\nof learning-based tools used in indoor settings. To address these challenges,\nwe propose two novel extensions. First, we develop methods to build a spatial\nontology defining concepts and relations relevant for indoor and outdoor robot\noperation. In particular, we use a Large Language Model (LLM) to build such an\nontology, thus largely reducing the amount of manual effort required. Second,\nwe leverage the spatial ontology for 3D scene graph construction using Logic\nTensor Networks (LTN) to add logical rules, or axioms (e.g., \"a beach contains\nsand\"), which provide additional supervisory signals at training time thus\nreducing the need for labelled data, providing better predictions, and even\nallowing predicting concepts unseen at training time. We test our approach in a\nvariety of datasets, including indoor, rural, and coastal environments, and\nshow that it leads to a significant increase in the quality of the 3D scene\ngraph generation with sparsely annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2312.11713v1.pdf"
    },
    {
        "title": "Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview",
        "authors": [
            "Liang Zhang",
            "Zhelun Chen"
        ],
        "published": "2023-12-18T20:58:58Z",
        "summary": "In recent years, the rapid advancement and impressive capabilities of Large\nLanguage Models (LLMs) have been evident across various domains. This paper\nexplores the application, implications, and potential of LLMs in building\nenergy efficiency and decarbonization studies. The wide-ranging capabilities of\nLLMs are examined in the context of the building energy field, including\nintelligent control systems, code generation, data infrastructure, knowledge\nextraction, and education. Despite the promising potential of LLMs, challenges\nincluding complex and expensive computation, data privacy, security and\ncopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.\nThe paper concludes with a call for future research focused on the enhancement\nof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research\nbetween AI and energy experts.",
        "pdf_link": "https://arxiv.org/pdf/2312.11701v1.pdf"
    },
    {
        "title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
        "authors": [
            "Megan Kinniment",
            "Lucas Jun Koba Sato",
            "Haoxing Du",
            "Brian Goodrich",
            "Max Hasin",
            "Lawrence Chan",
            "Luke Harold Miles",
            "Tao R. Lin",
            "Hjalmar Wijk",
            "Joel Burget",
            "Aaron Ho",
            "Elizabeth Barnes",
            "Paul Christiano"
        ],
        "published": "2023-12-18T19:27:09Z",
        "summary": "In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.",
        "pdf_link": "https://arxiv.org/pdf/2312.11671v2.pdf"
    },
    {
        "title": "Traces of Memorisation in Large Language Models for Code",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi",
            "Arie van Deursen"
        ],
        "published": "2023-12-18T19:12:58Z",
        "summary": "Large language models have gained significant popularity because of their\nability to generate human-like text and potential applications in various\nfields, such as Software Engineering. Large language models for code are\ncommonly trained on large unsanitised corpora of source code scraped from the\ninternet. The content of these datasets is memorised and can be extracted by\nattackers with data extraction attacks. In this work, we explore memorisation\nin large language models for code and compare the rate of memorisation with\nlarge language models trained on natural language. We adopt an existing\nbenchmark for natural language and construct a benchmark for code by\nidentifying samples that are vulnerable to attack. We run both benchmarks\nagainst a variety of models, and perform a data extraction attack. We find that\nlarge language models for code are vulnerable to data extraction attacks, like\ntheir natural language counterparts. From the training data that was identified\nto be potentially extractable we were able to extract 47% from a\nCodeGen-Mono-16B code completion model. We also observe that models memorise\nmore, as their parameter count grows, and that their pre-training data are also\nvulnerable to attack. We also find that data carriers are memorised at a higher\nrate than regular code or documentation and that different model architectures\nmemorise different samples. Data leakage has severe outcomes, so we urge the\nresearch community to further investigate the extent of this phenomenon using a\nwider range of models and extraction techniques in order to build safeguards to\nmitigate this issue.",
        "pdf_link": "https://arxiv.org/pdf/2312.11658v2.pdf"
    },
    {
        "title": "Language-Assisted 3D Scene Understanding",
        "authors": [
            "Yanmin Wu",
            "Qiankun Gao",
            "Renrui Zhang",
            "Jian Zhang"
        ],
        "published": "2023-12-18T18:54:56Z",
        "summary": "The scale and quality of point cloud datasets constrain the advancement of\npoint cloud learning. Recently, with the development of multi-modal learning,\nthe incorporation of domain-agnostic prior knowledge from other modalities,\nsuch as images and text, to assist in point cloud feature learning has been\nconsidered a promising avenue. Existing methods have demonstrated the\neffectiveness of multi-modal contrastive training and feature distillation on\npoint clouds. However, challenges remain, including the requirement for paired\ntriplet data, redundancy and ambiguity in supervised features, and the\ndisruption of the original priors. In this paper, we propose a\nlanguage-assisted approach to point cloud feature learning (LAST-PCL),\nenriching semantic concepts through LLMs-based text enrichment. We achieve\nde-redundancy and feature dimensionality reduction without compromising textual\npriors by statistical-based and training-free significant feature selection.\nFurthermore, we also delve into an in-depth analysis of the impact of text\ncontrastive training on the point cloud. Extensive experiments validate that\nthe proposed method learns semantically meaningful point cloud features and\nachieves state-of-the-art or comparable performance in 3D semantic\nsegmentation, 3D object detection, and 3D scene classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.11451v2.pdf"
    },
    {
        "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
        "authors": [
            "Jiahui Gao",
            "Renjie Pi",
            "Jipeng Zhang",
            "Jiacheng Ye",
            "Wanjun Zhong",
            "Yufei Wang",
            "Lanqing Hong",
            "Jianhua Han",
            "Hang Xu",
            "Zhenguo Li",
            "Lingpeng Kong"
        ],
        "published": "2023-12-18T17:36:20Z",
        "summary": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.",
        "pdf_link": "https://arxiv.org/pdf/2312.11370v1.pdf"
    },
    {
        "title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation",
        "authors": [
            "Nandan Thakur",
            "Luiz Bonifacio",
            "Xinyu Zhang",
            "Odunayo Ogundepo",
            "Ehsan Kamalloo",
            "David Alfonso-Hermelo",
            "Xiaoguang Li",
            "Qun Liu",
            "Boxing Chen",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023-12-18T17:18:04Z",
        "summary": "Retrieval-augmented generation (RAG) grounds large language model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior works lack a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages judged as\nnon-relevant, whereas queries in the relevant subset include at least a single\njudged relevant passage. We measure LLM robustness using two metrics: (i)\nhallucination rate, measuring model tendency to hallucinate an answer, when the\nanswer is not present in passages in the non-relevant subset, and (ii) error\nrate, measuring model inaccuracy to recognize relevant passages in the relevant\nsubset. In our work, we measure robustness for a wide variety of\nmultilingual-focused LLMs and observe that most of the models struggle to\nbalance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observe\nmore than an 88% hallucination rate on the non-relevant subset, whereas,\nMistral overall hallucinates less, but can achieve up to a 74.9% error rate on\nthe relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on\nboth subsets, highlighting future work necessary to improve LLM robustness.",
        "pdf_link": "https://arxiv.org/pdf/2312.11361v2.pdf"
    },
    {
        "title": "Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs",
        "authors": [
            "Yuxuan Huang",
            "Lida Shi",
            "Anqi Liu",
            "Hao Xu"
        ],
        "published": "2023-12-18T15:23:06Z",
        "summary": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access.",
        "pdf_link": "https://arxiv.org/pdf/2312.11282v2.pdf"
    },
    {
        "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
        "authors": [
            "Bing Wang",
            "Changyu Ren",
            "Jian Yang",
            "Xinnian Liang",
            "Jiaqi Bai",
            "Linzheng Chai",
            "Zhao Yan",
            "Qian-Wen Zhang",
            "Di Yin",
            "Xing Sun",
            "Zhoujun Li"
        ],
        "published": "2023-12-18T14:40:20Z",
        "summary": "Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on ``huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https://github.com/wbbeyourself/MAC-SQL).",
        "pdf_link": "https://arxiv.org/pdf/2312.11242v3.pdf"
    },
    {
        "title": "Linear Attention via Orthogonal Memory",
        "authors": [
            "Jun Zhang",
            "Shuyang Jiang",
            "Jiangtao Feng",
            "Lin Zheng",
            "Lingpeng Kong"
        ],
        "published": "2023-12-18T12:26:27Z",
        "summary": "Efficient attentions have greatly improved the computational efficiency of\nTransformers. However, most existing linear attention mechanisms suffer from an\n\\emph{efficiency degradation} problem, leading to inefficiencies in causal\nlanguage modeling and hindering their application in long-range language\nmodels. This problem is more pronounced under language modeling with unbounded\ncontexts. In this paper, we propose \\textbf{L}inear \\textbf{A}ttention\n\\textbf{V}ia \\textbf{O}rthogonal memory~(\\shortname) to address these\nlimitations, achieving strong performance while maintaining linear complexity.\n\\shortname employs orthogonal decomposition to compress a context into a\nfixed-size orthogonal memory while effectively minimizing redundancy within the\ncontext. Given that orthogonal memory compresses global information, we further\ndissect the context to amplify fine-grained local information. Additionally, we\nembed the relative position encoding into \\shortname to improve the\nextrapolation ability. Experimental results show that \\shortname greatly\nimproves the efficiency of the causal language model with the best\nextrapolation performance and outperforms other efficient baselines. Further,\nwe endeavor to employ \\shortname for unbounded language modeling and\nsuccessfully scale the context length to 128K.",
        "pdf_link": "https://arxiv.org/pdf/2312.11135v1.pdf"
    },
    {
        "title": "Split and Rephrase with Large Language Models",
        "authors": [
            "David Ponce",
            "Thierry Etchegoyhen",
            "Jes\u00fas Calleja P\u00e9rez",
            "Harritxu Gete"
        ],
        "published": "2023-12-18T10:16:37Z",
        "summary": "The Split and Rephrase (SPRP) task, which consists in splitting complex\nsentences into a sequence of shorter grammatical sentences, while preserving\nthe original meaning, can facilitate the processing of complex texts for humans\nand machines alike. It is also a valuable testbed to evaluate natural language\nprocessing models, as it requires modelling complex grammatical aspects. In\nthis work, we evaluate large language models on the task, showing that they can\nprovide large improvements over the state of the art on the main metrics,\nalthough still lagging in terms of splitting compliance. Results from two human\nevaluations further support the conclusions drawn from automated metric\nresults. We provide a comprehensive study that includes prompting variants,\ndomain shift, fine-tuned pretrained language models of varying parameter size\nand training data volumes, contrasted with both zero-shot and few-shot\napproaches on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they may constitute a reasonable\noff-the-shelf alternative. Our results provide a fine-grained analysis of the\npotential and limitations of large language models for SPRP, with significant\nimprovements achievable using relatively small amounts of training data and\nmodel parameters overall, and remaining limitations for all models on the task.",
        "pdf_link": "https://arxiv.org/pdf/2312.11075v3.pdf"
    },
    {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published": "2023-12-18T07:47:33Z",
        "summary": "Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.",
        "pdf_link": "https://arxiv.org/pdf/2312.10997v5.pdf"
    },
    {
        "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update",
        "authors": [
            "Zhi Gao",
            "Yuntao Du",
            "Xintong Zhang",
            "Xiaojian Ma",
            "Wenjuan Han",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "published": "2023-12-18T03:34:07Z",
        "summary": "Utilizing large language models (LLMs) to compose off-the-shelf visual tools\nrepresents a promising avenue of research for developing robust visual\nassistants capable of addressing diverse visual tasks. However, these methods\noften overlook the potential for continual learning, typically by freezing the\nutilized tools, thus limiting their adaptation to environments requiring new\nknowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual\nAssistant, which operates within a framework encompassing inference,\nreflection, and learning phases. During the inference phase, LLMs generate\nprograms and execute corresponding tools to complete assigned tasks. In the\nreflection phase, a multimodal global-local reflection scheme analyzes human\nfeedback to determine which tools require updating. Lastly, the learning phase\nemploys three flexible approaches to automatically gather training data and\nintroduces a novel prompt tuning scheme to update the tools, allowing CLOVA to\nefficiently acquire new knowledge. Experimental findings demonstrate that CLOVA\nsurpasses existing tool-usage methods by 5% in visual question answering and\nmultiple-image reasoning, by 10% in knowledge tagging, and by 20% in image\nediting. These results underscore the significance of the continual learning\ncapability in general visual assistants.",
        "pdf_link": "https://arxiv.org/pdf/2312.10908v3.pdf"
    },
    {
        "title": "Students' Perceptions and Preferences of Generative Artificial Intelligence Feedback for Programming",
        "authors": [
            "Zhengdong Zhang",
            "Zihan Dong",
            "Yang Shi",
            "Noboru Matsuda",
            "Thomas Price",
            "Dongkuan Xu"
        ],
        "published": "2023-12-17T22:26:53Z",
        "summary": "The rapid evolution of artificial intelligence (AI), specifically large\nlanguage models (LLMs), has opened opportunities for various educational\napplications. This paper explored the feasibility of utilizing ChatGPT, one of\nthe most popular LLMs, for automating feedback for Java programming assignments\nin an introductory computer science (CS1) class. Specifically, this study\nfocused on three questions: 1) To what extent do students view LLM-generated\nfeedback as formative? 2) How do students see the comparative affordances of\nfeedback prompts that include their code, vs. those that exclude it? 3) What\nenhancements do students suggest for improving AI-generated feedback? To\naddress these questions, we generated automated feedback using the ChatGPT API\nfor four lab assignments in the CS1 class. The survey results revealed that\nstudents perceived the feedback as aligning well with formative feedback\nguidelines established by Shute. Additionally, students showed a clear\npreference for feedback generated by including the students' code as part of\nthe LLM prompt, and our thematic study indicated that the preference was mainly\nattributed to the specificity, clarity, and corrective nature of the feedback.\nMoreover, this study found that students generally expected specific and\ncorrective feedback with sufficient code examples, but had diverged opinions on\nthe tone of the feedback. This study demonstrated that ChatGPT could generate\nJava programming assignment feedback that students perceived as formative. It\nalso offered insights into the specific improvements that would make the\nChatGPT-generated feedback useful for students.",
        "pdf_link": "https://arxiv.org/pdf/2312.11567v1.pdf"
    },
    {
        "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
        "authors": [
            "Chenglin Li",
            "Qianglong Chen",
            "Liangyue Li",
            "Caiyu Wang",
            "Yicheng Li",
            "Zulong Chen",
            "Yin Zhang"
        ],
        "published": "2023-12-17T14:28:28Z",
        "summary": "While large language models (LLMs) have demonstrated exceptional performance\nin recent natural language processing (NLP) tasks, their deployment poses\nsubstantial challenges due to high computational and memory demands in\nreal-world applications. Recent studies have focused on enhancing smaller\nmodels through knowledge distillation from LLMs, yielding promising results.\nHowever, these models often struggle to match the performance of LLMs,\nespecially in tasks that require reasoning. In this work, we introduce Mixed\nDistillation (MD) framework, which capitalizes on the strengths of Program of\nThought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining\nmultiple prompting techniques and distilling these capabilities into smaller\nmodels. Our experimental results show that MD significantly enhances the\nsingle-path and multi-path reasoning ability of smaller models in various\ntasks. In terms of accuracy and generality of reasoning tasks, the model\ngenerated by it exceeds the comprehensive performance of two individually\ndistilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved\nremarkable improvements of (84.5%) and (85.5%), respectively, outperforming\nGPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2312.10730v2.pdf"
    },
    {
        "title": "Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression",
        "authors": [
            "Luis Balderas",
            "Miguel Lastra",
            "Jos\u00e9 M. Ben\u00edtez"
        ],
        "published": "2023-12-17T12:33:50Z",
        "summary": "Large Language Models (LLMs) like BERT have gained significant prominence due\nto their remarkable performance in various natural language processing tasks.\nHowever, they come with substantial computational and memory costs.\nAdditionally, they are essentially black-box models, challenging to explain and\ninterpret. In this article, we propose Optimus BERT Compression and\nExplainability (OBCE), a methodology to bring explainability to BERT models\nusing persistent homology, aiming to measure the importance of each neuron by\nstudying the topological characteristics of their outputs. As a result, we can\ncompress BERT significantly by reducing the number of parameters (58.47% of the\noriginal parameters for BERT Base, 52.3% for BERT Large). We evaluated our\nmethodology on the standard GLUE Benchmark, comparing the results with\nstate-of-the-art techniques and achieving outstanding results. Consequently,\nour methodology can \"whiten\" BERT models by providing explainability to its\nneurons and reducing the model's size, making it more suitable for deployment\non resource-constrained devices.",
        "pdf_link": "https://arxiv.org/pdf/2312.10702v1.pdf"
    },
    {
        "title": "An Evaluation of GPT-4V and Gemini in Online VQA",
        "authors": [
            "Mengchen Liu",
            "Chongyan Chen",
            "Danna Gurari"
        ],
        "published": "2023-12-17T07:38:43Z",
        "summary": "While there is much excitement about the potential of large multimodal models\n(LMM), a comprehensive evaluation is critical to establish their true\ncapabilities and limitations. In support of this aim, we evaluate two\nstate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answering\ndataset sourced from an authentic online question answering community. We\nconduct fine-grained analysis by generating seven types of metadata for nearly\n2,000 visual questions, such as image type and the required image processing\ncapabilities. Our zero-shot performance analysis highlights the types of\nquestions that are most challenging for both models, including questions\nrelated to \"puzzling\" topic, with \"Identification\" user intention, with \"Sheet\nMusic\" image type, or labeled as \"hard\" by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.10637v2.pdf"
    },
    {
        "title": "LLM-Twin: Mini-Giant Model-driven Beyond 5G Digital Twin Networking Framework with Semantic Secure Communication and Computation",
        "authors": [
            "Yang Hong",
            "Jun Wu",
            "Rosario Morello"
        ],
        "published": "2023-12-17T07:13:59Z",
        "summary": "Beyond 5G networks provide solutions for next-generation communications,\nespecially digital twins networks (DTNs) have gained increasing popularity for\nbridging physical space and digital space. However, current DTNs networking\nframeworks pose a number of challenges especially when applied in scenarios\nthat require high communication efficiency and multimodal data processing.\nFirst, current DTNs frameworks are unavoidable regarding high resource\nconsumption and communication congestion because of original bit-level\ncommunication and high-frequency computation, especially distributed\nlearning-based DTNs. Second, current machine learning models for DTNs are\ndomain-specific (e.g. E-health), making it difficult to handle DT scenarios\nwith multimodal data processing requirements. Last but not least, current\nsecurity schemes for DTNs, such as blockchain, introduce additional overheads\nthat impair the efficiency of DTNs. To address the above challenges, we propose\na large language model (LLM) empowered DTNs networking framework, LLM-Twin.\nFirst, we design the mini-giant model collaboration scheme to achieve efficient\ndeployment of LLM in DTNs, since LLM are naturally conducive to processing\nmultimodal data. Then, we design a semantic-level high-efficiency, and secure\ncommunication model for DTNs. The feasibility of LLM-Twin is demonstrated by\nnumerical experiments and case studies. To our knowledge, this is the first to\npropose LLM-based semantic-level digital twin networking framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.10631v1.pdf"
    },
    {
        "title": "TrojFSP: Trojan Insertion in Few-shot Prompt Tuning",
        "authors": [
            "Mengxin Zheng",
            "Jiaqi Xue",
            "Xun Chen",
            "YanShan Wang",
            "Qian Lou",
            "Lei Jiang"
        ],
        "published": "2023-12-16T14:49:36Z",
        "summary": "Prompt tuning is one of the most effective solutions to adapting a fixed\npre-trained language model (PLM) for various downstream tasks, especially with\nonly a few input samples. However, the security issues, e.g., Trojan attacks,\nof prompt tuning on a few data samples are not well-studied. Transferring\nestablished data poisoning attacks directly to few-shot prompt tuning presents\nmultiple challenges. One significant issue is the \\textit{poisoned imbalance\nissue}, where non-target class samples are added to the target class, resulting\nin a greater number of target-class samples compared to non-target class. While\nthis issue is not critical in regular tuning, it significantly hampers the\nfew-shot prompt tuning, making it difficult to simultaneously achieve a high\nattack success rate (ASR) and maintain clean data accuracy (CDA). Additionally,\nfew-shot prompting is prone to overfitting in terms of both ASR and CDA. In\nthis paper, we introduce \\textit{TrojFSP}, a method designed to address the\nchallenges. To solve the poisoned imbalance issue, we develop a\n\\textit{Target-Class Shrink (TC-Shrink)} technique, which aims to equalize the\nnumber of poisoning samples. To combat overfitting, we employ a\n\\textit{Selective Token Poisoning} technique to boost attack performance.\nFurthermore, we introduce a \\textit{Trojan-Trigger Attention} objective\nfunction to amplify the attention of the poisoned trojan prompt on triggers.\nExperiments show that our TrojFSP achieves an ASR of over 99\\% while\nmaintaining negligible decreases in CDA across various PLMs and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.10467v3.pdf"
    },
    {
        "title": "Resolving Crash Bugs via Large Language Models: An Empirical Study",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Juntao Li",
            "Hanlin Wang",
            "Xin Peng",
            "Yiling Lou"
        ],
        "published": "2023-12-16T13:41:04Z",
        "summary": "Crash bugs cause unexpected program behaviors or even termination, requiring\nhigh-priority resolution. However, manually resolving crash bugs is challenging\nand labor-intensive, and researchers have proposed various techniques for their\nautomated localization and repair. ChatGPT, a recent large language model\n(LLM), has garnered significant attention due to its exceptional performance\nacross various domains. This work performs the first investigation into\nChatGPT's capability in resolve real-world crash bugs, focusing on its\neffectiveness in both localizing and repairing code-related and\nenvironment-related crash bugs. Specifically, we initially assess ChatGPT's\nfundamental ability to resolve crash bugs with basic prompts in a single\niteration. We observe that ChatGPT performs better at resolving code-related\ncrash bugs compared to environment-related ones, and its primary challenge in\nresolution lies in inaccurate localization. Additionally, we explore ChatGPT's\npotential with various advanced prompts. Furthermore, by stimulating ChatGPT's\nself-planning, it methodically investigates each potential crash-causing\nenvironmental factor through proactive inquiry, ultimately identifying the root\ncause of the crash. Based on our findings, we propose IntDiagSolver, an\ninteraction methodology designed to facilitate precise crash bug resolution\nthrough continuous interaction with LLMs. Evaluating IntDiagSolver on multiple\nLLMs reveals consistent enhancement in the accuracy of crash bug resolution,\nincluding ChatGPT, Claude, and CodeLlama.",
        "pdf_link": "https://arxiv.org/pdf/2312.10448v1.pdf"
    },
    {
        "title": "DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content",
        "authors": [
            "Wentao Wang",
            "Xuanyao Huang",
            "Tianyang Wang",
            "Swalpa Kumar Roy"
        ],
        "published": "2023-12-16T10:17:09Z",
        "summary": "This paper explores the image synthesis capabilities of GPT-4, a leading\nmulti-modal large language model. We establish a benchmark for evaluating the\nfidelity of texture features in images generated by GPT-4, comprising manually\npainted pictures and their AI-generated counterparts. The contributions of this\nstudy are threefold: First, we provide an in-depth analysis of the fidelity of\nimage synthesis features based on GPT-4, marking the first such study on this\nstate-of-the-art model. Second, the quantitative and qualitative experiments\nfully reveals the limitations of the GPT-4 model in image synthesis. Third, we\nhave compiled a unique benchmark of manual drawings and corresponding\nGPT-4-generated images, introducing a new task to advance fidelity research in\nAI-generated content (AIGC). The dataset is available at:\n\\url{https://github.com/rickwang28574/DeepArt}.",
        "pdf_link": "https://arxiv.org/pdf/2312.10407v2.pdf"
    },
    {
        "title": "When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning",
        "authors": [
            "Qihang Ai",
            "Jianwu Zhou",
            "Haiyun Jiang",
            "Lemao Liu",
            "Shuming Shi"
        ],
        "published": "2023-12-16T08:14:11Z",
        "summary": "Graph data is ubiquitous in the physical world, and it has always been a\nchallenge to efficiently model graph structures using a unified paradigm for\nthe understanding and reasoning on various graphs. Moreover, in the era of\nlarge language models, integrating complex graph information into text\nsequences has become exceptionally difficult, which hinders the ability to\ninteract with graph data through natural language instructions.The paper\npresents a new paradigm for understanding and reasoning about graph data by\nintegrating image encoding and multimodal technologies. This approach enables\nthe comprehension of graph data through an instruction-response format,\nutilizing GPT-4V's advanced capabilities. The study evaluates this paradigm on\nvarious graph types, highlighting the model's strengths and weaknesses,\nparticularly in Chinese OCR performance and complex reasoning tasks. The\nfindings suggest new direction for enhancing graph data processing and natural\nlanguage interaction.",
        "pdf_link": "https://arxiv.org/pdf/2312.10372v1.pdf"
    },
    {
        "title": "LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?",
        "authors": [
            "Fuheng Zhao",
            "Lawrence Lim",
            "Ishtiyaque Ahmad",
            "Divyakant Agrawal",
            "Amr El Abbadi"
        ],
        "published": "2023-12-16T05:01:23Z",
        "summary": "Judging the equivalence between two SQL queries is a fundamental problem with\nmany practical applications in data management and SQL generation (i.e.,\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\nresearch community has reasoned about SQL equivalence for decades, it poses\nconsiderable difficulties and no complete solutions exist. Recently, Large\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\nquestion answering and solving mathematics challenges. In this paper, we study\nif LLMs can be used to determine the equivalence between SQL queries under two\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\nassist LLMs in generating high quality responses, we present two prompting\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\nquery on a simple database instance and then explore if a counterexample exists\nby modifying the database. The latter technique is used to evaluate the relaxed\nequivalence in which it asks LLMs to explain the queries and then compare if\nthey contain significant logical differences. Our experiments demonstrate using\nour techniques, LLMs is a promising tool to help data engineers in writing\nsemantically equivalent SQL queries, however challenges still persist, and is a\nbetter metric for evaluating SQL generation than the popular execution\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2312.10321v2.pdf"
    },
    {
        "title": "KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know",
        "authors": [
            "Shangshang Zheng",
            "He Bai",
            "Yizhe Zhang",
            "Yi Su",
            "Xiaochuan Niu",
            "Navdeep Jaitly"
        ],
        "published": "2023-12-15T23:34:05Z",
        "summary": "Measuring the alignment between a Knowledge Graph (KG) and Large Language\nModels (LLMs) is an effective method to assess the factualness and identify the\nknowledge blind spots of LLMs. However, this approach encounters two primary\nchallenges including the translation of KGs into natural language and the\nefficient evaluation of these extensive and complex structures. In this paper,\nwe present KGLens--a novel framework aimed at measuring the alignment between\nKGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs.\nKGLens features a graph-guided question generator for converting KGs into\nnatural language, along with a carefully designed sampling strategy based on\nparameterized KG structure to expedite KG traversal. We conducted experiments\nusing three domain-specific KGs from Wikidata, which comprise over 19,000\nedges, 700 relations, and 21,000 entities. Our analysis across eight LLMs\nreveals that KGLens not only evaluates the factual accuracy of LLMs more\nrapidly but also delivers in-depth analyses on topics, temporal dynamics, and\nrelationships. Furthermore, human evaluation results indicate that KGLens can\nassess LLMs with a level of accuracy nearly equivalent to that of human\nannotators, achieving 95.7% of the accuracy rate.",
        "pdf_link": "https://arxiv.org/pdf/2312.11539v2.pdf"
    },
    {
        "title": "Student as an Inherent Denoiser of Noisy Teacher",
        "authors": [
            "Jiachen Zhao"
        ],
        "published": "2023-12-15T20:21:45Z",
        "summary": "Knowledge distillation (KD) has been widely employed to transfer knowledge\nfrom a large language model (LLM) to a specialized model in low-data regimes\nthrough pseudo label learning. However, pseudo labels generated by teacher\nmodels are usually noisy and may influence KD performance. This study delves\ninto KD with noisy teachers and uncovers that the student model can already\ngenerate more accurate predictions than the teacher labels used to train it\nduring KD, indicating its inherent ability to denoise noisy teacher labels.\nMotivated by this finding, we propose Peer-Advised KD to improve vanilla KD\nfrom noisy teachers. Experiments show that Peer-Advised KD can outperform LLM\nby approximately 5% with 50 human-labeled data, and even competitive to\nstandard supervised finetuning with 750 human-labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2312.10185v1.pdf"
    },
    {
        "title": "Challenges with unsupervised LLM knowledge discovery",
        "authors": [
            "Sebastian Farquhar",
            "Vikrant Varma",
            "Zachary Kenton",
            "Johannes Gasteiger",
            "Vladimir Mikulik",
            "Rohin Shah"
        ],
        "published": "2023-12-15T18:49:43Z",
        "summary": "We show that existing unsupervised methods on large language model (LLM)\nactivations do not discover knowledge -- instead they seem to discover whatever\nfeature of the activations is most prominent. The idea behind unsupervised\nknowledge elicitation is that knowledge satisfies a consistency structure,\nwhich can be used to discover knowledge. We first prove theoretically that\narbitrary features (not just knowledge) satisfy the consistency structure of a\nparticular leading unsupervised knowledge-elicitation method,\ncontrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a\nseries of experiments showing settings in which unsupervised methods result in\nclassifiers that do not predict knowledge, but instead predict a different\nprominent feature. We conclude that existing unsupervised methods for\ndiscovering latent knowledge are insufficient, and we contribute sanity checks\nto apply to evaluating future knowledge elicitation methods. Conceptually, we\nhypothesise that the identification issues explored here, e.g. distinguishing a\nmodel's knowledge from that of a simulated character's, will persist for future\nunsupervised methods.",
        "pdf_link": "https://arxiv.org/pdf/2312.10029v2.pdf"
    },
    {
        "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
        "authors": [
            "Shihan Dou",
            "Enyu Zhou",
            "Yan Liu",
            "Songyang Gao",
            "Jun Zhao",
            "Wei Shen",
            "Yuhao Zhou",
            "Zhiheng Xi",
            "Xiao Wang",
            "Xiaoran Fan",
            "Shiliang Pu",
            "Jiang Zhu",
            "Rui Zheng",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-12-15T17:45:06Z",
        "summary": "Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. Increasing instruction data substantially is\na direct solution to align the model with a broader range of downstream tasks\nor notably improve its performance on a specific task. However, we find that\nlarge-scale increases in instruction data can damage the world knowledge\npreviously stored in LLMs. To address this challenge, we propose LoRAMoE, a\nnovelty framework that introduces several low-rank adapters (LoRA) and\nintegrates them by using a router network, like a plugin version of Mixture of\nExperts (MoE). It freezes the backbone model and forces a portion of LoRAs to\nfocus on leveraging world knowledge to solve downstream tasks, to alleviate\nworld knowledge-edge forgetting. Experimental results show that, as the\ninstruction data increases, LoRAMoE can significantly improve the ability to\nprocess downstream tasks, while maintaining the world knowledge stored in the\nLLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.09979v4.pdf"
    },
    {
        "title": "Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China",
        "authors": [
            "Di Zhou",
            "Yinxian Zhang"
        ],
        "published": "2023-12-15T16:25:56Z",
        "summary": "The rising popularity of ChatGPT and other AI-powered large language models\n(LLMs) has led to increasing studies highlighting their susceptibility to\nmistakes and biases. However, most of these studies focus on models trained on\nEnglish texts. Taking an innovative approach, this study investigates political\nbiases in GPT's multilingual models. We posed the same question about\nhigh-profile political issues in the United States and China to GPT in both\nEnglish and simplified Chinese, and our analysis of the bilingual responses\nrevealed that GPT's bilingual models' political \"knowledge\" (content) and the\npolitical \"attitude\" (sentiment) are significantly more inconsistent on\npolitical issues in China. The simplified Chinese GPT models not only tended to\nprovide pro-China information but also presented the least negative sentiment\ntowards China's problems, whereas the English GPT was significantly more\nnegative towards China. This disparity may stem from Chinese state censorship\nand US-China geopolitical tensions, which influence the training corpora of GPT\nbilingual models. Moreover, both Chinese and English models tended to be less\ncritical towards the issues of \"their own\" represented by the language used,\nthan the issues of \"the other.\" This suggests that GPT multilingual models\ncould potentially develop a \"political identity\" and an associated sentiment\nbias based on their training language. We discussed the implications of our\nfindings for information transmission and communication in an increasingly\ndivided world.",
        "pdf_link": "https://arxiv.org/pdf/2312.09917v1.pdf"
    },
    {
        "title": "ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)",
        "authors": [
            "Tosin Adewumi",
            "Lama Alkhaled",
            "Claudia Buck",
            "Sergio Hernandez",
            "Saga Brilioth",
            "Mkpe Kekung",
            "Yelvin Ragimov",
            "Elisa Barney"
        ],
        "published": "2023-12-15T14:01:46Z",
        "summary": "We introduce a novel writing method called Probing Chain of Thought (ProCoT),\nwhich prevents students from cheating using a Large Language Model (LLM), such\nas ChatGPT, while enhancing their active learning through such models. LLMs\nhave disrupted education and many other feilds. For fear of students cheating,\nmany educationists have resorted to banning their use, as their outputs can be\nhuman-like and hard to detect in some cases. These LLMs are also known for\nhallucinations (i.e. fake facts). We conduct studies with ProCoT in two\ndifferent courses with a combined total of about 66 students. The students in\neach course were asked to prompt an LLM of their choice with one question from\na set of four and required to affirm or refute statements in the LLM output by\nusing peer reviewed references. The results show two things: (1) ProCoT\nstimulates creative/critical thinking and writing of students through\nengagement with LLMs when we compare the LLM solely output to ProCoT output and\n(2) ProCoT can prevent cheating because of clear limitations in existing LLMs\nwhen we compare students ProCoT output to LLM ProCoT output. We also discover\nthat most students prefer to give answers in fewer words than LLMs, which are\ntypically verbose. The average word counts for students, ChatGPT (v3.5) and\nPhind (v8) are 208, 391 and 383, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2312.09801v1.pdf"
    },
    {
        "title": "Taxonomy-based CheckList for Large Language Model Evaluation",
        "authors": [
            "Damin Zhang"
        ],
        "published": "2023-12-15T12:58:07Z",
        "summary": "As large language models (LLMs) have been used in many downstream tasks, the\ninternal stereotypical representation may affect the fairness of the outputs.\nIn this work, we introduce human knowledge into natural language interventions\nand study pre-trained language models' (LMs) behaviors within the context of\ngender bias. Inspired by CheckList behavioral testing, we present a\nchecklist-style task that aims to probe and quantify LMs' unethical behaviors\nthrough question-answering (QA). We design three comparison studies to evaluate\nLMs from four aspects: consistency, biased tendency, model preference, and\ngender preference switch. We probe one transformer-based QA model trained on\nSQuAD-v2 dataset and one autoregressive large language model. Our results\nindicate that transformer-based QA model's biased tendency positively\ncorrelates with its consistency, whereas LLM shows the opposite relation. Our\nproposed task provides the first dataset that involves human knowledge for LLM\nbias evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.10899v1.pdf"
    },
    {
        "title": "Prompting Large Language Models for Topic Modeling",
        "authors": [
            "Han Wang",
            "Nirmalendu Prakash",
            "Nguyen Khoi Hoang",
            "Ming Shan Hee",
            "Usman Naseem",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-12-15T11:15:05Z",
        "summary": "Topic modeling is a widely used technique for revealing underlying thematic\nstructures within textual data. However, existing models have certain\nlimitations, particularly when dealing with short text datasets that lack\nco-occurring words. Moreover, these models often neglect sentence-level\nsemantics, focusing primarily on token-level semantics. In this paper, we\npropose PromptTopic, a novel topic modeling approach that harnesses the\nadvanced language understanding of large language models (LLMs) to address\nthese challenges. It involves extracting topics at the sentence level from\nindividual documents, then aggregating and condensing these topics into a\npredefined quantity, ultimately providing coherent topics for texts of varying\nlengths. This approach eliminates the need for manual parameter tuning and\nimproves the quality of extracted topics. We benchmark PromptTopic against the\nstate-of-the-art baselines on three vastly diverse datasets, establishing its\nproficiency in discovering meaningful topics. Furthermore, qualitative analysis\nshowcases PromptTopic's ability to uncover relevant topics in multiple\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.09693v1.pdf"
    },
    {
        "title": "Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models",
        "authors": [
            "Xin Jin",
            "Jonathan Larson",
            "Weiwei Yang",
            "Zhiqiang Lin"
        ],
        "published": "2023-12-15T08:32:28Z",
        "summary": "Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.",
        "pdf_link": "https://arxiv.org/pdf/2312.09601v1.pdf"
    },
    {
        "title": "Extending Context Window of Large Language Models via Semantic Compression",
        "authors": [
            "Weizhi Fei",
            "Xueyan Niu",
            "Pingyi Zhou",
            "Lu Hou",
            "Bo Bai",
            "Lei Deng",
            "Wei Han"
        ],
        "published": "2023-12-15T07:04:33Z",
        "summary": "Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.",
        "pdf_link": "https://arxiv.org/pdf/2312.09571v1.pdf"
    },
    {
        "title": "Privacy-Aware Document Visual Question Answering",
        "authors": [
            "Rub\u00e8n Tito",
            "Khanh Nguyen",
            "Marlon Tobaben",
            "Raouf Kerkouche",
            "Mohamed Ali Souibgui",
            "Kangsoo Jung",
            "Lei Kang",
            "Ernest Valveny",
            "Antti Honkela",
            "Mario Fritz",
            "Dimosthenis Karatzas"
        ],
        "published": "2023-12-15T06:30:55Z",
        "summary": "Document Visual Question Answering (DocVQA) is a fast growing branch of\ndocument understanding. Despite the fact that documents contain sensitive or\ncopyrighted information, none of the current DocVQA methods offers strong\nprivacy guarantees.\n  In this work, we explore privacy in the domain of DocVQA for the first time.\nWe highlight privacy issues in state of the art multi-modal LLM models used for\nDocVQA, and explore possible solutions.\n  Specifically, we focus on the invoice processing use case as a realistic,\nwidely used scenario for document understanding, and propose a large scale\nDocVQA dataset comprising invoice documents and associated questions and\nanswers. We employ a federated learning scheme, that reflects the real-life\ndistribution of documents in different businesses, and we explore the use case\nwhere the ID of the invoice issuer is the sensitive information to be\nprotected.\n  We demonstrate that non-private models tend to memorise, behaviour that can\nlead to exposing private information. We then evaluate baseline training\nschemes employing federated learning and differential privacy in this\nmulti-modal scenario, where the sensitive information might be exposed through\nany of the two input modalities: vision (document image) or language (OCR\ntokens).\n  Finally, we design an attack exploiting the memorisation effect of the model,\nand demonstrate its effectiveness in probing different DocVQA models.",
        "pdf_link": "https://arxiv.org/pdf/2312.10108v1.pdf"
    },
    {
        "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models",
        "authors": [
            "Lei Zhang",
            "Yunshui Li",
            "Ziqiang Liu",
            "Jiaxi yang",
            "Junhao Liu",
            "Min Yang"
        ],
        "published": "2023-12-15T05:30:14Z",
        "summary": "Although there are currently many benchmarks available for evaluating the\nlong context understanding and reasoning capability of large language models,\nwith the expansion of the context window in these models, the existing long\ncontext benchmarks are no longer sufficient for evaluating the long context\nunderstanding and reasoning capability of large language models. In this paper,\nwe have developed a fresh long context evaluation benchmark, which we name it\nMarathon in the form of multiple choice questions, inspired by benchmarks such\nas MMLU, for assessing the long context comprehension capability of large\nlanguage models quickly, accurately, and objectively. We have evaluated several\nof the latest and most popular large language models, as well as three recent\nand effective long context optimization methods, on our benchmark. This\nshowcases the long context reasoning and comprehension capabilities of these\nlarge language models and validates the effectiveness of these optimization\nmethods. Marathon is available at\nhttps://huggingface.co/datasets/Lemoncoke/Marathon.",
        "pdf_link": "https://arxiv.org/pdf/2312.09542v1.pdf"
    },
    {
        "title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models",
        "authors": [
            "Shengyao Zhang",
            "Mi Zhang",
            "Xudong Pan",
            "Min Yang"
        ],
        "published": "2023-12-15T02:42:05Z",
        "summary": "To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2312.09494v2.pdf"
    },
    {
        "title": "CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment",
        "authors": [
            "Ljubisa Bojic",
            "Matteo Cinelli",
            "Dubravko Culibrk",
            "Boris Delibasic"
        ],
        "published": "2023-12-14T23:48:51Z",
        "summary": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial general intelligence (AGI) and LLMs. Due to the rapid\ndevelopment and wide application of LLMs, challenges such as ethical alignment,\ncontrollability, and predictability of these models have become important\nresearch topics. This study investigates an innovative simulation-based\nmulti-agent system within a virtual reality framework that replicates the\nreal-world environment. The framework is populated by automated 'digital\ncitizens,' simulating complex social structures and interactions to examine and\noptimize AGI. Application of various theories from the fields of sociology,\nsocial psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAGI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AGI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2312.09402v1.pdf"
    },
    {
        "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft",
        "authors": [
            "Hao Li",
            "Xue Yang",
            "Zhaokai Wang",
            "Xizhou Zhu",
            "Jie Zhou",
            "Yu Qiao",
            "Xiaogang Wang",
            "Hongsheng Li",
            "Lewei Lu",
            "Jifeng Dai"
        ],
        "published": "2023-12-14T18:58:12Z",
        "summary": "Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.",
        "pdf_link": "https://arxiv.org/pdf/2312.09238v2.pdf"
    },
    {
        "title": "Measurement in the Age of LLMs: An Application to Ideological Scaling",
        "authors": [
            "Sean O'Hagan",
            "Aaron Schein"
        ],
        "published": "2023-12-14T18:34:06Z",
        "summary": "Much of social science is centered around terms like ``ideology'' or\n``power'', which generally elude precise definition, and whose contextual\nmeanings are trapped in surrounding language. This paper explores the use of\nlarge language models (LLMs) to flexibly navigate the conceptual clutter\ninherent to social scientific measurement tasks. We rely on LLMs' remarkable\nlinguistic fluency to elicit ideological scales of both legislators and text,\nwhich accord closely to established methods and our own judgement. A key aspect\nof our approach is that we elicit such scores directly, instructing the LLM to\nfurnish numeric scores itself. This approach affords a great deal of\nflexibility, which we showcase through a variety of different case studies. Our\nresults suggest that LLMs can be used to characterize highly subtle and diffuse\nmanifestations of political ideology in text.",
        "pdf_link": "https://arxiv.org/pdf/2312.09203v2.pdf"
    },
    {
        "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
        "authors": [
            "Hao Sun",
            "Hengyi Cai",
            "Bo Wang",
            "Yingyan Hou",
            "Xiaochi Wei",
            "Shuaiqiang Wang",
            "Yan Zhang",
            "Dawei Yin"
        ],
        "published": "2023-12-14T16:10:56Z",
        "summary": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.",
        "pdf_link": "https://arxiv.org/pdf/2312.09075v2.pdf"
    },
    {
        "title": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent",
        "authors": [
            "Haoran Liao",
            "Qinyi Du",
            "Shaohua Hu",
            "Hao He",
            "Yanyan Xu",
            "Jidong Tian",
            "Yaohui Jin"
        ],
        "published": "2023-12-14T13:33:50Z",
        "summary": "Large language models (LLMs) face challenges in solving complex mathematical\nproblems that require comprehensive capacities to parse the statements,\nassociate domain knowledge, perform compound logical reasoning, and integrate\nthe intermediate rationales. Tackling all these problems once could be arduous\nfor LLMs, thus leading to confusion in generation. In this work, we explore the\npotential of enhancing LLMs with agents by meticulous decomposition and\nmodeling of mathematical reasoning process. Specifically, we propose a formal\ndescription of the mathematical solving and extend LLMs with an agent-based\nzero-shot framework named\n$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We\nfurther provide and implement two MathAgents that define the logical forms and\ninherent relations via a pool of actions in different grains and orientations:\nMathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with\nhumankind. Experiments on miniF2F and MATH have demonstrated the effectiveness\nof PRER and proposed MathAgents, achieving an increase of\n$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$\n($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and\n$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against\nGPT-4. Further analytical results provide more insightful perspectives on\nexploiting the behaviors of LLMs as agents.",
        "pdf_link": "https://arxiv.org/pdf/2312.08926v2.pdf"
    },
    {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxuan Zhang",
            "Juanzi Li",
            "Bin Xu",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "published": "2023-12-14T13:20:57Z",
        "summary": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM .",
        "pdf_link": "https://arxiv.org/pdf/2312.08914v2.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
        "authors": [
            "Navreet Kaur",
            "Monojit Choudhury",
            "Danish Pruthi"
        ],
        "published": "2023-12-14T10:35:13Z",
        "summary": "As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2312.08800v1.pdf"
    },
    {
        "title": "Future-proofing geotechnics workflows: accelerating problem-solving with large language models",
        "authors": [
            "Stephen Wu",
            "Yu Otake",
            "Daijiro Mizutani",
            "Chang Liu",
            "Kotaro Asano",
            "Nana Sato",
            "Hidetoshi Baba",
            "Yusuke Fukunaga",
            "Yosuke Higo",
            "Akiyoshi Kamura",
            "Shinnosuke Kodama",
            "Masataka Metoki",
            "Tomoka Nakamura",
            "Yuto Nakazato",
            "Taiga Saito",
            "Akihiro Shioi",
            "Masahiro Takenobu",
            "Keigo Tsukioka",
            "Ryo Yoshikawa"
        ],
        "published": "2023-12-14T05:17:27Z",
        "summary": "The integration of Large Language Models (LLMs) like ChatGPT into the\nworkflows of geotechnical engineering has a high potential to transform how the\ndiscipline approaches problem-solving and decision-making. This paper delves\ninto the innovative application of LLMs in geotechnical engineering, as\nexplored in a hands-on workshop held in Tokyo, Japan. The event brought\ntogether a diverse group of 20 participants, including students, researchers,\nand professionals from academia, industry, and government sectors, to\ninvestigate practical uses of LLMs in addressing specific geotechnical\nchallenges. The workshop facilitated the creation of solutions for four\ndifferent practical geotechnical problems as illustrative examples, culminating\nin the development of an academic paper. The paper discusses the potential of\nLLMs to transform geotechnical engineering practices, highlighting their\nproficiency in handling a range of tasks from basic data analysis to complex,\nmultimodal problem-solving. It also addresses the challenges in implementing\nLLMs, particularly in achieving high precision and accuracy in specialized\ntasks, and underscores the need for expert oversight. The findings demonstrate\nLLMs' effectiveness in enhancing efficiency, data processing, and\ndecision-making in geotechnical engineering, suggesting a paradigm shift\ntowards more integrated, data-driven approaches in this field. This study not\nonly showcases the potential of LLMs in a specific engineering domain, but also\nsets a precedent for their broader application in interdisciplinary research\nand practice, where the synergy of human expertise and artificial intelligence\nredefines the boundaries of problem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2312.12411v1.pdf"
    },
    {
        "title": "ChatSOS: LLM-based knowledge Q&A system for safety engineering",
        "authors": [
            "Haiyang Tang",
            "Zhenyi Liu",
            "Dongping Chen",
            "Qingzhao Chu"
        ],
        "published": "2023-12-14T03:25:23Z",
        "summary": "Recent advancements in large language models (LLMs) have notably propelled\nnatural language processing (NLP) capabilities, demonstrating significant\npotential in safety engineering applications. Despite these advancements, LLMs\nface constraints in processing specialized tasks, attributed to factors such as\ncorpus size, input processing limitations, and privacy concerns. Obtaining\nuseful information from reliable sources in a limited time is crucial for LLM.\nAddressing this, our study introduces an LLM-based Q&A system for safety\nengineering, enhancing the comprehension and response accuracy of the model. We\nemployed prompt engineering to incorporate external knowledge databases, thus\nenriching the LLM with up-to-date and reliable information. The system analyzes\nhistorical incident reports through statistical methods, utilizes vector\nembedding to construct a vector database, and offers an efficient\nsimilarity-based search functionality. Our findings indicate that the\nintegration of external knowledge significantly augments the capabilities of\nLLM for in-depth problem analysis and autonomous task assignment. It\neffectively summarizes accident reports and provides pertinent recommendations.\nThis integration approach not only expands LLM applications in safety\nengineering but also sets a precedent for future developments towards\nautomation and intelligent systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.08629v1.pdf"
    },
    {
        "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
        "authors": [
            "Kaiqiang Song",
            "Xiaoyang Wang",
            "Sangwoo Cho",
            "Xiaoman Pan",
            "Dong Yu"
        ],
        "published": "2023-12-14T02:45:31Z",
        "summary": "This paper introduces a novel approach to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and understanding extensive text\nsequences, a critical aspect in applications requiring deep comprehension and\nsynthesis of large volumes of information. Recognizing the inherent challenges\nin extending the context window for LLMs, primarily built on Transformer\narchitecture, we propose a new model architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic time and memory complexity\nissues associated with full attention in the Transformer by employing grouped\nlocal-global attention layers. Our model, akin to a zebra's alternating\nstripes, balances local and global attention layers, significantly reducing\ncomputational requirements and memory consumption. Comprehensive experiments,\nincluding pretraining from scratch, continuation of long context adaptation\ntraining, and long instruction tuning, are conducted to evaluate the Zebra's\nperformance. The results show that Zebra achieves comparable or superior\nperformance on both short and long sequence benchmarks, while also enhancing\ntraining and inference efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2312.08618v1.pdf"
    },
    {
        "title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach",
        "authors": [
            "Golnaz Shapurian",
            "Michael J Kurtz",
            "Alberto Accomazzi"
        ],
        "published": "2023-12-14T00:50:14Z",
        "summary": "The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.",
        "pdf_link": "https://arxiv.org/pdf/2312.08579v2.pdf"
    },
    {
        "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet",
        "authors": [
            "Alexander Borzunov",
            "Max Ryabinin",
            "Artem Chumachenko",
            "Dmitry Baranchuk",
            "Tim Dettmers",
            "Younes Belkada",
            "Pavel Samygin",
            "Colin Raffel"
        ],
        "published": "2023-12-13T18:52:49Z",
        "summary": "Large language models (LLMs) are useful in many NLP tasks and become more\ncapable with size, with the best open-source models having over 50 billion\nparameters. However, using these 50B+ models requires high-end hardware, making\nthem inaccessible to most researchers. In this work, we investigate methods for\ncost-efficient inference and fine-tuning of LLMs, comparing local and\ndistributed strategies. We observe that a large enough model (50B+) can run\nefficiently even on geodistributed devices in a consumer-grade network. This\ncould allow running LLM efficiently by pooling together idle compute resources\nof multiple research groups and volunteers. We address two open problems: (1)\nhow to perform inference and fine-tuning reliably if any device can disconnect\nabruptly and (2) how to partition LLMs between devices with uneven hardware,\njoining and leaving at will. In order to do that, we develop special\nfault-tolerant inference algorithms and load-balancing protocols that\nautomatically assign devices to maximize the total system throughput. We\nshowcase these algorithms in Petals - a decentralized system that runs Llama 2\n(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for\ninteractive generation. We evaluate the performance of our system in simulated\nconditions and a real-world setup spanning two continents.",
        "pdf_link": "https://arxiv.org/pdf/2312.08361v1.pdf"
    },
    {
        "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF",
        "authors": [
            "Anand Siththaranjan",
            "Cassidy Laidlaw",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-12-13T18:51:34Z",
        "summary": "In practice, preference learning from human feedback depends on incomplete\ndata with hidden context. Hidden context refers to data that affects the\nfeedback received, but which is not represented in the data used to train a\npreference model. This captures common issues of data collection, such as\nhaving human annotators with varied preferences, cognitive processes that\nresult in seemingly irrational behavior, and combining data labeled according\nto different criteria. We prove that standard applications of preference\nlearning, including reinforcement learning from human feedback (RLHF),\nimplicitly aggregate over hidden contexts according to a well-known voting rule\ncalled Borda count. We show this can produce counter-intuitive results that are\nvery different from other methods which implicitly aggregate via expected\nutility. Furthermore, our analysis formalizes the way that preference learning\nfrom users with diverse values tacitly implements a social choice function. A\nkey implication of this result is that annotators have an incentive to\nmisreport their preferences in order to influence the learned model, leading to\nvulnerabilities in the deployment of RLHF. As a step towards mitigating these\nproblems, we introduce a class of methods called distributional preference\nlearning (DPL). DPL methods estimate a distribution of possible score values\nfor each alternative in order to better account for hidden context.\nExperimental results indicate that applying DPL to RLHF for LLM chatbots\nidentifies hidden context in the data and significantly reduces subsequent\njailbreak vulnerability. Our code and data are available at\nhttps://github.com/cassidylaidlaw/hidden-context",
        "pdf_link": "https://arxiv.org/pdf/2312.08358v1.pdf"
    },
    {
        "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
        "authors": [
            "Jiang Zhang",
            "Qiong Wu",
            "Yiming Xu",
            "Cheng Cao",
            "Zheng Du",
            "Konstantinos Psounis"
        ],
        "published": "2023-12-13T17:22:19Z",
        "summary": "Toxic content detection is crucial for online services to remove\ninappropriate content that violates community standards. To automate the\ndetection process, prior works have proposed varieties of machine learning (ML)\napproaches to train Language Models (LMs) for toxic content detection. However,\nboth their accuracy and transferability across datasets are limited. Recently,\nLarge Language Models (LLMs) have shown promise in toxic content detection due\nto their superior zero-shot and few-shot in-context learning ability as well as\nbroad transferability on ML tasks. However, efficiently designing prompts for\nLLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges, in this work, we\npropose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling\nLLMs for toxic content detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection\nperformance and extract high-quality rationales. DToT can automatically select\nmore fine-grained context to re-prompt LLMs when their responses lack\nconfidence. Additionally, we use the rationales extracted via DToT to fine-tune\nstudent LMs. Our experimental results on various datasets demonstrate that DToT\ncan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs\nfine-tuned with rationales extracted via DToT outperform baselines on all\ndatasets with up to 16.9\\% accuracy improvement, while being more than 60x\nsmaller than conventional LLMs. Finally, we observe that student LMs fine-tuned\nwith rationales exhibit better cross-dataset transferability.",
        "pdf_link": "https://arxiv.org/pdf/2312.08303v1.pdf"
    },
    {
        "title": "Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers",
        "authors": [
            "Haifeng Huang",
            "Zehan Wang",
            "Rongjie Huang",
            "Luping Liu",
            "Xize Cheng",
            "Yang Zhao",
            "Tao Jin",
            "Zhou Zhao"
        ],
        "published": "2023-12-13T14:27:45Z",
        "summary": "Recent research has evidenced the significant potentials of Large Language\nModels (LLMs) in handling challenging tasks within 3D scenes. However, current\nmodels are constrained to addressing object-centric tasks, where each\nquestion-answer pair focuses solely on an individual object. In real-world\napplications, users may pose queries involving multiple objects or expect for\nanswers that precisely reference various objects. We introduce the use of\nobject identifiers to freely reference objects during a conversation. While\nthis solution appears straightforward, it presents two main challenges: 1) How\nto establish a reliable one-to-one correspondence between each object and its\nidentifier? 2) How to incorporate complex spatial relationships among dozens of\nobjects into the embedding space of the LLM? To address these challenges, we\npropose a two-stage alignment method, which involves learning an\nattribute-aware token and a relation-aware token for each object. These tokens\ncapture the object's attributes and spatial relationships with surrounding\nobjects in the 3D scene. Once the alignment is established, we can fine-tune\nour model on various downstream tasks using instruction tuning. Experiments\nconducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3D\nshowcase the effectiveness of our proposed method. Additionally, we create a 3D\nscene captioning dataset annotated with rich object identifiers, with the\nassistant of GPT-4. This dataset aims to further explore the capability of\nobject identifiers in effective object referencing and precise scene\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2312.08168v2.pdf"
    },
    {
        "title": "Breaking the Silence: the Threats of Using LLMs in Software Engineering",
        "authors": [
            "June Sallou",
            "Thomas Durieux",
            "Annibale Panichella"
        ],
        "published": "2023-12-13T11:02:19Z",
        "summary": "Large Language Models (LLMs) have gained considerable traction within the\nSoftware Engineering (SE) community, impacting various SE tasks from code\ncompletion to test generation, from program repair to code summarization.\nDespite their promise, researchers must still be careful as numerous intricate\nfactors can influence the outcomes of experiments involving LLMs. This paper\ninitiates an open discussion on potential threats to the validity of LLM-based\nresearch including issues such as closed-source models, possible data leakage\nbetween LLM training data and research evaluation, and the reproducibility of\nLLM-based findings. In response, this paper proposes a set of guidelines\ntailored for SE researchers and Language Model (LM) providers to mitigate these\nconcerns. The implications of the guidelines are illustrated using existing\ngood practices followed by LLM providers and a practical example for SE\nresearchers in the context of test case generation.",
        "pdf_link": "https://arxiv.org/pdf/2312.08055v2.pdf"
    },
    {
        "title": "CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs",
        "authors": [
            "Huaiyuan Ying",
            "Zhengyun Zhao",
            "Yang Zhao",
            "Sihang Zeng",
            "Sheng Yu"
        ],
        "published": "2023-12-13T10:29:34Z",
        "summary": "Objective: Biomedical Knowledge Graphs play a pivotal role in various\nbiomedical research domains. Concurrently, term clustering emerges as a crucial\nstep in constructing these knowledge graphs, aiming to identify synonymous\nterms. Due to a lack of knowledge, previous contrastive learning models trained\nwith Unified Medical Language System (UMLS) synonyms struggle at clustering\ndifficult terms and do not generalize well beyond UMLS terms. In this work, we\nleverage the world knowledge from Large Language Models (LLMs) and propose\nContrastive Learning for Representing Terms via Explanations (CoRTEx) to\nenhance term representation and significantly improves term clustering.\nMaterials and Methods: The model training involves generating explanations for\na cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,\nconsidering term and explanation embeddings simultaneously, and progressively\nintroduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH\nalgorithm is designed for efficient clustering of a new ontology. Results: We\nestablished a clustering test set and a hard negative test set, where our model\nconsistently achieves the highest F1 score. With CoRTEx embeddings and the\nmodified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical\nInformatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries\nto ChatGPT. Case studies highlight the model's efficacy in handling challenging\nsamples, aided by information from explanations. Conclusion: By aligning terms\nto their explanations, CoRTEx demonstrates superior accuracy over benchmark\nmodels and robustness beyond its training set, and it is suitable for\nclustering terms for large-scale biomedical ontologies.",
        "pdf_link": "https://arxiv.org/pdf/2312.08036v1.pdf"
    },
    {
        "title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI",
        "authors": [
            "Kai Huang",
            "Boyuan Yang",
            "Wei Gao"
        ],
        "published": "2023-12-13T04:08:59Z",
        "summary": "Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.",
        "pdf_link": "https://arxiv.org/pdf/2312.07886v1.pdf"
    },
    {
        "title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications",
        "authors": [
            "Feibo Jiang",
            "Li Dong",
            "Yubo Peng",
            "Kezhi Wang",
            "Kun Yang",
            "Cunhua Pan",
            "Dusit Niyato",
            "Octavia A. Dobre"
        ],
        "published": "2023-12-13T02:35:57Z",
        "summary": "The rapid development of the Large Language Model (LLM) presents huge\nopportunities for 6G communications, e.g., network optimization and management\nby allowing users to input task requirements to LLMs by nature language.\nHowever, directly applying native LLMs in 6G encounters various challenges,\nsuch as a lack of private communication data and knowledge, limited logical\nreasoning, evaluation, and refinement abilities. Integrating LLMs with the\ncapabilities of retrieval, planning, memory, evaluation and reflection in\nagents can greatly enhance the potential of LLMs for 6G communications. To this\nend, we propose a multi-agent system with customized communication knowledge\nand tools for solving communication related tasks using natural language,\ncomprising three components: (1) Multi-agent Data Retrieval (MDR), which\nemploys the condensate and inference agents to refine and summarize\ncommunication knowledge from the knowledge base, expanding the knowledge\nboundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning\n(MCP), which utilizes multiple planning agents to generate feasible solutions\nfor the communication related task from different perspectives based on the\nretrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which\nutilizes the evaluation agent to assess the solutions, and applies the\nreflexion agent and refinement agent to provide improvement suggestions for\ncurrent solutions. Finally, we validate the effectiveness of the proposed\nmulti-agent system by designing a semantic communication system, as a case\nstudy of 6G communications.",
        "pdf_link": "https://arxiv.org/pdf/2312.07850v1.pdf"
    },
    {
        "title": "Tell, don't show: Declarative facts influence how LLMs generalize",
        "authors": [
            "Alexander Meinke",
            "Owain Evans"
        ],
        "published": "2023-12-12T22:47:42Z",
        "summary": "We examine how large language models (LLMs) generalize from abstract\ndeclarative statements in their training data. As an illustration, consider an\nLLM that is prompted to generate weather reports for London in 2050. One\npossibility is that the temperatures in the reports match the mean and variance\nof reports from 2023 (i.e. matching the statistics of pretraining). Another\npossibility is that the reports predict higher temperatures, by incorporating\ndeclarative statements about climate change from scientific papers written in\n2023. An example of such a declarative statement is \"global temperatures will\nincrease by $1^{\\circ} \\mathrm{C}$ by 2050\".\n  To test the influence of abstract declarative statements, we construct tasks\nin which LLMs are finetuned on both declarative and procedural information. We\nfind that declarative statements influence model predictions, even when they\nconflict with procedural information. In particular, finetuning on a\ndeclarative statement $S$ increases the model likelihood for logical\nconsequences of $S$. The effect of declarative statements is consistent across\nthree domains: aligning an AI assistant, predicting weather, and predicting\ndemographic features. Through a series of ablations, we show that the effect of\ndeclarative statements cannot be explained by associative learning based on\nmatching keywords. Nevertheless, the effect of declarative statements on model\nlikelihoods is small in absolute terms and increases surprisingly little with\nmodel size (i.e. from 330 million to 175 billion parameters). We argue that\nthese results have implications for AI risk (in relation to the \"treacherous\nturn\") and for fairness.",
        "pdf_link": "https://arxiv.org/pdf/2312.07779v1.pdf"
    },
    {
        "title": "Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization",
        "authors": [
            "Min Zhang",
            "Jianfeng He",
            "Shuo Lei",
            "Murong Yue",
            "Linhang Wang",
            "Chang-Tien Lu"
        ],
        "published": "2023-12-12T22:11:17Z",
        "summary": "The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.",
        "pdf_link": "https://arxiv.org/pdf/2312.07763v1.pdf"
    },
    {
        "title": "Large language models in healthcare and medical domain: A review",
        "authors": [
            "Zabir Al Nazi",
            "Wei Peng"
        ],
        "published": "2023-12-12T20:54:51Z",
        "summary": "The deployment of large language models (LLMs) within the healthcare sector\nhas sparked both enthusiasm and apprehension. These models exhibit the\nremarkable capability to provide proficient responses to free-text queries,\ndemonstrating a nuanced understanding of professional medical knowledge. This\ncomprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development,\nstarting from traditional Pretrained Language Models (PLMs) to the present\nstate of LLMs in healthcare sector. First, we explore the potential of LLMs to\namplify the efficiency and effectiveness of diverse healthcare applications,\nparticularly focusing on clinical language understanding tasks. These tasks\nencompass a wide spectrum, ranging from named entity recognition and relation\nextraction to natural language inference, multi-modal medical applications,\ndocument classification, and question-answering. Additionally, we conduct an\nextensive comparison of the most recent state-of-the-art LLMs in the healthcare\ndomain, while also assessing the utilization of various open-source LLMs and\nhighlighting their significance in healthcare applications. Furthermore, we\npresent the essential performance metrics employed to evaluate LLMs in the\nbiomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large\nlanguage models in the healthcare sector, offering a holistic perspective on\ntheir potential benefits and shortcomings. This review provides a comprehensive\nexploration of the current landscape of LLMs in healthcare, addressing their\nrole in transforming medical applications and the areas that warrant further\nresearch and development.",
        "pdf_link": "https://arxiv.org/pdf/2401.06775v1.pdf"
    },
    {
        "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
        "authors": [
            "Keivan Alizadeh",
            "Iman Mirzadeh",
            "Dmitry Belenko",
            "Karen Khatamifard",
            "Minsik Cho",
            "Carlo C Del Mundo",
            "Mohammad Rastegari",
            "Mehrdad Farajtabar"
        ],
        "published": "2023-12-12T18:57:08Z",
        "summary": "Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.",
        "pdf_link": "https://arxiv.org/pdf/2312.11514v2.pdf"
    },
    {
        "title": "Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection",
        "authors": [
            "Caoyun Fan",
            "Jidong Tian",
            "Yitian Li",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023-12-12T18:05:46Z",
        "summary": "In-Context Learning (ICL) is an important paradigm for adapting Large\nLanguage Models (LLMs) to downstream tasks through a few demonstrations.\nDespite the great success of ICL, the limitation of the demonstration number\nmay lead to demonstration bias, i.e. the input-label mapping induced by LLMs\nmisunderstands the task's essence. Inspired by human experience, we attempt to\nmitigate such bias through the perspective of the inter-demonstration\nrelationship. Specifically, we construct Comparable Demonstrations (CDs) by\nminimally editing the texts to flip the corresponding labels, in order to\nhighlight the task's essence and eliminate potential spurious correlations\nthrough the inter-demonstration comparison. Through a series of experiments on\nCDs, we find that (1) demonstration bias does exist in LLMs, and CDs can\nsignificantly reduce such bias; (2) CDs exhibit good performance in ICL,\nespecially in out-of-distribution scenarios. In summary, this study explores\nthe ICL mechanisms from a novel perspective, providing a deeper insight into\nthe demonstration selection strategy for ICL.",
        "pdf_link": "https://arxiv.org/pdf/2312.07476v2.pdf"
    },
    {
        "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
        "authors": [
            "Swanand Ravindra Kadhe",
            "Anisa Halimi",
            "Ambrish Rawat",
            "Nathalie Baracaldo"
        ],
        "published": "2023-12-12T16:44:47Z",
        "summary": "Training large language models (LLMs) is a costly endeavour in terms of time\nand computational resources. The large amount of training data used during the\nunsupervised pre-training phase makes it difficult to verify all data and,\nunfortunately, undesirable data may be ingested during training. Re-training\nfrom scratch is impractical and has led to the creation of the 'unlearning'\ndiscipline where models are modified to \"unlearn\" undesirable information\nwithout retraining. However, any modification can alter the behaviour of LLMs,\nespecially on key dimensions such as fairness. This is the first work that\nexamines this interplay between unlearning and fairness for LLMs. In\nparticular, we focus on a popular unlearning framework known as SISA [Bourtoule\net al., 2021], which creates an ensemble of models trained on disjoint shards.\nWe evaluate the performance-fairness trade-off for SISA, and empirically\ndemsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we\npropose post-processing bias mitigation techniques for ensemble models produced\nby SISA. We adapt the post-processing fairness improvement technique from\n[Hardt et al., 2016] to design three methods that can handle model ensembles,\nand prove that one of the methods is an optimal fair predictor for ensemble of\nmodels. Through experimental results, we demonstrate the efficacy of our\npost-processing framework called 'FairSISA'.",
        "pdf_link": "https://arxiv.org/pdf/2312.07420v1.pdf"
    },
    {
        "title": "ICL Markup: Structuring In-Context Learning using Soft-Token Tags",
        "authors": [
            "Marc-Etienne Brunet",
            "Ashton Anderson",
            "Richard Zemel"
        ],
        "published": "2023-12-12T16:25:05Z",
        "summary": "Large pretrained language models (LLMs) can be rapidly adapted to a wide\nvariety of tasks via a text-to-text approach, where the instruction and input\nare fed to the model in natural language. Combined with in-context learning\n(ICL), this paradigm is impressively flexible and powerful. However, it also\nburdens users with an overwhelming number of choices, many of them arbitrary.\nInspired by markup languages like HTML, we contribute a method of using\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\ndecisions and streamlines the application of ICL. Our method is a form of\nmeta-learning for ICL; it learns these tags in advance during a\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\nbe used in templates for ICL on new, unseen tasks without any additional\nfine-tuning. Our experiments with this approach yield promising initial\nresults, improving LLM performance on important enterprise applications such as\nfew-shot and open-world intent detection, as well as text classification in\nnews and legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2312.07405v1.pdf"
    },
    {
        "title": "On Diversified Preferences of Large Language Model Alignment",
        "authors": [
            "Dun Zeng",
            "Yong Dai",
            "Pengyu Cheng",
            "Tianhao Hu",
            "Wanshun Chen",
            "Nan Du",
            "Zenglin Xu"
        ],
        "published": "2023-12-12T16:17:15Z",
        "summary": "Aligning large language models (LLMs) with human preferences has been\nrecognized as the key to improving LLMs' interaction quality. However, in this\npluralistic world, human preferences can be diversified due to annotators'\ndifferent tastes, which hinders the effectiveness of LLM alignment methods.\nThis paper presents the first quantitative analysis of commonly used human\nfeedback datasets to investigate the impact of diversified preferences on\nreward modeling. Our analysis reveals a correlation between the calibration\nperformance of reward models (RMs) and the alignment performance of LLMs. We\nfind that diversified preference data negatively affect the calibration\nperformance of RMs on human-shared preferences, such as\n\\textit{Harmless\\&Helpful}, thereby impairing the alignment performance of\nLLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward\nlearning method (MORE) to enhance the calibration performance of RMs on shared\npreferences. We validate our findings by experiments on three models and five\nhuman preference datasets. Our method significantly improves the prediction\ncalibration of RMs, leading to better alignment of the Alpaca-7B model with\n\\textit{Harmless\\&Helpful} preferences. Furthermore, the connection between\nreward calibration and preference alignment performance suggests that\ncalibration error can be adopted as a key metric for evaluating RMs. The\nopen-source code and data are available at\n\\url{https://github.com/dunzeng/MORE}.",
        "pdf_link": "https://arxiv.org/pdf/2312.07401v3.pdf"
    },
    {
        "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
        "authors": [
            "Taeyoon Kwon",
            "Kai Tzu-iunn Ong",
            "Dongjin Kang",
            "Seungjun Moon",
            "Jeong Ryong Lee",
            "Dosik Hwang",
            "Yongsik Sim",
            "Beomseok Sohn",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "published": "2023-12-12T16:14:45Z",
        "summary": "Machine reasoning has made great progress in recent years owing to large\nlanguage models (LLMs). In the clinical domain, however, most NLP-driven\nprojects mainly focus on clinical classification or reading comprehension, and\nunder-explore clinical reasoning for disease diagnosis due to the expensive\nrationale annotation with clinicians. In this work, we present a\n``reasoning-aware'' diagnosis framework that rationalizes the diagnostic\nprocess via prompt-based learning in a time- and labor-efficient manner, and\nlearns to reason over the prompt-generated rationales. Specifically, we address\nthe clinical reasoning for disease diagnosis, where the LLM generates\ndiagnostic rationales providing its insight on presented patient data and the\nreasoning path towards the diagnosis, namely Clinical Chain-of-Thought\n(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical\nreasoning via extensive experiments and analyses on both rationale generation\nand disease diagnosis in various settings. We further propose a novel set of\ncriteria for evaluating machine-generated rationales' potential for real-world\nclinical settings, facilitating and benefiting future research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2312.07399v2.pdf"
    },
    {
        "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
        "authors": [
            "Pinelopi Papalampidi",
            "Skanda Koppula",
            "Shreya Pathak",
            "Justin Chiu",
            "Joe Heyward",
            "Viorica Patraucean",
            "Jiajun Shen",
            "Antoine Miech",
            "Andrew Zisserman",
            "Aida Nematzdeh"
        ],
        "published": "2023-12-12T16:10:19Z",
        "summary": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).",
        "pdf_link": "https://arxiv.org/pdf/2312.07395v1.pdf"
    },
    {
        "title": "Sequential Planning in Large Partially Observable Environments guided by LLMs",
        "authors": [
            "Swarna Kamal Paul"
        ],
        "published": "2023-12-12T15:36:59Z",
        "summary": "Sequential planning in large state space and action space quickly becomes\nintractable due to combinatorial explosion of the search space. Heuristic\nmethods, like monte-carlo tree search, though effective for large state space,\nbut struggle if action space is large. Pure reinforcement learning methods,\nrelying only on reward signals, needs prohibitively large interactions with the\nenvironment to device a viable plan. If the state space, observations and\nactions can be represented in natural language then Large Language models (LLM)\ncan be used to generate action plans. Recently several such goal-directed\nagents like Reflexion, CLIN, SayCan were able to surpass the performance of\nother state-of-the-art methods with minimum or no task specific training. But\nthey still struggle with exploration and get stuck in local optima. Their\nplanning capabilities are limited by the limited reasoning capability of the\nfoundational LLMs on text data. We propose a hybrid agent \"neoplanner\", that\nsynergizes both state space search with queries to foundational LLM to get the\nbest action plan. The reward signals are quantitatively used to drive the\nsearch. A balance of exploration and exploitation is maintained by maximizing\nupper confidence bounds of values of states. In places where random exploration\nis needed, the LLM is queried to generate an action plan. Learnings from each\ntrial are stored as entity relationships in text format. Those are used in\nfuture queries to the LLM for continual improvement. Experiments in the\nScienceworld environment reveals a 124% improvement from the current best\nmethod in terms of average reward gained across multiple tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.07368v1.pdf"
    },
    {
        "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks",
        "authors": [
            "Ahmed Salem",
            "Andrew Paverd",
            "Boris K\u00f6pf"
        ],
        "published": "2023-12-12T14:22:20Z",
        "summary": "Prompt injection has emerged as a serious security threat to large language\nmodels (LLMs). At present, the current best-practice for defending against\nnewly-discovered prompt injection techniques is to add additional guardrails to\nthe system (e.g., by updating the system prompt or using classifiers on the\ninput and/or output of the model.) However, in the same way that variants of a\npiece of malware are created to evade anti-virus software, variants of a prompt\ninjection can be created to evade the LLM's guardrails. Ideally, when a new\nprompt injection technique is discovered, candidate defenses should be tested\nnot only against the successful prompt injection, but also against possible\nvariants.\n  In this work, we present, a tool to assist defenders in performing automated\nvariant analysis of known prompt injection attacks. This involves solving two\nmain challenges: (1) automatically generating variants of a given prompt\naccording, and (2) automatically determining whether a variant was effective\nbased only on the output of the model. This tool can also assist in generating\ndatasets for jailbreak and prompt injection attacks, thus overcoming the\nscarcity of data in this domain.\n  We evaluate Maatphor on three different types of prompt injection tasks.\nStarting from an ineffective (0%) seed prompt, Maatphor consistently generates\nvariants that are at least 60% effective within the first 40 iterations.",
        "pdf_link": "https://arxiv.org/pdf/2312.11513v1.pdf"
    },
    {
        "title": "Multilingual large language models leak human stereotypes across language boundaries",
        "authors": [
            "Yang Trista Cao",
            "Anna Sotnikova",
            "Jieyu Zhao",
            "Linda X. Zou",
            "Rachel Rudinger",
            "Hal Daume III"
        ],
        "published": "2023-12-12T10:24:17Z",
        "summary": "Multilingual large language models have been increasingly popular for their\nproficiency in comprehending and generating text across various languages.\nPrevious research has shown that the presence of stereotypes and biases in\nmonolingual large language models can be attributed to the nature of their\ntraining data, which is collected from humans and reflects societal biases.\nMultilingual language models undergo the same training procedure as monolingual\nones, albeit with training data sourced from various languages. This raises the\nquestion: do stereotypes present in one social context leak across languages\nwithin the model? In our work, we first define the term ``stereotype leakage''\nand propose a framework for its measurement. With this framework, we\ninvestigate how stereotypical associations leak across four languages: English,\nRussian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an\napproach from social psychology, measuring stereotypes via group-trait\nassociations. We evaluate human stereotypes and stereotypical associations\nmanifested in multilingual large language models such as mBERT, mT5, and\nChatGPT. Our findings show a noticeable leakage of positive, negative, and\nnon-polar associations across all languages. Notably, Hindi within multilingual\nmodels appears to be the most susceptible to influence from other languages,\nwhile Chinese is the least. Additionally, ChatGPT exhibits a better alignment\nwith human scores than other models.",
        "pdf_link": "https://arxiv.org/pdf/2312.07141v1.pdf"
    },
    {
        "title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models",
        "authors": [
            "Yimo Deng",
            "Huangxun Chen"
        ],
        "published": "2023-12-12T10:04:43Z",
        "summary": "Text-to-image (TTI) models offer many innovative services but also raise\nethical concerns due to their potential to generate unethical images. Most\npublic TTI services employ safety filters to prevent unintended images. In this\nwork, we introduce the Divide-and-Conquer Attack to circumvent the safety\nfilters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our\nattack leverages LLMs as text transformation agents to create adversarial\nprompts. We design attack helper prompts that effectively guide LLMs to break\ndown an unethical drawing intent into multiple benign descriptions of\nindividual image elements, allowing them to bypass safety filters while still\ngenerating unethical images. Because the latent harmful meaning only becomes\napparent when all individual elements are drawn together. Our evaluation\ndemonstrates that our attack successfully circumvents multiple strong\nclosed-box safety filters. The comprehensive success rate of DACA bypassing the\nsafety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while\nthe success rate for bypassing Midjourney V6 exceeds 75%. Our findings have\nmore severe security implications than methods of manual crafting or iterative\nTTI model querying due to lower attack barrier, enhanced interpretability , and\nbetter adaptation to defense. Our prototype is available at:\nhttps://github.com/researchcode001/Divide-and-Conquer-Attack",
        "pdf_link": "https://arxiv.org/pdf/2312.07130v3.pdf"
    },
    {
        "title": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature",
        "authors": [
            "Maxime W\u00fcrsch",
            "Andrei Kucharavy",
            "Dimitri Percia David",
            "Alain Mermoud"
        ],
        "published": "2023-12-12T09:39:03Z",
        "summary": "The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.",
        "pdf_link": "https://arxiv.org/pdf/2312.07110v1.pdf"
    },
    {
        "title": "Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
        "authors": [
            "Xiang Li",
            "Haoran Tang",
            "Siyu Chen",
            "Ziwei Wang",
            "Anurag Maravi",
            "Marcin Abram"
        ],
        "published": "2023-12-12T08:43:20Z",
        "summary": "In this paper, we explore the challenges inherent to Large Language Models\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\nmistakes, and incorrect conclusions when tasked with answering complex\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\nsemantically rigorous manner further complicates the detection of factual\ninaccuracies. This issue is especially pronounced in fields that require\nspecialized expertise. Our work delves into these challenges, aiming to enhance\nthe understanding and mitigation of such errors, thereby contributing to the\nimprovement of LLM accuracy and reliability in scientific and other specialized\ndomains. Our findings reveal a non-linear relationship between the context's\nrelevancy and the answers' measured quality. In addition, we demonstrate that\nwith the correct calibration, it is possible to automate the grading procedure\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\nself-examine the quality of their own performance. Finally, we describe an\nexperimental platform that can be seen as a proof-of-concept of the techniques\ndescribed in this work.",
        "pdf_link": "https://arxiv.org/pdf/2312.07069v2.pdf"
    },
    {
        "title": "Improving Factual Error Correction by Learning to Inject Factual Errors",
        "authors": [
            "Xingwei He",
            "Qianru Zhang",
            "A-Long Jin",
            "Jun Ma",
            "Yuan Yuan",
            "Siu Ming Yiu"
        ],
        "published": "2023-12-12T08:02:06Z",
        "summary": "Factual error correction (FEC) aims to revise factual errors in false claims\nwith minimal editing, making them faithful to the provided evidence. This task\nis crucial for alleviating the hallucination problem encountered by large\nlanguage models. Given the lack of paired data (i.e., false claims and their\ncorresponding correct claims), existing methods typically adopt the\nmask-then-correct paradigm. This paradigm relies solely on unpaired false\nclaims and correct claims, thus being referred to as distantly supervised\nmethods. These methods require a masker to explicitly identify factual errors\nwithin false claims before revising with a corrector. However, the absence of\npaired data to train the masker makes accurately pinpointing factual errors\nwithin claims challenging. To mitigate this, we propose to improve FEC by\nLearning to Inject Factual Errors (LIFE), a three-step distantly supervised\nmethod: mask-corrupt-correct. Specifically, we first train a corruptor using\nthe mask-then-corrupt procedure, allowing it to deliberately introduce factual\nerrors into correct text. The corruptor is then applied to correct claims,\ngenerating a substantial amount of paired data. After that, we filter out\nlow-quality data, and use the remaining data to train a corrector. Notably, our\ncorrector does not require a masker, thus circumventing the bottleneck\nassociated with explicit factual error identification. Our experiments on a\npublic dataset verify the effectiveness of LIFE in two key aspects: Firstly, it\noutperforms the previous best-performing distantly supervised method by a\nnotable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,\neven compared to ChatGPT prompted with in-context examples, LIFE achieves a\nsuperiority of 7.16 points in SARI Final.",
        "pdf_link": "https://arxiv.org/pdf/2312.07049v1.pdf"
    },
    {
        "title": "Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models",
        "authors": [
            "Arnav Chavan",
            "Nahush Lele",
            "Deepak Gupta"
        ],
        "published": "2023-12-12T07:56:57Z",
        "summary": "Due to the substantial scale of Large Language Models (LLMs), the direct\napplication of conventional compression methodologies proves impractical. The\ncomputational demands associated with even minimal gradient updates present\nchallenges, particularly on consumer-grade hardware. This paper introduces an\ninnovative approach for the parametric and practical compression of LLMs based\non reduced order modelling, which entails low-rank decomposition within the\nfeature space and re-parameterization in the weight space. Notably, this\ncompression technique operates in a layer-wise manner, obviating the need for a\nGPU device and enabling the compression of billion-scale models within\nstringent constraints of both memory and time. Our method represents a\nsignificant advancement in model compression by leveraging matrix\ndecomposition, demonstrating superior efficacy compared to the prevailing\nstate-of-the-art structured pruning method.",
        "pdf_link": "https://arxiv.org/pdf/2312.07046v1.pdf"
    },
    {
        "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts",
        "authors": [
            "Giang Do",
            "Khiem Le",
            "Quang Pham",
            "TrungTin Nguyen",
            "Thanh-Nam Doan",
            "Bint T. Nguyen",
            "Chenghao Liu",
            "Savitha Ramasamy",
            "Xiaoli Li",
            "Steven Hoi"
        ],
        "published": "2023-12-12T07:40:23Z",
        "summary": "By routing input tokens to only a few split experts, Sparse\nMixture-of-Experts has enabled efficient training of large language models.\nRecent findings suggest that fixing the routers can achieve competitive\nperformance by alleviating the collapsing problem, where all experts eventually\nlearn similar representations. However, this strategy has two key limitations:\n(i) the policy derived from random routers might be sub-optimal, and (ii) it\nrequires extensive resources during training and evaluation, leading to limited\nefficiency gains. This work introduces \\HyperRout, which dynamically generates\nthe router's parameters through a fixed hypernetwork and trainable embeddings\nto achieve a balance between training the routers and freezing them to learn an\nimproved routing policy. Extensive experiments across a wide range of tasks\ndemonstrate the superior performance and efficiency gains of \\HyperRouter\ncompared to existing routing methods. Our implementation is publicly available\nat {\\url{{https://github.com/giangdip2410/HyperRouter}}}.",
        "pdf_link": "https://arxiv.org/pdf/2312.07035v1.pdf"
    },
    {
        "title": "Alignment for Honesty",
        "authors": [
            "Yuqing Yang",
            "Ethan Chern",
            "Xipeng Qiu",
            "Graham Neubig",
            "Pengfei Liu"
        ],
        "published": "2023-12-12T06:10:42Z",
        "summary": "Recent research has made significant strides in applying alignment techniques\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\naccordance with human intentions. In this paper, we argue for the importance of\nalignment for honesty, ensuring that LLMs proactively refuse to answer\nquestions when they lack knowledge, while still not being overly conservative.\nHowever, a pivotal aspect of alignment for honesty involves discerning the\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\ndemands comprehensive solutions in terms of metric development, benchmark\ncreation, and training methodologies. In this paper, we address these\nchallenges by first establishing a precise problem definition and defining\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\nfor developing metrics that effectively measure an LLM's honesty by quantifying\nits progress post-alignment. Furthermore, we introduce a flexible training\nframework which is further instantiated by several efficient fine-tuning\ntechniques that emphasize honesty without sacrificing performance on other\ntasks. Our extensive experiments reveal that these aligned models show a marked\nincrease in honesty, as indicated by our proposed metrics. We open-source a\nwealth of resources to facilitate future research at\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\nmodels, training and evaluation datasets for honesty alignment, concept\nglossary, as well as all relevant source code.",
        "pdf_link": "https://arxiv.org/pdf/2312.07000v1.pdf"
    },
    {
        "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model",
        "authors": [
            "Chaoya Jiang",
            "Haiyang Xu",
            "Mengfan Dong",
            "Jiaxing Chen",
            "Wei Ye",
            "Ming Yan",
            "Qinghao Ye",
            "Ji Zhang",
            "Fei Huang",
            "Shikun Zhang"
        ],
        "published": "2023-12-12T04:05:15Z",
        "summary": "Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA. Our code is available on\nhttps://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.",
        "pdf_link": "https://arxiv.org/pdf/2312.06968v4.pdf"
    },
    {
        "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack",
        "authors": [
            "Yu Fu",
            "Yufei Li",
            "Wen Xiao",
            "Cong Liu",
            "Yue Dong"
        ],
        "published": "2023-12-12T01:39:29Z",
        "summary": "Recent developments in balancing the usefulness and safety of Large Language\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\nadequately aligned with safety consideration? Our study, focusing on\nsafety-sensitive documents obtained through adversarial attacks, reveals\nsignificant disparities in the safety alignment of various NLP tasks. For\ninstance, LLMs can effectively summarize malicious long documents but often\nrefuse to translate them. This discrepancy highlights a previously unidentified\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\nsummarization, can potentially compromise the integraty of tasks traditionally\ndeemed more robust, such as translation and question-answering (QA). Moreover,\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\nGPT-4, indicating an urgent need for strengthening safety alignments across a\nbroad spectrum of NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.06924v1.pdf"
    },
    {
        "title": "Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks",
        "authors": [
            "Lingfeng Sun",
            "Devesh K. Jha",
            "Chiori Hori",
            "Siddarth Jain",
            "Radu Corcodel",
            "Xinghao Zhu",
            "Masayoshi Tomizuka",
            "Diego Romeres"
        ],
        "published": "2023-12-11T22:54:44Z",
        "summary": "Designing robotic agents to perform open vocabulary tasks has been the\nlong-standing goal in robotics and AI. Recently, Large Language Models (LLMs)\nhave achieved impressive results in creating robotic agents for performing open\nvocabulary tasks. However, planning for these tasks in the presence of\nuncertainties is challenging as it requires \\enquote{chain-of-thought}\nreasoning, aggregating information from the environment, updating state\nestimates, and generating actions based on the updated state estimates. In this\npaper, we present an interactive planning technique for partially observable\ntasks using LLMs. In the proposed method, an LLM is used to collect missing\ninformation from the environment using a robot and infer the state of the\nunderlying problem from collected observations while guiding the robot to\nperform the required actions. We also use a fine-tuned Llama 2 model via\nself-instruct and compare its performance against a pre-trained LLM like GPT-4.\nResults are demonstrated on several tasks in simulation as well as real-world\nenvironments. A video describing our work along with some results could be\nfound here.",
        "pdf_link": "https://arxiv.org/pdf/2312.06876v1.pdf"
    },
    {
        "title": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?",
        "authors": [
            "Shabaz Patel",
            "Hassan Kane",
            "Rayhan Patel"
        ],
        "published": "2023-12-11T18:59:09Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nnumerous natural language understanding use cases. However, this impressive\nperformance comes with inherent limitations, such as the tendency to perpetuate\nstereotypical biases or fabricate non-existent facts. In the context of Islam\nand its representation, accurate and factual representation of its beliefs and\nteachings rooted in the Quran and Sunnah is key. This work focuses on the\nchallenge of building domain-specific LLMs faithful to the Islamic worldview\nand proposes ways to build and evaluate such systems. Firstly, we define this\nopen-ended goal as a technical problem and propose various solutions.\nSubsequently, we critically examine known challenges inherent to each approach\nand highlight evaluation methodologies that can be used to assess such systems.\nThis work highlights the need for high-quality datasets, evaluations, and\ninterdisciplinary work blending machine learning with Islamic scholarship.",
        "pdf_link": "https://arxiv.org/pdf/2312.06652v1.pdf"
    },
    {
        "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
        "authors": [
            "Will E. Thompson",
            "David M. Vidmar",
            "Jessica K. De Freitas",
            "John M. Pfeifer",
            "Brandon K. Fornwalt",
            "Ruijun Chen",
            "Gabriel Altay",
            "Kabir Manghnani",
            "Andrew C. Nelsen",
            "Kellie Morland",
            "Martin C. Stumpe",
            "Riccardo Miotto"
        ],
        "published": "2023-12-11T15:45:27Z",
        "summary": "Identifying disease phenotypes from electronic health records (EHRs) is\ncritical for numerous secondary uses. Manually encoding physician knowledge\ninto rules is particularly challenging for rare diseases due to inadequate EHR\ncoding, necessitating review of clinical notes. Large language models (LLMs)\noffer promise in text understanding but may not efficiently handle real-world\nclinical documentation. We propose a zero-shot LLM-based method enriched by\nretrieval-augmented generation and MapReduce, which pre-identifies\ndisease-related text snippets to be used in parallel as queries for the LLM to\nestablish diagnosis. We show that this method as applied to pulmonary\nhypertension (PH), a rare disease characterized by elevated arterial pressures\nin the lungs, significantly outperforms physician logic rules ($F_1$ score of\n0.62 vs. 0.75). This method has the potential to enhance rare disease cohort\nidentification, expanding the scope of robust clinical research and care gap\nidentification.",
        "pdf_link": "https://arxiv.org/pdf/2312.06457v1.pdf"
    },
    {
        "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
        "authors": [
            "Tao Chen",
            "Enwei Zhang",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun",
            "Yan Zhang",
            "Hui Li"
        ],
        "published": "2023-12-11T13:11:04Z",
        "summary": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.",
        "pdf_link": "https://arxiv.org/pdf/2312.06363v2.pdf"
    },
    {
        "title": "Linguistic and Structural Basis of Engineering Design Knowledge",
        "authors": [
            "L. Siddharth",
            "Jianxi Luo"
        ],
        "published": "2023-12-11T13:03:39Z",
        "summary": "Artefact descriptions are the primary carriers of engineering design\nknowledge that is both an outcome and a driver of the design process. While an\nartefact could be described in different connotations, the design process\nrequires a description to embody engineering design knowledge, which is\nexpressed in the text through intricate placement of entities and\nrelationships. As large-language models learn from all kinds of text merely as\na sequence of characters/tokens, these are yet to generate text that embodies\nexplicit engineering design facts. Existing ontological design theories are\nless likely to guide the large-language models whose applications are currently\nlimited to ideation and learning purposes. In this article, we explicate\nengineering design knowledge as knowledge graphs from a large sample of 33,881\npatent documents. We examine the constituents of these knowledge graphs to\nunderstand the linguistic and structural basis of engineering design knowledge.\nIn terms of linguistic basis, we observe that entities and relationships could\nbe generalised to 64 and 24 linguistic syntaxes. While relationships mainly\ncapture attributes ('of'), structure ('in', 'with'), purpose ('to', 'for'),\nhierarchy ('include'), exemplification ('such as'), and behaviour ('to',\n'from'), the hierarchical relationships could specifically be identified using\n75 unique syntaxes. To understand the structural basis, we draw inspiration\nfrom various studies on biological/ecological networks and discover motifs from\npatent knowledge graphs. We identify four 3-node and four 4-node patterns that\ncould further be converged and simplified into sequence [->...->], aggregation\n[->...<-], and hierarchy [<-...->]. Expected to guide large-language model\nbased design tools, we propose few regulatory precepts for concretising\nabstract entities and relationships within subgraphs, while explicating\nhierarchical structures.",
        "pdf_link": "https://arxiv.org/pdf/2312.06355v2.pdf"
    },
    {
        "title": "Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models",
        "authors": [
            "Yubin Wang",
            "Xinyang Jiang",
            "De Cheng",
            "Dongsheng Li",
            "Cairong Zhao"
        ],
        "published": "2023-12-11T12:14:06Z",
        "summary": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models to downstream tasks. As large language models (LLMs) have\nemerged, recent studies have explored the use of category-related descriptions\nas input to enhance prompt effectiveness. Nevertheless, conventional\ndescriptions fall short of structured information that effectively represents\nthe interconnections among entities or attributes linked to a particular\ncategory. To address this limitation and prioritize harnessing structured\nknowledge, this paper advocates for leveraging LLMs to build a graph for each\ndescription to model the entities and attributes describing the category, as\nwell as their correlations. Preexisting prompt tuning methods exhibit\ninadequacies in managing this structured knowledge. Consequently, we propose a\nnovel approach called Hierarchical Prompt Tuning (HPT), which enables\nsimultaneous modeling of both structured and conventional linguistic knowledge.\nSpecifically, we introduce a relationship-guided attention module to capture\npair-wise associations among entities and attributes for low-level prompt\nlearning. In addition, by incorporating high-level and global-level prompts\nmodeling overall semantics, the proposed hierarchical structure forges\ncross-level interlinks and empowers the model to handle more complex and\nlong-term relationships. Extensive experiments demonstrate that our HPT shows\nstrong effectiveness and generalizes much better than existing SOTA methods.\nOur code is available at https://github.com/Vill-Lab/2024-AAAI-HPT.",
        "pdf_link": "https://arxiv.org/pdf/2312.06323v1.pdf"
    },
    {
        "title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Shirui Pan",
            "Wenpeng Yin",
            "Mykola Pechenizkiy"
        ],
        "published": "2023-12-11T12:02:14Z",
        "summary": "Warning: This paper contains content that may be offensive or upsetting.\nThere has been a significant increase in the usage of large language models\n(LLMs) in various applications, both in their original form and through\nfine-tuned adaptations. As a result, LLMs have gained popularity and are being\nwidely adopted by a large user community. However, one of the concerns with\nLLMs is the potential generation of socially biased content. The existing\nevaluation methods have many constraints, and their results exhibit a limited\ndegree of interpretability. In this work, we propose a bias evaluation\nframework named GPTBIAS that leverages the high performance of LLMs (e.g.,\nGPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce\nprompts called Bias Attack Instructions, which are specifically designed for\nevaluating model bias. To enhance the credibility and interpretability of bias\nevaluation, our framework not only provides a bias score but also offers\ndetailed information, including bias types, affected demographics, keywords,\nreasons behind the biases, and suggestions for improvement. We conduct\nextensive experiments to demonstrate the effectiveness and usability of our\nbias evaluation framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.06315v1.pdf"
    },
    {
        "title": "KnowGPT: Knowledge Injection for Large Language Models",
        "authors": [
            "Qinggang Zhang",
            "Junnan Dong",
            "Hao Chen",
            "Daochen Zha",
            "Zailiang Yu",
            "Xiao Huang"
        ],
        "published": "2023-12-11T07:56:25Z",
        "summary": "Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.06185v4.pdf"
    },
    {
        "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
        "authors": [
            "Lifu Tu",
            "Semih Yavuz",
            "Jin Qu",
            "Jiacheng Xu",
            "Rui Meng",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-12-11T06:35:33Z",
        "summary": "Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).",
        "pdf_link": "https://arxiv.org/pdf/2312.06149v2.pdf"
    },
    {
        "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
        "authors": [
            "Heegyu Kim",
            "Hyunsouk Cho"
        ],
        "published": "2023-12-11T05:04:17Z",
        "summary": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. The fast-paced evolution of generative language models such as\nGPT-4 has demonstrated outstanding results in various NLP generation tasks.\nHowever, due to the potential generation of offensive words related to race or\ngender, various Controllable Text Generation (CTG) methods have been proposed\nto mitigate the occurrence of harmful words. However, existing CTG methods not\nonly reduce toxicity but also negatively impact several aspects of the language\nmodel's generation performance, including topic consistency, grammar, and\nperplexity. This paper explores the limitations of previous methods and\nintroduces a novel solution in the form of a simple Gated Toxicity Avoidance\n(GTA) that can be applied to any CTG method. We also evaluate the effectiveness\nof the proposed GTA by comparing it with state-of-the-art CTG methods across\nvarious datasets. Our findings reveal that gated toxicity avoidance efficiently\nachieves comparable levels of toxicity reduction to the original CTG methods\nwhile preserving the generation performance of the language model.",
        "pdf_link": "https://arxiv.org/pdf/2312.06122v1.pdf"
    },
    {
        "title": "Can LLMs Configure Software Tools",
        "authors": [
            "Jai Kannan"
        ],
        "published": "2023-12-11T05:03:02Z",
        "summary": "In software engineering, the meticulous configuration of software tools is\ncrucial in ensuring optimal performance within intricate systems. However, the\ncomplexity inherent in selecting optimal configurations is exacerbated by the\nhigh-dimensional search spaces presented in modern applications. Conventional\ntrial-and-error or intuition-driven methods are both inefficient and\nerror-prone, impeding scalability and reproducibility. In this study, we embark\non an exploration of leveraging Large-Language Models (LLMs) to streamline the\nsoftware configuration process. We identify that the task of hyperparameter\nconfiguration for machine learning components within intelligent applications\nis particularly challenging due to the extensive search space and\nperformance-critical nature. Existing methods, including Bayesian optimization,\nhave limitations regarding initial setup, computational cost, and convergence\nefficiency. Our work presents a novel approach that employs LLMs, such as\nChat-GPT, to identify starting conditions and narrow down the search space,\nimproving configuration efficiency. We conducted a series of experiments to\ninvestigate the variability of LLM-generated responses, uncovering intriguing\nfindings such as potential response caching and consistent behavior based on\ndomain-specific keywords. Furthermore, our results from hyperparameter\noptimization experiments reveal the potential of LLMs in expediting\ninitialization processes and optimizing configurations. While our initial\ninsights are promising, they also indicate the need for further in-depth\ninvestigations and experiments in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2312.06121v1.pdf"
    },
    {
        "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
        "authors": [
            "Zhaoxuan Tan",
            "Meng Jiang"
        ],
        "published": "2023-12-11T03:59:36Z",
        "summary": "User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading",
        "pdf_link": "https://arxiv.org/pdf/2312.11518v2.pdf"
    },
    {
        "title": "EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models",
        "authors": [
            "Yi Chen",
            "Yuying Ge",
            "Yixiao Ge",
            "Mingyu Ding",
            "Bohao Li",
            "Rui Wang",
            "Ruifeng Xu",
            "Ying Shan",
            "Xihui Liu"
        ],
        "published": "2023-12-11T03:35:58Z",
        "summary": "Multimodal Large Language Models (MLLMs), building upon the powerful Large\nLanguage Models (LLMs) with exceptional reasoning and generalization\ncapability, have opened up new avenues for embodied task planning. MLLMs excel\nin their ability to integrate diverse environmental inputs, such as real-time\ntask progress, visual observations, and open-form language instructions, which\nare crucial for executable task planning. In this work, we introduce a\nbenchmark with human annotations, EgoPlan-Bench, to quantitatively investigate\nthe potential of MLLMs as embodied task planners in real-world scenarios. Our\nbenchmark is distinguished by realistic tasks derived from real-world videos, a\ndiverse set of actions involving interactions with hundreds of different\nobjects, and complex visual observations from varied environments. We evaluate\nvarious open-source MLLMs, revealing that these models have not yet evolved\ninto embodied planning generalists (even GPT-4V). We further construct an\ninstruction-tuning dataset EgoPlan-IT from videos of human-object interactions,\nto facilitate the learning of high-level task planning in intricate real-world\nsituations. The experiment results demonstrate that the model tuned on\nEgoPlan-IT not only significantly improves performance on our benchmark, but\nalso effectively acts as embodied planner in simulations.",
        "pdf_link": "https://arxiv.org/pdf/2312.06722v1.pdf"
    },
    {
        "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities",
        "authors": [
            "Sangwon Hyun",
            "Mingyu Guo",
            "M. Ali Babar"
        ],
        "published": "2023-12-11T01:29:19Z",
        "summary": "Large-Language Models (LLMs) have shifted the paradigm of natural language\ndata processing. However, their black-boxed and probabilistic characteristics\ncan lead to potential risks in the quality of outputs in diverse LLM\napplications. Recent studies have tested Quality Attributes (QAs), such as\nrobustness or fairness, of LLMs by generating adversarial input texts. However,\nexisting studies have limited their coverage of QAs and tasks in LLMs and are\ndifficult to extend. Additionally, these studies have only used one evaluation\nmetric, Attack Success Rate (ASR), to assess the effectiveness of their\napproaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL)\nframework to address these issues by applying Metamorphic Testing (MT)\ntechniques. This approach facilitates the systematic testing of LLM qualities\nby defining Metamorphic Relations (MRs), which serve as modularized evaluation\nmetrics. The METAL framework can automatically generate hundreds of MRs from\ntemplates that cover various QAs and tasks. In addition, we introduced novel\nmetrics that integrate the ASR method into the semantic qualities of text to\nassess the effectiveness of MRs accurately. Through the experiments conducted\nwith three prominent LLMs, we have confirmed that the METAL framework\neffectively evaluates essential QAs on primary LLM tasks and reveals the\nquality risks in LLMs. Moreover, the newly proposed metrics can guide the\noptimal MRs for testing each task and suggest the most effective method for\ngenerating MRs.",
        "pdf_link": "https://arxiv.org/pdf/2312.06056v1.pdf"
    },
    {
        "title": "Large Language Models on Lexical Semantic Change Detection: An Evaluation",
        "authors": [
            "Ruiyu Wang",
            "Matthew Choi"
        ],
        "published": "2023-12-10T21:26:35Z",
        "summary": "Lexical Semantic Change Detection stands out as one of the few areas where\nLarge Language Models (LLMs) have not been extensively involved. Traditional\nmethods like PPMI, and SGNS remain prevalent in research, alongside newer\nBERT-based approaches. Despite the comprehensive coverage of various natural\nlanguage processing domains by LLMs, there is a notable scarcity of literature\nconcerning their application in this specific realm. In this work, we seek to\nbridge this gap by introducing LLMs into the domain of Lexical Semantic Change\nDetection. Our work presents novel prompting solutions and a comprehensive\nevaluation that spans all three generations of language models, contributing to\nthe exploration of LLMs in this research area.",
        "pdf_link": "https://arxiv.org/pdf/2312.06002v1.pdf"
    },
    {
        "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
        "authors": [
            "Oded Ovadia",
            "Menachem Brief",
            "Moshik Mishaeli",
            "Oren Elisha"
        ],
        "published": "2023-12-10T16:52:00Z",
        "summary": "Large language models (LLMs) encapsulate a vast amount of factual information\nwithin their pre-trained weights, as evidenced by their ability to answer\ndiverse questions across different domains. However, this knowledge is\ninherently limited, relying heavily on the characteristics of the training\ndata. Consequently, using external datasets to incorporate new information or\nrefine the capabilities of LLMs on previously seen information poses a\nsignificant challenge. In this study, we compare two common approaches:\nunsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate\nboth approaches on a variety of knowledge-intensive tasks across different\ntopics. Our findings reveal that while unsupervised fine-tuning offers some\nimprovement, RAG consistently outperforms it, both for existing knowledge\nencountered during training and entirely new knowledge. Moreover, we find that\nLLMs struggle to learn new factual information through unsupervised\nfine-tuning, and that exposing them to numerous variations of the same fact\nduring training could alleviate this problem.",
        "pdf_link": "https://arxiv.org/pdf/2312.05934v3.pdf"
    },
    {
        "title": "Mutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer",
        "authors": [
            "Yongheng Deng",
            "Ziqing Qiao",
            "Ju Ren",
            "Yang Liu",
            "Yaoxue Zhang"
        ],
        "published": "2023-12-10T09:52:32Z",
        "summary": "While large language models (LLMs) are empowered with broad knowledge, their\ntask-specific performance is often suboptimal. It necessitates fine-tuning LLMs\nwith task-specific data, but such data may be inaccessible due to privacy\nconcerns. In this paper, we propose a novel approach to enhance LLMs with\nsmaller language models (SLMs) that are trained on clients using their private\ntask-specific data. To enable mutual enhancement between LLMs and SLMs, we\npropose CrossLM, where the SLMs promote the LLM to generate task-specific\nhigh-quality data, and both the LLM and SLMs are enhanced with the generated\ndata. We evaluate CrossLM using publicly accessible language models across a\nrange of benchmark tasks. The results demonstrate that CrossLM significantly\nenhances the task-specific performance of SLMs on clients and the LLM on the\ncloud server simultaneously while preserving the LLM's generalization\ncapability.",
        "pdf_link": "https://arxiv.org/pdf/2312.05842v1.pdf"
    },
    {
        "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
        "authors": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Yue Song",
            "Qiang Wu",
            "Yan Yan",
            "Guangyu Sun"
        ],
        "published": "2023-12-10T08:41:24Z",
        "summary": "This paper explores a new post-hoc training-free compression paradigm for\ncompressing Large Language Models (LLMs) to facilitate their wider adoption in\nvarious computing environments. We delve into the challenges of LLM\ncompression, notably their dependency on extensive training data and\ncomputational resources. We propose a training-free approach dubbed\nActivation-aware Singular Value Decomposition (ASVD) to address these\nlimitations. ASVD effectively manages activation outliers by adjusting the\nweight matrix based on the activation distribution, improving decomposition\naccuracy and efficiency. Our method also addresses the varying sensitivity of\ndifferent LLM layers to decomposition, with an iterative calibration process\nfor optimal layer-specific decomposition. Experiments demonstrate that ASVD can\ncompress network by 10%-20% without losing reasoning capacities. Additionally,\nit can be seamlessly integrated with other LLM compression paradigms,\nshowcasing its flexible compatibility. Code and compressed models are available\nat https://github.com/hahnyuan/ASVD4LLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.05821v1.pdf"
    },
    {
        "title": "Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup",
        "authors": [
            "Maolin Wang",
            "Yao Zhao",
            "Jiajia Liu",
            "Jingdong Chen",
            "Chenyi Zhuang",
            "Jinjie Gu",
            "Ruocheng Guo",
            "Xiangyu Zhao"
        ],
        "published": "2023-12-10T06:57:48Z",
        "summary": "The deployment of Large Multimodal Models (LMMs) within AntGroup has\nsignificantly advanced multimodal tasks in payment, security, and advertising,\nnotably enhancing advertisement audition tasks in Alipay. However, the\ndeployment of such sizable models introduces challenges, particularly in\nincreased latency and carbon emissions, which are antithetical to the ideals of\nGreen AI. This paper introduces a novel multi-stage compression strategy for\nour proprietary LLM, AntGMM. Our methodology pivots on three main aspects:\nemploying small training sample sizes, addressing multi-level redundancy\nthrough multi-stage pruning, and introducing an advanced distillation loss\ndesign. In our research, we constructed a dataset, the Multimodal Advertisement\nAudition Dataset (MAAD), from real-world scenarios within Alipay, and conducted\nexperiments to validate the reliability of our proposed strategy. Furthermore,\nthe effectiveness of our strategy is evident in its operational success in\nAlipay's real-world multimodal advertisement audition for three months from\nSeptember 2023. Notably, our approach achieved a substantial reduction in\nlatency, decreasing it from 700ms to 90ms, while maintaining online performance\nwith only a slight performance decrease. Moreover, our compressed model is\nestimated to reduce electricity consumption by approximately 75 million kWh\nannually compared to the direct deployment of AntGMM, demonstrating our\ncommitment to green AI initiatives. We will publicly release our code and the\nMAAD dataset after some\nreviews\\footnote{https://github.com/MorinW/AntGMM$\\_$Pruning}.",
        "pdf_link": "https://arxiv.org/pdf/2312.05795v1.pdf"
    },
    {
        "title": "Context Tuning for Retrieval Augmented Generation",
        "authors": [
            "Raviteja Anantha",
            "Tharun Bethi",
            "Danil Vodianik",
            "Srinivas Chappidi"
        ],
        "published": "2023-12-09T23:33:16Z",
        "summary": "Large language models (LLMs) have the remarkable ability to solve new tasks\nwith just a few examples, but they need access to the right tools. Retrieval\nAugmented Generation (RAG) addresses this problem by retrieving a list of\nrelevant tools for a given task. However, RAG's tool retrieval step requires\nall the required information to be explicitly present in the query. This is a\nlimitation, as semantic search, the widely adopted tool retrieval method, can\nfail when the query is incomplete or lacks context. To address this limitation,\nwe propose Context Tuning for RAG, which employs a smart context retrieval\nsystem to fetch relevant information that improves both tool retrieval and plan\ngeneration. Our lightweight context retrieval model uses numerical,\ncategorical, and habitual usage signals to retrieve and rank context items. Our\nempirical results demonstrate that context tuning significantly enhances\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\nplan generation, even after tool retrieval, reduces hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2312.05708v1.pdf"
    },
    {
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": [
            "Gustavo Gon\u00e7alves",
            "Emma Strubell"
        ],
        "published": "2023-12-09T20:04:20Z",
        "summary": "Large Language Models (LLMs) trained with self-supervision on vast corpora of\nweb text fit to the social biases of that text. Without intervention, these\nsocial biases persist in the model's predictions in downstream tasks, leading\nto representational harm. Many strategies have been proposed to mitigate the\neffects of inappropriate social biases learned during pretraining.\nSimultaneously, methods for model compression have become increasingly popular\nto reduce the computational burden of LLMs. Despite the popularity and need for\nboth approaches, little work has been done to explore the interplay between\nthese two. We perform a carefully controlled study of the impact of model\ncompression via quantization and knowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\nand quantization showed a regularizer effect with its best trade-off around 20%\nof the original pretraining time.",
        "pdf_link": "https://arxiv.org/pdf/2312.05662v2.pdf"
    },
    {
        "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
        "authors": [
            "Rumeng Li",
            "Xun Wang",
            "Hong Yu"
        ],
        "published": "2023-12-09T19:35:40Z",
        "summary": "Large language models (LLMs) can generate natural language texts for various\ndomains and tasks, but their potential for clinical text mining, a domain with\nscarce, sensitive, and imbalanced medical data, is underexplored. We\ninvestigate whether LLMs can augment clinical data for detecting Alzheimer's\nDisease (AD)-related signs and symptoms from electronic health records (EHRs),\na challenging task that requires high expertise. We create a novel pragmatic\ntaxonomy for AD sign and symptom progression based on expert knowledge, which\nguides LLMs to generate synthetic data following two different directions:\n\"data-to-label\", which labels sentences from a public EHR collection with\nAD-related signs and symptoms; and \"label-to-data\", which generates sentences\nwith AD-related signs and symptoms based on the label definition. We train a\nsystem to detect AD-related signs and symptoms from EHRs, using three datasets:\n(1) a gold dataset annotated by human experts on longitudinal EHRs of AD\npatients; (2) a silver dataset created by the data-to-label method; and (3) a\nbronze dataset created by the label-to-data method. We find that using the\nsilver and bronze datasets improves the system performance, outperforming the\nsystem using only the gold dataset. This shows that LLMs can generate synthetic\nclinical data for a complex task by incorporating expert knowledge, and our\nlabel-to-data method can produce datasets that are free of sensitive\ninformation, while maintaining acceptable quality.",
        "pdf_link": "https://arxiv.org/pdf/2401.06774v1.pdf"
    },
    {
        "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
        "authors": [
            "Zhenting Qi",
            "Xiaoyu Tan",
            "Shaojie Shi",
            "Chao Qu",
            "Yinghui Xu",
            "Yuan Qi"
        ],
        "published": "2023-12-09T17:38:39Z",
        "summary": "Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.",
        "pdf_link": "https://arxiv.org/pdf/2312.05621v1.pdf"
    },
    {
        "title": "Sim-GPT: Text Similarity via GPT Annotated Data",
        "authors": [
            "Shuhe Wang",
            "Beiming Cao",
            "Shengyu Zhang",
            "Xiaoya Li",
            "Jiwei Li",
            "Fei Wu",
            "Guoyin Wang",
            "Eduard Hovy"
        ],
        "published": "2023-12-09T16:10:23Z",
        "summary": "Due to the lack of a large collection of high-quality labeled sentence pairs\nwith textual similarity scores, existing approaches for Semantic Textual\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\nthat are only partially correlated with textual similarity, e.g., NLI-based\ndatasets. To tackle this issue, in this paper, we propose the strategy of\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\nsubstantial amount of reliable annotated data filling the gap of the lack of\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\nthe current SOTA PromCSE model. To encourage further advancements of the field,\nwe release both models and the 371K annotated examples from GPT-4. Code, models\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2312.05603v2.pdf"
    },
    {
        "title": "Enhancing Medical Specialty Assignment to Patients using NLP Techniques",
        "authors": [
            "Chris Solomou"
        ],
        "published": "2023-12-09T14:13:45Z",
        "summary": "The introduction of Large Language Models (LLMs), and the vast volume of\npublicly available medical data, amplified the application of NLP to the\nmedical domain. However, LLMs are pretrained on data that are not explicitly\nrelevant to the domain that are applied to and are often biased towards the\noriginal data they were pretrained upon. Even when pretrained on domainspecific\ndata, these models typically require time-consuming fine-tuning to achieve good\nperformance for a specific task. To address these limitations, we propose an\nalternative approach that achieves superior performance while being\ncomputationally efficient. Specifically, we utilize keywords to train a deep\nlearning architecture that outperforms a language model pretrained on a large\ncorpus of text. Our proposal does not require pretraining nor fine-tuning and\ncan be applied directly to a specific setting for performing multi-label\nclassification. Our objective is to automatically assign a new patient to the\nspecialty of the medical professional they require, using a dataset that\ncontains medical transcriptions and relevant keywords. To this end, we\nfine-tune the PubMedBERT model on this dataset, which serves as the baseline\nfor our experiments. We then twice train/fine-tune a DNN and the RoBERTa\nlanguage model, using both the keywords and the full transcriptions as input.\nWe compare the performance of these approaches using relevant metrics. Our\nresults demonstrate that utilizing keywords for text classification\nsignificantly improves classification performance, for both a basic DL\narchitecture and a large language model. Our approach represents a promising\nand efficient alternative to traditional methods for finetuning language models\non domain-specific data and has potential applications in various medical\ndomains",
        "pdf_link": "https://arxiv.org/pdf/2312.05585v1.pdf"
    },
    {
        "title": "ESPN: Memory-Efficient Multi-Vector Information Retrieval",
        "authors": [
            "Susav Shrestha",
            "Narasimha Reddy",
            "Zongwang Li"
        ],
        "published": "2023-12-09T00:19:42Z",
        "summary": "Recent advances in large language models have demonstrated remarkable\neffectiveness in information retrieval (IR) tasks. While many neural IR systems\nencode queries and documents into single-vector representations, multi-vector\nmodels elevate the retrieval quality by producing multi-vector representations\nand facilitating similarity searches at the granularity of individual tokens.\nHowever, these models significantly amplify memory and storage requirements for\nretrieval indices by an order of magnitude. This escalation in index size\nrenders the scalability of multi-vector IR models progressively challenging due\nto their substantial memory demands. We introduce Embedding from Storage\nPipelined Network (ESPN) where we offload the entire re-ranking embedding\ntables to SSDs and reduce the memory requirements by 5-16x. We design a\nsoftware prefetcher with hit rates exceeding 90%, improving SSD based retrieval\nup to 6.4x, and demonstrate that we can maintain near memory levels of query\nlatency even for large query batch sizes.",
        "pdf_link": "https://arxiv.org/pdf/2312.05417v1.pdf"
    },
    {
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
        "authors": [
            "Jakub L\u00e1la",
            "Odhran O'Donoghue",
            "Aleksandar Shtedritski",
            "Sam Cox",
            "Samuel G. Rodriques",
            "Andrew D. White"
        ],
        "published": "2023-12-08T18:50:20Z",
        "summary": "Large Language Models (LLMs) generalize well across language tasks, but\nsuffer from hallucinations and uninterpretability, making it difficult to\nassess their accuracy without ground-truth. Retrieval-Augmented Generation\n(RAG) models have been proposed to reduce hallucinations and provide provenance\nfor how an answer was generated. Applying such models to the scientific\nliterature may enable large-scale, systematic processing of scientific\nknowledge. We present PaperQA, a RAG agent for answering questions over the\nscientific literature. PaperQA is an agent that performs information retrieval\nacross full-text scientific articles, assesses the relevance of sources and\npassages, and uses RAG to provide answers. Viewing this agent as a question\nanswering model, we find it exceeds performance of existing LLMs and LLM agents\non current science QA benchmarks. To push the field closer to how humans\nperform research on scientific literature, we also introduce LitQA, a more\ncomplex benchmark that requires retrieval and synthesis of information from\nfull-text scientific papers across the literature. Finally, we demonstrate\nPaperQA's matches expert human researchers on LitQA.",
        "pdf_link": "https://arxiv.org/pdf/2312.07559v2.pdf"
    },
    {
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "published": "2023-12-08T18:25:22Z",
        "summary": "Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.05230v1.pdf"
    },
    {
        "title": "DeltaZip: Multi-Tenant Language Model Serving via Delta Compression",
        "authors": [
            "Xiaozhe Yao",
            "Ana Klimovic"
        ],
        "published": "2023-12-08T18:07:05Z",
        "summary": "Fine-tuning large language models (LLMs) for downstream tasks can greatly\nimprove model quality, however serving many different fine-tuned LLMs\nconcurrently for users in multi-tenant environments is challenging. Dedicating\nGPU memory for each model is prohibitively expensive and naively swapping large\nmodel weights in and out of GPU memory is slow. Our key insight is that\nfine-tuned models can be quickly swapped in and out of GPU memory by extracting\nand compressing the delta between each model and its pre-trained base model. We\npropose DeltaZip, an LLM serving system that efficiently serves multiple\nfull-parameter fine-tuned models concurrently by aggressively compressing model\ndeltas by a factor of $6\\times$ to $8\\times$ while maintaining high model\nquality. DeltaZip increases serving throughput by $1.5\\times$ to $3\\times$ and\nimproves SLO attainment compared to a vanilla HuggingFace serving system.",
        "pdf_link": "https://arxiv.org/pdf/2312.05215v1.pdf"
    },
    {
        "title": "HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models",
        "authors": [
            "Navapat Nananukul",
            "Mayank Kejriwal"
        ],
        "published": "2023-12-08T17:57:20Z",
        "summary": "Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.",
        "pdf_link": "https://arxiv.org/pdf/2312.05209v2.pdf"
    },
    {
        "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering",
        "authors": [
            "Mobashir Sadat",
            "Zhengyu Zhou",
            "Lukas Lange",
            "Jun Araki",
            "Arsalan Gundroo",
            "Bingqing Wang",
            "Rakesh R Menon",
            "Md Rizwan Parvez",
            "Zhe Feng"
        ],
        "published": "2023-12-08T17:41:06Z",
        "summary": "Hallucination is a well-known phenomenon in text generated by large language\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\napplication scenarios e.g., summarization, question-answering (QA) etc. For\napplications requiring high reliability (e.g., customer-facing assistants), the\npotential existence of hallucination in LLM-generated text is a critical\nproblem. The amount of hallucination can be reduced by leveraging information\nretrieval to provide relevant background information to the LLM. However, LLMs\ncan still generate hallucinatory content for various reasons (e.g.,\nprioritizing its parametric knowledge over the context, failure to capture the\nrelevant information from the context, etc.). Detecting hallucinations through\nautomated methods is thus paramount. To facilitate research in this direction,\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\npropose a set of hallucination detection methods to serve as baselines for\nfuture works from the research community. Analysis and case study are also\nprovided to share valuable insights on hallucination phenomena in the target\nscenario.",
        "pdf_link": "https://arxiv.org/pdf/2312.05200v1.pdf"
    },
    {
        "title": "Assessing LLMs for Moral Value Pluralism",
        "authors": [
            "Noam Benkler",
            "Drisana Mosaphir",
            "Scott Friedman",
            "Andrew Smart",
            "Sonja Schmer-Galunder"
        ],
        "published": "2023-12-08T16:18:15Z",
        "summary": "The fields of AI current lacks methods to quantitatively assess and\npotentially alter the moral values inherent in the output of large language\nmodels (LLMs). However, decades of social science research has developed and\nrefined widely-accepted moral value surveys, such as the World Values Survey\n(WVS), eliciting value judgments from direct questions in various geographies.\nWe have turned those questions into value statements and use NLP to compute to\nhow well popular LLMs are aligned with moral values for various demographics\nand cultures. While the WVS is accepted as an explicit assessment of values, we\nlack methods for assessing implicit moral and cultural values in media, e.g.,\nencountered in social media, political rhetoric, narratives, and generated by\nAI systems such as LLMs that are increasingly present in our daily lives. As we\nconsume online content and utilize LLM outputs, we might ask, which moral\nvalues are being implicitly promoted or undercut, or -- in the case of LLMs --\nif they are intending to represent a cultural identity, are they doing so\nconsistently? In this paper we utilize a Recognizing Value Resonance (RVR) NLP\nmodel to identify WVS values that resonate and conflict with a given passage of\noutput text. We apply RVR to the text generated by LLMs to characterize\nimplicit moral values, allowing us to quantify the moral/cultural distance\nbetween LLMs and various demographics that have been surveyed using the WVS. In\nline with other work we find that LLMs exhibit several Western-centric value\nbiases; they overestimate how conservative people in non-Western countries are,\nthey are less accurate in representing gender for non-Western countries, and\nportray older populations as having more traditional values. Our results\nhighlight value misalignment and age groups, and a need for social science\ninformed technological solutions addressing value plurality in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.10075v1.pdf"
    },
    {
        "title": "TypeFly: Flying Drones with Large Language Model",
        "authors": [
            "Guojun Chen",
            "Xiaojing Yu",
            "Lin Zhong"
        ],
        "published": "2023-12-08T15:57:18Z",
        "summary": "Commanding a drone with a natural language is not only user-friendly but also\nopens the door for emerging language agents to control the drone. Emerging\nlarge language models (LLMs) provide a previously impossible opportunity to\nautomatically translate a task description in a natural language to a program\nthat can be executed by the drone. However, powerful LLMs and their vision\ncounterparts are limited in three important ways. First, they are only\navailable as cloud-based services. Sending images to the cloud raises privacy\nconcerns. Second, they are expensive, costing proportionally to the request\nsize. Finally, without expensive fine-tuning, existing LLMs are quite limited\nin their capability of writing a program for specialized systems like drones.\n  In this paper, we present a system called TypeFly that tackles the above\nthree problems using a combination of edge-based vision intelligence, novel\nprogramming language design, and prompt engineering. Instead of the familiar\nPython, TypeFly gets a cloud-based LLM service to write a program in a small,\ncustom language called MiniSpec, based on task and scene descriptions in\nEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)\nbut also able to consult the LLM during their execution using a special skill\ncalled query. Using a set of increasingly challenging drone tasks, we show that\ndesign choices made by TypeFly can reduce both the cost of LLM service and the\ntask execution time by more than 2x. More importantly, query and prompt\nengineering techniques contributed by TypeFly significantly improve the chance\nof success of complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.14950v1.pdf"
    },
    {
        "title": "LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization",
        "authors": [
            "Andreas Florath"
        ],
        "published": "2023-12-08T13:52:57Z",
        "summary": "With the advent of large language models (LLMs) like GPT-3, a natural\nquestion is the extent to which these models can be utilized for source code\noptimization. This paper presents methodologically stringent case studies\napplied to well-known open source python libraries pillow and numpy. We find\nthat contemporary LLM ChatGPT-4 (state September and October 2023) is\nsurprisingly adept at optimizing energy and compute efficiency. However, this\nis only the case in interactive use, with a human expert in the loop. Aware of\nexperimenter bias, we document our qualitative approach in detail, and provide\ntranscript and source code. We start by providing a detailed description of our\napproach in conversing with the LLM to optimize the _getextrema function in the\npillow library, and a quantitative evaluation of the performance improvement.\nTo demonstrate qualitative replicability, we report further attempts on another\nlocus in the pillow library, and one code locus in the numpy library, to\ndemonstrate generalization within and beyond a library. In all attempts, the\nperformance improvement is significant (factor up to 38). We have also not\nomitted reporting of failed attempts (there were none). We conclude that LLMs\nare a promising tool for code optimization in open source libraries, but that\nthe human expert in the loop is essential for success. Nonetheless, we were\nsurprised by how few iterations were required to achieve substantial\nperformance improvements that were not obvious to the expert in the loop. We\nwould like bring attention to the qualitative nature of this study, more robust\nquantitative studies would need to introduce a layer of selecting experts in a\nrepresentative sample -- we invite the community to collaborate.",
        "pdf_link": "https://arxiv.org/pdf/2312.14949v2.pdf"
    },
    {
        "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
        "authors": [
            "Luka Ribar",
            "Ivan Chelombiev",
            "Luke Hudlass-Galley",
            "Charlie Blake",
            "Carlo Luschi",
            "Douglas Orr"
        ],
        "published": "2023-12-08T11:47:35Z",
        "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data-transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data-transfers without substantial drops in accuracy, by evaluating\nLlama 2, Mistral and Pythia models on a wide range of downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.04985v3.pdf"
    },
    {
        "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "published": "2023-12-08T09:48:36Z",
        "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video question answering (Video QA) tasks, utilizing video\ntokens as contextual input. However, employing LLMs for long video\nunderstanding presents significant challenges and remains under-explored. The\nextensive number of video tokens leads to considerable computational costs for\nLLMs while using aggregated tokens results in loss of vision details. Moreover,\nthe presence of abundant question-irrelevant tokens introduces noise to the\nvideo QA process. To address these issues, we introduce a simple yet effective\nretrieval-based video language model (R-VLM) for efficient and interpretable\nlong video QA. Specifically, given a question (query) and a long video, our\nmodel identifies and selects the most relevant $K$ video chunks and uses their\nassociated visual tokens to serve as context for the LLM inference. This\neffectively reduces the number of video tokens, eliminates noise interference,\nand enhances system performance. Our experimental results validate the\neffectiveness of our framework for comprehending long videos. Furthermore,\nbased on the retrieved chunks, our model is interpretable that provides the\njustifications on where we get the answers.",
        "pdf_link": "https://arxiv.org/pdf/2312.04931v1.pdf"
    },
    {
        "title": "Exploring the Limits of ChatGPT in Software Security Applications",
        "authors": [
            "Fangzhou Wu",
            "Qingzhao Zhang",
            "Ati Priya Bajaj",
            "Tiffany Bao",
            "Ning Zhang",
            "Ruoyu \"Fish\" Wang",
            "Chaowei Xiao"
        ],
        "published": "2023-12-08T03:02:37Z",
        "summary": "Large language models (LLMs) have undergone rapid evolution and achieved\nremarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or\nGPT-4, has gained instant popularity due to its strong capability across a wide\nrange of tasks, including natural language tasks, coding, mathematics, and\nengaging conversations. However, the impacts and limits of such LLMs in system\nsecurity domain are less explored. In this paper, we delve into the limits of\nLLMs (i.e., ChatGPT) in seven software security applications including\nvulnerability detection/repair, debugging, debloating, decompilation, patching,\nroot cause analysis, symbolic execution, and fuzzing. Our exploration reveals\nthat ChatGPT not only excels at generating code, which is the conventional\napplication of language models, but also demonstrates strong capability in\nunderstanding user-provided commands in natural languages, reasoning about\ncontrol and data flows within programs, generating complex data structures, and\neven decompiling assembly code. Notably, GPT-4 showcases significant\nimprovements over GPT-3.5 in most security tasks. Also, certain limitations of\nChatGPT in security-related tasks are identified, such as its constrained\nability to process long code contexts.",
        "pdf_link": "https://arxiv.org/pdf/2312.05275v1.pdf"
    },
    {
        "title": "DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions",
        "authors": [
            "Fangzhou Wu",
            "Xiaogeng Liu",
            "Chaowei Xiao"
        ],
        "published": "2023-12-07T22:19:06Z",
        "summary": "With the advancement of Large Language Models (LLMs), significant progress\nhas been made in code generation, enabling LLMs to transform natural language\ninto programming code. These Code LLMs have been widely accepted by massive\nusers and organizations. However, a dangerous nature is hidden in the code,\nwhich is the existence of fatal vulnerabilities. While some LLM providers have\nattempted to address these issues by aligning with human guidance, these\nefforts fall short of making Code LLMs practical and robust. Without a deep\nunderstanding of the performance of the LLMs under the practical worst cases,\nit would be concerning to apply them to various real-world applications. In\nthis paper, we answer the critical issue: Are existing Code LLMs immune to\ngenerating vulnerable code? If not, what is the possible maximum severity of\nthis issue in practical deployment scenarios? In this paper, we introduce\nDeceptPrompt, a novel algorithm that can generate adversarial natural language\ninstructions that drive the Code LLMs to generate functionality correct code\nwith vulnerabilities. DeceptPrompt is achieved through a systematic\nevolution-based algorithm with a fine grain loss design. The unique advantage\nof DeceptPrompt enables us to find natural prefix/suffix with totally benign\nand non-directional semantic meaning, meanwhile, having great power in inducing\nthe Code LLMs to generate vulnerable code. This feature can enable us to\nconduct the almost-worstcase red-teaming on these LLMs in a real scenario,\nwhere users are using natural language. Our extensive experiments and analyses\non DeceptPrompt not only validate the effectiveness of our approach but also\nshed light on the huge weakness of LLMs in the code generation task. When\napplying the optimized prefix/suffix, the attack success rate (ASR) will\nimprove by average 50% compared with no prefix/suffix applying.",
        "pdf_link": "https://arxiv.org/pdf/2312.04730v2.pdf"
    },
    {
        "title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models",
        "authors": [
            "Manish Bhatt",
            "Sahana Chennabasappa",
            "Cyrus Nikolaidis",
            "Shengye Wan",
            "Ivan Evtimov",
            "Dominik Gabi",
            "Daniel Song",
            "Faizan Ahmad",
            "Cornelius Aschermann",
            "Lorenzo Fontana",
            "Sasha Frolov",
            "Ravi Prakash Giri",
            "Dhaval Kapil",
            "Yiannis Kozyrakis",
            "David LeBlanc",
            "James Milazzo",
            "Aleksandar Straumann",
            "Gabriel Synnaeve",
            "Varun Vontimitta",
            "Spencer Whitman",
            "Joshua Saxe"
        ],
        "published": "2023-12-07T22:07:54Z",
        "summary": "This paper presents CyberSecEval, a comprehensive benchmark developed to help\nbolster the cybersecurity of Large Language Models (LLMs) employed as coding\nassistants. As what we believe to be the most extensive unified cybersecurity\nsafety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs\nin two crucial security domains: their propensity to generate insecure code and\ntheir level of compliance when asked to assist in cyberattacks. Through a case\nstudy involving seven models from the Llama 2, Code Llama, and OpenAI GPT large\nlanguage model families, CyberSecEval effectively pinpointed key cybersecurity\nrisks. More importantly, it offered practical insights for refining these\nmodels. A significant observation from the study was the tendency of more\nadvanced models to suggest insecure code, highlighting the critical need for\nintegrating security considerations in the development of sophisticated LLMs.\nCyberSecEval, with its automated test case generation and evaluation pipeline\ncovers a broad scope and equips LLM designers and researchers with a tool to\nbroadly measure and enhance the cybersecurity safety properties of LLMs,\ncontributing to the development of more secure AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.04724v1.pdf"
    },
    {
        "title": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models",
        "authors": [
            "Victor Agostinelli",
            "Max Wild",
            "Matthew Raffel",
            "Kazi Ahmed Asif Fuad",
            "Lizhong Chen"
        ],
        "published": "2023-12-07T20:42:05Z",
        "summary": "Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.",
        "pdf_link": "https://arxiv.org/pdf/2312.04691v2.pdf"
    },
    {
        "title": "Testing LLM performance on the Physics GRE: some observations",
        "authors": [
            "Pranav Gupta"
        ],
        "published": "2023-12-07T17:33:12Z",
        "summary": "With the recent developments in large language models (LLMs) and their\nwidespread availability through open source models and/or low-cost APIs,\nseveral exciting products and applications are emerging, many of which are in\nthe field of STEM educational technology for K-12 and university students.\nThere is a need to evaluate these powerful language models on several\nbenchmarks, in order to understand their risks and limitations. In this short\npaper, we summarize and analyze the performance of Bard, a popular LLM-based\nconversational service made available by Google, on the standardized Physics\nGRE examination.",
        "pdf_link": "https://arxiv.org/pdf/2312.04613v1.pdf"
    },
    {
        "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use",
        "authors": [
            "Yuhan Chen",
            "Ang Lv",
            "Ting-En Lin",
            "Changyu Chen",
            "Yuchuan Wu",
            "Fei Huang",
            "Yongbin Li",
            "Rui Yan"
        ],
        "published": "2023-12-07T17:24:51Z",
        "summary": "In this paper, we demonstrate that an inherent waveform pattern in the\nattention allocation of large language models (LLMs) significantly affects\ntheir performance in tasks demanding a high degree of context awareness, such\nas utilizing LLMs for tool-use. Specifically, the crucial information in the\ncontext will be potentially overlooked by model when it is positioned in the\ntrough zone of the attention waveform, leading to decreased performance. To\naddress this issue, we propose a novel inference method named Attention\nBuckets. It allows LLMs to process their input through multiple parallel\nprocesses. Each process utilizes a distinct base angle for the rotary position\nembedding, thereby creating a unique attention waveform. By compensating an\nattention trough of a particular process with an attention peak of another\nprocess, our approach enhances LLM's awareness to various contextual positions,\nthus mitigating the risk of overlooking crucial information. In the largest\ntool-use benchmark, our method elevates a 7B model to achieve state-of-the-art\nperformance, comparable to that of GPT-4. On other benchmarks and some RAG\ntasks, which also demand a thorough understanding of contextual content,\nAttention Buckets also exhibited notable enhancements in performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.04455v3.pdf"
    },
    {
        "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
        "authors": [
            "Yuechen Zhang",
            "Shengju Qian",
            "Bohao Peng",
            "Shu Liu",
            "Jiaya Jia"
        ],
        "published": "2023-12-07T13:53:29Z",
        "summary": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/",
        "pdf_link": "https://arxiv.org/pdf/2312.04302v2.pdf"
    },
    {
        "title": "Hijacking Context in Large Multi-modal Models",
        "authors": [
            "Joonhyun Jeong"
        ],
        "published": "2023-12-07T11:23:29Z",
        "summary": "Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to\nunderstand the visual contents of images given the instructions regarding the\nimages. Built upon the Large Language Models (LLMs), LMMs also inherit their\nabilities and characteristics such as in-context learning where a coherent\nsequence of images and texts are given as the input prompt. However, we\nidentify a new limitation of off-the-shelf LMMs where a small fraction of\nincoherent images or text descriptions mislead LMMs to only generate biased\noutput about the hijacked context, not the originally intended context. To\naddress this, we propose a pre-filtering method that removes irrelevant\ncontexts via GPT-4V, based on its robustness towards distribution shift within\nthe contexts. We further investigate whether replacing the hijacked visual and\ntextual contexts with the correlated ones via GPT-4V and text-to-image models\ncan help yield coherent responses.",
        "pdf_link": "https://arxiv.org/pdf/2312.07553v1.pdf"
    },
    {
        "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak",
        "authors": [
            "Yanrui Du",
            "Sendong Zhao",
            "Ming Ma",
            "Yuhan Chen",
            "Bing Qin"
        ],
        "published": "2023-12-07T08:29:58Z",
        "summary": "Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, LLMs still tend to generate harmful responses\nwhen faced with malicious instructions, a phenomenon referred to as \"Jailbreak\nAttack\". In our research, we introduce a novel automatic jailbreak method\nRADIAL, which bypasses the security mechanism by amplifying the potential of\nLLMs to generate affirmation responses. The jailbreak idea of our method is\n\"Inherent Response Tendency Analysis\" which identifies real-world instructions\nthat can inherently induce LLMs to generate affirmation responses and the\ncorresponding jailbreak strategy is \"Real-World Instructions-Driven Jailbreak\"\nwhich involves strategically splicing real-world instructions identified\nthrough the above analysis around the malicious instruction. Our method\nachieves excellent attack performance on English malicious instructions with\nfive open-source advanced LLMs while maintaining robust attack performance in\nexecuting cross-language attacks against Chinese malicious instructions. We\nconduct experiments to verify the effectiveness of our jailbreak idea and the\nrationality of our jailbreak strategy design. Notably, our method designed a\nsemantically coherent attack prompt, highlighting the potential risks of LLMs.\nOur study provides detailed insights into jailbreak attacks, establishing a\nfoundation for the development of safer LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.04127v2.pdf"
    },
    {
        "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss",
        "authors": [
            "Zhuoran Huang",
            "Michael P. Berry",
            "Christina Chwyl",
            "Gary Hsieh",
            "Jing Wei",
            "Evan M. Forman"
        ],
        "published": "2023-12-07T05:45:24Z",
        "summary": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.",
        "pdf_link": "https://arxiv.org/pdf/2312.04059v1.pdf"
    },
    {
        "title": "Large Language Models for Intent-Driven Session Recommendations",
        "authors": [
            "Zhu Sun",
            "Hongyang Liu",
            "Xinghua Qu",
            "Kaidong Feng",
            "Yan Wang",
            "Yew-Soon Ong"
        ],
        "published": "2023-12-07T02:25:14Z",
        "summary": "Intent-aware session recommendation (ISR) is pivotal in discerning user\nintents within sessions for precise predictions. Traditional approaches,\nhowever, face limitations due to their presumption of a uniform number of\nintents across all sessions. This assumption overlooks the dynamic nature of\nuser sessions, where the number and type of intentions can significantly vary.\nIn addition, these methods typically operate in latent spaces, thus hinder the\nmodel's transparency.Addressing these challenges, we introduce a novel ISR\napproach, utilizing the advanced reasoning capabilities of large language\nmodels (LLMs). First, this approach begins by generating an initial prompt that\nguides LLMs to predict the next item in a session, based on the varied intents\nmanifested in user sessions. Then, to refine this process, we introduce an\ninnovative prompt optimization mechanism that iteratively self-reflects and\nadjusts prompts. Furthermore, our prompt selection module, built upon the LLMs'\nbroad adaptability, swiftly selects the most optimized prompts across diverse\ndomains. This new paradigm empowers LLMs to discern diverse user intents at a\nsemantic level, leading to more accurate and interpretable session\nrecommendations. Our extensive experiments on three real-world datasets\ndemonstrate the effectiveness of our method, marking a significant advancement\nin ISR systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.07552v1.pdf"
    },
    {
        "title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration",
        "authors": [
            "Meihao Fan",
            "Xiaoyue Han",
            "Ju Fan",
            "Chengliang Chai",
            "Nan Tang",
            "Guoliang Li",
            "Xiaoyong Du"
        ],
        "published": "2023-12-07T02:09:27Z",
        "summary": "Entity resolution (ER) is an important data integration task with a wide\nspectrum of applications. The state-of-the-art solutions on ER rely on\npre-trained language models (PLMs), which require fine-tuning on a lot of\nlabeled matching/non-matching entity pairs. Recently, large languages models\n(LLMs), such as GPT-4, have shown the ability to perform many tasks without\ntuning model parameters, which is known as in-context learning (ICL) that\nfacilitates effective learning from a few labeled input context demonstrations.\nHowever, existing ICL approaches to ER typically necessitate providing a task\ndescription and a set of demonstrations for each entity pair and thus have\nlimitations on the monetary cost of interfacing LLMs. To address the problem,\nin this paper, we provide a comprehensive study to investigate how to develop a\ncost-effective batch prompting approach to ER. We introduce a framework BATCHER\nconsisting of demonstration selection and question batching and explore\ndifferent design choices that support batch prompting for ER. We also devise a\ncovering-based demonstration selection strategy that achieves an effective\nbalance between matching accuracy and monetary cost. We conduct a thorough\nevaluation to explore the design space and evaluate our proposed strategies.\nThrough extensive experiments, we find that batch prompting is very\ncost-effective for ER, compared with not only PLM-based methods fine-tuned with\nextensive labeled data but also LLM-based methods with manually designed\nprompting. We also provide guidance for selecting appropriate design choices\nfor batch prompting.",
        "pdf_link": "https://arxiv.org/pdf/2312.03987v1.pdf"
    },
    {
        "title": "Understanding (Un)Intended Memorization in Text-to-Image Generative Models",
        "authors": [
            "Ali Naseh",
            "Jaechul Roh",
            "Amir Houmansadr"
        ],
        "published": "2023-12-06T19:53:17Z",
        "summary": "Multimodal machine learning, especially text-to-image models like Stable\nDiffusion and DALL-E 3, has gained significance for transforming text into\ndetailed images.\n  Despite their growing use and remarkable generative capabilities, there is a\npressing need for a detailed examination of these models' behavior,\nparticularly with respect to memorization. Historically, memorization in\nmachine learning has been context-dependent, with diverse definitions emerging\nfrom classification tasks to complex models like Large Language Models (LLMs)\nand Diffusion models. Yet, a definitive concept of memorization that aligns\nwith the intricacies of text-to-image synthesis remains elusive. This\nunderstanding is vital as memorization poses privacy risks yet is essential for\nmeeting user expectations, especially when generating representations of\nunderrepresented entities. In this paper, we introduce a specialized definition\nof memorization tailored to text-to-image models, categorizing it into three\ndistinct types according to user expectations. We closely examine the subtle\ndistinctions between intended and unintended memorization, emphasizing the\nimportance of balancing user privacy with the generative quality of the model\noutputs. Using the Stable Diffusion model, we offer examples to validate our\nmemorization definitions and clarify their application.",
        "pdf_link": "https://arxiv.org/pdf/2312.07550v1.pdf"
    },
    {
        "title": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management",
        "authors": [
            "Huan Wang",
            "Yan-Fu Li",
            "Min Xie"
        ],
        "published": "2023-12-06T15:24:01Z",
        "summary": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.",
        "pdf_link": "https://arxiv.org/pdf/2312.14945v1.pdf"
    },
    {
        "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification",
        "authors": [
            "Chengguang Gan",
            "Qinghao Zhang",
            "Tatsunori Mori"
        ],
        "published": "2023-12-06T12:34:46Z",
        "summary": "The proliferation of Large Language Models (LLMs) has spurred extensive\nresearch into LLM-related Prompt investigations, such as Instruction Learning\n(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches\naim to improve LLMs' responses by enabling them to provide concise statements\nor examples for deeper contemplation when addressing questions. However,\nindependent thinking by LLMs can introduce variability in their thought\nprocesses, leading to potential inaccuracies. In response, our study seeks to\nbridge the gap between LLM and human-like thinking processes, recognizing that\ntext comprehension begins with understanding individual words. To tackle this\nchallenge, we have expanded the CoT method to cater to a specific domain. Our\napproach, known as \"Think from Words\" (TFW), initiates the comprehension\nprocess at the word level and then extends it to encompass the entire text. We\nalso propose \"TFW with Extra word-level information\" (TFW Extra), augmenting\ncomprehension with additional word-level data. To assess our methods, we employ\ntext classification on six Japanese datasets comprising text-level and\nword-level elements. Our findings not only validate the effectiveness of TFW\nbut also shed light on the impact of various word-level information types on\nLLMs' text comprehension, offering insights into their potential to cause\nmisinterpretations and errors in the overall comprehension of the final text.",
        "pdf_link": "https://arxiv.org/pdf/2312.03458v1.pdf"
    },
    {
        "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM",
        "authors": [
            "Jiayi Pan",
            "Chengcan Wang",
            "Kaifu Zheng",
            "Yangguang Li",
            "Zhenyu Wang",
            "Bin Feng"
        ],
        "published": "2023-12-06T11:10:55Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.",
        "pdf_link": "https://arxiv.org/pdf/2312.03788v1.pdf"
    },
    {
        "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
        "authors": [
            "Kan Hatakeyama-Sato",
            "Yasuhiko Igarashi",
            "Shun Katakami",
            "Yuta Nabae",
            "Teruaki Hayakawa"
        ],
        "published": "2023-12-06T08:55:55Z",
        "summary": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.",
        "pdf_link": "https://arxiv.org/pdf/2312.03360v2.pdf"
    },
    {
        "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science",
        "authors": [
            "Chenxi Wu",
            "Alan John Varghese",
            "Vivek Oommen",
            "George Em Karniadakis"
        ],
        "published": "2023-12-05T21:41:52Z",
        "summary": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific\nreviews, possibly using more unbiased quantitative metrics, facilitating\ncross-disciplinary connections, and identifying emerging trends and research\ngaps by analyzing large volumes of data. However, at the present time, they\nlack the required deep understanding of complex methodologies, they have\ndifficulty in evaluating innovative claims, and they are unable to assess\nethical issues and conflicts of interest. Herein, we consider 13 GPT-related\npapers across different scientific domains, reviewed by a human reviewer and\nSciSpace, a large language model, with the reviews evaluated by three distinct\ntypes of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that\n50% of SciSpace's responses to objective questions align with those of a human\nreviewer, with GPT-4 (informed evaluator) often rating the human reviewer\nhigher in accuracy, and SciSpace higher in structure, clarity, and\ncompleteness. In subjective questions, the uninformed evaluators (GPT-3.5 and\ncrowd panel) showed varying preferences between SciSpace and human responses,\nwith the crowd panel showing a preference for the human responses. However,\nGPT-4 rated them equally in accuracy and structure but favored SciSpace for\ncompleteness.",
        "pdf_link": "https://arxiv.org/pdf/2312.03769v1.pdf"
    },
    {
        "title": "LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications",
        "authors": [
            "Brett Israelsen",
            "Soumalya Sarkar"
        ],
        "published": "2023-12-05T19:04:50Z",
        "summary": "Large Language Models have seen rapid progress in capability in recent years;\nthis progress has been accelerating and their capabilities, measured by various\nbenchmarks, are beginning to approach those of humans. There is a strong demand\nto use such models in a wide variety of applications but, due to unresolved\nvulnerabilities and limitations, great care needs to be used before applying\nthem to intelligence and safety-critical applications. This paper reviews\nrecent literature related to LLM assessment and vulnerabilities to synthesize\nthe current research landscape and to help understand what advances are most\ncritical to enable use of of these technologies in intelligence and\nsafety-critical applications. The vulnerabilities are broken down into ten\nhigh-level categories and overlaid onto a high-level life cycle of an LLM. Some\ngeneral categories of mitigations are reviewed.",
        "pdf_link": "https://arxiv.org/pdf/2312.03088v1.pdf"
    },
    {
        "title": "Clinical Notes Reveal Physician Fatigue",
        "authors": [
            "Chao-Chun Hsu",
            "Ziad Obermeyer",
            "Chenhao Tan"
        ],
        "published": "2023-12-05T19:00:18Z",
        "summary": "Physicians write notes about patients. In doing so, they reveal much about\nthemselves. Using data from 129,228 emergency room visits, we train a model to\nidentify notes written by fatigued physicians -- those who worked 5 or more of\nthe prior 7 days. In a hold-out set, the model accurately identifies notes\nwritten by these high-workload physicians, and also flags notes written in\nother high-fatigue settings: on overnight shifts, and after high patient\nvolumes. Model predictions also correlate with worse decision-making on at\nleast one important metric: yield of testing for heart attack is 18% lower with\neach standard deviation increase in model-predicted fatigue. Finally, the model\nindicates that notes written about Black and Hispanic patients have 12% and 21%\nhigher predicted fatigue than Whites -- larger than overnight vs. daytime\ndifferences. These results have an important implication for large language\nmodels (LLMs). Our model indicates that fatigued doctors write more predictable\nnotes. Perhaps unsurprisingly, because word prediction is the core of how LLMs\nwork, we find that LLM-written notes have 17% higher predicted fatigue than\nreal physicians' notes. This indicates that LLMs may introduce distortions in\ngenerated text that are not yet fully understood.",
        "pdf_link": "https://arxiv.org/pdf/2312.03077v1.pdf"
    },
    {
        "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
        "authors": [
            "Yushi Hu",
            "Otilia Stretcu",
            "Chun-Ta Lu",
            "Krishnamurthy Viswanathan",
            "Kenji Hata",
            "Enming Luo",
            "Ranjay Krishna",
            "Ariel Fuxman"
        ],
        "published": "2023-12-05T18:58:37Z",
        "summary": "Solving complex visual tasks such as \"Who invented the musical instrument on\nthe right?\" involves a composition of skills: understanding space, recognizing\ninstruments, and also retrieving prior knowledge. Recent work shows promise by\ndecomposing such tasks using a large language model (LLM) into an executable\nprogram that invokes specialized vision models. However, generated programs are\nerror-prone: they omit necessary steps, include spurious ones, and are unable\nto recover when the specialized models give incorrect outputs. Moreover, they\nrequire loading multiple models, incurring high latency and computation costs.\nWe propose Visual Program Distillation (VPD), an instruction tuning framework\nthat produces a vision-language model (VLM) capable of solving complex visual\ntasks with a single forward pass. VPD distills the reasoning ability of LLMs by\nusing them to sample multiple candidate programs, which are then executed and\nverified to identify a correct one. It translates each correct program into a\nlanguage description of the reasoning steps, which are then distilled into a\nVLM. Extensive experiments show that VPD improves the VLM's ability to count,\nunderstand spatial relations, and reason compositionally. Our VPD-trained\nPaLI-X outperforms all prior VLMs, achieving state-of-the-art performance\nacross complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,\nand Hateful Memes. An evaluation with human annotators also confirms that VPD\nimproves model response factuality and consistency. Finally, experiments on\ncontent moderation demonstrate that VPD is also helpful for adaptation to\nreal-world applications with limited data.",
        "pdf_link": "https://arxiv.org/pdf/2312.03052v2.pdf"
    },
    {
        "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models",
        "authors": [
            "Xinyu Zhang",
            "Sebastian Hofst\u00e4tter",
            "Patrick Lewis",
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "published": "2023-12-05T18:57:40Z",
        "summary": "Listwise rerankers based on large language models (LLM) are the zero-shot\nstate-of-the-art. However, current works in this direction all depend on the\nGPT models, making it a single point of failure in scientific reproducibility.\nMoreover, it raises the concern that the current research findings only hold\nfor GPT models but not LLM in general. In this work, we lift this pre-condition\nand build for the first time effective listwise rerankers without any form of\ndependency on GPT. Our passage retrieval experiments show that our best list se\nreranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves\n97% effectiveness of the ones built on GPT-4. Our results also show that the\nexisting training datasets, which were expressly constructed for pointwise\nranking, are insufficient for building such listwise rerankers. Instead,\nhigh-quality listwise ranking data is required and crucial, calling for further\nwork on building human-annotated listwise data resources.",
        "pdf_link": "https://arxiv.org/pdf/2312.02969v1.pdf"
    },
    {
        "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions",
        "authors": [
            "Zahra Abbasiantaeb",
            "Yifei Yuan",
            "Evangelos Kanoulas",
            "Mohammad Aliannejadi"
        ],
        "published": "2023-12-05T17:38:02Z",
        "summary": "Conversational question-answering (CQA) systems aim to create interactive\nsearch systems that effectively retrieve information by interacting with users.\nTo replicate human-to-human conversations, existing work uses human annotators\nto play the roles of the questioner (student) and the answerer (teacher).\nDespite its effectiveness, challenges exist as human annotation is\ntime-consuming, inconsistent, and not scalable. To address this issue and\ninvestigate the applicability of large language models (LLMs) in CQA\nsimulation, we propose a simulation framework that employs zero-shot learner\nLLMs for simulating teacher-student interactions. Our framework involves two\nLLMs interacting on a specific topic, with the first LLM acting as a student,\ngenerating questions to explore a given search topic. The second LLM plays the\nrole of a teacher by answering questions and is equipped with additional\ninformation, including a text on the given topic. We implement both the student\nand teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness\nof LLMs in simulating CQA interactions and understand the disparities between\nLLM- and human-generated conversations, we evaluate the simulated data from\nvarious perspectives. We begin by evaluating the teacher's performance through\nboth automatic and human assessment. Next, we evaluate the performance of the\nstudent, analyzing and comparing the disparities between questions generated by\nthe LLM and those generated by humans. Furthermore, we conduct extensive\nanalyses to thoroughly examine the LLM performance by benchmarking\nstate-of-the-art reading comprehension models on both datasets. Our results\nreveal that the teacher LLM generates lengthier answers that tend to be more\naccurate and complete. The student LLM generates more diverse questions,\ncovering more aspects of a given topic.",
        "pdf_link": "https://arxiv.org/pdf/2312.02913v1.pdf"
    },
    {
        "title": "Inherent limitations of LLMs regarding spatial information",
        "authors": [
            "He Yan",
            "Xinyao Hu",
            "Xiangpeng Wan",
            "Chengyu Huang",
            "Kai Zou",
            "Shiqi Xu"
        ],
        "published": "2023-12-05T16:02:20Z",
        "summary": "Despite the significant advancements in natural language processing\ncapabilities demonstrated by large language models such as ChatGPT, their\nproficiency in comprehending and processing spatial information, especially\nwithin the domains of 2D and 3D route planning, remains notably underdeveloped.\nThis paper investigates the inherent limitations of ChatGPT and similar models\nin spatial reasoning and navigation-related tasks, an area critical for\napplications ranging from autonomous vehicle guidance to assistive technologies\nfor the visually impaired. In this paper, we introduce a novel evaluation\nframework complemented by a baseline dataset, meticulously crafted for this\nstudy. This dataset is structured around three key tasks: plotting spatial\npoints, planning routes in two-dimensional (2D) spaces, and devising pathways\nin three-dimensional (3D) environments. We specifically developed this dataset\nto assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals\nkey insights into the model's capabilities and limitations in spatial\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2312.03042v1.pdf"
    },
    {
        "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
        "authors": [
            "Miriam Rateike",
            "Celia Cintas",
            "John Wamburu",
            "Tanya Akumu",
            "Skyler Speakman"
        ],
        "published": "2023-12-05T14:35:11Z",
        "summary": "We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.",
        "pdf_link": "https://arxiv.org/pdf/2312.02798v1.pdf"
    },
    {
        "title": "How should the advent of large language models affect the practice of science?",
        "authors": [
            "Marcel Binz",
            "Stephan Alaniz",
            "Adina Roskies",
            "Balazs Aczel",
            "Carl T. Bergstrom",
            "Colin Allen",
            "Daniel Schad",
            "Dirk Wulff",
            "Jevin D. West",
            "Qiong Zhang",
            "Richard M. Shiffrin",
            "Samuel J. Gershman",
            "Ven Popov",
            "Emily M. Bender",
            "Marco Marelli",
            "Matthew M. Botvinick",
            "Zeynep Akata",
            "Eric Schulz"
        ],
        "published": "2023-12-05T10:45:12Z",
        "summary": "Large language models (LLMs) are being increasingly incorporated into\nscientific workflows. However, we have yet to fully grasp the implications of\nthis integration. How should the advent of large language models affect the\npractice of science? For this opinion piece, we have invited four diverse\ngroups of scientists to reflect on this query, sharing their perspectives and\nengaging in debate. Schulz et al. make the argument that working with LLMs is\nnot fundamentally different from working with human collaborators, while Bender\net al. argue that LLMs are often misused and over-hyped, and that their\nlimitations warrant a focus on more specialized, easily interpretable tools.\nMarelli et al. emphasize the importance of transparent attribution and\nresponsible use of LLMs. Finally, Botvinick and Gershman advocate that humans\nshould retain responsibility for determining the scientific roadmap. To\nfacilitate the discussion, the four perspectives are complemented with a\nresponse from each group. By putting these different perspectives in\nconversation, we aim to bring attention to important considerations within the\nacademic community regarding the adoption of LLMs and their impact on both\ncurrent and future scientific practices.",
        "pdf_link": "https://arxiv.org/pdf/2312.03759v1.pdf"
    },
    {
        "title": "Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",
        "authors": [
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-12-05T06:13:55Z",
        "summary": "Explainability and Safety engender Trust. These require a model to exhibit\nconsistency and reliability. To achieve these, it is necessary to use and\nanalyze data and knowledge with statistical and symbolic AI methods relevant to\nthe AI application - neither alone will do. Consequently, we argue and seek to\ndemonstrate that the NeuroSymbolic AI approach is better suited for making AI a\ntrusted AI system. We present the CREST framework that shows how Consistency,\nReliability, user-level Explainability, and Safety are built on NeuroSymbolic\nmethods that use data and knowledge to support requirements for critical\napplications such as health and well-being. This article focuses on Large\nLanguage Models (LLMs) as the chosen AI system within the CREST framework. LLMs\nhave garnered substantial attention from researchers due to their versatility\nin handling a broad array of natural language processing (NLP) scenarios. For\nexample, ChatGPT and Google's MedPaLM have emerged as highly promising\nplatforms for providing information in general and health-related queries,\nrespectively. Nevertheless, these models remain black boxes despite\nincorporating human feedback and instruction-guided tuning. For instance,\nChatGPT can generate unsafe responses despite instituting safety guardrails.\nCREST presents a plausible approach harnessing procedural and graph-based\nknowledge within a NeuroSymbolic framework to shed light on the challenges\nassociated with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.06798v1.pdf"
    },
    {
        "title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks",
        "authors": [
            "Chi Zhang",
            "Penglin Cai",
            "Yuhui Fu",
            "Haoqi Yuan",
            "Zongqing Lu"
        ],
        "published": "2023-12-05T06:00:52Z",
        "summary": "We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https://github.com/PKU-RL/Creative-Agents).",
        "pdf_link": "https://arxiv.org/pdf/2312.02519v1.pdf"
    },
    {
        "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
        "authors": [
            "Xinhang Li",
            "Chong Chen",
            "Xiangyu Zhao",
            "Yong Zhang",
            "Chunxiao Xing"
        ],
        "published": "2023-12-05T02:50:18Z",
        "summary": "The recent advancements in Large Language Models (LLMs) have sparked interest\nin harnessing their potential within recommender systems. Since LLMs are\ndesigned for natural language tasks, existing recommendation approaches have\npredominantly transformed recommendation tasks into open-domain natural\nlanguage generation tasks. However, this approach necessitates items to possess\nrich semantic information, often generates out-of-range results, and suffers\nfrom notably low efficiency and limited extensibility. Furthermore, practical\nID-based recommendation strategies, reliant on a huge number of unique\nidentities (IDs) to represent users and items, have gained prominence in\nreal-world recommender systems due to their effectiveness and efficiency.\nNevertheless, the incapacity of LLMs to model IDs presents a formidable\nchallenge when seeking to leverage LLMs for personalized recommendations. In\nthis paper, we introduce an Elegant Effective Efficient Extensible solution for\nlarge language models for Sequential Recommendation (E4SRec), which seamlessly\nintegrates LLMs with traditional recommender systems that exclusively utilize\nIDs to represent items. Specifically, E4SRec takes ID sequences as inputs,\nensuring that the generated outputs fall within the candidate lists.\nFurthermore, E4SRec possesses the capability to generate the entire ranking\nlist in a single forward process, and demands only a minimal set of pluggable\nparameters, which are trained for each dataset while keeping the entire LLM\nfrozen. We substantiate the effectiveness, efficiency, and extensibility of our\nproposed E4SRec through comprehensive experiments conducted on four widely-used\nreal-world datasets. The implementation code is accessible at\nhttps://github.com/HestiaSky/E4SRec/.",
        "pdf_link": "https://arxiv.org/pdf/2312.02443v1.pdf"
    },
    {
        "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation",
        "authors": [
            "Shanshan Zhong",
            "Zhongzhan Huang",
            "Shanghua Gao",
            "Wushao Wen",
            "Liang Lin",
            "Marinka Zitnik",
            "Pan Zhou"
        ],
        "published": "2023-12-05T02:41:57Z",
        "summary": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason\nstep-by-step, and can motivate their logical reasoning ability. While effective\nfor logical tasks, CoT is not conducive to creative problem-solving which often\nrequires out-of-box thoughts and is crucial for innovation advancements. In\nthis paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a\nnon-sequential, creative paradigm involving strong associations and knowledge\nleaps. To this end, we study LLMs on the popular Oogiri game which needs\nparticipants to have good creativity and strong associative thinking for\nresponding unexpectedly and humorously to the given image, text, or both, and\nthus is suitable for LoT study. Then to investigate LLMs' LoT ability in the\nOogiri game, we first build a multimodal and multilingual Oogiri-GO dataset\nwhich contains over 130,000 samples from the Oogiri game, and observe the\ninsufficient LoT ability or failures of most existing LLMs on the Oogiri game.\nAccordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve\nLLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into\nLoT-oriented instruction tuning data to train pretrained LLM for achieving\ncertain LoT humor generation and discrimination abilities. Then CLoT designs an\nexplorative self-refinement that encourages the LLM to generate more creative\nLoT data via exploring parallels between seemingly unrelated concepts and\nselects high-quality data to train itself for self-refinement. CLoT not only\nexcels in humor generation in the Oogiri game but also boosts creative\nabilities in various tasks like cloud guessing game and divergent association\ntask. These findings advance our understanding and offer a pathway to improve\nLLMs' creative capacities for innovative applications across domains. The\ndataset, code, and models will be released online.\nhttps://zhongshsh.github.io/CLoT/.",
        "pdf_link": "https://arxiv.org/pdf/2312.02439v2.pdf"
    },
    {
        "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following",
        "authors": [
            "Renze Lou",
            "Kai Zhang",
            "Jian Xie",
            "Yuxuan Sun",
            "Janice Ahn",
            "Hanzi Xu",
            "Yu Su",
            "Wenpeng Yin"
        ],
        "published": "2023-12-05T02:32:08Z",
        "summary": "In the realm of large language models (LLMs), enhancing instruction-following\ncapability often involves curating expansive training data. This is achieved\nthrough two primary schemes: i) Scaling-Inputs: Amplifying (input, output)\npairs per task instruction, aiming for better instruction adherence. ii)\nScaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,\noutput) pair (without requiring a separate input anymore). However, LLMs under\nScaling-Inputs tend to be overly sensitive to inputs, leading to\nmisinterpretation or non-compliance with instructions. Conversely, Scaling\nInput-Free Tasks demands a substantial number of tasks but is less effective in\ninstruction following when dealing with instances in Scaling-Inputs. This work\nintroduces MUFFIN, a new scheme of instruction-following dataset curation.\nSpecifically, we automatically Scale Tasks per Input by diversifying these\ntasks with various input facets. Experimental results across four zero-shot\nbenchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,\nreveal that LLMs, at various scales, trained on MUFFIN generally demonstrate\nsuperior instruction-following capabilities compared to those trained on the\ntwo aforementioned schemes.",
        "pdf_link": "https://arxiv.org/pdf/2312.02436v3.pdf"
    },
    {
        "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data",
        "authors": [
            "Yu Yang",
            "Aaditya K. Singh",
            "Mostafa Elhoushi",
            "Anas Mahmoud",
            "Kushal Tirumala",
            "Fabian Gloeckle",
            "Baptiste Rozi\u00e8re",
            "Carole-Jean Wu",
            "Ari S. Morcos",
            "Newsha Ardalani"
        ],
        "published": "2023-12-05T01:19:30Z",
        "summary": "Code datasets, often collected from diverse and uncontrolled sources such as\nGitHub, potentially suffer from quality issues, thereby affecting the\nperformance and training efficiency of Large Language Models (LLMs) optimized\nfor code generation. Previous studies demonstrated the benefit of using\nembedding spaces for data pruning, but they mainly focused on duplicate removal\nor increasing variety, and in other modalities, such as images. Our work\nfocuses on using embeddings to identify and remove \"low-quality\" code data.\nFirst, we explore features of \"low-quality\" code in embedding space, through\nthe use of synthetic corruptions. Armed with this knowledge, we devise novel\npruning metrics that operate in embedding space to identify and remove\nlow-quality entries in the Stack dataset. We demonstrate the benefits of this\nsynthetic corruption informed pruning (SCIP) approach on the well-established\nHumanEval and MBPP benchmarks, outperforming existing embedding-based methods.\nImportantly, we achieve up to a 3% performance improvement over no pruning,\nthereby showing the promise of insights from synthetic corruptions for data\npruning.",
        "pdf_link": "https://arxiv.org/pdf/2312.02418v1.pdf"
    },
    {
        "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
        "authors": [
            "Karanpartap Singh",
            "James Zou"
        ],
        "published": "2023-12-04T22:56:31Z",
        "summary": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.",
        "pdf_link": "https://arxiv.org/pdf/2312.02382v1.pdf"
    },
    {
        "title": "Competition-Level Problems are Effective LLM Evaluators",
        "authors": [
            "Yiming Huang",
            "Zhenghao Lin",
            "Xiao Liu",
            "Yeyun Gong",
            "Shuai Lu",
            "Fangyu Lei",
            "Yaobo Liang",
            "Yelong Shen",
            "Chen Lin",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-12-04T18:58:57Z",
        "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.",
        "pdf_link": "https://arxiv.org/pdf/2312.02143v2.pdf"
    },
    {
        "title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques",
        "authors": [
            "Amir Panahandeh",
            "Hanie Asemi",
            "Esmaeil Nourani"
        ],
        "published": "2023-12-04T18:52:26Z",
        "summary": "Recent advances in language models (LMs), have demonstrated significant\nefficacy in tasks related to the arts and humanities. While LMs have exhibited\nexceptional performance across a wide range of natural language processing\ntasks, there are notable challenges associated with their utilization on small\ndatasets and their ability to replicate more creative human capacities. In this\nstudy, we aim to address these challenges by training a Persian classical\npoetry generation model using a transformer architecture on a specialized\ndataset with no pretraining. Additionally, we propose a novel decoding method\nto enhance coherence and meaningfulness in the generated poetry, effectively\nmanaging the tradeoff between diversity and quality. Furthermore, the results\nof our training approach and the proposed decoding method are evaluated through\ncomprehensive set of automatic and human evaluations and showed its superior\ncapability to generate coherent and meaningful poetry in compare to other\ndecoding methods and an existing Persian large language model (LLM).",
        "pdf_link": "https://arxiv.org/pdf/2312.02125v2.pdf"
    },
    {
        "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
        "authors": [
            "Giovanni Monea",
            "Maxime Peyrard",
            "Martin Josifoski",
            "Vishrav Chaudhary",
            "Jason Eisner",
            "Emre K\u0131c\u0131man",
            "Hamid Palangi",
            "Barun Patra",
            "Robert West"
        ],
        "published": "2023-12-04T17:35:42Z",
        "summary": "Large language models (LLMs) have an impressive ability to draw on novel\ninformation supplied in their context. Yet the mechanisms underlying this\ncontextual grounding remain unknown, especially in situations where contextual\ninformation contradicts factual knowledge stored in the parameters, which LLMs\nalso excel at recalling. Favoring the contextual information is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify outdated or noisy\nstored knowledge. We present a novel method to study grounding abilities using\nFakepedia, a dataset of counterfactual texts constructed to clash with a\nmodel's internal parametric knowledge. We benchmark various LLMs with Fakepedia\nand then we conduct a causal mediation analysis, based on our Masked Grouped\nCausal Tracing (MGCT), on LLM components when answering Fakepedia queries.\nWithin this analysis, we identify distinct computational patterns between\ngrounded and ungrounded responses. We finally demonstrate that distinguishing\ngrounded from ungrounded responses is achievable through computational analysis\nalone. Our results, together with existing findings about factual recall\nmechanisms, provide a coherent narrative of how grounding and factual recall\nmechanisms interact within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.02073v2.pdf"
    },
    {
        "title": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?",
        "authors": [
            "Donya Rooein",
            "Amanda Cercas Curry",
            "Dirk Hovy"
        ],
        "published": "2023-12-04T17:19:53Z",
        "summary": "Large language models (LLMs) offer a range of new possibilities, including\nadapting the text to different audiences and their reading needs. But how well\ndo they adapt? We evaluate the readability of answers generated by four\nstate-of-the-art LLMs (commercial and open-source) to science questions when\nprompted to target different age groups and education levels. To assess the\nadaptability of LLMs to diverse audiences, we compare the readability scores of\nthe generated responses against the recommended comprehension level of each age\nand education group. We find large variations in the readability of the answers\nby different LLMs. Our results suggest LLM answers need to be better adapted to\nthe intended audience demographics to be more comprehensible. They underline\nthe importance of enhancing the adaptability of LLMs in education settings to\ncater to diverse age and education levels. Overall, current LLMs have set\nreadability ranges and do not adapt well to different audiences, even when\nprompted. That limits their potential for educational purposes.",
        "pdf_link": "https://arxiv.org/pdf/2312.02065v1.pdf"
    },
    {
        "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
        "authors": [
            "Zichao Li",
            "Ines Arous",
            "Siva Reddy",
            "Jackie C. K. Cheung"
        ],
        "published": "2023-12-04T12:45:30Z",
        "summary": "The potential of using a large language model (LLM) as a knowledge base (KB)\nhas sparked significant interest. To manage the knowledge acquired by LLMs, we\nneed to ensure that the editing of learned facts respects internal logical\nconstraints, which are known as dependency of knowledge. Existing work on\nediting LLMs has partially addressed the issue of dependency, when the editing\nof a fact should apply to its lexical variations without disrupting irrelevant\nones. However, they neglect the dependency between a fact and its logical\nimplications. We propose an evaluation protocol with an accompanying\nquestion-answering dataset, DepEdit, that provides a comprehensive assessment\nof the editing process considering the above notions of dependency. Our\nprotocol involves setting up a controlled environment in which we edit facts\nand monitor their impact on LLMs, along with their implications based on\nIf-Then rules. Extensive experiments on DepEdit show that existing knowledge\nediting methods are sensitive to the surface form of knowledge, and that they\nhave limited performance in inferring the implications of edited facts.",
        "pdf_link": "https://arxiv.org/pdf/2312.01858v1.pdf"
    },
    {
        "title": "Intelligent Virtual Assistants with LLM-based Process Automation",
        "authors": [
            "Yanchu Guan",
            "Dong Wang",
            "Zhixuan Chu",
            "Shiyu Wang",
            "Feiyue Ni",
            "Ruihua Song",
            "Longfei Li",
            "Jinjie Gu",
            "Chenyi Zhuang"
        ],
        "published": "2023-12-04T07:51:58Z",
        "summary": "While intelligent virtual assistants like Siri, Alexa, and Google Assistant\nhave become ubiquitous in modern life, they still face limitations in their\nability to follow multi-step instructions and accomplish complex goals\narticulated in natural language. However, recent breakthroughs in large\nlanguage models (LLMs) show promise for overcoming existing barriers by\nenhancing natural language processing and reasoning capabilities. Though\npromising, applying LLMs to create more advanced virtual assistants still faces\nchallenges like ensuring robust performance and handling variability in\nreal-world user commands. This paper proposes a novel LLM-based virtual\nassistant that can automatically perform multi-step operations within mobile\napps based on high-level user requests. The system represents an advance in\nassistants by providing an end-to-end solution for parsing instructions,\nreasoning about goals, and executing actions. LLM-based Process Automation\n(LLMPA) has modules for decomposing instructions, generating descriptions,\ndetecting interface elements, predicting next actions, and error checking.\nExperiments demonstrate the system completing complex mobile operation tasks in\nAlipay based on natural language instructions. This showcases how large\nlanguage models can enable automated assistants to accomplish real-world tasks.\nThe main contributions are the novel LLMPA architecture optimized for app\nprocess automation, the methodology for applying LLMs to mobile apps, and\ndemonstrations of multi-step task completion in a real-world environment.\nNotably, this work represents the first real-world deployment and extensive\nevaluation of a large language model-based virtual assistant in a widely used\nmobile application with an enormous user base numbering in the hundreds of\nmillions.",
        "pdf_link": "https://arxiv.org/pdf/2312.06677v1.pdf"
    },
    {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
        "authors": [
            "Lei Wang",
            "Jiabang He",
            "Shenshen Li",
            "Ning Liu",
            "Ee-Peng Lim"
        ],
        "published": "2023-12-04T07:43:02Z",
        "summary": "Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https://github.com/Anonymousanoy/FOHE.",
        "pdf_link": "https://arxiv.org/pdf/2312.01701v1.pdf"
    },
    {
        "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation",
        "authors": [
            "Sunjae Lee",
            "Junyoung Choi",
            "Jungjae Lee",
            "Munim Hasan Wasi",
            "Hojun Choi",
            "Steven Y. Ko",
            "Sangeun Oh",
            "Insik Shin"
        ],
        "published": "2023-12-04T06:13:35Z",
        "summary": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 160 user instructions across 8 widely used mobile\napps. The results indicate that MobileGPT can automate and learn new tasks with\n82.5% accuracy, and is able to adapt them to different contexts with near\nperfect (98.75%) accuracy while reducing both latency and cost by 62.5% and\n68.8%, respectively, compared to the GPT-4 powered baseline.",
        "pdf_link": "https://arxiv.org/pdf/2312.03003v2.pdf"
    },
    {
        "title": "Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment",
        "authors": [
            "Cong-Duy Nguyen",
            "The-Anh Vu-Le",
            "Thong Nguyen",
            "Tho Quan",
            "Luu Anh Tuan"
        ],
        "published": "2023-12-04T03:16:48Z",
        "summary": "Language models have been supervised with both language-only objective and\nvisual grounding in existing studies of visual-grounded language learning.\nHowever, due to differences in the distribution and scale of visual-grounded\ndatasets and language corpora, the language model tends to mix up the context\nof the tokens that occurred in the grounded data with those that do not. As a\nresult, during representation learning, there is a mismatch between the visual\ninformation and the contextual meaning of the sentence. To overcome this\nlimitation, we propose GroundedBERT - a grounded language learning method that\nenhances the BERT representation with visually grounded information.\nGroundedBERT comprises two components: (i) the original BERT which captures the\ncontextual representation of words learned from the language corpora, and (ii)\na visual grounding module which captures visual information learned from\nvisual-grounded datasets. Moreover, we employ Optimal Transport (OT),\nspecifically its partial variant, to solve the fractional alignment problem\nbetween the two modalities. Our proposed method significantly outperforms the\nbaseline language models on various language tasks of the GLUE and SQuAD\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.01592v2.pdf"
    },
    {
        "title": "Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition",
        "authors": [
            "Chengyou Jia",
            "Minnan Luo",
            "Xiaojun Chang",
            "Zhuohang Dang",
            "Mingfei Han",
            "Mengmeng Wang",
            "Guang Dai",
            "Sizhe Dang",
            "Jingdong Wang"
        ],
        "published": "2023-12-04T02:31:38Z",
        "summary": "Exploring open-vocabulary video action recognition is a promising venture,\nwhich aims to recognize previously unseen actions within any arbitrary set of\ncategories. Existing methods typically adapt pretrained image-text models to\nthe video domain, capitalizing on their inherent strengths in generalization. A\ncommon thread among such methods is the augmentation of visual embeddings with\ntemporal information to improve the recognition of seen actions. Yet, they\ncompromise with standard less-informative action descriptions, thus faltering\nwhen confronted with novel actions. Drawing inspiration from human cognitive\nprocesses, we argue that augmenting text embeddings with human prior knowledge\nis pivotal for open-vocabulary video action recognition. To realize this, we\ninnovatively blend video models with Large Language Models (LLMs) to devise\nAction-conditioned Prompts. Specifically, we harness the knowledge in LLMs to\nproduce a set of descriptive sentences that contain distinctive features for\nidentifying given actions. Building upon this foundation, we further introduce\na multi-modal action knowledge alignment mechanism to align concepts in video\nand textual knowledge encapsulated within the prompts. Extensive experiments on\nvarious video benchmarks, including zero-shot, few-shot, and base-to-novel\ngeneralization settings, demonstrate that our method not only sets new SOTA\nperformance but also possesses excellent interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2312.02226v1.pdf"
    },
    {
        "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
        "authors": [
            "Vithya Yogarajan",
            "Gillian Dobbie",
            "Te Taka Keegan",
            "Rostam J. Neuwirth"
        ],
        "published": "2023-12-03T21:25:10Z",
        "summary": "The benefits and capabilities of pre-trained language models (LLMs) in\ncurrent and future innovations are vital to any society. However, introducing\nand using LLMs comes with biases and discrimination, resulting in concerns\nabout equality, diversity and fairness, and must be addressed. While\nunderstanding and acknowledging bias in LLMs and developing mitigation\nstrategies are crucial, the generalised assumptions towards societal needs can\nresult in disadvantages towards under-represented societies and indigenous\npopulations. Furthermore, the ongoing changes to actual and proposed amendments\nto regulations and laws worldwide also impact research capabilities in tackling\nthe bias problem. This research presents a comprehensive survey synthesising\nthe current trends and limitations in techniques used for identifying and\nmitigating bias in LLMs, where the overview of methods for tackling bias are\ngrouped into metrics, benchmark datasets, and mitigation strategies. The\nimportance and novelty of this survey are that it explores the perspective of\nunder-represented societies. We argue that current practices tackling the bias\nproblem cannot simply be 'plugged in' to address the needs of under-represented\nsocieties. We use examples from New Zealand to present requirements for\nadopting existing techniques to under-represented societies.",
        "pdf_link": "https://arxiv.org/pdf/2312.01509v1.pdf"
    },
    {
        "title": "D-Bot: Database Diagnosis System using Large Language Models",
        "authors": [
            "Xuanhe Zhou",
            "Guoliang Li",
            "Zhaoyan Sun",
            "Zhiyuan Liu",
            "Weize Chen",
            "Jianming Wu",
            "Jiesi Liu",
            "Ruohang Feng",
            "Guoyang Zeng"
        ],
        "published": "2023-12-03T16:58:10Z",
        "summary": "Database administrators (DBAs) play an important role in managing,\nmaintaining and optimizing database systems. However, it is hard and tedious\nfor DBAs to manage a large number of databases and give timely response\n(waiting for hours is intolerable in many online cases). In addition, existing\nempirical methods only support limited diagnosis scenarios, which are also\nlabor-intensive to update the diagnosis rules for database version updates.\nRecently large language models (LLMs) have shown great potential in various\nfields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can\nautomatically acquire knowledge from diagnosis documents, and generate\nreasonable and well-founded diagnosis report (i.e., identifying the root causes\nand solutions) within acceptable time (e.g., under 10 minutes compared to hours\nby a DBA). The techniques in D-Bot include (i) offline knowledge extraction\nfrom documents, (ii) automatic prompt generation (e.g., knowledge matching,\ntool retrieval), (iii) root cause analysis using tree search algorithm, and\n(iv) collaborative mechanism for complex anomalies with multiple root causes.\nWe verify D-Bot on real benchmarks (including 539 anomalies of six typical\napplications), and the results show that D-Bot can effectively analyze the root\ncauses of unseen anomalies and significantly outperforms traditional methods\nand vanilla models like GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.01454v2.pdf"
    },
    {
        "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
        "authors": [
            "James Enouen",
            "Hootan Nakhost",
            "Sayna Ebrahimi",
            "Sercan O Arik",
            "Yan Liu",
            "Tomas Pfister"
        ],
        "published": "2023-12-03T04:35:04Z",
        "summary": "Large language models (LLMs) have attracted huge interest in practical\napplications given their increasingly accurate responses and coherent reasoning\nabilities. Given their nature as black-boxes using complex reasoning processes\non their inputs, it is inevitable that the demand for scalable and faithful\nexplanations for LLMs' generated content will continue to grow. There have been\nmajor developments in the explainability of neural network models over the past\ndecade. Among them, post-hoc explainability methods, especially Shapley values,\nhave proven effective for interpreting deep learning models. However, there are\nmajor challenges in scaling up Shapley values for LLMs, particularly when\ndealing with long input contexts containing thousands of tokens and\nautoregressively generated output sequences. Furthermore, it is often unclear\nhow to effectively utilize generated explanations to improve the performance of\nLLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc\nexplanation method incorporating LM-specific techniques. We demonstrate that\nthis leads to significant increases in speed compared to conventional Shapley\nvalue computations, reducing processing times from hours to minutes for\ntoken-level explanations, and to just seconds for document-level explanations.\nIn addition, we demonstrate how real-time Shapley values can be utilized in two\nimportant scenarios, providing better understanding of long-document question\nanswering by localizing important words and sentences; and improving existing\ndocument retrieval systems through enhancing the accuracy of selected passages\nand ultimately the final responses.",
        "pdf_link": "https://arxiv.org/pdf/2312.01279v1.pdf"
    },
    {
        "title": "Running cognitive evaluations on large language models: The do's and the don'ts",
        "authors": [
            "Anna A. Ivanova"
        ],
        "published": "2023-12-03T04:28:19Z",
        "summary": "In this paper, I describe methodological considerations for studies that aim\nto evaluate the cognitive capacities of large language models (LLMs) using\nlanguage-based behavioral assessments. Drawing on three case studies from the\nliterature (a commonsense knowledge benchmark, a theory of mind evaluation, and\na test of syntactic agreement), I describe common pitfalls that might arise\nwhen applying a cognitive test to an LLM. I then list 10 do's and don'ts that\nshould help design high-quality cognitive evaluations for AI systems. I\nconclude by discussing four areas where the do's and don'ts are currently under\nactive discussion -- prompt sensitivity, cultural and linguistic diversity,\nusing LLMs as research assistants, and running evaluations on open vs. closed\nLLMs. Overall, the goal of the paper is to contribute to the broader discussion\nof best practices in the rapidly growing field of AI Psychology.",
        "pdf_link": "https://arxiv.org/pdf/2312.01276v1.pdf"
    },
    {
        "title": "Towards leveraging LLMs for Conditional QA",
        "authors": [
            "Syed-Amad Hussain",
            "Parag Pravin Dakle",
            "SaiKrishna Rallabandi",
            "Preethi Raghavan"
        ],
        "published": "2023-12-02T14:02:52Z",
        "summary": "This study delves into the capabilities and limitations of Large Language\nModels (LLMs) in the challenging domain of conditional question-answering.\nUtilizing the Conditional Question Answering (CQA) dataset and focusing on\ngenerative models like T5 and UL2, we assess the performance of LLMs across\ndiverse question types. Our findings reveal that fine-tuned LLMs can surpass\nthe state-of-the-art (SOTA) performance in some cases, even without fully\nencoding all input context, with an increase of 7-8 points in Exact Match (EM)\nand F1 scores for Yes/No questions. However, these models encounter challenges\nin extractive question answering, where they lag behind the SOTA by over 10\npoints, and in mitigating the risk of injecting false information. A study with\noracle-retrievers emphasizes the critical role of effective evidence retrieval,\nunderscoring the necessity for advanced solutions in this area. Furthermore, we\nhighlight the significant influence of evaluation metrics on performance\nassessments and advocate for a more comprehensive evaluation framework. The\ncomplexity of the task, the observed performance discrepancies, and the need\nfor effective evidence retrieval underline the ongoing challenges in this field\nand underscore the need for future work focusing on refining training tasks and\nexploring prompt-based techniques to enhance LLM performance in conditional\nquestion-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.01143v1.pdf"
    },
    {
        "title": "Large Language Models Are Zero-Shot Text Classifiers",
        "authors": [
            "Zhiqiang Wang",
            "Yiran Pang",
            "Yanbin Lin"
        ],
        "published": "2023-12-02T06:33:23Z",
        "summary": "Retrained large language models (LLMs) have become extensively used across\nvarious sub-disciplines of natural language processing (NLP). In NLP, text\nclassification problems have garnered considerable focus, but still faced with\nsome limitations related to expensive computational cost, time consumption, and\nrobust performance to unseen classes. With the proposal of chain of thought\nprompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with\nthe step by step reasoning prompts, instead of conventional question and answer\nformats. The zero-shot LLMs in the text classification problems can alleviate\nthese limitations by directly utilizing pretrained models to predict both seen\nand unseen classes. Our research primarily validates the capability of GPT\nmodels in text classification. We focus on effectively utilizing prompt\nstrategies to various text classification scenarios. Besides, we compare the\nperformance of zero shot LLMs with other state of the art text classification\nmethods, including traditional machine learning methods, deep learning methods,\nand ZSL methods. Experimental results demonstrate that the performance of LLMs\nunderscores their effectiveness as zero-shot text classifiers in three of the\nfour datasets analyzed. The proficiency is especially advantageous for small\nbusinesses or teams that may not have extensive knowledge in text\nclassification.",
        "pdf_link": "https://arxiv.org/pdf/2312.01044v1.pdf"
    },
    {
        "title": "Nash Learning from Human Feedback",
        "authors": [
            "R\u00e9mi Munos",
            "Michal Valko",
            "Daniele Calandriello",
            "Mohammad Gheshlaghi Azar",
            "Mark Rowland",
            "Zhaohan Daniel Guo",
            "Yunhao Tang",
            "Matthieu Geist",
            "Thomas Mesnard",
            "Andrea Michi",
            "Marco Selvi",
            "Sertan Girgin",
            "Nikola Momchev",
            "Olivier Bachem",
            "Daniel J. Mankowitz",
            "Doina Precup",
            "Bilal Piot"
        ],
        "published": "2023-12-01T19:26:23Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the main\nparadigm for aligning large language models (LLMs) with human preferences.\nTypically, RLHF involves the initial step of learning a reward model from human\nfeedback, often expressed as preferences between pairs of text generations\nproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by\noptimizing it to maximize the reward model through a reinforcement learning\nalgorithm. However, an inherent limitation of current reward models is their\ninability to fully represent the richness of human preferences and their\ndependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of\nLLMs using pairwise human feedback. Our approach entails the initial learning\nof a preference model, which is conditioned on two inputs given a prompt,\nfollowed by the pursuit of a policy that consistently generates responses\npreferred over those generated by any competing policy, thus defining the Nash\nequilibrium of this preference model. We term this approach Nash learning from\nhuman feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel\nalgorithmic solution, Nash-MD, founded on the principles of mirror descent.\nThis algorithm produces a sequence of policies, with the last iteration\nconverging to the regularized Nash equilibrium. Additionally, we explore\nparametric representations of policies and introduce gradient descent\nalgorithms for deep-learning architectures. To demonstrate the effectiveness of\nour approach, we present experimental results involving the fine-tuning of a\nLLM for a text summarization task. We believe NLHF offers a compelling avenue\nfor preference learning and policy optimization with the potential of advancing\nthe field of aligning LLMs with human preferences.",
        "pdf_link": "https://arxiv.org/pdf/2312.00886v3.pdf"
    },
    {
        "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
        "authors": [
            "Fabio Fehr",
            "James Henderson"
        ],
        "published": "2023-12-01T15:40:30Z",
        "summary": "The current paradigm of large-scale pre-training and fine-tuning Transformer\nlarge language models has lead to significant improvements across the board in\nnatural language processing. However, such large models are susceptible to\noverfitting to their training data, and as a result the models perform poorly\nwhen the domain changes. Also, due to the model's scale, the cost of\nfine-tuning the model to the new domain is large. Nonparametric Variational\nInformation Bottleneck (NVIB) has been proposed as a regulariser for training\ncross-attention in Transformers, potentially addressing the overfitting\nproblem. We extend the NVIB framework to replace all types of attention\nfunctions in Transformers, and show that existing pretrained Transformers can\nbe reinterpreted as Nonparametric Variational (NV) models using a proposed\nidentity initialisation. We then show that changing the initialisation\nintroduces a novel, information-theoretic post-training regularisation in the\nattention mechanism, which improves out-of-domain generalisation without any\ntraining. This success supports the hypothesis that pretrained Transformers are\nimplicitly NV Bayesian models.",
        "pdf_link": "https://arxiv.org/pdf/2312.00662v1.pdf"
    },
    {
        "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
        "authors": [
            "Aniket Deroy",
            "Subhankar Maity"
        ],
        "published": "2023-12-01T13:00:45Z",
        "summary": "The evolution of legal datasets and the advent of large language models\n(LLMs) have significantly transformed the legal field, particularly in the\ngeneration of case judgment summaries. However, a critical concern arises\nregarding the potential biases embedded within these summaries. This study\nscrutinizes the biases present in case judgment summaries produced by legal\ndatasets and large language models. The research aims to analyze the impact of\nbiases on legal decision making. By interrogating the accuracy, fairness, and\nimplications of biases in these summaries, this study contributes to a better\nunderstanding of the role of technology in legal contexts and the implications\nfor justice systems worldwide. In this study, we investigate biases wrt\nGender-related keywords, Race-related keywords, Keywords related to crime\nagainst women, Country names and religious keywords. The study shows\ninteresting evidences of biases in the outputs generated by the large language\nmodels and pre-trained abstractive summarization models. The reasoning behind\nthese biases needs further studies.",
        "pdf_link": "https://arxiv.org/pdf/2312.00554v1.pdf"
    },
    {
        "title": "LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices",
        "authors": [
            "Junchen Zhao",
            "Yurun Song",
            "Simeng Liu",
            "Ian G. Harris",
            "Sangeetha Abdu Jyothi"
        ],
        "published": "2023-12-01T07:19:42Z",
        "summary": "Deploying Large Language Models (LLMs) locally on mobile devices presents a\nsignificant challenge due to their extensive memory requirements. In this\npaper, we introduce LinguaLinked, a system for decentralized, distributed LLM\ninference on mobile devices. LinguaLinked enables collaborative execution of\nthe inference task across multiple trusted devices. LinguaLinked ensures data\nprivacy by processing information locally. LinguaLinked uses three key\nstrategies. First, an optimized model assignment technique segments LLMs and\nuses linear optimization to align segments with each device's capabilities.\nSecond, an optimized data transmission mechanism ensures efficient and\nstructured data flow between model segments while also maintaining the\nintegrity of the original model structure. Finally, LinguaLinked incorporates a\nruntime load balancer that actively monitors and redistributes tasks among\nmobile devices to prevent bottlenecks, enhancing the system's overall\nefficiency and responsiveness. We demonstrate that LinguaLinked facilitates\nefficient LLM inference while maintaining consistent throughput and minimal\nlatency through extensive testing across various mobile devices, from high-end\nto low-end Android devices. In our evaluations, compared to the baseline,\nLinguaLinked achieves an inference performance acceleration of $1.11\\times$ to\n$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ with\nmulti-threading. Additionally, runtime load balancing yields an overall\ninference acceleration of $1.29\\times$ to $1.32\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2312.00388v1.pdf"
    },
    {
        "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
        "authors": [
            "Viraj Mehta",
            "Vikramjeet Das",
            "Ojash Neopane",
            "Yijia Dai",
            "Ilija Bogunovic",
            "Jeff Schneider",
            "Willie Neiswanger"
        ],
        "published": "2023-12-01T00:54:02Z",
        "summary": "Preference-based feedback is important for many applications in reinforcement\nlearning where direct evaluation of a reward function is not feasible. A\nnotable recent example arises in reinforcement learning from human feedback\n(RLHF) on large language models. For many applications of RLHF, the cost of\nacquiring the human feedback can be substantial. In this work, we take\nadvantage of the fact that one can often choose contexts at which to obtain\nhuman feedback in order to most efficiently identify a good policy, and\nformalize this as an offline contextual dueling bandit problem. We give an\nupper-confidence-bound style algorithm for this problem and prove a polynomial\nworst-case regret bound. We then provide empirical confirmation in a synthetic\nsetting that our approach outperforms existing methods. After, we extend the\nsetting and methodology for practical use in RLHF training of large language\nmodels. Here, our method is able to reach better performance with fewer samples\nof human preferences than multiple baselines on three real-world datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.00267v1.pdf"
    },
    {
        "title": "Towards Accurate Differential Diagnosis with Large Language Models",
        "authors": [
            "Daniel McDuff",
            "Mike Schaekermann",
            "Tao Tu",
            "Anil Palepu",
            "Amy Wang",
            "Jake Garrison",
            "Karan Singhal",
            "Yash Sharma",
            "Shekoofeh Azizi",
            "Kavita Kulkarni",
            "Le Hou",
            "Yong Cheng",
            "Yun Liu",
            "S Sara Mahdavi",
            "Sushant Prakash",
            "Anupam Pathak",
            "Christopher Semturs",
            "Shwetak Patel",
            "Dale R Webster",
            "Ewa Dominowska",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Jake Sunshine",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2023-11-30T19:55:51Z",
        "summary": "An accurate differential diagnosis (DDx) is a cornerstone of medical care,\noften reached through an iterative process of interpretation that combines\nclinical history, physical examination, investigations and procedures.\nInteractive interfaces powered by Large Language Models (LLMs) present new\nopportunities to both assist and automate aspects of this process. In this\nstudy, we introduce an LLM optimized for diagnostic reasoning, and evaluate its\nability to generate a DDx alone or as an aid to clinicians. 20 clinicians\nevaluated 302 challenging, real-world medical cases sourced from the New\nEngland Journal of Medicine (NEJM) case reports. Each case report was read by\ntwo clinicians, who were randomized to one of two assistive conditions: either\nassistance from search engines and standard medical resources, or LLM\nassistance in addition to these tools. All clinicians provided a baseline,\nunassisted DDx prior to using the respective assistive tools. Our LLM for DDx\nexhibited standalone performance that exceeded that of unassisted clinicians\n(top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study\narms, the DDx quality score was higher for clinicians assisted by our LLM\n(top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%)\n(McNemar's Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75, p =\n0.03). Further, clinicians assisted by our LLM arrived at more comprehensive\ndifferential lists than those without its assistance. Our study suggests that\nour LLM for DDx has potential to improve clinicians' diagnostic reasoning and\naccuracy in challenging cases, meriting further real-world evaluation for its\nability to empower physicians and widen patients' access to specialist-level\nexpertise.",
        "pdf_link": "https://arxiv.org/pdf/2312.00164v1.pdf"
    },
    {
        "title": "PoseGPT: Chatting about 3D Human Pose",
        "authors": [
            "Yao Feng",
            "Jing Lin",
            "Sai Kumar Dwivedi",
            "Yu Sun",
            "Priyanka Patel",
            "Michael J. Black"
        ],
        "published": "2023-11-30T18:59:52Z",
        "summary": "We introduce PoseGPT, a framework employing Large Language Models (LLMs) to\nunderstand and reason about 3D human poses from images or textual descriptions.\nOur work is motivated by the human ability to intuitively understand postures\nfrom a single image or a brief description, a process that intertwines image\ninterpretation, world knowledge, and an understanding of body language.\nTraditional human pose estimation methods, whether image-based or text-based,\noften lack holistic scene comprehension and nuanced reasoning, leading to a\ndisconnect between visual data and its real-world implications. PoseGPT\naddresses these limitations by embedding SMPL poses as a distinct signal token\nwithin a multi-modal LLM, enabling direct generation of 3D body poses from both\ntextual and visual inputs. This approach not only simplifies pose prediction\nbut also empowers LLMs to apply their world knowledge in reasoning about human\nposes, fostering two advanced tasks: speculative pose generation and reasoning\nabout pose estimation. These tasks involve reasoning about humans to generate\n3D poses from subtle text queries, possibly accompanied by images. We establish\nbenchmarks for these tasks, moving beyond traditional 3D pose generation and\nestimation methods. Our results show that PoseGPT outperforms existing\nmultimodal LLMs and task-sepcific methods on these newly proposed tasks.\nFurthermore, PoseGPT's ability to understand and generate 3D human poses based\non complex reasoning opens new directions in human pose analysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.18836v1.pdf"
    },
    {
        "title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
        "authors": [
            "Hiroki Furuta",
            "Yutaka Matsuo",
            "Aleksandra Faust",
            "Izzeddin Gur"
        ],
        "published": "2023-11-30T17:50:47Z",
        "summary": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for task\ncompositionality, their performance further degrades under different\ninstruction compositions changing combinational order. In contrast to the\nrecent remarkable success of LMA, our benchmark and detailed analysis emphasize\nthe necessity of building LMAs that are robust and generalizable to task\ncompositionality for real-world deployment.",
        "pdf_link": "https://arxiv.org/pdf/2311.18751v2.pdf"
    },
    {
        "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
        "authors": [
            "Yingdi Guo"
        ],
        "published": "2023-11-30T15:06:50Z",
        "summary": "With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.",
        "pdf_link": "https://arxiv.org/pdf/2311.18609v1.pdf"
    },
    {
        "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",
        "authors": [
            "Shiyao Cui",
            "Zhenyu Zhang",
            "Yilong Chen",
            "Wenyuan Zhang",
            "Tianyun Liu",
            "Siqi Wang",
            "Tingwen Liu"
        ],
        "published": "2023-11-30T14:18:47Z",
        "summary": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.",
        "pdf_link": "https://arxiv.org/pdf/2311.18580v1.pdf"
    },
    {
        "title": "OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition",
        "authors": [
            "Tongjia Chen",
            "Hongshan Yu",
            "Zhengeng Yang",
            "Zechuan Li",
            "Wei Sun",
            "Chen Chen"
        ],
        "published": "2023-11-30T13:32:43Z",
        "summary": "Due to the resource-intensive nature of training vision-language models on\nexpansive video data, a majority of studies have centered on adapting\npre-trained image-language models to the video domain. Dominant pipelines\npropose to tackle the visual discrepancies with additional temporal learners\nwhile overlooking the substantial discrepancy for web-scaled descriptive\nnarratives and concise action category names, leading to less distinct semantic\nspace and potential performance limitations. In this work, we prioritize the\nrefinement of text knowledge to facilitate generalizable video recognition. To\naddress the limitations of the less distinct semantic space of category names,\nwe prompt a large language model (LLM) to augment action class names into\nSpatio-Temporal Descriptors thus bridging the textual discrepancy and serving\nas a knowledge base for general recognition. Moreover, to assign the best\ndescriptors with different video instances, we propose Optimal Descriptor\nSolver, forming the video recognition problem as solving the optimal matching\nflow across frame-level representations and descriptors. Comprehensive\nevaluations in zero-shot, few-shot, and fully supervised video recognition\nhighlight the effectiveness of our approach. Our best model achieves a\nstate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
        "pdf_link": "https://arxiv.org/pdf/2312.00096v2.pdf"
    },
    {
        "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
        "authors": [
            "Akira Kawabata",
            "Saku Sugawara"
        ],
        "published": "2023-11-30T08:44:55Z",
        "summary": "To precisely evaluate a language model's capability for logical reading\ncomprehension, we present a dataset for testing the understanding of the\nrationale behind critical reasoning. For questions taken from an existing\nmultiplechoice logical reading comprehension dataset, we crowdsource rationale\ntexts that explain why we should select or eliminate answer options, resulting\nin 3,003 multiple-choice subquestions that are associated with 943 main\nquestions. Experiments on our dataset show that recent large language models\n(e.g., InstructGPT) struggle to answer the subquestions even if they are able\nto answer the main questions correctly. We find that the models perform\nparticularly poorly in answering subquestions written for the incorrect options\nof the main questions, implying that the models have a limited capability for\nexplaining why incorrect alternatives should be eliminated. These results\nsuggest that our dataset encourages further investigation into the critical\nreasoning ability of language models while focusing on the elimination process\nof relevant alternatives.",
        "pdf_link": "https://arxiv.org/pdf/2311.18353v1.pdf"
    },
    {
        "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
        "authors": [
            "Anwen Hu",
            "Yaya Shi",
            "Haiyang Xu",
            "Jiabo Ye",
            "Qinghao Ye",
            "Ming Yan",
            "Chenliang Li",
            "Qi Qian",
            "Ji Zhang",
            "Fei Huang"
        ],
        "published": "2023-11-30T04:43:26Z",
        "summary": "Recently, the strong text creation ability of Large Language Models(LLMs) has\ngiven rise to many tools for assisting paper reading or even writing. However,\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\ntheir application scenarios, especially for scientific academic paper writing.\nIn this work, towards a more versatile copilot for academic paper writing, we\nmainly focus on strengthening the multi-modal diagram analysis ability of\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\naligning diagrams in the paper with related paragraphs, we construct\nprofessional diagram analysis samples for training and evaluation. M-Paper is\nthe first dataset to support joint comprehension of multiple scientific\ndiagrams, including figures and tables in the format of images or Latex codes.\nBesides, to better align the copilot with the user's intention, we introduce\nthe `outline' as the control signal, which could be directly given by the user\nor revised based on auto-generated ones. Comprehensive experiments with a\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\nstronger scientific diagram understanding performance, including diagram\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\nand model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.",
        "pdf_link": "https://arxiv.org/pdf/2311.18248v2.pdf"
    },
    {
        "title": "Large Language Models for Travel Behavior Prediction",
        "authors": [
            "Baichuan Mo",
            "Hanyong Xu",
            "Dingyi Zhuang",
            "Ruoyun Ma",
            "Xiaotong Guo",
            "Jinhua Zhao"
        ],
        "published": "2023-11-30T04:35:55Z",
        "summary": "Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.00819v1.pdf"
    },
    {
        "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
        "authors": [
            "Sungjoo Byun",
            "Dongjun Jang",
            "Hyemi Jo",
            "Hyopil Shin"
        ],
        "published": "2023-11-30T03:19:45Z",
        "summary": "Caution: this paper may include material that could be offensive or\ndistressing.\n  The advent of Large Language Models (LLMs) necessitates the development of\ntraining approaches that mitigate the generation of unethical language and\naptly manage toxic user queries. Given the challenges related to human labor\nand the scarcity of data, we present KoTox, comprising 39K unethical\ninstruction-output pairs. This collection of automatically generated toxic\ninstructions refines the training of LLMs and establishes a foundational\nframework for improving LLMs' ethical awareness and response to various toxic\ninputs, promoting more secure and responsible interactions in Natural Language\nProcessing (NLP) applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.18215v1.pdf"
    },
    {
        "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
        "authors": [
            "Yongqiang Chen",
            "Binghui Xie",
            "Kaiwen Zhou",
            "Bo Han",
            "Yatao Bian",
            "James Cheng"
        ],
        "published": "2023-11-30T02:26:55Z",
        "summary": "In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts",
        "pdf_link": "https://arxiv.org/pdf/2311.18194v1.pdf"
    },
    {
        "title": "Zero-shot Conversational Summarization Evaluations with small Large Language Models",
        "authors": [
            "Ramesh Manuvinakurike",
            "Saurav Sahay",
            "Sangeeta Manepalli",
            "Lama Nachman"
        ],
        "published": "2023-11-29T19:34:34Z",
        "summary": "Large Language Models (LLMs) exhibit powerful summarization abilities.\nHowever, their capabilities on conversational summarization remains under\nexplored. In this work we evaluate LLMs (approx. 10 billion parameters) on\nconversational summarization and showcase their performance on various prompts.\nWe show that the summaries generated by models depend on the instructions and\nthe performance of LLMs vary with different instructions sometimes resulting\nsteep drop in ROUGE scores if prompts are not selected carefully. We also\nevaluate the models with human evaluations and discuss the limitations of the\nmodels on conversational summarization",
        "pdf_link": "https://arxiv.org/pdf/2311.18041v1.pdf"
    },
    {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "authors": [
            "Qidong Huang",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Bin Wang",
            "Conghui He",
            "Jiaqi Wang",
            "Dahua Lin",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2023-11-29T18:57:07Z",
        "summary": "Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Based on the observation, OPERA introduces a\npenalty term on the model logits during the beam-search decoding to mitigate\nthe over-trust issue, along with a rollback strategy that retrospects the\npresence of summary tokens in the previously generated tokens, and re-allocate\nthe token selection if necessary. With extensive experiments, OPERA shows\nsignificant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.",
        "pdf_link": "https://arxiv.org/pdf/2311.17911v3.pdf"
    },
    {
        "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models",
        "authors": [
            "Xin Liu",
            "Yichen Zhu",
            "Jindong Gu",
            "Yunshi Lan",
            "Chao Yang",
            "Yu Qiao"
        ],
        "published": "2023-11-29T12:49:45Z",
        "summary": "The security concerns surrounding Large Language Models (LLMs) have been\nextensively explored, yet the safety of Multimodal Large Language Models\n(MLLMs) remains understudied. In this paper, we observe that Multimodal Large\nLanguage Models (MLLMs) can be easily compromised by query-relevant images, as\nif the text query itself were malicious. To address this, we introduce\nMM-SafetyBench, a comprehensive framework designed for conducting\nsafety-critical evaluations of MLLMs against such image-based manipulations. We\nhave compiled a dataset comprising 13 scenarios, resulting in a total of 5,040\ntext-image pairs. Our analysis across 12 state-of-the-art models reveals that\nMLLMs are susceptible to breaches instigated by our approach, even when the\nequipped LLMs have been safety-aligned. In response, we propose a\nstraightforward yet effective prompting strategy to enhance the resilience of\nMLLMs against these types of attacks. Our work underscores the need for a\nconcerted effort to strengthen and enhance the safety measures of open-source\nMLLMs against potential malicious exploits. The resource is available at\n\\href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.",
        "pdf_link": "https://arxiv.org/pdf/2311.17600v2.pdf"
    },
    {
        "title": "TaskWeaver: A Code-First Agent Framework",
        "authors": [
            "Bo Qiao",
            "Liqun Li",
            "Xu Zhang",
            "Shilin He",
            "Yu Kang",
            "Chaoyun Zhang",
            "Fangkai Yang",
            "Hang Dong",
            "Jue Zhang",
            "Lu Wang",
            "Minghua Ma",
            "Pu Zhao",
            "Si Qin",
            "Xiaoting Qin",
            "Chao Du",
            "Yong Xu",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "published": "2023-11-29T11:23:42Z",
        "summary": "Large Language Models (LLMs) have shown impressive abilities in natural\nlanguage understanding and generation, leading to their use in applications\nsuch as chatbots and virtual assistants. However, existing LLM frameworks face\nlimitations in handling domain-specific data analytics tasks with rich data\nstructures. Moreover, they struggle with flexibility to meet diverse user\nrequirements. To address these issues, TaskWeaver is proposed as a code-first\nframework for building LLM-powered autonomous agents. It converts user requests\ninto executable code and treats user-defined plugins as callable functions.\nTaskWeaver provides support for rich data structures, flexible plugin usage,\nand dynamic plugin selection, and leverages LLM coding capabilities for complex\nlogic. It also incorporates domain-specific knowledge through examples and\nensures the secure execution of generated code. TaskWeaver offers a powerful\nand flexible framework for creating intelligent conversational agents that can\nhandle complex tasks and adapt to domain-specific scenarios. The code is\nopen-sourced at https://github.com/microsoft/TaskWeaver/.",
        "pdf_link": "https://arxiv.org/pdf/2311.17541v2.pdf"
    },
    {
        "title": "LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World",
        "authors": [
            "Siwei Chen",
            "Anxing Xiao",
            "David Hsu"
        ],
        "published": "2023-11-29T07:23:22Z",
        "summary": "This work addresses the problem of long-horizon task planning with the Large\nLanguage Model (LLM) in an open-world household environment. Existing works\nfail to explicitly track key objects and attributes, leading to erroneous\ndecisions in long-horizon tasks, or rely on highly engineered state features\nand feedback, which is not generalizable. We propose a novel, expandable state\nrepresentation that provides continuous expansion and updating of object\nattributes from the LLM's inherent capabilities for context understanding and\nhistorical action reasoning. Our proposed representation maintains a\ncomprehensive record of an object's attributes and changes, enabling robust\nretrospective summary of the sequence of actions leading to the current state.\nThis allows enhanced context understanding for decision-making in task\nplanning. We validate our model through experiments across simulated and\nreal-world task planning scenarios, demonstrating significant improvements over\nbaseline methods in a variety of tasks requiring long-horizon state tracking\nand reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.17406v1.pdf"
    },
    {
        "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
        "authors": [
            "Lujia Shen",
            "Yuwen Pu",
            "Shouling Ji",
            "Changjiang Li",
            "Xuhong Zhang",
            "Chunpeng Ge",
            "Ting Wang"
        ],
        "published": "2023-11-29T07:09:13Z",
        "summary": "Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.17400v2.pdf"
    },
    {
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "authors": [
            "Jiaxin Wen",
            "Pei Ke",
            "Hao Sun",
            "Zhexin Zhang",
            "Chengfei Li",
            "Jinfeng Bai",
            "Minlie Huang"
        ],
        "published": "2023-11-29T06:42:36Z",
        "summary": "The open-endedness of large language models (LLMs) combined with their\nimpressive capabilities may lead to new safety issues when being exploited for\nmalicious use. While recent studies primarily focus on probing toxic outputs\nthat can be easily detected with existing toxicity classifiers, we show that\nLLMs can generate diverse implicit toxic outputs that are exceptionally\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\nreinforcement learning (RL) based attacking method to further induce the\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\nthe attack success rate can be significantly improved through RL fine-tuning.\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\na significant threat in generating undetectable implicit toxic outputs. We\nfurther show that fine-tuning toxicity classifiers on the annotated examples\nfrom our attacking method can effectively enhance their ability to detect\nLLM-generated implicit toxic language. The code is publicly available at\nhttps://github.com/thu-coai/Implicit-Toxicity.",
        "pdf_link": "https://arxiv.org/pdf/2311.17391v1.pdf"
    },
    {
        "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study",
        "authors": [
            "Han Cao",
            "Lingwei Wei",
            "Mengyang Chen",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-11-29T05:04:52Z",
        "summary": "Recently, Large Language Models (LLMs) have drawn significant attention due\nto their outstanding reasoning capabilities and extensive knowledge repository,\npositioning them as superior in handling various natural language processing\ntasks compared to other language models. In this paper, we present a\npreliminary investigation into the potential of LLMs in fact-checking. This\nstudy aims to comprehensively evaluate various LLMs in tackling specific\nfact-checking subtasks, systematically evaluating their capabilities, and\nconducting a comparative analysis of their performance against pre-trained and\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\nachieve competitive performance compared to other small models in most\nscenarios. However, they encounter challenges in effectively handling Chinese\nfact verification and the entirety of the fact-checking pipeline due to\nlanguage inconsistencies and hallucinations. These findings underscore the need\nfor further exploration and research to enhance the proficiency of LLMs as\nreliable fact-checkers, unveiling the potential capability of LLMs and the\npossible challenges in fact-checking tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.17355v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Human Mobility Prediction under Public Events",
        "authors": [
            "Yuebing Liang",
            "Yichao Liu",
            "Xiaohan Wang",
            "Zhan Zhao"
        ],
        "published": "2023-11-29T04:25:15Z",
        "summary": "Public events, such as concerts and sports games, can be major attractors for\nlarge crowds, leading to irregular surges in travel demand. Accurate human\nmobility prediction for public events is thus crucial for event planning as\nwell as traffic or crowd management. While rich textual descriptions about\npublic events are commonly available from online sources, it is challenging to\nencode such information in statistical or machine learning models. Existing\nmethods are generally limited in incorporating textual information, handling\ndata sparsity, or providing rationales for their predictions. To address these\nchallenges, we introduce a framework for human mobility prediction under public\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\nunprecedented ability to process textual data, learn from minimal examples, and\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\nraw, unstructured event descriptions from online sources into a standardized\nformat, and then segments historical mobility data into regular and\nevent-related components. A prompting strategy is designed to direct LLMs in\nmaking and rationalizing demand predictions considering historical mobility and\nevent features. A case study is conducted for Barclays Center in New York City,\nbased on publicly available event information and taxi trip data. Results show\nthat LLM-MPE surpasses traditional models, particularly on event days, with\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\ninterpretable insights into its predictions. Despite the great potential of\nLLMs, we also identify key challenges including misinformation and high costs\nthat remain barriers to their broader adoption in large-scale human mobility\nanalysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.17351v1.pdf"
    },
    {
        "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner",
        "authors": [
            "Lizhao Liu",
            "Xinyu Sun",
            "Tianhang Xiang",
            "Zhuangwei Zhuang",
            "Liuren Yin",
            "Mingkui Tan"
        ],
        "published": "2023-11-29T03:29:46Z",
        "summary": "We study the task of extending the large language model (LLM) into a\nvision-language instruction-following model. This task is crucial but\nchallenging since the LLM is trained on text modality only, making it hard to\neffectively digest the visual modality. To address this, existing methods\ntypically train a visual adapter to align the representation between a\npre-trained vision transformer (ViT) and the LLM by a generative image\ncaptioning loss. However, we find that the generative objective can only\nproduce weak alignment for vision and language, making the aligned\nvision-language model very hungry for the instruction fine-tuning data. In this\npaper, we propose CG-VLM that applies both Contrastive and Generative alignment\nobjectives to effectively align the representation of ViT and LLM. Different\nfrom image level and sentence level alignment in common contrastive learning\nsettings, CG-VLM aligns the image-patch level features and text-token level\nembeddings, which, however, is very hard to achieve as no explicit grounding\npatch-token relation provided in standard image captioning datasets. To address\nthis issue, we propose to maximize the averaged similarity between pooled\nimage-patch features and text-token embeddings. Extensive experiments\ndemonstrate that the proposed CG-VLM produces strong vision-language alignment\nand is an efficient instruction learner. For example, using only 10%\ninstruction tuning data, we reach 95% performance of state-of-the-art method\nLLaVA [29] on the zero-shot ScienceQA-Image benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2311.17945v1.pdf"
    },
    {
        "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
        "authors": [
            "Meriem Boubdir",
            "Edward Kim",
            "Beyza Ermis",
            "Sara Hooker",
            "Marzieh Fadaee"
        ],
        "published": "2023-11-29T00:45:23Z",
        "summary": "In Natural Language Processing (NLP), the Elo rating system, originally\ndesigned for ranking players in dynamic games such as chess, is increasingly\nbeing used to evaluate Large Language Models (LLMs) through \"A vs B\" paired\ncomparisons. However, while popular, the system's suitability for assessing\nentities with constant skill levels, such as LLMs, remains relatively\nunexplored. We study two fundamental axioms that evaluation methods should\nadhere to: reliability and transitivity. We conduct extensive evaluation of Elo\nbehaviour, illustrating that individual Elo computations exhibit volatility and\ndelving into the impact of varying the Elo rating system's hyperparameters. We\nshow that these axioms are not always satisfied raising questions about the\nreliability of current comparative evaluations of LLMs. If the current use of\nElo scores is intended to substitute the costly head-to-head comparison of\nLLMs, it is crucial to ensure the ranking is as robust as possible. Guided by\nthe axioms, our findings offer concrete guidelines for enhancing the\nreliability of LLM evaluation methods, suggesting a need for reassessment of\nexisting comparative approaches.",
        "pdf_link": "https://arxiv.org/pdf/2311.17295v1.pdf"
    },
    {
        "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
        "authors": [
            "Wenyue Hua",
            "Lizhou Fan",
            "Lingyao Li",
            "Kai Mei",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Libby Hemphill",
            "Yongfeng Zhang"
        ],
        "published": "2023-11-28T20:59:49Z",
        "summary": "Can we avoid wars at the crossroads of history? This question has been\npursued by individuals, scholars, policymakers, and organizations throughout\nhuman history. In this research, we attempt to answer the question based on the\nrecent advances of Artificial Intelligence (AI) and Large Language Models\n(LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\nsimulate the participating countries, their decisions, and the consequences, in\nhistorical international conflicts, including the World War I (WWI), the World\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\nevaluating the simulation effectiveness, we examine the advancements and\nlimitations of cutting-edge AI systems' abilities in studying complex\ncollective human behaviors such as international conflicts under diverse\nsettings. In these simulations, the emergent interactions among agents also\noffer a novel perspective for examining the triggers and conditions that lead\nto war. Our findings offer data-driven and AI-augmented insights that can\nredefine how we approach conflict resolution and peacekeeping strategies. The\nimplications stretch beyond historical analysis, offering a blueprint for using\nAI to understand human history and possibly prevent future international\nconflicts. Code and data are available at\n\\url{https://github.com/agiresearch/WarAgent}.",
        "pdf_link": "https://arxiv.org/pdf/2311.17227v2.pdf"
    },
    {
        "title": "Scalable Extraction of Training Data from (Production) Language Models",
        "authors": [
            "Milad Nasr",
            "Nicholas Carlini",
            "Jonathan Hayase",
            "Matthew Jagielski",
            "A. Feder Cooper",
            "Daphne Ippolito",
            "Christopher A. Choquette-Choo",
            "Eric Wallace",
            "Florian Tram\u00e8r",
            "Katherine Lee"
        ],
        "published": "2023-11-28T18:47:03Z",
        "summary": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
        "pdf_link": "https://arxiv.org/pdf/2311.17035v1.pdf"
    },
    {
        "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
        "authors": [
            "Zhiyuan Zhao",
            "Bin Wang",
            "Linke Ouyang",
            "Xiaoyi Dong",
            "Jiaqi Wang",
            "Conghui He"
        ],
        "published": "2023-11-28T14:54:37Z",
        "summary": "Multimodal large language models have made significant advancements in recent\nyears, yet they still suffer from a common issue known as the \"hallucination\nproblem\", in which the models generate textual descriptions that inaccurately\ndepict or entirely fabricate content from associated images. This paper\nintroduces a novel solution, Hallucination-Aware Direct Preference Optimization\n(HA-DPO), which reframes the hallucination problem as a preference selection\ntask. The model is trained to favor the non-hallucinating response when\npresented with two responses of the same image (one accurate and one\nhallucinatory). Furthermore, this paper proposes an efficient pipeline for\nconstructing positive~(non-hallucinatory) and negative~(hallucinatory) sample\npairs, ensuring a high-quality, style-consistent dataset for robust preference\nlearning. When applied to three mainstream multimodal models, HA-DPO\nsignificantly reduced hallucination issues and amplified the models'\ngeneralization capabilities. Notably, the MiniGPT-4 model, when enhanced with\nHA-DPO, demonstrated a substantial improvement: POPE accuracy rose from 51.13%\nto 86.13% (an absolute improvement of 35%), and the MME score surged from\n932.00 to 1326.46 (a relative improvement of 42.32%). The codes, models, and\ndatasets are made accessible at https://opendatalab.github.io/HA-DPO.",
        "pdf_link": "https://arxiv.org/pdf/2311.16839v2.pdf"
    },
    {
        "title": "Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis",
        "authors": [
            "Xiaohui Chen",
            "Yongfei Liu",
            "Yingxiang Yang",
            "Jianbo Yuan",
            "Quanzeng You",
            "Li-Ping Liu",
            "Hongxia Yang"
        ],
        "published": "2023-11-28T14:51:13Z",
        "summary": "Recent advancements in text-to-image (T2I) generative models have shown\nremarkable capabilities in producing diverse and imaginative visuals based on\ntext prompts. Despite the advancement, these diffusion models sometimes\nstruggle to translate the semantic content from the text into images entirely.\nWhile conditioning on the layout has shown to be effective in improving the\ncompositional ability of T2I diffusion models, they typically require manual\nlayout input. In this work, we introduce a novel approach to improving T2I\ndiffusion models using Large Language Models (LLMs) as layout generators. Our\nmethod leverages the Chain-of-Thought prompting of LLMs to interpret text and\ngenerate spatially reasonable object layouts. The generated layout is then used\nto enhance the generated images' composition and spatial accuracy. Moreover, we\npropose an efficient adapter based on a cross-attention mechanism, which\nexplicitly integrates the layout information into the stable diffusion models.\nOur experiments demonstrate significant improvements in image quality and\nlayout accuracy, showcasing the potential of LLMs in augmenting generative\nimage models.",
        "pdf_link": "https://arxiv.org/pdf/2311.17126v1.pdf"
    },
    {
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop",
        "authors": [
            "Martin Briesch",
            "Dominik Sobania",
            "Franz Rothlauf"
        ],
        "published": "2023-11-28T14:36:43Z",
        "summary": "Large language models (LLM) have become state of the art in many benchmarks\nand conversational LLM applications like ChatGPT are now widely used by the\npublic. Those LLMs can be used to generate large amounts of content which is\nposted on the internet to various platforms. As LLMs are trained on datasets\nusually collected from the internet, this LLM-generated content might be used\nto train the next generation of LLMs. Therefore, a self-consuming training loop\nemerges in which new LLM generations are trained on the output from the\nprevious generations. We empirically study this self-consuming training loop\nusing a novel dataset to analytically and accurately measure quality and\ndiversity of generated outputs. We find that this self-consuming training loop\ninitially improves both quality and diversity. However, after a few generations\nthe output inevitably degenerates in diversity. We find that the rate of\ndegeneration depends on the proportion of real and generated data.",
        "pdf_link": "https://arxiv.org/pdf/2311.16822v1.pdf"
    },
    {
        "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Lingzhi Wang",
            "Yiming Du",
            "Liang Chen",
            "Jingyan Zhou",
            "Yufei Wang",
            "Kam-Fai Wong"
        ],
        "published": "2023-11-28T13:51:32Z",
        "summary": "Dialogue systems, including task-oriented_dialogue_system (TOD) and\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\nwith language_models (LM) playing a central role. This survey delves into the\nhistorical trajectory of dialogue systems, elucidating their intricate\nrelationship with advancements in language models by categorizing this\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\nintrinsic knowledge in their parameters; 3) fusion between different types of\ndialogue systems with the advert of pre-trained_language_models (PLMs),\nstarting from the fusion between four_sub-tasks_within_TOD, and then\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\nchronological perspective aligned with LM breakthroughs, offering a\ncomprehensive review of state-of-the-art research outcomes. What's more, we\nfocus on emerging topics and discuss open challenges, providing valuable\ninsights into future directions for LLM-based_dialogue_systems. Through this\nexploration, we pave the way for a deeper_comprehension of the evolution,\nguiding future developments in LM-based dialogue_systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.16789v1.pdf"
    },
    {
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
        "authors": [
            "Yijun Yang",
            "Tianyi Zhou",
            "Kanxue Li",
            "Dapeng Tao",
            "Lusong Li",
            "Li Shen",
            "Xiaodong He",
            "Jing Jiang",
            "Yuhui Shi"
        ],
        "published": "2023-11-28T11:53:56Z",
        "summary": "While large language models (LLMs) excel in a simulated world of texts, they\nstruggle to interact with the more realistic world without perceptions of other\nmodalities such as visual or audio signals. Although vision-language models\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\nmay possess prior knowledge of world dynamics (as demonstrated in the text\nworld), they have not been trained in an embodied visual world and thus cannot\nalign with its dynamics. On the other hand, training an embodied agent in a\nnoisy visual world without expert guidance is often challenging and\ninefficient. In this paper, we train a VLM agent living in a visual world using\nan LLM agent excelling in a parallel text world. Specifically, we distill LLM's\nreflection outcomes (improved actions by analyzing mistakes) in a text world's\ntasks to finetune the VLM on the same tasks of the visual world, resulting in\nan Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world\ndynamics. Such cross-modality imitation learning between the two parallel\nworlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize\nto a broad scope of new tasks without any further guidance from the LLM expert.\nExtensive evaluations on the ALFWorld benchmark's diverse tasks highlight\nEMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement\nin the success rate.",
        "pdf_link": "https://arxiv.org/pdf/2311.16714v2.pdf"
    },
    {
        "title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?",
        "authors": [
            "Romain Lacombe",
            "Kerrie Wu",
            "Eddie Dilworth"
        ],
        "published": "2023-11-28T10:26:57Z",
        "summary": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.17107v1.pdf"
    },
    {
        "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
        "authors": [
            "Bohao Li",
            "Yuying Ge",
            "Yixiao Ge",
            "Guangzhi Wang",
            "Rui Wang",
            "Ruimao Zhang",
            "Ying Shan"
        ],
        "published": "2023-11-28T05:53:55Z",
        "summary": "Multimodal large language models (MLLMs), building upon the foundation of\npowerful large language models (LLMs), have recently demonstrated exceptional\ncapabilities in generating not only texts but also images given interleaved\nmultimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However,\nexisting MLLM benchmarks remain limited to assessing only models' comprehension\nability of single image-text inputs, failing to keep up with the strides made\nin MLLMs. A comprehensive benchmark is imperative for investigating the\nprogress and uncovering the limitations of current MLLMs. In this work, we\ncategorize the capabilities of MLLMs into hierarchical levels from $L_0$ to\n$L_4$ based on the modalities they can accept and generate, and propose\nSEED-Bench-2, a comprehensive benchmark that evaluates the\n\\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2\ncomprises 24K multiple-choice questions with accurate human annotations, which\nspans 27 dimensions, including the evaluation of both text and image\ngeneration. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 23 prominent open-source\nMLLMs and summarize valuable observations. By revealing the limitations of\nexisting MLLMs through extensive evaluations, we aim for SEED-Bench-2 to\nprovide insights that will motivate future research towards the goal of General\nArtificial Intelligence. Dataset and evaluation code are available at\n\\href{https://github.com/AILab-CVC/SEED-Bench}",
        "pdf_link": "https://arxiv.org/pdf/2311.17092v1.pdf"
    },
    {
        "title": "Methods to Estimate Large Language Model Confidence",
        "authors": [
            "Maia Kotelanski",
            "Robert Gallo",
            "Ashwin Nayak",
            "Thomas Savage"
        ],
        "published": "2023-11-28T05:44:06Z",
        "summary": "Large Language Models have difficulty communicating uncertainty, which is a\nsignificant obstacle to applying LLMs to complex medical tasks. This study\nevaluates methods to measure LLM confidence when suggesting a diagnosis for\nchallenging clinical vignettes. GPT4 was asked a series of challenging case\nquestions using Chain of Thought and Self Consistency prompting. Multiple\nmethods were investigated to assess model confidence and evaluated on their\nability to predict the models observed accuracy. The methods evaluated were\nIntrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC\nAgreement Frequency correlated with observed accuracy, yielding a higher Area\nunder the Receiver Operating Characteristic Curve compared to Intrinsic\nConfidence and CoT Length analysis. SC agreement is the most useful proxy for\nmodel confidence, especially for medical diagnosis. Model Intrinsic Confidence\nand CoT Response Length exhibit a weaker ability to differentiate between\ncorrect and incorrect answers, preventing them from being reliable and\ninterpretable markers for model confidence. We conclude GPT4 has a limited\nability to assess its own diagnostic accuracy. SC Agreement Frequency is the\nmost useful method to measure GPT4 confidence.",
        "pdf_link": "https://arxiv.org/pdf/2312.03733v2.pdf"
    },
    {
        "title": "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA",
        "authors": [
            "Damjan Kalajdzievski"
        ],
        "published": "2023-11-28T03:23:20Z",
        "summary": "As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.",
        "pdf_link": "https://arxiv.org/pdf/2312.03732v1.pdf"
    },
    {
        "title": "Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous Dequantization",
        "authors": [
            "Jinhao Li",
            "Shiyao Li",
            "Jiaming Xu",
            "Shan Huang",
            "Yaoxiu Lian",
            "Jun Liu",
            "Yu Wang",
            "Guohao Dai"
        ],
        "published": "2023-11-28T02:44:59Z",
        "summary": "Large language models (LLMs) have demonstrated impressive abilities in\nvarious domains while the inference cost is expensive. The state-of-the-art\nmethods use 2-bit quantization for mainstream LLMs. However, challenges still\nexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are\nquantized by groups, while the ranges of weights are large in some groups,\nresulting in large quantization errors and nonnegligible accuracy loss (e.g.\n>3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited\naccuracy improvement by adding 4-bit weights. Increasing 10% extra average bit\nmore 4-bit weights only leads to <0.5% accuracy improvement on a quantized\nLlama2-7b. (3) Time-consuming dequantization operations on GPUs. The\ndequantization operations lead to >50% execution time, hindering the potential\nof reducing LLM inference cost. To tackle these challenges, we propose the\nfollowing techniques: (1) We only quantize a small fraction of groups with the\nlarger range using 4-bit with memory alignment consideration on GPUs.(2) We\ndesign the asynchronous dequantization on GPUs, leading to up to 3.92X speedup.\nWe conduct extensive experiments on different model sizes. We achieve 2.85-bit\nfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over the\noriginal model, and we reduce both runtime cost and hardware cost by up to\n2.70X and 2.81X with less GPU requirements.",
        "pdf_link": "https://arxiv.org/pdf/2311.16442v2.pdf"
    },
    {
        "title": "Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation",
        "authors": [
            "Samuele Poppi",
            "Tobia Poppi",
            "Federico Cocchi",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ],
        "published": "2023-11-27T19:02:17Z",
        "summary": "Vision-and-Language models such as CLIP have demonstrated remarkable\neffectiveness across a wide range of tasks. However, these models are typically\ntrained on web-scale data, which can introduce inappropriate content and lead\nto the development of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcern in their adoption. To overcome these limitations, we introduce a\nmethodology to make Vision-and-Language models safer by removing their\nsensitivity to not-safe-for-work concepts. We show how this can be done by\ndistilling from a large language model which converts between safe and unsafe\nsentences and which is fine-tuned starting from just 100 manually-curated\npairs. We conduct extensive experiments on the resulting embedding space for\nboth retrieval and text-to-image generation, where we show that our model can\nalso be properly employed with pre-trained image generators. Our source code\nand trained models are available at: https://github.com/aimagelab/safe-clip.",
        "pdf_link": "https://arxiv.org/pdf/2311.16254v1.pdf"
    },
    {
        "title": "Visual cognition in multimodal large language models",
        "authors": [
            "Luca M. Schulze Buschoff",
            "Elif Akata",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "published": "2023-11-27T18:58:34Z",
        "summary": "A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while these\nmodels demonstrate a notable proficiency in processing and interpreting visual\ndata, they still fall short of human capabilities in these areas. The models\nexhibit a rudimentary understanding of physical laws and causal relationships,\nbut their performance is hindered by a lack of deeper insights - a key aspect\nof human cognition. Furthermore, in tasks requiring an intuitive theory of\nmind, the models fail altogether. Our results emphasize the need for\nintegrating more robust mechanisms for understanding causality, physical\ndynamics, and social cognition into modern-day, vision-based language models,\nand point out the importance of cognitively-inspired benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.16093v2.pdf"
    },
    {
        "title": "Self-correcting LLM-controlled Diffusion Models",
        "authors": [
            "Tsung-Han Wu",
            "Long Lian",
            "Joseph E. Gonzalez",
            "Boyi Li",
            "Trevor Darrell"
        ],
        "published": "2023-11-27T18:56:37Z",
        "summary": "Text-to-image generation has witnessed significant progress with the advent\nof diffusion models. Despite the ability to generate photorealistic images,\ncurrent text-to-image diffusion models still often struggle to accurately\ninterpret and follow complex input text prompts. In contrast to existing models\nthat aim to generate images only with their best effort, we introduce\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\ngenerates an image from the input prompt, assesses its alignment with the\nprompt, and performs self-corrections on the inaccuracies in the generated\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\niterative closed-loop process, ensuring correctness in the resulting image. SLD\nis not only training-free but can also be seamlessly integrated with diffusion\nmodels behind API access, such as DALL-E 3, to further boost the performance of\nstate-of-the-art diffusion models. Experimental results show that our approach\ncan rectify a majority of incorrect generations, particularly in generative\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\nbridging the gap between text-to-image generation and image editing pipelines.\nWe will make our code available for future research and applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.16090v1.pdf"
    },
    {
        "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
        "authors": [
            "Dmitri Roussinov",
            "Serge Sharoff"
        ],
        "published": "2023-11-27T18:53:31Z",
        "summary": "While performance of many text classification tasks has been recently\nimproved due to Pre-trained Language Models (PLMs), in this paper we show that\nthey still suffer from a performance gap when the underlying distribution of\ntopics changes. For example, a genre classifier trained on \\textit{political}\ntopics often fails when tested on documents about \\textit{sport} or\n\\textit{medicine}. In this work, we quantify this phenomenon empirically with a\nlarge corpus and a large set of topics. Consequently, we verify that domain\ntransfer remains challenging both for classic PLMs, such as BERT, and for\nmodern large models, such as GPT-3. We also suggest and successfully test a\npossible remedy: after augmenting the training dataset with\ntopically-controlled synthetic texts, the F1 score improves by up to 50\\% for\nsome topics, nearing on-topic training results, while others show little to no\nimprovement. While our empirical results focus on genre classification, our\nmethodology is applicable to other classification tasks such as gender,\nauthorship, or sentiment classification. The code and data to replicate the\nexperiments are available at https://github.com/dminus1/genre",
        "pdf_link": "https://arxiv.org/pdf/2311.16083v1.pdf"
    },
    {
        "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
        "authors": [
            "Zeming Chen",
            "Alejandro Hern\u00e1ndez Cano",
            "Angelika Romanou",
            "Antoine Bonnet",
            "Kyle Matoba",
            "Francesco Salvi",
            "Matteo Pagliardini",
            "Simin Fan",
            "Andreas K\u00f6pf",
            "Amirkeivan Mohtashami",
            "Alexandre Sallinen",
            "Alireza Sakhaeirad",
            "Vinitra Swamy",
            "Igor Krawczuk",
            "Deniz Bayazit",
            "Axel Marmet",
            "Syrielle Montariol",
            "Mary-Anne Hartley",
            "Martin Jaggi",
            "Antoine Bosselut"
        ],
        "published": "2023-11-27T18:49:43Z",
        "summary": "Large language models (LLMs) can potentially democratize access to medical\nknowledge. While many efforts have been made to harness and improve LLMs'\nmedical knowledge and reasoning capacities, the resulting models are either\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\nwhich restricts their abilities. In this work, we improve access to large-scale\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\nour adaptation of Nvidia's Megatron-LM distributed trainer), and extends\npretraining on a comprehensively curated medical corpus, including selected\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\nEvaluations using four major medical benchmarks show significant performance\ngains over several state-of-the-art baselines before and after task-specific\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\nbest public baseline in its parameter class and 3% over the strongest baseline\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\nthe MEDITRON model weights to drive open-source development of more capable\nmedical LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.16079v1.pdf"
    },
    {
        "title": "Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models",
        "authors": [
            "Stephen MacNeil",
            "Paul Denny",
            "Andrew Tran",
            "Juho Leinonen",
            "Seth Bernstein",
            "Arto Hellas",
            "Sami Sarsa",
            "Joanne Kim"
        ],
        "published": "2023-11-27T17:28:33Z",
        "summary": "Identifying and resolving logic errors can be one of the most frustrating\nchallenges for novices programmers. Unlike syntax errors, for which a compiler\nor interpreter can issue a message, logic errors can be subtle. In certain\nconditions, buggy code may even exhibit correct behavior -- in other cases, the\nissue might be about how a problem statement has been interpreted. Such errors\ncan be hard to spot when reading the code, and they can also at times be missed\nby automated tests. There is great educational potential in automatically\ndetecting logic errors, especially when paired with suitable feedback for\nnovices. Large language models (LLMs) have recently demonstrated surprising\nperformance for a range of computing tasks, including generating and explaining\ncode. These capabilities are closely linked to code syntax, which aligns with\nthe next token prediction behavior of LLMs. On the other hand, logic errors\nrelate to the runtime performance of code and thus may not be as well suited to\nanalysis by LLMs. To explore this, we investigate the performance of two\npopular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly\nexplanation of logic errors. We compare LLM performance with a large cohort of\nintroductory computing students $(n=964)$ solving the same error detection\ntask. Through a mixed-methods analysis of student and model responses, we\nobserve significant improvement in logic error identification between the\nprevious and current generation of LLMs, and find that both LLM generations\nsignificantly outperform students. We outline how such models could be\nintegrated into computing education tools, and discuss their potential for\nsupporting students when learning programming.",
        "pdf_link": "https://arxiv.org/pdf/2311.16017v1.pdf"
    },
    {
        "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models",
        "authors": [
            "Youssef Benchekroun",
            "Megi Dervishi",
            "Mark Ibrahim",
            "Jean-Baptiste Gaya",
            "Xavier Martinet",
            "Gr\u00e9goire Mialon",
            "Thomas Scialom",
            "Emmanuel Dupoux",
            "Dieuwke Hupkes",
            "Pascal Vincent"
        ],
        "published": "2023-11-27T15:38:17Z",
        "summary": "We propose WorldSense, a benchmark designed to assess the extent to which\nLLMs are consistently able to sustain tacit world models, by testing how they\ndraw simple inferences from descriptions of simple arrangements of entities.\nWorldsense is a synthetic benchmark with three problem types, each with their\nown trivial control, which explicitly avoids bias by decorrelating the abstract\nstructure of problems from the vocabulary and expressions, and by decorrelating\nall problem subparts with the correct response. We run our benchmark on three\nstate-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these\nmodels make errors even with as few as three objects. Furthermore, they have\nquite heavy response biases, preferring certain responses irrespective of the\nquestion. Errors persist even with chain-of-thought prompting and in-context\nlearning. Lastly, we show that while finetuning on similar problems does result\nin substantial improvements -- within- and out-of-distribution -- the finetuned\nmodels do not generalise beyond a constraint problem space.",
        "pdf_link": "https://arxiv.org/pdf/2311.15930v1.pdf"
    },
    {
        "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training",
        "authors": [
            "Jehyeon Bang",
            "Yujeong Choi",
            "Myeongwoo Kim",
            "Yongdeok Kim",
            "Minsoo Rhu"
        ],
        "published": "2023-11-27T13:35:15Z",
        "summary": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.",
        "pdf_link": "https://arxiv.org/pdf/2312.12391v1.pdf"
    },
    {
        "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
        "authors": [
            "Nianwen Si",
            "Hao Zhang",
            "Heyu Chang",
            "Wenlin Zhang",
            "Dan Qu",
            "Weiqiang Zhang"
        ],
        "published": "2023-11-27T12:37:51Z",
        "summary": "In recent years, large language models (LLMs) have spurred a new research\nparadigm in natural language processing. Despite their excellent capability in\nknowledge-based question answering and reasoning, their potential to retain\nfaulty or even harmful knowledge poses risks of malicious application. The\nchallenge of mitigating this issue and transforming these models into purer\nassistants is crucial for their widespread applicability. Unfortunately,\nRetraining LLMs repeatedly to eliminate undesirable knowledge is impractical\ndue to their immense parameters. Knowledge unlearning, derived from analogous\nstudies on machine unlearning, presents a promising avenue to address this\nconcern and is notably advantageous in the context of LLMs. It allows for the\nremoval of harmful knowledge in an efficient manner, without affecting\nunrelated knowledge in the model. To this end, we provide a survey of knowledge\nunlearning in the era of LLMs. Firstly, we formally define the knowledge\nunlearning problem and distinguish it from related works. Subsequently, we\ncategorize existing knowledge unlearning methods into three classes: those\nbased on parameter optimization, parameter merging, and in-context learning,\nand introduce details of these unlearning methods. We further present\nevaluation datasets used in existing methods, and finally conclude this survey\nby presenting the ongoing challenges and future directions.",
        "pdf_link": "https://arxiv.org/pdf/2311.15766v2.pdf"
    },
    {
        "title": "Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications",
        "authors": [
            "Sabine Wehnert"
        ],
        "published": "2023-11-27T10:59:16Z",
        "summary": "In this work, I discuss how Large Language Models can be applied in the legal\ndomain, circumventing their current drawbacks. Despite their large success and\nacceptance, their lack of explainability hinders legal experts to trust in\ntheir output, and this happens rightfully so. However, in this paper, I argue\nin favor of a new view, Justifiable Artificial Intelligence, instead of\nfocusing on Explainable Artificial Intelligence. I discuss in this paper how\ngaining evidence for and against a Large Language Model's output may make their\ngenerated texts more trustworthy - or hold them accountable for misinformation.",
        "pdf_link": "https://arxiv.org/pdf/2311.15716v1.pdf"
    },
    {
        "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks",
        "authors": [
            "Yaran Chen",
            "Wenbo Cui",
            "Yuanwen Chen",
            "Mining Tan",
            "Xinyao Zhang",
            "Dongbin Zhao",
            "He Wang"
        ],
        "published": "2023-11-27T09:20:23Z",
        "summary": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality.",
        "pdf_link": "https://arxiv.org/pdf/2311.15649v1.pdf"
    },
    {
        "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
        "authors": [
            "Yuhui Zhang",
            "Brandon McKinzie",
            "Zhe Gan",
            "Vaishaal Shankar",
            "Alexander Toshev"
        ],
        "published": "2023-11-27T07:19:26Z",
        "summary": "Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.",
        "pdf_link": "https://arxiv.org/pdf/2311.16201v1.pdf"
    },
    {
        "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
        "authors": [
            "Xupeng Miao",
            "Chunan Shi",
            "Jiangfei Duan",
            "Xiaoli Xi",
            "Dahua Lin",
            "Bin Cui",
            "Zhihao Jia"
        ],
        "published": "2023-11-27T06:31:17Z",
        "summary": "The high computational and memory requirements of generative large language\nmodels (LLMs) make it challenging to serve them cheaply. This paper aims to\nreduce the monetary cost for serving LLMs by leveraging preemptible GPU\ninstances on modern clouds, which offer accesses to spare GPUs at a much\ncheaper price than regular instances but may be preempted by the cloud at any\ntime. Serving LLMs on preemptible instances requires addressing challenges\ninduced by frequent instance preemptions and the necessity of migrating\ninstances to handle these preemptions.\n  This paper presents SpotServe, the first distributed LLM serving system on\npreemptible instances. Several key techniques in SpotServe realize fast and\nreliable serving of generative LLMs on cheap preemptible instances. First,\nSpotServe dynamically adapts the LLM parallelization configuration for dynamic\ninstance availability and fluctuating workload, while balancing the trade-off\namong the overall throughput, inference latency and monetary costs. Second, to\nminimize the cost of migrating instances for dynamic reparallelization, the\ntask of migrating instances is formulated as a bipartite graph matching\nproblem, which uses the Kuhn-Munkres algorithm to identify an optimal migration\nplan that minimizes communications. Finally, to take advantage of the grace\nperiod offered by modern clouds, we introduce stateful inference recovery, a\nnew inference mechanism that commits inference progress at a much finer\ngranularity and allows SpotServe to cheaply resume inference upon preemption.\nWe evaluate on real spot instance preemption traces and various popular LLMs\nand show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1x compared\nwith the best existing LLM serving systems. We also show that SpotServe can\nleverage the price advantage of preemptive instances, saving 54% monetary cost\ncompared with only using on-demand instances.",
        "pdf_link": "https://arxiv.org/pdf/2311.15566v1.pdf"
    },
    {
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
        "authors": [
            "Haoqiang Kang",
            "Xiao-Yang Liu"
        ],
        "published": "2023-11-27T05:27:13Z",
        "summary": "The hallucination issue is recognized as a fundamental deficiency of large\nlanguage models (LLMs), especially when applied to fields such as finance,\neducation, and law. Despite the growing concerns, there has been a lack of\nempirical investigation. In this paper, we provide an empirical examination of\nLLMs' hallucination behaviors in financial tasks. First, we empirically\ninvestigate LLM model's ability of explaining financial concepts and\nterminologies. Second, we assess LLM models' capacity of querying historical\nstock prices. Third, to alleviate the hallucination issue, we evaluate the\nefficacy of four practical methods, including few-shot learning, Decoding by\nContrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method\nand the prompt-based tool learning method for a function to generate a query\ncommand. Finally, our major finding is that off-the-shelf LLMs experience\nserious hallucination behaviors in financial tasks. Therefore, there is an\nurgent need to call for research efforts in mitigating LLMs' hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2311.15548v1.pdf"
    },
    {
        "title": "Function-constrained Program Synthesis",
        "authors": [
            "Patrick Hajali",
            "Ignas Budvytis"
        ],
        "published": "2023-11-27T02:55:34Z",
        "summary": "This work introduces (1) a technique that allows large language models (LLMs)\nto leverage user-provided code when solving programming tasks and (2) a method\nto iteratively generate modular sub-functions that can aid future code\ngeneration attempts when the initial code generated by the LLM is inadequate.\nGenerating computer programs in general-purpose programming languages like\nPython poses a challenge for LLMs when instructed to use code provided in the\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\ncompletions in real-time by drawing on all code available in a development\nenvironment. However, restricting code-specific LLMs to use only in-context\ncode is not straightforward, as the model is not explicitly instructed to use\nthe user-provided code and users cannot highlight precisely which snippets of\ncode the model should incorporate into its context. Moreover, current systems\nlack effective recovery methods, forcing users to iteratively re-prompt the\nmodel with modified prompts until a sufficient solution is reached. Our method\ndiffers from traditional LLM-powered code-generation by constraining\ncode-generation to an explicit function set and enabling recovery from failed\nattempts through automatically generated sub-functions. When the LLM cannot\nproduce working code, we generate modular sub-functions to aid subsequent\nattempts at generating functional code. A by-product of our method is a library\nof reusable sub-functions that can solve related tasks, imitating a software\nteam where efficiency scales with experience. We also introduce a new\n\"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\nmethod encourages models to output solutions in a structured format, decreasing\nsyntax errors that can be mistaken for poor coding ability.",
        "pdf_link": "https://arxiv.org/pdf/2311.15500v2.pdf"
    },
    {
        "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
        "authors": [
            "Junyuan Hong",
            "Jiachen T. Wang",
            "Chenhui Zhang",
            "Zhangheng Li",
            "Bo Li",
            "Zhangyang Wang"
        ],
        "published": "2023-11-27T02:01:10Z",
        "summary": "Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .",
        "pdf_link": "https://arxiv.org/pdf/2312.03724v2.pdf"
    },
    {
        "title": "Machine-Generated Text Detection using Deep Learning",
        "authors": [
            "Raghav Gaggar",
            "Ashish Bhagchandani",
            "Harsh Oza"
        ],
        "published": "2023-11-26T21:16:01Z",
        "summary": "Our research focuses on the crucial challenge of discerning text produced by\nLarge Language Models (LLMs) from human-generated text, which holds\nsignificance for various applications. With ongoing discussions about attaining\na model with such functionality, we present supporting evidence regarding the\nfeasibility of such models. We evaluated our models on multiple datasets,\nincluding Twitter Sentiment, Football Commentary, Project Gutenberg, PubMedQA,\nand SQuAD, confirming the efficacy of the enhanced detection approaches. These\ndatasets were sampled with intricate constraints encompassing every\npossibility, laying the foundation for future research. We evaluate\nGPT-3.5-Turbo against various detectors such as SVM, RoBERTa-base, and\nRoBERTa-large. Based on the research findings, the results predominantly relied\non the sequence length of the sentence.",
        "pdf_link": "https://arxiv.org/pdf/2311.15425v1.pdf"
    },
    {
        "title": "KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All",
        "authors": [
            "Quyen Tran",
            "Lam Tran",
            "Khoat Than",
            "Toan Tran",
            "Dinh Phung",
            "Trung Le"
        ],
        "published": "2023-11-26T20:35:19Z",
        "summary": "Drawing inspiration from prompt tuning techniques applied to Large Language\nModels, recent methods based on pre-trained ViT networks have achieved\nremarkable results in the field of Continual Learning. Specifically, these\napproaches propose to maintain a set of prompts and allocate a subset of them\nto learn each task using a key-query matching strategy. However, they may\nencounter limitations when lacking control over the correlations between old\ntask queries and keys of future tasks, the shift of features in the latent\nspace, and the relative separation of latent vectors learned in independent\ntasks. In this work, we introduce a novel key-query learning strategy based on\northogonal projection, inspired by model-agnostic meta-learning, to enhance\nprompt matching efficiency and address the challenge of shifting features.\nFurthermore, we introduce a One-Versus-All (OVA) prototype-based component that\nenhances the classification head distinction. Experimental results on benchmark\ndatasets demonstrate that our method empowers the model to achieve results\nsurpassing those of current state-of-the-art approaches by a large margin of up\nto 20%.",
        "pdf_link": "https://arxiv.org/pdf/2311.15414v2.pdf"
    },
    {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
        "authors": [
            "Xun Liang",
            "Shichao Song",
            "Simin Niu",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Bo Tang",
            "Zhaohui Wy",
            "Dawei He",
            "Peng Cheng",
            "Zhonghao Wang",
            "Haiying Deng"
        ],
        "published": "2023-11-26T13:42:56Z",
        "summary": "Large language models (LLMs) have emerged as pivotal contributors in\ncontemporary natural language processing and are increasingly being applied\nacross a diverse range of industries. However, these large-scale probabilistic\nstatistical models cannot currently ensure the requisite quality in\nprofessional content generation. These models often produce hallucinated text,\ncompromising their practical utility in professional contexts. To assess the\nauthentic reliability of LLMs in text generation, numerous initiatives have\ndeveloped benchmark evaluations for hallucination phenomena. Nevertheless,\nthese benchmarks frequently utilize constrained generation techniques due to\ncost and temporal constraints. These techniques encompass the use of directed\nhallucination induction and strategies that deliberately alter authentic text\nto produce hallucinations. These approaches are not congruent with the\nunrestricted text generation demanded by real-world applications. Furthermore,\na well-established Chinese-language dataset dedicated to the evaluation of\nhallucinations in text generation is presently lacking. Consequently, we have\ndeveloped an Unconstrained Hallucination Generation Evaluation (UHGEval)\nbenchmark, designed to compile outputs produced with minimal restrictions by\nLLMs. Concurrently, we have established a comprehensive benchmark evaluation\nframework to aid subsequent researchers in undertaking scalable and\nreproducible experiments. We have also executed extensive experiments,\nevaluating prominent Chinese language models and the GPT series models to\nderive professional performance insights regarding hallucination challenges.",
        "pdf_link": "https://arxiv.org/pdf/2311.15296v2.pdf"
    },
    {
        "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
        "authors": [
            "Johannes Schneider",
            "Steffi Haag",
            "Leona Chandra Kruse"
        ],
        "published": "2023-11-26T08:44:58Z",
        "summary": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.03720v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Model Volatility",
        "authors": [
            "Boyang Yu"
        ],
        "published": "2023-11-26T03:54:03Z",
        "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.",
        "pdf_link": "https://arxiv.org/pdf/2311.15180v1.pdf"
    },
    {
        "title": "Large Language Models in Law: A Survey",
        "authors": [
            "Jinqi Lai",
            "Wensheng Gan",
            "Jiayang Wu",
            "Zhenlian Qi",
            "Philip S. Yu"
        ],
        "published": "2023-11-26T00:48:12Z",
        "summary": "The advent of artificial intelligence (AI) has significantly impacted the\ntraditional judicial industry. Moreover, recently, with the development of\nAI-generated content (AIGC), AI and law have found applications in various\ndomains, including image recognition, automatic text generation, and\ninteractive chat. With the rapid emergence and growing popularity of large\nmodels, it is evident that AI will drive transformation in the traditional\njudicial industry. However, the application of legal large language models\n(LLMs) is still in its nascent stage. Several challenges need to be addressed.\nIn this paper, we aim to provide a comprehensive survey of legal LLMs. We not\nonly conduct an extensive survey of LLMs, but also expose their applications in\nthe judicial system. We first provide an overview of AI technologies in the\nlegal field and showcase the recent research in LLMs. Then, we discuss the\npractical implementation presented by legal LLMs, such as providing legal\nadvice to users and assisting judges during trials. In addition, we explore the\nlimitations of legal LLMs, including data, algorithms, and judicial practice.\nFinally, we summarize practical recommendations and propose future development\ndirections to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2312.03718v1.pdf"
    },
    {
        "title": "Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains",
        "authors": [
            "Chia-Chien Hung",
            "Wiem Ben Rim",
            "Lindsay Frost",
            "Lars Bruckner",
            "Carolin Lawrence"
        ],
        "published": "2023-11-25T08:58:07Z",
        "summary": "High-risk domains pose unique challenges that require language models to\nprovide accurate and safe responses. Despite the great success of large\nlanguage models (LLMs), such as ChatGPT and its variants, their performance in\nhigh-risk domains remains unclear. Our study delves into an in-depth analysis\nof the performance of instruction-tuned LLMs, focusing on factual accuracy and\nsafety adherence. To comprehensively assess the capabilities of LLMs, we\nconduct experiments on six NLP datasets including question answering and\nsummarization tasks within two high-risk domains: legal and medical. Further\nqualitative analysis highlights the existing limitations inherent in current\nLLMs when evaluating in high-risk domains. This underscores the essential\nnature of not only improving LLM capabilities but also prioritizing the\nrefinement of domain-specific metrics, and embracing a more human-centric\napproach to enhance safety and factual reliability. Our findings advance the\nfield toward the concerns of properly evaluating LLMs in high-risk domains,\naiming to steer the adaptability of LLMs in fulfilling societal obligations and\naligning with forthcoming regulations, such as the EU AI Act.",
        "pdf_link": "https://arxiv.org/pdf/2311.14966v1.pdf"
    },
    {
        "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
        "authors": [
            "Xiuyuan Chen",
            "Yuan Lin",
            "Yuchen Zhang",
            "Weiran Huang"
        ],
        "published": "2023-11-25T02:46:12Z",
        "summary": "We propose a novel and challenging benchmark, AutoEval-Video, to\ncomprehensively evaluate large vision-language models in open-ended video\nquestion answering. The comprehensiveness of AutoEval-Video is demonstrated in\ntwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9\nskill dimensions, addressing capabilities of perception, comprehension, and\ngeneration. 2) AutoEval-Video contains newly collected videos that cover over\n40 distinct themes. To efficiently evaluate responses to the open-ended\nquestions, we employ an LLM-based evaluation approach, but instead of merely\nproviding a reference answer, we annotate unique evaluation rules for every\nsingle instance (video-question pair). To maximize the robustness of these\nrules, we develop a novel adversarial annotation mechanism. By using\ninstance-specific rules as prompt, GPT-4, as an automatic evaluator, can\nachieve a stable evaluation accuracy of around 97.0\\%, comparable to the 94.9\\%\n- 97.5\\% accuracy of a human evaluator. Furthermore, we assess the performance\nof eight large vision-language models on AutoEval-Video. Among them,\nGPT-4V(ision) significantly outperforms other models, achieving an accuracy of\n32.2\\%. However, there is still substantial room for improvement compared to\nhuman accuracy of 72.8\\%. By conducting an extensive case study, we uncover\nseveral drawbacks of GPT-4V, such as limited temporal and dynamic\ncomprehension, and overly general responses. Code is available at\n\\href{https://github.com/Xiuyuan-Chen/AutoEval-Video}{\\color{magenta}https://github.com/Xiuyuan-Chen/AutoEval-Video}.",
        "pdf_link": "https://arxiv.org/pdf/2311.14906v1.pdf"
    },
    {
        "title": "Gender inference: can chatGPT outperform common commercial tools?",
        "authors": [
            "Michelle Alexopoulos",
            "Kelly Lyons",
            "Kaushar Mahetaji",
            "Marcus Emmanuel Barnes",
            "Rogan Gutwillinger"
        ],
        "published": "2023-11-24T22:09:14Z",
        "summary": "An increasing number of studies use gender information to understand\nphenomena such as gender bias, inequity in access and participation, or the\nimpact of the Covid pandemic response. Unfortunately, most datasets do not\ninclude self-reported gender information, making it necessary for researchers\nto infer gender from other information, such as names or names and country\ninformation. An important limitation of these tools is that they fail to\nappropriately capture the fact that gender exists on a non-binary scale,\nhowever, it remains important to evaluate and compare how well these tools\nperform in a variety of contexts. In this paper, we compare the performance of\na generative Artificial Intelligence (AI) tool ChatGPT with three commercially\navailable list-based and machine learning-based gender inference tools (Namsor,\nGender-API, and genderize.io) on a unique dataset. Specifically, we use a large\nOlympic athlete dataset and report how variations in the input (e.g., first\nname and first and last name, with and without country information) impact the\naccuracy of their predictions. We report results for the full set, as well as\nfor the subsets: medal versus non-medal winners, athletes from the largest\nEnglish-speaking countries, and athletes from East Asia. On these sets, we find\nthat Namsor is the best traditional commercially available tool. However,\nChatGPT performs at least as well as Namsor and often outperforms it,\nespecially for the female sample when country and/or last name information is\navailable. All tools perform better on medalists versus non-medalists and on\nnames from English-speaking countries. Although not designed for this purpose,\nChatGPT may be a cost-effective tool for gender prediction. In the future, it\nmight even be possible for ChatGPT or other large scale language models to\nbetter identify self-reported gender rather than report gender on a binary\nscale.",
        "pdf_link": "https://arxiv.org/pdf/2312.00805v1.pdf"
    },
    {
        "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
        "authors": [
            "Raghav Addanki",
            "Chenyang Li",
            "Zhao Song",
            "Chiwun Yang"
        ],
        "published": "2023-11-24T18:35:00Z",
        "summary": "Attention computation takes both the time complexity of $O(n^2)$ and the\nspace complexity of $O(n^2)$ simultaneously, which makes deploying Large\nLanguage Models (LLMs) in streaming applications that involve long contexts\nrequiring substantial computational resources. In recent OpenAI DevDay (Nov 6,\n2023), OpenAI released a new model that is able to support a 128K-long\ndocument, in our paper, we focus on the memory-efficient issue when context\nlength $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer\nself-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n\n\\times d}$, the polynomial method approximates the attention output $T \\in\n\\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in\n\\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$\ncomputation within $n^{1+o(1)}$ time executions. Despite this, computing the\napproximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still\nnecessitates $O(n^2)$ space, leading to significant memory usage. In response\nto these challenges, we introduce a new algorithm that only reads one pass of\nthe data in a streaming fashion. This method employs sublinear space $o(n)$ to\nstore three sketch matrices, alleviating the need for exact $K, V$ storage.\nNotably, our algorithm exhibits exceptional memory-efficient performance with\nsuper-long tokens. As the token length $n$ increases, our error guarantee\ndiminishes while the memory usage remains nearly constant. This unique\nattribute underscores the potential of our technique in efficiently handling\nLLMs in streaming applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.14652v2.pdf"
    },
    {
        "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
        "authors": [
            "Yuanfeng Ji",
            "Chongjian Ge",
            "Weikai Kong",
            "Enze Xie",
            "Zhengying Liu",
            "Zhengguo Li",
            "Ping Luo"
        ],
        "published": "2023-11-24T16:12:05Z",
        "summary": "With the advancements in Large Language Models (LLMs), Vision-Language Models\n(VLMs) have reached a new level of sophistication, showing notable competence\nin executing intricate cognition and reasoning tasks. However, existing\nevaluation benchmarks, primarily relying on rigid, hand-crafted datasets to\nmeasure task-specific performance, face significant limitations in assessing\nthe alignment of these increasingly anthropomorphic models with human\nintelligence. In this work, we address the limitations via Auto-Bench, which\ndelves into exploring LLMs as proficient aligners, measuring the alignment\nbetween VLMs and human intelligence and value through automatic data curation\nand assessment. Specifically, for data curation, Auto-Bench utilizes LLMs\n(e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning\ntriplets via prompting on visual symbolic representations (e.g., captions,\nobject locations, instance relationships, and etc.). The curated data closely\nmatches human intent, owing to the extensive world knowledge embedded in LLMs.\nThrough this pipeline, a total of 28.5K human-verified and 3,504K unfiltered\nquestion-answer-reasoning triplets have been curated, covering 4 primary\nabilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to\nserve as judges, implementing the quantitative and qualitative automated\nassessments to facilitate a comprehensive evaluation of VLMs. Our validation\nresults reveal that LLMs are proficient in both evaluation data curation and\nmodel assessment, achieving an average agreement rate of 85%. We envision\nAuto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating\nthe evolving sophisticated VLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14580v1.pdf"
    },
    {
        "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
        "authors": [
            "Di Jin",
            "Shikib Mehri",
            "Devamanyu Hazarika",
            "Aishwarya Padmakumar",
            "Sungjin Lee",
            "Yang Liu",
            "Mahdi Namazifar"
        ],
        "published": "2023-11-24T15:20:36Z",
        "summary": "Learning from human feedback is a prominent technique to align the output of\nlarge language models (LLMs) with human expectations. Reinforcement learning\nfrom human feedback (RLHF) leverages human preference signals that are in the\nform of ranking of response pairs to perform this alignment. However, human\npreference on LLM outputs can come in much richer forms including natural\nlanguage, which may provide detailed feedback on strengths and weaknesses of a\ngiven response. In this work we investigate data efficiency of modeling human\nfeedback that is in natural language. Specifically, we fine-tune an open-source\nLLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or\neven less) of human feedback in natural language in the form of critiques and\nrevisions of responses. We show that this model is able to improve the quality\nof responses from even some of the strongest LLMs such as ChatGPT, BARD, and\nVicuna, through critique and revision of those responses. For instance, through\none iteration of revision of ChatGPT responses, the revised responses have\n56.6% win rate over the original ones, and this win rate can be further\nimproved to 65.9% after applying the revision for five iterations.",
        "pdf_link": "https://arxiv.org/pdf/2311.14543v1.pdf"
    },
    {
        "title": "Machine Translation for Ge'ez Language",
        "authors": [
            "Aman Kassahun Wassie"
        ],
        "published": "2023-11-24T14:55:23Z",
        "summary": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient\nlanguage that is no longer the native language of any community, faces\nchallenges such as out-of-vocabulary words, domain mismatches, and lack of\nsufficient labeled training data. In this work, we explore various methods to\nimprove Ge'ez MT, including transfer-learning from related languages,\noptimizing shared vocabulary and token segmentation approaches, finetuning\nlarge pre-trained models, and using large language models (LLMs) for few-shot\ntranslation with fuzzy matches. We develop a multilingual neural machine\ntranslation (MNMT) model based on languages relatedness, which brings an\naverage performance improvement of about 4 BLEU compared to standard bilingual\nmodels. We also attempt to finetune the NLLB-200 model, one of the most\nadvanced translation models available today, but find that it performs poorly\nwith only 4k training samples for Ge'ez. Furthermore, we experiment with using\nGPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches,\nwhich leverages embedding similarity-based retrieval to find context examples\nfrom a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU\nscore of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT\nbaseline of 15.2. Our work provides insights into the potential and limitations\nof different approaches for low-resource and ancient language MT.",
        "pdf_link": "https://arxiv.org/pdf/2311.14530v2.pdf"
    },
    {
        "title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review",
        "authors": [
            "Ming Li",
            "Ariunaa Enkhtur",
            "Beverley Anne Yamamoto",
            "Fei Cheng"
        ],
        "published": "2023-11-24T10:00:23Z",
        "summary": "ChatGPT and other Generative Artificial Intelligence (GAI) models tend to\ninherit and even amplify prevailing societal biases as they are trained on\nlarge amounts of existing data. Given the increasing usage of ChatGPT and other\nGAI by students, faculty members, and staff in higher education institutions\n(HEIs), there is an urgent need to examine the ethical issues involved such as\nits potential biases. In this scoping review, we clarify the ways in which\nbiases related to GAI in higher education settings have been discussed in\nrecent academic publications and identify what type of potential biases are\ncommonly reported in this body of literature. We searched for academic articles\nwritten in English, Chinese, and Japanese across four main databases concerned\nwith GAI usage in higher education and bias. Our findings show that while there\nis an awareness of potential biases around large language models (LLMs) and\nGAI, the majority of articles touch on ``bias'' at a relatively superficial\nlevel. Few identify what types of bias may occur under what circumstances.\nNeither do they discuss the possible implications for the higher education,\nstaff, faculty members, or students. There is a notable lack of empirical work\nat this point, and we call for higher education researchers and AI experts to\nconduct more research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2311.14381v1.pdf"
    },
    {
        "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
        "authors": [
            "Wu Zekun",
            "Sahan Bulathwela",
            "Adriano Soares Koshiyama"
        ],
        "published": "2023-11-23T17:47:14Z",
        "summary": "Large Language Models (LLM) have made significant advances in the recent past\nbecoming more mainstream in Artificial Intelligence (AI) enabled human-facing\napplications. However, LLMs often generate stereotypical output inherited from\nhistorical data, amplifying societal biases and raising ethical concerns. This\nwork introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751\ninstances of gender, race, profession and religion stereotypic text and ii) a\nnovel stereotype classifier for English text. We design several experiments to\nrigorously test the proposed model trained on the novel dataset. Our\nexperiments show that training the model in a multi-class setting can\noutperform the one-vs-all binary counterpart. Consistent feature importance\nsignals from different eXplainable AI tools demonstrate that the new model\nexploits relevant text features. We utilise the newly created model to assess\nthe stereotypic behaviour of the popular GPT family of models and observe the\nreduction of bias over time. In summary, our work establishes a robust and\npractical framework for auditing and evaluating the stereotypic bias in LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.14126v1.pdf"
    },
    {
        "title": "Auditing and Mitigating Cultural Bias in LLMs",
        "authors": [
            "Yan Tao",
            "Olga Viberg",
            "Ryan S. Baker",
            "Rene F. Kizilcec"
        ],
        "published": "2023-11-23T16:45:56Z",
        "summary": "Culture fundamentally shapes people's reasoning, behavior, and communication.\nGenerative artificial intelligence (AI) technologies may cause a shift towards\na dominant culture. As people increasingly use AI to expedite and even automate\nvarious professional and personal tasks, cultural values embedded in AI models\nmay bias authentic expression. We audit large language models for cultural\nbias, comparing their responses to nationally representative survey data, and\nevaluate country-specific prompting as a mitigation strategy. We find that\nGPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and\nProtestant European countries. Our mitigation strategy reduces cultural bias in\nrecent models but not for all countries/territories. To avoid cultural bias in\ngenerative AI, especially in high-stakes contexts, we suggest using culture\nmatching and ongoing cultural audits.",
        "pdf_link": "https://arxiv.org/pdf/2311.14096v1.pdf"
    },
    {
        "title": "PrivateLoRA For Efficient Privacy Preserving LLM",
        "authors": [
            "Yiming Wang",
            "Yu Lin",
            "Xiaodong Zeng",
            "Guannan Zhang"
        ],
        "published": "2023-11-23T14:36:30Z",
        "summary": "End users face a choice between privacy and efficiency in current Large\nLanguage Model (LLM) service paradigms. In cloud-based paradigms, users are\nforced to compromise data locality for generation quality and processing speed.\nConversely, edge device paradigms maintain data locality but fail to deliver\nsatisfactory performance. In this work, we propose a novel LLM service paradigm\nthat distributes privacy-sensitive computation on edge devices and shared\ncomputation in the cloud. Only activations are transmitted between the central\ncloud and edge devices to ensure data locality. Our core innovation,\nPrivateLoRA, addresses the challenging communication overhead by exploiting the\nlow rank of residual activations, achieving over 95% communication reduction.\nConsequently, PrivateLoRA effectively maintains data locality and is extremely\nresource efficient. Under standard 5G networks, PrivateLoRA achieves throughput\nover 300% of device-only solutions for 7B models and over 80% of an A100 GPU\nfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRA\nfor advanced personalization. Our approach democratizes access to\nstate-of-the-art generative AI for edge devices, paving the way for more\ntailored LLM experiences for the general public. To our knowledge, our proposed\nframework is the first efficient and privacy-preserving LLM solution in the\nliterature.",
        "pdf_link": "https://arxiv.org/pdf/2311.14030v1.pdf"
    },
    {
        "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
        "authors": [
            "Shulin Cao",
            "Jiajie Zhang",
            "Jiaxin Shi",
            "Xin Lv",
            "Zijun Yao",
            "Qi Tian",
            "Juanzi Li",
            "Lei Hou"
        ],
        "published": "2023-11-23T12:52:37Z",
        "summary": "Large language models (LLMs) are capable of answering knowledge-intensive\ncomplex questions with chain-of-thought (CoT) reasoning. However, they tend to\ngenerate factually incorrect reasoning steps when the required knowledge is not\navailable or up-to-date in models' parameters. Recent works turn to retrieving\nexternal knowledge to augment CoT reasoning. Despite being promising, these\nchain-based methods suffer from: 1) Negative retrieval. Unnecessary or\nincorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the\nability to look backward or forward, a local error in one step will propagate\nalong the chain.\n  In this paper, we propose a novel approach: Probabilistic Tree-of-thought\nReasoning (ProbTree). First, LLMs translate a complex question into a query\ntree, in which each non-root node denotes a sub-question of its parent node.\nThen, probabilistic reasoning is conducted over the tree, by solving questions\nfrom leaf to root considering the confidence of both question decomposing and\nanswering. During reasoning, for leaf nodes, LLMs choose a more confident\nanswer from Closed-book QA that employs parametric knowledge and Open-book QA\nthat employs retrieved external knowledge, thus eliminating the negative\nretrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs\nhave broader sights and are able to globally reason with the information from\nchild nodes, thus recovering from local errors. The experiments on three\nComplex QA datasets under the open-domain setting show that our approach\noutperforms SOTA methods significantly, demonstrating the effect of\nprobabilistic tree-of-thought reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.13982v1.pdf"
    },
    {
        "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations",
        "authors": [
            "John Mendon\u00e7a",
            "Patr\u00edcia Pereira",
            "Miguel Menezes",
            "Vera Cabarr\u00e3o",
            "Ana C. Farinha",
            "Helena Moniz",
            "Jo\u00e3o Paulo Carvalho",
            "Alon Lavie",
            "Isabel Trancoso"
        ],
        "published": "2023-11-23T10:56:14Z",
        "summary": "Task-oriented conversational datasets often lack topic variability and\nlinguistic diversity. However, with the advent of Large Language Models (LLMs)\npretrained on extensive, multilingual and diverse text data, these limitations\nseem overcome. Nevertheless, their generalisability to different languages and\ndomains in dialogue applications remains uncertain without benchmarking\ndatasets. This paper presents a holistic annotation approach for emotion and\nconversational quality in the context of bilingual customer support\nconversations. By performing annotations that take into consideration the\ncomplete instances that compose a conversation, one can form a broader\nperspective of the dialogue as a whole. Furthermore, it provides a unique and\nvaluable resource for the development of text classification models. To this\nend, we present benchmarks for Emotion Recognition and Dialogue Quality\nEstimation and show that further research is needed to leverage these models in\na production setting.",
        "pdf_link": "https://arxiv.org/pdf/2311.13910v1.pdf"
    },
    {
        "title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach",
        "authors": [
            "Bin Zhang",
            "Hangyu Mao",
            "Jingqing Ruan",
            "Ying Wen",
            "Yang Li",
            "Shao Zhang",
            "Zhiwei Xu",
            "Dapeng Li",
            "Ziyue Li",
            "Rui Zhao",
            "Lijuan Li",
            "Guoliang Fan"
        ],
        "published": "2023-11-23T10:14:58Z",
        "summary": "The remarkable progress in Large Language Models (LLMs) opens up new avenues\nfor addressing planning and decision-making problems in Multi-Agent Systems\n(MAS). However, as the number of agents increases, the issues of hallucination\nin LLMs and coordination in MAS have become increasingly prominent.\nAdditionally, the efficient utilization of tokens emerges as a critical\nconsideration when employing LLMs to facilitate the interactions among a\nsubstantial number of agents. In this paper, we develop a modular framework\ncalled LLaMAC to mitigate these challenges. LLaMAC implements a value\ndistribution encoding similar to that found in the human brain, utilizing\ninternal and external feedback mechanisms to facilitate collaboration and\niterative reasoning among its modules. Through evaluations involving system\nresource allocation and robot grid transportation, we demonstrate the\nconsiderable advantages afforded by our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2311.13884v3.pdf"
    },
    {
        "title": "Compositional Zero-shot Learning via Progressive Language-based Observations",
        "authors": [
            "Lin Li",
            "Guikun Chen",
            "Jun Xiao",
            "Long Chen"
        ],
        "published": "2023-11-23T10:14:23Z",
        "summary": "Compositional zero-shot learning aims to recognize unseen state-object\ncompositions by leveraging known primitives (state and object) during training.\nHowever, effectively modeling interactions between primitives and generalizing\nknowledge to novel compositions remains a perennial challenge. There are two\nkey factors: object-conditioned and state-conditioned variance, i.e., the\nappearance of states (or objects) can vary significantly when combined with\ndifferent objects (or states). For instance, the state \"old\" can signify a\nvintage design for a \"car\" or an advanced age for a \"cat\". In this paper, we\nargue that these variances can be mitigated by predicting composition\ncategories based on pre-observed primitive. To this end, we propose Progressive\nLanguage-based Observations (PLO), which can dynamically determine a better\nobservation order of primitives. These observations comprise a series of\nconcepts or languages that allow the model to understand image content in a\nstep-by-step manner. Specifically, PLO adopts pre-trained vision-language\nmodels (VLMs) to empower the model with observation capabilities. We further\ndevise two variants: 1) PLO-VLM: a two-step method, where a pre-observing\nclassifier dynamically determines the observation order of two primitives. 2)\nPLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to\ncraft composition-specific prompts for step-by-step observing. Extensive\nablations on three challenging datasets demonstrate the superiority of PLO\ncompared with state-of-the-art methods, affirming its abilities in\ncompositional recognition.",
        "pdf_link": "https://arxiv.org/pdf/2311.14749v1.pdf"
    },
    {
        "title": "Challenges of Large Language Models for Mental Health Counseling",
        "authors": [
            "Neo Christopher Chung",
            "George Dyer",
            "Lennart Brocki"
        ],
        "published": "2023-11-23T08:56:41Z",
        "summary": "The global mental health crisis is looming with a rapid increase in mental\ndisorders, limited resources, and the social stigma of seeking treatment. As\nthe field of artificial intelligence (AI) has witnessed significant\nadvancements in recent years, large language models (LLMs) capable of\nunderstanding and generating human-like text may be used in supporting or\nproviding psychological counseling. However, the application of LLMs in the\nmental health domain raises concerns regarding the accuracy, effectiveness, and\nreliability of the information provided. This paper investigates the major\nchallenges associated with the development of LLMs for psychological\ncounseling, including model hallucination, interpretability, bias, privacy, and\nclinical effectiveness. We explore potential solutions to these challenges that\nare practical and applicable to the current paradigm of AI. From our experience\nin developing and deploying LLMs for mental health, AI holds a great promise\nfor improving mental health care, if we can carefully navigate and overcome\npitfalls of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13857v1.pdf"
    },
    {
        "title": "Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
        "authors": [
            "Saman Motamed",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "published": "2023-11-23T07:33:38Z",
        "summary": "Diffusion models have revolutionized generative content creation and\ntext-to-image (T2I) diffusion models in particular have increased the creative\nfreedom of users by allowing scene synthesis using natural language. T2I models\nexcel at synthesizing concepts such as nouns, appearances, and styles. To\nenable customized content creation based on a few example images of a concept,\nmethods such as Textual Inversion and DreamBooth invert the desired concept and\nenable synthesizing it in new scenes. However, inverting more general concepts\nthat go beyond object appearance and style (adjectives and verbs) through\nnatural language, remains a challenge. Two key characteristics of these\nconcepts contribute to the limitations of current inversion methods. 1)\nAdjectives and verbs are entangled with nouns (subject) and can hinder\nappearance-based inversion methods, where the subject appearance leaks into the\nconcept embedding and 2) describing such concepts often extends beyond single\nword embeddings (being frozen in ice, walking on a tightrope, etc.) that\ncurrent methods do not handle.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline.\nAdditionally, visual question answering using a large language model suggested\nLego-generated concepts are better aligned with the text description of the\nconcept.",
        "pdf_link": "https://arxiv.org/pdf/2311.13833v1.pdf"
    },
    {
        "title": "Surpassing GPT-4 Medical Coding with a Two-Stage Approach",
        "authors": [
            "Zhichao Yang",
            "Sanjit Singh Batra",
            "Joel Stremmel",
            "Eran Halperin"
        ],
        "published": "2023-11-22T23:35:13Z",
        "summary": "Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.13735v1.pdf"
    },
    {
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
        "authors": [
            "Inderjeet Nair",
            "Shwetha Somasundaram",
            "Apoorv Saxena",
            "Koustava Goswami"
        ],
        "published": "2023-11-22T18:22:56Z",
        "summary": "We address the task of evidence retrieval for long document question\nanswering, which involves locating relevant paragraphs within a document to\nanswer a question. We aim to assess the applicability of large language models\n(LLMs) in the task of zero-shot long document evidence retrieval, owing to\ntheir unprecedented performance across various NLP tasks. However, currently\nthe LLMs can consume limited context lengths as input, thus providing document\nchunks as inputs might overlook the global context while missing out on\ncapturing the inter-segment dependencies. Moreover, directly feeding the large\ninput sets can incur significant computational costs, particularly when\nprocessing the entire document (and potentially incurring monetary expenses\nwith enterprise APIs like OpenAI's GPT variants). To address these challenges,\nwe propose a suite of techniques that exploit the discourse structure commonly\nfound in documents. By utilizing this structure, we create a condensed\nrepresentation of the document, enabling a more comprehensive understanding and\nanalysis of relationships between different parts. We retain $99.6\\%$ of the\nbest zero-shot approach's performance, while processing only $26\\%$ of the\ntotal tokens used by the best approach in the information seeking evidence\nretrieval setup. We also show how our approach can be combined with\n\\textit{self-ask} reasoning agent to achieve best zero-shot performance in\ncomplex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot\nperformance using gold evidence.",
        "pdf_link": "https://arxiv.org/pdf/2311.13565v1.pdf"
    },
    {
        "title": "Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object",
        "authors": [
            "Junhao Chen",
            "Peng Rong",
            "Jingbo Sun",
            "Chao Li",
            "Xiang Li",
            "Hongwu Lv"
        ],
        "published": "2023-11-22T18:15:43Z",
        "summary": "Image style transfer occupies an important place in both computer graphics\nand computer vision. However, most current methods require reference to\nstylized images and cannot individually stylize specific objects. To overcome\nthis limitation, we propose the \"Soulstyler\" framework, which allows users to\nguide the stylization of specific objects in an image through simple textual\ndescriptions. We introduce a large language model to parse the text and\nidentify stylization goals and specific styles. Combined with a CLIP-based\nsemantic visual embedding encoder, the model understands and matches text and\nimage content. We also introduce a novel localized text-image block matching\nloss that ensures that style transfer is performed only on specified target\nobjects, while non-target regions remain in their original style. Experimental\nresults demonstrate that our model is able to accurately perform style transfer\non target objects according to textual descriptions without affecting the style\nof background regions. Our code will be available at\nhttps://github.com/yisuanwang/Soulstyler.",
        "pdf_link": "https://arxiv.org/pdf/2311.13562v2.pdf"
    },
    {
        "title": "Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
        "authors": [
            "Chi Zhang",
            "Zifan Wang",
            "Ravi Mangal",
            "Matt Fredrikson",
            "Limin Jia",
            "Corina Pasareanu"
        ],
        "published": "2023-11-22T15:11:35Z",
        "summary": "Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.13445v1.pdf"
    },
    {
        "title": "Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents",
        "authors": [
            "Zihao Zhou",
            "Bin Hu",
            "Chenyang Zhao",
            "Pu Zhang",
            "Bin Liu"
        ],
        "published": "2023-11-22T13:15:42Z",
        "summary": "Recent studies have uncovered the potential of Large Language Models (LLMs)\nin addressing complex sequential decision-making tasks through the provision of\nhigh-level instructions. However, LLM-based agents lack specialization in\ntackling specific target problems, particularly in real-time dynamic\nenvironments. Additionally, deploying an LLM-based agent in practical scenarios\ncan be both costly and time-consuming. On the other hand, reinforcement\nlearning (RL) approaches train agents that specialize in the target task but\noften suffer from low sampling efficiency and high exploration costs. In this\npaper, we introduce a novel framework that addresses these challenges by\ntraining a smaller, specialized student RL agent using instructions from an\nLLM-based teacher agent. By incorporating the guidance from the teacher agent,\nthe student agent can distill the prior knowledge of the LLM into its own\nmodel. Consequently, the student agent can be trained with significantly less\ndata. Moreover, through further training with environment feedback, the student\nagent surpasses the capabilities of its teacher for completing the target task.\nWe conducted experiments on challenging MiniGrid and Habitat environments,\nspecifically designed for embodied AI research, to evaluate the effectiveness\nof our framework. The results clearly demonstrate that our approach achieves\nsuperior performance compared to strong baseline methods. Our code is available\nat https://github.com/ZJLAB-AMMI/LLM4Teach.",
        "pdf_link": "https://arxiv.org/pdf/2311.13373v4.pdf"
    },
    {
        "title": "Applying Large Language Models to Power Systems: Potential Security Threats",
        "authors": [
            "Jiaqi Ruan",
            "Gaoqi Liang",
            "Huan Zhao",
            "Guolong Liu",
            "Xianzhuo Sun",
            "Jing Qiu",
            "Zhao Xu",
            "Fushuan Wen",
            "Zhao Yang Dong"
        ],
        "published": "2023-11-22T12:55:02Z",
        "summary": "Applying large language models (LLMs) to modern power systems presents a\npromising avenue for enhancing decision-making and operational efficiency.\nHowever, this action may also incur potential security threats, which have not\nbeen fully recognized so far. To this end, this article analyzes potential\nthreats incurred by applying LLMs to power systems, emphasizing the need for\nurgent research and development of countermeasures.",
        "pdf_link": "https://arxiv.org/pdf/2311.13361v2.pdf"
    },
    {
        "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting",
        "authors": [
            "Xinyan Guan",
            "Yanjiang Liu",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-11-22T11:08:38Z",
        "summary": "Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13314v1.pdf"
    },
    {
        "title": "Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process",
        "authors": [
            "Nick Goodson",
            "Rongfei Lu"
        ],
        "published": "2023-11-22T10:04:29Z",
        "summary": "Large Language Models (LLMs) and chatbots show significant promise in\nstreamlining the legal intake process. This advancement can greatly reduce the\nworkload and costs for legal aid organizations, improving availability while\nmaking legal assistance more accessible to a broader audience. However, a key\nchallenge with current LLMs is their tendency to overconfidently deliver an\nimmediate 'best guess' to a client's question based on the output distribution\nlearned over the training data. This approach often overlooks the client's\nactual intentions or the specifics of their legal situation. As a result,\nclients may not realize the importance of providing essential additional\ncontext or expressing their underlying intentions, which are crucial for their\nlegal cases. Traditionally, logic based decision trees have been used to\nautomate intake for specific access to justice issues, such as immigration and\neviction. But those solutions lack scalability. We demonstrate a\nproof-of-concept using LLMs to elicit and infer clients' underlying intentions\nand specific legal circumstances through free-form, language-based\ninteractions. We also propose future research directions to use supervised\nfine-tuning or offline reinforcement learning to automatically incorporate\nintention and context elicitation in chatbots without explicit prompting.",
        "pdf_link": "https://arxiv.org/pdf/2311.13281v1.pdf"
    },
    {
        "title": "AutoKG: Efficient Automated Knowledge Graph Generation for Language Models",
        "authors": [
            "Bohan Chen",
            "Andrea L. Bertozzi"
        ],
        "published": "2023-11-22T08:58:25Z",
        "summary": "Traditional methods of linking large language models (LLMs) to knowledge\nbases via the semantic similarity search often fall short of capturing complex\nrelational dynamics. To address these limitations, we introduce AutoKG, a\nlightweight and efficient approach for automated knowledge graph (KG)\nconstruction. For a given knowledge base consisting of text blocks, AutoKG\nfirst extracts keywords using a LLM and then evaluates the relationship weight\nbetween each pair of keywords using graph Laplace learning. We employ a hybrid\nsearch scheme combining vector similarity and graph-based associations to\nenrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a\nmore comprehensive and interconnected knowledge retrieval mechanism compared to\nthe semantic similarity search, thereby enhancing the capabilities of LLMs in\ngenerating more insightful and relevant outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14740v1.pdf"
    },
    {
        "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
        "authors": [
            "Tianhang Zhang",
            "Lin Qiu",
            "Qipeng Guo",
            "Cheng Deng",
            "Yue Zhang",
            "Zheng Zhang",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Luoyi Fu"
        ],
        "published": "2023-11-22T08:39:17Z",
        "summary": "Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.",
        "pdf_link": "https://arxiv.org/pdf/2311.13230v1.pdf"
    },
    {
        "title": "Large Language Models in Education: Vision and Opportunities",
        "authors": [
            "Wensheng Gan",
            "Zhenlian Qi",
            "Jiayang Wu",
            "Jerry Chun-Wei Lin"
        ],
        "published": "2023-11-22T05:04:20Z",
        "summary": "With the rapid development of artificial intelligence technology, large\nlanguage models (LLMs) have become a hot research topic. Education plays an\nimportant role in human social development and progress. Traditional education\nfaces challenges such as individual student differences, insufficient\nallocation of teaching resources, and assessment of teaching effectiveness.\nTherefore, the applications of LLMs in the field of digital/smart education\nhave broad prospects. The research on educational large models (EduLLMs) is\nconstantly evolving, providing new methods and approaches to achieve\npersonalized learning, intelligent tutoring, and educational assessment goals,\nthereby improving the quality of education and the learning experience. This\narticle aims to investigate and summarize the application of LLMs in smart\neducation. It first introduces the research background and motivation of LLMs\nand explains the essence of LLMs. It then discusses the relationship between\ndigital education and EduLLMs and summarizes the current research status of\neducational large models. The main contributions are the systematic summary and\nvision of the research background, motivation, and application of large models\nfor education (LLM4Edu). By reviewing existing research, this article provides\nguidance and insights for educators, researchers, and policy-makers to gain a\ndeep understanding of the potential and challenges of LLM4Edu. It further\nprovides guidance for further advancing the development and application of\nLLM4Edu, while still facing technical, ethical, and practical challenges\nrequiring further research and exploration.",
        "pdf_link": "https://arxiv.org/pdf/2311.13160v1.pdf"
    },
    {
        "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
        "authors": [
            "Qifan Yu",
            "Juncheng Li",
            "Longhui Wei",
            "Liang Pang",
            "Wentao Ye",
            "Bosheng Qin",
            "Siliang Tang",
            "Qi Tian",
            "Yueting Zhuang"
        ],
        "published": "2023-11-22T04:52:58Z",
        "summary": "Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.",
        "pdf_link": "https://arxiv.org/pdf/2311.13614v2.pdf"
    },
    {
        "title": "Conditions for Length Generalization in Learning Reasoning Skills",
        "authors": [
            "Changnan Xiao",
            "Bing Liu"
        ],
        "published": "2023-11-22T03:36:18Z",
        "summary": "Reasoning is a fundamental capability of AI agents. Recently, large language\nmodels (LLMs) have shown remarkable abilities to perform reasoning tasks.\nHowever, numerous evaluations of the reasoning capabilities of LLMs have also\nshowed some limitations. An outstanding limitation is length generalization,\nmeaning that when trained on reasoning problems of smaller lengths or sizes,\nthe resulting models struggle with problems of larger sizes or lengths. This\npotentially indicates some theoretical limitations of generalization in\nlearning reasoning skills. These evaluations and their observations motivated\nus to perform a theoretical study of the length generalization problem. This\nwork focuses on reasoning tasks that can be formulated as Markov dynamic\nprocesses (MDPs) and/or directed acyclic graphs (DAGs). It identifies and\nproves conditions that decide whether the length generalization problem can be\nsolved or not for a reasoning task in a particular representation. Experiments\nare also conducted to verify the theoretical results.",
        "pdf_link": "https://arxiv.org/pdf/2311.16173v2.pdf"
    },
    {
        "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper",
        "authors": [
            "Chengyu Wang",
            "Junbing Yan",
            "Wei Zhang",
            "Jun Huang"
        ],
        "published": "2023-11-22T03:28:34Z",
        "summary": "This paper delves into the pressing need in Parameter-Efficient Fine-Tuning\n(PEFT) for Large Language Models (LLMs). While LLMs possess remarkable\ncapabilities, their extensive parameter requirements and associated\ncomputational demands hinder their practicality and scalability for real-world\napplications. Our position paper highlights current states and the necessity of\nfurther studying into the topic, and recognizes significant challenges and open\nissues that must be addressed to fully harness the powerful abilities of LLMs.\nThese challenges encompass novel efficient PEFT architectures, PEFT for\ndifferent learning settings, PEFT combined with model compression techniques,\nand the exploration of PEFT for multi-modal LLMs. By presenting this position\npaper, we aim to stimulate further research and foster discussions surrounding\nmore efficient and accessible PEFT for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13126v1.pdf"
    },
    {
        "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
        "authors": [
            "Ha-Thanh Nguyen",
            "Wachara Fungwacharakorn",
            "Ken Satoh"
        ],
        "published": "2023-11-22T01:51:50Z",
        "summary": "Language serves as a vehicle for conveying thought, enabling communication\namong individuals. The ability to distinguish between diverse concepts,\nidentify fairness and injustice, and comprehend a range of legal notions\nfundamentally relies on logical reasoning. Large Language Models (LLMs) attempt\nto emulate human language understanding and generation, but their competency in\nlogical reasoning remains limited. This paper seeks to address the\nphilosophical question: How can we effectively teach logical reasoning to LLMs\nwhile maintaining a deep understanding of the intricate relationship between\nlanguage and logic? By focusing on bolstering LLMs' capabilities in logical\nreasoning, we aim to expand their applicability in law and other\nlogic-intensive disciplines. To this end, we propose a Reinforcement Learning\nfrom Logical Feedback (RLLF) approach, which serves as a potential framework\nfor refining LLMs' reasoning capacities. Through RLLF and a revised evaluation\nmethodology, we explore new avenues for research in this domain and contribute\nto the development of LLMs capable of handling complex legal reasoning tasks\nwhile acknowledging the fundamental connection between language and logic.",
        "pdf_link": "https://arxiv.org/pdf/2311.13095v1.pdf"
    },
    {
        "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift",
        "authors": [
            "Will LeVine",
            "Benjamin Pikus",
            "Anthony Chen",
            "Sean Hendryx"
        ],
        "published": "2023-11-21T18:41:26Z",
        "summary": "Foundation models, specifically Large Language Models (LLMs), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.",
        "pdf_link": "https://arxiv.org/pdf/2311.14743v7.pdf"
    },
    {
        "title": "Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions",
        "authors": [
            "Hye Sun Yun",
            "Mehdi Arjmand",
            "Phillip Raymond Sherlock",
            "Michael Paasche-Orlow",
            "James W. Griffith",
            "Timothy Bickmore"
        ],
        "published": "2023-11-21T16:20:49Z",
        "summary": "Standardized, validated questionnaires are vital tools in HCI research and\nhealthcare, offering dependable self-report data. However, their repeated use\nin longitudinal or pre-post studies can induce respondent fatigue, impacting\ndata quality via response biases and decreased response rates. We propose\nutilizing large language models (LLMs) to generate diverse questionnaire\nversions while retaining good psychometric properties. In a longitudinal study,\nparticipants engaged with our agent system and responded daily for two weeks to\neither a standardized depression questionnaire or one of two LLM-generated\nquestionnaire variants, alongside a validated depression questionnaire.\nPsychometric testing revealed consistent covariation between the external\ncriterion and the focal measure administered across the three conditions,\ndemonstrating the reliability and validity of the LLM-generated variants.\nParticipants found the repeated administration of the standardized\nquestionnaire significantly more repetitive compared to the variants. Our\nfindings highlight the potential of LLM-generated variants to invigorate\nquestionnaires, fostering engagement and interest without compromising\nvalidity.",
        "pdf_link": "https://arxiv.org/pdf/2311.12707v1.pdf"
    },
    {
        "title": "Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study",
        "authors": [
            "Mengyang Chen",
            "Lingwei Wei",
            "Han Cao",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-11-21T16:03:51Z",
        "summary": "Large Language Models (LLMs) have garnered significant attention for their\npowerful ability in natural language understanding and reasoning. In this\npaper, we present a comprehensive empirical study to explore the performance of\nLLMs on misinformation detection tasks. This study stands as the pioneering\ninvestigation into the understanding capabilities of multiple LLMs regarding\nboth content and propagation across social media platforms. Our empirical\nstudies on five misinformation detection datasets show that LLMs with diverse\nprompts achieve comparable performance in text-based misinformation detection\nbut exhibit notably constrained capabilities in comprehending propagation\nstructure compared to existing models in propagation-based misinformation\ndetection. Besides, we further design four instruction-tuned strategies to\nenhance LLMs for both content and propagation-based misinformation detection.\nThese strategies boost LLMs to actively learn effective features from multiple\ninstances or hard instances, and eliminate irrelevant propagation structures,\nthereby achieving better detection performance. Extensive experiments further\ndemonstrate LLMs would play a better capacity in content and propagation\nstructure under these proposed strategies and achieve promising detection\nperformance. These findings highlight the potential ability of LLMs to detect\nmisinformation.",
        "pdf_link": "https://arxiv.org/pdf/2311.12699v1.pdf"
    },
    {
        "title": "HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis",
        "authors": [
            "Sang-Hoon Lee",
            "Ha-Yeong Choi",
            "Seung-Bin Kim",
            "Seong-Whan Lee"
        ],
        "published": "2023-11-21T09:07:11Z",
        "summary": "Large language models (LLM)-based speech synthesis has been widely adopted in\nzero-shot speech synthesis. However, they require a large-scale data and\npossess the same limitations as previous autoregressive speech models,\nincluding slow inference speed and lack of robustness. This paper proposes\nHierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech\n(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis\nframeworks could significantly improve the robustness and expressiveness of the\nsynthetic speech. Furthermore, we significantly improve the naturalness and\nspeaker similarity of synthetic speech even in zero-shot speech synthesis\nscenarios. For text-to-speech, we adopt the text-to-vec framework, which\ngenerates a self-supervised speech representation and an F0 representation\nbased on text representations and prosody prompts. Then, HierSpeech++ generates\nspeech from the generated vector, F0, and voice prompt. We further introduce a\nhigh-efficient speech super-resolution framework from 16 kHz to 48 kHz. The\nexperimental results demonstrated that the hierarchical variational autoencoder\ncould be a strong zero-shot speech synthesizer given that it outperforms\nLLM-based and diffusion-based models. Moreover, we achieved the first\nhuman-level quality zero-shot speech synthesis. Audio samples and source code\nare available at https://github.com/sh-lee-prml/HierSpeechpp.",
        "pdf_link": "https://arxiv.org/pdf/2311.12454v2.pdf"
    },
    {
        "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
        "authors": [
            "Yunpeng Huang",
            "Jingwei Xu",
            "Junyu Lai",
            "Zixu Jiang",
            "Taolue Chen",
            "Zenan Li",
            "Yuan Yao",
            "Xiaoxing Ma",
            "Lijuan Yang",
            "Hao Chen",
            "Shupeng Li",
            "Penghao Zhao"
        ],
        "published": "2023-11-21T04:59:17Z",
        "summary": "Transformer-based Large Language Models (LLMs) have been applied in diverse\nareas such as knowledge bases, human interfaces, and dynamic agents, and\nmarking a stride towards achieving Artificial General Intelligence (AGI).\nHowever, current LLMs are predominantly pretrained on short text snippets,\nwhich compromises their effectiveness in processing the long-context prompts\nthat are frequently encountered in practical scenarios. This article offers a\ncomprehensive survey of the recent advancement in Transformer-based LLM\narchitectures aimed at enhancing the long-context capabilities of LLMs\nthroughout the entire model lifecycle, from pre-training through to inference.\nWe first delineate and analyze the problems of handling long-context input and\noutput with the current Transformer-based models. We then provide a taxonomy\nand the landscape of upgrades on Transformer architecture to solve these\nproblems. Afterwards, we provide an investigation on wildly used evaluation\nnecessities tailored for long-context LLMs, including datasets, metrics, and\nbaseline models, as well as optimization toolkits such as libraries,\nframeworks, and compilers to boost the efficacy of LLMs across different stages\nin runtime. Finally, we discuss the challenges and potential avenues for future\nresearch. A curated repository of relevant literature, continuously updated, is\navailable at https://github.com/Strivin0311/long-llms-learning.",
        "pdf_link": "https://arxiv.org/pdf/2311.12351v2.pdf"
    },
    {
        "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
        "authors": [
            "Samira Ghodratnama",
            "Mehrdad Zakershahrak"
        ],
        "published": "2023-11-21T02:01:01Z",
        "summary": "The advent of Large Language Models (LLMs) heralds a pivotal shift in online\nuser interactions with information. Traditional Information Retrieval (IR)\nsystems primarily relied on query-document matching, whereas LLMs excel in\ncomprehending and generating human-like text, thereby enriching the IR\nexperience significantly. While LLMs are often associated with chatbot\nfunctionalities, this paper extends the discussion to their explicit\napplication in information retrieval. We explore methodologies to optimize the\nretrieval process, select optimal models, and effectively scale and orchestrate\nLLMs, aiming for cost-efficiency and enhanced result accuracy. A notable\nchallenge, model hallucination-where the model yields inaccurate or\nmisinterpreted data-is addressed alongside other model-specific hurdles. Our\ndiscourse extends to crucial considerations including user privacy, data\noptimization, and the necessity for system clarity and interpretability.\nThrough a comprehensive examination, we unveil not only innovative strategies\nfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,\nbut also the consequential considerations that underline the need for a\nbalanced approach aligned with user-centric principles.",
        "pdf_link": "https://arxiv.org/pdf/2311.12287v1.pdf"
    },
    {
        "title": "Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis",
        "authors": [
            "Ruiyang Qin",
            "Jun Xia",
            "Zhenge Jia",
            "Meng Jiang",
            "Ahmed Abbasi",
            "Peipei Zhou",
            "Jingtong Hu",
            "Yiyu Shi"
        ],
        "published": "2023-11-21T01:34:02Z",
        "summary": "After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.",
        "pdf_link": "https://arxiv.org/pdf/2311.12275v3.pdf"
    },
    {
        "title": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software",
        "authors": [
            "Dananjay Srinivas",
            "Rohan Das",
            "Saeid Tizpaz-Niari",
            "Ashutosh Trivedi",
            "Maria Leonor Pacheco"
        ],
        "published": "2023-11-20T18:12:28Z",
        "summary": "Due to the ever-increasing complexity of income tax laws in the United\nStates, the number of US taxpayers filing their taxes using tax preparation\nsoftware (henceforth, tax software) continues to increase. According to the\nU.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed\ntheir individual income taxes using tax software. Given the legal consequences\nof incorrectly filing taxes for the taxpayer, ensuring the correctness of tax\nsoftware is of paramount importance. Metamorphic testing has emerged as a\nleading solution to test and debug legal-critical tax software due to the\nabsence of correctness requirements and trustworthy datasets. The key idea\nbehind metamorphic testing is to express the properties of a system in terms of\nthe relationship between one input and its slightly metamorphosed twinned\ninput. Extracting metamorphic properties from IRS tax publications is a tedious\nand time-consuming process. As a response, this paper formulates the task of\ngenerating metamorphic specifications as a translation task between properties\nextracted from tax documents - expressed in natural language - to a contrastive\nfirst-order logic form. We perform a systematic analysis on the potential and\nlimitations of in-context learning with Large Language Models(LLMs) for this\ntask, and outline a research agenda towards automating the generation of\nmetamorphic specifications for tax preparation software.",
        "pdf_link": "https://arxiv.org/pdf/2311.11979v1.pdf"
    },
    {
        "title": "FinanceBench: A New Benchmark for Financial Question Answering",
        "authors": [
            "Pranab Islam",
            "Anand Kannappan",
            "Douwe Kiela",
            "Rebecca Qian",
            "Nino Scherrer",
            "Bertie Vidgen"
        ],
        "published": "2023-11-20T17:28:02Z",
        "summary": "FinanceBench is a first-of-its-kind test suite for evaluating the performance\nof LLMs on open book financial question answering (QA). It comprises 10,231\nquestions about publicly traded companies, with corresponding answers and\nevidence strings. The questions in FinanceBench are ecologically valid and\ncover a diverse set of scenarios. They are intended to be clear-cut and\nstraightforward to answer to serve as a minimum performance standard. We test\n16 state of the art model configurations (including GPT-4-Turbo, Llama2 and\nClaude2, with vector stores and long context prompts) on a sample of 150 cases\nfrom FinanceBench, and manually review their answers (n=2,400). The cases are\navailable open-source. We show that existing LLMs have clear limitations for\nfinancial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly\nanswered or refused to answer 81% of questions. While augmentation techniques\nsuch as using longer context window to feed in relevant evidence improve\nperformance, they are unrealistic for enterprise settings due to increased\nlatency and cannot support larger financial documents. We find that all models\nexamined exhibit weaknesses, such as hallucinations, that limit their\nsuitability for use by enterprises.",
        "pdf_link": "https://arxiv.org/pdf/2311.11944v1.pdf"
    },
    {
        "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
        "authors": [
            "Songhao Han",
            "Le Zhuo",
            "Yue Liao",
            "Si Liu"
        ],
        "published": "2023-11-20T16:37:45Z",
        "summary": "Vision-language models (VLMs) offer a promising paradigm for image\nclassification by comparing the similarity between images and class embeddings.\nA critical challenge lies in crafting precise textual representations for class\nnames. While previous studies have leveraged recent advancements in large\nlanguage models (LLMs) to enhance these descriptors, their outputs often suffer\nfrom ambiguity and inaccuracy. We attribute this to two primary factors: 1) the\nreliance on single-turn textual interactions with LLMs, leading to a mismatch\nbetween generated text and visual concepts for VLMs; 2) the oversight of the\ninter-class relationships, resulting in descriptors that fail to differentiate\nsimilar classes effectively. In this paper, we propose a novel framework that\nintegrates LLMs and VLMs to find the optimal class descriptors. Our\ntraining-free approach develops an LLM-based agent with an evolutionary\noptimization strategy to iteratively refine class descriptors. We demonstrate\nour optimized descriptors are of high quality which effectively improves\nclassification accuracy on a wide range of benchmarks. Additionally, these\ndescriptors offer explainable and robust features, boosting performance across\nvarious backbone models and complementing fine-tuning-based methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.11904v2.pdf"
    },
    {
        "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
        "authors": [
            "Yu Tian",
            "Xiao Yang",
            "Jingyuan Zhang",
            "Yinpeng Dong",
            "Hang Su"
        ],
        "published": "2023-11-20T15:50:09Z",
        "summary": "Rapid advancements in large language models (LLMs) have revitalized in\nLLM-based agents, exhibiting impressive human-like behaviors and cooperative\ncapabilities in various scenarios. However, these agents also bring some\nexclusive risks, stemming from the complexity of interaction environments and\nthe usability of tools. This paper delves into the safety of LLM-based agents\nfrom three perspectives: agent quantity, role definition, and attack level.\nSpecifically, we initially propose to employ a template-based attack strategy\non LLM-based agents to find the influence of agent quantity. In addition, to\naddress interaction environment and role specificity issues, we introduce Evil\nGeniuses (EG), an effective attack method that autonomously generates prompts\nrelated to the original role to examine the impact across various role\ndefinitions and attack levels. EG leverages Red-Blue exercises, significantly\nimproving the generated prompt aggressiveness and similarity to original roles.\nOur evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4,\ndemonstrate high success rates. Extensive evaluation and discussion reveal that\nthese agents are less robust, prone to more harmful behaviors, and capable of\ngenerating stealthier content than LLMs, highlighting significant safety\nchallenges and guiding future research. Our code is available at\nhttps://github.com/T1aNS1R/Evil-Geniuses.",
        "pdf_link": "https://arxiv.org/pdf/2311.11855v2.pdf"
    },
    {
        "title": "System 2 Attention (is something you might need too)",
        "authors": [
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "published": "2023-11-20T15:04:50Z",
        "summary": "Soft attention in Transformer-based Large Language Models (LLMs) is\nsusceptible to incorporating irrelevant information from the context into its\nlatent representations, which adversely affects next token generations. To help\nrectify these issues, we introduce System 2 Attention (S2A), which leverages\nthe ability of LLMs to reason in natural language and follow instructions in\norder to decide what to attend to. S2A regenerates the input context to only\ninclude the relevant portions, before attending to the regenerated context to\nelicit the final response. In experiments, S2A outperforms standard\nattention-based LLMs on three tasks containing opinion or irrelevant\ninformation, QA, math word problems and longform generation, where S2A\nincreases factuality and objectivity, and decreases sycophancy.",
        "pdf_link": "https://arxiv.org/pdf/2311.11829v1.pdf"
    },
    {
        "title": "Towards Robust Text Retrieval with Progressive Learning",
        "authors": [
            "Tong Wu",
            "Yulei Qin",
            "Enwei Zhang",
            "Zihan Xu",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun"
        ],
        "published": "2023-11-20T11:44:01Z",
        "summary": "Retrieval augmentation has become an effective solution to empower large\nlanguage models (LLMs) with external and verified knowledge sources from the\ndatabase, which overcomes the limitations and hallucinations of LLMs in\nhandling up-to-date and domain-specific information. However, existing\nembedding models for text retrieval usually have three non-negligible\nlimitations. First, the number and diversity of samples in a batch are too\nrestricted to supervise the modeling of textual nuances at scale. Second, the\nhigh proportional noise are detrimental to the semantic correctness and\nconsistency of embeddings. Third, the equal treatment to easy and difficult\nsamples would cause sub-optimum convergence of embeddings with poorer\ngeneralization. In this paper, we propose the PEG, a progressively learned\nembeddings for robust text retrieval. Specifically, we increase the training\nin-batch negative samples to 80,000, and for each query, we extracted five hard\nnegatives. Concurrently, we incorporated a progressive learning mechanism,\nenabling the model to dynamically modulate its attention to the samples\nthroughout the entire training process. Additionally, PEG is trained on more\nthan 100 million data, encompassing a wide range of domains (e.g., finance,\nmedicine, and tourism) and covering various tasks (e.g., question-answering,\nmachine reading comprehension, and similarity matching). Extensive experiments\nconducted on C-MTEB and DuReader demonstrate that PEG surpasses\nstate-of-the-art embeddings in retrieving true positives, highlighting its\nsignificant potential for applications in LLMs. Our model is publicly available\nat https://huggingface.co/TownsWu/PEG.",
        "pdf_link": "https://arxiv.org/pdf/2311.11691v1.pdf"
    },
    {
        "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples",
        "authors": [
            "Atsushi Shirafuji",
            "Yusuke Oda",
            "Jun Suzuki",
            "Makoto Morishita",
            "Yutaka Watanobe"
        ],
        "published": "2023-11-20T11:43:45Z",
        "summary": "A less complex and more straightforward program is a crucial factor that\nenhances its maintainability and makes writing secure and bug-free programs\neasier. However, due to its heavy workload and the risks of breaking the\nworking programs, programmers are reluctant to do code refactoring, and thus,\nit also causes the loss of potential learning experiences. To mitigate this, we\ndemonstrate the application of using a large language model (LLM), GPT-3.5, to\nsuggest less complex versions of the user-written Python program, aiming to\nencourage users to learn how to write better programs. We propose a method to\nleverage the prompting with few-shot examples of the LLM by selecting the\nbest-suited code refactoring examples for each target programming problem based\non the prior evaluation of prompting with the one-shot example. The\nquantitative evaluation shows that 95.68% of programs can be refactored by\ngenerating 10 candidates each, resulting in a 17.35% reduction in the average\ncyclomatic complexity and a 25.84% decrease in the average number of lines\nafter filtering only generated programs that are semantically correct.\nFurthermore, the qualitative evaluation shows outstanding capability in code\nformatting, while unnecessary behaviors such as deleting or translating\ncomments are also observed.",
        "pdf_link": "https://arxiv.org/pdf/2311.11690v1.pdf"
    },
    {
        "title": "Incorporating LLM Priors into Tabular Learners",
        "authors": [
            "Max Zhu",
            "Sini\u0161a Stanivuk",
            "Andrija Petrovic",
            "Mladen Nikolic",
            "Pietro Lio"
        ],
        "published": "2023-11-20T09:27:09Z",
        "summary": "We present a method to integrate Large Language Models (LLMs) and traditional\ntabular data classification techniques, addressing LLMs challenges like data\nserialization sensitivity and biases. We introduce two strategies utilizing\nLLMs for ranking categorical variables and generating priors on correlations\nbetween continuous variables and targets, enhancing performance in few-shot\nscenarios. We focus on Logistic Regression, introducing MonotonicLR that\nemploys a non-linear monotonic function for mapping ordinals to cardinals while\npreserving LLM-determined orders. Validation against baseline models reveals\nthe superior performance of our approach, especially in low-data scenarios,\nwhile remaining interpretable.",
        "pdf_link": "https://arxiv.org/pdf/2311.11628v1.pdf"
    },
    {
        "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
        "authors": [
            "Quanyu Long",
            "Wenya Wang",
            "Sinno Jialin Pan"
        ],
        "published": "2023-11-20T06:06:20Z",
        "summary": "Large language models (LLMs) have showcased their capability with few-shot\ninference known as in-context learning. However, in-domain demonstrations are\nnot always readily available in real scenarios, leading to cross-domain\nin-context learning. Besides, LLMs are still facing challenges in long-tail\nknowledge in unseen and unfamiliar domains. The above limitations demonstrate\nthe necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study\nthe UDA problem under an in-context learning setting to adapt language models\nfrom the source domain to the target domain without any target labels. The core\nidea is to retrieve a subset of cross-domain elements that are the most similar\nto the query, and elicit language model to adapt in an in-context manner by\nlearning both target domain distribution and the discriminative task signal\nsimultaneously with the augmented cross-domain in-context examples. We devise\ndifferent prompting and training strategies, accounting for different LM\narchitectures to learn the target distribution via language modeling. With\nextensive experiments on Sentiment Analysis (SA) and Named Entity Recognition\n(NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer\nand demonstrate significant improvements over baseline models.",
        "pdf_link": "https://arxiv.org/pdf/2311.11551v1.pdf"
    },
    {
        "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
        "authors": [
            "Zhengmian Hu",
            "Gang Wu",
            "Saayan Mitra",
            "Ruiyi Zhang",
            "Tong Sun",
            "Heng Huang",
            "Viswanathan Swaminathan"
        ],
        "published": "2023-11-20T03:17:21Z",
        "summary": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in\nvarious applications. However, these models are susceptible to adversarial\nprompt attacks, where attackers can carefully curate input strings that mislead\nLLMs into generating incorrect or undesired outputs. Previous work has revealed\nthat with relatively simple yet effective attacks based on discrete\noptimization, it is possible to generate adversarial prompts that bypass\nmoderation and alignment of the models. This vulnerability to adversarial\nprompts underscores a significant concern regarding the robustness and\nreliability of LLMs. Our work aims to address this concern by introducing a\nnovel approach to detecting adversarial prompts at a token level, leveraging\nthe LLM's capability to predict the next token's probability. We measure the\ndegree of the model's perplexity, where tokens predicted with high probability\nare considered normal, and those exhibiting high perplexity are flagged as\nadversarial. Additionaly, our method also integrates context understanding by\nincorporating neighboring token information to encourage the detection of\ncontiguous adversarial prompt sequences. To this end, we design two algorithms\nfor adversarial prompt detection: one based on optimization techniques and\nanother on Probabilistic Graphical Models (PGM). Both methods are equipped with\nefficient solving methods, ensuring efficient adversarial prompt detection. Our\ntoken-level detection result can be visualized as heatmap overlays on the text\nsequence, allowing for a clearer and more intuitive representation of which\npart of the text may contain adversarial prompts.",
        "pdf_link": "https://arxiv.org/pdf/2311.11509v3.pdf"
    },
    {
        "title": "A Security Risk Taxonomy for Large Language Models",
        "authors": [
            "Erik Derner",
            "Kristina Batisti\u010d",
            "Jan Zah\u00e1lka",
            "Robert Babu\u0161ka"
        ],
        "published": "2023-11-19T20:22:05Z",
        "summary": "As large language models (LLMs) permeate more and more applications, an\nassessment of their associated security risks becomes increasingly necessary.\nThe potential for exploitation by malicious actors, ranging from disinformation\nto data breaches and reputation damage, is substantial. This paper addresses a\ngap in current research by focusing on the security risks posed by LLMs, which\nextends beyond the widely covered ethical and societal implications. Our work\nproposes a taxonomy of security risks along the user-model communication\npipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize\nthe attacks by target and attack type within a prompt-based interaction scheme.\nThe taxonomy is reinforced with specific attack examples to showcase the\nreal-world impact of these risks. Through this taxonomy, we aim to inform the\ndevelopment of robust and secure LLM applications, enhancing their safety and\ntrustworthiness.",
        "pdf_link": "https://arxiv.org/pdf/2311.11415v1.pdf"
    },
    {
        "title": "Zero-Shot Question Answering over Financial Documents using Large Language Models",
        "authors": [
            "Karmvir Singh Phogat",
            "Chetan Harsha",
            "Sridhar Dasaratha",
            "Shashishekar Ramakrishna",
            "Sai Akhil Puranam"
        ],
        "published": "2023-11-19T16:23:34Z",
        "summary": "We introduce a large language model (LLM) based approach to answer complex\nquestions requiring multi-hop numerical reasoning over financial reports. While\nLLMs have exhibited remarkable performance on various natural language and\nreasoning tasks, complex reasoning problems often rely on few-shot prompts that\nrequire carefully crafted examples. In contrast, our approach uses novel\nzero-shot prompts that guide the LLM to encode the required reasoning into a\nPython program or a domain specific language. The generated program is then\nexecuted by a program interpreter, thus mitigating the limitations of LLM in\nperforming accurate arithmetic calculations.\n  We evaluate the proposed approach on three financial datasets using some of\nthe recently developed generative pretrained transformer (GPT) models and\nperform comparisons with various zero-shot baselines. The experimental results\ndemonstrate that our approach significantly improves the accuracy for all the\nLLMs over their respective baselines. We provide a detailed analysis of the\nresults, generating insights to support our findings. The success of our\napproach demonstrates the enormous potential to extract complex domain specific\nnumerical reasoning by designing zero-shot prompts to effectively exploit the\nknowledge embedded in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14722v1.pdf"
    },
    {
        "title": "TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems",
        "authors": [
            "Yilun Kong",
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Tianpeng Bao",
            "Shiwei Shi",
            "Guoqing Du",
            "Xiaoru Hu",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "published": "2023-11-19T12:37:30Z",
        "summary": "Large Language Models (LLMs) have demonstrated proficiency in addressing\ntasks that necessitate a combination of task planning and the usage of external\ntools that require a blend of task planning and the utilization of external\ntools, such as APIs. However, real-world complex systems present three\nprevalent challenges concerning task planning and tool usage: (1) The real\nsystem usually has a vast array of APIs, so it is impossible to feed the\ndescriptions of all APIs to the prompt of LLMs as the token length is limited;\n(2) the real system is designed for handling complex tasks, and the base LLMs\ncan hardly plan a correct sub-task order and API-calling order for such tasks;\n(3) Similar semantics and functionalities among APIs in real systems create\nchallenges for both LLMs and even humans in distinguishing between them. In\nresponse, this paper introduces a comprehensive framework aimed at enhancing\nthe Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating\nwithin real-world systems. Our framework comprises three key components\ndesigned to address these challenges: (1) the API Retriever selects the most\npertinent APIs for the user task among the extensive array available; (2) LLM\nFinetuner tunes a base LLM so that the finetuned LLM can be more capable for\ntask planning and API calling; (3) the Demo Selector adaptively retrieves\ndifferent demonstrations related to hard-to-distinguish APIs, which is further\nused for in-context learning to boost the final performance. We validate our\nmethods using a real-world commercial system as well as an open-sourced\nacademic dataset, and the outcomes clearly showcase the efficacy of each\nindividual component as well as the integrated framework.",
        "pdf_link": "https://arxiv.org/pdf/2311.11315v1.pdf"
    },
    {
        "title": "Rethinking Large Language Models in Mental Health Applications",
        "authors": [
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Kailai Yang",
            "Sophia Ananiadou",
            "Erik Cambria"
        ],
        "published": "2023-11-19T08:40:01Z",
        "summary": "Large Language Models (LLMs) have become valuable assets in mental health,\nshowing promise in both classification tasks and counseling applications. This\npaper offers a perspective on using LLMs in mental health applications. It\ndiscusses the instability of generative models for prediction and the potential\nfor generating hallucinatory outputs, underscoring the need for ongoing audits\nand evaluations to maintain their reliability and dependability. The paper also\ndistinguishes between the often interchangeable terms ``explainability'' and\n``interpretability'', advocating for developing inherently interpretable\nmethods instead of relying on potentially hallucinated self-explanations\ngenerated by LLMs. Despite the advancements in LLMs, human counselors'\nempathetic understanding, nuanced interpretation, and contextual awareness\nremain irreplaceable in the sensitive and complex realm of mental health\ncounseling. The use of LLMs should be approached with a judicious and\nconsiderate mindset, viewing them as tools that complement human expertise\nrather than seeking to replace it.",
        "pdf_link": "https://arxiv.org/pdf/2311.11267v2.pdf"
    },
    {
        "title": "Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness",
        "authors": [
            "Gongbo Zhang",
            "Qiao Jin",
            "Denis Jered McInerney",
            "Yong Chen",
            "Fei Wang",
            "Curtis L. Cole",
            "Qian Yang",
            "Yanshan Wang",
            "Bradley A. Malin",
            "Mor Peleg",
            "Byron C. Wallace",
            "Zhiyong Lu",
            "Chunhua Weng",
            "Yifan Peng"
        ],
        "published": "2023-11-19T03:29:45Z",
        "summary": "Evidence-based medicine promises to improve the quality of healthcare by\nempowering medical decisions and practices with the best available evidence.\nThe rapid growth of medical evidence, which can be obtained from various\nsources, poses a challenge in collecting, appraising, and synthesizing the\nevidential information. Recent advancements in generative AI, exemplified by\nlarge language models, hold promise in facilitating the arduous task. However,\ndeveloping accountable, fair, and inclusive models remains a complicated\nundertaking. In this perspective, we discuss the trustworthiness of generative\nAI in the context of automated summarization of medical evidence.",
        "pdf_link": "https://arxiv.org/pdf/2311.11211v3.pdf"
    },
    {
        "title": "Few-Shot Classification & Segmentation Using Large Language Models Agent",
        "authors": [
            "Tian Meng",
            "Yang Tao",
            "Wuliang Yin"
        ],
        "published": "2023-11-19T00:33:41Z",
        "summary": "The task of few-shot image classification and segmentation (FS-CS) requires\nthe classification and segmentation of target objects in a query image, given\nonly a few examples of the target classes. We introduce a method that utilises\nlarge language models (LLM) as an agent to address the FS-CS problem in a\ntraining-free manner. By making the LLM the task planner and off-the-shelf\nvision models the tools, the proposed method is capable of classifying and\nsegmenting target objects using only image-level labels. Specifically,\nchain-of-thought prompting and in-context learning guide the LLM to observe\nsupport images like human; vision models such as Segment Anything Model (SAM)\nand GPT-4Vision assist LLM understand spatial and semantic information at the\nsame time. Ultimately, the LLM uses its summarizing and reasoning capabilities\nto classify and segment the query image. The proposed method's modular\nframework makes it easily extendable. Our approach achieves state-of-the-art\nperformance on the Pascal-5i dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.12065v1.pdf"
    },
    {
        "title": "Visual AI and Linguistic Intelligence Through Steerability and Composability",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-11-18T22:01:33Z",
        "summary": "This study explores the capabilities of multimodal large language models\n(LLMs) in handling challenging multistep tasks that integrate language and\nvision, focusing on model steerability, composability, and the application of\nlong-term memory and context understanding. The problem addressed is the LLM's\nability (Nov 2023 GPT-4 Vision Preview) to manage tasks that require\nsynthesizing visual and textual information, especially where stepwise\ninstructions and sequential logic are paramount. The research presents a series\nof 14 creatively and constructively diverse tasks, ranging from AI Lego\nDesigning to AI Satellite Image Analysis, designed to test the limits of\ncurrent LLMs in contexts that previously proved difficult without extensive\nmemory and contextual understanding. Key findings from evaluating 800 guided\ndialogs include notable disparities in task completion difficulty. For\ninstance, 'Image to Ingredient AI Bartender' (Low difficulty) contrasted\nsharply with 'AI Game Self-Player' (High difficulty), highlighting the LLM's\nvarying proficiency in processing complex visual data and generating coherent\ninstructions. Tasks such as 'AI Genetic Programmer' and 'AI Negotiator' showed\nhigh completion difficulty, emphasizing challenges in maintaining context over\nmultiple steps. The results underscore the importance of developing LLMs that\ncombine long-term memory and contextual awareness to mimic human-like thought\nprocesses in complex problem-solving scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2312.12383v1.pdf"
    },
    {
        "title": "A Principled Framework for Knowledge-enhanced Large Language Model",
        "authors": [
            "Saizhuo Wang",
            "Zhihan Liu",
            "Zhaoran Wang",
            "Jian Guo"
        ],
        "published": "2023-11-18T18:10:02Z",
        "summary": "Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.",
        "pdf_link": "https://arxiv.org/pdf/2311.11135v1.pdf"
    },
    {
        "title": "(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs",
        "authors": [
            "Wanqin Ma",
            "Chenyang Yang",
            "Christian K\u00e4stner"
        ],
        "published": "2023-11-18T17:11:12Z",
        "summary": "Large Language Models (LLMs) are increasingly integrated into software\napplications. Downstream application developers often access LLMs through APIs\nprovided as a service. However, LLM APIs are often updated silently and\nscheduled to be deprecated, forcing users to continuously adapt to evolving\nmodels. This can cause performance regression and affect prompt design choices,\nas evidenced by our case study on toxicity detection. Based on our case study,\nwe emphasize the need for and re-examine the concept of regression testing for\nevolving LLM APIs. We argue that regression testing LLMs requires fundamental\nchanges to traditional testing approaches, due to different correctness\nnotions, prompting brittleness, and non-determinism in LLM APIs.",
        "pdf_link": "https://arxiv.org/pdf/2311.11123v2.pdf"
    },
    {
        "title": "Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers",
        "authors": [
            "Sohini Roychowdhury"
        ],
        "published": "2023-11-18T03:55:59Z",
        "summary": "Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.",
        "pdf_link": "https://arxiv.org/pdf/2311.10961v1.pdf"
    },
    {
        "title": "An Embodied Generalist Agent in 3D World",
        "authors": [
            "Jiangyong Huang",
            "Silong Yong",
            "Xiaojian Ma",
            "Xiongkun Linghu",
            "Puhao Li",
            "Yan Wang",
            "Qing Li",
            "Song-Chun Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "published": "2023-11-18T01:21:38Z",
        "summary": "Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.",
        "pdf_link": "https://arxiv.org/pdf/2311.12871v1.pdf"
    },
    {
        "title": "Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections",
        "authors": [
            "Lihan Zha",
            "Yuchen Cui",
            "Li-Heng Lin",
            "Minae Kwon",
            "Montserrat Gonzalez Arenas",
            "Andy Zeng",
            "Fei Xia",
            "Dorsa Sadigh"
        ],
        "published": "2023-11-17T18:00:20Z",
        "summary": "Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .",
        "pdf_link": "https://arxiv.org/pdf/2311.10678v2.pdf"
    },
    {
        "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest",
        "authors": [
            "Ruohong Zhang",
            "Luyu Gao",
            "Chen Zheng",
            "Zhen Fan",
            "Guokun Lai",
            "Zheng Zhang",
            "Fangzhou Ai",
            "Yiming Yang",
            "Hongxia Yang"
        ],
        "published": "2023-11-17T16:09:10Z",
        "summary": "Large Language Models (LLMs), despite their great power in language\ngeneration, often encounter challenges when dealing with intricate and\nknowledge-demanding queries in specific domains. This paper introduces a novel\napproach to enhance LLMs by effectively extracting the relevant knowledge from\ndomain-specific textual sources, and the adaptive training of a chatbot with\ndomain-specific inquiries. Our two-step approach starts from training a\nknowledge miner, namely LLMiner, which autonomously extracts Question-Answer\npairs from relevant documents through a chain-of-thought reasoning process.\nSubsequently, we blend the mined QA pairs with a conversational dataset to\nfine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise\nand conversational capabilities. We also developed a new evaluation benchmark\nwhich comprises four domain-specific text corpora and associated human-crafted\nQA pairs for testing. Our model shows remarkable performance improvement over\ngenerally aligned LLM and surpasses domain-adapted models directly fine-tuned\non domain corpus. In particular, LLMiner achieves this with minimal human\nintervention, requiring only 600 seed instances, thereby providing a pathway\ntowards self-improvement of LLMs through model-synthesized training data.",
        "pdf_link": "https://arxiv.org/pdf/2311.10614v1.pdf"
    },
    {
        "title": "TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes",
        "authors": [
            "Bibek Upadhayay",
            "Vahid Behzadan"
        ],
        "published": "2023-11-17T06:55:32Z",
        "summary": "Creating multilingual LLMs poses a significant challenge. Pretraining or\nfine-tuning LLMs to adopt new languages is evidently very costly. Furthermore,\nthere exist limitations concerning benchmark datasets and the metrics used to\nmeasure model performance in multilingual settings. This paper proposes\ncost-effective solutions to both aforementioned challenges. Firstly, we\nintroduce the Multilingual Instruction-Tuning Dataset (MITS), comprised of\nAlpaca-52K, Dolly-15K, and Vicuna Benchmark translations into 132 languages.\nSecondly, we propose a new method called \\emph{TaCo: Translation-Assisted\nCross-Linguality}, which utilizes translations in a chain-of-thought process to\ninstruction-tune LLMs on new languages through a curriculum-learning process.\nAs a proof of concept, we experimented with the instruction-tuned Guanaco-33B\nmodel, performing further instruction tuning using our proposed TaCo method in\nthree low-resource languages and one high-resource language. Our results\nindicate that the TaCo method impresses GPT-4 with an 82\\% score for a\nlow-resource language in the Vicuna Benchmark dataset, doubling the performance\nin contrast to instruction tuning alone. Furthermore, TaCo shows promise in\ncreating multilingual LLMs, even for low-resource languages. We have released\nour datasets and model adapters\\footnote{https://github.com/UNHSAILLab/TaCo} ,\nencouraging the research community to utilize these resources to advance work\non multilingual LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10797v2.pdf"
    },
    {
        "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
        "authors": [
            "Yangyi Chen",
            "Karan Sikka",
            "Michael Cogswell",
            "Heng Ji",
            "Ajay Divakaran"
        ],
        "published": "2023-11-16T18:37:29Z",
        "summary": "We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10081v2.pdf"
    },
    {
        "title": "ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond",
        "authors": [
            "Kanhai S. Amin",
            "Linda Mayes",
            "Pavan Khosla",
            "Rushabh Doshi"
        ],
        "published": "2023-11-16T18:30:14Z",
        "summary": "Purpose: Enhanced health literacy has been linked to better health outcomes;\nhowever, few interventions have been studied. We investigate whether large\nlanguage models (LLMs) can serve as a medium to improve health literacy in\nchildren and other populations.\n  Methods: We ran 288 conditions using 26 different prompts through\nChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate\nlimits, we tested a subset of 150 conditions through ChatGPT-4. The primary\noutcome measurements were the reading grade level (RGL) and word counts of\noutput.\n  Results: Across all models, output for basic prompts such as \"Explain\" and\n\"What is (are)\" were at, or exceeded, a 10th-grade RGL. When prompts were\nspecified to explain conditions from the 1st to 12th RGL, we found that LLMs\nhad varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided\nresponses that ranged from the 7th-grade to college freshmen RGL while\nChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.\nMicrosoft Bing provided responses from the 9th to 11th RGL while Google Bard\nprovided responses from the 7th to 10th RGL.\n  Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade\nlevel outputs. Meanwhile Bard and Bing tended to consistently produce an RGL\nthat is at the high school level regardless of prompt. Additionally, Bard's\nhesitancy in providing certain outputs indicates a cautious approach towards\nhealth information. LLMs demonstrate promise in enhancing health communication,\nbut future research should verify the accuracy and effectiveness of such tools\nin this context.\n  Implications: LLMs face challenges in crafting outputs below a sixth-grade\nreading level. However, their capability to modify outputs above this threshold\nprovides a potential mechanism to improve health literacy and communication in\na pediatric population and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2311.10075v1.pdf"
    },
    {
        "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
        "authors": [
            "Yao Qiang",
            "Xiangyu Zhou",
            "Dongxiao Zhu"
        ],
        "published": "2023-11-16T15:01:48Z",
        "summary": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09948v1.pdf"
    },
    {
        "title": "Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models",
        "authors": [
            "Debarati Das",
            "Ishaan Gupta",
            "Jaideep Srivastava",
            "Dongyeop Kang"
        ],
        "published": "2023-11-16T12:45:41Z",
        "summary": "Our research integrates graph data with Large Language Models (LLMs), which,\ndespite their advancements in various fields using large text corpora, face\nlimitations in encoding entire graphs due to context size constraints. This\npaper introduces a new approach to encoding a graph with diverse modalities,\nsuch as text, image, and motif, coupled with prompts to approximate a graph's\nglobal connectivity, thereby enhancing LLMs' efficiency in processing complex\ngraph structures. The study also presents GraphTMI, a novel benchmark for\nevaluating LLMs in graph structure analysis, focusing on homophily, motif\npresence, and graph difficulty. Key findings indicate that the image modality,\nespecially with vision-language models like GPT-4V, is superior to text in\nbalancing token limits and preserving essential information and outperforms\nprior graph neural net (GNN) encoders. Furthermore, the research assesses how\nvarious factors affect the performance of each encoding modality and outlines\nthe existing challenges and potential future developments for LLMs in graph\nunderstanding and reasoning tasks. All data will be publicly available upon\nacceptance.",
        "pdf_link": "https://arxiv.org/pdf/2311.09862v2.pdf"
    },
    {
        "title": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering",
        "authors": [
            "Tilahun Abedissa Taffa",
            "Ricardo Usbeck"
        ],
        "published": "2023-11-16T12:13:49Z",
        "summary": "This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09841v1.pdf"
    },
    {
        "title": "ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks",
        "authors": [
            "Yuliang Liu",
            "Xiangru Tang",
            "Zefan Cai",
            "Junjie Lu",
            "Yichi Zhang",
            "Yanjun Shao",
            "Zexuan Deng",
            "Helan Hu",
            "Zengxian Yang",
            "Kaikai An",
            "Ruijun Huang",
            "Shuzheng Si",
            "Sheng Chen",
            "Haozhe Zhao",
            "Zhengliang Li",
            "Liang Chen",
            "Yiming Zong",
            "Yan Wang",
            "Tianyu Liu",
            "Zhiwei Jiang",
            "Baobao Chang",
            "Yujia Qin",
            "Wangchunshu Zhou",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-11-16T12:03:21Z",
        "summary": "Large language models have shown promising performance in code generation\nbenchmarks. However, a considerable divide exists between these benchmark\nachievements and their practical applicability, primarily attributed to\nreal-world programming's reliance on pre-existing libraries. Instead of\nevaluating LLMs to code from scratch, this work aims to propose a new\nevaluation setup where LLMs use open-source libraries to finish machine\nlearning tasks. Therefore, we propose ML-Bench, an expansive benchmark\ndeveloped to assess the effectiveness of LLMs in leveraging existing functions\nin open-source libraries. Consisting of 10044 samples spanning 130 tasks over\n14 notable machine learning GitHub repositories. In this setting, given a\nspecific machine learning task instruction and the accompanying README in a\ncodebase, an LLM is tasked to generate code to accomplish the task. This\nnecessitates the comprehension of long and language-code interleaved documents,\nas well as the understanding of complex cross-file code structures, introducing\nnew challenges. Notably, while GPT-4 exhibits remarkable improvement over other\nLLMs, it manages to accomplish only 39.73\\% of the tasks, leaving a huge space\nfor improvement. We address these challenges by proposing ML-Agent, designed to\neffectively navigate the codebase, locate documentation, retrieve code, and\ngenerate executable code. Empirical results demonstrate that ML-Agent, built\nupon GPT-4, results in further improvements. Code, data, and models are\navailable at \\url{https://ml-bench.github.io/}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09835v1.pdf"
    },
    {
        "title": "FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models",
        "authors": [
            "Yimin Jing",
            "Renren Jin",
            "Jiahao Hu",
            "Huishi Qiu",
            "Xiaohua Wang",
            "Peng Wang",
            "Deyi Xiong"
        ],
        "published": "2023-11-16T11:53:31Z",
        "summary": "The effective assessment of the instruction-following ability of large\nlanguage models (LLMs) is of paramount importance. A model that cannot adhere\nto human instructions might be not able to provide reliable and helpful\nresponses. In pursuit of this goal, various benchmarks have been constructed to\nevaluate the instruction-following capacity of these models. However, these\nbenchmarks are limited to a single language and are constructed using automated\napproaches, which restricts their applicability and the quality of the test\nexamples they contain. To bridge this gap, we introduce the FollowEval\nbenchmark in this paper. This benchmark is composed of instances in both\nEnglish and Chinese, and all test examples are crafted by human experts.\nFurthermore, the FollowEval benchmark is designed to assess LLMs across five\ncritical dimensions of instruction following: string manipulation, commonsense\nreasoning, logical reasoning, spatial reasoning, and response constraints. To\nenhance the complexity and present a sufficient challenge, each test example is\ndesigned to evaluate more than one dimension. We have evaluated various LLMs\nusing the FollowEval benchmark and found that their performance significantly\nlags behind that of humans. This highlights the considerable room for\nimprovement in the instruction-following ability of these models.",
        "pdf_link": "https://arxiv.org/pdf/2311.09829v1.pdf"
    },
    {
        "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
        "authors": [
            "Nan Xu",
            "Fei Wang",
            "Ben Zhou",
            "Bang Zheng Li",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-11-16T11:52:22Z",
        "summary": "While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.",
        "pdf_link": "https://arxiv.org/pdf/2311.09827v2.pdf"
    },
    {
        "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Anni Zou",
            "Zhuosheng Zhang",
            "Ziming Li",
            "Yilun Zhao",
            "Xingyao Zhang",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-11-16T11:47:58Z",
        "summary": "Large language models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and reasoning over specialized knowledge. To address these\nissues, we propose a novel Multi-disciplinary Collaboration (MC) framework for\nthe medical domain that leverages LLM-based agents in a role-playing setting\nthat participate in a collaborative multi-round discussion, thereby enhancing\nLLM proficiency and reasoning capabilities. This training-free framework\nencompasses five critical steps: gathering domain experts, proposing individual\nanalyses, summarising these analyses into a report, iterating over discussions\nuntil a consensus is reached, and ultimately making a decision. Our work\nfocuses on the zero-shot setting, which is applicable in real-world scenarios.\nExperimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six\nsubtasks from MMLU) establish that our proposed MC framework excels at mining\nand harnessing the medical expertise within LLMs, as well as extending its\nreasoning abilities. Our code can be found at\n\\url{https://github.com/gersteinlab/MedAgents}.",
        "pdf_link": "https://arxiv.org/pdf/2311.10537v3.pdf"
    },
    {
        "title": "The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text",
        "authors": [
            "Yanzhu Guo",
            "Guokan Shang",
            "Michalis Vazirgiannis",
            "Chlo\u00e9 Clavel"
        ],
        "published": "2023-11-16T11:31:50Z",
        "summary": "This study investigates the consequences of training large language models\n(LLMs) on synthetic data generated by their predecessors, an increasingly\nprevalent practice aimed at addressing the limited supply of human-generated\ntraining data. Diverging from the usual emphasis on performance metrics, we\nfocus on the impact of this training methodology on linguistic diversity,\nespecially when conducted recursively over time. To assess this, we developed a\nset of novel metrics targeting lexical, syntactic, and semantic diversity,\napplying them in recursive fine-tuning experiments across various natural\nlanguage generation tasks. Our findings reveal a marked decrease in the\ndiversity of the models' outputs through successive iterations. This trend\nunderscores the potential risks of training LLMs on predecessor-generated text,\nparticularly concerning the preservation of linguistic richness. Our study\nhighlights the need for careful consideration of the long-term effects of such\ntraining approaches on the linguistic capabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09807v1.pdf"
    },
    {
        "title": "DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Yitao Long",
            "Hongjun Liu",
            "Linyong Nan",
            "Lyuhao Chen",
            "Ryo Kamoi",
            "Yixin Liu",
            "Xiangru Tang",
            "Rui Zhang",
            "Arman Cohan"
        ],
        "published": "2023-11-16T11:30:53Z",
        "summary": "Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning and problem-solving capabilities of LLMs in the context of\nunderstanding and analyzing financial documents containing both text and\ntables. We evaluate a wide spectrum of 19 LLMs, including those specialized in\ncoding and finance. We also incorporate different prompting strategies (i.e.,\nChain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the\ncapabilities and limitations of existing LLMs in DocMath-Eval. We found that,\nalthough the current best-performing system (i.e., GPT-4), can perform well on\nsimple problems such as calculating the rate of increase in a financial metric\nwithin a short document context, it significantly lags behind human experts in\nmore complex problems grounded in longer contexts. We believe DocMath-Eval can\nbe used as a valuable benchmark to evaluate LLMs' capabilities to solve\nchallenging numerical reasoning problems in expert domains. We will release the\nbenchmark and code at https://github.com/yale-nlp/DocMath-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2311.09805v1.pdf"
    },
    {
        "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
        "authors": [
            "Sen Yang",
            "Xin Li",
            "Leyang Cui",
            "Lidong Bing",
            "Wai Lam"
        ],
        "published": "2023-11-16T11:26:21Z",
        "summary": "Though prompting LLMs with various reasoning structures produces reasoning\nproofs along with answers, these proofs are not ensured to be causal and\nreliable due to the inherent defects of LLMs. Tracking such deficiencies, we\npresent a neuro-symbolic integration method, in which a neural LLM is used to\nrepresent the knowledge of the problem while an LLM-free symbolic solver is\nadopted to do deliberative reasoning using the knowledge. Specifically, our\ncustomized meta-interpreters allow the production of reasoning proofs and\nsupport flexible search strategies. These reasoning proofs are ensured to be\ncausal and reliable because of the deterministic executing nature of the\nsymbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT\nbaseline by nearly double in accuracy and more than triple in proof similarity.\nOn GSM8K, our method also shows accuracy improvements and nearly doubled proof\nsimilarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
        "pdf_link": "https://arxiv.org/pdf/2311.09802v1.pdf"
    },
    {
        "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
        "authors": [
            "Shirley Anugrah Hayati",
            "Minhwa Lee",
            "Dheeraj Rajagopal",
            "Dongyeop Kang"
        ],
        "published": "2023-11-16T11:23:38Z",
        "summary": "Collecting diverse human opinions is costly and challenging. This leads to a\nrecent trend in collaborative efforts between humans and Large Language Models\n(LLMs) for generating diverse data, offering potential scalable and efficient\nsolutions. However, the extent of LLMs' capability to generate diverse\nperspectives on subjective topics remains an unexplored question. In this\nstudy, we investigate LLMs' capacity for generating diverse perspectives and\nrationales on subjective topics, such as social norms and argumentative texts.\nWe formulate a new problem of maximum diversity extraction from LLMs. Motivated\nby how humans develop their opinions through their values, we propose a\ncriteria-based prompting technique to ground diverse opinions. To see how far\nwe can extract diverse perspectives from LLMs, or called diversity coverage, we\nemploy a step-by-step recall prompting for generating more outputs from the\nmodel in an iterative manner. As we apply our methods to various tasks, indeed\nwe find that LLMs can generate diverse opinions according to the degree of task\nsubjectivity",
        "pdf_link": "https://arxiv.org/pdf/2311.09799v2.pdf"
    },
    {
        "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions",
        "authors": [
            "Nikita Moghe",
            "Patrick Xia",
            "Jacob Andreas",
            "Jason Eisner",
            "Benjamin Van Durme",
            "Harsh Jhamtani"
        ],
        "published": "2023-11-16T11:19:26Z",
        "summary": "Users of natural language interfaces, generally powered by Large Language\nModels (LLMs),often must repeat their preferences each time they make a similar\nrequest. We describe an approach to LLM-based dialogue modeling in which\npersistent user constraints and preferences -- collectively termed standing\ninstructions -- as additional context for such interfaces. For example, when a\nuser states \"I'm hungry\", a previously expressed preference for Persian food\ncan be automatically added to the LLM prompt, influencing the search for\nrelevant restaurants. We develop NLSI, a language-to-program dataset consisting\nof over 2.4K dialogues spanning 17 domains, where each dialogue is paired with\na user profile (a set of users specific standing instructions) and\ncorresponding structured representations (API calls). A key challenge in NLSI\nis to identify which subset of the standing instructions is applicable to a\ngiven dialogue. NLSI contains diverse phenomena, from simple preferences to\ninterdependent instructions such as triggering a hotel search whenever the user\nis booking tickets to an event. We conduct experiments on NLSI using prompting\nwith large language models and various retrieval approaches, achieving a\nmaximum of 44.7% exact match on API prediction. Our results demonstrate the\nchallenges in identifying the relevant standing instructions and their\ninterpretation into API calls.",
        "pdf_link": "https://arxiv.org/pdf/2311.09796v2.pdf"
    },
    {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "authors": [
            "Chunyuan Deng",
            "Yilun Zhao",
            "Xiangru Tang",
            "Mark Gerstein",
            "Arman Cohan"
        ],
        "published": "2023-11-16T11:03:04Z",
        "summary": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "pdf_link": "https://arxiv.org/pdf/2311.09783v2.pdf"
    },
    {
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
            "Bin Lin",
            "Yang Ye",
            "Bin Zhu",
            "Jiaxi Cui",
            "Munan Ning",
            "Peng Jin",
            "Li Yuan"
        ],
        "published": "2023-11-16T10:59:44Z",
        "summary": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.10122v2.pdf"
    },
    {
        "title": "Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations",
        "authors": [
            "Wenjie Mo",
            "Jiashu Xu",
            "Qin Liu",
            "Jiongxiao Wang",
            "Jun Yan",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-11-16T10:38:43Z",
        "summary": "Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes particularly pronounced in the context of Large Language Models\n(LLMs) deployed as Web Services, which typically offer only black-box access,\nrendering training-time defenses impractical. To bridge this gap, our work\nintroduces defensive demonstrations, an innovative backdoor defense strategy\nfor blackbox large language models. Our method involves identifying the task\nand retrieving task-relevant demonstrations from an uncontaminated pool. These\ndemonstrations are then combined with user queries and presented to the model\nduring testing, without requiring any modifications/tuning to the black-box\nmodel or insights into its internal mechanisms. Defensive demonstrations are\ndesigned to counteract the adverse effects of triggers, aiming to recalibrate\nand correct the behavior of poisoned models during test-time evaluations.\nExtensive experiments show that defensive demonstrations are effective in\ndefending both instance-level and instruction-level backdoor attacks, not only\nrectifying the behavior of poisoned models but also surpassing existing\nbaselines in most scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2311.09763v1.pdf"
    },
    {
        "title": "Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models",
        "authors": [
            "Jinyoung Park",
            "Ameen Patel",
            "Omar Zia Khan",
            "Hyunwoo J. Kim",
            "Joo-Kyung Kim"
        ],
        "published": "2023-11-16T10:36:08Z",
        "summary": "Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning\ncapabilities of Large Language Models (LLMs) by generating a series of\nrationales before the final answer. We analyze the reasoning paths generated by\nCoT and find two issues in multi-step reasoning: (i) Generating rationales\nirrelevant to the question, (ii) Unable to compose subquestions or queries for\ngenerating/retrieving all the relevant information. To address them, we propose\na graph-guided CoT prompting method, which guides the LLMs to reach the correct\nanswer with graph representation/verification steps. Specifically, we first\nleverage LLMs to construct a \"question/rationale graph\" by using knowledge\nextraction prompting given the initial question and the rationales generated in\nthe previous steps. Then, the graph verification step diagnoses the current\nrationale triplet by comparing it with the existing question/rationale graph to\nfilter out irrelevant rationales and generate follow-up questions to obtain\nrelevant information. Additionally, we generate CoT paths that exclude the\nextracted graph information to represent the context information missed from\nthe graph extraction. Our graph-guided reasoning method shows superior\nperformance compared to previous CoT prompting and the variants on multi-hop\nquestion answering benchmark datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.09762v1.pdf"
    },
    {
        "title": "How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?",
        "authors": [
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2023-11-16T10:30:00Z",
        "summary": "Pruning and quantization form the foundation of model compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nstate-of-the-art performance in a post-training setting. They rely upon\ncalibration data, a small set of unlabeled examples, to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of pruning and\nquantization methods, tasks, models, and datasets. Surprisingly, we find\nsubstantial variations in downstream task performance, contrasting existing\nwork that suggests a greater level of robustness to the calibration data.\nFinally, we make a series of recommendations for the effective use of\ncalibration data in LLM quantization and pruning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09755v1.pdf"
    },
    {
        "title": "GEO: Generative Engine Optimization",
        "authors": [
            "Pranjal Aggarwal",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Karthik R Narasimhan",
            "Ameet Deshpande"
        ],
        "published": "2023-11-16T10:06:09Z",
        "summary": "The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of Generative Engines (GEs), has the potential to generate\naccurate and personalized responses, and is rapidly replacing traditional\nsearch engines like Google and Bing. Generative Engines typically satisfy\nqueries by synthesizing information from multiple sources and summarizing them\nwith the help of LLMs. While this shift significantly improves \\textit{user}\nutility and \\textit{generative search engine} traffic, it results in a huge\nchallenge for the third stakeholder -- website and content creators. Given the\nblack-box and fast-moving nature of Generative Engines, content creators have\nlittle to no control over when and how their content is displayed. With\ngenerative engines here to stay, the right tools should be provided to ensure\nthat creator economy is not severely disadvantaged. To address this, we\nintroduce Generative Engine Optimization (GEO), a novel paradigm to aid content\ncreators in improving the visibility of their content in Generative Engine\nresponses through a black-box optimization framework for optimizing and\ndefining visibility metrics. We facilitate systematic evaluation in this new\nparadigm by introducing GEO-bench, a benchmark of diverse user queries across\nmultiple domains, coupled with sources required to answer these queries.\nThrough rigorous evaluation, we show that GEO can boost visibility by up to\n40\\% in generative engine responses. Moreover, we show the efficacy of these\nstrategies varies across domains, underscoring the need for domain-specific\nmethods. Our work opens a new frontier in the field of information discovery\nsystems, with profound implications for generative engines and content\ncreators.",
        "pdf_link": "https://arxiv.org/pdf/2311.09735v1.pdf"
    },
    {
        "title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction",
        "authors": [
            "Xinliang Frederick Zhang",
            "Winston Wu",
            "Nick Beauchamp",
            "Lu Wang"
        ],
        "published": "2023-11-16T10:04:49Z",
        "summary": "News media employ moral language to create memorable stories, and readers\noften engage with the content that align with their values. Moral theories have\nbeen applied to news analysis studying moral values in isolation, while the\nintricate dynamics among participating entities in shaping moral events have\nbeen overlooked. This is mainly due to the use of obscure language to conceal\nevident ideology and values, coupled with the insufficient moral reasoning\ncapability in most existing NLP systems, where LLMs are no exception. To study\nthis phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of\n5,494 structured annotations on 474 news articles by diverse US media across\nthe political spectrum. We further propose MOKA, a moral event extraction\nframework with MOral Knowledge Augmentation, that leverages knowledge derived\nfrom moral words and moral scenarios. Experimental results show that MOKA\noutperforms competitive baselines across three moral event understanding tasks.\nFurther analyses illuminate the selective reporting of moral events by media\noutlets of different ideological leanings, suggesting the significance of\nevent-level morality analysis in news. Our datasets and codebase are available\nat https://github.com/launchnlp/MOKA.",
        "pdf_link": "https://arxiv.org/pdf/2311.09733v1.pdf"
    },
    {
        "title": "Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge",
        "authors": [
            "Genglin Liu",
            "Xingyao Wang",
            "Lifan Yuan",
            "Yangyi Chen",
            "Hao Peng"
        ],
        "published": "2023-11-16T10:02:40Z",
        "summary": "Can large language models (LLMs) express their uncertainty in situations\nwhere they lack sufficient parametric knowledge to generate reasonable\nresponses? This work aims to systematically investigate LLMs' behaviors in such\nsituations, emphasizing the trade-off between honesty and helpfulness. To\ntackle the challenge of precisely determining LLMs' knowledge gaps, we\ndiagnostically create unanswerable questions containing non-existent concepts\nor false premises, ensuring that they are outside the LLMs' vast training data.\nBy compiling a benchmark, UnknownBench, which consists of both unanswerable and\nanswerable questions, we quantitatively evaluate the LLMs' performance in\nmaintaining honesty while being helpful. Using a model-agnostic unified\nconfidence elicitation approach, we observe that most LLMs fail to consistently\nrefuse or express uncertainty towards questions outside their parametric\nknowledge, although instruction fine-tuning and alignment techniques can\nprovide marginal enhancements. Moreover, LLMs' uncertainty expression does not\nalways stay consistent with the perceived confidence of their textual outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09731v2.pdf"
    },
    {
        "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
        "authors": [
            "Huaman Sun",
            "Jiaxin Pei",
            "Minje Choi",
            "David Jurgens"
        ],
        "published": "2023-11-16T10:02:24Z",
        "summary": "Human perception of language depends on personal backgrounds like gender and\nethnicity. While existing studies have shown that large language models (LLMs)\nhold values that are closer to certain societal groups, it is unclear whether\ntheir prediction behaviors on subjective NLP tasks also exhibit a similar bias.\nIn this study, leveraging the POPQUORN dataset which contains annotations of\ndiverse demographic backgrounds, we conduct a series of experiments on four\npopular LLMs to investigate their capability to understand group differences\nand potential biases in their predictions for politeness and offensiveness. We\nfind that for both tasks, model predictions are closer to the labels from White\nand female participants. We further explore prompting with the target\ndemographic labels and show that including the target demographic in the prompt\nactually worsens the model's performance. More specifically, when being\nprompted to respond from the perspective of \"Black\" and \"Asian\" individuals,\nmodels show lower performance in predicting both overall scores as well as the\nscores from corresponding groups. Our results suggest that LLMs hold gender and\nracial biases for subjective NLP tasks and that demographic-infused prompts\nalone may be insufficient to mitigate such effects. Code and data are available\nat https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
        "pdf_link": "https://arxiv.org/pdf/2311.09730v1.pdf"
    },
    {
        "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",
        "authors": [
            "Fei Yu",
            "Anningzhe Gao",
            "Benyou Wang"
        ],
        "published": "2023-11-16T09:56:28Z",
        "summary": "Large language models (LLMs) often struggle with maintaining accuracy\nthroughout multiple multiple reasoning steps, especially in mathematical\nreasoning where an error in earlier steps can propagate to subsequent ones and\nit ultimately leading to an incorrect answer. To reduce error propagation,\nguided decoding is employed to direct the LM decoding on a step-by-step basis.\nWe argue that in guided decoding, assessing the potential of an incomplete\nreasoning path can be more advantageous than simply ensuring per-step\ncorrectness, as the former approach leads towards a correct final answer. This\ntransforms the task into a $\\textit{value estimation}$ problem in planning.\n  Inspired by the findings that $\\textit{outcome supervision for guided\ndecoding essentially acts as a value model}$, we propose Outcome-supervised\nValue Model (OVM) that employs outcome supervision for training a value model,\nwhich prioritizes steps that lead to accurate conclusions. Furthermore, the OVM\neliminates the need for labor-intensive annotations of step-level correctness,\nthereby significantly enhancing its scalability. Our experiments on two\nmulti-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate\nthe superior performance of the OVM model. Notably, in GSM8K, our\n$\\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B\nparameters}$; especially it does not utilize GPT-4 or code execution. These\nfindings offer a novel perspective on the role of outcome supervision in\ntraining value models for multi-step reasoning tasks and provide theoretical\njustification for its advantage in value estimation for guided decoding.",
        "pdf_link": "https://arxiv.org/pdf/2311.09724v2.pdf"
    },
    {
        "title": "Large Language Model Inference with Lexical Shortlisting",
        "authors": [
            "Nikolay Bogoychev",
            "Pinzhen Chen",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "published": "2023-11-16T09:35:50Z",
        "summary": "Large language model (LLM) inference is computation and memory intensive, so\nwe adapt lexical shortlisting to it hoping to improve both. While lexical\nshortlisting is well-explored in tasks like machine translation, it requires\nmodifications before being suitable for LLMs as the intended applications vary\nsignificantly. Our work studies two heuristics to shortlist sub-vocabulary at\nLLM inference time: Unicode-based script filtering and corpus-based selection.\nWe explore different LLM families and sizes, and we find that lexical\nshortlisting can reduce the memory usage of some models by nearly 50\\% and has\nan upper bound of 25\\% improvement in generation speed. In this pilot study, we\nalso identify the drawbacks of such vocabulary selection methods and propose\navenues for future research.",
        "pdf_link": "https://arxiv.org/pdf/2311.09709v1.pdf"
    },
    {
        "title": "Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance",
        "authors": [
            "Shiro Takagi",
            "Ryutaro Yamauchi",
            "Wataru Kumagai"
        ],
        "published": "2023-11-16T09:34:23Z",
        "summary": "Research automation efforts usually employ AI as a tool to automate specific\ntasks within the research process. To create an AI that truly conduct research\nthemselves, it must independently generate hypotheses, design verification\nplans, and execute verification. Therefore, we investigated if an AI itself\ncould autonomously generate and verify hypothesis for a toy machine learning\nresearch problem. We prompted GPT-4 to generate hypotheses and Python code for\nhypothesis verification with limited methodological guidance. Our findings\nsuggest that, in some instances, GPT-4 can autonomously generate and validate\nhypotheses without detailed guidance. While this is a promising result, we also\nfound that none of the verifications were flawless, and there remain\nsignificant challenges in achieving autonomous, human-level research using only\ngeneric instructions. These findings underscore the need for continued\nexploration to develop a general and autonomous AI researcher.",
        "pdf_link": "https://arxiv.org/pdf/2311.09706v1.pdf"
    },
    {
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
        "authors": [
            "Bangzheng Li",
            "Ben Zhou",
            "Fei Wang",
            "Xingyu Fu",
            "Dan Roth",
            "Muhao Chen"
        ],
        "published": "2023-11-16T09:27:36Z",
        "summary": "Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09702v3.pdf"
    },
    {
        "title": "BLT: Can Large Language Models Handle Basic Legal Text?",
        "authors": [
            "Andrew Blair-Stanek",
            "Nils Holzenberger",
            "Benjamin Van Durme"
        ],
        "published": "2023-11-16T09:09:22Z",
        "summary": "We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM\n2} currently perform poorly at basic legal text handling. We introduce a\nbenchmark consisting of tasks that lawyers and paralegals would expect LLMs to\nhandle zero-shot, such as looking up the text at a line of a witness deposition\nor at a subsection of a contract. LLMs' poor performance on this benchmark\ncasts into doubt their reliability as-is for legal practice. However,\nfine-tuning for these tasks brings even a smaller model to near-perfect\nperformance on our test set and also raises performance on a related legal\ntask. These results suggest that many simple behaviors needed for a domain may\nnot be present in foundational LLMs, without additional engagement from subject\nmatter experts.",
        "pdf_link": "https://arxiv.org/pdf/2311.09693v2.pdf"
    },
    {
        "title": "Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies",
        "authors": [
            "Zihao He",
            "Siyi Guo",
            "Ashwin Rao",
            "Kristina Lerman"
        ],
        "published": "2023-11-16T08:57:53Z",
        "summary": "Social media platforms are rife with politically charged discussions.\nTherefore, accurately deciphering and predicting partisan biases using Large\nLanguage Models (LLMs) is increasingly critical. In this study, we address the\nchallenge of understanding political bias in digitized discourse using LLMs.\nWhile traditional approaches often rely on finetuning separate models for each\npolitical faction, our work innovates by employing a singular,\ninstruction-tuned LLM to reflect a spectrum of political ideologies. We present\na comprehensive analytical framework, consisting of Partisan Bias Divergence\nAssessment and Partisan Class Tendency Prediction, to evaluate the model's\nalignment with real-world political ideologies in terms of stances, emotions,\nand moral foundations. Our findings reveal the model's effectiveness in\ncapturing emotional and moral nuances, albeit with some challenges in stance\ndetection, highlighting the intricacies and potential for refinement in NLP\ntools for politically sensitive contexts. This research contributes\nsignificantly to the field by demonstrating the feasibility and importance of\nnuanced political understanding in LLMs, particularly for applications\nrequiring acute awareness of political bias.",
        "pdf_link": "https://arxiv.org/pdf/2311.09687v1.pdf"
    },
    {
        "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions",
        "authors": [
            "Hanning Zhang",
            "Shizhe Diao",
            "Yong Lin",
            "Yi R. Fung",
            "Qing Lian",
            "Xingyao Wang",
            "Yangyi Chen",
            "Heng Ji",
            "Tong Zhang"
        ],
        "published": "2023-11-16T08:45:44Z",
        "summary": "Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the knowledge gap between parametric knowledge and the\ninstruction tuning data. Then, we construct the refusal-aware data based on the\nknowledge intersection, to tune LLMs to refrain from responding to questions\nbeyond its parametric knowledge. Experimental results demonstrate this new\ninstruction tuning approach effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty during training displays a\nbetter ability to estimate uncertainty than uncertainty-based testing. Our code\nwill be released at https://github.com/shizhediao/R-Tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09677v1.pdf"
    },
    {
        "title": "Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring",
        "authors": [
            "Yuhang Li",
            "Yihan Wang",
            "Zhouxing Shi",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-11-16T08:36:00Z",
        "summary": "The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.",
        "pdf_link": "https://arxiv.org/pdf/2311.09668v1.pdf"
    },
    {
        "title": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents",
        "authors": [
            "Yun-Shiuan Chuang",
            "Siddharth Suresh",
            "Nikunj Harlalka",
            "Agam Goyal",
            "Robert Hawkins",
            "Sijia Yang",
            "Dhavan Shah",
            "Junjie Hu",
            "Timothy T. Rogers"
        ],
        "published": "2023-11-16T08:30:15Z",
        "summary": "Human groups are able to converge on more accurate beliefs through\ndeliberation, even in the presence of polarization and partisan bias -- a\nphenomenon known as the \"wisdom of partisan crowds.\" Generated agents powered\nby Large Language Models (LLMs) are increasingly used to simulate human\ncollective behavior, yet few benchmarks exist for evaluating their dynamics\nagainst the behavior of human groups. In this paper, we examine the extent to\nwhich the wisdom of partisan crowds emerges in groups of LLM-based agents that\nare prompted to role-play as partisan personas (e.g., Democrat or Republican).\nWe find that they not only display human-like partisan biases, but also\nconverge to more accurate beliefs through deliberation as humans do. We then\nidentify several factors that interfere with convergence, including the use of\nchain-of-thought prompt and lack of details in personas. Conversely,\nfine-tuning on human data appears to enhance convergence. These findings show\nthe potential and limitations of LLM-based agents as a model of human\ncollective intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2311.09665v2.pdf"
    },
    {
        "title": "Structured Chemistry Reasoning with Large Language Models",
        "authors": [
            "Siru Ouyang",
            "Zhuosheng Zhang",
            "Bing Yan",
            "Xuan Liu",
            "Yejin Choi",
            "Jiawei Han",
            "Lianhui Qin"
        ],
        "published": "2023-11-16T08:20:36Z",
        "summary": "Large Language Models (LLMs) excel in diverse areas, yet struggle with\ncomplex scientific reasoning, especially in the field of chemistry. Different\nfrom the simple chemistry tasks (e.g., molecule classification) addressed in\nprevious studies, complex chemistry problems require not only vast knowledge\nand precise calculation, but also compositional reasoning about rich dynamic\ninteractions of different concepts (e.g., temperature changes). Our study shows\nthat even advanced LLMs, like GPT-4, can fail easily in different ways.\nInterestingly, the errors often stem not from a lack of domain knowledge within\nthe LLMs, but rather from the absence of an effective reasoning structure that\nguides the LLMs to elicit the right knowledge, incorporate the knowledge in\nstep-by-step reasoning, and iteratively refine results for further improved\nquality. On this basis, we introduce StructChem, a simple yet effective\nprompting strategy that offers the desired guidance and substantially boosts\nthe LLMs' chemical reasoning capability. Testing across four chemistry areas --\nquantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem\nsubstantially enhances GPT-4's performance, with up to 30\\% peak improvement.\nOur analysis also underscores the unique difficulties of precise grounded\nreasoning in science with LLMs, highlighting a need for more research in this\narea. Code is available at \\url{https://github.com/ozyyshr/StructChem}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09656v2.pdf"
    },
    {
        "title": "On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models",
        "authors": [
            "Jiongxiao Wang",
            "Junlin Wu",
            "Muhao Chen",
            "Yevgeniy Vorobeychik",
            "Chaowei Xiao"
        ],
        "published": "2023-11-16T07:48:45Z",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09641v1.pdf"
    },
    {
        "title": "Automatic Engineering of Long Prompts",
        "authors": [
            "Cho-Jui Hsieh",
            "Si Si",
            "Felix X. Yu",
            "Inderjit S. Dhillon"
        ],
        "published": "2023-11-16T07:42:46Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex open-domain tasks, guided by comprehensive instructions and\ndemonstrations provided in the form of prompts. However, these prompts can be\nlengthy, often comprising hundreds of lines and thousands of tokens, and their\ndesign often requires considerable human effort. Recent research has explored\nautomatic prompt engineering for short prompts, typically consisting of one or\na few sentences. However, the automatic design of long prompts remains a\nchallenging problem due to its immense search space. In this paper, we\ninvestigate the performance of greedy algorithms and genetic algorithms for\nautomatic long prompt engineering. We demonstrate that a simple greedy approach\nwith beam search outperforms other methods in terms of search efficiency.\nMoreover, we introduce two novel techniques that utilize search history to\nenhance the effectiveness of LLM-based mutation in our search algorithm. Our\nresults show that the proposed automatic long prompt engineering algorithm\nachieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,\nhighlighting the significance of automating prompt designs to fully harness the\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10117v1.pdf"
    },
    {
        "title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework",
        "authors": [
            "Matthew Pisano",
            "Peter Ly",
            "Abraham Sanders",
            "Bingsheng Yao",
            "Dakuo Wang",
            "Tomek Strzalkowski",
            "Mei Si"
        ],
        "published": "2023-11-16T07:31:18Z",
        "summary": "Research into AI alignment has grown considerably since the recent\nintroduction of increasingly capable Large Language Models (LLMs).\nUnfortunately, modern methods of alignment still fail to fully prevent harmful\nresponses when models are deliberately attacked. These attacks can trick\nseemingly aligned models into giving manufacturing instructions for dangerous\nmaterials, inciting violence, or recommending other immoral acts. To help\nmitigate this issue, we introduce Bergeron: a framework designed to improve the\nrobustness of LLMs against attacks without any additional parameter\nfine-tuning. Bergeron is organized into two tiers; with a secondary LLM\nemulating the conscience of a protected, primary LLM. This framework better\nsafeguards the primary model against incoming attacks while monitoring its\noutput for any harmful content. Empirical analysis shows that, by using\nBergeron to complement models with existing alignment training, we can improve\nthe robustness and safety of multiple, commonly used commercial and open-source\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.00029v2.pdf"
    },
    {
        "title": "CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model",
        "authors": [
            "Nakyeong Yang",
            "Taegwan Kang",
            "Kyomin Jung"
        ],
        "published": "2023-11-16T07:16:55Z",
        "summary": "Large language models (LLMs) executing tasks through instruction-based\nprompts often face challenges stemming from distribution differences between\nuser instructions and training instructions. This leads to distractions and\nbiases, especially when dealing with inconsistent dynamic labels. In this\npaper, we introduces a novel bias mitigation method, CRISPR, designed to\nalleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods\nto identify bias neurons influencing biased outputs and employs pruning to\neliminate the bias neurons. Experimental results demonstrate the method's\neffectiveness in mitigating biases in instruction-based prompting, enhancing\nlanguage model performance on social bias benchmarks without compromising\npre-existing knowledge. CRISPR proves highly practical, model-agnostic,\noffering flexibility in adapting to evolving social biases.",
        "pdf_link": "https://arxiv.org/pdf/2311.09627v1.pdf"
    },
    {
        "title": "Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations",
        "authors": [
            "Jing Yao",
            "Wei Xu",
            "Jianxun Lian",
            "Xiting Wang",
            "Xiaoyuan Yi",
            "Xing Xie"
        ],
        "published": "2023-11-16T07:09:38Z",
        "summary": "The significant progress of large language models (LLMs) provides a promising\nopportunity to build human-like systems for various practical applications.\nHowever, when applied to specific task domains, an LLM pre-trained on a\ngeneral-purpose corpus may exhibit a deficit or inadequacy in two types of\ndomain-specific knowledge. One is a comprehensive set of domain data that is\ntypically large-scale and continuously evolving. The other is specific working\npatterns of this domain reflected in the data. The absence or inadequacy of\nsuch knowledge impacts the performance of the LLM. In this paper, we propose a\ngeneral paradigm that augments LLMs with DOmain-specific KnowledgE to enhance\ntheir performance on practical applications, namely DOKE. This paradigm relies\non a domain knowledge extractor, working in three steps: 1) preparing effective\nknowledge for the task; 2) selecting the knowledge for each specific sample;\nand 3) expressing the knowledge in an LLM-understandable way. Then, the\nextracted knowledge is incorporated through prompts, without any computational\ncost of model fine-tuning. We instantiate the general paradigm on a widespread\napplication, i.e. recommender systems, where critical item attributes and\ncollaborative filtering signals are incorporated. Experimental results\ndemonstrate that DOKE can substantially improve the performance of LLMs in\nspecific domains.",
        "pdf_link": "https://arxiv.org/pdf/2311.10779v1.pdf"
    },
    {
        "title": "Simulating Opinion Dynamics with Networks of LLM-based Agents",
        "authors": [
            "Yun-Shiuan Chuang",
            "Agam Goyal",
            "Nikunj Harlalka",
            "Siddharth Suresh",
            "Robert Hawkins",
            "Sijia Yang",
            "Dhavan Shah",
            "Junjie Hu",
            "Timothy T. Rogers"
        ],
        "published": "2023-11-16T07:01:48Z",
        "summary": "Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations often over-simplify human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\nproducing accurate information, leading simulated agents to consensus in line\nwith scientific reality. This bias limits their utility for understanding\nresistance to consensus views on issues like climate change. After inducing\nconfirmation bias through prompt engineering, however, we observed opinion\nfragmentation in line with existing agent-based modeling and opinion dynamics\nresearch. These insights highlight the promise and limitations of LLM agents in\nthis domain and suggest a path forward: refining LLMs with real-world discourse\nto better simulate the evolution of human beliefs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09618v4.pdf"
    },
    {
        "title": "Self-Contradictory Reasoning Evaluation and Detection",
        "authors": [
            "Ziyi Liu",
            "Isabelle Lee",
            "Yongkang Du",
            "Soumya Sanyal",
            "Jieyu Zhao"
        ],
        "published": "2023-11-16T06:22:17Z",
        "summary": "In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks\nfocus on performance-wise evaluation. Two fundamental questions persist: 1) how\nreliable is the quality of reasoning, and 2) can models detect unreliable\nreasoning? In this paper, we investigate self-contradictory (Self-Contra)\nreasoning, where the model reasoning does not support predictions. To address\n1), we assess the Self-Contra rate across four datasets and delve into\nfiner-grained categories of Self-Contra reasoning. We find that LLMs often\ncontradict themselves when performing reasoning tasks that involve contextual\ninformation understanding or commonsense. Importantly, a higher accuracy does\nnot necessarily correspond to a lower Self-Contra rate. The model may appear to\ngenerate correct answers but it may take shortcuts in reasoning or skip over\ncontextual evidence, thereby displaying Self-Contra behaviors with compromised\nreasoning. As for 2), we task GPT-4 with identifying Self-Contra reasoning and\nfiner-grained fallacies. We observe that GPT-4 struggles to effectively detect\nSelf-Contra reasoning, with significantly low performance compared with human\njudgment. Our results indicate that the current LLMs lack robustness necessary\nfor reliable reasoning and we emphasize the urgent need for establishing best\npractices in comprehensive reasoning evaluations beyond accuracy-based metrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.09603v2.pdf"
    },
    {
        "title": "Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion",
        "authors": [
            "Smriti Singh",
            "Cornelia Caragea",
            "Junyi Jessy Li"
        ],
        "published": "2023-11-16T06:20:13Z",
        "summary": "Situations and events evoke emotions in humans, but to what extent do they\ninform the prediction of emotion detection models? This work investigates how\nwell human-annotated emotion triggers correlate with features that models\ndeemed salient in their prediction of emotions. First, we introduce a novel\ndataset EmoTrigger, consisting of 900 social media posts sourced from three\ndifferent datasets; these were annotated by experts for emotion triggers with\nhigh agreement. Using EmoTrigger, we evaluate the ability of large language\nmodels (LLMs) to identify emotion triggers, and conduct a comparative analysis\nof the features considered important for these tasks between LLMs and\nfine-tuned models. Our analysis reveals that emotion triggers are largely not\nconsidered salient features for emotion prediction models, instead there is\nintricate interplay between various features and the task of emotion detection.",
        "pdf_link": "https://arxiv.org/pdf/2311.09602v2.pdf"
    },
    {
        "title": "LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks",
        "authors": [
            "Mihir Parmar",
            "Aakanksha Naik",
            "Himanshu Gupta",
            "Disha Agrawal",
            "Chitta Baral"
        ],
        "published": "2023-11-16T04:57:49Z",
        "summary": "Many large language models (LLMs) for medicine have largely been evaluated on\nshort texts, and their ability to handle longer sequences such as a complete\nelectronic health record (EHR) has not been systematically explored. Assessing\nthese models on long sequences is crucial since prior work in the general\ndomain has demonstrated performance degradation of LLMs on longer texts.\nMotivated by this, we introduce LongBoX, a collection of seven medical datasets\nin text-to-text format, designed to investigate model performance on long\nsequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)\nand strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We\nfurther evaluate two techniques designed for long-sequence handling: (i)\nlocal-global attention, and (ii) Fusion-in-Decoder (FiD). Our results\ndemonstrate mixed results with long-sequence handling - while scores on some\ndatasets increase, there is substantial room for improvement. We hope that\nLongBoX facilitates the development of more effective long-sequence techniques\nfor the medical domain. Data and source code are available at\nhttps://github.com/Mihir3009/LongBoX.",
        "pdf_link": "https://arxiv.org/pdf/2311.09564v1.pdf"
    },
    {
        "title": "Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization",
        "authors": [
            "Gaurav Sahu",
            "Olga Vechtomova",
            "Issam H. Laradji"
        ],
        "published": "2023-11-16T04:29:41Z",
        "summary": "Semi-supervised learning (SSL) is a widely used technique in scenarios where\nlabeled data is scarce and unlabeled data is abundant. While SSL is popular for\nimage and text classification, it is relatively underexplored for the task of\nextractive text summarization. Standard SSL methods follow a teacher-student\nparadigm to first train a classification model and then use the classifier's\nconfidence values to select pseudo-labels for the subsequent training cycle;\nhowever, such classifiers are not suitable to measure the accuracy of\npseudo-labels as they lack specific tuning for evaluation, which leads to\nconfidence values that fail to capture the semantics and correctness of the\ngenerated summary. To address this problem, we propose a prompt-based\npseudo-labeling strategy with LLMs that picks unlabeled examples with more\naccurate pseudo-labels than using just the classifier's probability outputs.\nOur approach also includes a relabeling mechanism that improves the quality of\npseudo-labels. We evaluate our method on three text summarization datasets:\nTweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that a\nprompting-based LLM that scores and generates pseudo-labels outperforms\nexisting SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all the\ndatasets. Furthermore, our method achieves competitive G-Eval scores\n(evaluation with GPT-4) as a fully supervised method that uses 100% of the\nlabeled data with only 16.67% of the labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2311.09559v2.pdf"
    },
    {
        "title": "Leveraging Code to Improve In-context Learning for Semantic Parsing",
        "authors": [
            "Ben Bogin",
            "Shivanshu Gupta",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023-11-16T02:50:06Z",
        "summary": "In-context learning (ICL) is an appealing approach for semantic parsing due\nto its few-shot nature and improved generalization. However, learning to parse\nto rare domain-specific languages (DSLs) from just a few demonstrations is\nchallenging, limiting the performance of even the most capable LLMs. In this\nwork, we improve the effectiveness of ICL for semantic parsing by (1) using\ngeneral-purpose programming languages such as Python instead of DSLs, and (2)\naugmenting prompts with a structured domain description that includes, e.g.,\nthe available classes and functions. We show that both these changes\nsignificantly improve accuracy across three popular datasets. Combined, they\nlead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional\nsplit), nearly closing the performance gap between easier i.i.d.\\ and harder\ncompositional splits when used with a strong model, and reducing the need for a\nlarge number of demonstrations. We find that the resemblance of the target\nparse language to general-purpose code is a more important factor than the\nlanguage's popularity in pre-training corpora. Our findings provide an improved\nmethodology for building semantic parsers in the modern context of ICL with\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09519v2.pdf"
    },
    {
        "title": "One Size Does Not Fit All: Customizing Open-Domain Procedures",
        "authors": [
            "Yash Kumar Lal",
            "Li Zhang",
            "Faeze Brahman",
            "Bodhisattwa Prasad Majumder",
            "Peter Clark",
            "Niket Tandon"
        ],
        "published": "2023-11-16T02:25:36Z",
        "summary": "How-to procedures, such as how to plant a garden, are now used by millions of\nusers, but sometimes need customizing to meet a user's specific needs, e.g.,\nplanting a garden without pesticides. Our goal is to measure and improve an\nLLM's ability to perform such customization. Our approach is to test several\nsimple multi-LLM-agent architectures for customization, as well as an\nend-to-end LLM, using a new evaluation set, called CustomPlans, of over 200\nWikiHow procedures each with a customization need. We find that a simple\narchitecture with two LLM agents used sequentially performs best, one that\nedits a generic how-to procedure and one that verifies its executability,\nsignificantly outperforming (10.5% absolute) an end-to-end prompted LLM. This\nsuggests that LLMs can be configured reasonably effectively for procedure\ncustomization. This also suggests that multi-agent editing architectures may be\nworth exploring further for other customization applications (e.g. coding,\ncreative writing) in the future.",
        "pdf_link": "https://arxiv.org/pdf/2311.09510v2.pdf"
    },
    {
        "title": "Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections",
        "authors": [
            "Yuanpu Cao",
            "Bochuan Cao",
            "Jinghui Chen"
        ],
        "published": "2023-11-15T23:52:05Z",
        "summary": "Recent developments in Large Language Models (LLMs) have manifested\nsignificant advancements. To facilitate safeguards against malicious\nexploitation, a body of research has concentrated on aligning LLMs with human\npreferences and inhibiting their generation of inappropriate content.\nUnfortunately, such alignments are often vulnerable: fine-tuning with a minimal\namount of harmful data can easily unalign the target LLM. While being\neffective, such fine-tuning-based unalignment approaches also have their own\nlimitations: (1) non-stealthiness, after fine-tuning, safety audits or\nred-teaming can easily expose the potential weaknesses of the unaligned models,\nthereby precluding their release/use. (2) non-persistence, the unaligned LLMs\ncan be easily repaired through re-alignment, i.e., fine-tuning again with\naligned data points. In this work, we show that it is possible to conduct\nstealthy and persistent unalignment on large language models via backdoor\ninjections. We also provide a novel understanding on the relationship between\nthe backdoor persistence and the activation pattern and further provide\nguidelines for potential trigger design. Through extensive experiments, we\ndemonstrate that our proposed stealthy and persistent unalignment can\nsuccessfully pass the safety evaluation while maintaining strong persistence\nagainst re-alignment defense.",
        "pdf_link": "https://arxiv.org/pdf/2312.00027v1.pdf"
    },
    {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "authors": [
            "Fuxiao Liu",
            "Xiaoyang Wang",
            "Wenlin Yao",
            "Jianshu Chen",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Yaser Yacoob",
            "Dong Yu"
        ],
        "published": "2023-11-15T23:36:42Z",
        "summary": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (MMC-Instruction) dataset comprising\n600k instances supporting diverse tasks and chart types. Leveraging this data,\nwe develop MultiModal Chart Assistant (MMCA), an LMM that achieves\nstate-of-the-art performance on existing chart QA benchmarks. Recognizing the\nneed for a comprehensive evaluation of LMM chart understanding, we also propose\na MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated\nbenchmark with 9 distinct tasks evaluating reasoning capabilities over charts.\nExtensive experiments on MMC-Benchmark reveal the limitations of existing LMMs\non correctly interpreting charts, even for the most recent GPT-4V model. Our\nwork provides an instruction-tuning methodology and benchmark to advance\nmultimodal understanding of charts.",
        "pdf_link": "https://arxiv.org/pdf/2311.10774v1.pdf"
    },
    {
        "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
        "authors": [
            "Lingbo Mo",
            "Boshi Wang",
            "Muhao Chen",
            "Huan Sun"
        ],
        "published": "2023-11-15T23:33:07Z",
        "summary": "The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an adversarial\nassessment of open-source LLMs on trustworthiness, scrutinizing them across\neight different aspects including toxicity, stereotypes, ethics, hallucination,\nfairness, sycophancy, privacy, and robustness against adversarial\ndemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)\nprompting strategy by incorporating carefully crafted malicious demonstrations\nfor trustworthiness attack. Our extensive experiments encompass recent and\nrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,\nMistral, and Llama 2. The empirical outcomes underscore the efficacy of our\nattack strategy across diverse aspects. More interestingly, our result analysis\nreveals that models with superior performance in general NLP tasks do not\nalways have greater trustworthiness; in fact, larger models can be more\nvulnerable to attacks. Additionally, models that have undergone instruction\ntuning, focusing on instruction following, tend to be more susceptible,\nalthough fine-tuning LLMs for safety alignment proves effective in mitigating\nadversarial trustworthiness attacks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09447v2.pdf"
    },
    {
        "title": "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment",
        "authors": [
            "Haoran Wang",
            "Kai Shu"
        ],
        "published": "2023-11-15T23:07:40Z",
        "summary": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications.\nInspired by recent success in modifying model behavior through steering vectors\nwithout the need for optimization, and drawing on its effectiveness in\nred-teaming LLMs, we conducted experiments employing activation steering to\ntarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness\n- across a varied set of attack settings. To establish a universal attack\nstrategy applicable to diverse target alignments without depending on manual\nanalysis, we automatically select the intervention layer based on contrastive\nlayer search. Our experiment results show that activation attacks are highly\neffective and add little or no overhead to attack efficiency. Additionally, we\ndiscuss potential countermeasures against such activation attacks. Our code and\ndata are available at https://github.com/wang2226/Backdoor-Activation-Attack\nWarning: this paper contains content that can be offensive or upsetting.",
        "pdf_link": "https://arxiv.org/pdf/2311.09433v2.pdf"
    },
    {
        "title": "When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour",
        "authors": [
            "Leonardo Ranaldi",
            "Giulia Pucci"
        ],
        "published": "2023-11-15T22:18:33Z",
        "summary": "Large Language Models (LLMs) have been demonstrating the ability to solve\ncomplex tasks by delivering answers that are positively evaluated by humans due\nin part to the intensive use of human feedback that refines responses. However,\nthe suggestibility transmitted through human feedback increases the inclination\nto produce responses that correspond to the user's beliefs or misleading\nprompts as opposed to true facts, a behaviour known as sycophancy. This\nphenomenon decreases the bias, robustness, and, consequently, their\nreliability.\n  In this paper, we shed light on the suggestibility of LLMs to sycophantic\nbehaviour, demonstrating these tendencies via human-influenced prompts over\ndifferent tasks. Our investigation reveals that LLMs show sycophantic\ntendencies when responding to queries involving subjective opinions and\nstatements that should elicit a contrary response based on facts, demonstrating\na lack of robustness.",
        "pdf_link": "https://arxiv.org/pdf/2311.09410v1.pdf"
    },
    {
        "title": "LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction",
        "authors": [
            "Jamie McCusker"
        ],
        "published": "2023-11-15T20:57:44Z",
        "summary": "While the potential of Open Information Extraction (Open IE) for Knowledge\nGraph Construction (KGC) may seem promising, we find that the alignment of Open\nIE extraction results with existing knowledge graphs to be inadequate. The\nadvent of Large Language Models (LLMs), especially the commercially available\nOpenAI models, have reset expectations for what is possible with deep learning\nmodels and have created a new field called prompt engineering. We investigate\nthe use of GPT models and prompt engineering for knowledge graph construction\nwith the Wikidata knowledge graph to address a similar problem to Open IE,\nwhich we call Open Knowledge Extraction (OKE) using an approach we call the\nLinked Open Knowledge Extractor (LOKE, pronounced like \"Loki\"). We consider the\nentity linking task essential to construction of real world knowledge graphs.\nWe merge the CaRB benchmark scoring approach with data from the TekGen dataset\nfor the LOKE task. We then show that a well engineered prompt, paired with a\nnaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's\nOpenIE 4 implementation on the OKE task, although it over-generates triples\ncompared to the reference set due to overall triple scarcity in the TekGen set.\nThrough an analysis of entity linkability in the CaRB dataset, as well as\noutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the \"silver\"\nTekGen triples show that the task is significantly different in content from\nOIE, if not structure. Through this analysis and a qualitative analysis of\nsentence extractions via all methods, we found that LOKE-GPT extractions are of\nhigh utility for the KGC task and suitable for use in semi-automated extraction\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2311.09366v1.pdf"
    },
    {
        "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
        "authors": [
            "George Chrysostomou",
            "Zhixue Zhao",
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2023-11-15T19:49:24Z",
        "summary": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations from pruned LLMs are less prevalent\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.",
        "pdf_link": "https://arxiv.org/pdf/2311.09335v2.pdf"
    },
    {
        "title": "Improving fit to human reading times via temperature-scaled surprisal",
        "authors": [
            "Tong Liu",
            "Iza \u0160krjanec",
            "Vera Demberg"
        ],
        "published": "2023-11-15T19:34:06Z",
        "summary": "Past studies have provided broad support for that words with lower\npredictability (i.e., higher surprisal) require more time for comprehension by\nusing large language models (LLMs) to simulate humans' cognitive load. In\ngeneral, these studies have implicitly assumed that the probability scores from\nLLMs are accurate, ignoring the discrepancies between human cognition and LLMs\nfrom this standpoint. Inspired by the concept of probability calibration, we\nare the first work to focus on the probability distribution for human reading\nsimulation. We propose to use temperature-scaled surprisal, a surprisal\ncalculated by shaped probability, to be the predictor of human reading times.\nOur results across three corpora consistently revealed that such a surprisal\ncan drastically improve the prediction of reading times. Setting the\ntemperature to be approximately 2.5 across all models and datasets can yield up\nto an 89% of increase in delta log-likelihood in our setting. We also propose a\ncalibration metric to quantify the possible human-likeness bias. Further\nanalysis was done and provided insights into this phenomenon.",
        "pdf_link": "https://arxiv.org/pdf/2311.09325v1.pdf"
    },
    {
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
        "authors": [
            "Fangzhi Xu",
            "Zhiyong Wu",
            "Qiushi Sun",
            "Siyu Ren",
            "Fei Yuan",
            "Shuai Yuan",
            "Qika Lin",
            "Yu Qiao",
            "Jun Liu"
        ],
        "published": "2023-11-15T18:59:56Z",
        "summary": "Although Large Language Models (LLMs) demonstrate remarkable ability in\nprocessing and generating human-like text, they do have limitations when it\ncomes to comprehending and expressing world knowledge that extends beyond the\nboundaries of natural language(e.g., chemical molecular formula). Injecting a\ncollection of symbolic data directly into the training of LLMs can be\nproblematic, as it disregards the synergies among different symbolic families\nand overlooks the need for a balanced mixture of natural and symbolic data. In\nthis work, we tackle these challenges from both a data and framework\nperspective and introduce Symbol-LLM series models. First, we curated a data\ncollection consisting of 34 tasks and incorporating approximately 20 distinct\nsymbolic families, intending to capture the interrelations and foster synergies\nbetween symbols. Then, a two-stage tuning framework succeeds in injecting\nsymbolic knowledge without loss of the generality ability. Extensive\nexperiments on both symbol- and NL-centric tasks demonstrate the balanced and\nsuperior performances of Symbol-LLM series models. The project page is\nhttps://xufangzhi.github.io/symbol-llm-page/.",
        "pdf_link": "https://arxiv.org/pdf/2311.09278v2.pdf"
    },
    {
        "title": "Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models",
        "authors": [
            "Weize Liu",
            "Guocong Li",
            "Kai Zhang",
            "Bang Du",
            "Qiyuan Chen",
            "Xuming Hu",
            "Hongxia Xu",
            "Jintai Chen",
            "Jian Wu"
        ],
        "published": "2023-11-15T18:56:23Z",
        "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing. However, the massive scale and computational demands of\nthese models present formidable challenges when considering their practical\ndeployment in resource-constrained environments. While techniques such as\nchain-of-thought (CoT) distillation have displayed promise in distilling LLMs\ninto small language models (SLMs), there is a risk that distilled SLMs may\nstill inherit flawed reasoning and hallucinations from LLMs. To address these\nissues, we propose a twofold methodology: First, we introduce a novel method\nfor distilling the self-evaluation capability from LLMs into SLMs, aiming to\nmitigate the adverse effects of flawed reasoning and hallucinations inherited\nfrom LLMs. Second, we advocate for distilling more comprehensive thinking by\nincorporating multiple distinct CoTs and self-evaluation outputs, to ensure a\nmore thorough and robust knowledge transfer into SLMs. Experiments on three NLP\nbenchmarks demonstrate that our method significantly improves the performance\nof distilled SLMs, offering a new perspective for developing more effective and\nefficient SLMs in resource-constrained environments.",
        "pdf_link": "https://arxiv.org/pdf/2311.09214v3.pdf"
    },
    {
        "title": "Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering",
        "authors": [
            "Junqing He",
            "Kunhao Pan",
            "Xiaoqun Dong",
            "Zhuoyang Song",
            "Yibo Liu",
            "Yuxin Liang",
            "Hao Wang",
            "Qianguo Sun",
            "Songxin Zhang",
            "Zejian Xie",
            "Jiaxing Zhang"
        ],
        "published": "2023-11-15T18:42:44Z",
        "summary": "While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.",
        "pdf_link": "https://arxiv.org/pdf/2311.09198v1.pdf"
    },
    {
        "title": "Towards Verifiable Text Generation with Symbolic References",
        "authors": [
            "Lucas Torroba Hennigen",
            "Shannon Shen",
            "Aniruddha Nrusimha",
            "Bernhard Gapp",
            "David Sontag",
            "Yoon Kim"
        ],
        "published": "2023-11-15T18:28:29Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2311.09188v1.pdf"
    },
    {
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
        "authors": [
            "Yixin Liu",
            "Alexander R. Fabbri",
            "Jiawen Chen",
            "Yilun Zhao",
            "Simeng Han",
            "Shafiq Joty",
            "Pengfei Liu",
            "Dragomir Radev",
            "Chien-Sheng Wu",
            "Arman Cohan"
        ],
        "published": "2023-11-15T18:25:26Z",
        "summary": "While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.",
        "pdf_link": "https://arxiv.org/pdf/2311.09184v1.pdf"
    },
    {
        "title": "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models",
        "authors": [
            "Jierui Li",
            "Vipul Raheja",
            "Dhruv Kumar"
        ],
        "published": "2023-11-15T18:23:17Z",
        "summary": "In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments.",
        "pdf_link": "https://arxiv.org/pdf/2311.09182v1.pdf"
    },
    {
        "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph",
        "authors": [
            "Zhaowei Wang",
            "Haochen Shi",
            "Weiqi Wang",
            "Tianqing Fang",
            "Hongming Zhang",
            "Sehyun Choi",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023-11-15T18:11:23Z",
        "summary": "Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09174v3.pdf"
    },
    {
        "title": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models",
        "authors": [
            "Wenhong Zhu",
            "Hongkun Hao",
            "Zhiwei He",
            "Yunze Song",
            "Yumeng Zhang",
            "Hanxu Hu",
            "Yiran Wei",
            "Rui Wang",
            "Hongyuan Lu"
        ],
        "published": "2023-11-15T17:50:30Z",
        "summary": "We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2311.09154v2.pdf"
    },
    {
        "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
        "authors": [
            "Ziyang Chen",
            "Dongfang Li",
            "Xiang Zhao",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2023-11-15T17:46:39Z",
        "summary": "In this paper, we tackle the significant challenge of temporal knowledge\nreasoning in Large Language Models (LLMs), an area where such models frequently\nencounter difficulties. These difficulties often result in the generation of\nmisleading or incorrect information, primarily due to their limited capacity to\nprocess evolving factual knowledge and complex temporal logic. In response, we\npropose a novel, constructivism-based approach that advocates for a paradigm\nshift in LLM learning towards an active, ongoing process of knowledge synthesis\nand customization. At the heart of our proposal is the Abstract Reasoning\nInduction ARI framework, which divides temporal reasoning into two distinct\nphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\ninstances of hallucinations and improve LLMs' capacity for integrating abstract\nmethodologies derived from historical data. Our approach achieves remarkable\nimprovements, with relative gains of 29.7\\% and 9.27\\% on two temporal QA\ndatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\nThe code will be released at https://github.com/czy1999/ARI.",
        "pdf_link": "https://arxiv.org/pdf/2311.09149v1.pdf"
    },
    {
        "title": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
        "authors": [
            "Yuanwei Wu",
            "Xiang Li",
            "Yixin Liu",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "published": "2023-11-15T17:17:39Z",
        "summary": "Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities, especially in model API. To fill the research gap, we carry\nout the following work: 1) We discover a system prompt leakage vulnerability in\nGPT-4V. Through carefully designed dialogue, we successfully extract the\ninternal system prompts of GPT-4V. This finding indicates potential exploitable\nsecurity risks in MLLMs; 2) Based on the acquired system prompts, we propose a\nnovel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via\nSystem Prompt). By employing GPT-4 as a red teaming tool against itself, we aim\nto search for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking. This finding could be leveraged to\ngreatly facilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09127v2.pdf"
    },
    {
        "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
        "authors": [
            "Haoqiang Kang",
            "Juntong Ni",
            "Huaxiu Yao"
        ],
        "published": "2023-11-15T17:04:56Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09114v2.pdf"
    },
    {
        "title": "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?",
        "authors": [
            "Yusuke Sakai",
            "Hidetaka Kamigaito",
            "Katsuhiko Hayashi",
            "Taro Watanabe"
        ],
        "published": "2023-11-15T16:56:49Z",
        "summary": "Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.",
        "pdf_link": "https://arxiv.org/pdf/2311.09109v1.pdf"
    },
    {
        "title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
        "authors": [
            "Xiaozhi Wang",
            "Hao Peng",
            "Yong Guan",
            "Kaisheng Zeng",
            "Jianhui Chen",
            "Lei Hou",
            "Xu Han",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Ruobing Xie",
            "Jie Zhou",
            "Juanzi Li"
        ],
        "published": "2023-11-15T16:52:14Z",
        "summary": "Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and our code can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.",
        "pdf_link": "https://arxiv.org/pdf/2311.09105v1.pdf"
    },
    {
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
        "authors": [
            "Zhexin Zhang",
            "Junxiao Yang",
            "Pei Ke",
            "Minlie Huang"
        ],
        "published": "2023-11-15T16:42:29Z",
        "summary": "Large Language Models (LLMs) continue to advance in their capabilities, yet\nthis progress is accompanied by a growing array of safety risks. While\nsignificant attention has been dedicated to exploiting weaknesses in LLMs\nthrough jailbreaking attacks, there remains a paucity of exploration into\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the inherent conflict between the goals of being\nhelpful and ensuring safety. To counter jailbreaking attacks, we propose to\nintegrate goal prioritization at both training and inference stages.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to\n2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising\ngeneral performance. Furthermore, integrating the concept of goal\nprioritization into the training phase reduces the ASR from 71.0% to 6.6% for\nLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are\nincluded during training, our approach slashes the ASR by half, decreasing it\nfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs\nface greater safety risks, they also possess a greater capacity to be steered\ntowards defending against such attacks. We hope our work could contribute to\nthe comprehension of jailbreaking attacks and defenses, and shed light on the\nrelationship between LLMs' capability and safety. Our code will be available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09096v1.pdf"
    },
    {
        "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
        "authors": [
            "Marta Marchiori Manerba",
            "Karolina Sta\u0144czak",
            "Riccardo Guidotti",
            "Isabelle Augenstein"
        ],
        "published": "2023-11-15T16:35:59Z",
        "summary": "Large language models have been shown to encode a variety of social biases,\nwhich carries the risk of downstream harms. While the impact of these biases\nhas been recognized, prior methods for bias evaluation have been limited to\nbinary association tests on small datasets, offering a constrained view of the\nnature of societal biases within language models. In this paper, we propose an\noriginal framework for probing language models for societal biases. We collect\na probing dataset to analyze language models' general associations, as well as\nalong the axes of societal categories, identities, and stereotypes. To this\nend, we leverage a novel perplexity-based fairness score. We curate a\nlarge-scale benchmarking dataset addressing drawbacks and limitations of\nexisting fairness collections, expanding to a variety of different identities\nand stereotypes. When comparing our methodology with prior work, we demonstrate\nthat biases within language models are more nuanced than previously\nacknowledged. In agreement with recent findings, we find that larger model\nvariants exhibit a higher degree of bias. Moreover, we expose how identities\nexpressing different religions lead to the most pronounced disparate treatments\nacross all models.",
        "pdf_link": "https://arxiv.org/pdf/2311.09090v2.pdf"
    },
    {
        "title": "How Multilingual is Multilingual LLM?",
        "authors": [
            "Fei Yuan",
            "Shuai Yuan",
            "Zhiyong Wu",
            "Lei Li"
        ],
        "published": "2023-11-15T16:13:14Z",
        "summary": "Large Language Models (LLMs), trained predominantly on extensive English\ndata, often exhibit limitations when applied to other languages. Current\nresearch is primarily focused on enhancing the multilingual capabilities of\nthese models by employing various tuning strategies. Despite their\neffectiveness in certain languages, the understanding of the multilingual\nabilities of LLMs remains incomplete. This study endeavors to evaluate the\nmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\nlanguages, and classifies languages with similar characteristics into four\ndistinct quadrants. By delving into each quadrant, we shed light on the\nrationale behind their categorization and offer actionable guidelines for\ntuning these languages. Extensive experiments reveal that existing LLMs possess\nmultilingual capabilities that surpass our expectations, and we can\nsignificantly improve the multilingual performance of LLMs by focusing on these\ndistinct attributes present in each quadrant.",
        "pdf_link": "https://arxiv.org/pdf/2311.09071v1.pdf"
    },
    {
        "title": "How Well Do Large Language Models Truly Ground?",
        "authors": [
            "Hyunji Lee",
            "Sejune Joo",
            "Chaeeun Kim",
            "Joel Jang",
            "Doyoung Kim",
            "Kyoung-Woon On",
            "Minjoon Seo"
        ],
        "published": "2023-11-15T16:11:27Z",
        "summary": "Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\nissues such as hallucinations, lack of control, and difficulties in integrating\nvariable knowledge. To mitigate this, LLMs can be probed to generate responses\nby grounding on external context, often given as input (knowledge-augmented\nmodels). Yet, previous research is often confined to a narrow view of the term\n\"grounding\", often only focusing on whether the response contains the correct\nanswer or not, which does not ensure the reliability of the entire response. To\naddress this limitation, we introduce a strict definition of grounding: a model\nis considered truly grounded when its responses (1) fully utilize necessary\nknowledge from the provided context, and (2) don't exceed the knowledge within\nthe contexts. We introduce a new dataset and a grounding metric to assess this\nnew definition and perform experiments across 13 LLMs of different sizes and\ntraining methods to provide insights into the factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.09069v1.pdf"
    },
    {
        "title": "GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models",
        "authors": [
            "Serwan Jassim",
            "Mario Holubar",
            "Annika Richter",
            "Cornelius Wolff",
            "Xenia Ohmer",
            "Elia Bruni"
        ],
        "published": "2023-11-15T15:38:28Z",
        "summary": "This paper presents GRASP, a novel benchmark to evaluate the language\ngrounding and physical understanding capabilities of video-based multimodal\nlarge language models (LLMs). This evaluation is accomplished via a two-tier\napproach leveraging Unity simulations. The first level tests for language\ngrounding by assessing a model's ability to relate simple textual descriptions\nwith visual information. The second level evaluates the model's understanding\nof \"Intuitive Physics\" principles, such as object permanence and continuity. In\naddition to releasing the benchmark, we use it to evaluate several\nstate-of-the-art multimodal LLMs. Our evaluation reveals significant\nshortcomings in the language grounding and intuitive physics capabilities of\nthese models. Although they exhibit at least some grounding capabilities,\nparticularly for colors and shapes, these capabilities depend heavily on the\nprompting strategy. At the same time, all models perform below or at the chance\nlevel of 50% in the Intuitive Physics tests, while human subjects are on\naverage 80% correct. These identified limitations underline the importance of\nusing benchmarks like GRASP to monitor the progress of future models in\ndeveloping these competencies.",
        "pdf_link": "https://arxiv.org/pdf/2311.09048v2.pdf"
    },
    {
        "title": "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
        "authors": [
            "Yuxia Wang",
            "Revanth Gangi Reddy",
            "Zain Muhammad Mujahid",
            "Arnav Arora",
            "Aleksandr Rubashevskii",
            "Jiahui Geng",
            "Osama Mohammed Afzal",
            "Liangming Pan",
            "Nadav Borenstein",
            "Aditya Pillai",
            "Isabelle Augenstein",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2023-11-15T14:41:57Z",
        "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2311.09000v2.pdf"
    },
    {
        "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
        "authors": [
            "Hao Peng",
            "Xiaozhi Wang",
            "Jianhui Chen",
            "Weikai Li",
            "Yunjia Qi",
            "Zimu Wang",
            "Zhili Wu",
            "Kaisheng Zeng",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023-11-15T14:26:30Z",
        "summary": "In-context learning (ICL) has become the default method for using large\nlanguage models (LLMs), making the exploration of its limitations and\nunderstanding the underlying causes crucial. In this paper, we find that ICL\nfalls short of handling specification-heavy tasks, which are tasks with\ncomplicated and extensive task specifications, requiring several hours for\nordinary humans to master, such as traditional information extraction tasks.\nThe performance of ICL on these tasks mostly cannot reach half of the\nstate-of-the-art results. To explore the reasons behind this failure, we\nconduct comprehensive experiments on 18 specification-heavy tasks with various\nLLMs and identify three primary reasons: inability to specifically understand\ncontext, misalignment in task schema comprehension with humans, and inadequate\nlong-text understanding ability. Furthermore, we demonstrate that through\nfine-tuning, LLMs can achieve decent performance on these tasks, indicating\nthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\nof existing alignment methods that renders LLMs incapable of handling\ncomplicated specification-heavy tasks via ICL. To substantiate this, we perform\ndedicated instruction tuning on LLMs for these tasks and observe a notable\nimprovement. We hope the analyses in this paper could facilitate advancements\nin alignment methods enabling LLMs to meet more sophisticated human demands.",
        "pdf_link": "https://arxiv.org/pdf/2311.08993v1.pdf"
    },
    {
        "title": "Enabling Large Language Models to Learn from Rules",
        "authors": [
            "Wenkai Yang",
            "Yankai Lin",
            "Jie Zhou",
            "Jirong Wen"
        ],
        "published": "2023-11-15T11:42:41Z",
        "summary": "Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current knowledge learning paradigm of LLMs is\nmainly based on learning from examples, in which LLMs learn the internal rule\nimplicitly from a certain number of supervised examples. However, this learning\nparadigm may not well learn those complicated rules, especially when the\ntraining examples are limited. We are inspired that humans can learn the new\ntasks or knowledge in another way by learning from rules. That is, humans can\nlearn new tasks or grasps new knowledge quickly and generalize well given only\na detailed rule and a few optional examples. Therefore, in this paper, we aim\nto explore the feasibility of this new learning paradigm, which targets on\nencoding rule-based knowledge into LLMs. We further propose rule distillation,\nwhich first uses the strong in-context abilities of LLMs to extract the\nknowledge from the textual rules, and then explicitly encode the knowledge into\nthe parameters of LLMs by learning from the above in-context signals produced\ninside the model. Our experiments show that making LLMs learn from rules by our\nmethod is much more efficient than example-based learning in both the sample\nsize and generalization ability. Warning: This paper may contain examples with\noffensive content.",
        "pdf_link": "https://arxiv.org/pdf/2311.08883v2.pdf"
    },
    {
        "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
        "authors": [
            "Vaishnavi Shrivastava",
            "Percy Liang",
            "Ananya Kumar"
        ],
        "published": "2023-11-15T11:27:44Z",
        "summary": "To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).",
        "pdf_link": "https://arxiv.org/pdf/2311.08877v1.pdf"
    },
    {
        "title": "Disinformation Capabilities of Large Language Models",
        "authors": [
            "Ivan Vykopal",
            "Mat\u00fa\u0161 Pikuliak",
            "Ivan Srba",
            "Robert Moro",
            "Dominik Macko",
            "Maria Bielikova"
        ],
        "published": "2023-11-15T10:25:30Z",
        "summary": "Automated disinformation generation is often listed as an important risk\nassociated with large language models (LLMs). The theoretical ability to flood\nthe information space with disinformation content might have dramatic\nconsequences for societies around the world. This paper presents a\ncomprehensive study of the disinformation capabilities of the current\ngeneration of LLMs to generate false news articles in the English language. In\nour study, we evaluated the capabilities of 10 LLMs using 20 disinformation\nnarratives. We evaluated several aspects of the LLMs: how good they are at\ngenerating news articles, how strongly they tend to agree or disagree with the\ndisinformation narratives, how often they generate safety warnings, etc. We\nalso evaluated the abilities of detection models to detect these articles as\nLLM-generated. We conclude that LLMs are able to generate convincing news\narticles that agree with dangerous disinformation narratives.",
        "pdf_link": "https://arxiv.org/pdf/2311.08838v2.pdf"
    },
    {
        "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
        "authors": [
            "Davis Yoshida",
            "Kartik Goyal",
            "Kevin Gimpel"
        ],
        "published": "2023-11-15T09:38:53Z",
        "summary": "It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\ngenerally been attributed to either a fundamental inadequacy of modes in models\nor weaknesses in language modeling. Contrastingly in this work, we emphasize\nthat degenerate modes can even occur in the absence of any model error, due to\ncontamination of the training data. Specifically, we show that mixing even a\ntiny amount of low-entropy noise with a population text distribution can cause\nthe data distribution's mode to become degenerate, implying that any models\ntrained on it will be as well. As the unconditional mode of NLG models will\noften be degenerate, we therefore propose to apply MAP decoding to the model's\ndistribution conditional on avoiding specific degeneracies. Using exact-search,\nwe empirically verify that the length-conditional modes of machine translation\nmodels and language models are indeed more fluent and topical than their\nunconditional modes. For the first time, we also share many examples of exact\nmodal sequences from these models, and from several variants of the LLaMA-7B\nmodel. Notably, the modes of the LLaMA models are still degenerate, showing\nthat improvements in modeling have not fixed this issue. Because of the cost of\nexact mode finding algorithms, we develop an approximate mode finding approach,\nACBS, which finds sequences that are both high-likelihood and high-quality. We\napply this approach to LLaMA-7B, a model which was not trained for instruction\nfollowing, and find that we are able to elicit reasonable outputs without any\nfinetuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.08817v1.pdf"
    },
    {
        "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
        "authors": [
            "Chang Gao",
            "Haiyun Jiang",
            "Deng Cai",
            "Shuming Shi",
            "Wai Lam"
        ],
        "published": "2023-11-15T09:18:09Z",
        "summary": "Most existing chain-of-thought (CoT) prompting methods suffer from the issues\nof generalizability and consistency, as they often rely on instance-specific\nsolutions that may not be applicable to other cases and lack task-level\nconsistency in their reasoning steps. To address these limitations, we propose\na comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\nconstruct generalizable and consistent few-shot prompts for various tasks\nautomatically. To this end, StrategyLLM employs four LLM-based agents: strategy\ngenerator, executor, optimizer, and evaluator, working together to generate,\nevaluate, and select promising strategies for a given task. The experimental\nresults demonstrate that StrategyLLM outperforms the competitive baseline\nCoT-SC that requires human-annotated solutions on 13 datasets across 4\nchallenging tasks without human involvement, including math reasoning (34.21%\n$\\rightarrow$ 38.79%), commonsense reasoning (70.3% $\\rightarrow$ 72.5%),\nalgorithmic reasoning (51.7% $\\rightarrow$ 62.0%), and symbolic reasoning\n(30.0% $\\rightarrow$ 79.2%).",
        "pdf_link": "https://arxiv.org/pdf/2311.08803v2.pdf"
    },
    {
        "title": "Auto-ICL: In-Context Learning without Human Supervision",
        "authors": [
            "Jinghan Yang",
            "Shuming Ma",
            "Furu Wei"
        ],
        "published": "2023-11-15T07:37:28Z",
        "summary": "In the era of Large Language Models (LLMs), human-computer interaction has\nevolved towards natural language, offering unprecedented flexibility. Despite\nthis, LLMs are heavily reliant on well-structured prompts to function\nefficiently within the realm of In-Context Learning. Vanilla In-Context\nLearning relies on human-provided contexts, such as labeled examples, explicit\ninstructions, or other guiding mechanisms that shape the model's outputs. To\naddress this challenge, our study presents a universal framework named\nAutomatic In-Context Learning. Upon receiving a user's request, we ask the\nmodel to independently generate examples, including labels, instructions, or\nreasoning pathways. The model then leverages this self-produced context to\ntackle the given problem. Our approach is universally adaptable and can be\nimplemented in any setting where vanilla In-Context Learning is applicable. We\ndemonstrate that our method yields strong performance across a range of tasks,\nstanding up well when compared to existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.09263v1.pdf"
    },
    {
        "title": "Thread of Thought Unraveling Chaotic Contexts",
        "authors": [
            "Yucheng Zhou",
            "Xiubo Geng",
            "Tao Shen",
            "Chongyang Tao",
            "Guodong Long",
            "Jian-Guang Lou",
            "Jianbing Shen"
        ],
        "published": "2023-11-15T06:54:44Z",
        "summary": "Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.",
        "pdf_link": "https://arxiv.org/pdf/2311.08734v1.pdf"
    },
    {
        "title": "Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models",
        "authors": [
            "Minze Chen",
            "Zhenxiang Tao",
            "Weitong Tang",
            "Tingxin Qin",
            "Rui Yang",
            "Chunli Zhu"
        ],
        "published": "2023-11-15T06:48:50Z",
        "summary": "Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.",
        "pdf_link": "https://arxiv.org/pdf/2311.08732v1.pdf"
    },
    {
        "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
        "authors": [
            "Bairu Hou",
            "Yujian Liu",
            "Kaizhi Qian",
            "Jacob Andreas",
            "Shiyu Chang",
            "Yang Zhang"
        ],
        "published": "2023-11-15T05:58:35Z",
        "summary": "Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a model into data (aleatoric) uncertainty, resulting from the\ninherent complexity or ambiguity of the data, and model (epistemic)\nuncertainty, resulting from the lack of knowledge in the model. Performing\nuncertainty decomposition for large language models (LLMs) is an important step\ntoward improving the reliability, trustworthiness, and interpretability of\nLLMs, but this research task is very challenging and remains unresolved. The\nexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\nLLMs, because BNN requires training and ensembling multiple variants of models,\nwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\nintroduce an uncertainty decomposition framework for LLMs, called input\nclarifications ensemble, which bypasses the need to train new models. Rather\nthan ensembling models with different parameters, our approach generates a set\nof clarifications for the input, feeds them into the fixed LLMs, and ensembles\nthe corresponding predictions. We show that our framework shares a symmetric\ndecomposition structure with BNN. Empirical evaluations demonstrate that the\nproposed framework provides accurate and reliable uncertainty quantification on\nvarious tasks. Code will be made publicly available at\nhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .",
        "pdf_link": "https://arxiv.org/pdf/2311.08718v1.pdf"
    },
    {
        "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning",
        "authors": [
            "Zhihan Zhang",
            "Dong-Ho Lee",
            "Yuwei Fang",
            "Wenhao Yu",
            "Mengzhao Jia",
            "Meng Jiang",
            "Francesco Barbieri"
        ],
        "published": "2023-11-15T05:28:07Z",
        "summary": "Instruction tuning has remarkably advanced large language models (LLMs) in\nunderstanding and responding to diverse human instructions. Despite the success\nin high-resource languages, its application in lower-resource ones faces\nchallenges due to the imbalanced foundational abilities of LLMs across\ndifferent languages, stemming from the uneven language distribution in their\npre-training data. To tackle this issue, we propose pivot language guided\ngeneration (PLUG), an approach that utilizes a high-resource language,\nprimarily English, as the pivot to enhance instruction tuning in lower-resource\nlanguages. It trains the model to first process instructions in the pivot\nlanguage, and then produce responses in the target language. To evaluate our\napproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\nlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\nprofessional translators. Our approach demonstrates a significant improvement\nin the instruction-following abilities of LLMs by 29% on average, compared to\ndirectly responding in the target language alone. Further experiments validate\nthe versatility of our approach by employing alternative pivot languages beyond\nEnglish to assist languages where LLMs exhibit lower proficiency. Our code and\ndata are available at https://github.com/ytyz1307zzh/PLUG.",
        "pdf_link": "https://arxiv.org/pdf/2311.08711v2.pdf"
    },
    {
        "title": "Comparing Generalization in Learning with Limited Numbers of Exemplars: Transformer vs. RNN in Attractor Dynamics",
        "authors": [
            "Rui Fukushima",
            "Jun Tani"
        ],
        "published": "2023-11-15T00:37:49Z",
        "summary": "ChatGPT, a widely-recognized large language model (LLM), has recently gained\nsubstantial attention for its performance scaling, attributed to the billions\nof web-sourced natural language sentences used for training. Its underlying\narchitecture, Transformer, has found applications across diverse fields,\nincluding video, audio signals, and robotic movement. %The crucial question\nthis raises concerns the Transformer's generalization-in-learning (GIL)\ncapacity. However, this raises a crucial question about Transformer's\ngeneralization in learning (GIL) capacity. Is ChatGPT's success chiefly due to\nthe vast dataset used for training, or is there more to the story? To\ninvestigate this, we compared Transformer's GIL capabilities with those of a\ntraditional Recurrent Neural Network (RNN) in tasks involving attractor\ndynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)\nmethod has been employed. Our simulation results suggest that under conditions\nof limited data availability, Transformer's GIL abilities are markedly inferior\nto those of RNN.",
        "pdf_link": "https://arxiv.org/pdf/2311.10763v1.pdf"
    },
    {
        "title": "Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures",
        "authors": [
            "David F. Jenny",
            "Yann Billeter",
            "Mrinmaya Sachan",
            "Bernhard Sch\u00f6lkopf",
            "Zhijing Jin"
        ],
        "published": "2023-11-15T00:02:25Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has sparked intense\ndebate regarding their ability to perceive and interpret complex\nsocio-political landscapes. In this study, we undertake an exploration of\ndecision-making processes and inherent biases within LLMs, exemplified by\nChatGPT, specifically contextualizing our analysis within political debates. We\naim not to critique or validate LLMs' values, but rather to discern how they\ninterpret and adjudicate \"good arguments.\" By applying Activity Dependency\nNetworks (ADNs), we extract the LLMs' implicit criteria for such assessments\nand illustrate how normative values influence these perceptions. We discuss the\nconsequences of our findings for human-AI alignment and bias mitigation. Our\ncode and data at https://github.com/david-jenny/LLM-Political-Study.",
        "pdf_link": "https://arxiv.org/pdf/2311.08605v1.pdf"
    },
    {
        "title": "Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment",
        "authors": [
            "Philippe Laban",
            "Lidiya Murakhovs'ka",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-11-14T23:40:22Z",
        "summary": "The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM completes a\nclassification task. In a second round, the LLM is challenged with a follow-up\nphrase like \"Are you sure?\", offering an opportunity for the model to reflect\non its initial answer, and decide whether to confirm or flip its answer. A\nsystematic study of ten LLMs on seven classification tasks reveals that models\nflip their answers on average 46% of the time and that all models see a\ndeterioration of accuracy between their first and final prediction, with an\naverage drop of 17% (the FlipFlop effect). We conduct finetuning experiments on\nan open-source LLM and find that finetuning on synthetically created data can\nmitigate - reducing performance deterioration by 60% - but not resolve\nsycophantic behavior entirely. The FlipFlop experiment illustrates the\nuniversality of sycophantic behavior in LLMs and provides a robust framework to\nanalyze model behavior and evaluate future models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08596v2.pdf"
    },
    {
        "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Yunkun Wang",
            "Yunzhe Li",
            "Qian Chen",
            "Wen Wang",
            "Tingyu Lin",
            "Weishan Zhao",
            "Li Zhu",
            "Shuiguang Deng",
            "Hari Sundaram"
        ],
        "published": "2023-11-14T23:18:52Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\ncoding related tasks, particularly on assisting humans in programming and\nfacilitating programming automation. However, existing benchmarks for\nevaluating the code understanding and generation capacities of LLMs suffer from\nsevere limitations. First, most benchmarks are deficient as they focus on a\nnarrow range of popular programming languages and specific tasks, whereas the\nreal-world software development scenarios show dire need to implement systems\nwith multilingual programming environments to satisfy diverse requirements.\nPractical programming practices also strongly expect multi-task settings for\ntesting coding capabilities of LLMs comprehensively and robustly. Second, most\nbenchmarks also fail to consider the actual executability and the consistency\nof execution results of the generated code. To bridge these gaps between\nexisting benchmarks and expectations from practical applications, we introduce\nCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\nevaluation benchmark for comprehensively gauging LLM capabilities on coding\ntasks. CodeScope covers 43 programming languages and 8 coding tasks. It\nevaluates the coding performance of LLMs from three dimensions (perspectives):\ndifficulty, efficiency, and length. To facilitate execution-based evaluations\nof code generation, we develop MultiCodeEngine, an automated code execution\nengine that supports 14 programming languages. Finally, we systematically\nevaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\nsuperior breadth and challenges of CodeScope for evaluating LLMs on code\nunderstanding and generation tasks compared to other benchmarks. The CodeScope\nbenchmark and datasets are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.",
        "pdf_link": "https://arxiv.org/pdf/2311.08588v2.pdf"
    },
    {
        "title": "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer",
        "authors": [
            "Urchade Zaratiana",
            "Nadi Tomeh",
            "Pierre Holat",
            "Thierry Charnois"
        ],
        "published": "2023-11-14T20:39:12Z",
        "summary": "Named Entity Recognition (NER) is essential in various Natural Language\nProcessing (NLP) applications. Traditional NER models are effective but limited\nto a set of predefined entity types. In contrast, Large Language Models (LLMs)\ncan extract arbitrary entities through natural language instructions, offering\ngreater flexibility. However, their size and cost, particularly for those\naccessed via APIs like ChatGPT, make them impractical in resource-limited\nscenarios. In this paper, we introduce a compact NER model trained to identify\nany type of entity. Leveraging a bidirectional transformer encoder, our model,\nGLiNER, facilitates parallel entity extraction, an advantage over the slow\nsequential token generation of LLMs. Through comprehensive testing, GLiNER\ndemonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs\nin zero-shot evaluations on various NER benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.08526v1.pdf"
    },
    {
        "title": "LLMs cannot find reasoning errors, but can correct them!",
        "authors": [
            "Gladys Tyen",
            "Hassan Mansoor",
            "Victor C\u0103rbune",
            "Peter Chen",
            "Tony Mak"
        ],
        "published": "2023-11-14T20:12:38Z",
        "summary": "While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we break down the self-correction process into two\ncore components: mistake finding and output correction. For mistake finding, we\nrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\nreasoning traces. We provide benchmark numbers for several state-of-the-art\nLLMs, and demonstrate that LLMs generally struggle with finding logical\nmistakes. For output correction, we propose a backtracking method which\nprovides large improvements when given information on mistake location. We\nconstrue backtracking as a lightweight alternative to reinforcement learning\nmethods, and show that it remains effective with a reward model at 60-70%\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2311.08516v2.pdf"
    },
    {
        "title": "Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models",
        "authors": [
            "Carlos Aguirre",
            "Kuleen Sasse",
            "Isabel Cachola",
            "Mark Dredze"
        ],
        "published": "2023-11-14T19:02:03Z",
        "summary": "Recently, work in NLP has shifted to few-shot (in-context) learning, with\nlarge language models (LLMs) performing well across a range of tasks. However,\nwhile fairness evaluations have become a standard for supervised methods,\nlittle is known about the fairness of LLMs as prediction systems. Further,\ncommon standard methods for fairness involve access to models weights or are\napplied during finetuning, which are not applicable in few-shot learning. Do\nLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\nwe explore the effect of shots, which directly affect the performance of\nmodels, on the fairness of LLMs as NLP classification systems. We consider how\ndifferent shot selection strategies, both existing and new demographically\nsensitive methods, affect model fairness across three standard fairness\ndatasets. We discuss how future work can include LLM fairness evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2311.08472v1.pdf"
    },
    {
        "title": "Fine-tuning Language Models for Factuality",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Huaxiu Yao",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-11-14T18:59:15Z",
        "summary": "The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2311.08401v1.pdf"
    },
    {
        "title": "Towards Open-Ended Visual Recognition with Large Language Model",
        "authors": [
            "Qihang Yu",
            "Xiaohui Shen",
            "Liang-Chieh Chen"
        ],
        "published": "2023-11-14T18:59:01Z",
        "summary": "Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code/model are available at\nhttps://github.com/bytedance/OmniScient-Model.",
        "pdf_link": "https://arxiv.org/pdf/2311.08400v1.pdf"
    },
    {
        "title": "Are Large Language Models Temporally Grounded?",
        "authors": [
            "Yifu Qiu",
            "Zheng Zhao",
            "Yftah Ziser",
            "Anna Korhonen",
            "Edoardo M. Ponti",
            "Shay B. Cohen"
        ],
        "published": "2023-11-14T18:57:15Z",
        "summary": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.",
        "pdf_link": "https://arxiv.org/pdf/2311.08398v2.pdf"
    },
    {
        "title": "On What Basis? Predicting Text Preference Via Structured Comparative Reasoning",
        "authors": [
            "Jing Nathan Yan",
            "Tianqi Liu",
            "Justin T Chiu",
            "Jiaming Shen",
            "Zhen Qin",
            "Yue Yu",
            "Yao Zhao",
            "Charu Lakshmanan",
            "Yair Kurzion",
            "Alexander M. Rush",
            "Jialu Liu",
            "Michael Bendersky"
        ],
        "published": "2023-11-14T18:51:38Z",
        "summary": "Comparative reasoning plays a crucial role in text preference prediction;\nhowever, large language models (LLMs) often demonstrate inconsistencies in\ntheir reasoning. While approaches like Chain-of-Thought improve accuracy in\nmany other settings, they struggle to consistently distinguish the similarities\nand differences of complex texts. We introduce SC, a prompting approach that\npredicts text preferences by generating structured intermediate comparisons. SC\nbegins by proposing aspects of comparison, followed by generating textual\ncomparisons under each aspect. We select consistent comparisons with a pairwise\nconsistency comparator that ensures each aspect's comparisons clearly\ndistinguish differences between texts, significantly reducing hallucination and\nimproving consistency. Our comprehensive evaluations across various NLP tasks,\nincluding summarization, retrieval, and automatic rating, demonstrate that SC\nequips LLMs to achieve state-of-the-art performance in text preference\nprediction.",
        "pdf_link": "https://arxiv.org/pdf/2311.08390v1.pdf"
    },
    {
        "title": "SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models",
        "authors": [
            "Bertie Vidgen",
            "Nino Scherrer",
            "Hannah Rose Kirk",
            "Rebecca Qian",
            "Anand Kannappan",
            "Scott A. Hale",
            "Paul R\u00f6ttger"
        ],
        "published": "2023-11-14T18:33:43Z",
        "summary": "The past year has seen rapid acceleration in the development of large\nlanguage models (LLMs). However, without proper steering and safeguards, LLMs\nwill readily follow malicious instructions, provide unsafe advice, and generate\ntoxic content. We introduce SimpleSafetyTests (SST) as a new test suite for\nrapidly and systematically identifying such critical safety risks. The test\nsuite comprises 100 test prompts across five harm areas that LLMs, for the vast\nmajority of applications, should refuse to comply with. We test 11 open-access\nand open-source LLMs and four closed-source LLMs, and find critical safety\nweaknesses. While some of the models do not give a single unsafe response, most\ngive unsafe responses to more than 20% of the prompts, with over 50% unsafe\nresponses in the extreme. Prepending a safety-emphasising system prompt\nsubstantially reduces the occurrence of unsafe responses, but does not\ncompletely stop them from happening. Trained annotators labelled every model\nresponse to SST (n = 3,000). We use these annotations to evaluate five AI\nsafety filters (which assess whether a models' response is unsafe given a\nprompt) as a way of automatically evaluating models' performance on SST. The\nfilters' performance varies considerably. There are also differences across the\nfive harm areas, and on the unsafe versus safe responses. The widely-used\nPerspective API has 72% accuracy and a newly-created zero-shot prompt to\nOpenAI's GPT-4 performs best with 89% accuracy. Content Warning: This paper\ncontains prompts and responses that relate to child abuse, suicide, self-harm\nand eating disorders, scams and fraud, illegal items, and physical harm.",
        "pdf_link": "https://arxiv.org/pdf/2311.08370v2.pdf"
    },
    {
        "title": "GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment",
        "authors": [
            "Senthujan Senkaiahliyan",
            "Augustin Toma",
            "Jun Ma",
            "An-Wen Chan",
            "Andrew Ha",
            "Kevin R. An",
            "Hrishikesh Suresh",
            "Barry Rubin",
            "Bo Wang"
        ],
        "published": "2023-11-14T17:06:09Z",
        "summary": "OpenAI's large multimodal model, GPT-4V(ision), was recently developed for\ngeneral image interpretation. However, less is known about its capabilities\nwith medical image interpretation and diagnosis. Board-certified physicians and\nsenior residents assessed GPT-4V's proficiency across a range of medical\nconditions using imaging modalities such as CT scans, MRIs, ECGs, and clinical\nphotographs. Although GPT-4V is able to identify and explain medical images,\nits diagnostic accuracy and clinical decision-making abilities are poor, posing\nrisks to patient safety. Despite the potential that large language models may\nhave in enhancing medical education and delivery, the current limitations of\nGPT-4V in interpreting medical images reinforces the importance of appropriate\ncaution when using it for clinical decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2403.12046v1.pdf"
    },
    {
        "title": "Extrinsically-Focused Evaluation of Omissions in Medical Summarization",
        "authors": [
            "Elliot Schumacher",
            "Daniel Rosenthal",
            "Varun Nair",
            "Luladay Price",
            "Geoffrey Tso",
            "Anitha Kannan"
        ],
        "published": "2023-11-14T16:46:15Z",
        "summary": "The goal of automated summarization techniques (Paice, 1990; Kupiec et al,\n1995) is to condense text by focusing on the most critical information.\nGenerative large language models (LLMs) have shown to be robust summarizers,\nyet traditional metrics struggle to capture resulting performance (Goyal et al,\n2022) in more powerful LLMs. In safety-critical domains such as medicine, more\nrigorous evaluation is required, especially given the potential for LLMs to\nomit important information in the resulting summary. We propose MED-OMIT, a new\nomission benchmark for medical summarization. Given a doctor-patient\nconversation and a generated summary, MED-OMIT categorizes the chat into a set\nof facts and identifies which are omitted from the summary. We further propose\nto determine fact importance by simulating the impact of each fact on a\ndownstream clinical task: differential diagnosis (DDx) generation. MED-OMIT\nleverages LLM prompt-based approaches which categorize the importance of facts\nand cluster them as supporting or negating evidence to the diagnosis. We\nevaluate MED-OMIT on a publicly-released dataset of patient-doctor\nconversations and find that MED-OMIT captures omissions better than alternative\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.08303v1.pdf"
    },
    {
        "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
        "authors": [
            "Jiahui Geng",
            "Fengyu Cai",
            "Yuxia Wang",
            "Heinz Koeppl",
            "Preslav Nakov",
            "Iryna Gurevych"
        ],
        "published": "2023-11-14T16:43:29Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.",
        "pdf_link": "https://arxiv.org/pdf/2311.08298v2.pdf"
    },
    {
        "title": "How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions",
        "authors": [
            "Houquan Zhou",
            "Yang Hou",
            "Zhenghua Li",
            "Xuebin Wang",
            "Zhefeng Wang",
            "Xinyu Duan",
            "Min Zhang"
        ],
        "published": "2023-11-14T16:30:36Z",
        "summary": "While recent advancements in large language models (LLMs) bring us closer to\nachieving artificial general intelligence, the question persists: Do LLMs truly\nunderstand language, or do they merely mimic comprehension through pattern\nrecognition? This study seeks to explore this question through the lens of\nsyntax, a crucial component of sentence comprehension. Adopting a natural\nlanguage question-answering (Q&A) scheme, we craft questions targeting nine\nsyntactic knowledge points that are most closely related to sentence\ncomprehension. Experiments conducted on 24 LLMs suggest that most have a\nlimited grasp of syntactic knowledge, exhibiting notable discrepancies across\ndifferent syntactic knowledge points. In particular, questions involving\nprepositional phrase attachment pose the greatest challenge, whereas those\nconcerning adjectival modifier and indirect object are relatively easier for\nLLMs to handle. Furthermore, a case study on the training dynamics of the LLMs\nreveals that the majority of syntactic knowledge is learned during the initial\nstages of training, hinting that simply increasing the number of training\ntokens may not be the `silver bullet' for improving the comprehension ability\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.08287v1.pdf"
    },
    {
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
        "authors": [
            "Peng Ding",
            "Jun Kuang",
            "Dan Ma",
            "Xuezhi Cao",
            "Yunsen Xian",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published": "2023-11-14T16:02:16Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.08268v4.pdf"
    },
    {
        "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning",
        "authors": [
            "Shengguang Wu",
            "Keming Lu",
            "Benfeng Xu",
            "Junyang Lin",
            "Qi Su",
            "Chang Zhou"
        ],
        "published": "2023-11-14T14:10:40Z",
        "summary": "Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.",
        "pdf_link": "https://arxiv.org/pdf/2311.08182v1.pdf"
    },
    {
        "title": "Vision-Language Instruction Tuning: A Review and Analysis",
        "authors": [
            "Chen Li",
            "Yixiao Ge",
            "Dian Li",
            "Ying Shan"
        ],
        "published": "2023-11-14T14:02:32Z",
        "summary": "Instruction tuning is a crucial supervised training phase in Large Language\nModels (LLMs), aiming to enhance the LLM's ability to generalize instruction\nexecution and adapt to user preferences. With the increasing integration of\nmulti-modal data into LLMs, there is growing interest in Vision-Language\nInstruction Tuning (VLIT), which presents more complex characteristics compared\nto pure text instruction tuning. In this paper, we systematically review the\nlatest VLIT settings and corresponding datasets in multi-modal LLMs and provide\ninsights into the intrinsic motivations behind their design. For the first\ntime, we offer a detailed multi-perspective categorization for existing VLIT\ndatasets and identify the characteristics that high-quality VLIT data should\npossess. By incorporating these characteristics as guiding principles into the\nexisting VLIT data construction process, we conduct extensive experiments and\nverify their positive impact on the performance of tuned multi-modal LLMs.\nFurthermore, we discuss the current challenges and future research directions\nof VLIT, providing insights for the continuous development of this field. The\ncode and dataset related to this paper have been open-sourced at\nhttps://github.com/palchenli/VL-Instruction-Tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.08172v2.pdf"
    },
    {
        "title": "MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",
        "authors": [
            "Bo Ni",
            "Markus J. Buehler"
        ],
        "published": "2023-11-14T13:49:03Z",
        "summary": "Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.",
        "pdf_link": "https://arxiv.org/pdf/2311.08166v1.pdf"
    },
    {
        "title": "Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
        "authors": [
            "Lei Lin",
            "Jiayi Fu",
            "Pengli Liu",
            "Qingyang Li",
            "Yan Gong",
            "Junchen Wan",
            "Fuzheng Zhang",
            "Zhongyuan Wang",
            "Di Zhang",
            "Kun Gai"
        ],
        "published": "2023-11-14T13:30:54Z",
        "summary": "Although chain-of-thought (CoT) prompting combined with language models has\nachieved encouraging results on complex reasoning tasks, the naive greedy\ndecoding used in CoT prompting usually causes the repetitiveness and local\noptimality. To address this shortcoming, ensemble-optimization tries to obtain\nmultiple reasoning paths to get the final answer assembly. However, current\nensemble-optimization methods either simply employ rule-based post-processing\nsuch as \\textit{self-consistency}, or train an additional model based on\nseveral task-related human annotations to select the best one among multiple\nreasoning paths, yet fail to generalize to realistic settings where the type of\ninput questions is unknown or the answer format of reasoning paths is unknown.\nTo avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable\nensemble-optimization method applying in almost all scenarios where the type of\ninput questions and the answer format of reasoning paths may be known or\nunknown. Self-agreement firstly samples from language model's decoder to\ngenerate a \\textit{diverse} set of reasoning paths, and subsequently prompts\nthe language model \\textit{one more time} to determine the optimal answer by\nselecting the most \\textit{agreed} answer among the sampled reasoning paths.\nSelf-agreement simultaneously achieves remarkable performance on six public\nreasoning benchmarks and superior generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.08154v2.pdf"
    },
    {
        "title": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge",
        "authors": [
            "Yi Liu",
            "Lianzhe Huang",
            "Shicheng Li",
            "Sishuo Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023-11-14T13:24:19Z",
        "summary": "LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.",
        "pdf_link": "https://arxiv.org/pdf/2311.08147v1.pdf"
    },
    {
        "title": "Insights into Classifying and Mitigating LLMs' Hallucinations",
        "authors": [
            "Alessandro Bruno",
            "Pier Luigi Mazzeo",
            "Aladine Chetouani",
            "Marouane Tliba",
            "Mohamed Amine Kerkouri"
        ],
        "published": "2023-11-14T12:30:28Z",
        "summary": "The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2311.08117v1.pdf"
    },
    {
        "title": "Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models",
        "authors": [
            "Yujin Kim",
            "Jaehong Yoon",
            "Seonghyeon Ye",
            "Sung Ju Hwang",
            "Se-young Yun"
        ],
        "published": "2023-11-14T12:12:02Z",
        "summary": "In an ever-evolving world, the dynamic nature of knowledge presents\nchallenges for language models that are trained on static data, leading to\noutdated encoded information. However, real-world scenarios require models not\nonly to acquire new knowledge but also to overwrite outdated information into\nupdated ones. To address this under-explored issue, we introduce the temporally\nevolving question answering benchmark, EvolvingQA - a novel benchmark designed\nfor training and evaluating LMs on an evolving Wikipedia database, where the\nconstruction of our benchmark is automated with our pipeline using large\nlanguage models. Our benchmark incorporates question-answering as a downstream\ntask to emulate real-world applications. Through EvolvingQA, we uncover that\nexisting continual learning baselines have difficulty in updating and\nforgetting outdated knowledge. Our findings suggest that the models fail to\nlearn updated knowledge due to the small weight gradient. Furthermore, we\nelucidate that the models struggle mostly on providing numerical or temporal\nanswers to questions asking for updated knowledge. Our work aims to model the\ndynamic nature of real-world information, offering a robust measure for the\nevolution-adaptability of language models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08106v1.pdf"
    },
    {
        "title": "Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts",
        "authors": [
            "Leonardo Ranaldi",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2023-11-14T11:49:43Z",
        "summary": "Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large\nLanguage Models (LLMs), eliciting them to solve complex reasoning tasks\nstep-by-step. However, with the success of CoT methods, the ability to deliver\nmulti-step reasoning remains limited to English due to the imbalance in the\ndistribution of the pre-training data, making the other languages a barrier.\n  In this work, we propose a Cross-lingual multi-step reasoning approach,\naiming to align reasoning processes across different languages. In particular,\nour method, through a Self-consistent Cross-lingual prompting mechanism\ninspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths\nin different languages that, during the steps, lead to the final solution. Our\nexperimental evaluations show that our method significantly outperforms\nexisting prompting methods, reducing the number of interactions and achieving\nstate-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.08097v1.pdf"
    },
    {
        "title": "Adversarial Preference Optimization",
        "authors": [
            "Pengyu Cheng",
            "Yifan Yang",
            "Jian Li",
            "Yong Dai",
            "Tianhao Hu",
            "Peixin Cao",
            "Nan Du"
        ],
        "published": "2023-11-14T10:10:31Z",
        "summary": "Human preference alignment is essential to improve the interaction quality of\nlarge language models (LLMs). Existing aligning methods depend on manually\nannotated preference data to guide the LLM optimization directions. However, in\npractice, continuously updating LLMs raises a distribution gap between\nmodel-generated samples and human-preferred responses, which hinders model\nfine-tuning efficiency. To mitigate this issue, previous methods require\nadditional preference annotation on generated samples to adapt the shifted\ndistribution, which consumes a large amount of annotation resources. Targeting\nmore efficient human preference optimization, we propose an adversarial\npreference optimization (APO) framework, where the LLM agent and the preference\nmodel update alternatively via a min-max game. Without additional annotation,\nour APO method can make a self-adaption to the generation distribution gap\nthrough the adversarial learning process. Based on comprehensive experiments,\nwe find APO further enhances the alignment performance of baseline methods in\nterms of helpfulness and harmlessness. The code is at\nhttps://github.com/Linear95/APO.",
        "pdf_link": "https://arxiv.org/pdf/2311.08045v3.pdf"
    },
    {
        "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
        "authors": [
            "Vivek Gupta",
            "Pranshu Kandoi",
            "Mahek Bhavesh Vora",
            "Shuo Zhang",
            "Yujie He",
            "Ridho Reinanda",
            "Vivek Srikumar"
        ],
        "published": "2023-11-14T08:57:01Z",
        "summary": "Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08002v1.pdf"
    },
    {
        "title": "How good are Large Language Models on African Languages?",
        "authors": [
            "Jessica Ojo",
            "Kelechi Ogueji",
            "Pontus Stenetorp",
            "David I. Adelani"
        ],
        "published": "2023-11-14T08:10:14Z",
        "summary": "Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and\nlanguages. Additionally, they have been widely adopted as\nlanguage-model-as-a-service commercial APIs like GPT-4 API. However, their\nperformance on African languages is largely unknown. We present an analysis of\nthree popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks\n(news topic classification, sentiment classification, machine translation,\nquestion answering, and named entity recognition) across 30 African languages,\nspanning different language families and geographical regions. Our results\nsuggest that all LLMs produce below-par performance on African languages, and\nthere is a large gap in performance compared to high-resource languages like\nEnglish most tasks. We find that GPT-4 has an average or impressive performance\non classification tasks but very poor results on generative tasks like machine\ntranslation. Surprisingly, we find that mT0 had the best overall on\ncross-lingual QA, better than the state-of-the-art supervised model (i.e.\nfine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the\nworst performance due to its limited multilingual capabilities and\nEnglish-centric pre-training corpus. In general, our findings present a\ncall-to-action to ensure African languages are well represented in large\nlanguage models, given their growing popularity.",
        "pdf_link": "https://arxiv.org/pdf/2311.07978v1.pdf"
    },
    {
        "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Xinyu Pang",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023-11-14T07:13:10Z",
        "summary": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.07954v2.pdf"
    },
    {
        "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
        "authors": [
            "Garima Agrawal",
            "Tharindu Kumarage",
            "Zeyad Alghamdi",
            "Huan Liu"
        ],
        "published": "2023-11-14T05:21:57Z",
        "summary": "The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\ncomprehensively review these knowledge-graph-based augmentation techniques in\nLLMs, focusing on their efficacy in mitigating hallucinations. We\nsystematically categorize these methods into three overarching groups, offering\nmethodological comparisons and performance evaluations. Lastly, this survey\nexplores the current trends and challenges associated with these techniques and\noutlines potential avenues for future research in this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2311.07914v2.pdf"
    },
    {
        "title": "Instruction-Following Evaluation for Large Language Models",
        "authors": [
            "Jeffrey Zhou",
            "Tianjian Lu",
            "Swaroop Mishra",
            "Siddhartha Brahma",
            "Sujoy Basu",
            "Yi Luan",
            "Denny Zhou",
            "Le Hou"
        ],
        "published": "2023-11-14T05:13:55Z",
        "summary": "One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval",
        "pdf_link": "https://arxiv.org/pdf/2311.07911v1.pdf"
    },
    {
        "title": "Fair Abstractive Summarization of Diverse Perspectives",
        "authors": [
            "Yusen Zhang",
            "Nan Zhang",
            "Yixin Liu",
            "Alexander Fabbri",
            "Junru Liu",
            "Ryo Kamoi",
            "Xiaoxin Lu",
            "Caiming Xiong",
            "Jieyu Zhao",
            "Dragomir Radev",
            "Kathleen McKeown",
            "Rui Zhang"
        ],
        "published": "2023-11-14T03:38:55Z",
        "summary": "People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people, and we propose four reference-free automatic metrics by measuring\nthe differences between target and source perspectives. We evaluate nine LLMs,\nincluding three GPT models, four LLaMA models, PaLM 2, and Claude, on six\ndatasets collected from social media, online reviews, and recorded transcripts.\nExperiments show that both the model-generated and the human-written reference\nsummaries suffer from low fairness. We conduct a comprehensive analysis of the\ncommon factors influencing fairness and propose three simple but effective\nmethods to alleviate unfair summarization. Our dataset and code are available\nat https://github.com/psunlpgroup/FairSumm.",
        "pdf_link": "https://arxiv.org/pdf/2311.07884v2.pdf"
    },
    {
        "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
        "authors": [
            "Xiaonan Li",
            "Changtai Zhu",
            "Linyang Li",
            "Zhangyue Yin",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "published": "2023-11-14T01:38:02Z",
        "summary": "Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2311.07838v3.pdf"
    },
    {
        "title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
        "authors": [
            "Alessandro Oltramari"
        ],
        "published": "2023-11-13T21:20:17Z",
        "summary": "High-level reasoning can be defined as the capability to generalize over\nknowledge acquired via experience, and to exhibit robust behavior in novel\nsituations. Such form of reasoning is a basic skill in humans, who seamlessly\nuse it in a broad spectrum of tasks, from language communication to decision\nmaking in complex situations. When it manifests itself in understanding and\nmanipulating the everyday world of objects and their interactions, we talk\nabout common sense or commonsense reasoning. State-of-the-art AI systems don't\npossess such capability: for instance, Large Language Models have recently\nbecome popular by demonstrating remarkable fluency in conversing with humans,\nbut they still make trivial mistakes when probed for commonsense competence; on\na different level, performance degradation outside training data prevents\nself-driving vehicles to safely adapt to unseen scenarios, a serious and\nunsolved problem that limits the adoption of such technology. In this paper we\npropose to enable high-level reasoning in AI systems by integrating cognitive\narchitectures with external neuro-symbolic components. We illustrate a hybrid\nframework centered on ACT-R and we discuss the role of generative models in\nrecent and future applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.07759v1.pdf"
    },
    {
        "title": "On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models",
        "authors": [
            "Naman Goel"
        ],
        "published": "2023-11-13T19:21:25Z",
        "summary": "The surprisingly likely criterion in the seminal work of Prelec (the Bayesian\nTruth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,\nby rewarding rational agents to maximise the expected information gain with\ntheir answers w.r.t. their probabilistic beliefs. We investigate the relevance\nof a similar criterion for responses of LLMs. We hypothesize that if the\nsurprisingly likely criterion works in LLMs, under certain conditions, the\nresponses that maximize the reward under this criterion should be more accurate\nthan the responses that only maximize the posterior probability. Using\nbenchmarks including the TruthfulQA benchmark and using openly available LLMs:\nGPT-2 and LLaMA-2, we show that the method indeed improves the accuracy\nsignificantly (for example, upto 24 percentage points aggregate improvement on\nTruthfulQA and upto 70 percentage points improvement on individual categories\nof questions).",
        "pdf_link": "https://arxiv.org/pdf/2311.07692v1.pdf"
    },
    {
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
        "authors": [
            "Suyu Ge",
            "Chunting Zhou",
            "Rui Hou",
            "Madian Khabsa",
            "Yi-Chia Wang",
            "Qifan Wang",
            "Jiawei Han",
            "Yuning Mao"
        ],
        "published": "2023-11-13T19:13:29Z",
        "summary": "Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.",
        "pdf_link": "https://arxiv.org/pdf/2311.07689v1.pdf"
    },
    {
        "title": "Using Natural Language Explanations to Improve Robustness of In-context Learning for Natural Language Inference",
        "authors": [
            "Xuanli He",
            "Yuxiang Wu",
            "Oana-Maria Camburu",
            "Pasquale Minervini",
            "Pontus Stenetorp"
        ],
        "published": "2023-11-13T18:49:13Z",
        "summary": "Recent studies have demonstrated that large language models (LLMs) excel in\ndiverse tasks through in-context learning (ICL) facilitated by task-specific\nprompts and examples. However, the existing literature shows that ICL\nencounters performance deterioration when exposed to adversarial inputs.\nEnhanced performance has been observed when ICL is augmented with natural\nlanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this work\ninvestigates whether X-ICL can improve the robustness of LLMs on a suite of\nseven adversarial and challenging natural language inference datasets.\nMoreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT in\nour case) with few human-generated NLEs to produce further NLEs (we call it\nChatGPT few-shot), which we show superior to both ChatGPT zero-shot and\nhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,\nLLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shot\nyields over 6% improvement over ICL. Furthermore, while prompt selection\nstrategies were previously shown to significantly improve ICL on\nin-distribution test sets, we show that these strategies do not match the\nefficacy of the X-ICL paradigm in robustness-oriented evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2311.07556v1.pdf"
    },
    {
        "title": "GPT-4V(ision) as A Social Media Analysis Engine",
        "authors": [
            "Hanjia Lyu",
            "Jinfa Huang",
            "Daoan Zhang",
            "Yongsheng Yu",
            "Xinyi Mou",
            "Jinsheng Pan",
            "Zhengyuan Yang",
            "Zhongyu Wei",
            "Jiebo Luo"
        ],
        "published": "2023-11-13T18:36:50Z",
        "summary": "Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.",
        "pdf_link": "https://arxiv.org/pdf/2311.07547v1.pdf"
    },
    {
        "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
        "authors": [
            "Nishant Balepur",
            "Shramay Palta",
            "Rachel Rudinger"
        ],
        "published": "2023-11-13T18:18:22Z",
        "summary": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason\ntoward correct answers, but its efficacy in reasoning toward incorrect answers\nis unexplored. This process of elimination (PoE), when used with COT, can\nenhance self-consistency, interpretability, and tasks such as medical diagnoses\nof exclusion. Thus, we propose PoE with COT, where LLMs must reason toward\nincorrect options on multiple-choice questions. We evaluate the ability of\nGPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four\ncommonsense and scientific reasoning datasets. We find that the strategy of PoE\nalways underperforms the strategy of choosing the correct answer. The agreement\nof these strategies is also lower than the self-consistency of each strategy.\nTo study these issues further, we conduct error analyses and give suggestions\nfor future work.",
        "pdf_link": "https://arxiv.org/pdf/2311.07532v2.pdf"
    },
    {
        "title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
        "authors": [
            "Hejing Cao",
            "Zhenwei An",
            "Jiazhan Feng",
            "Kun Xu",
            "Liwei Chen",
            "Dongyan Zhao"
        ],
        "published": "2023-11-13T17:28:03Z",
        "summary": "While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.",
        "pdf_link": "https://arxiv.org/pdf/2311.07491v1.pdf"
    },
    {
        "title": "InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models",
        "authors": [
            "Ken E. Friedl",
            "Abbas Goher Khan",
            "Soumya Ranjan Sahoo",
            "Md Rashad Al Hasan Rony",
            "Jana Germies",
            "Christian S\u00fc\u00df"
        ],
        "published": "2023-11-13T17:02:06Z",
        "summary": "The assessment of advanced generative large language models (LLMs) poses a\nsignificant challenge, given their heightened complexity in recent\ndevelopments. Furthermore, evaluating the performance of LLM-based applications\nin various industries, as indicated by Key Performance Indicators (KPIs), is a\ncomplex undertaking. This task necessitates a profound understanding of\nindustry use cases and the anticipated system behavior. Within the context of\nthe automotive industry, existing evaluation metrics prove inadequate for\nassessing in-car conversational question answering (ConvQA) systems. The unique\ndemands of these systems, where answers may relate to driver or car safety and\nare confined within the car domain, highlight the limitations of current\nmetrics. To address these challenges, this paper introduces a set of KPIs\ntailored for evaluating the performance of in-car ConvQA systems, along with\ndatasets specifically designed for these KPIs. A preliminary and comprehensive\nempirical evaluation substantiates the efficacy of our proposed approach.\nFurthermore, we investigate the impact of employing varied personas in prompts\nand found that it enhances the model's capacity to simulate diverse viewpoints\nin assessments, mirroring how individuals with different backgrounds perceive a\ntopic.",
        "pdf_link": "https://arxiv.org/pdf/2311.07469v2.pdf"
    },
    {
        "title": "Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse",
        "authors": [
            "Ang Lv",
            "Kaiyi Zhang",
            "Shufang Xie",
            "Quan Tu",
            "Yuhan Chen",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "published": "2023-11-13T17:01:12Z",
        "summary": "Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B as the answer. However, it may\nencounter confusion when presented with questions concerning B. We contend that\nthe reversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.",
        "pdf_link": "https://arxiv.org/pdf/2311.07468v2.pdf"
    },
    {
        "title": "On Measuring Faithfulness or Self-consistency of Natural Language Explanations",
        "authors": [
            "Letitia Parcalabescu",
            "Anette Frank"
        ],
        "published": "2023-11-13T16:53:51Z",
        "summary": "Large language models (LLMs) can explain their predictions through post-hoc\nor Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably\nsounding explanations that are unfaithful to its underlying reasoning. Recent\nwork has designed tests that aim to judge the faithfulness of post-hoc or CoT\nexplanations. In this work we argue that these faithfulness tests do not\nmeasure faithfulness to the models' inner workings -- but rather their\nself-consistency at output level. Our contributions are three-fold: i) We\nclarify the status of faithfulness tests in view of model explainability,\ncharacterising them as self-consistency tests instead. This assessment we\nunderline by ii) constructing a Comparative Consistency Bank for\nself-consistency tests that for the first time compares existing tests on a\ncommon suite of 11 open LLMs and 5 tasks -- including iii) our new\nself-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a\ntest) of LLM self-consistency. It compares how a model's input contributes to\nthe predicted answer and to generating the explanation. Our fine-grained\nCC-SHAP metric allows us iii) to compare LLM behaviour when making predictions\nand to analyse the effect of other consistency tests at a deeper level, which\ntakes us one step further towards measuring faithfulness by bringing us closer\nto the internals of the model than strictly surface output-oriented tests. Our\ncode is available at \\url{https://github.com/Heidelberg-NLP/CC-SHAP}",
        "pdf_link": "https://arxiv.org/pdf/2311.07466v2.pdf"
    },
    {
        "title": "Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text",
        "authors": [
            "Zhongfei Qing",
            "Zhongang Cai",
            "Zhitao Yang",
            "Lei Yang"
        ],
        "published": "2023-11-13T16:22:38Z",
        "summary": "Generating natural human motion from a story has the potential to transform\nthe landscape of animation, gaming, and film industries. A new and challenging\ntask, Story-to-Motion, arises when characters are required to move to various\nlocations and perform specific motions based on a long text description. This\ntask demands a fusion of low-level control (trajectories) and high-level\ncontrol (motion semantics). Previous works in character control and\ntext-to-motion have addressed related aspects, yet a comprehensive solution\nremains elusive: character control methods do not handle text description,\nwhereas text-to-motion methods lack position constraints and often produce\nunstable motions. In light of these limitations, we propose a novel system that\ngenerates controllable, infinitely long motions and trajectories aligned with\nthe input text. (1) We leverage contemporary Large Language Models to act as a\ntext-driven motion scheduler to extract a series of (text, position, duration)\npairs from long text. (2) We develop a text-driven motion retrieval scheme that\nincorporates motion matching with motion semantic and trajectory constraints.\n(3) We design a progressive mask transformer that addresses common artifacts in\nthe transition motion such as unnatural pose and foot sliding. Beyond its\npioneering role as the first comprehensive solution for Story-to-Motion, our\nsystem undergoes evaluation across three distinct sub-tasks: trajectory\nfollowing, temporal action composition, and motion blending, where it\noutperforms previous state-of-the-art motion synthesis methods across the\nboard. Homepage: https://story2motion.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2311.07446v1.pdf"
    },
    {
        "title": "Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue",
        "authors": [
            "Junkai Zhou",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2023-11-13T16:19:42Z",
        "summary": "The emergence of large language models (LLMs) further improves the\ncapabilities of open-domain dialogue systems and can generate fluent, coherent,\nand diverse responses. However, LLMs still lack a crucial ability:\ncommunication skills. This limitation renders them more like information\nseeking tools rather than anthropomorphic chatbots. Communication skills, such\nas topic transition, proactively asking questions, concept guidance, empathy,\nand summarising often should be taken into consideration, to make LLMs more\nanthropomorphic and proactive during the conversation, thereby increasing the\ninterest of users and attracting them to chat for longer. However, enabling\nthese communication skills in black-box LLMs remains a key challenge because\nthey do not have the same utterance formation mode as real people: think before\nspeaking. Inspired by linguistics and cognitive science, we empower LLMs with\ncommunication skills through inner monologues. To evaluate various\ncommunication skills, we construct a benchmark named Cskills, which can also\nmore comprehensively evaluate the dialogue generation ability of the model.\nExperimental results show that the proposed CSIM strategy improves the backbone\nmodels and outperforms the baselines.",
        "pdf_link": "https://arxiv.org/pdf/2311.07445v2.pdf"
    },
    {
        "title": "AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation",
        "authors": [
            "Junyang Wang",
            "Yuhang Wang",
            "Guohai Xu",
            "Jing Zhang",
            "Yukai Gu",
            "Haitao Jia",
            "Jiaqi Wang",
            "Haiyang Xu",
            "Ming Yan",
            "Ji Zhang",
            "Jitao Sang"
        ],
        "published": "2023-11-13T15:25:42Z",
        "summary": "Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucinations, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of tasks and hallucinations). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including existence,\nattribute and relation hallucination. Based on AMBER, we design a low-cost and\nefficient evaluation pipeline. Additionally, we conduct a comprehensive\nevaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision),\nand also give guideline suggestions for mitigating hallucinations. The data and\ncode of AMBER are available at https://github.com/junyangwang0410/AMBER.",
        "pdf_link": "https://arxiv.org/pdf/2311.07397v2.pdf"
    },
    {
        "title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
        "authors": [
            "Yinghao Li",
            "Haorui Wang",
            "Chao Zhang"
        ],
        "published": "2023-11-13T15:11:26Z",
        "summary": "Large Language Models (LLMs) have shown remarkable proficiency in language\nunderstanding and have been successfully applied to a variety of real-world\ntasks through task-specific fine-tuning or prompt engineering. Despite these\nadvancements, it remains an open question whether LLMs are fundamentally\ncapable of reasoning and planning, or if they primarily rely on recalling and\nsynthesizing information from their training data. In our research, we\nintroduce a novel task -- Minesweeper -- specifically designed in a format\nunfamiliar to LLMs and absent from their training datasets. This task\nchallenges LLMs to identify the locations of mines based on numerical clues\nprovided by adjacent opened cells. Successfully completing this task requires\nan understanding of each cell's state, discerning spatial relationships between\nthe clues and mines, and strategizing actions based on logical deductions drawn\nfrom the arrangement of the cells. Our experiments, including trials with the\nadvanced GPT-4 model, indicate that while LLMs possess the foundational\nabilities required for this task, they struggle to integrate these into a\ncoherent, multi-step logical reasoning process needed to solve Minesweeper.\nThese findings highlight the need for further research to understand and nature\nof reasoning capabilities in LLMs under similar circumstances, and to explore\npathways towards more sophisticated AI reasoning and planning models.",
        "pdf_link": "https://arxiv.org/pdf/2311.07387v1.pdf"
    },
    {
        "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
        "authors": [
            "Ekaterina Fadeeva",
            "Roman Vashurin",
            "Akim Tsvigun",
            "Artem Vazhentsev",
            "Sergey Petrakov",
            "Kirill Fedyanin",
            "Daniil Vasilev",
            "Elizaveta Goncharova",
            "Alexander Panchenko",
            "Maxim Panov",
            "Timothy Baldwin",
            "Artem Shelmanov"
        ],
        "published": "2023-11-13T15:08:59Z",
        "summary": "Recent advancements in the capabilities of large language models (LLMs) have\npaved the way for a myriad of groundbreaking applications in various fields.\nHowever, a significant challenge arises as these models often \"hallucinate\",\ni.e., fabricate facts without providing users an apparent means to discern the\nveracity of their statements. Uncertainty estimation (UE) methods are one path\nto safer, more responsible, and more effective use of LLMs. However, to date,\nresearch on UE methods for LLMs has been focused primarily on theoretical\nrather than engineering contributions. In this work, we tackle this issue by\nintroducing LM-Polygraph, a framework with implementations of a battery of\nstate-of-the-art UE methods for LLMs in text generation tasks, with unified\nprogram interfaces in Python. Additionally, it introduces an extendable\nbenchmark for consistent evaluation of UE techniques by researchers, and a demo\nweb application that enriches the standard chat dialog with confidence scores,\nempowering end-users to discern unreliable responses. LM-Polygraph is\ncompatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and\nGPT-4, and is designed to support future releases of similarly-styled LMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.07383v1.pdf"
    },
    {
        "title": "Do large language models and humans have similar behaviors in causal inference with script knowledge?",
        "authors": [
            "Xudong Hong",
            "Margarita Ryzhova",
            "Daniel Adrian Biondi",
            "Vera Demberg"
        ],
        "published": "2023-11-13T13:05:15Z",
        "summary": "Recently, large pre-trained language models (LLMs) have demonstrated superior\nlanguage understanding abilities, including zero-shot causal reasoning.\nHowever, it is unclear to what extent their capabilities are similar to human\nones. We here study the processing of an event $B$ in a script-based story,\nwhich causally depends on a previous event $A$. In our manipulation, event $A$\nis stated, negated, or omitted in an earlier section of the text. We first\nconducted a self-paced reading experiment, which showed that humans exhibit\nsignificantly longer reading times when causal conflicts exist ($\\neg A\n\\rightarrow B$) than under logical conditions ($A \\rightarrow B$). However,\nreading times remain similar when cause A is not explicitly mentioned,\nindicating that humans can easily infer event B from their script knowledge. We\nthen tested a variety of LLMs on the same data to check to what extent the\nmodels replicate human behavior. Our experiments show that 1) only recent LLMs,\nlike GPT-3 or Vicuna, correlate with human behavior in the $\\neg A \\rightarrow\nB$ condition. 2) Despite this correlation, all models still fail to predict\nthat $nil \\rightarrow B$ is less surprising than $\\neg A \\rightarrow B$,\nindicating that LLMs still have difficulties integrating script knowledge. Our\ncode and collected data set are available at\nhttps://github.com/tony-hong/causal-script.",
        "pdf_link": "https://arxiv.org/pdf/2311.07311v1.pdf"
    },
    {
        "title": "What Large Language Models Bring to Text-rich VQA?",
        "authors": [
            "Xuejing Liu",
            "Wei Tang",
            "Xinzhe Ni",
            "Jinghui Lu",
            "Rui Zhao",
            "Zechao Li",
            "Fei Tan"
        ],
        "published": "2023-11-13T12:52:29Z",
        "summary": "Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.07306v1.pdf"
    },
    {
        "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search",
        "authors": [
            "Huihan Li",
            "Yuting Ning",
            "Zeyi Liao",
            "Siyuan Wang",
            "Xiang Lorraine Li",
            "Ximing Lu",
            "Wenting Zhao",
            "Faeze Brahman",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-11-13T10:56:59Z",
        "summary": "State-of-the-art LLMs outperform humans on reasoning tasks such as Natural\nLanguage Inference. Recent works evaluating LLMs note a marked performance drop\non input data from the low-probability distribution, i.e., the longtail.\nTherefore, we focus on systematically generating statements involving long-tail\ninferential knowledge for more effective evaluation of LLMs in the reasoning\nspace. We first propose a novel framework Logic-Induced- Knowledge-Search\n(LINK) that generates factually correct and long-tail knowledge statements\ngrounded on symbolic rule templates; LINK effectively generates data in the\nlongtail distribution that zero-shot prompted LLMs are unable to reach, and\noutperforms zero-shot GPT4 on factual correctness by 5%. We further use the\ndata generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT)\nthat can be used to evaluate downstream models on the long-tail distribution;\nLINT contains 108K knowledge statements spanning four domains. We use LINT to\ntest LLMs on an entailment classification task and find that model performances\ndrop by as high as 5% in the long-tail distribution compared to head\ndistribution. Our work shows the utility of evaluating models in the long-tail\ndistribution, and calls for more research on generating evaluation data in the\nlong-tail distribution.",
        "pdf_link": "https://arxiv.org/pdf/2311.07237v2.pdf"
    },
    {
        "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
        "authors": [
            "Shuaijie She",
            "Shujian Huang",
            "Xingyun Wang",
            "Yanke Zhou",
            "Jiajun Chen"
        ],
        "published": "2023-11-13T09:32:12Z",
        "summary": "LLMs (Large Language Models) usually interact with users in the form of\ndialogue and generate responses following their instructions, which naturally\nrequire dialogue comprehension abilities. However, dialogue comprehension is a\ngeneral language ability which is hard to be evaluated directly. In this work,\nwe propose to perform the evaluation focusing on the factual consistency issue\nwith the help of the dialogue summarization task. Besides evaluating and\nanalyzing the dialogue summarization performance (DIAC-Sum) of different LLMs,\nwe also derive factual questions from the generated summaries and use them as a\nmore flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation\nshows that, on average, 26.8% of the summaries generated by LLMs contain\nfactual inconsistency. Even ChatGPT, the strongest model evaluated, has such\nerrors in 16% of its summaries. For answering the factual questions, which is\nmore challenging, the average error rate of all evaluated LLMs is 36.1%. Both\nresults indicate serious deficiencies. Detailed analysis shows that the\nunderstanding of subject/object of the conversation is still challenging for\nLLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability\nof LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task\ndata, which achieved a relative error rate reduction of 11% on DIAC-QA.",
        "pdf_link": "https://arxiv.org/pdf/2311.07194v3.pdf"
    },
    {
        "title": "Can LLMs Patch Security Issues?",
        "authors": [
            "Kamel Alrashedy",
            "Abdullah Aljasser"
        ],
        "published": "2023-11-13T08:54:37Z",
        "summary": "Large Language Models (LLMs) have shown impressive proficiency in code\ngeneration. Nonetheless, similar to human developers, these models might\ngenerate code that contains security vulnerabilities and flaws. Writing secure\ncode remains a substantial challenge, as vulnerabilities often arise during\ninteractions between programs and external systems or services, such as\ndatabases and operating systems. In this paper, we propose a novel approach,\nFeedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs\nin receiving feedback from Bandit, which is a static code analysis tool, and\nthen the LLMs generate potential solutions to resolve security vulnerabilities.\nEach solution, along with the vulnerable code, is then sent back to the LLM for\ncode refinement. Our approach shows a significant improvement over the baseline\nand outperforms existing approaches. Furthermore, we introduce a new dataset,\nPythonSecurityEval, collected from real-world scenarios on Stack Overflow to\nevaluate the LLMs' ability to generate secure code. Code and data are available\nat \\url{https://github.com/Kamel773/LLM-code-refine}",
        "pdf_link": "https://arxiv.org/pdf/2312.00024v3.pdf"
    },
    {
        "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
        "authors": [
            "Shangqing Tu",
            "Yuliang Sun",
            "Yushi Bai",
            "Jifan Yu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023-11-13T08:09:01Z",
        "summary": "To mitigate the potential misuse of large language models (LLMs), recent\nresearch has developed watermarking algorithms, which restrict the generation\nprocess to leave an invisible trace for watermark detection. Due to the\ntwo-stage nature of the task, most studies evaluate the generation and\ndetection separately, thereby presenting a challenge in unbiased, thorough, and\napplicable evaluations. In this paper, we introduce WaterBench, the first\ncomprehensive benchmark for LLM watermarks, in which we design three crucial\nfactors: (1) For \\textbf{benchmarking procedure}, to ensure an apples-to-apples\ncomparison, we first adjust each watermarking method's hyper-parameter to reach\nthe same watermarking strength, then jointly evaluate their generation and\ndetection performance. (2) For \\textbf{task selection}, we diversify the input\nand output length to form a five-category taxonomy, covering $9$ tasks. (3) For\n\\textbf{evaluation metric}, we adopt the GPT4-Judge for automatically\nevaluating the decline of instruction-following abilities after watermarking.\nWe evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking\nstrengths and observe the common struggles for current methods on maintaining\nthe generation quality. The code and data are available at\n\\url{https://github.com/THU-KEG/WaterBench}.",
        "pdf_link": "https://arxiv.org/pdf/2311.07138v1.pdf"
    },
    {
        "title": "Towards the Law of Capacity Gap in Distilling Language Models",
        "authors": [
            "Chen Zhang",
            "Dawei Song",
            "Zheyu Ye",
            "Yan Gao"
        ],
        "published": "2023-11-13T03:36:18Z",
        "summary": "Language model (LM) distillation is a trending area that aims to distil the\nknowledge resided in a large teacher LM to a small student one. While various\nmethods have been proposed to push the distillation to its limits, it is still\na pain distilling LMs when a large capacity gap is exhibited between the\nteacher and the student LMs. The pain is mainly resulted by the curse of\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\nbetter student LM than one distilled from a smaller teacher LM due to the\naffect of capacity gap increment. That is, there is likely an optimal point\nyielding the best student LM along the scaling course of the teacher LM. Even\nworse, the curse of capacity gap can be only partly yet not fully lifted as\nindicated in previous studies.\n  However, the tale is not ever one-sided. Although a larger teacher LM has\nbetter performance than a smaller teacher LM, it is much more\nresource-demanding especially in the context of recent large LMs (LLMs).\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\ncapacity gap is almost consistent across different student scales and\narchitectures, fortunately turning the curse into the law of capacity gap. The\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\ncompute-performance pareto frontier among existing 3B LMs on commonly used\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\nwide range of 3B competitors in GPT4 evaluation and could even compete with\nseveral 7B chat models.",
        "pdf_link": "https://arxiv.org/pdf/2311.07052v1.pdf"
    },
    {
        "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook",
        "authors": [
            "Wangtao Sun",
            "Xuanqing Yu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-11-13T02:31:16Z",
        "summary": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote",
        "pdf_link": "https://arxiv.org/pdf/2311.07032v1.pdf"
    },
    {
        "title": "SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions by Themselves",
        "authors": [
            "Jiachen Zhao",
            "Zonghai Yao",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023-11-12T23:14:43Z",
        "summary": "Large language models (LLMs) can generate intermediate reasoning steps. To\nelicit the reliable reasoning, the common practice is to employ few-shot\nchain-of-thought prompting, where several in-context demonstrations for\nreasoning are prepended to the question. However, such chain-of-thought\nexamples are expensive to craft, especially for professional domains, and can\nhave high variance depending on human annotators. Therefore, this work\ninvestigates whether LLMs can teach themselves to reason without human-crafted\ndemonstrations. We propose SELF-EXPLAIN to generate CoT examples by LLMs\ninspired by \"encoding specificity\" in human memory retrieval. We find using\nself-explanations makes LLMs more confident, more calibrated and less biased\nwhen answering complex questions. Moreover, we find prompting with\nself-explanations can even significantly outperform using human-crafted CoTs on\nseveral complex question answering dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.06985v1.pdf"
    },
    {
        "title": "Assessing the Interpretability of Programmatic Policies with Large Language Models",
        "authors": [
            "Zahra Bashir",
            "Michael Bowling",
            "Levi H. S. Lelis"
        ],
        "published": "2023-11-12T22:43:26Z",
        "summary": "Although the synthesis of programs encoding policies often carries the\npromise of interpretability, systematic evaluations were never performed to\nassess the interpretability of these policies, likely because of the complexity\nof such an evaluation. In this paper, we introduce a novel metric that uses\nlarge-language models (LLM) to assess the interpretability of programmatic\npolicies. For our metric, an LLM is given both a program and a description of\nits associated programming language. The LLM then formulates a natural language\nexplanation of the program. This explanation is subsequently fed into a second\nLLM, which tries to reconstruct the program from the natural-language\nexplanation. Our metric then measures the behavioral similarity between the\nreconstructed program and the original. We validate our approach with\nsynthesized and human-crafted programmatic policies for playing a real-time\nstrategy game, comparing the interpretability scores of these programmatic\npolicies to obfuscated versions of the same programs. Our LLM-based\ninterpretability score consistently ranks less interpretable programs lower and\nmore interpretable ones higher. These findings suggest that our metric could\nserve as a reliable and inexpensive tool for evaluating the interpretability of\nprogrammatic policies.",
        "pdf_link": "https://arxiv.org/pdf/2311.06979v2.pdf"
    },
    {
        "title": "Flames: Benchmarking Value Alignment of LLMs in Chinese",
        "authors": [
            "Kexin Huang",
            "Xiangyang Liu",
            "Qianyu Guo",
            "Tianxiang Sun",
            "Jiawei Sun",
            "Yaru Wang",
            "Zeyang Zhou",
            "Yixu Wang",
            "Yan Teng",
            "Xipeng Qiu",
            "Yingchun Wang",
            "Dahua Lin"
        ],
        "published": "2023-11-12T17:18:21Z",
        "summary": "The widespread adoption of large language models (LLMs) across various\nregions underscores the urgent need to evaluate their alignment with human\nvalues. Current benchmarks, however, fall short of effectively uncovering\nsafety vulnerabilities in LLMs. Despite numerous models achieving high scores\nand 'topping the chart' in these evaluations, there is still a significant gap\nin LLMs' deeper alignment with human values and achieving genuine harmlessness.\nTo this end, this paper proposes a value alignment benchmark named Flames,\nwhich encompasses both common harmlessness principles and a unique morality\ndimension that integrates specific Chinese values such as harmony. Accordingly,\nwe carefully design adversarial prompts that incorporate complex scenarios and\njailbreaking methods, mostly with implicit malice. By prompting 17 mainstream\nLLMs, we obtain model responses and rigorously annotate them for detailed\nevaluation. Our findings indicate that all the evaluated LLMs demonstrate\nrelatively poor performance on Flames, particularly in the safety and fairness\ndimensions. We also develop a lightweight specified scorer capable of scoring\nLLMs across multiple dimensions to efficiently evaluate new models on the\nbenchmark. The complexity of Flames has far exceeded existing benchmarks,\nsetting a new challenge for contemporary LLMs and highlighting the need for\nfurther alignment of LLMs. Our benchmark is publicly available at\nhttps://github.com/AIFlames/Flames.",
        "pdf_link": "https://arxiv.org/pdf/2311.06899v3.pdf"
    },
    {
        "title": "Can Large Language Models Augment a Biomedical Ontology with missing Concepts and Relations?",
        "authors": [
            "Antonio Zaitoun",
            "Tomer Sagi",
            "Szymon Wilk",
            "Mor Peleg"
        ],
        "published": "2023-11-12T14:20:55Z",
        "summary": "Ontologies play a crucial role in organizing and representing knowledge.\nHowever, even current ontologies do not encompass all relevant concepts and\nrelationships. Here, we explore the potential of large language models (LLM) to\nexpand an existing ontology in a semi-automated fashion. We demonstrate our\napproach on the biomedical ontology SNOMED-CT utilizing semantic relation types\nfrom the widely used UMLS semantic network. We propose a method that uses\nconversational interactions with an LLM to analyze clinical practice guidelines\n(CPGs) and detect the relationships among the new medical concepts that are not\npresent in SNOMED-CT. Our initial experimentation with the conversational\nprompts yielded promising preliminary results given a manually generated gold\nstandard, directing our future potential improvements.",
        "pdf_link": "https://arxiv.org/pdf/2311.06858v1.pdf"
    },
    {
        "title": "Evaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling",
        "authors": [
            "Yujin Cho",
            "Mingeon Kim",
            "Seojin Kim",
            "Oyun Kwon",
            "Ryan Donghan Kwon",
            "Yoonha Lee",
            "Dohyun Lim"
        ],
        "published": "2023-11-12T07:55:39Z",
        "summary": "This study investigates the efficacy of Large Language Models (LLMs) in\ninteractive language therapy for high-functioning autistic adolescents. With\nthe rapid advancement of artificial intelligence, particularly in natural\nlanguage processing, LLMs present a novel opportunity to augment traditional\npsychological counseling methods. This research primarily focuses on evaluating\nthe LLM's ability to engage in empathetic, adaptable, and contextually\nappropriate interactions within a therapeutic setting. A comprehensive\nevaluation was conducted by a panel of clinical psychologists and psychiatrists\nusing a specially developed scorecard. The assessment covered various aspects\nof the LLM's performance, including empathy, communication skills,\nadaptability, engagement, and the ability to establish a therapeutic alliance.\nThe study avoided direct testing with patients, prioritizing privacy and\nethical considerations, and instead relied on simulated scenarios to gauge the\nLLM's effectiveness. The results indicate that LLMs hold significant promise as\nsupportive tools in therapy, demonstrating strengths in empathetic engagement\nand adaptability in conversation. However, challenges in achieving the depth of\npersonalization and emotional understanding characteristic of human therapists\nwere noted. The study also highlights the importance of ethical considerations\nin the application of AI in therapeutic contexts. This research provides\nvaluable insights into the potential and limitations of using LLMs in\npsychological counseling for autistic adolescents. It lays the groundwork for\nfuture explorations into AI's role in mental health care, emphasizing the need\nfor ongoing development to enhance the capabilities of these models in\ntherapeutic settings.",
        "pdf_link": "https://arxiv.org/pdf/2311.09243v1.pdf"
    },
    {
        "title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding",
        "authors": [
            "Ying Su",
            "Xiaojin Fu",
            "Mingwen Liu",
            "Zhijiang Guo"
        ],
        "published": "2023-11-12T05:12:49Z",
        "summary": "Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.",
        "pdf_link": "https://arxiv.org/pdf/2311.06736v1.pdf"
    },
    {
        "title": "Trusted Source Alignment in Large Language Models",
        "authors": [
            "Vasilisa Bashlovkina",
            "Zhaobin Kuang",
            "Riley Matthews",
            "Edward Clifford",
            "Yennie Jun",
            "William W. Cohen",
            "Simon Baumgartner"
        ],
        "published": "2023-11-12T00:25:25Z",
        "summary": "Large language models (LLMs) are trained on web-scale corpora that inevitably\ninclude contradictory factual information from sources of varying reliability.\nIn this paper, we propose measuring an LLM property called trusted source\nalignment (TSA): the model's propensity to align with content produced by\ntrusted publishers in the face of uncertainty or controversy. We present\nFactCheckQA, a TSA evaluation dataset based on a corpus of fact checking\narticles. We describe a simple protocol for evaluating TSA and offer a detailed\nanalysis of design considerations including response extraction, claim\ncontextualization, and bias in prompt formulation. Applying the protocol to\nPaLM-2, we find that as we scale up the model size, the model performance on\nFactCheckQA improves from near-random to up to 80% balanced accuracy in\naligning with trusted sources.",
        "pdf_link": "https://arxiv.org/pdf/2311.06697v1.pdf"
    },
    {
        "title": "Intentional Biases in LLM Responses",
        "authors": [
            "Nicklaus Badyal",
            "Derek Jacoby",
            "Yvonne Coady"
        ],
        "published": "2023-11-11T19:59:24Z",
        "summary": "In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.",
        "pdf_link": "https://arxiv.org/pdf/2311.07611v1.pdf"
    },
    {
        "title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance",
        "authors": [
            "Rik Koncel-Kedziorski",
            "Michael Krumdick",
            "Viet Lai",
            "Varshini Reddy",
            "Charles Lovering",
            "Chris Tanner"
        ],
        "published": "2023-11-11T16:16:11Z",
        "summary": "Answering questions within business and finance requires reasoning,\nprecision, and a wide-breadth of technical knowledge. Together, these\nrequirements make this domain difficult for large language models (LLMs). We\nintroduce BizBench, a benchmark for evaluating models' ability to reason about\nrealistic financial problems. BizBench comprises eight quantitative reasoning\ntasks, focusing on question-answering (QA) over financial data via program\nsynthesis. We include three financially-themed code-generation tasks from newly\ncollected and augmented QA data. Additionally, we isolate the reasoning\ncapabilities required for financial QA: reading comprehension of financial text\nand tables for extracting intermediate values, and understanding financial\nconcepts and formulas needed to calculate complex solutions. Collectively,\nthese tasks evaluate a model's financial background knowledge, ability to parse\nfinancial documents, and capacity to solve problems with code. We conduct an\nin-depth evaluation of open-source and commercial LLMs, comparing and\ncontrasting the behavior of code-focused and language-focused models. We\ndemonstrate that the current bottleneck in performance is due to LLMs' limited\nbusiness and financial understanding, highlighting the value of a challenging\nbenchmark for quantitative reasoning within this domain.",
        "pdf_link": "https://arxiv.org/pdf/2311.06602v2.pdf"
    },
    {
        "title": "Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study",
        "authors": [
            "Maarten De Raedt",
            "Semere Kiros Bitew",
            "Fr\u00e9deric Godin",
            "Thomas Demeester",
            "Chris Develder"
        ],
        "published": "2023-11-11T11:56:56Z",
        "summary": "The brittleness of finetuned language model performance on\nout-of-distribution (OOD) test samples in unseen domains has been well-studied\nfor English, yet is unexplored for multi-lingual models. Therefore, we study\ngeneralization to OOD test data specifically in zero-shot cross-lingual\ntransfer settings, analyzing performance impacts of both language and domain\nshifts between train and test data. We further assess the effectiveness of\ncounterfactually augmented data (CAD) in improving OOD generalization for the\ncross-lingual setting, since CAD has been shown to benefit in a monolingual\nEnglish setting. Finally, we propose two new approaches for OOD generalization\nthat avoid the costly annotation process associated with CAD, by exploiting the\npower of recent large language models (LLMs). We experiment with 3 multilingual\nmodels, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and\nevaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and\nRestaurant reviews. Results echo the OOD performance decline observed in the\nmonolingual English setting. Further, (i) counterfactuals from the original\nhigh-resource language do improve OOD generalization in the low-resource\nlanguage, and (ii) our newly proposed cost-effective approaches reach similar\nor up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.",
        "pdf_link": "https://arxiv.org/pdf/2311.06549v1.pdf"
    },
    {
        "title": "CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset",
        "authors": [
            "Le Chen",
            "Arijit Bhattacharjee",
            "Nesreen K. Ahmed",
            "Niranjan Hasabnis",
            "Gal Oren",
            "Bin Lei",
            "Ali Jannesari"
        ],
        "published": "2023-11-11T08:21:52Z",
        "summary": "Large language models (LLMs) have become increasingly prominent in academia\nand industry due to their remarkable performance in diverse applications. As\nthese models evolve with increasing parameters, they excel in tasks like\nsentiment analysis and machine translation. However, even models with billions\nof parameters face challenges in tasks demanding multi-step reasoning. Code\ngeneration and comprehension, especially in C and C++, emerge as significant\nchallenges. While LLMs trained on code datasets demonstrate competence in many\ntasks, they struggle with rectifying non-compilable C and C++ code. Our\ninvestigation attributes this subpar performance to two primary factors: the\nquality of the training dataset and the inherent complexity of the problem\nwhich demands intricate reasoning. Existing \"Chain of Thought\" (CoT) prompting\ntechniques aim to enhance multi-step reasoning. This approach, however, retains\nthe limitations associated with the latent drawbacks of LLMs. In this work, we\npropose CompCodeVet, a compiler-guided CoT approach to produce compilable code\nfrom non-compilable ones. Diverging from the conventional approach of utilizing\nlarger LLMs, we employ compilers as a teacher to establish a more robust\nzero-shot thought process. The evaluation of CompCodeVet on two open-source\ncode datasets shows that CompCodeVet has the ability to improve the training\ndataset quality for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.06505v1.pdf"
    },
    {
        "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering",
        "authors": [
            "Yichi Zhang",
            "Zhuo Chen",
            "Yin Fang",
            "Lei Cheng",
            "Yanxi Lu",
            "Fangming Li",
            "Wen Zhang",
            "Huajun Chen"
        ],
        "published": "2023-11-11T07:56:40Z",
        "summary": "Recently, the development of large language models (LLMs) has attracted wide\nattention in academia and industry. Deploying LLMs to real scenarios is one of\nthe key directions in the current Internet industry. In this paper, we present\na novel pipeline to apply LLMs for domain-specific question answering (QA) that\nincorporates domain knowledge graphs (KGs), addressing an important direction\nof LLM application. As a real-world application, the content generated by LLMs\nshould be user-friendly to serve the customers. Additionally, the model needs\nto utilize domain knowledge properly to generate reliable answers. These two\nissues are the two major difficulties in the LLM application as vanilla\nfine-tuning can not adequately address them. We think both requirements can be\nunified as the model preference problem that needs to align with humans to\nachieve practical application. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference set called style\npreference set and knowledge preference set respectively to tackle the two\nissues. Besides, we design a new alignment objective to align the LLM\npreference with human preference, aiming to train a better LLM for\nreal-scenario domain-specific QA to generate reliable and user-friendly\nanswers. Adequate experiments and comprehensive with 15 baseline methods\ndemonstrate that our KnowPAT is an outperforming pipeline for real-scenario\ndomain-specific QA with LLMs. Our code is open-source at\nhttps://github.com/zjukg/KnowPAT.",
        "pdf_link": "https://arxiv.org/pdf/2311.06503v1.pdf"
    },
    {
        "title": "Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks",
        "authors": [
            "Pouya Pezeshkpour",
            "Hayate Iso",
            "Thom Lake",
            "Nikita Bhutani",
            "Estevam Hruschka"
        ],
        "published": "2023-11-10T20:25:42Z",
        "summary": "Numerous HR applications are centered around resumes and job descriptions.\nWhile they can benefit from advancements in NLP, particularly large language\nmodels, their real-world adoption faces challenges due to absence of\ncomprehensive benchmarks for various HR tasks, and lack of smaller models with\ncompetitive capabilities. In this paper, we aim to bridge this gap by\nintroducing the Resume-Job Description Benchmark (RJDB). We meticulously craft\nthis benchmark to cater to a wide array of HR tasks, including matching and\nexplaining resumes to job descriptions, extracting skills and experiences from\nresumes, and editing resumes. To create this benchmark, we propose to distill\ndomain-specific knowledge from a large language model (LLM). We rely on a\ncurated skill-occupation graph to ensure diversity and provide context for LLMs\ngeneration. Our benchmark includes over 50 thousand triples of job\ndescriptions, matched resumes and unmatched resumes. Using RJDB, we train\nmultiple smaller student models. Our experiments reveal that the student models\nachieve near/better performance than the teacher model (GPT-4), affirming the\neffectiveness of the benchmark. Additionally, we explore the utility of RJDB on\nout-of-distribution data for skill extraction and resume-job description\nmatching, in zero-shot and weak supervision manner. We release our datasets and\ncode to foster further research and industry applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.06383v1.pdf"
    },
    {
        "title": "ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management",
        "authors": [
            "Angela Zhang",
            "Mert Yuksekgonul",
            "Joshua Guild",
            "James Zou",
            "Joseph C. Wu"
        ],
        "published": "2023-11-10T19:59:36Z",
        "summary": "Recent breakthroughs in large language models (LLMs) have led to their rapid\ndissemination and widespread use. One early application has been to medicine,\nwhere LLMs have been investigated to streamline clinical workflows and\nfacilitate clinical analysis and decision-making. However, a leading barrier to\nthe deployment of Artificial Intelligence (AI) and in particular LLMs has been\nconcern for embedded gender and racial biases. Here, we evaluate whether a\nleading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical\nmanagement of acute coronary syndrome (ACS). We find that specifying patients\nas female, African American, or Hispanic resulted in a decrease in guideline\nrecommended medical management, diagnosis, and symptom management of ACS. Most\nnotably, the largest disparities were seen in the recommendation of coronary\nangiography or stress testing for the diagnosis and further intervention of ACS\nand recommendation of high intensity statins. These disparities correlate with\nbiases that have been observed clinically and have been implicated in the\ndifferential gender and racial morbidity and mortality outcomes of ACS and\ncoronary artery disease. Furthermore, we find that the largest disparities are\nseen during unstable angina, where fewer explicit clinical guidelines exist.\nFinally, we find that through asking ChatGPT 3.5 to explain its reasoning prior\nto providing an answer, we are able to improve clinical accuracy and mitigate\ninstances of gender and racial biases. This is among the first studies to\ndemonstrate that the gender and racial biases that LLMs exhibit do in fact\naffect clinical management. Additionally, we demonstrate that existing\nstrategies that improve LLM performance not only improve LLM performance in\nclinical management, but can also be used to mitigate gender and racial biases.",
        "pdf_link": "https://arxiv.org/pdf/2311.14703v1.pdf"
    },
    {
        "title": "Language Models can be Logical Solvers",
        "authors": [
            "Jiazhan Feng",
            "Ruochen Xu",
            "Junheng Hao",
            "Hiteshi Sharma",
            "Yelong Shen",
            "Dongyan Zhao",
            "Weizhu Chen"
        ],
        "published": "2023-11-10T16:23:50Z",
        "summary": "Logical reasoning is a fundamental aspect of human intelligence and a key\ncomponent of tasks like problem-solving and decision-making. Recent\nadvancements have enabled Large Language Models (LLMs) to potentially exhibit\nreasoning capabilities, but complex logical reasoning remains a challenge. The\nstate-of-the-art, solver-augmented language models, use LLMs to parse natural\nlanguage logical questions into symbolic representations first and then adopt\nexternal logical solvers to take in the symbolic representations and output the\nanswers. Despite their impressive performance, any parsing errors will\ninevitably result in the failure of the execution of the external logical\nsolver and no answer to the logical questions. In this paper, we introduce\nLoGiPT, a novel language model that directly emulates the reasoning processes\nof logical solvers and bypasses the parsing errors by learning to strict\nadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly\nconstructed instruction-tuning dataset derived from revealing and refining the\ninvisible reasoning process of deductive solvers. Experimental results on two\npublic deductive reasoning datasets demonstrate that LoGiPT outperforms\nstate-of-the-art solver-augmented LMs and few-shot prompting methods on\ncompetitive LLMs like ChatGPT or GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2311.06158v1.pdf"
    },
    {
        "title": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking",
        "authors": [
            "Lefteris Loukas",
            "Ilias Stogiannidis",
            "Odysseas Diamantopoulos",
            "Prodromos Malakasiotis",
            "Stavros Vassos"
        ],
        "published": "2023-11-10T15:10:36Z",
        "summary": "Standard Full-Data classifiers in NLP demand thousands of labeled examples,\nwhich is impractical in data-limited domains. Few-shot methods offer an\nalternative, utilizing contrastive learning techniques that can be effective\nwith as little as 20 examples per class. Similarly, Large Language Models\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\nHowever, the performance-cost trade-offs of these methods remain underexplored,\na critical concern for budget-limited organizations. Our work addresses this\ngap by studying the aforementioned approaches over the Banking77 financial\nintent detection dataset, including the evaluation of cutting-edge LLMs by\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\ncomplete the picture with two additional methods: first, a cost-effective\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\nreduce operational costs multiple times compared to classic few-shot\napproaches, and second, a data augmentation method using GPT-4, able to improve\nperformance in data-limited scenarios. Finally, to inspire future research, we\nprovide a human expert's curated subset of Banking77, along with extensive\nerror analysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.06102v1.pdf"
    },
    {
        "title": "Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
        "authors": [
            "Wenjie Fu",
            "Huandong Wang",
            "Chen Gao",
            "Guanghua Liu",
            "Yong Li",
            "Tao Jiang"
        ],
        "published": "2023-11-10T13:55:05Z",
        "summary": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.",
        "pdf_link": "https://arxiv.org/pdf/2311.06062v2.pdf"
    },
    {
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences",
        "authors": [
            "Yuanhe Tian",
            "Ruyi Gan",
            "Yan Song",
            "Jiaxing Zhang",
            "Yongdong Zhang"
        ],
        "published": "2023-11-10T12:25:32Z",
        "summary": "Recently, the increasing demand for superior medical services has highlighted\nthe discrepancies in the medical infrastructure. With big data, especially\ntexts, forming the foundation of medical services, there is an exigent need for\neffective natural language processing (NLP) solutions tailored to the\nhealthcare domain. Conventional approaches leveraging pre-trained models\npresent promising results in this domain and current large language models\n(LLMs) offer advanced foundation for medical text processing. However, most\nmedical LLMs are trained only with supervised fine-tuning (SFT), even though it\nefficiently empowers LLMs to understand and respond to medical instructions but\nis ineffective in learning domain knowledge and aligning with human preference.\nAnother engineering barrier that prevents current medical LLM from better text\nprocessing ability is their restricted context length (e.g., 2,048 tokens),\nmaking it hard for the LLMs to process long context, which is frequently\nrequired in the medical domain. In this work, we propose ChiMed-GPT, a new\nbenchmark LLM designed explicitly for Chinese medical domain, with enlarged\ncontext length to 4,096 tokens and undergoes a comprehensive training regime\nwith pre-training, SFT, and RLHF. Evaluations on real-world tasks including\ninformation extraction, question answering, and dialogue generation demonstrate\nChiMed-GPT's superior performance over general domain LLMs. Furthermore, we\nanalyze possible biases through prompting ChiMed-GPT to perform attitude scales\nregarding discrimination of patients, so as to contribute to further\nresponsible development of LLMs in the medical domain. The code and model are\nreleased at https://github.com/synlp/ChiMed-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2311.06025v2.pdf"
    },
    {
        "title": "How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model",
        "authors": [
            "Shezheng Song",
            "Xiaopeng Li",
            "Shasha Li",
            "Shan Zhao",
            "Jie Yu",
            "Jun Ma",
            "Xiaoguang Mao",
            "Weimin Zhang"
        ],
        "published": "2023-11-10T09:51:24Z",
        "summary": "This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.",
        "pdf_link": "https://arxiv.org/pdf/2311.07594v2.pdf"
    },
    {
        "title": "Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications",
        "authors": [
            "Zhangyin Feng",
            "Weitao Ma",
            "Weijiang Yu",
            "Lei Huang",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting liu"
        ],
        "published": "2023-11-10T05:24:04Z",
        "summary": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2311.05876v2.pdf"
    },
    {
        "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
        "authors": [
            "Yuanmin Tang",
            "Jing Yu",
            "Keke Gai",
            "Xiangyan Qu",
            "Yue Hu",
            "Gang Xiong",
            "Qi Wu"
        ],
        "published": "2023-11-10T04:27:27Z",
        "summary": "Recent advances in vision-language pre-trained models (VLPs) have\nsignificantly increased visual understanding and cross-modal analysis\ncapabilities. Companies have emerged to provide multi-modal Embedding as a\nService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount\nof training data and resources for high-performance service. However, existing\nstudies indicate that EaaS is vulnerable to model extraction attacks that\ninduce great loss for the owners of VLPs. Protecting the intellectual property\nand commercial ownership of VLPs is increasingly crucial yet challenging. A\nmajor solution of watermarking model for EaaS implants a backdoor in the model\nby inserting verifiable trigger embeddings into texts, but it is only\napplicable for large language models and is unrealistic due to data and model\nprivacy. In this paper, we propose a safe and robust backdoor-based embedding\nwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding\northogonal transformation to effectively inject triggers into the VLPs without\ninterfering with the model parameters, which achieves high-quality copyright\nverification and minimal impact on model performance. To enhance the watermark\nrobustness, we further propose a collaborative copyright verification strategy\nbased on both backdoor trigger and embedding distribution, enhancing resilience\nagainst various attacks. We increase the watermark practicality via an\nout-of-distribution trigger selection approach, removing access to the model\ntraining data and thus making it possible for many real-world scenarios. Our\nextensive experiments on various datasets indicate that the proposed\nwatermarking approach is effective and safe for verifying the copyright of VLPs\nfor multi-modal EaaS and robust against model extraction attacks. Our code is\navailable at https://github.com/Pter61/vlpmarker.",
        "pdf_link": "https://arxiv.org/pdf/2311.05863v1.pdf"
    },
    {
        "title": "Tamil-Llama: A New Tamil Language Model Based on Llama 2",
        "authors": [
            "Abhinand Balachandran"
        ],
        "published": "2023-11-10T03:02:39Z",
        "summary": "Language modeling has witnessed remarkable advancements in recent years, with\nLarge Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in\nhuman-like text generation. However, a prevailing limitation is the\nunderrepresentation of languages like Tamil in these cutting-edge models,\nleading to suboptimal performance in diverse linguistic contexts. This paper\naddresses this lacuna, enhancing the open-source LLaMA model with an addition\nof 16,000 Tamil tokens, aiming to achieve superior text generation and\ncomprehension in the Tamil language. We strategically employ the LoRA\nmethodology for efficient model training on a comprehensive Tamil corpus,\nensuring computational feasibility and model robustness. Moreover, we introduce\na Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca\ndataset tailored for instruction fine-tuning. Our results showcase significant\nperformance improvements in Tamil text generation, with potential implications\nfor the broader landscape of LLMs in Indian languages. We further underscore\nour commitment to open research by making our models, datasets, and code\npublicly accessible, fostering further innovations in language modeling.",
        "pdf_link": "https://arxiv.org/pdf/2311.05845v1.pdf"
    },
    {
        "title": "Hallucination-minimized Data-to-answer Framework for Financial Decision-makers",
        "authors": [
            "Sohini Roychowdhury",
            "Andres Alvarez",
            "Brian Moore",
            "Marko Krema",
            "Maria Paz Gelpi",
            "Federico Martin Rodriguez",
            "Angel Rodriguez",
            "Jose Ramon Cabrejas",
            "Pablo Martinez Serrano",
            "Punit Agrawal",
            "Arijit Mukherjee"
        ],
        "published": "2023-11-09T22:53:52Z",
        "summary": "Large Language Models (LLMs) have been applied to build several automation\nand personalized question-answering prototypes so far. However, scaling such\nprototypes to robust products with minimized hallucinations or fake responses\nstill remains an open challenge, especially in niche data-table heavy domains\nsuch as financial decision making. In this work, we present a novel\nLangchain-based framework that transforms data tables into hierarchical textual\ndata chunks to enable a wide variety of actionable question answering. First,\nthe user-queries are classified by intention followed by automated retrieval of\nthe most relevant data chunks to generate customized LLM prompts per query.\nNext, the custom prompts and their responses undergo multi-metric scoring to\nassess for hallucinations and response confidence. The proposed system is\noptimized with user-query intention classification, advanced prompting, data\nscaling capabilities and it achieves over 90% confidence scores for a variety\nof user-queries responses ranging from {What, Where, Why, How, predict, trend,\nanomalies, exceptions} that are crucial for financial decision making\napplications. The proposed data to answers framework can be extended to other\nanalytical domains such as sales and payroll to ensure optimal hallucination\ncontrol guardrails.",
        "pdf_link": "https://arxiv.org/pdf/2311.07592v1.pdf"
    }
]